id	title	review1	review2	review3
001	3D CVT-GAN: A 3D Convolutional Vision Transformer-GAN for PET Reconstruction	The expression and figures can be improved for better understanding and clear illustration.	"The quality of the low-dose PET in Fig.2 is actually quite good. It leads to the question whether this application is meaningful. This can be proved by either the similar performance on a much higher dose reduction rate (visually worse image quality) or the comparison of the diagnosis (e.g. classification of NC vs MCI) on low-dose and each synthesized standard-dose PET.
Even for a patch-based model, a dataset of 16 subjects is still very small.
The improvement in Table 1 is very subtle. In table 1 and 2, add statistical test to show significance."	It's better to provide more clinical evaluations. Consider to use a bigger dataset.
002	3D Global Fourier Network for Alzheimer's Disease Diagnosis using Structural MRI	"in page 2 correct ""explain f_ff in details"" to ""in detail""
In Fig 1 correct layer nrom to layer norm."	-Why the proposed approach only applied to Alzheimer data. We suggest testing the pipeline on many public data...,etc. The paper is almost close to clinical paper. We suggest author to note the novelty in technical aspect.	"(1) It would be better to verify how much the  frequency domain improve the performance.
(2) It would be better to report AUC values for such binary classification.
(3) It would be more useful to report results on MCI classificaiton.
(4) How many patch extracted in each image? The classification results decrease along patch size, it is because of the reduce of patch counts?
(5) Some typos, e.g., 0.915+-0.031 should be 91.5+-3.1."
003	4D-OR: Semantic Scene Graphs for OR Domain Modeling	Please see weaknesses.	"Please find below a set of comments/questions that I believe can help improve the paper if addressed:

Did you consider having additional data as input besides images and point clouds? I am talking about sound or signals from electronic equipment (e.g. oscillating saw or drill). It seems complicated to imagine future OR to have six RGB-D cameras installed.

Regarding the RGB-D cameras: how was the placement decided? How is the calibration between the cameras done, I mean the extrinsic parameters enabling to fuse all point clouds properly? More details about this could be useful to the reader.

Could you give more information about all pre-processing steps for the point clouds. The fused point cloud seems noisy in the figures. Did you consider working directly with the depth images instead? Could you discuss how much ambient illumination can be an issue for your approach? The scialytic lamps were always off during the recordings?

As mentioned before, I think the paper could benefit from more details about data. Knee replacement surgeries can have different workflows and surgical gestures depending on many things such as surgical planning (mechanical or kinematic alignment), type of implant, surgical technique. Were all simulated surgeries the same? Is the ancillary used (such as intramedullary rod, cutting guides, pins...) also included in the simulation? It seems to me that the specific parts of a knee replacement surgery happen when the knee is exposed, and your dataset rather considers the activities happening around the OR but not directly the surgical gestures.

How would this method be applied in a clinical setting? How do you envision its integration to routine clinical practice?

Could you provide the list of relationships and activities considered? I do not see things like bone resection, implant placement, cement .... Also providing the list of objects considered (only large medical equipment?) can be interesting."	"Can the author provide any information on depth interference among the views.
One can imagine that the accuracy of the model decreases on non-simulated cases. I think it would be interesting to see the affect of perturbed 3D poses on the performance of the model.
Considering the model have achieved high performance, do the authors plan to make the dataset more challenging by adding either extra data or tasks?
The model had achieved almost perfect prediction for patient and head surgeon. Can the author comment if this could be due to the fact that their 3D location are very similar across the data. Have the authors simulated both left and right knee replacement to get different location for the head surgeon."
004	A Comprehensive Study of Modern Architectures and Regularization Approaches on CheXpert5000	Nothing significant. I would have liked to see domain specific models in comparison also.	"Major comments:

I agree with the authors that the small data regime is the most interesting one when exploring transfer learning from the RGB to the medical domain. However, given the large data imbalance in the CheXpert dataset, the authors should report how the dataset was sampled (if random or stratified), and what is the resulting label distribution.
The authors report the results on the standard split, as well as a new split with larger validation and test set. This is based on previous work by Mustafa et al. who pointed out that the standard validation set is too small to allow meaningful comparison. At the same time, it seems to me deeply unrealistic that a practitioner would train on 5K images, validate on 17K and test on 25K images. A more realistic comparison would be for instance to sample multiple test set from the 25K sequestered images to include the variability due to the test set into the confidence interval. I understand that fully solving this problem would be another paper in itself, but in my opinion it is still a limitation and should be at least addressed
Which pretrained models were used in the experiments? The authors mention the timm library, which does not include BiT models.
In Table 1, there are two models BiT-50x1, one trained on 89944 images, and one trained on ""all"".  I would report the actual number of images, and clarify what is the different between the two sets.
In Table 2, the number of iterations for REsNet50 appears to be much larger than those for BiT-* models. It is not clear why and it would be interesting to compare how the different models converge
The description of the fine-tuning setting is not very clear. For BiT, the papers first state that the BiT-HyperRule was used, but then concluded that training with a batch size of 32 with the same plateau scheduler as ResNet yielded better results. As a consequence, it Is not clear which protocol was used exactly for each experiment in Tables 1 and 2.

Minor comments:

The confidence intervals are lacking as experiments for some models were still underway at the time of writing. The results are unlikely to change substantially, therefore in my opinion this is not a major limitation
The paper feels somewhat rushed with several typos. For instance deramatology -> dermatology (page 4), presumambly -> presumably (page 4), it's small size -> its small size (page 5), smalles datasets -> smallest? Datasets (page 6)
Some sentences have grammar issues or lack the subject. For instance, at page 5, Split to intra: automatically created from report, and valid: 234 manually annotated X-rays

[1] Matsoukas, C., Haslum, J. F., Sorkhei, M., Soderberg, M., & Smith, K. (2022). What Makes Transfer Learning Work For Medical Images: Feature Reuse & Other Factors. arXiv preprint arXiv:2203.01825."	ViT's benefit from large pre-training dataset yet the authors conducted experiments on the same dataset. It would be good to obtain more data - perhaps not exactly chest x-ray but other related medical images - to see the benefit of ViT pre-training.
005	A Deep-Discrete Learning Framework for Spherical Surface Registration	"The authors pointed out rotation equivariance does not hold in S3Reg, in which a filter orientation is inconsistently represented at the poles. This might be motivation of this study in using MoNet. I like a rotation-equivariant approach but was disappointed in how rotational equivariance was used in this work. Although I do understand the orientation problem, the MoNet approach (or rotation equivariance) seems not critical in the proposed method. In particular, MoNet was used for rigid alignment as preprocessing in this work, which may be able to capture the global orientations because of rotation equivariance. However, rigid alignment is relatively quick and generally works well without learning in conventional methods such as FreeSurfer or Spherical Demons. When it comes to spherical registration, rotation equivariance seems not valuable as the input data are already rigidly aligned. Also, I think ""good"" rigid alignment is key in the proposed method because of the label order, which may not be modeled even with rotation equivariance. In this step, MoNet and Spherical Unet (or other spherical CNNs) perhaps make no huge difference in terms of the label inference. It is unclear how critical the orientation failure at the poles is on overall registration performance from a practical view.

The overall registration framework is heavily based on [22,23]. The proposed registration is clearly faster than conventional surface registration. Since diffeomorphism is implicit yet, how about the regularity of the deformations (e.g., self-intersection)? Does the proposed method unfold flipped triangles (triangles with negative area)?

In the introduction, the authors claim that ""there is a limit to these improvements since cortical topographies vary in ways that break the diffeomorphic assumptions of classical registration algorithms"". I am unsure if this is overcome by the proposed method. Since the framework follows [22,23], what makes methodological difference from the classic methods? Any supporting examples?

In the results, Spherical Demons seems better in all benchmarks except for computing time. Although the proposed method is fastest, the computing time is often not more important than registration accuracy in many neuroscience studies. Can the authors comment on the results? Along this, some manual labels (e.g., cortical parcellation) would be also helpful to understand the performance of the work, though I am unsure if this is feasible in this work."	"Overall this is a very nice paper, I would recommend addressing the question of diffeomorphism. Some additional specifics could be provided, for example it wasn't clear until seeing Figure 3 that the features being matched were sulcal depths; the loss function could also be more explicitly stated.
It would also be interesting if the approach would also work well for non-learning based registration, i.e., for optimizing the loss function for a given pair of inputs. After all one really wants the best registration possible, even if it takes a bit more time (i.e. a few iterations of gradient descent) to compute the solution."	"I enjoyed reading this paper (see comments above). A few minor points:

More details about the loss function desirable. While the authors state loss function components for different parts of their pipeline, it is unclear whether all components are jointly or independently trained (Rotation + DDR1 + DDR2). It would help to state the loss function including the cross correlation term (maybe instead of equations (1) and (2) if space is an issue).
Formulas for J and R are the same (page 6)"
006	A Geometry-Constrainted Deformable Attention Network for Aortic Segmentation	"My main comments to improve this article are :

Provide the Hausdorff distance
Indicate the results according to the diseases
Some errors in the text, particularly the values in the discussion
Specify the units for the Hausdorff distance and the RMSE"	"For major points please see 4) and partly 7).
In addition:

Minor typos/punctuation errors are present throughout the manuscript. Although minor, I would suggest the authors to read once more through the text and figure captions. 
E.g. sometimes it says ""extracter"" and some ""extractor"". I assume the latter is correct?
What do the authors mean with ""reference point"" in 2.2?
Figures need to be replaced later in the text. Ex: fig.3 is mentioned much later.
Figure 4 is currently not mentioned in the text.
conclusions: the reported error should have a +/- sign?
It would be beneficial to add more details about image statistics (e.g. size) and about ground truth generation protocols/processes.
While it is meaningul to show that the proposed approach outperform other methods, it would be also interesting to show how the different methods perform on the different classes (dissection, coarctation, etc..), perhaps this could be part of a larger journal version."	"The methodology is in general well described. The following comments could be taken into account for further improvements.
In the experimental part, what technologies are mainly used in other methods[2,3,4,5,18.27]? Comparison should be added in related work or the experimental part to better demonstrate the advantages of the method in this paper.
In clinical application, is the vascular segmentation and diameter calculation real-time? It is suggested to increase the calculation amount compared with other methods.
There are some expressions which should be clarified, though, as they are sometimes difficult to interpret:


Sec 2.3: The number of center-points, M, should not be a component of C, which would make formula(5) ambiguous. M should represent the number of centerpoint information tuples. If my understanding above is correct, I think to rewrite for C^m = (l_m, v_m, d_m), C = {C^m
m = 1,... M}"
007	A Hybrid Propagation Network for Interactive Volumetric Image Segmentation	"Adding a small user study showing better accuracy with less interaction time/clicks with this method compared to others would move this paper from good to great.
Another way to expand the validation would be to include other image modalities in the analysis."	"In Fig. 1, space character is missing for some labels.
The language needs some improvement:

""refine the volume segmentation result a time""
""both two networks for slice and volume propagation are included in our method""
""This two networks collaborate with each other for segmentation.""

In Fig. 3 including the performance for one or two baseline methods would have been nice."	"Overview

I am not sure to understand why we need both a 3D and a 2D network.
How does the network know when to stop the propagation (at the of the ""end of the organ"")?
How is the 3D crop first defined? How does the network know the extent in the out-of-slice direction?
Is the volume network trained at a fixed resolution in pixels? (or in mm?) or do you train it with varying input size?
Figure 1: The slices shown are upside/down (CT table is on top of the images), it's not very important but it does look weird.
Figure 1: What do the yellow arrow mean above/below the memory modules?

The experiment numbers are not very detailed.

Reporting the mean is not enough: we need more details (at least standard deviation, quartiles or even better full boxplots or violin plots)
No statistical tests were performed
Dice coefficients are only one part of the picture, other complementaty metric Hausdorff distances
How many interactions were necessary to obtain the results from Table 1?
Failure cases/areas of improvements are not discussed (also no future work is mentioned in the conclusion)

Misc

""Given a 3D volumetric image, the user interaction can only be performed on one of all slices,""
I disagree with this statement, several commercial and open-source software (Slicer, ITK-SNAP) offer 3D brushes.

I appreciate the comparison with several other baseline methods, but it seems to me that in the context of interactive segmentation, the current standard is not necessarily deep learning-based approaches. It would have been even nicer to compare with other standard approaches like watershed, grabcut [https://docs.opencv.org/3.4/d8/d83/tutorial_py_grabcut.html], random walker [http://vision.cse.psu.edu/people/chenpingY/paper/grady2006random.pdf], etc.

Figure 2

Which cases are shown here? median ones? good ones? Please indicate how they were selected
Some images with a particularly low quality (first two crops), please regenerate them

A 3D visualization of the different segmentations would also be nice

Does Figure 3 contain average numbers over the whole dataset or just one example?

There are a number of typos:

""This process will stop when reach a stop slice,""
""Note that the VPN is not work in the first iteration""
""both two networks for slice and volume propagation are included""
""This two networks""
""to crop and generate volume patch input of our VPN"""
008	A Learnable Variational Model for Joint Multimodal MRI Reconstruction and Synthesis	"(1) The authors are encouraged to strengthen the introduction section.
(2) A detailed analysis and explanation of the proposed model is recommended. 
(3) Paper should be reorganized to make the presentation of the optimization uncluttered."	"Please add clear description on how you pre-processed the data and generate the k-space inputs, which is important for people to use it and evaluate it.
I have a comment on those contrast synthesis works in general: usually, deep learning model can succeed in medical image synthesis because the output information is encoded in the inputs (e.g., High Fidelity Direct-Contrast Synthesis from Magnetic Resonance Fingerprinting in Diagnostic Imaging). But for those T1w -> T2w, I don't think the contrast information of T2w is encoded in T1w, nor FLAIR, which means that its hard to guarantee that the output contrast is authentic, can you elaborate on this point?
In the paper, you showed a few examples on from 2 contrasts to 1 contrast, I wonder do you have any experience on generating 2 contrasts from 1 contrast?"	"The convergence of the learnable algorithm is proved in the paper, which can show the convergence curve of the optimization objective, so as to ensure the consistency of the theoretical and experimental results.

Add experiments to verify the advantages of double-layer optimization superparametric design over general fixed superparametric design."
009	A Medical Semantic-Assisted Transformer for Radiographic Report Generation	"1.You can draw the frame diagram more carefully and mark clearly what work each module has to do.
2.When you write formulas, you can list them separately instead of confusing them with textual content.

You should clearly describe the working mode of the model testing phase.
You should explain the working process of the MSA module in more detail.
You should evaluate the content of the production report with clinical accuracy."	"Some weaknesses are listed in question 5. 
The figure may needs edition and paper needs some explainations about similar work before.
The experiment on IU X-Ray is lack."	"Not clear if metric scores are increasing while using RL is due to training more for 20 epochs or actually because of using RL. For better comparison please provide a score for training the model without RL for 60 + 20 epochs.
Dc and nm not explained when defined in section 2.1.
mathbfVc in section 2.4 is a typo from latex, please fix it.
Organization and notations for various variables are not well explained and are hard to follow. This needs to be fixed."
010	A Multi-task Network with Weight Decay Skip Connection Training for Anomaly Detection in Retinal Fundus Images	I hope the authors can have a more thorough study on the proposed components, and conduct more convincing experiments on more datasets and compare with more advanced skip-connected networks.	"  The authors are supposed to clearly explain the motivation for the proposed auxiliary HOG prediction task and illustrate the correlation between the auxiliary task and the main anomaly detection task in more detail.
  In addition to the reconstruction results and the prediction A_M displayed in Fig.3, we hope that the authors can provide visualization results of the auxiliary HOG prediction task to verify its effectiveness.
  The authors should evaluate the advancement of the proposed method by further comparing it with the appropriate state-of-the-art approaches for medical image reconstruction. 
 "	"1.In Section 2.3,""with the normal images Taking a gray image"".You miss a period before the word ""Taking"".
2.In the introduction part of the paper, you mentioned the reconstruction-based and non-reconstructionbased methods. References of the reconstruction-based method you've not mentioned."
011	A New Dataset and A Baseline Model for Breast Lesion Detection in Ultrasound Videos	"Experimental results
In the introduction, you mentioned the need for detection and classification. However, for detection you would need to include video clips without lesions. Since all the training videos contained  lesions, the task you are solving is a detection in the case that a lesion is known to be present in the frames. It would be good the make that clear. As well, to use your algorithm, a physician would need to identify a lesion first and then use your method to detect it in the frame. Since the physician would need to collect a video of a region with a lesion, then its identification has already been done and an algorithm is not needed. Thus, the value of a detection method would be if the algorithm could detect lesions in videos with and without lesions with a high true-positive rate and low false-negative rate. An explanation of the value of detecting a lesion in videos, in which it is known that a lesion is present would help too clarify the value of the method.
How many images were typically collected in the videos.
How did the physicians annotate the lesions as benign or malignant? Were the lesions classified by a biopsy or visually? If visually, what is the variability in generating the annotations of the images."	Please either try to justify better your choice of the random shuffling or think of providing a more systematic approach making sure that the method can always take advantage of the global features, if you strongly beleive that these play crucial roles. Thanks for providing enough details and doing the ablation studies.	"Despite the authors' interesting comparison and ablation study, the fact that they used a private dataset, even though the dataset is supposed to be released upon acceptance, makes the judgement of the true effectiveness of the proposed structure difficult, especially given the ever-growing number of studies in the field. 
Of course, half of the paper's contribution is devoted to releasing the dataset, the other half is devoted to the proposed structure design, which lacks specific explanations on how and why the proposed modules are useful. Although, the main focus of the paper is on ultrasound videos, it will be highly beneficial to the impact of the paper if authors experiment their proposed architecture design on different video datasets (either natural or medical).
Because several aspects of the methodology were difficult to follow, giving more specifics would be advantageous. For example, it was stated that:
"" For a current video frame (It), our CVA-Net takes three neighboring images (denoted as Ik, Ik-1, and Ik+1) "", and ""Figure 3(c) shows the details of our intra-
video fusion module, which further integrates three output features (Pk-1, Pk,
and Pk+1) of inter-video fusion from three adjacent video frames."" 
The use of subscripts (k-1,k,k+1) and superscipts (1,2,3) for ""neighboring images "" and ""adjacent video frames"", respectively, is confusing.  If subscripts relate to consecutive frames/images, please make sure your manuscript is consistent in this regard."
012	A Novel Deep Learning System for Breast Lesion Risk Stratification in Ultrasound Images	"It is unclear how the authors handled cases in which the BIRADS (radiologist's assessment) and pathology (cancer/benign) don't agree, i.e., cancer with low BIRADS, or  benign and high BIRADS.  In many cases a suspicious finding is found to be benign, and mammography may reveal additional suspicious findings that are not shown in ultrasound (e.g. calcifications). Were such cases excluded from the training?
How were the probabilities in Figure 1 computed?  If they were taken from a paper, please add a citation to the reference in the caption of the figure.
The computation of the soft labels is unclear:
a.	Equation 1 is unclear:  what are the two summations on?  is the summation limited to a single patient, or involves all patients? what does the ""while"" mean?  Is there any iteration involved??
b.	The following sentence is unclear: ""N_i and N_j represent the total of the images being counted in corresponding sum formula.""  What is the criteria of these images?  Does N_j stands for N_0 and N_1, and these are the number of cancer and no-cancer patients or images respectively?
c.	The task-correlated labels are then used to train the student model that has the same
How did you handle the issue of multiple images by the same patient? Was the presented evaluation (AUC^P, AUC^B, and other measures) made on the set of images or the set of unique patients?  Did you perform any kind of aggregation on the images from the same patient?"	"Would be nice to have an ablation for CSM + CCLF
Consider renaming Table 4 - Teacher to RepVGG-A2 to remind readers that it is the same as that row in Table 2.
How do you calculate AUC^B since BI-RADS is multi-class?
What does [3] do to get better performance on AUC?"	"The average number of images per lesion should be mentioned.
In ""Implementation Details"" section the authors state that the images are resized to 224x224 but the ""aspect ratios remained to unify input size"" this statement needs more clarity as its not evident how the aspect ratio is maintained while resizing to a fixed size.
In Fig-5, the CAM method used should be cited."
013	A Novel Fusion Network for Morphological Analysis of Common Iliac Artery	"Limited innovation in methodology. The U-shaped and Cross-fused is actually not new. They are simple modifications and applications of [12] and [3], respectively.
Insufficient results for validation. Could you add more experiments to show the accuracy of morphological? The paper is the lacks evaluation related to comparing the quantitative results between anatomical information and the ground truth.
The author claims ""the shape and size of the iliac leg are important indicators"", which require some references.
The method is complicated and has included many modules. However, there are modules, such as Hybrid Decoder, that are not well validated.
There are some misspellings in the manuscript. e.g."" the maximum value of the remains represent the largest tortuosity of the vessel""."	"Is there any comparison between the method in the paper and criss-fusion Block[15] or other attention methods, such as spatial attention, CBAM(Convolutional Block Attention Module), etc.
It is recommended to add comparison of speed or calculation amount with Unet or other networks.
Minor problems: Page 4: Ps R^(H+W-1)x(WW) , I think it should be HW
Is there a quantitative analysis for calculating vessel diameter and curvature? For example, comparison with manual measurement through radiologist, or, clinical significance and guidance."	"The proposed morphological analysis algorithm isn't outlined in Abstract.
""make it difficult"" is changed to ""makes it difficult"" in Abstract.
The chaotic topological order of modules in Fig. 1 makes the figure unclear at a glance.
The relationship between the method and task can be better explained.
""represent the largest tortuosity"" is changed to ""represents the largest tortuosity"" In Diagnosis Algorithm"
014	A Novel Knowledge Keeper Network for 7T-Free But 7T-Guided Brain Tissue Segmentation	"I suggest appending the comparison results with methods that ""synthesizing 7T-like image first and then conducting segmentation"" to justify the proposed method."	"Provide more information to verify the effectiveness of the teacher network.
Please further specify how obtain 7T representations from 3T images. (Figure 2 (b))
Recovery tissue details of 3T images are more important than simply simulating intensity distribution."	The author claims that existing methods lack applicability and generalizability on datasets not including 7T images because they require pairs of a 3T image and its corresponding 7T image for training. But in this manuscript, the author still needs paired data to train the teacher model. How to explain that? Besides, I don't know what the practical application is. In my opinion, the task is to segment brain tissues, why don't you segment those directly by developing a better approach? Although the performance will be better if you translate a 3T image to a 7T image, you need 7T images to train the KNN, which is almost not available. So I think the practical application is limited.
015	A Penalty Approach for Normalizing Feature Distributions to Build Confounder-Free Models	"The proposed work is heavily inspired by the previous work entitled ""MetaData Normalization"" (MDN). The MDN approach is removing bias from learned feature representation by using metadata, capturing confounding variables, to estimate the residuals of latent representation and metadata, such that the confounder-free representation is guaranteed to be orthogonal the space spanned by the confounding variables (metadata). While residuals can be obtained in closed-form, such an approach becomes unstable when using batch-based learning. The authors propose to address this problem by replacing the hard constraint (orthogonality) with a soft constraint and performing alternating optimisation.
The proposed approach seems to be effective in removing confounders and to be more stable than the original work. However, there are some issues that if addressed could improve this work.

As mentioned above, the discussion on related work is insufficient and has to be extended to give a comprehensive overview on learning confounding/bias-free representations.
The authors write that the benefit of MDN, and therefore the proposed PMDN, over adversarial approaches is that it can remove confounding effects from multiple layers. I am not sure whether this is a valid argument. If I remove the bias only from the latent representation prior to the network's final prediction layer, I would guarantee that the network's prediction is confounding-free. From a theoretical perspective, I do not see any benefit from removing bias from intermediate representations. In fact, ""Causality-aware counterfactual confounding adjustment for feature representations learned by deep model"" by Neto follow this approach.
Several assumptions implied in MDN are not made explicit or are discussed. Three crucial assumptions are that (i) the confounding variables (metadata) must block all backdoor paths between image/latent representation and the outcome to be predicted, (ii) the confounding variables influence the latent representation linearly, and (iii) that the confounding variables must be linearly independent. It would be best to be upfront about these assumptions and discuss their rational.
Since the hard constraint of orthogonality is replaced with a soft constraint (via regularisation), lambda in eq. (7) effectively determines to which degree this constraint is enforced. A study on the effect of lambda on learning a confounder-free representation would be helpful to understand whether there is a downside to going from a hard to a soft constraint.
The authors claim that the original MDN suffers from instability, yet the experiments on synthetic data in section 3.1 have not been carried out repeatedly to demonstrate the variance of the proposed PMDN is indeed lower than that of MDN."	"The iterative updating of W and \beta can be performed as Block Stochastic Gradient Decent, and thus can have better theoretical convergence.

Another important baseline is Canonical Correlation Analysis (CCA) which maps the MetaData and the learned features in the same domain and compute the correlation. If we minimize the correlation, we can guarantee that the data is independent to the MetaData, which shows the same benefit as the proposed method in this paper."	Q5
016	A Projection-Based K-space Transformer Network for Undersampled Radial MRI Reconstruction with Limited Training Subjects	"Given that the authors effectively interpolate data in  k-space, it would be interesting to see the accuracy on that domain. How well does this mehtod generalize to further undersampling?
Is the ordering of the data essential for MR reconstruction?"	"In Fig. 1b, the spokes were inverse FFT transformed to generate a projection vector. What is the projection vector mean and how does the vector be generated?
Was the data consistency be implemented via the projection step? Based on Fig. 1e, it's not clear where the data consistency was implemented.
In the 3.1 data sets section, it's not clear whether the radial imaging was acquired in a full sampling scheme or undersampled. Were the training and testing based on fully sampled images and retrospectively undersampled images?
After using the transformer network to predict the radial spokes, how the final image was reconstructed?"	Provide comparisons against relevant baselines, so it would be possible to properly judge the merits of the proposed method.
017	A Robust Volumetric Transformer for Accurate 3D Tumor Segmentation	"1) Some baselines' results deviate too much from their original papers. For example, the Transunet and UNETR, in their original paper, they are both better than UNet and nnUNet. However, in Table 1, their accuracies are much lower than nnUNet. The authors need to discuss a little on these results.
2) Why are Fourier feature positions used in the decoding process? Why not use no position information or normal position information? The authors need to do ablation study on this design choice.
3) In Equation 5, why alpha=0.5?"	Ablation study and more discussion on the reason behind reduction in parameters and complexity can be added to make the paper look better.	"It is better to spend more content to introduce the new-designed work and the true contribution of this paper.
The authors should add some explanation and analysis of the model parameters and size.
There are few detailed introductions about the Fourier positional encoding in this paper. The authors should add more details about it."
018	A Self-Guided Framework for Radiology Report Generation	"(1) Please clearly state the difference from previous studies.
(2) The authors need to carefully discuss for which types of image samples your method is more effective and for which samples it fails.
(3) The authors should state what the flaws of the method in this paper are and what future work is possible."	"*	It's not clear what ""data bias problem"" the authors are referring to, throughout the paper.
*	In Knowledge distiller (KD), the dimension reduction is mapping the report embedding to a 2-d space. It is not clear why authors mapped to a 2-dimensional space. Are the UMap embeddings used in some 2D visualization of reports clusters? If the final goal is to do unsupervised clustering, then the authors should consider doing a hyper-parameter tuning to evaluate the best embedding dimensions before clustering. 
*	The experiments don't evaluate the quality of clusters extracted from knowledge clustering. Figure 3(c) is too small to draw any meaningful conclusion.
*	At multiple times, the manuscript lacks proper details and have very generic statements:
o	Recently, some works [7-9] have been proposed for more specific tasks. Which tasks?
o	Researchers have made a lot of attempts and efforts to fill these gaps. Which gaps?
o	SB is a multi-layer bidirectional transformer that has been well pre-trained on two large and widely covered corpora [19, 20]. Which corpora?
o	One reasonable explanation is that our method ignores some meaningless words and pays more attention to the long phrases used to describe diseases. Which meaningless words? Please provide examples.
o	KMVE can help the framework to distinguish image details and alleviate the data bias problem. It's not clear how?
*	In Ablation study, how image-features are calculated for Base transformer without KMVE?
*	The text reads: For the meaningless words like ""the"", ""are"" and ""."", our model assigns relatively low probabilities. While in Figure 3 (a), the probability of . is over 0.9. 
*	The paper is over-loaded with abbreviations that makes it difficult to read."	"1) How does the similarity comparer in RG overcome the image-text data bias? If one image can be reported in numerous ways by different radiologists, then which one are the model generated reports compared to? 
2) The three step process- KD, KMVE and RG should work well for a given imaging modality and a specific lesion detection or classification task. Would it generalize across modalities or transfer to new tasks? To what extent would the model need retraining?
3) Does the number of neighbors and minimum distance set in Dimension Reduction have any bearing on why BASE+SC performs better than SGF for BLEU 1? Would setting larger number of neighbors allow KMVE to understand semantic relationships between distant words?"
019	A Sense of Direction in Biomedical Neural Networks	"My main comment is that the current manuscript is not clear enough to understand the methodology, and a lot of important details are missing.
More precisely:
A better motivation/introduction on the calibrated response shaping is necessary. In the current version of the paper, it is hard to understand without reading [1] what is the motivation of this and what is the novelty in comparison to [1]. The paper is written as if the response shaping was something obvious. Without a better explanation, I would remove Fig. 2 (a) since it is impossible to comprehend without context (furthermore, in the legend the authors use the terms ""stress"" and ""node"" which are not defined).
The parametrization of the kernels is not clear and if they used weight sharing between the different orientations of the same kernel. It is also not that clear why they need two base kernels to cover 8 directions, I think it is to avoid rotating the kernels on smaller angles than pi/2, a comment on that point would be welcome since I think it is a strength of the method to avoid relying on steerability which can lead to resampling issue.
Furthermore, it is not clear from Section 2, what is the position of the proposed approach toward the state of the art (especially toward [1]) and also how the current approach compares to the steerable CNNs. I understand that the information of the local orientation is encoded and propagated throughout the network, but what would be the advantages/disadvantages in comparison to a fully rotation equivariant network?
I have other minor points and questions:
Probably that I am missing the point, but If the authors used weight sharing of the two base kernels for the 8 orientations. Why does the PoRE loss need to be applied to the n=16 kernels? If they are rotated/mirrored versions of each other, I would expect Eq. 5 to reduce to something like 4 times the distances to the two base kernels. 
A quick definition of the metrics would be welcome, for instance, the F1-score can be computed the instance-wise for the MoNuseg dataset, but I think the authors used it pixel-wise.
The authors mention doing an experiment with non-Hybrid G-MASC. First, the term hybrid was not clearly defined (either change the formulation or clearly define it). Second, I would be interested to see the result of the same experiment but keep only the Gabor part.
I would like to see a discussion on the computation efficiency of the current approach compared to the other baseline. I expect it to be a strong selling point.
What is the architecture of the U-Net (Double Conv)?
In Fig 3. f) the rescale is not explained.
To conclude, the proposed method is very interesting in terms of novelty. Furthermore, the experiments are convincing. However, the authors fail to clearly explain and motivate the approach. I had to read [1] to have a better grasp of what the method was."	"The paper is well written and provides a very good evaluation. The MASC approach is an interesting way to incorporate orientation to CNN filters in order to reduce redundancy and parameters and an improvement would definitely be of interest to the community in my view. The presented extensions, in particular the regularisation function could have been better motivated, though. There are some indicators like ""..the response shaping function which we found to be unstable in some cases"" and ""..it helps maintain a stable orientation relationship amongst its kernels"", but the evaluation does not show major differences between MASC and G-MASC. So I am wondering whether the original formulation really has significant drawbacks. At least these drawbacks do not materialise in the performance on the tested datasets.
As for the integration of scale representations, the ablation study does not show significant differences compared to not having them. The authors note that the tested dataset may not be suitable as there are only minor scale differences. However, I think as the scale integration is one of the proposed novelties, it would have been good to provide an example to demonstrate an improvement."	"Those names across figures and main text should be consistent. For example, ""Rotational penalty reference node"" in Figure 2 has not been mentioned in the main text. Also, some illustration is missed. What do (c) in Figure 3(b) and Figure 3(c) mean? What does ""c kernel sets"" in Figure 3(f) correspond to?

The authors mention that ""a smaller set of training patches (4000 samples) was used in the ablation experiments to show differences more clearly"". Does it mean the proposed modules only show obvious efficacy when the dataset is small? What will happen if the data size is large?

I appreciate authors have reported the parameters, but considering the proposed network involves some matrix calculations, I believe some metrics like FLOPS can complementarily show the efficiency of the proposed method.

Supplement more comparison between G-MASC and MASC, e.g., it would be good if the authors compares some directional kernels of G-MASC to those of MASC in Figure 1."
020	A Spatiotemporal Model for Precise and Efficient Fully-automatic 3D Motion Correction in OCT	"The paper was quite clear and well organized. The introduction section was very detailed and contained good details on what the state of the art is, and what are the differences introduced by the proposed method.
The method is well defined and described. The contributions are made clearly, and the method provides several improvements in speed, accuracy and in comfort to the patient, as less acquisitions are required.
The weakest part of the paper would be the experiments section. There were no accuracy comparisons with competing methods, just time comparisons. Although estimations are provided according to the characteristics of the diverse methods, the lack of hard numbers decreases the confidence on the provided results, and it does not really facilitate obtaining an objective comparison by the reader. While this is justified and explained by the authors, one wonders if one or some of the competing methods could have been replicated, or the authors contacted to run experiments in the utilized dataset. The dataset is not available and there is no word on if it will be made available in the future. While its characteristics are described in good detail, having a common dataset with ground truth metrics would facilitate that future methods can be compared with the presented one.
The discussion is quite interesting. It goes into detail on why there was no direct comparison, and it makes an effort on providing estimates on the range of error of competing methods. It also provides a good analysis on what the future steps for the framework might be."	There is no conclusion in the paper.	"1- author should provide complete reference which are written in the paper (table1, Zhang? Ploner? Makita? Chen?)
2-  in terms of accuracy comparison , some team have worked on this topic for long term which are missing in references such as Martin F. Kraus et al, your results should be comparted to them"
021	A Transformer-Based Iterative Reconstruction Model for Sparse-View CT Reconstruction	"1)	In this paper, one of the main contributions was the iteration transmission module. AirNet contained dense connections between iterations, which was a different form of iteration transmission. It is recommended to compare the proposed network with AirNet.
2)	Ablation study was needed to verify the effectiveness of the proposed iteration transmission module.
3)  As shown in figure 3, local and nonlocal regularizations are alternatively conducted in the proposed method. Does it work better than the workflow using only nonlocal regularization? Please give explanations or make a comparison.
4)  In Result section, only 64-view-reconstructions are presented. Could the proposed method handle the reconstruction with fewer projections with little degradation? This should be added to the manuscript.
5)  According to the paper, both nonlocal iteration block and the iteration transmission improve the reconstructions. Whether the NLIB or the IT makes more contributions to the reconstruction improvement? Please make the comparison.
6)  The authors might modify their writings. There were some grammatical errors, e.g., in section 2.2, 'After the merged windows are fed into ...'"	"a.	The authors should clarify the motivation of IT module, i.e. why existing networks cannot extract deep features.
b.	Task-based evaluations should be added.
c.	Clinical metrics should be adopted."	"The authors are expected to provide the evaluation using full dimension dataset, usually with more than a few hundreds of 2d projection data, each of more than 1024*1024 dimension. 3d CT images are also expected to fully demonstrate the calculation speed and memory consumption of the proposed network.
The FBP operation is not a common way to perform backprojection though the literature mentioned this operation (ref.28). It destroys the conjugate property of the forward and backward operations. Not sure if the authors derived the formula which may justify the FBP operation.
The presented results are in perfect low noise level. The authors are expected to evaluate the algorithm when facing high noise fluctuation."
022	AANet: Artery-Aware Network for Pulmonary Embolism Detection in CTPA Images	None	"At the end of the first paragraph of the introduction section, the author only stated that the high false positive PE diagnosis was partly caused by doctors, and did not discuss other reasons in detail, such as the impact of the gray, size and other characteristics on the CTPA images.
At the end of the third paragraph of the introduction section, the data set mentioned by the authors should be cited.
In the AANet of the Methods section, the authors did not explain the role of using different numbers (two or three) residual blocks, the difference between stdConv and Conv, and the role of fusion multiscale modules for PE segmentation.
There are too few method measures to confirm the accuracy of the method.
In the results section, the PE 3D or 2D segmentation results should be compared with the ground truth.This method should be compared with other methods.
The discussion and conclusions are insufficient, and more should be discussed about its advantages over other methods and its possible future applications."	"in Table 1, I believe ""SDiceLoss"" should be ""PSDiceLoss"" based on this paper's context.
Mention if all methods were pretrained with LUNA16.
Show AANnet's performance on the 80+ cases subsets (as mentioned in [13])."
023	Accelerated pseudo 3D dynamic speech MR imaging at 3T using unsupervised deep variational manifold learning	"The authors have strived to improve image reconstruction quality compared to previously available compressed sensing reconstructions. A clear understanding of the target acceleration and image quality required will provide meaningful insights into algorithm development. The approach to leverage a ""latency vector"" is intuitive as an input to the generator and perform the data consistency step. The strengths and weaknesses have been listed above"	"1: According to the data acquisition parameters, the acceleration factor of the multi-slice speech MRI is 9-fold? Could the proposed method be applied for higher acceleration factors or even 3D accelerated speech MRI? Please comment?
2: The reconstruction time of each method should be provided
3: The number of subjects recruited in the study should be clarified in the Section of 3.1"	"(1) The model novelty should be further clarified and stressed. What is the key difference between this model and references [23.24], except for the applications? In my opinion, the vocal tract shaping during speech has regularity, and different letter has different pattern. However, cardiac imaging show distinct regularity.
(2) The equation (1), the gradient operation is applied on both s and t, while in the text it seems only t is differentiated.
(3) The authors claimed the network learns noise in the reconstruction when certain number of epochs is reached. I wonder if it is due to the self-supervised nature of the proposed method (over-fitting). More analysis would be helpful."
024	Accurate and Explainable Image-based Prediction Using a Lightweight Generative Model	"It is not clear to me why age is used as covariate for gender classification while no other variables are employed when doing age prediction.
I would also like to have seen mentioned the training times of the SFCN and RVoxM methods. And what is the age range on the UK Biobank."	See above for my comments.	"The paper is generally very  well written and easy to follow. I didn't see any major change in flow either. However, the paper initially focuses a lot on an ""explainable"" generative model, and terms like ""causal forward model"" that are not used in the paper whatsoever. Deep learning models also follow a ""causal structure"", the only difference being the model/hypothesis space, which has implications on learning capacity and optimization. 
However, I think the Bayesian method proposed is overly simple. Given only a upto two raw covariates (age, and gender), the generative model is simply a linear combination of the two variables with additive noise, which doesn't seem to have sufficient model capacity. 
The statement ""... naive Bayesian classifiers can empirically outperform more powerful methods when the training size is limited ..."" which is also the premise for the proposed method is a commonly known fact - naive bayesian classifiers have low model capacity, and are less likely to overfit and have better performance than high-capacity models (deep networks). However, then the paper goes on to say ""...even when the number of training subjects is the thousands, our lightweight linear generative method yields prediction performance that is competitive with state-of-the-art nonlinear..."". So it is not clear which case the proposed method is handling - the low or high data regime. Experiments however, do not support the claim as SFCN outperforms the proposed method at every training set size.
In page1, the phrase ""In a recent survey on single-subject ..."" the survey itself is not cited. Please cite it. 
In the subsection ""Discriminative methods are hard to interpret"",  explainability of attention models is questioned but the proposed method doesn't solve or handle the issue itself.
[1] LIME: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. ""Why should i trust you?: Explaining the predictions of any classifier."" Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016.
[2] Shapley sampling values: Strumbelj, Erik, and Igor Kononenko. ""Explaining prediction models and individual predictions with feature contributions."" Knowledge and information systems 41.3 (2014): 647-665.
[3] Selvaraju, Ramprasaath R., et al. ""Grad-cam: Visual explanations from deep networks via gradient-based localization."" Proceedings of the IEEE international conference on computer vision. 2017."
025	Accurate and Robust Lesion RECIST Diameter Prediction and Segmentation with Transformers	"Transformers on CNN is not a new contribution - as noted in the paper, this is based on prior work. Instead, the authors can list SOTA performance as a contribution.
How did you select the loss weights (lambda)?
Do you only predict two heatmaps in step 1 (bounding box)? Please clarify in the text.
Do you apply the second consistency loss (distance to segmentation boundary) for the bounding box corner prediction in step 1? Please clarify in the text. If you do, this seems suboptimal as bounding box corners can be expected to typically be outside of the segmentation boundary. What is the impact of omitting this consistency loss in step 1?
The ablation on consistency losses and model components is useful. Please perform a statistical significance test since results are so similar.
Please better detail the weak supervision method. Is the first pseudo-mask an ellipse based on RECIST diameters? Are the second and third pseudo-masks the intersection of the prediction with the previous pseudo-mask? Is there an ignored region? Is step 1 (bounding box prediction) trained without the segmentation objective?"	"(Note: In the following, original references are referred to as [X] while newly introduced are referred to as [rX])
Results

As pointed out above, while I absolutely understand the clinical motivation of the method at hand, I did not feel completely convinced regarding the practical impact of the method at hand.
- This is, first, as a click-based segmentation is part of typical clinical assessment software. Would it be possible to point out the main differences to these solutions?
- Secondly, while deep learning-based solutions often constitute the state of the art, especially for this purpose well-functioning more classical algorithms are already existing in a clinical deployment.
- Would it thus be possible for the authors to point out why only deep learning-based solutions were taken into account for comparison?
- Would it further be possible to briefly comment on the very large authorship overlap across the chosen comparison algorithms (5 out of 9), namely [2] and [15-18]?
- Could the authors briefly comment why they did not compare to other state of the art semi-automatic segmentation approaches, such as [r1-2]?

Further, to me the claimed superiority seemed to be rather questionable:
- While the authors provided standard deviations, they did not conduct any statistical testing. Many of the depicted standard deviations, however, imply that the results are not significant, in particular if taking into account values such as the Dice coefficient of AHRNet in Tab. 1, but also the long- and short-axes deviations in comparison to PDNet, or the segmentation-based results in comparison to TransFuse. As a result, I was not able to rule out the possibility of mere difference by chance.
- If possible, I would like to ask the authors to add significance tests, such as t-tests, in order to rule out this possibility.

Regarding the nnUNet results, it did unfortunately not become clear to me why the authors did not provide segmentation values, since nnUNet by its very nature is a segmentation approach.
- Further, on p6, the authors state that the listed results have been copied from the related nnUNet paper. The paper, however, does not contain an evaluation on the DeepLesion dataset. Could the authors briefly comment on that?

Method

To me, the value of L_cons1 and L_cons2 might have been better motivated and assessed in more detail. Due to their formulation, both L_cons1 and L_cons2 are likely to provide only sparse feedback.
- While the authors have assessed them in an ablation study, the results to me seemed to be mostly inconclusive. This is especially, as the results of the heatmap accuracy did not seem to change significantly while already being better than the regression output, neither did the Dice coefficient. Thus, finally the introduction of L1 and L2 does not promise a significant improvement, but a significant additional effort during the training.

Drawing the axis the other way around, i.e. exchanging start and end point for each axis, would lead to a total of 4 permutations, which all would describe the same axes.
- How do the authors ensure consistency across the keypoints to combat this issue?

Some of the hyperparameter choices seemed rather arbitrary and barely motivated.
- The choice of N_en=6 layers seemed somewhat arbitrary. Why did the authors chose exactly 6 layers?
- Similarly, why was the weighting of lambda_1 to lambda_5 on p5 optimal? How was this assessed?

Discussion

I felt that in light of the above mentioned points some of the statements in the discussion were in my perspective not sufficiently shown.
- I would recommend to mitigate some of the claims in the result discussion, namely the consistent improvement (p8), and that long-distance information is encoded, which was not directly assessed (p8).

Further, the derivation of RECIST diameters from segmentations in my perspective is mostly state of the art, and is done by likely all large vendors as part of their clinical assessment software, as well as by a variety of institutes who have previously published on lesion segmentation. In a publication from 2008 [r4] this has already been part of the used software. 
- I would therefore recommend to not state this as a major contribution of the manuscript at hand (cf. p7).

The results on the DLS dataset felt somewhat difficult to interpet without further context. While an accuracy of 91.7% at first sounds positive, this is a rather low value if 95% of the lesions would show a similar response pattern (and thus the method would be worse than informed guessing).
- As the dataset has not been further used in the manuscript, I would recommend to leave out this information, or to add a separate supplementary with a more thorough evaluation on that, which may then be referred to in the manuscript.

Dataset

The choice of the data should have been somewhat better motivated. It did not become clear to me, how the 1,000 test samples have been chosen and how they were distributed across the various types of lesions in the DeepLesion dataset.
- Would it be possible to briefly add some explanation on this?

Minor

The table styling might be refined. A good resource for this purpose is [r3].
On p6, there is a formatting issue with a larger reference.
The DLS dataset should be briefly described at its first mention, introducing the abbreviation would further help the reader to understand the sentence without having to look into the reference.
Where does the stated accuracy of 99.1% come from (p5, bottom)? Why was this 2% better than the previous result? Could the authors add a note on where to find this data?
Fig. 2 seemed to be a bit small, which first made it difficult to me to follow the discourse in Sec. 2.
The manuscript currently contains a few typos and language issues that the authors might want to fix prior to publication: ""On the other hand, transformer is designed [...]  which is able to [...]"", p2; ""which attracts tremendous attentions increasingly"", p2; ""[...] matrix Q is token as input"", p4; ""The purposed MeaFormer"", p4;

References
[r1] Lin, Zheng, et al. ""Interactive image segmentation with first click attention."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020.
[r2] Mahadevan, Sabarinath, Paul Voigtlaender, and Bastian Leibe. ""Iteratively trained interactive segmentation."" arXiv preprint arXiv:1805.04398 (2018).
[r3] https://people.inf.ethz.ch/markusp/teaching/guides/guide-tables.pdf
[r4] Jolly, Marie-Pierre, and Leo Grady. ""3D general lesion segmentation in CT."" 2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro. IEEE, 2008."	"I have read this paper. My advice is minor repairs.
In this paper, the authors propose a transformer-based network (Meaformer, Measurement Transformer) for lesion RECIST diameter prediction and segmentation
(LRDPS), which involves three related and complementary tasks, including lesion segmentation, heat map prediction and key point regression. Two consistency losses are introduced to explicitly establish the relationship between these tasks.
Existing problems:
	In the abstract, you should point out the existing problems in the current research, and then put forward the methods of this paper.
	What is the meaning of the last sentence ""All figures are best viewed in color."" in Figure 1.
	Is there a prediction head in the MeaFormer in step 1? The MeaFormer difference between step 1 and step 2 is not indicated.
	In 2.1, the image I_c and I_d  should be displayed graphically.
	In the backbone in 2.1, why is HRNet-W48 used to extract features?
	What's the internal structure of the transformer you are using? It's best to show it with figure. For example, whether the position encodings are all added to Q, K and V or only added to K and Q. What kind of structure is the encoder-decoder attention you mentioned in the decoder?
	In the final objective function in 2.2, how to set the weight factors of different loss? Is the weight factor set in this paper the best combination of weight factors?"
026	Accurate Corresponding Fiber Tract Segmentation via FiberGeoMap Learner	"In the experimental part, the proposed method completes the segmentation of 103 fiber bundles in total, resulting in an average dice score of 0.93. Some of the fiber bundles segmented by the proposed method are not included in the other two comparison methods, therefore it is unreasonable to directly compare the average dice of the other two methods with the average dice of 103 fiber bundles. The evaluation should be improved by comparing the dice scores of fiber bundles belonging to the results of all methods.

Are the models trained and tested on the same sets of HCP data? Taking one of the baseline methods, TractSeg, as an example, TractSeg uses 105 subjects for training and testing, and used five-fold cross-validation, which is different from the setting of the proposed method, i.e., 205 subjects. The paper must include more experimental details to ensure fair comparisons of different methods.

The two comparison methods in the experimental part were both proposed in 2018. There are new methods proposed in this field in recent years, such as DeepWMA (proposed in 2020). These new methods should be considered in the evaluation.

Why is there only quantitative analysis in the experimental part? It will be more convincing if the qualitative analysis can be included in the experimental part.

The resolution of Fig. 6 should be increased. Currently, if you zoom in to see the performance of individual fiber bundles, they are blurry and hard to see.

Evaluation with only one dataset is not very sufficient to demonstrate the effectiveness of the proposed method. More datasets should be considered."	"The authors propose a tractography segmentation method. This is a nicely designed method and shows a good result. Below are some major concern about the paper.

The experimental evaluation seems not fair to either TractSeg or WMA. The author combined the two atlases together. The two methods have different definitions even for the same tract. This is a known issue due to the lack of concuss. Please refers to  https://doi.org/10.1016/j.neuroimage.2021.118502. So when comparing the tracts that are overlapped in these two atlases, there should be bias introduced. Also the two methods are performed with different tractography algorithms.

Second, the computation of the evolutions is not clearly described. Dice is for volumetric overlap, while prevision and recall are class prediction. Did the authors convert the streamlines to masks somehow?

The application on autism seems not necessary and redundant. Is ""the proportion of fibers"" a fair measure for ""abnormities""? Do we expect ASD individuals have such abnormities? As this is a technical paper, I would suggest adding experiments on additional datasets from multiple acquisitors as performed in TractSeg and WMA studies."	"Congratulations on this great work. It was easy to read and follow. Please consider some minor suggestions, mainly pertaining to some complex sentences. 
Minor:

Please check for complex sentences, and break into multiple sentences to improve readability. See examples below.
Complex sentence in the Abstract: ""Experimental results showed that the FiberGeoMap combined with FiberGeoMap Learner can effectively express fiber's geometric features, and differentiate the 103 various fiber tracts, furthermore, the common fiber tracts across individuals can be identified by this method, thus avoiding additional image registration in preprocessing.""
Not clear, what is implied. Please check the following sentence on page 2:""therefore, these fibers need to be clustered or segmented into a relatively small number of fiber tracts, the fibers within each tract are similar and each fiber tract should have the relatively independent meaning, namely fiber tract segmentation or fiber clustering.""
Complex sentence on page 2: ""Previous studies have focused on three cate-gories of features, which are geometrical [4,5], anatomical [6,7] and functional [8,9] features in chronological order, and seem more and more reasonable and in line with the requirement of fiber clustering, but the uncertainty also increased in sequence, for example, anatomical feature based method depend on anatomical segmentation and registration, while the anatomical atlases are various, registration techniques are also not mature.""
Complex sentence on page 2: ""Accordingly, for FiberGeoMap, we proposed a revised Transformer network, called as FiberGeoMap Learner, which can efficiently explore the FiberGe-oMap features, and then we trained the model with the all fibers from 205 HCP sub-jects [12], the experimental results showed that our method can obtain the accurate and corresponding fiber tract segmentation across individuals.""
Please define the names of the tracts before the first use of acronyms on page 4, and Figure 3. ""3 fiber tracts (STR_right, STR_left, and MCP) from....""
Typo on page 6: ""The hyper-parameters of mainly include:"", remove of or modify sentence.
Complex sentence on page 6: ""In order to quantitatively demonstrate the results, we used dice score [14], accuracy, precision and recall as the evaluation criterion, and computed these values between each predicted fiber tract and the 
corresponding fiber tract atlas from each subject in the test set, and averaged the values among fiber tracts and individuals""
Complex sentence on page 8: ""Considering an extreme situation, for a fiber streamline, our method can only classify the all voxels on the fiber streamline as one fiber tract, but TractSeg may classify voxel #1 as tract #1, and voxel #2 as tract #2, and so on, but actually these voxels should belong to the same class, this apparently did not conform to the common sense in neuroscience and may resulted in the higher dice score than ours.""
Complex sentence in Conclusion on page 8: ""In this paper, we proposed an effective fiber tract segmentation method, which can identify the accurate and common fiber tracts across individuals based on a novel representation of fiber's geometrical feature, called as FiberGeoMap, and we also tailored Transformer neural network to meet the input FiberGeoMap"""
027	ACT: Semi-supervised Domain-adaptive Medical Image Segmentation with Asymmetric Co-Training	I think the paper needs a stronger motivation for the proposed approach instead of simply stating this is what we propose and it works.	"The writing needs to be substantially improved. A suggestion is to avoid using very long sentences which are unnecessary in most cases.
Page 6: change ""Without loss generality"" to ""Without loss of generality""
Page 7: ""ACT-EMD"" in ""The better performance of ACT over ACT-EMD demonstrated the effectiveness of our EMD scheme for smooth adaptation with pseudo-label"" is confusing. Suggest using ACT (w/o EMD).
It is suggested to briefly introduce ""Target Only"" and ""Supervised Joint Training "" in Table 1 and 2. It is not clear how they are implemented. For example, what is training data used in these two settings? 
Table 1: change ""SSAD"" in Table 1 to ""SSDA"""	"The authors should discuss more recent UDA and SSDA works for medical image segmentation, and compare with these methods in the experiment section to further prove the effectiveness of the proposed method.

More cross-modality datasets, such as MM-WHS and Chaos, can be implemented for further evaluation."
028	Adaptation of Surgical Activity Recognition Models Across Operating Rooms	The paper is an incremental work for AdaMatch which is properly cited. Compared with AdaMatch, the contributions are 1) queued predictions; 2) video level augmentation; and 3) pseudo label sampling. However, the authors also remove the random logit interpolation from AdaMatch.	"The description of the dataset could be improved. For example, there is no information about the 28 types of surgery. Did their all come for the same specialty, as gynecology or digestive? What are the 10 phases and how are they defined? How many observers have annotated the data? How the data were merged if there are several annotations for the same surgery?  What are the instructions given to the observers?
The authors did not discuss the limitation of their model. Did some phases are more difficult to recognize than others?
The authors talked about the execution time of their model (supplementary material) but not of other models. Are these execution times similar? and if not, does the performance increase justify the use of the proposed method?"	the paper misses information on the dataset and we need to see other evaluation metrics that are often used in the literature.
029	Adapting the Mean Teacher for keypoint-based lung registration under geometric domain shifts	"The motivation should be further clarified and re-considered. From my experience, it is inherently a semi-supervised registration method that attempts to utilize the unlabeled data via consistency. Claiming this a domain adaptation problem is strange to the image registration community.
Experiments should be re-designed to support their arguments and motivations."	"The quality of the paper could be improved by discussing the following points :

Could the proposed Mean Teacher approach be expanded with CNN instead of GCN and with full volumes instead of points clouds ?

The regularity of the proposed method (negative Jacobian/foldings)

The adaptation to unsupervised registration without groundtruths."	This paper fills in the gap of the need for establishing local correspondence in the context of UDA for registration which is deemed new and innovative. Specifically, the authors combined the optimization registration approach (i.e., LBP) with the mean teacher framework. It is surprising that the proposed method even outperformed the target-only methods (including VM++ and target only), which is typically considered an upper-bound in other UDA tasks (e.g., segmentation or classification).
030	Adaptive 3D Localization of 2D Freehand Ultrasound Brain Images	"It is not entirely clear why localizing the 2D image in the application you described is needed. Is it to find the optimal plane to perform measurements? Although the 2D ultrasound approach can be variable, does it take much time and is the variability significant? Although plane detection can be useful in many clinical applications, I think the motivation for this work needs some more justification.
What is used to normalize NSTD (normalized standard deviation)? Is it the mean value?
Table 1. It would be good to understand ED in units of mm to give the reader a better understanding of the error.
Table 1. Since it appears you performed statistical tests, it is not clear whether you corrected for multiple t-test to avoid a type 1 error, i.e., Bonferroni correction.
I am not sure how to interpret a NSTD of 0.553. It is lower than what was generated in Yeung at al, but I am not sure if it sufficiently low to be used clinically. It would help to know what NSTD was normalized with."	"A possible evaluation of free hand 2D US image location inside a 3D volume, could be performed if the set of 2D images is acquired with a tracking device attached to the US probe. Then the set can be reconstructed accurately and all the plane positions calculated could be evaluated against this reference volume.
What is the voxel size in table 1?
I suggest to include table 3 in the main text"	"The manuscript was written smoothly, and I like reading that. It is better to focus more on qualitative results than providing many formula for training objectives. 
Literature has two great papers in this field that can be cite in introduction:
Mohamed, Farhan, and C. Vei Siang. ""A survey on 3D ultrasound reconstruction techniques."" Artificial Intelligence--Applications in Medicine and Biology (2019).
Mozaffari, Mohammad Hamed, and Won-Sook Lee. ""Freehand 3-D ultrasound imaging: a systematic review."" Ultrasound in medicine & biology 43.10 (2017): 2099-2124."
031	AdaTriplet: Adaptive Gradient Triplet Loss with Automatic Margin Learning for Forensic Medical Image Matching	"Some sentenses are confusing and affect the readability. For example, in Introduction, 'Unlike general CBIR, longitudinal medical imaging data of a person evolves in time due to aging and the progression of various diseases'. In Section 2.4, 'The prior work [22], considered incorporating an additional term that is minimized when a hard negative example is detected.'

Some abbreviations need to be written in full with proper explaination when they first appear, such as CV."	This a great work. My questions according to the AutoMargin method is listed above in the weaknesses section. The authors mentioned that they will test more models and datasets. I totally agree with that.	The paper is very well written and structured, and both AdaTriplet and AutoMargin make intuitive sense. Unfortunately, the paper disregards the large corpus of existing DML methods, in particular more recent sample-ranking objectives and tuple mining approaches, which effectively tackle what both methods aim to do. As such, the paper would heavily benefit from an extended experimental section which compares to some of the more recent sample-based objectives alongside respective tuple-mining methods.
032	Addressing Class Imbalance in Semi-supervised Image Segmentation: A Study on Cardiac MRI	"The paper may consider the following to improve:
Demonstrate effects on the sensitivity of each class (especially the minority and under-performing class).
Demonstrate the value of the proposed method for various levels of class imbalance (imbalance factors).
Further improve the proposed method to convincingly beat SOTA on ACDC. 
Include PCL results for MMWHS.
The writing needs to be polished to improve readability and fix grammatical errors."	"The proposed method is novel and interesting. Can the method be used with fully supervised or self supervised learning? Can the method be applied to radiological images other than cardiac MRI?
In Table 1, the PCL method also shows good results for L=2.5% and 10% in ACDC. But there is no result of PCL for the MMWHS dataset, can the authors explain why?
On page 4, how the range for R_c^k was determined? How about the penalty values  for P_c^CCF and P_c^FR?
""ACDC"" needs to be referenced first time in the text."	"(1) It's highly recommanded to make Figure1 more detailed to include all of the main notations and show all of the components of the proposed training strategy.
(2) Please clarify in Equation (6) that how entropy, variance and confidence are fused.
(3) Please add comparisons on each single anatomical object in ablation studies and/or method comparison to demonstrate that the proposed strategy won't let those good-performing classes decay.
(4) Please have a discussion on, after applying class-wise sampling rate, how choosing pixels on boundary, inside the object, or purely randomly will influnce the segmentation accuracy."
033	Adversarial Consistency for Single Domain Generalization in Medical Image Segmentation	"The descriptions of ADS network architecture are some implementation detail, which is better written in the experiment section to increase the model's flexibility.
There is a gap between the image X and its bathes (X_p and X_n). More explanations are needed to clarify equation 2.
The paper argues that the proposed method outperforms the meta-learning-based methods on computation cost. It would be better to give some theoretical or experimental evidence.
As the proposed method includes an adversarial network, the authors should consider analyzing the convergence of the algorithm."	"The idea of using adversarial domain synthesizer for better generalizability is interesting. However, the authors should narrow down their focus and application scenarios (e.g., which imaging modalities, healthy or pathological data, etc) and show experimentally and (hopefully) theoretically that the method holds the water.
Given that the objective is domain generalizability, showing cases that the method fails is particularly critical.
The authors should present some examples of the synthetic images and explore how samples $z$ affect the ""style"" of the synthetic images.
Figure 2 is inconsistent with the text. Fig. 2 left (blue boxes) shows negative examples are selected from $X$, while Fig. 2 middle shows negatives are selected from $\hat{X}_n$. Additionally, $f_q$ in the text is inconsistent with $f_n$."	I think overall the paper is good and interesting, but the detailed motivation of loss function and network architecture design need to be reported for providing decent insights.
034	Adversarially Robust Prototypical Few-shot Segmentation with Neural-ODEs	"1- Support the claims made in the introduction with correct citations.
2- Improve experimental section with correct comparisons.
3- Compare the work to the existing methods of adversarially robust few-shot learning methods."	"As above, citing relevant references and test your method with auto attack 
Minor: Defence should be ""defense"""	"The authors can perform ablation study to demonstrate the effectiveness of proposed components.
From Fig.2 (right), the proposed PNODE method's performance seems to drop the most as the attack intensity increase, can authors provide some explanation for that?"
035	Agent with Tangent-based Formulation and Anatomical Perception for Standard Plane Localization in 3D Ultrasound	"Some clarifications that might improve readability:

The actions are defined as change in the tangent point location along the x, y and z axis. The section after that definition is confusing: ""we model the agent-environment interaction as a multi-stage motion process by progressively scaling down the step size from 1.0 to 0.01 when the agent appears to oscillate for three steps"". So, what is the final definition of the actions?
Explain how the 2D image is reconstructed from the tangent point.
It's not clear how the ground truth heat-maps were computed. Were those available with the dataset? If not, briefly describe the heat-map generation process.
Improve the Fig. 1 caption. Currently it says nothing about the figure. Figures should be self-sufficient."	The paper is well-written and proposes a solution to a challenging clinical problem.They demonstrate significant improvements over the state-of-the-art methods. Their model has robust performance on abnormal data. In ablation studies, the performance of proposed model has no significant improvements especially in SSIM. The initialization of agent is important to learning performance. The proposed model adopt imitation learning as an initialization of the agent. There is also lack of ablation study to demonstrate the efficiency of the imitation learning. In the comparison experiments, the proposed model compare with  the RL_wsadt and RL_avp which are based on the traditional formulation (8 parameters).  I wonder the performance of the proposed model based on traditional formulation with SCSP and SAR module.	Overall this is a strong contribution to the field with only minor weaknesses, which if addressed, would make the paper stringer. In particular if some of the comments about justifying some claims and performing statistical tests, that would make the manuscript even stronger and more self-contained.
036	Aggregative Self-Supervised Feature Learning from Limited Medical Images	"(1)	There is a typo in Eq.(2), please correct it.
(2)	In Fig.1(b), it is suggested to highlight the iteration of training. So please modify it.
(3)	Please correct line 8 in Algorithm 2.
(4)	The two datasets are separated into training and testing sets according to the ratio of 80:20. However, in the training process, an extra validation set is needed to determine whether a further training iteration will continue. It is not appropriate to use the testing set for model tuning during training. Please elaborate on it.
(5)	The authors use the average classification accuracy as the evaluation metric. Why not use the overall accuracy (i.e., the number of correctly predicted samples/the total number of testing samples)? Please also provide the confusion matrix of the proposed method on two datasets.
(6)	From Table 2, we can observe that the VGG network outperforms ResNet18. But the authors mentioned in Section Implementation details that they use ResNet18 as backbone. Why not VGG?"	"As mentioned above, the Reviewer strongly recommends that authors provide further experiments for the conventional approach where all SSL tasks are combined.
The experiment described in Table 1 at the 'MT-ASSL ACC' is confusing to the reader initially. Therefore, the authors should detail how these results are computed.
It is unclear how the method trains the feature representation  \theta in Eq.(5) given a selected subset A. Is \theta trained from scratch given all SSL tasks in A, or \ will theta be fine-tuned given a new SSL task added to A after each iteration? What is the difference between these two options?
After each iteration, rather than purely combining all SSL tasks in A and training with equal parameters, the authors may apply to set proper weights for each SSL task, e.g., using a grid search to find the best combination."	"If I understand it correctly, by minimizing Eq.(9), L_com is minimized and the similarity between \phi and \phi^\prime is maximized, which contradicts the goal of learning a self-complementary representation. Could you please explain more on this equation?
The data augmentation (i.e. horizontal flip) is very weak compared the ones used in SOTA self-supervised learning approaches such as [1]. By using stronger data augmentations, [1] can be greatly improved. Could you compared the accuracy of the proposed method and [1] with strong augmentations?
SSL training has high computation cost, and iterative SSL training will be higher. Could you compare the total training cost (in terms of training time or FLOPs) of the proposed method and the baselines?

[1] A simple framework for contrastive learning of visual representations, ICML 2020"
037	An Accurate Unsupervised Liver Lesion Detection Method Using Pseudo-Lesions	"How is the AUC computed? What is the measure over which you vary the operating point?
Is AUC computed considering per-pixel detection?
The anomaly map looks like a pretty good segmentation! Please evaluate the Dice score with respect to the reference segmentation masks so that this fully unsupervised method could be compared to supervised and semi-supervised methods.
Please discuss other (tumor) anomaly localization works that rely on image-to-image translation between healthy and diseased data, such as: (1) ""Towards annotation-efficient segmentation via image-to-image translation"" by Vorontsov et al.; (2) ""Visual feature attribution using wasserstein gans."" by Baumgartner et al.; (3) ""Pathology segmentation using distributional differences to images of healthy origin"" by Andermatt et al.
Is the unaltered CycleGAN architecture used in this work?
Thanks for the ablation study; please add a test without SSIM and possibly a test with non-multi-scale GMSD."	"-Please see above

Some details regarding the implementation  of the SOTA algorithms (eg fANOGAN etc should be provided) at least in a supplementary section to improve the soundness of the comparison.
-The rationale of the method proposed to create pseudo-lesion creation should be motivated. Why did the authors not consider more simple shapes, eg spherical lesion?  These pseudo-lesion have normal liver pattern, how do these patterns compare to lesional patterns?
-The authors should detail if the AUC is computed at the voxel level and  if some kind of processing is performed on the reconstruction maps (eg clustering, removal of small clusters etc..)
The paper should be proofread by a native English speaker.
Fig 1 is not correct, should X and Y should look similar, ie have the same background?"	An inherent limitation of the cycleGAN method is that it may not learn realistic lesions, or may learn a very polarized distribution of outputs. It would be important to analyze and discuss the types/sizes/ characteristics of lesions that can and cannot be detected with the proposed approach. It also seems like the the liver regions that are input to the algorithm are segmented from the CT. It would again be important to explain that the method is a proof-of-concept, and addresses a specific problem in the more general pipeline for lesion detection.
038	An adaptive network with extragradient for diffusion MRI-based microstructure estimation	"It is unclear why the downsampled data lack the b=3000 shell. Since the HCP data have 3 shell, it is better to equally downsample all 3 shells.

Is the number of shell or which shell used affect the prediction results? For example, will the error be higher if one just use 60 directions in b=1000 shell and no other shell to test?

While the numerical results show significance, there is little improvement in Fig. 3. Is there any better way to make the results in Fig. 3 more convincing? When compare AEME with MEDN+ or MESC2, it is impossible to notice the differences in Fig. 3"	"Results. The authors used NODDI parameter maps computed on high angular resolution data as a reference gold standard to test their and other methods. This is done and reported with various subsampled diffusion data. The issue here is that the authors also compared their results with the AMICO methods. Although the NODDI toolbox and AMICO both estimate the same parameter maps, they do it in a different fashion and as such, it is not fair to report the difference between AMCIO and NODDI toolbox as the AMICO error. It is also not clear why the authors selected the NODDI toolbox as the gold standard and used AMICO on the subsampled data. The same method should be used for both to clearly highlight the error due to using fewer diffusion measurements. Please either replaced AMICO with the NODDI toolbox results on the subsampled data or use the AMICO maps computed on the fully sampled data as the gold standard.
P7. Figs 3 and 4 and too small to appreciate the difference on the maps. Please increase the size or focus the figures on selected brain areas.
P8. Fig 5 is too small. The axis labels and text are too small to read.
P8. Conclusion. The authors report a reduction in the needed diffusion measurements of 11.25 (270/24). The manuscript suggests the lowest q-space sampling tested was 12 measurements per shell on the 3-shells HCP data, which would be a reduction factor of 270/36. Please correct, or otherwise clarify."	"The adaptive mechanism is the main contribution of this study. Authors may provide a pseudo code algorithm in methods part. Fig. 1 (b) is not informative. For example, what's the 'reuse EG-Unit'? Author may make it specific (reusing the network architecture or the trained weights; how to reuse when there are several EG-Units).
Extragradient-based method was originally proposed to address LISTA in [9]. Authors may further clarify the motivation of using it in this study and provide the justification of the performance improvement brought by extragradient."
039	An Advanced Deep Learning Framework for Video-based Diagnosis of ASD	"Please avoid using vague and self-gratifying terms (such as ""advanced deep learning framework"", where the term advance is not well-defined)."	"The paper lacks technical novelty as it utilizes existing methods or software (Open Face) to extract the features followed by simple approach for reduction and classification using regular CNN. But the paper presented strong evaluation.  The paper has a sound experimental setup and strong evaluation. Another major contribution of the paper is the dataset, which will be made publicly available. 
The fact that the dataset will be made public is a huge PLUS. The research community is of a great need for such datasets.
I just have few comments:

why the faces are blurred in the figure? Due to human protection agreement? If this is the case, how will you share the dataset publicly?
Can you provide more information about dataset accessibility condition and agreement?

Although the paper is well-written, there are several typos here and there. Please do several rounds of proofreading before final submission."	"There is a previous work[1] that also does video-based autism detection. It would be better to discuss the relationship.

[1] Machine Learning Based Autism Spectrum Disorder Detection from Videos"
040	An End-to-End Combinatorial Optimization Method for R-band Chromosome Recognition with Grouping Guided Attention	The research is conducted in a proper way, from problem formulation to experimental results. It is nice to see an example of a properly-conducted research and well-written paper.	"*	Please address the weaknesses mentioned above.
*	""one can hardly to identify"" should read ""one can hardly identify""."	"Please go over the weaknesses and reproducibility issues. The paper is generally good for a MICCAI paper. The reproducibility report does not accurately represent what is in the manuscript, and that needs to be addressed.
I don't think that you need to run any extra experiments, just be a bit more clear on exactly what you did. Stating that the results are significantly better than state-of-the-art without a formal statistical test or comparison to results on a already reported dataset, is misleading."
041	An Inclusive Task-Aware Framework for Radiology Report Generation	"Fig(1) is not clear.
Output after each step is not clear. Notation alone is not clear. 
Task distillation module is not explained properly.
Few other questions need to be addressed:
What information knowledge graph is providing? What is the type of the data that's generated after building the knowledge graph? How is it being used by this network in terms of dimension of the data?
Are the image embeddings and classification tokens processed parallely in the encoder.
What is the difference between classification embedding and token? It's not clear.
There is no mention of ground truth description for each structure.
There is no mention of ground truth labels between normal and abnormal for every structure.
How much performance variation has been observed with auto-balance loss function and weighted loss function can be added."	"Please address the questions listed above.
Other than that I have the following suggestions (non-decision related):
1, do multiple rounds of proofreading especially for breaking down long sentences into small pieces. e.g. in the abstract, the sentence that starts with ""Therefore, we propose"" spans 9 lines, making it really hard to follow.
2, Fig 1. seems to have typos in the red blocks. Since this is the main illustration of a solution, it is not acceptable that misinformation exists."	"This paper is interesting.
Comments below:
Discuss about the possible missclassifications and mislabelling (wrong report generated). Possible ways to handle them and how severe are those errors."
042	An Optimal Control Problem for Elastic Registration and Force Estimation in Augmented Surgery	"Well written manuscript on important topic of image registration and estimation of forces that causes intraoperative organ deformation. The manuscript would benefit from:
(a)	More detailed/clear explanation of the methods used (including the role of tetrahedral mesh);
(b)	Interpretation of the reported accuracy, robustness, and computational efficiency (computation time) in the context of time constrains of clinical workflows;
(c)	 Clear statement of how patient-specific material properties of soft tissues required for the proposed framewrok can be reliably obtained using methods (and equipment) available in clinics/clinical settings (or offer perspective of the use in clicnical setting in a reasonable future)."	"""The box plot shows that significant improvement is achieved by the elastic registration step with respect to the rigid registration result.""
-> Well yes, that was expected! It would be more relevant to compare your method with other non-rigid registration methods. Are statistics for other teams available on the website of your dataset/challenge?

""As these datasets were used to calibrate the method, results are better for these sets than in average.""
-> this is clear bias, but thanks for noticing. Obviously you should avoid estimating errors on already seen datasets, although my understanding here is that no other data is available.
-> what do you mean by ""calibrate"" here, did you tune some parameters with this datasets? Which ones, the admissible forces properties?

rheological parameters vary significantly with the experiment: (E=1Pa, nu=.4) and (E=20kPa, nu=.45) in experience 1 and 2, respectively. Could you explain this huge difference, especially for the stiffness?
why limit yourself to a linear elastic model if you can simulate a hyperlastic law? Computation time is not a factor in this study, and the ""small deformations"" threshold is never clearly known/real.
""avoid the inverse crime""? Strange sentence in this context, although we can get the point.
proof read before the last version (""registration registration"" in the abstract, ""onyl"", ...)"	"1- Dice or Jaccard indices could be included for measuring
2- Jacobian Determinant tool is a powerful tool for measuring registration accuracy. The JD of borders value could describe the expansion and shrink of border points. 
3- Hausdorff distance is another more suitable tool for accuracy measurement"
043	Analyzing and Improving Low Dose CT Denoising Network via HU Level Slicing	"In fig. 5 one subfigure seems to be missing
Table 3 could have in bold the best results for a better visualization
A discussion on how to choose number of ranges and the thresholds would be interesting"	"It would be interesting that authors gave some information about the computational time required by the proposed method.
Authors should include a description of what software framework and version used.
Authors should include a description of the computing infrastructure used.
Give a reference (and/or formulas) for RMSE, PSNR, SSIM.
Summarize the research limitations and future research directions.
English needs a revision."	The authors need to justify why HU value slicing is working theoretically. In addition, they need to discuss the physical meaning of HU value in terms of attenuation. Moreover, they need ablation study with different HU values and discuss how to set HU values. Theoretically, they can make infinite number of HU slices and their linear assumption on performance with the number of HU slices may be violated. They also need to consider weighting in terms in L_recon for different slices. Moreover, learning procedures and implementation details after fusing the features should be illustrated.
044	Analyzing Brain Structural Connectivity as Continuous Random Functions	"Methodology:
-Would there be any advantage to defining U_i as bounded (vs potentially infinite)? In practice of course it is bounded.
-Is the Borel-measurable a required condition? All the regions E_i are closed sets in practice. If not required, why is it mentioned?
-""for any pair (omega_1, omega_2)"": Is this a point, or a small discrete region with dimension dx, dy (eg a pixel)?
-Re ""underlying random function"": Is it practical to encode known biophysical connectivity information about the cortex as a prior (a function with pre-defined shape), instead of assuming no information?
-I was not clear about the interaction between the non-discrete assumptions in the math and the discrete reality of the data: Is the infinite dimensional, continuous surface assumption necessary to develop the method? If no, would it be clearer to define the problem over discrete space (eg a certain sized voxel/pixel)?
Reduced-Rank Embedding:
""infinite dimensionality of the continuous connectivity"": Is this due to a continuous (ie not discretized) assumption on the surface? If yes, is the continuous assumption necessary?
Definition of V_K: Is this effectively imposing a discretization on the 2-spheres?
Misc items:
""assume the U has been centered"": you could just state this, plus ""WLOG""
Fig 1: Bigger axis labels would help readability
Define ""HCP""
Square brackets are clearer for references.
A few typos:
-diffomorph -> diffeomorph
-for for -> for
-it's -> its
-analysis are known -> analyses are known
-analysis N -> analyis of N
-Between subject -> add hyphen, or say ""Inter-subject"""	"The choice of algorithm parameters needs more explanations, e.g., alpha 1, alpha 2 in page 6 and the thresholds for subnetwork discovery in page 7. Why did you choose those specific values? Is the algorithm sensitive to the parameter choice?
Page 6, from the implementation aspect, you gave the computer configuration but still need to report the computation time.
Page 7, $200, $40 and $160 seems typos or latex error.
Page 8, the color bar of fig.2 B is too small and numbers on it are vague. Please enlarge it.
Page 6, 'the fours panels' should be 'the four panels'."	"Validation Issues:
The authors only validate proposed embedding methods with the other three peer algorithms. Nevertheless, further validation is required. The reviewers hope that the authors can validate the proposed technique with current machine learning embedding techniques such as word2vec."
045	Anatomy-Guided Weakly-Supervised Abnormality Localization in Chest X-rays	Please refer to the main weaknesses.	see 5	"The proposed proposed method should be compared with [23].
More state-of-the-art methods need to be compared on NIH Chest X-ray and MIMIC-CXR dataset.
Authors should give more details in how the contrast algorithms were trained such as RetinaNet.
Which part of DesenNet-121 is the observation feature map f_o?"
046	Anomaly-aware multiple instance learning for rare anemia disorder classification	"This paper aims to address an important problem, the idea and the overflow is clear, but i still have some concerns as follows:

For the introduction section, the contributions are not clearly summarized.
Why the proposed strategy can overcome the limitations of the attention mechanism?
For the Mask-RNN, is it pretrained on another large-scale dataset? or it is directly applied to the dataset used in the experiments.
For the Anomaly scoring, why using the Mahalanobis distance in Eq. (3).
The number of comparison methods is relatively few.
It is difficult to understand the difference between the Anomaly method and attention method in Fig. 3 and Fig. 4."	The paper is well-written and the novelty can be deemed as sufficient given the use of anomaly GMM module. However, there are still a few things can be strengthen during rebuttal.	"Please refer to the weakness part in Sec5 for details. The major issues to improve this work include: justification of motivation; more theoretic insight on whether the MIL pooling function; and the enrich of some strongly relevant MIL based related work.
Also, the authors are suggested to validate the proposed method on more publicly available benchmarks."
047	Assessing the Performance of Automated Prediction and Ranking of Patient Age from Chest X-rays Against Clinicians	Please refer to the weakness.	"Some of the results are not clear and not well described. Below are my main concerns:
1) Please define some of the metrics used during evaluation (i.e. MAE ME, and R2)
2) Some of the results mentioned in the text are missing from the tables. For example, a) ""...CNN is saturated with this size of dataset
and the specific modeling approach is only marginally relevant. Using DenseNet- 169 CNN (14.1M parameters) in place of EfficientNet-B3 (12M parameters) led to a slightly lower performance (3.40 vs 3.33)...""
b) ""....Taking a mean ensemble estimate from models trained at the four resolution levels reduces MAE to 2.78 years, which to the best of our knowledge is the best accuracy reported in a heterogeneous non-curated dataset....""
3) Some sentences on the results are very confusing. For example what the following sentence means? Please rewrite it:
We observe actual success rates of 67.1% for the humans and 82.5% and 85.5% for the regression and ranking models; with respective p-values of 0.001, 0.201 and 0.029 we find that the radiologists significantly exceed the baseline expectations, no evidence that the regression model outperforms (as expected) and weak significance for the ranking model exceeding the baseline regression expectation.
Why in Fig. 4 the ""model incorrect"" results are not provided?"	"Very nice study and idea. Well design and delivered.
My only concern is in the human AI comparison.
The authors used test cohort for this which may include images from the six hospitals protocols whic the AI network already know through training. This si not a fair comparison with human experts. The authors need to compare in a cohort where the AI tool never show before the hospital's protocol modality resolution and quality of the images. By this way the comparison is fair with the human experts.
Please kindly verify if there was images from the six hospital in the test cohort which already excisted in the training cohoirt of AI. If yes please remove these images and test again in the new test cohort and observe the differences with the experts. Thank you."
048	Asymmetry Disentanglement Network for Interpretable Acute Ischemic Stroke Infarct Segmentation in Non-Contrast CT Scans	"In the rebuttal, the authors should aim answering primarily about the weaknesses I raised in previous section. I below provide some additional points, that they should also clarify at least within the article in case of publication, if they do not find space to clarify them in the rebuttal.
Sec 2, ""However, this can lead to a trivial solution... network."": I dont think that this would happen, because (I think) gradients of D are not a function of A (because of the addition). So D does not receive information about the shape of A. It just processes X and learns to ""highlight"" specific parts of it (pathology assymetries) to enable segmentation. E.g. if output of D is 0, then X+A-0 = an image without asymmetries, so segmentation is impossible. Therefore to enable segmentation, D learns to ""highlight"" (P) the pathological-assymetries. It would be good to have an experiment to study this (perhaps I am wrong), so that the reader understands the method's behaviour: Just run the whole framework with regularizer = 0. I believe it would still predict the pathology-assymmetries, but with more false positives, which the regularizer resolves via the size/location penalties. If the authors agree that this sentence of the article is not 100% true, then consider replacing this statement.
Sec 3, it is unclear how the tanh activations predict rotation and translation degrees, and how rotation is restricted to 60 degrees. Do you associate -1 and +1 of the output tanh to correspond to e.g. -60 and +60 degrees rotation, and something similar for translation? Please clarify.
""warm-start strategy ... for training D."": Please clarify this a bit more. Do you add supervised cross-entropy loss with G at the very output of D? Which output of D do you penalise to look as G? P? E.g. via L_bce(P,G)? If so, please clarify within text, e.g. by writing ""L_bce(P,G)"". Will help reproducibility.
In the regularizer of ""mean size"", is mean(T(G)) computed for each case (G) separately at each iteration, or is this mean computed over the whole database (mean of all subjects)? Please clarify (also in article). Note that if it is over whole database, it biases the network, which is less likely to predict extreme cases (too small or too big infarcts), in which case it should be discussed.
(minor, in case you find this helpful) I think the title is a bit confusing. Most readers, and this reviewer included, when they see ""Disentanglement Network"", they will assume it is a paper focusing on methodology how to learn to disentangle in a data-driven manner. Here, the method does not really learn how to disentangle one different asymmetries. Instead, the asymmetries are actually modelled (total asymmetry = X-reflect(x), pathological asymmetry via the regulariser, non-pathological via A-P) and the model learns to predict pathological asymmetry P. A less confusing title could be something like ""Pathological Asymmtry Prediction Network for..."" or something like that."	"Applications 
Not sure if it is relavant to include MR image for stroke segmentation such as ISLES-2018/2016. More datasets on demonstrating the idea would be helpful.

Potential issues: 
The authors propose to modify the input image X^ by X^ = X + Q = X + A  - P. This might be problematic as it might generate false positives due to the intensity range difference between X and Q (or X, A and P).  A clarification or discussion on this might be needed.

Clarifications. 
x. Details of the training strategy of the transformation network T. Essentially, a question would be: how good the network is to regress the parameters? The transformation module is very important in the proposed framework as it generate A, which is base for next steps. A clarification on this would be needed. 
x. For training D, is there any weighting strategy for four loss terms? 
x. For self-mirrored version X', which plane/view is used?

Presentation
X in second row of Fig. 4 is not transformed while X+Q is transformed."	"I was interested to find out how the parameters were set - I would like to see some strategy performed to find lambda - unless this was done, if so how?
If the work aims to improve AIS assesment - if it can't detect the right bleeding spots, do you think that the outcomes are overconfident?
I would like to see it perform on a larger dataset, is that int eh scope for future work?"
049	Atlas-based Semantic Segmentation of Prostate Zones	"The major concern has been listed in my comments to question 5.
Also, the comparison with other similar methods (section 4) is not specific enough, more study is needed in this aspect."	As mentioned above, lambda could be viewed as human in the loop. Fig 3 does not clarify for me why lambda=0.4 was chosen.	"This paper is clearly structured, with the motivation for the segmentation work well defined. 
Fig. 2 depicts the Dice for segmentations produced at baseline and with different lambda values for the dataset as a whole. However, it is unclear from this figure if there are some types of cases in which small lambda values yield better results and others in which large lambda values yield better results. For example, does prostate size, presence of BPH, or location of cancer influence which lambda value would be best for use in a specific prostate segmentation? While classifying what types of cases perform best at given lambda values is beyond the scope of this work, it would be interesting to see how the ability to fine-tune results by adjusting lambda on a case-by-case basis rather than setting it as a fixed and evaluating on the entire dataset improves results. If someone who had not seen the ground truth segmentations were to adjust lambda to optimize segmentation accuracy on a case-by-case basis, how would these results compare to others in Fig. 2?
One advantage of automated segmentation is that it minimizes the bias of an individual in performing segmentation. Allowing users to modify the influence of each segmentation technique re-introduces user bias while also possibly enabling more accurate segmentations. It would be beneficial to mention inter-reader variability in this work, including DSC for two-zone prostate segmentation."
050	Atlas-powered deep learning (ADL) - application to diffusion weighted MRI	It is never easy to validate implementations of other's methods. It would be better to use the original code/model for comparison purposes;	"In addition to comments above, the paper lacks details in the methods explanation. In particular, is the atlas built in an unbiased way or towards a given reference ? If the second option, which one ? How re the average tensor images computed, what type of tensor rotation, etc. ?
Also it seems to me that there are mismatches in notations between section 2.2 and eq 2 (what is \bar{T}_k)
On a more phylosophical point of view, there is no clear justification as to why DL would work better on atlas than on the input images themselves. Is there also a reason why not considering full models rather than ""just"" microstructure maps. This is a bit deceiving in terms of novelty. There should be a discussion on those aspects.
That being said the results section is well constructed and the evaluation shows the difference between results without and with the atlas, showing improved results."	"Overall, I have found this paper very interesting and as raised in the strong points there is the novelty of the proposed framework. The paper is clearly written and organized. The proposed framework is methodological sound though some choices are unclear to me. The major drawback of the presented work is the lack of discussion and other analysis that would have been in my opinion valuable to better illustrated the contribution of this work. But I do of course acknowledge the limited space of the MICCAI template. Still, I will raise here below my concerns/comments that could maybe help the authors improve their work in the future.
The authors did a very nice work by including several existing methods for comparison an adding statistical test on the results.
I was wondering on why the authors selected the given downsampling factors that indeed ending with very low number of measurements overall. It would have been interesting to explore which is the number of directions if any where the proposed method does not outperform the others. Or is this systematically the case? For instance 32 directions is still plausible setting for new born data and should be included.
Equally, this reviewer is very curious to understand if the results vary across age of scan? Would have been interesting to see boxplots (in order to understand also if there are outliers) through different age. Maybe if the authors already explored this, some comment can be included.
Similarly, can the authors discuss if there is any specifc anatomical region where method is outperforming? Given that atlases are less accurate at GM/WM cortical interface given large cortex variability, maybe in those areas atlas-prior is less accurate and then not so relevant? How the accuracy change for WM structure where overall these models are most appropriately defined?
It would have been interesting also to push further the study of the influence of the registration error. Authors could have included rotation/transaltion errors as to see how this influences the performance of the proposed method.
I do appreciate the ablation study that is indeed very needed and should be clarify if the results on Table 3 are also statistically significant or not. It is not so obvious though which is the contribution of each of the steps given those values on table 3. At least in my opinion two first columns seem quite equivalent. Please clarify. Also why not including also n= 12 (FA) and n=30 (OD) in that table? Maybe Figure 2 panel c can be removed to give more space for ablation study results.
As the authors already acknowledge the choice of diffusion tensor registration can be questionable. As for accurate registration the use of T2w image seems more appropriate. Though still T2/dMRI is then needed. If space allows some more discussion /justification on this registration choice could be included.
Minor

Authors could clarify since end of introduction page 2 that the down-sample dMRI refers to diffusion gradient direction downsampling and not spatial resolution.
Typo in page 6, 2nd paragraph, twice ""to"" before refernces 30, 17
could the authors clarify if all the methods work at the same spatial resolution?"
051	Attention mechanisms for physiological signal deep learning: which attention should we take?	"The self-attention mechanisms should be described in mathematical detail or schematics.
More citations for the conditions of interest should be provided. The authors cite a few deep learning papers that are applied to similar problems but it would be good for the authors to include some more clinical citations and background.
Some more description of the data would be good. It is not clear if the ECG is 12 lead. Similarly it's not clear how the PPG was collected. It would be good to include these details. The dataset is not cited.
In figures 2 and 3, the fact that higher is better and lower is better, respectively, lead me to a bit of confusion. Explicitly stating this, as the figures look very similar, would be good.
Similarly, the discussion mentions that different self-attention mechanisms work better in each task because of the phenomena that must be identified for the tasks to be solved. Clinical citations for these phenomena would be good to verify these assertions."	"The manuscript presents the comparison results of four attention mechanisms: squeeze and excitation, non-local, convolutional block attention module, and multi-head self-attention.
Although the samples were randomly split into the training set and testing set, it still has bias. Cross-validation is a more fair way to evaluate the models.
The demographic data were used as features for the classification. Statistical analysis should be performed to check whether or not the differences between patients and healthy people in these demographic data were significant.
The results shown in Fig. 2 demonstrate that the attention mechanism did not significantly improve the performance. The authors should provide insights. Does it imply that the attention mechanism is not effective in this case?
It is better to acronymise area under the receiver operating characteristics curve as AU-ROC.
I strongly suggest that the authors identify typos and correct them. For example, ""mean percentage absolute error (MAPE)"""	"Comments:

Need to include more related work that are highly important
[1] Chen W, McDuff D. Deepphys: Video-based physiological measurement using convolutional attention networks[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 349-365.
[2] Zhu X, Cheng D, Zhang Z, et al. An empirical study of spatial attention mechanisms in deep networks[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 6688-6697.
[3] Wang S, Li B Z, Khabsa M, et al. Linformer: Self-attention with linear complexity[J]. arXiv preprint arXiv:2006.04768, 2020.
The authors need to introduce some related work on the self-attention mechanisms in the introduction.

Need more justifications about the novelty claims
This paper comprehensively investigates four attention mechanisms fused with three convolutional neural network (CNN) architectures for two processing physiological signal prediction tasks. Although this paper can provide a good guide for researchers, the novelty still needs to be further justified and improved.
It would be better if the authors could devise some novel ways of combining attention mechanisms with convolutional networks efficiently.
The authors should explore the combination of efficient self-attention mechanisms with convolutional networks.

Need to check for grammatical errors and typos
The editorial quality of this paper is largely unsatisfactory. It contains quite a lot of inconsistent/non-precise description, as also reflected in the above comments."
052	Attentional Generative Multimodal Network for Neonatal Postoperative Pain Estimation	"I enjoyed this paper and it was hard to find issues in the experimental design as well as any criticisim in the techniques used to accomplish the given task of pain estimation. 
The ablation experiment is interesting, although I would expect that the system should require all inputs as it achieves the higuest classification accuracy."	"There are many types of scores in the results. However, which is the most important score from clinical point of view? What cases can the model classify correctly (but other state-of-the-art fail to classify)? These discussions might help to understand and improve the model.
The pain intensity needs to be estimated so accurately? If not, is it usuful to estimate the range of the pain intensity? For example, range1(0-1), range2(2-4), and range3(5-7), it become a bit easier, and the model might achieve the performance good enough."	"1)	Page 2, Paragraph 3, ""i.e., it makes final assessment of pain"" should be ""i.e., it makes the final assessment of pain""
2)	Page 4, Paragraph 2, ""Finally, feature sequences of each modality are used to train the LSTM-based AE ..."" should be ""Finally, feature sequences of each modality were used to train the LSTM-based AE ..."". 
3)	Page 6, Paragraph1, ""All the models are developed based on PyTorch using GPU"" should be ""All the models were developed based on PyTorch using GPU"". The past tense is suggested to be used, please modify the tense of the whole paper."
053	Attention-enhanced Disentangled Representation Learning for Unsupervised Domain Adaptation in Cardiac Segmentation	This paper proposed a new and innovative UDA for segmentation by means of an attention-enhanced disentangled framework. This paper presented a few key innovations for the considered application, though not entirely novel, including (1) the embedding space is disentangled in a channel-wise manner into domain-invariant and domain-specific subspaces; (2) attention bias is proposed to boost capturing task-related domain invariant features. The experiments are comprehensive and the results are encouraging.	"Although #Alignment of Imaging Characteristics# is not your main contribution, it is recommended to add references on cross-domain alignment. (section 2.3)
Some figures are too small to catch all details, and improvement or re-design is recommended. (Fig. 2-5)
As the main contribution, how the HSIC is applied to disentangle representations extracted by the encoder is not clearly explained. It is recommended to describe more details than Eq.(2)."	"The authors should discuss the issues I mentioned.
I encourage the authors to visualize the domain-invariant features and domain-specific features and discuss the level of disentanglement between domain-invariant and domain-specific features."
054	Attentive Symmetric Autoencoder for Brain MRI Segmentation	This paper shows the masked pre-training can improve the performance of the segmentation. Experimental results validate it and I believe it is a good finding for 3D medical image segmentation	"I'd suggest some discussion and possibly adding some evaluations on the model's ability for handling challenged diseased brains with asymmetric structures, and explore/explain whether SPE is still effective on these brain volumes.
It's better to add some visualization results to demonstrate that the proposed model can generate better segmentations on symmetric structures compared with other SOTA methods, in order to verify the motivation and assumption of the proposed method.
Please explain how the SW-ViT backbone pretrained on 1-channel input is adapted to the 4-channel BraTS image in Methods part."	"It is better to add the ablation study about the influence of P.
It is better to add more details about Shifted Linear Window-based Multihead Self-attention (SLW-MSA). I wonder it is computed on patch-level or pixel-level."
055	Autofocusing+: Noise-Resilient Motion Correction in Magnetic Resonance Imaging	The authors should clarify the novelty of the methods in addition to the autofocusing methods. The authors should compare their method with the existing methods using GAN setup since their construction of loss mechanism is similar to an GAN based critic. Therefore, it is important to validate both the novelty and benefits of the proposed model.	"Other comments:

""Compensation for the motion artifacts is referred to as demotion"". I cannot say I have heard the term ""demotion"" before in the field of motion correction for medical imaging.
The autofocus method is vaguely described. Please provide more detail.
The authors mention that the disadvantages of MedGAN are the ""extra adversarial low functions and the associated long training routine"". How does the training time compare with the proposed method? It would be interesting to see how the outputs of both methods compare as well, since MedGAN provides images in seconds and the proposed method takes 7-9 minutes.
Is autofocus+ conceptually similar to medGAN (expect for the deep regularization)? Is the performance of the proposed method superior to medGAN with a L1-norm regularization?
It is not clear how blind motion correction can be performed from solving eq. 5. How is this related to autofocus, which uses entropy as a quality metric?
It is not clear what is Sp, the output of the U-Net. Is it an image or the motion parameters?
Section 3, ""10374 scans for training and 1992 scans for validation"". How many for testing?
The fastMRI database provides about 1500 knee k-space data. However, 10374 were used for training. Please clarify. If DICOMS (magnitude images) were used, please clarify how eq 4 was applied.
Please provide the network architecture details.
Please provide the training time for all deep learning methods and computation time of motion correction for all methods.
Please provide more details about the other methods. How was the regularization parameter for gradMC selected? Are the SOTA and autofocus+ U-NETs the same?
GradMC seems to perform poorly for all cases and sometimes is worse than the corrupted image. Would results improve if the regularization parameter was optimized?
Fig 3. Visually gradMC seems better than the corrupted image, but the PSNR and SSIM indicate otherwise. Similarly, gradMC looks better than autofocusing but the latter has better PSNR and SSIM. Please comment.
Fig 3. Please add a zoomed section to the ""clean image""."	"Motion artifact reduction is more commonly known as motion correction/compensation/reduction than ""demotion"".
Please clearly highlight in Introduction and Abstract that this work only addressed rigid motion artifacts.
Please clarify if motion vectors were selected randomly from the reported ranges. Please specify how many different motion trajectories were generated per image and on what grounds the motion trajectories were selected and taken.
Why were the rotation, translation and scaling parameters not jointly estimated inside one network or at least inside the same optimization steps (instead of shifting between rotation/translation and scaling estimation)?
Was the UNet operated on the magnitude-only or on the complex-valued image?
Were separate networks trained for each motion type or was a joint network trained? If the former, were cross-testings performed (train on motion type A, test on motion type B)?
Was the amount of inner autofocusing iterations empirically optimized or selected?
Did the authors investigate the proposed method's performance on out-of-distribution data, i.e. stronger motion amplitudes, different imaging contrasts etc.? At least for increasing noise levels, the network seems to be only affected mildly.
Was a GPU-based NUFFT implementation used? What is the main computational bottleneck for the proposed approach?
Please report if source code is shared."
056	AutoGAN-Synthesizer: Neural Architecture Search for Cross-Modality MRI Synthesis	"1: More experiments are needed: comparison to more recent and SOTA cross-modality methods; the convergence of the GAN model. Besides, the ablation studies are insufficient. For example, the learning rate.
2: There are not comparison of the InstanceNorm and BatchNorm. It's claimed that the InstanceNorm is superior to the BatchNorm in low-level image processing task"	"This paper is well organized. Authours deal with the challenging tasks. 
They provide comprehensive experiments to verify the advantage of their method and detailed methods in supplementary materials.
I find the Equation(2) is hard to read. First, the ""Upsampled Fr"" is not the equation. Second, authors don't mention how to implement Upsample and down-sample. Please explain your methods."	"It's not sure that by using proposed method whether the authors need to train different models for different number of modality MRI images as input? Or is it a uniform model that no matter how many modality MRI image there are, only one model is trained.
How is the K-space data acquired? I think for those two public datasets they only give the magnitude image.
As K-space Learning has been proposed for fastMRI in other works, I won't think adopting this strategy for cross-modality MRI synthesis can be summarized as a contribution.
In figure 3 and 4, it's better to give the synthetic T2 images by different methods, not just the difference map.
In figure 6, a part of the image is missing in Flair, but after adding adversarial and k-space learning, there is some context predicted in this region, why?
Why do the authors choose to synthesizing T2 from Flair, rather than the opposite?"
057	AutoLaparo: A New Dataset of Integrated Multi-tasks for Image-guided Surgical Automation in Laparoscopic Hysterectomy	"I strongly recommend renaming the dataset. The current name ""AutoLap"" is the trade mark for an autonomous camera system for laparoscopic surgery, see: https://www.youtube.com/watch?v=_fXPgRgTEAY&ab_channel=MST-MedicalSurgeryTechnologies
The term ""Image-guided"" implies that you provide data from imaging modalities such as ultrasound. That is why I suggest the authors use the term ""vision-based"" instead throughout the paper.
On the title: Is the use of this dataset restricted to just automation? If not, I would change the term ""automation"" to be ""activity"". This should widen the possible use cases by researchers in areas other than automation as well.
It is really difficult to reason how this dataset is useful without actually seeing the dataset itself (or at least a sample of it). Would it be at all possible to find some way to share it so that we can also review it?
In the introduction: the authors wrote: ""To enhance the surgical scene understanding towards image-guided automation, the most promising solution is to rely on learning-based methods"". I would say ""one promising solution"" instead of ""the most promising"". The latter is a very strong statement that need substantial and clear evidence.
I find the word ""task"" very confusing in the context of this paper, with respect to the use of this term in automation for surgery papers. In these papers, the term tasks or subtasks refer to tasks such as suturing, knot tying and so on. To avoid this confusion, I recommend that the authors use another term.
On the data set collection: Were all the videos coming from one surgeon's practice or from multiple surgeons? If it is the latter, did the authors account for the individual preferences of surgeons (such as their preferences on moving the laparoscope ) by any means in their dataset and models?
On the data set collection: Did the imaging platform used provide 3D view of the scene? And if so, which video channel was eventually used to record the video (the left or right channel)? Please add this to the description of the dataset.
On the data set collection: Were there cases of disagreement between the annotations of the gynecologist and specialist? If so, how did the authors handled these cases?
The part of the data set on the laparoscope motion can be significantly improved to avoid confusing the reader. This is mainly because of the design decisions that the authors made in this part that are not justified such as:
a. using part of the data set and not the entire data set for motion prediction. 
b. Discretization of the motion of the laparoscope into 6 motion types instead of treating the motion as continuous variable. 
c. The choice of the types of motion which seems to miss cases such as moving the camera diagonally. Currently, I guess this can only happen by discretizing the diagonal motion into two types of motion (e.g., up and then left).
d. Setting T to be 5 seconds
What not considering the entire data set for the part on anatomy segmentation as well?
What is the value of presenting the results in table 4 especially since the accuracy is not very high?"	The authors should revise the paper according to the above weaknesses.	"This reviewer is generally pleased to see a paper that contributes a comprehensive dataset to the CAI field. As the paper is well-structured and provides details/examples of how to use the proposed dataset for benchmarking, this reviewer would think this paper is acceptable and only have a couple of comments listed as below:

Please mention the differences between laparoscopic videos vs robotic laparoscopic videos (tool appearances, etc), this would help the readers to understand that how learning on such a dataset could be transferred to ""towards surgical automation"".

It would be good if the authors can also provides insights how the size of the this particular dataset is correlated to the success of evaluating ML approaches? Is the size of each sub-dataset fair enough for judging the model performance?

It was nice that the authors have provided the benchmarking results of the several existing frameworks. As the dataset is provided with clinical relevance, it would be better that if the author can discuss what are the clinically-acceptable accuracies in these sub-tasks? One common question for most of these existing datasets is that after a researcher evaluate his/her approach on the dataset, how would the researcher know if the approach is doing good enough for the tasks? The readers would appreciate if the authors could provide more discussion on this."
058	Automated Classification of General Movements in Infants Using Two-stream Spatiotemporal Fusion Network	"It is a good work to solve the GM task with explicit and reasonable motivation. And experiments prove its effectiveness. But I think it may be better to show the details of implements of compared methods and give more analysis of the experiments result.
And experiments on more datasets is encouraged to make the conclusion sound. And some latest methods could try to applied in the motion classification network to replace the two-stream network for processing spatiotemporal infomation."	"evaluation on the preprocessing networks (3.1) is missing. One wonders what is the quality of the output. In Related Work authors write ""The pose-based approach [5,18] ... performance depends on the accuracy of the pose estimation algorithm."" to motivate the use of a video-based approach. However, the proposed process is based on OpenPose (and later on Farneback optical flow method [10]) which may have errors. Could they be quantified? 
The underlying question is: is it worth to improve these preprocessing steps to increase the performance or should one focus on the two-stream networks?

No evaluation of the computed flow (Farneback method [10]). How reliable is it? Are outputs noisy, temporally consistent? Same underlying question as before.

It is unclear if STAM [18] was retrained with the new dataset to have a fair comparison with the proposed method.

It is unclear how authors compare to [26]. To the best of my knowledge their code is not available. Did authors re-implement it? Did authors retrain on the new  dataset to have a fair comparison with the proposed method?

Authors do not evaluate on the [26] dataset, which is available ""upon reasonable request"" (https://www.nature.com/articles/s41598-020-57580-z#data-availability). It would have been interesting to have a second comparison."	See weakness.
059	Automatic Detection of Steatosis in Ultrasound Images with Comparative Visual Labeling	Please do a thorough proof-reading of the paper.	"The general idea of the work is very interesting.

It would be interesting to evaluate the proposed approach on several datasets from different medical imaging modalities.

Evaluations on a larger liver US image dataset would probably better highlight the usefulness of the proposed approach.

Authors separated the dataset into four groups for the evaluations, Fig. 3. I think that it would be interesting to directly related the error rate with the liver steatosis level. The mild group included cases with the steatosis level between 5% and 33%. In practice, I would expect the labeling errors  mostly for the cases with the steatosis level between 5% and 10%.

Since the radiologists need to assess pairs of liver US images, the proposed method is more complicated and time consuming than the conventional approach. Results presented in Table 2 show that the proposed labeling method does not improve the classification performance. Unfortunately, this suggest that the conventional method would be probably preferable in this case."	"I believe the text should be further edited. For example, In the abstract the authors do not need to mention SVL because it is never used again in that section, and this sentence ""Code and data will be
made publicly available."" should be moved to probably after you've talked about your data and models in the Methods section. 
The acronym inside the parenthesis in line 12 of the second paragraph of page 2 should be SVL. And also I believe this sentence is missing a verb. Or in the following line it should be surgical ""skill"" assessment. And ...
For this sentence in the results section, ""This indicates that the CNNs have some inherent robustness to training label errors in this task."" authors should either provide a justification or a reference."
060	Automatic identification of segmentation errors for radiotherapy using geometric learning	X	"Next to the identified major weaknesses only a few minor problems are identified:
Page 2, Introduction at the end: An ablation study is probably not a contribution, self-supervision terminology
Page 5, Unsupervised pre-training: pre-training vs transfer learning terminology, self-supervised vs unsupervised pretext task terminology
Page 7, Ablation tests: what is an erratic validation? Loss curves could probably be included, but in combination with the training loss. The validation loss is really smoothed by the pre-training, seeing the curves also helps to understand the statement in the discussion, claiming that the training is smoothed.
Page 8, Discussion: The sentence: ""However, these approaches require the adoption of the new segmentation models themselves."" is unclear. Also: ""... fills a void as most ..."", what is meant by fills a void? The statement that the Parotid Gland is a difficult organ to segment needs a reference.
General: Most of the paper is written in past tense, which, if at all, should only be used while discussing related works."	see  main weaknesses part
061	Automatic Segmentation of Hip Osteophytes in DXA Scans using U-Nets	"This paper deals with the automatic segmentation of osteophytes in hip dual X-ray absorptiometry scans (DXAs). The proposed approach uses U-Net.
The paper is well written.
The contribution is rather fair.
Contributions of the study are not well described in the paper. As contributions, only the performance of the proposed system are reported.
The cropping method of the patches is not clear enough."	"Page 3: While most of the studies in the domain are mentioned, please, add a citation to the recent study on segmentation of ostephytes from MRI - https://www.oarsijournal.com/article/S1063-4584(21)00484-2/fulltext . Also, please, include those results into Discussion.
Page 5: It would be appropriate to elaborate on how the manual and automatic keypoint annotations differ. Perhaps, by showing a sample from each of the groups.
Page 5: The authors say ""Automatic point placements were available for a subset of all images"". Please, provide more informative details on how the subset was selected - certain cohort? / random? / etc.
Page 5: Please, specify software versions for BoneFinder, Keras, and Tensorflow.
Page 5: ""optimized with Adam [28] (default parameter values used)"". Please, state explicitly the default parameter values.
""dropout rate, with probability 0.3"" -> ""dropout rate of 0.3"" / ""dropout with probability 0.3"".
Table 2: The authors use osteophyte detection Sensitivity/Specificity as the performance metric. However, it is not clear from the text what is the detection threshold (> 0 voxels? other?). Please, state explicitly.
Table 2: The standard deviation of the Dice scores are considerable in comparison to the mean. For the reader to better understand the extent of the segmentation errors, it would be informative to visualize additionally either or both: (I) the distribution of sample Dice scores - histogram where (x) sample Dice score, (y) number of samples, (II) the distribution of sample Dice scores at different osteophyte severities - scatter plot where (x) ground truth osteophyte area, (y) sample Dice score. Please, consider adding those plots in the article or as a Supplemental material.
Page 6: The authors make a hypothesis ""could it be underfitting due to insufficient training examples?"", which eventually does not hold. This assumption sounds somewhat questionable, in the first place, since the number of the training/validation samples is actually the highest for ""Lateral acetabulum"". I would suggest to remove this point from the text."	"As discussed by the authors, the initial landmark detection step can adversely affect the U-Nets performance. I would integrate the landmark placement step into the U-Nets architecture to segmentent osteophytes directly from DXA scans.

Approximately 2% of scans were excluded due to poor image quality. Identifying these scans in large dataset with 40,000 scans could be demanding. will you consider automatic image quality assessment?"
062	Automating Blastocyst Formation and Quality Prediction in Time-Lapse Imaging with Adaptive Key Frame Selection	"The authors mention ""kinetic parameters"" as a data input, however, this is never explicitly described/defined. Is this just a frame index / timestamp? Are these manual annotations, or are they automatically extracted? If manual, then it means the method is not fully automatic which is not clear in the current submission. It would be important to clarify.

Table 1: Could you report the number of selected frames for competing methods? At least one of them is mentioned to do frame selection.

Table 2:  I do not understand LSTM having #SF=32. From how the dataset is described, I would expect the number of frames to be in the thousands (every 5 min, over 3 days), so why is LSTM only using 32? Is there any subsampling?

Fig. 3: Images have visible person names on the top. I don't know if this is any sensible information, should it be hidden?

Fig. 3: Could you display which frames (index/time) are being selected?

Qualitative evaluation: ""AdaKFS can select a small number of informative frames of different embryo development stages, such as 2-5 cell and 8-cell stages"". 
A more in-depth analysis of selected frames would be useful: showing more results, including failure cases (in suppl material). Additionally, it would be useful to have more quantitative statistics of specific events that are detected (i understand this is a lot of work though).

Can embryologists make an accurate prediction based on selected frames? I wonder if the selection mechanism could be useful just by itself and make embryo analysis faster / more convenient. Any comments?"	"Do you plan to release the dataset as well as the code?
The definition of morphokinetics parameter should be clarified.
There are several statements which are imprecise: e.g., 'using a small number of frames and their morphokinetic parameters, without any extra annotations' in method. However, the method still requires the classification annotation for training model. The determiners should be added.
What are the dimensions of f_t and k_t?
It seems that the numbers of selected frames for different sequences are different with the proposed method. It is better to illustrate the range of the total length of different sequences, as well as the range of the numbers of selected frames.
In the ablation study, how to determine whether the frame are selected or passed when only using policy network?
It is interesting to see the separate efficacy of kinetics features. The ablation using kinetics with only LSTM could be added."	"Although the authors implicitly indicate how the image features ft and kinetic features kt are generated in the framework shown in Figure 2, it would be better if the authors could explicitly mention that in the method section.

How did the authors tune the hyper-parameters for different methods in experiments?

Page 6, 'We uniformly sample T = 32 frames ...'. Was the same sampling process performed in the testing period? Will the sampling rate affect the prediction performance?

Page 6, last line: The authors may want to cite a reference for 'ImageNet-pretrained weights'."
063	Automation of clinical measurements on radiographs of children's hips	"The manuscript is very well written and easy to read and follow. I just have a few comments:

I would like to bring the following publication to your attention: Xu W, Shu L., Huang C, et al. A Deep Learning Aided Diagnostic System in Assessing Developmental Dysplasia of the Hip in Pediatric Pelvic Radiographs. Frontiers in Pediatrics, 8 March 2022, Volume 9, Article 785480, doi: 10.3389/fped.2021.785480. 
The authors of this paper published results of a deep-learning approach to classify Hip Dysplasia, based on radiographic parameters, including AcI. For AcI, the intraclass consistency for the automatic generated and the manual measured value was >0.75. 
However, in all fairness, with a publication date of March 2022, I would not considers this a ""missed reference"" :-) However, since it seem to be relevant to your work, I'm adding it to my comments.
The calculation for AiC relies heavily on a small subset of acetabulum landmarks (9,39,5,8). I therefore think it would be meaningful to add the point-to-point errors for just these selected landmarks to the evaluation of landmark detection accuracy.
Can you provide an average (or approximated) runtime for the proposed method?"	Provide some clarification on which data was used for the initial model training. Is it the data from the 50 randomly selected images? If not, it is not exactly clear what these 50 were used for? Some clarification on which models were built from which data in the main text would be useful. No comparison to other methods is provided.	"The notion of replication dataset is not clear to the reviewer in this context. It looks more like a hold-out test dataset.

The level of expertise of operators that annotated dataR is not described. As on this dataset, there is only moderate agreement between operators, this should be discussed somewhere in the paper.

While ICC provide interesting information, reproducibility of a measurement method should be quantified with a confidence interval of reproducibility (and repetability) when possible. See for instance ISO 5725-2:2019.

There is no detail about the random forests' hyperparameters and features. The authors should be more specific about their models.

It is not clear to me what is actually the ground truth for AcI and RMP in dataI and dataR. Is it the average of manual annotations or the annotations of just one chosen operator ?

there is no evidence in the paper that the manual annotations provided to the random forest lead to robust estimates of AcI and RMP.

The values of ICC give here misleading information about the real performance of the algorithm in terms of agreement with manual placement. Figure 4 illustrates that there many patients for which the difference between mean manual and automatic placement is over 10deg for AcI for instance. It can also be seen that ""moderate agreement"" in dataR is in fact associated with a large confidence interval of reproducibility.  Finally, it can also be seen in these graphs that for similar mean manual values, the automatic algorithm can provide variable results. These points should be fairly discussed.

The authors claim SOTA results on RMP but this claim is not supported by proofs in the text."
064	BabyNet: Residual Transformer Module for Birth Weight Prediction on Fetal Ultrasound Video	"The network is not innovative enough. The proposed network only extends the temporal encoding based on the BoT.
What is the sweep mode of the fetal video used for trained and validated? Is linear or sector scan?
The mean frame number of scan videos is 852, while the temporal sequences input to BabyNet have only 16 frames. Would better performance be obtained if the temporal sequences are longer?
The authors describe in the Discussion section that the way to combine BabyNet with clinicians results is to take an average, which is best described in the Experiments section.
In the penultimate paragraph on page 6, ""Clinicians (this work) BabyNet"" should be ""Clinicians (this work) & BabyNet""."	"Section 2.3
Images should be uniformly partitioned into patches (tokens) before they are input into transformer layers. What is the patch size in the proposed RTM architecture?
Section 2
What is the loss function adopted in the proposed method?
Section 3, Implementation Details
According to authors, acquired ultrasound images size 960x720 or 852x1136 pixels. Input video frames size 64x64 in heightxwidth. If I understand it correctly, lots of spatial information would be lost during the image resize process?
Section 3, Implementation Details
The input sequence to the BabyNet sizes 16 frames in the length. The number of frames in a US video is about 852 frames. Do the sequences from the same video overlap?
Figure 1
It seems that one weight number is predicted for a sequence with 16 frames. A US video has about 852 frames. Then, does one US video scan correspond to multiple predicted weight numbers?"	"The authors may provide more details and citations about the clinical significance of the proposed work. Is this decision of natural/cesarian delivery solely based on fetal birth weight or other factors are also considered? Is this widely adopted in practice? Is this decision usually 1 day before birth i.e., from when these videos are analyzed?

The model is trained using 225 videos. In general, vision transformers are trained using large-scale datasets and have shown to suffer from overfitting with smaller datasets. Was any overfitting observed as the data seems to be small-scale. How was the model regularized in this case?

How was the network or its components initialized?

Length of the video or frame rate could be provided (currently only total number of frames are provided)

What is the computational complexity of the different compared models? This could be additionally mentioned in Table 2.

It is not clear how clinicians obtain their estimates of the fetal weights. Do they use only the ultrasound video or additional patient metadata?"
065	Bayesian dense inverse searching algorithm for real-time stereo matching in minimally invasive surgery	accept as is	"This paper is overall written well. The flow of the paper is easy for readers to follow. I have minor comments listed as below:


The second equation on Page 5 is missing a label. In addition, in this equation, there might be a typo in the second part:
 
.
 
^2_F instead of
 
.
 
^2_2.


In the first paragraph of Section 3 /gamma was set to two different values for two different sizes. Can the authors provide more details how this value is chosen?

There are regions in Fig 4 being highlighted in red circles, however it is not clear in the text why these regions are highlighted and what particular points the authors would like to discuss. I would recommend the authors to include a brief description of this in the figure's caption.

Currently, quantitatively results are only provided on synthetic data. Although qualitative in vivo results have been provided, it is not clear what the actual accuracy numbers are. Please have a look at the SACRED ""Stereo Correspondence and Reconstruction of Endoscopic Data Challenge"" dataset where ground truth data are available."	"Few comments to specific points:
In eq 2, the computed weights for the fusion of the different patches may include a division by zero, in case of perfect egality between the left and right intensity. This point is not mentioned in the submitted article.
""We compared ELAS, BDIS, DIS, and SGBM on the in-vivo data sets. Since no ground truth is provided, DNN-based methods cannot be implemented."" -> do not understand since it is just about doing inference on these data and not training?"
066	Bayesian Pseudo Labels: Expectation Maximization for Robust and Efficient Semi-Supervised Segmentation	"Please mention  the number of EM iterations used and how the average value of T changes over the EM iterations?
In Equation 8, the number 50 seem to be typo. Do you mean 0.5?"	See the weakness comments.	"Minor comments:

If T should be between 0 and 1, why not use a Beta prior and/or Beta likelihood? Using a Normal distribution as variational distribution can lead to values outside [0, 1]. How do you deal with invalid values?
I appreciate the Bland-Altman plot. However, I did not know that statistical significance can be derived from it. Please elaborate on that or use a proper statistical test and report p-values.
Tab. 1 and 2: I assume that the values are mean +- std. Please state that.
Uncertainty estimation with SegPL-VI: As far as I understand, only the estimation of T is implemented in a probabilistic/variational manner. I find it quite a stretch to argue that the deterministic segmentation network can produce reasonable stochastic segmentations with that.
The problem with Brier score as calibration metric is that it heavily depends on model accuracy. I think that a proper calibration metric such as (classwise) ECE or adaptive calibration error would be better suited."
067	Benchmarking the Robustness of Deep Neural Networks to Common Corruptions in Digital Pathology	See my weakness section.	"I suggest that authors publish the code.
I suggest authors further investigate whether the existing robust studies work on the proposed benchmarks."	"The biggest weakness IMHO is that the authors compare single shot trainings of various architectures, that are hardly comparable. If the authors want to evaluate the robustness of an architecture, then they should IMHO use several training runs of one architecture (and please also report the distribution).
I would recommend to include a subjective evaluation. If the relevant information in the input image is destroyed, a model deterioration can not be called as missing robustness. Hence, this is only possible if a human expert can still retrieve information from that. I think the authors should try to limit the pertubations to effects that only effect model robustness and not general (e.g. by experienced experts) recognition.
I would also question the sense of the rCE metric in that sense. If all models have the same (mediocre) results after the corruption of input images, it is not really informative to set that into relation against the original performance.
The metric that I liked the most was the CEC metric, as it tackles the model confidence. It would be interesting to compare that against a standard metric such as rank correlation."
068	BERTHop: An Effective Vision-and-Language Model for Chest X-ray Disease Diagnosis	"While the results show the effectiveness of using PixelHop++, I was not able to clearly understand why the use of PixelHop++ is improving the learning outcome.
The description of PixelHop++ needs to be articulated - what do you mean by image at different frequencies? why does this improve the result?
The writing of paper needs to be improved; there are many grammatical errors and repetitions throughout"	Some aspect could be better explained	The simplicity of the architecture cannot be countered with a lack of novelty perspective. Also, detailed comparisons are reported with different backbones.  Please clarify whether the method is an extrapolation of the concept of visual Q/A, and if the authors have tested it previously on non-medical datasets.
069	Bi-directional Encoding for Explicit Centerline Segmentation by Fully-Convolutional Networks	"Typos: 
'we propose a different data structure for for the efficient segmentation of tube-shaped objects' -> 'for for'"	Please add more discussions (or ablation studies) and fix the unclearness as stated in the weakness section above.	"General comment:
I have to admit that I have a hard time understanding why the methods works with apparently very good performance. The network is required to obtain a substantial understanding of the global structure of the target object (with partially very different sizes and shapes) to be able to place landmarks equidistantly on the target image. I would have liked to see a more detailed discussion and potentially analysis on this aspect, e.g., is this dependent on the receptive field of the network?, is the HRNet an essential ingredient for this task? what happens to the predictions of a structure if an image is cropped or stretched?
If I am overlooking something and this is a rather straightforward insight, I am happy to stand corrected.
The description of the method can be improved:

It is a bit tricky to say that no post processing is required when there is an obvious step to get from the heatmaps to the line. I would encourage the authors to diffuse such sentences.
From my perspective, the description of the ground truth heatmap should be part of the method description (not of the encoding) - I was missing this in ""Training and Inference"" and was surprised to find this later in the text.
The clarity in the method description could be improved: I was not sure what the authors mean by ""horizontal coordinates of the endpoints are closer than some threshold""? Closer to what? How were these coordinates extracted? The output of the network should still be a heatmap. Also, how are s (scaling) and t (threshold) selected? Why n=31?
There is very little information on the selected architecture (see also comment above). One additional sentence that summarizes the concepts of this would help put this into context.
It semi-synthetic data is described very briefly, and makes it rather difficult to assess how realistic these images are. Also, for the real and semi-synthetic data, multiple structures are segmented, however, it is not clear how this was performed. Are separate networks trained for each centerline type? Was there a multi-task setting?

The evaluation and description of the results can be improved:

The experiments do not reveal what role the architecture itself plays - potentially in combination with the proposed encoding scheme (see also comment above).
The results on the synthetic data are - from my perspective - not particularly interesting. They are okay in supplementary material, however, I find the added value of Table 1 in the paper to be relatively limited.
Figure 2 is rather hard to read/interpret (also: variance of results?). I understand that the authors want to also illustrate the ccs-aspect, but this overloads the figure from my perspective and contains little additional value (e.g., if there are a few false positive pixels, this will have limited impact on a line fitting afterwards). A bar or box chart (that includes the standard deviation) would have been more expressive from my perspective.
The authors show impressive qualitative results, however, I am missing an analysis of failure cases. I am missing an analysis of cases where it didn't work. At least for some structure, a SDR of 15% indicates that there are some points missing. Also, in the real and semi-synthetic examples shown, the centerlines seem to be rather well behaved. It is not clear how such a method would behave in case of loops. Did the authors observe any outliers?

Additional comments and typos:

abstract: ""mainly addressed via dense segmentation"" - this is a bit simplified. There have also been regression approaches that formulate this as regression tasks in Kordon et al., MICCAI 2019, https://doi.org/10.1007/978-3-030-32226-7_69
abstract: ""agnostic in the number of points in their centerlines"" - this seems to contradict the statement that the authors propose to use a fixed number of points. This should potentially be rephrased to improve clarity.
Convexity as such doesn't really play a role for pixel-based segmentation. It is true that it may be easier to design postprocessing for some convex objects, but this can be formulate more precisely in the text.
p. 1: ""and with the attention mechanisms [18] to address a guidewire segmentation and tracking in endovascular aneurysm repair problem. A U-Net model with the spatial location priors was used in [10]."" - these sentences reads a bit bumpy and could potentially be improved.
p. 2: ""to reconstruct [the] true shape of [the / a] device"" - expression.
p. 2: ""for for"" - typo.
p. 2: ""n-connected points"" - no dash, otherwise this distorts the meaning (compare n-connected graphs)
p. 3: ""We use the conventional for landmark detection heatmap regression loss"" - expression.
p. 4: ""fracture of points"" - typo.
p. 6: ""Hausdorf"" - Hausdorff."
070	BiometryNet: Landmark-based Fetal Biometry Estimation from Standard Ultrasound Planes	"(1) In section 4, the description of proving the robustness of proposed approach was not detailed enough. It should be added the results of training on FC dataset and testing on HC18 dataset.
(2) In section 3.1, the details of landmark annotation should be given. Moreover, the author extracts the BPD and OFD biometry from the major and minor axes of an ellipse least square. Did this approach affect the accuracy of result? 
(3) The comparison with the advanced methods on fetal biometry estimation should be added.
(4) The influence of simple landmark annotations and fine annotation on accuracy should be illustrated. Moreover, the protocol of annotation among obstetricians should be given in the paper.
(5) The femur length prediction task is not clear in the paper and the author should declare why the femur Length image are inputted in the network."	"I just would like to suggest to the authors to revise the introduction and include previous works on automatic landmark detection in medical images, such as:
Amir Alansary et al., (2019), ""Evaluating reinforcement learning agents for anatomical landmark detection"", Med. Im. Ana., vol53, pp.156-164.
where was reported a 3D landmark detection method based on deep Q-network (DQN) architectures  which was  evaluated on the detection of multiple landmarks in three different medical imaging datasets: fetal head ultrasound (US), adult brain and cardiac magnetic resonance imaging (MRI)."	"The authors should detail the motivation of the landmark class reassignment module as it is the main novelty of the proposed method. It is hard to follow the necessity of this module without reading the reference [3] in the original paper.
Performing the experiment to demonstrate the superiority of the method can not be assigned as a contribution, please remove it.
The metric curve in Fig.4 is hard to recognize. Please put the zoom-in patch of the convergence stage on it for better visualization"
071	BMD-GAN: Bone mineral density estimation using x-ray image decomposition into projections of bone-segmented quantitative computed tomography using hierarchical learning	"This paper proposes a method to estimate BMD (Bone Mineral Density) from a plain X-Ray image. The proposed approach combines the QCT in training and decomposes an x-ray
image into a projection of a bone-segmented QCT.
The paper is a hard bit to follow, due to lack of details and lack of precision in the definition of the equations and variables used. 
Several steps of the proposed method are not described, e.g. cropping, registration, etc.
The original data used for the study are not described.
Experiments were achieved on a limited dataset and were not validated on a different data.
It is unclear what GAN brings to the proposed method?
The structure of the paper could also be improved by restructuring the ""Proposed Approach"" section into different sections linked to the different steps of the proposed approach.
Too many variables are not defined in the different Equations. Please fix.
Details are lacking about how the different models were implemented. Were they tuned favorably?"	"Figure 1 - landmarks are present on some of the 3D bone surfaces, however, these landmarks are not explained.
Figure 1 / dataset - It is unclear if the x-ray images and QCT are matched.  I think they are because of the comparisons of BMD derived from DXA and QCT with the synthetic BMD measurements.  However for the GAN training it would not be requirements that the x-ray and QCT be matched.
o	Comment on: Are all the x-rays taken from a consistent orientation?  How much variability is there in the x-ray image orientation and how will this affect the result?
Can you comment on the magnitude of the errors in BMD in z-score terms, which is often how osteoporosis is reported."	"-Please provided the baseline characteristics of the cohort? age, gender, mean BMD T-scores,...

The precision of the method is not reported here. If you can collect repeated X-rays from the same subject for a subset of data, for example N=30, you can also compared computed BMDs and report coefficient of variation (CV) to reflect the precision of the compuatioanl framework. We should have CV<3% for practical use; the lower the better."
072	Boundary-Enhanced Self-Supervised Learning for Brain Structure Segmentation	"Please address the problems in the weakness section.
Overall, this is a good paper with promising technical novelty."	"Eqn. (1): what is the ""inf""?
Table 1 and Table 2: (1) how do you obtain the numbers for methods in comparison? (2) please add a space between ""Dice"" and ""(%)"", and (3) ""3d"" -> ""3D""
Page 7: ""..., for CANDI and LPB40, Suggesting that ..."" -> ""... , suggesting that ...""
Page 7: ""..., one enhancing the fundamental boundaries and the other enhancing the semantic boundaries."" Please differentiate the fundamental and semantic boundaries.
Ref. [12]: ""cnns"" -> ""CNNs""; please also check other references for similar cases."	"*Why not use one decoder and two output channels to train the two proxy tasks? Is there any technical limitation to doing that? I think this can save model capacity and learn more fused task-relevant features.
*Significance test should be conducted for the results.
*Report the final convergence points in Figure 4 and 5. They seem quite close to each other in later epochs.
*Highlight the boundary improvements if there are any in Figure 6, such as over-segmentation and under-segmentation.
*Add necessary citations that use supervoxel and registration in self-supervision. The literature review is not complete enough."
073	BoxPolyp: Boost Generalized Polyp Segmentation using Extra Coarse Bounding Box Annotations	"Please, clearly say what data was used for training (particularly in section 4.1). I assume the models were trained with LDPolypVideo but this is not clearly stated.  Aditionally, a dataset with segmentation masks is needed for the ""pipeline with masks annotations"". What data is used for this?

Please state if the baselines (from other methods) were retrained by the authors and if the same data was used. If so, please also state if randomness was accounted for (the same seed was used so the same augmentations, and data loading order was kept)

Figure 4 is unclear when the authors refer to ""Dice values of the above models under different thresholds."" What threshold is modified? Is this showing the dice score when using different thresholds over the predicted maps? If so, it's interesting that the Dice score tends to increase with higher thresholds.

What threshold was selected for the results shown in Table 1?

In 3.1. ""Meanwhile, a pre-trained SANet [22] model (trained on small segmentation dataset) is applied to get a coarse prediction P for I."", please explain how the  pretrained model was pretrained or obtained. What data was used, was there any data contamination with the LDPolypVideo dataset, etc.

Minor:

""In particular, the widely adopted training set [8, 22] contains only 1,451 images"" (please mention dataset names for clarity)
In the introduction, the authors fail to mention other segmentation methods that use coarse polyp boxes uniquely as ground truth [refs]
In the first paragraph of related work, it sounds like UNet architectures are a subtype of FCN networks. This can lead to misunderstandings, as Fully Convolutional Networks are a segmentation architecture separate from UNet.

Typos:

""a generalized polyp segmentation model is urgently needed"" -> a ""generalizable""
But box annotations in LDPolypVideo exist two -> But box annotations in LDPolypVideo have two
There might be more"	"The connection and relationships among the main components (i.e., fusion filtering sample, mixture of annotations and pseudo, and Inter-image consistency loss) in the proposed method should be elaborated.
More details of the comparison algorithms should be added. For example, What type of the annotations were used in the network training? And what's the number of the pixel-wise annotations and bounding box annotations used in experiments respectively."	"The reviewer would argue on following points and addressing these could improve the paper:
1) Subjectivity in labelling is more of a problem than erroneous labels. Several available datasets that are well-annotated but assessing there subjectivity and finding an agreement is the way forward. Direction of research should include a clear distinction between what does author mean by accurate mask and erroneous mask. 
2) Overfitting of previous segmentation models - how do the authors know that these models overfit unless they perform generalisability tests. The same question would be for this work, does the model generalise on unseen datasets. E.g., if you train on one dataset could you try inference on the other dataset? 
3) Data shortage in which sense? Argument - there are many publicly available datasets for polyp segmentation are available that can be used to develop methods. How much is sufficient? Authors could cite some papers that reflect to this and make an argument on that?
4) Authors did comment about time consuming in related work but they did not provide the inference time. Also is the network end-to-end trainable"
074	Brain-Aware Replacements for Supervised Contrastive Learning in Detection of Alzheimer's Disease	I very much enjoyed reading the paper. I suggest the authors expand their scope of their method to other problems as well.	Since the introduced method is architecture agnostic and convolutional networks are very successful  in solving object detection,  including results for convolutional networks should make the contribution even stronger. (ViT are successful on image classification but way less efficient on other vision tasks). Also, reporting the performance of previous state of the art methods should help evaluating the importance of the introduced augmentations. An even better scenario would be to take the old state of the art method and include the augmentations in an appropriate way and report the performance.	"They can generate the grad-cam maps from the different models for some more detailed analysis but not just some metric values.
They can compare the performance of replacing brain regions related to AD (e.g., hippocampus) or other unrelated regions.
They should compare with more SOTA methods of model pretraining, e.g., moco [1].

[1] He, Kaiming, et al. ""Momentum contrast for unsupervised visual representation learning."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020."
075	Breaking with Fixed Set Pathology Recognition through Report-Guided Contrastive Training	"Minor :

Typo in the line below Eq. 5, there are two settings for \lambda_4 mentioned

The explanation above Eq. 1 for breaking the symmetry is a bit hard to parse. Perhaps a few lines of explanation can be provided on which portion of the loss differs from the traditional MILNCE construction"	"This is a well written manuscript that presents relevant work on a widely studied topic. The reduction in the need of data annotation is a very helpful tool for any other medical image application. 
A more in depth statistical analysis of the results may be helpful to better understand the impact of the improvements achieved"	"The novelty of the framework is limited. In my opinion, this work mainly adopts the existing contrastive language-image pertaining model, i.e., CLIP [1], from computer vision to a new domain or a new task. However, introducing large pre-trained models to solve a new downstream task cannot bring new insights to the community. It's very important to explain why the proposed approach can improve the performance and what problems can be solved by the approach?

I recommend the authors add a Related Work section to help the readers better understand the differences between this work and previous works, .e.g, [2]. Besides, I recommend further discussing the advantage and disadvantage of each previous work, instead of just listing them, which can help the readers understand the strengths and weaknesses of this work.

Many important hyper-parameters are missing, such as learning rate, batch size, and the number of epochs, which hinder reproducibility. Besides, it is necessary to report some model details and training details for the proposed approach, for example, the training time, numbers of model parameters and memory cost, and so on.

The paper is written in an optimistic tone that leads the reader to assume the proposed approach is rather good. However, I am more interested in knowing if the approach brings errors? And what type of errors does it bring? And why?

I would like to see a statistical significance test, due to the performance gap between the proposed approach and the previous state-of-the-art methods is small.

[1] Learning Transferable Visual Models From Natural Language Supervision. 2021.
[2] Contrastive Learning of Medical Visual Representations from Paired Images and Text. 2020."
076	Building Brains: Subvolume Recombination for Data Augmentation in Large Vessel Occlusion Detection	"*	Overall, the motivation for this work is not very clear. It is really not that challenging to identify if a patient has an LVO. It would be clinically more useful to locate the LVO.
*	The authors argue that data availability may be limited. However, the previous research papers in this domain were able to collect a vast amount of data. Thus, the claim that data availability is a problem is not well justified.
*	Only a subset of the patients included actually suffered of an LVO. What was the reason for imaging in the other patients?
*	The pre-processing section could be improved by adding more details rather than just citing other papers.
*	Figure 1 shows multiple discontinuities after recombination, which are not biologically plausible and may lead to false detections of clots. It is unclear what benefit these simulated datasets have.
*	More generally, the artery tree is highly variable between individuals and the branching pattern can be very different. It may be questioned what value such an augmentation has.
*	For the Recombination of ICA and MCA Subvolumes, I don't see the information that if the ICA is affected there also should be only reduced signal (at most) for the connected MCA branches."	"The paper is a natural extension of [14], which introduced deformation and mirroring as augmentation methods to help large vessel occlusions classification from the vessel segmentation masks. With the new recombination augmentation method, the healthy parts of the brain vessels from all the training set are shared and thus improve the classification performance. This idea is especially useful when the dataset is limited and the task is relatively simple. 
The DenseNet used in three models was not the same (number of parameters), which introduces another variable in comparison between three models. 
Considering the dataset was from the same source as ref [14] (151 patients in this paper, 168 in ref [14]), it is important to make clear whether this is using the same or partial data (why cherry picking if that is the case) as ref [14]. It would be ideal if this paper follows the same dataset as previous ones, if available. 
In ref [14] segmentation masks were deformed 20 times as deformation augmentation (10 times with 4 random anchors + 10 times with 5 random anchors), but the authors implemented with ""Each data set is deformed 10 times with a random elastic field"". Is that an implementation difference?
All the cases were successfully segmented and registered? How the performance is impacted by segmentation and registration quality?
The basilar arteries might be tortuous and in both sides of the brain mid-plane. Mirroring all left-sided hemisphere's vessel trees in sagittal direction might cause broken/irregular basilar arteries in the artificial images. 
Grammar errors: 
it can supplemented
system enables to split"	"Please check for typos and incongruences: ""as time is brain"", ""it can supplemented"", ""models specifically designed exploiting""...
Please check for missing or undefined acronyms (I recommend the \acronym LaTeX package)."
077	CACTUSS: Common Anatomical CT-US Space for US examinations	"Section 2 - What is meant by 'anisotropic properties' of the IR?
Section 2 - The authors should briefly explain the conv ray-tracing approach in addition to citing [16]
Section 2.1 - Was the test set of 100 frames acquired from a separate volunteer?
Section 2.1 - No unhealthy (i.e. patients with AAA) in CT or US datasets.
Figure 3 - Labels of what is simulated US/IR ('fake') and what is real would be helpful
Section 2.4 - What about an experiment where no IR is required? Could you use CUT to go between abdominal CT and US directly instead of using an IR? What would the performance be? I expect the authors did some preliminary testing with this even if it wasn't a thorough experiment, and it would be good to report these preliminary results.
Table 3 - Was any statistical testing done for significance?
Table 3 - On the siemens machine, an MAE of 7.6+1.5 seems that in some cases the mean error is exceeding the 8mm clinically acceptable threshold? Is this correct?
Table 5 - Please include MAE as well.
Section 3 - Overall I would argue that DSC is not a good measure for AAA diagnosis compared to a distance error such as MAE or Hausdorff. I would emphasize the distance metrics more than DSC. In particular, for the Siemen's machine it seems that a supervised U-net is better at distance errors (although not sure if this is statistically significant as mentioned above).
Section 3 - FID was measured but not reported. Would be interesting to read how well CUT does in this application.
Section 3 - A better way to present the results in Table 3 would be to do a Bland-Altman style plot where it is clear if/how the error changes as the ground-truth diameter varies."	"The author should try ultrasound images with different body habitus for the learning.
I am not sure if the proposed solution is a good candidate for using ultrasound for AAA diagnosis.
Some minor typos (e.g. ""Germangy"")"	Overall this is impressive work with a novel concept that if further validated in a larger and more diverse number of patients, and for different clinical tasks, could create a paradigm for accurate US imaging segmentation and classification models. Further understanding of the how training sample sizes and hyperparameters can affect the performance of the CUT and segmentation models would be enlightening.
078	Calibrating Label Distribution for Class-Imbalanced Barely-Supervised Knee Segmentation	"There are grammatical errors throughout for example:
o	Abstract
	""we statistic"" should be ""we did the following""
o	Introduction
	""no ionizing radiation"" should be ""without radiation""
the word ""statistic"" is often misused
Acronyms should all be defined with their first use, for example CPS was not defined with its first use
Methods
Script L and capital L in the equations seem to be used interchangeably.  Pick one and be consistent or better define the distinction.
Sub-volume vs cropping patch vs volume.  Are these terms used to refer to the same thing or is the distinction important?  This should be made clear with definitions for each term or by using the same term.  In the description of DUS and WL, the term volume is used; in the descriptions of PRC sub-volume is used; and in the experiments description cropping patch is used. It may be they are different things and this could be an important part, but this reviewer is confused on this point
Experiments
How does the patch size relate to the thickness of the segmentations and the original image size?

Figure 3

Is this a case where both CPS and the proposed method did not do well but the proposed method did much better.  Can you quote DSC here?

Table 2

Why did you not examine the effect of PRC or DUS or the combination of PRC and DUS (without WL) The analysis would be improved if these experiments were included to better understand the effect of each technique.
How sensitive is the result to the samples used as the labels?  Did you do any cross-fold validation or experiments where you chose different samples as the labeled data.  If these could be included this would help  better understand the robustness of the result and also the reproducibility.

The paper would be improved by a conclusion or discussion that puts the results in better context in terms of translation or generalization.  It seems that the authors are not that concerned with the specific segmentation task but this is more of a convenient dataset to use to test out the methods.  What barriers exist to using this method for other more challenging image analysis tasks?  How useful will these approaches be beyond this task?  How will the authors use the methods developed?"	"Although your experimental results beat some related papers, I do not find a clear description about how your modules have substantial improvement or difference to others, and how your method obtains the increase in quantitative values for the knee segmentation problem.

Visual comparisons in 3D are important. With these comparisons, I can clearly ""improved"" segmented results in detail in the 3D space.

If possible, you should also give segmentation results with different number of labeled and unlabeled data, respectively. And the unlabeled data could also be different modalities (The OAI usually uses DESS MR data, but in hospital, T1 and even T2 are very common). With these experimental settings, you could show your method have a higher extensibility and effectiveness.

In the second section in the introduction part, you did not clearly state how your work addresses the shortcomings of these related articles.

In Fig. 3, ""Comparison of segmentation results with CPS [13]"", the CPS is citation [3]?"	"This paper is well written. There are not many comments from the reviewer. However, there are many typos in the paper that need author's attention. Also, abbreviations used should be defined at their first occurrence.
As mentioned earlier, the conclusion is not at all what is expected. Conclusion should focus on what this study achieves - specifically, and can it be made general in certain way. Also, few discussion points on why you see improvements in the ablation study is important."
079	Calibration of Medical Imaging Classification Systems with Weight Scaling	Making the code available will be beneficial for the community	"I enjoyed reviewing this paper and only have a few questions:

A bit more clarity re the method and why only the ECE for top-1 predictions was looked at? Not clear what you refer to here, apologies if something is missed.
On Page 7: The 'WS calibration was lower than the ECE... by more than half', I think here it should be worth noticing that the HAM1000 dataset did not perform well for any of the TS->WS, but perhaps you have an indication as to why that would be? Also try some statistical significance testing?
the level of confidence in Fig 2, how come there is no representation for the COVID group?"	"1- Employ vector notation for mathematical equations and be concise with the variable usage.
2- Showcase results with other metrics.
3- Discuss the differences (if any) obtained with different metrics.
4- Please be consistent in your referencing style, capitalization of venues, abbreviations etc."
080	Camera Adaptation for Fundus-Image-Based CVD Risk Estimation	"The manuscript proposes a novel method to adapt fundus images captured by two different fundus cameras to explore the domain discrepancy issue. First, the authors collect a dataset (FCP) containing pair-wise fundus images captured by two cameras with different image quality of the same patients. Second, the authors propose a cross-laterality feature alignment pre-training scheme and a self-attention camera adaptor module. Overall, this work is of importance for device adaption for fundus image analysis.
Suggestions and questions are as follows:

The motivation of selection of CVD risk estimation after the camera adaption is not clear.
The manuscript doesn't compare the proposed method to Cycle-Gan, which is famous for image translation.
The organization and the writing needs improvement."	"Is the assumption that ""the visual clues of CVD risk have invariant representation over the two eyes"" supportted by any literature? Or this can be verified by your experiment result.
There are some loose expressions in this paper, which are easy for readers to misunderstand, for example expression ""S={x_i^l, x_i^r, y_i^r, y_i^c}"", in chapter 2.1 paragraph 2, the symbol ""r"" has two definitions (right fundus photo and regression).
The collapsing performance with SimSiam is confusing. Have you analyzed the reason? Your approach is not limited to a specific network architecture. Thus, more typical network architures need to be evaluated. Besides, the proposed methods do not compare with recent domain adaptation methods.
The data provided is insufficient, such as the results of multi-task network. What's more, the accuracy metrics of CVD risk estimation mentioned in the title is not demonstrated in the article. I think the accuracy is one of the most importanct factor to determine the application value of this work."	"The quantification results are suggested in the abstract. 
Please provide the detailed description or reference of the stop-gradient operation in Section 2.1.
In Fig.2, z^s and z^t were input into the loss function, but there is no description of this operation. Please specify the relationship between the loss function and z^s and z^t."
081	Capturing Shape Information with Multi-Scale Topological Loss Terms for 3D Reconstruction	It would be interesting to see how things might compare to other persistent homology representations (e.g., persistence landscapes), or even simpler topological descriptions.	"Please provide full details about the construction of the cubical complex from the image data. The choice of cubical complex should be explained (I assume this is to align with the voxel basis but this should be made explicit).
A brief introduction to the key aspects of persistent homology could usefully be included for the majority of readers.
On P2, microscopy images are referred to as having a multiscale nature. It was unclear what this meant, especially since microscopy covers a very broad range of techniques. This should be clarified.
Some small corrections to the text are required:
P1: ""permit to draw conclusions"" should be ""permit conclusions to be drawn""
P2: ""likelihood of a voxel x to be part of"" should be ""likelihood that a voxel x is part of""
P7: the panels in Figure 2 are not aligned (the graphs for the nuclei are vertically offset)"	As described in the weaknesses, providing more experimental results on the robustness of the proposed L_T under different strength parameters and analyzing its impact on the final reconstruction performance will make the submission stronger.
082	Carbon Footprint of Selecting and Training Deep Learning Models for Medical Image Analysis	"One way, as a community, to reduce CO2 emissions would be to avoid unnecessary, repeated experiments. Many works present similar baseline results (e.g., running the same U-net model on same data), over and over again. The paper discusses the briefly as part of the idea of open science. I think this point is important and could be made stronger and highlighted a bit more.
Maybe a way forward would be to construct a library of trained models that can be shared and re-used for comparative analyses avoiding many training runs of similar models, and thus reducing CO2 emissions. But this would need to go hand in hand with recommendations for publications, reviewing, etc. I would suspect that many reviewers are asking for (sometimes unnecessary) comparisons which require the authors to run many more model trainings than needed. We should be more careful, as a community, to ask for ablation studies, etc. Any additional experiment should come with a clear justification and trade-off analysis of added (scientific) value over the increased carbon footprint. Similar applies to cross-validation, etc."	"Please double-check for grammar, e.g., ""an year"". 
Please replace ""f.x"" with ""e.g.,"".
In Table 1, abbreviations are probably not necessary.
Fig. 3:

It says ""total [...] energy [...] over the five-fold cross validation"", but it seems to me that it is the mean consumption, not the total.
On the right graph, please modify the bars so they are separated instead of stacked (i.e., three groups of three bars, where each group has a bar for each region). The current visualization suggests the consumptions add up and each region is responsible for a certain percentage.
Some labels are barely visible when printing the manuscript in black and white.
The paper claims AMP made a large difference when training with brain images, but this is not clear in Table 2. I think either the table or the text should be corrected.
In Section 6, it is not clear where 2.89 comes from (the number used to calculate the carbon emissions from MICCAI papers)."	The presented approach seems to work from the test results. However, the contributions of the paper are not clear and not sufficiently significant to be published.  In addition, the overall technical quality of this paper is below average as this work is not well presented.
083	CaRTS: Causality-driven Robot Tool Segmentation from Vision and Kinematics Data	"I think the authors should try training a state of the art segmentation model and add augmentation before continuing with attempting to perform segmentation via 3D pose estimation.
I think the approach would be useful/interesting evaluated as it is, which is 3D pose estimation. There are some previous works (e.g. Real-time 3D Tracking of Articulated Tools for Robotic Surgery, Ye et al, MICCAI & 3-D pose estimation of articulated instruments in robotic minimally invasive surgery, Allan et al, TMI) which would be good comparison points."	"1) The baseline UNet's performance significantly drops when tested in unseen image-space conditions. However, it is trained without any augmentation (as stated in supplementary). It would be interesting to explore the limits of this baseline when data augmentation is employed, in the form of simulated image-space alterations (such as smoke, bleeding etc). Including a data augmentation pipeline is essential in most standard tool segmetation approaches and thus provides a stronger baseline to compare the proposed method to.
2) In page 6, it is mentioned that the feature extractor is trained on collected images and hybrid images where the average image background is added to rendered images. The latter should be justified in the paper.
3) It would greatly benefit the reader's understanding of the method section, if a figure describing the various parts of the robotic setup and linking them to the referenced variables in the text, was added to the paper."	In addition to the suggestions above, authors might refine the paper by improving the readability of the figures by defining all variables in their caption. For example in Figure 2.
084	CASHformer: Cognition Aware SHape Transformer for Longitudinal Analysis	Provide more experiments to address my questions in the wakness question.	"Please provide more details on the dataset. How to choose the subset from ADNI, which would help researchers follow and reproduce your works.
The author could provide more exploration in their further works on the frozen pre-trained transformer, e.g., fine-tuning partial layers instead of layer normalization (LN), prompts."	"Major comments:

an alternative evualuation idea could focus on trying to predict the ADAS score with different time horizons ? which seems to be highly valuable for clinical pratice.

Minor comments:

p.5 line 3: ""between the the line"": remove 1 the
p.5 line 16: ""The increase of the classification accuracy by 3%"" : percentage point increase
does the mean absolute error (metric used) between meshes has a unit ? (mm?)
fig S3. authors could add other usual binary classification metrics (e.g. precision, recall, f1, roc)"
085	Censor-aware Semi-supervised Learning for Survival Time Prediction from Medical Images	This a very well written paper with some very interesting results. I think it would benefit from a clear explanation on the preprocessing of the images e.g. how to go from whole slide images to patches instead of just citing the work where this was done. If there is space problem, this can even go in the supplemental material.	"Thank you for the interesting paper. I enjoyed reading it. However, I found a few small points that could be improved for the revision of the paper:

Not for MICCAI but generally: Include more datasets in the evaluation.
Further clarification is needed for the combination of the losses: Are all losses weighted equally? If not, how were the weights estimated?
Table 1 lists results from the best runs. I found this slightly irritating as it seems to give the proposed approach an unfair advantage (Which should not be necessary, based on the table)
Please comment on how the configuration selected for Table 1 was chosen. It seems that this had been done after the ablation study, which then could lead to methodical overfitting.
In the discussion, the authors claim that l_ca_rnk and l_elr are usually beneficial or not worse than not using it. Judging from the results, the same is true for not using them. The corresponding comments should be rewritten."	Please refer to the previous questions.
086	CephalFormer: Incorporating Global Structure Constraint into Visual Features for General Cephalometric Landmark Detection	"the landmark embedding should be well defined and evaluated, 
Is it a coarse visual feature from the coarse predicted heatmap?

the statement about the global structure constraints should be turned down.

More details are needed in the section on Fine-Scale Coordinate Refinement."	"Are the models in the proposed method trained end-to-end? Or are they trained sequentially?
Why not predict the coordinates of the landmarks from the first-stage model? Or predict the heatmap from the second-stage model?
If the first model misses the landmarks in the predictions, is it possible for the second-stage model to get them back in the final predictions?
In experiments with 3D CT, are all volume resampled to the same size or the same the resolution/spacing? If resampling to the same spacing, how does the model address the situation that image shape is different from model input shape?
In general, will more CephalFormer blocks help for better model performance?
What is the ground truth for heatmap prediction? Why is cross-entropy loss used here since it has been used classification tasks in the literature? It is unclear about the regression formulation in the manuscript.
Comparing GPU memory footprint (which is critical in 3D medical image analysis) with other state-of-the-art methods will help understand model efficiency.
How does the typical failure case in the prediction look like? What is the cause of the failure?"	"Except for the landmark detection, can the LGA and GGRA be used for other purposes? For example, use it for the normal semantic segmentation? I think the author should give some discussion of this topic.
The figure 3 seems quite unclear when for comparing with other methods for details. Either the author split it to two different figures and zoom in the landmark area to let the reader see clear how the CephalFormer make it better."
087	Cerebral Microbleeds Detection Using a 3D Feature Fused Region Proposal Network with Hard Sample Prototype Learning	"This paper presents a new deep neural network approach for detecting cerebral microbleeds based on a 1-stage implementation instead of 2 stages as seen in the literature. With the demonstrated performance, this method definitely warrants further validation. Additional attention to the following points should help enhance the paper.
Introduction
1)	It is worth noting that neuroradiologists do not always have trouble to detect the microbleeds, and often do not need to quantify them if not clearly indicated. See end of 1st paragraph, page 2.
Method
1)	In section 2.1, are the microbleeds incidental findings in the 114 subjects or under some specific diseases? What are the size ranges of the microbleeds? That would impact model performance.
2)	In the same section, are there any co-registration procedures applied in data preprocessing, why and why not? What does 'random cropped data' mean - referring to any location of the brain?
3)	In section 2.2, the term 'number of lengths' is confusing. E.g. how would a bounding box sized 20x20x20 give 'the number of lengths for each dimension to zero'? A related question, in Fig. 1, do all the feature maps to be fused have the size of 32 with 16 channels? The middle part is unclear.
4)	In section 2.3, please explain this sentence: 'due to the sparse and tiny properties of CMBs, the HSPL crops the data based on the rule that the number of crops containing CMBs corresponded to the crops not containing the CMBs'; e.g. how would the two crops correspond to each other? Overall, as mentioned above, this and the RPN sections would benefit from additional information.
Experiments
1)	The method proposes to use both SWI and phase images but it is unclear how that is implemented, alone or together. 
2)	(Minor) For figure captions, start with a brief title would help improve clarify."	"*	The most promising concurrent method of the references should be trained and evaluated on the author's data set to be able to directly compare performance conclusively.
*	The limitations of this methods should be shown and discussed.
*	The wording should be improved, e.g.
*	stating that with two-stage models ""there is an annoyance"" is hardly objective and valid criticism and objective arguments should be found to motivate a one-stage approach.
*	The sentence ""Obviously, the proposed net significantly detected the CMBs [...]"" should be rephrased as well.
*	All important details on the training procedure need to be added.
*	A cross validation-based evaluation scheme would be beneficial and avoid a possible testset bias"	"In this paper authors aim to detect cerebral microbleeds in MRI images. They fuse ideas from the U-Net and the YOLO-based architectures. A total of 114 subjects including 365 CMBs were used to train & test the approach.
The technological proposal seems feasible, although authors needed to add an ""artificial"" loss (ie, concentration loss) to enhance the results. Without this term the results were improved with respect the U-Net, but not significantly. The performance of the simple U-Net using the concentration loss is not shown.
I have some issues regarding the dataset. Firstly, since authors used only diseased brains, I'm not sure if the approach will be able to detect non-diseased brains (ie, if the brain is healthy, would the net do not detect any lesion?). An analysis with normal brains should be added in the experimental section.
My second main concern is about generalisation. Using images from the same dataset is a very favourable scenario for deep learning algorithms. Authors should test what is the performance of the algorithm in images from a different dataset."
088	CFDA: Collaborative Feature Disentanglement and Augmentation for Pulmonary Airway Tree Modeling of COVID-19 CTs	For possible future research direction I would suggest testing this approach to improve segmentation for other similar problems, e.g. Airway tree for patients with pneumonia and/or for noisy images, e.g. low-dose CT.	It could be helpful for readers if the authors could explain more details about the augmentation and the training procedure.	"FLA is applied in every layer of the feature extractor. What if it is applied on only one layer or some layers?
More visualizations are needed to demonstrate the effectiveness of  disentanglement.
What if the network input is clean + clean and noisy + noisy? I'm curious about the performance of them."
089	Characterization of brain activity patterns across states of consciousness based on variational auto-encoders	"It seems reasonable by visualizing how different the consciousness state and how different brain states separated in the embedded space. But what is missing here is some quantitative analysis

Why did the authors choose to apply VAE to dFC? There are a large amount of debates regarding the sliding window based estimate, e.g. window size, reproducibility, etc. Is it possible to work on the original time series?"	"Firstly, defining more equations are necessary, such as the loss function for dVAE, receptive field analysis. 
Secondly, the paper introduces too many contents but fails to explain them clearly. And the writing of the paper is poor. For example, the order for the subfigures of Fig.3 is a mess and Fig. 4 have few explanations in context. 
Thirdly, the sample size and the splits of training and testing dataset are not clear. 5-fold cross validation is used on training dataset. What is the test set?"	"Page 2 ""A small number of animals was investigate as it is advised in nonhuman primate studies."": investigate -> investigated
Page 5 bottom ""The highest the difference, ..."": highest -> higher"
090	CheXRelNet: An Anatomy-Aware Model for Tracking Longitudinal Relationships between Chest X-Rays	The authors present CheXRelNet, an anatomy-aware model for tracking longitudinal relationships between chest X-rays. The technical novelty of the paper is at a good level. Evaluation is reasonable but lacks an ablation study for the classification module. The paper is well written and easy to reproduce. The authors investigated an important problem in the field of medical image analysis and achieved state-of-the-art results.	"For the graph construction, the authors should give some ablation study or discussion on how to choose the threshold. I also wonder if the authors can propose some soft version to merge information among regions.
For the comparison, the proposed global-local network needs more computation and parameters compared to the plain Global model. So it is not clear whether the performance gain is from the global-local consideration or more parameters. I recommend that the authors think of a way to justify the proposed network's effectiveness further.
Zero-shot evaluation results are presented for comparison. The authors can show more details for the inference during zero-shot evaluation for completeness.
The authors use the ResNet101 autoencoder [26] to extract features, and the autoencoder is pre-trained on several imaging datasets. I am not clear if the features are extracted from the encoder.
For the two off-diagonal k by k blocks, the authors can further clarify the explanation, e.g., with the s, t index range. It is a bit hard to understand at first glance."	nice study.
091	ChrSNet: Chromosome Straightening using Self-attention Guided Networks	"The loss function is well-described and thoughtfully incorporates domain-specific metrics. Reading the results, I wondered whether length is an important metric - does it affect the later analysis of chromosomes, or is the most important point clear banding?
As mentioned above, is the use of attention networks indicated for this use-case? (a) are there any salient long-range relationships that a CNN will not encode? (b) Putting the patches into a sequence (pg 4) seems to lose ground, since relevant spatial neighbor relationships are lost.
In the results (table 1), evaluating length and straightness is arguably a bit unfair, since this is exactly what ChrSNet explicitly optimizes. LPIPS is a more neutral metric - does it relate meaningfully to the needs of later analysis? (I don't know).
On a related note: Are there metrics about clarity of banding that derive from the needs of later analysis, that would make good assessment metrics?
Providing uncertainty intervals in the tables is well done - a vital element for assessing the findings.
A key finding, seen in table 2, is that the various added-on architectural blocks do not add value vs using just RG (all accuracies vary by less than 1 std dev). It seems that this should be discussed. In fact, I would argue that the paper could be restructured to decrease the description of the other blocks, because they were experimental deadends (in the best sense) - worth reporting, but in the context of ""ideas that did not work out"".
Miscellaneous:
-deep features: what does this mean?
-a type from 1 to 12: what does this mean (unclear to a domain outsider)
Fig 2: what is the stippling artifact in UNet Real-world (bottom row, second from right)?
Fig 1: y is supposed to be a straightened version of x. Perhaps modify the figure to show this, for clarity.
Miscellaneous typos:
complimentary -> complementary
flat it into -> flatten it into
agained -> again
slop -> slope
-Unet -> U-Net, or UNet
the from -> from
-parts[19] -> add space
A careful review of grammar would be good (eg some definite articles are dropped). As an aside: if English is not your first language, I congratulate you on a far better command of English than I will ever have of your native tongue, whatever it is :)"	"Dear Authors,
I read your manuscript with great interest and I found it of very good quality. Also the results are quite impressive and opens the field for further improvements.
I congratulate with you also for the conduction of the experimental evaluation: very clear and well presented.
I have no major concerns. My only suggestion is the following:
I would suggest to improve the introduction so that it provides a deep overview of the
study. In fact, I think it is unclear what unique challenges are associated with this task."	"Both predictions from Unet and ChrSNet are more or less very noisy, comparing to real images. I think adding some adversarial training to the learning step would help further imcrease the performance.
For Fig 2, I would suggest report the S score, L score and LPIPS for the example.
In general, I would suggest to do more investigation on why Unet performs so badly on real-world example, but does a very good job on synthetic data"
092	CIRDataset: A large-scale Dataset for Clinically-Interpretable lung nodule Radiomics and malignancy prediction	"While I really like the contribution, I see two major problems with the paper in its current form:
(1) The actual novelty is hard to assess as the paper is not really clear on how much the proposed architecture differs from a vanilla Voxel2Mesh network. It seems like that the major difference between Voxel2Mesh and the pipeline used here, is the addition of the classification heads to perform (global) malignancy classification as well as vertex-wise spiculation and lobulation classification. I don't think that this would actually be a major problem as the proposed application scenario seems to be novel, but the authors should make this more clear then. I am also wondering how the Voxel2Mesh part is trained. Is the whole pipeline trained end-to-end using the BCE loss? That seems to be unlikely, but I am not able to find additional information in the paper. It would be, for example, interesting to learn how the authors accurately segment the spiculations, which will most likely be smoothed out by Voxel2Mesh. Is this achieved by just removing that part of the loss? Is that what the authors refer to in Sec. 2.1 when saying ""We did not apply regularization terms to the deformations to capture their irregular and sharp surfaces.""
(2) If find it hard to assess the results of the quantitative evaluation. The authors neither compare their approach to any baselines nor do they discuss their numbers wrt to existing work. While I understand that (most likely) no comparable approaches exist for vertex-wise spiculation and lobulation classification the performance of the malignancy classifier should have been discussed and compared to other methods. The authors indicate in Sec. 5 that ""Although the segmentation performance was better than the previous deep learning methods, [...]"". However, I cannot find any comparisons to other DL methods in the paper.
Minor comments:

Fig. 2 is not referenced in the paper and at least parts of its caption should be moved to the main text of Sec. 2
The first two sentences of Sec. 4 are not really results
In terms of reproducibility it would be better to directly mention the GPU(s) being utilized instead of saying that ""Nvidia HPC clusters"" were used"	"""Voxel2Mesh for multi-objects, this paper single object"": unclear what this means or what the challenge is
Which 32 mesh features are used?
Sec 3.1: unclear how classification of peaks was done - by radiologists?
Generally good description of experiments with many details.
Sec 4, about Table 2: ""On the external LUNGx testing dataset (N=70), the hybrid voxel classifier model does better in terms of both the metrics for all three classes."" -> Jaccard index for nodules is actually worse.
Table 3: LIDC-PM results are surprisingly good, better than on training set?!?
LUNG-X results much worse, this might indicates open issues, would be nice to have a discussion on this.
No comparison with other state of the art methods wrt malignancy prediction.
Fig. 3 not that convincing, unfortunately - is this a typical example?
Sec 5: ""the segmentation performance was better than the previous deep learning methods"": I could not find a comparison in the paper."	"I think the title is not well suited to the paper. There is no mention of lung nodule and it is not a toolkit that is proposed.
Although mentioned, it is not clear from the beginning that the method is end-to-end. I first thought it was a mistake. It could be worth briefly mentioning early in the paper how it is trained end-to-end.
I would tend to contest the motivation in the abstract ""tend to smooth out ... making subsequent outcomes prediction difficult"" Deep models can capture it, not necessarily needing precise segmentations. 
Some typos to fix: e.g. ""to classy""
Maybe the Voxel2Mesh part, that is most of Fig. 2, could be evidently split in the figure from the novelty that is taking the encoder features for the Malignancy prediction.
The results seem promising, yet I agree with the limitations stated by the authors on the vertex-level classification. (This comment could be merged with the Limitations and Future Work section)"
093	Class Impression for Data-free Incremental Learning	"A.	The efficacy of the paper would be emphasized with a broader range of applications in medical incremental learning. For example, breast lesion classification, or chest x-ray classification could be a good application to show clinical effectiveness. 
B.	Additional ablation study would help demonstrate the effect of the mean initialization method, and data synthesizing scheme."	"Please double-check the correctness of reference [22] in the manuscript.
The experimental evaluation could be expanded, especially for the ablation studies."	This is a very interesting approach with real potential in medical imaging applications due to privacy regulations of data storage.
094	Classification-aided High-quality PET Image Synthesis via Bidirectional Contrastive GAN with Shared Information Maximization	"The quality of the low-dose PET in Fig.2 is actually quite good. It leads to the question whether this application is meaningful. This can be proved by either the similar performance on a much higher dose reduction rate (visually worse image quality) or the comparison of the diagnosis (e.g. classification of NC vs MCI) on low-dose and each synthesized standard-dose PET.
The model is quite big for a dataset of 16 subjects. Any overfitting problem?
In table 1 and 2, the results show limited improvement. Statistical tests are needed.
Since the classifier of NC vs MCI is already a part of the model, it would be natural to show the diagnosis of low-dose, synthesized, and ground-truth PET, and compare them.
In Fig.2 and Fig.3, please add zoom-in image. It's hard to differentiate the quality between methods.
In introduction, it says current methods ""do not take into account the applications of the synthetic images in analytical and diagnostic tasks"". This paper actually used the similar method.

Ouyang, Jiahong, Kevin T. Chen, Enhao Gong, John Pauly, and Greg Zaharchuk. ""Ultra-low-dose PET reconstruction using generative adversarial network with feature matching and task-specific perceptual loss."" Medical physics 46, no. 8 (2019): 3555-3564."	"The proposed method obtained similar results compared with the current SOTA method AR-GAN, which is good, but it would be better if the authors can discuss the pros and cons of the proposed method compared with the SOTA AR-GAN in different aspects, or how the proposed method can outperform AR-GAN in certain scenarios.
In Eq.(2), in the last term, I think it should be D_M(l, s_syn) instead of D_M(s, s_syn), according to Fig.1.
In the ""Contrastive Learning Module"" section, the local features are processed by a 3x3 conv kernel. Since the local feature spatial dimension is 2x2, why use a 3x3 conv kernel?
The symbol ""C"" is referred to both the classifier (in Section 2.4) and the positive constant in Eq. (6). I suggest the author to change one of them."	"This paper combines several existing technologies to form a new framework, thus the novelty is not significant. This is OK if the performance is extremely expressive, or the analysis is sufficient.
Although the quantitative and qualitative performances of the proposed method described in the paper have shown better than state-of-the-art methods, the performance gains seem marginal, as shown in Table 2 and Figures 2,3. The magnitude of the improvement of the proposed method remains unclear.
To overcome marginal improvements, the authors should compare more recent works. Furthermore, the experiments on more benchmarks should be presented to verify robustness.
Although the overall architecture is novel, each individual components are largely inspired by previous works.
Lack of enough ablation study analyzes the contributions of individual components to the final performance.
The authors seem to have missed some relevant literature. Specifically they miss out on several relevant citations, e.g. "" CT Super-Resolution GAN Constrained by the Identical, Residual, and Cycle Learning Ensemble (GAN-CIRCLE)"", """"Unpaired brain MR-to-CT synthesis using a structure-constrained CycleGAN"", and ""CycleGAN denoising of extreme low-dose cardiac CT using wavelet-assisted noise disentanglement""."
095	Clinical-realistic Annotation for Histopathology Images with Probabilistic Semi-supervision: A Worst-case Study	"As already stated, I think that the paper presents a very clear, new and original approach for semi-supervised segmentation of whole slide images. The results seem to prove that this approach provides best results than others. 
I would have like to read a more complete study on the influence of the parameters of the method and especially the ratio given by the user on tumour/tissue. What if the pathologist underestimate or overestimate this ratio?
Maybe a second experiment on another publicly available dataset would also strenghthen the evaluation of the method."	"I wonder the assumption (""hard"" and ""easy"") in Figure 1 is theoretically supported by pathologists. Can you provide references or further explanation?
2.The experiments were conducted on a portion of Camelyon16, which limits the generality of the strategy. Would it be possible to experiment on more datasets?
3.The method is only compared with limited self-supervised and weakly-supervised methods. It would be better to have comparison with latest semi-supervised methods, such as FlexMatch, SimMatch, etc.
4.All the tables illustrate FROC, and it would be better if annotation time cost of the methods is also listed in a table for comparison."	"Please refer to the weakness section.
In addition to that, I suggest doing another ablation that uses the full amount of WSIs in Camelyon16 to pre-train the SimCLR encoder. This also meets the realistic scenario -- you don't have to pre-train only on labeled data."
096	CLTS-GAN: Color-Lighting-Texture-Specular Reflection Augmentation for Colonoscopy	"figure 1: F and G should have arrows in and out.  It can be confusing to understand the flow
for simulation of colonoscopy do the augmentations need to conform to physical constraints?  if a video fly through of a virtual colon is made do the video and individual frames look to be a coherent set?"	"When creating the VC data similar to [18], is this by using Blender? if not please describe the details of how this is done Also, it is unclear what the inverse square fall-off property is for or where you modify this. The reason there but what does it mean more accurately and why
Mathematical description, Fig. 1 and text:
In second sentence of Sec 4, isn't it related to G rather than F? and later on F instead of G?
Define G_im, G_cl and G_ts in Eq 2
""The discriminator has the network use random ..."" is very confusing
After Eq 7, ""G may ignore Z_ts"" is confusing since G only takes OC images as input
Define I in L_text
""Two VC images are passed to F"" is confusing since F only receives one
In the definition of L_t, shouldn't it be F rather than G? F does receives as inputs the image, z_cl, and z_ts

When setting values to parameters, did you mean lambda cyc rather than lambda T? But overall, how are these values found/set? are there any experiments to support their choice?
Define VC at the beginning of Sec 3"	"The abbreviation ""VC"" is never explained (I assume it is virtual colonoscopy).
The authors should make clear already in section 3 why the VC frames are needed and what they are used for. While reading the paper it is confusing that they are introduced in section 3 without giving any context.
The authors should explain what is the ""number of matrices"" in z_ts?
The authors should explain the forward and backward cycle in the paper and how exactly a train step is implemented. Furthermore, it should be explained how exactly the non-corresponding OC and VC frames are used in the framework and how they are fed to the model in the training process.
How do the authors determine the number of epochs? How is the quality of the generated samples assessed?
There is a typo in the caption of figure 4.
The authors should explain why a novel dataset based on VC is introduced even though there are no corresponding ground truth images available. Why did the authors not use an existing synthetic dataset?
The authors illustrate different use cases of the proposed method in qualitative figures, which include:

passing an input image to F and random sample z_ts and z_cl (figure 4)
extract z_cl from reference images with G and then passing it together with an input image to F (figure 3)
colon-specific color and lighting, while polyp specific textures and speculars like it is used in the evaluation of the presented work
Furthermore: combining input images and CLTS latent vectors from different datasets (figure 4) would also be possible
...
However, only one specific configuration is evaluated and discussed in the paper. The authors should at least discuss the different options of using their framework for augmentation."
097	Collaborative Quantization Embeddings for Intra-Subject Prostate MR Image Registration	"-- The author showed enough ablation study to prove the effectiveness of the model design. However, when compared to existed works, they only compared two works that are not SOTA. Please provide more comparison results with the following SOTA works in registration: They are in unsupervised settings but could be adapted to supervised/weakly supervised setting with public code.
[1]. Balakrishnan, G., et al.: Voxelmorph: a learning framework for deformable medical image registration. (2019)
[2]. Meng. Y, et. al.: DeepTag: An Unsupervised Deep Learning Method for Motion Tracking on Cardiac Tagging Magnetic Resonance Images. (2020)
[3]. Liu, L., et al.: Contrastive registration for unsupervised medical image segmentation. (2021)
Minor modification: 
-- Please change the name of section 2.6. from Training to Loss Function, since you have another subsubsection called training in section 3.1.
-- It is better to modify the paper title so that the hierarchical quantizer idea is included in the title."	As is apparent from my other comments, I enjoyed reading this paper very much. I liked the overall organization, the motivation with t-SNE embeddings, the way quantization was introduced and built into the approach. Following the methods section, I expected to see an ablation which is exactly what the authors provided. In addition, their method out-performed other approaches which is a huge plus. I hope that this work finds it's way into a journal at a later point.	"The authors might consider including the VAE and its variants in the literature as it shares similar goals with the discrete quantization methods.

Please double-check the typos and grammar throughout the paper (e.g. section 2.2, decoder G()->D()?).

Please add some detailed support when making big arguments. For instance, ""However, deep registration models are over-parameterized..."" is not supported by any papers."
098	Combining mixed-format labels for AI-based pathology detection pipeline in a large-scale knee MRI study	"Dear authors,
thanks for submitting your work to MICCAI. While in overall the paper investigates an interesting direction, there are clearly some limitations.

As I have mentioned earlier, this paper re-invents object detection. The authors should compare to methods like SSD and YOLO. The labels can be easily converted to the required format. In my opinion, it is fairly straightforward to build a 3D implementation of YOLO.
Please, run statistical testing of your results, i.e. compute a standard error over runs, and preferably execute a statistical test itself."	"Page 2: ""even different image types"". The previous paragraph actually prioritizes the difference in the imaging protocols, so it would be better perhaps to rephrase ""even"" here.
Page 2: ""However, methods to combine categorical labels with other positional label types, such as point-landmarks, were not addressed."". This is a false statement. A direct counter-example - Mask-RCNN (He et al), in knee domain - KNEEL + DeepKnee (XR, Tiulpin et al) and Namiri et al 2020 (MRI, https://doi.org/10.1148/ryai.2020190207), broad overview of the methods - Crawshaw et al 2020 (https://arxiv.org/abs/2009.09796). Please, rephrase.
Page 2: ""A first model, to our knowledge, trained for ACL injury-age and subchondral edema underlying the cartilage defect pathology detection."". Firstly, according to the paper, the model is not trained for the tasks simultaneously but 1 model per task. Secondly, please, see Astuto et al 2021 (https://doi.org/10.1148/ryai.2021200165) and adjust the claims accordingly.
Page 3: Please, elaborate on how the splits are done - balancing w.r.t. number of samples controls/cases, other?
Page 3: ""25 different institutions"". Please, provide a reference to the dataset or the prior publications with it. For camera-ready version, please, strongly consider also elaborating on the demographic info (at least, age range, sex balance, geography of the institutions).
Page 3: ""Labels used by models"". Please, provide few references on the rationale behind pooling of the grades, i.e. why are ACL and MCC injuries were pooled in a certain way.
Page 3: ""1398 studies"". Please, rephrase to not start the paragraph with the number.
Page 3: ""detected using a deep reinforcement learning model"". It would be great to also briefly provide the detection performance in the text.
Page 6: ""only the ""best"" candidate was selected"". It may read as only one defect per knee was considered. Please, elaborate here what ""candidate"" means.
Page 7: ""Stage I training was designed to achieve high sensitivity, since false positive studies would be filtered by stage II. Indeed, in three tasks we observed sensitivity exceeding 95% (Table 2). However, in the Cartilage Edema task we obtained 89%."". There is actually not a single task where sensitivity is >=95%. For MCC, 89% is only in AUC, not sensitivity. Please, review and fix this paragraph.
Page 7: ""4.9 mm localization accuracy"". Please, provide the credible intervals, if available.
Page 7. ""Two models only used positional-labels in training (Labels = Posit. in Table 2)."". Please, elaborate on how the classifier is trained in this case. These two experiements are rather difficult to understand."	"While the authors demonstrate a clever strategy in using positional labels to improve accuracy of categorical labels, the assumption is there will be real world gains in decreased need for pixel level annotation to build robust models. The paper falls short in exploring this practical aspect (e.g. time taken to used mixed format or reduction in computational resources)
How are the categorical labels obtained? Please expand on this crucial aspect. Are they labelled prospectively by the MSK radiologists or are they retrospectively obtained from radiology reports? If later, how are the labels for the 4 task obtained-, eg. NLP based extraction for key words.
How was resizing done in Stage II, if more than one lesion in different locations in the same study?
The application of peak finding algorithm in Stage I is unclear. Is the best candidate the central point on a given slice, with the shortest distance to lesion of interest?
One of the claimed main contributions of the paper is that this is the first manuscript to detect subchondral edema. There are already multiple state of the art papers which quantify, detect and classify subchondral edema in multiple knee compartments, not just MCC. Please rectify this claim.
Finally, the biggest focus of the paper is that use of positional or pixel level labels augment categorical labels by helping it localize the lesion. Please provide some visual proof of how the model attention performs (eg saliency maps) if possible."
099	Combining multiple atlases to estimate data-driven mappings between functional connectomes using optimal transport	"For the first comment in Q5, the authors may consider some connectome-related metrics to evaluate the performance, such as graph metrics (degree, clustering coefficient, etc.)
For the second comment in Q5, some discussion may be provided. For example, why other connectomes only have a prediction accuracy around 0.5 while Craddock has it as high as 0.75. Also, it seems that Craddock always yields the best single-source performance. Does the choice of atlas strongly affect the performance?"	See above	"1) It is hard to understand from abstract how the authors have validated their approach and what improvement they get. I think briefly mentioning the dataset and a sentence with key finding would help reader. 
2) Authors could replace training and testing word with 80% for  optimal 'parameter estimation' which was then applied on 20% remaining data for measuring the efficacy of the method. This will help readers not to be confused with the deep learning works. 
3) Authors should take special care with regard to anonymity and not put the acknowledgment during submission of double blind reviews."
100	Computer-aided Tuberculosis Diagnosis with Attribute Reasoning Assistance	"8.(1) In the ablation study part, please show the effectiveness of relative positional encoding which is adopted in the multi-head attention module. The author needs to discuss why relative positional encoding is needed for TB detection.
(2) The loss function for detection branch is not clarified. Please provide specification.
(3) The influence of the level of multi-scale feature pyramid is not well studied. Directly using attention module in the low-level with large resolution feature map will cause great computation and memory burden, although a multi-head style and stride down-sampling are adopted. Please show the experiment results under different choices of depth and provide conclusion about the relation between performance and pyramid level, like whether the high-resolution low-level feature maps contributes much less the higher levels."	"1.There are some clerical errors, e.g., Loss_seg has no been provided before in equation 4. 
2.The attribute feature representation should be proved validity.

The method is not compared with enough current work, so it is not convincing."	"There were several points that I found unclear/missing.
I did not understand why the presented task is presented as a weakly supervised learning, when the dataset contains both bounding boxes and labels for every data point.
Also, what is the advantage of the proposed attribute relational reasoning network over more standard region proposal networks (e.g., RCNN, Yolo, etc.)?
Results in Table 2 also need a lot more description. Is it possible to evaluate the attribute classification and detection separately? Is it also possible to perform analysis on each attribute separately and compare and contrast?"
101	Conditional Generative Data Augmentation for Clinical Audio Datasets	"The authors have presented an interested applications of GANs for clinical audio data. However, the data collected in this work is through a controlled process. And has been manually edited/segmented to extract audio segments for each action. In a real clinical settings, the audio data will be a lot different. And it will involve human conversations, ambient noise etc. The authors did not provide any insights on this or how this can be addressed.
Did the authors consider using SSIM too for comparison?
Check for typo in conclusion: ""can to improve..."""	"The main weakness is the novelty of proposed method. It seems that just applying GAN techniques with Wasserstein loss for generating clinical audio, while do not consider the unique properties of clinical audio when designing the method.
-As shown in Fig.1, 'Suction' seems always being overlap with other phases, therefore, how do you define the recordings for 'Suction' class? Do they also belong to other classes?
How many samples for each class do you generate when doing the comparison in Table 1? If only doubling for each class, why you say the generation can help tackle the class imbalance problem?
The results on all five folds are better to be shown, rather than only the average results.
Actually, from the Fig.3, it seems that the generated samples are not well aligned with the GT in each class. Maybe you could show the samples generated by other GAN-based approaches for comparison, to validate the effectiveness of your method."	"Abstract

The authors should make the primary use case of this approach more clear.  Is this meant for A/V surgical annotation of the stage, is it meant to guide interventions, is it mean for training?  It is even unclear what the nature of the sounds that will be analysed are, is it the speech of the surgical team, the heart beat of the patient, or the pitch of the drill?  It is unclear what labelling the dataset collected in an automated fashion will be useful for.
Introduction
The reason for analysing the THA procedure is unclear.  The authors successfully make the case that there are many useful reasons to analyse audio in medical application, drilling sounds as guidance in ortho procedures, lungs sounds as a diagnostic measure.  However the authors need to point out why they are working with the audio dataset that they are.  It is not clear what this dataset is useful for or if it is a good surrogate for other audio datasets.
Methods
The dataset seems small (N=5), which would likely affect generalizability. Do the crossfolds split based on the different recordings, 5 folds, 5 different recordings?
What is the frame rate of the recordings?  L=16380, how many seconds does this correspond with?
Why is the spectrogram distribution and the dataset recording distribution differ?  The percentage of samples that are suction in the spectrogram data is far smaller than in the recording dataset.
How is the data split for training the GAN vs training the classifier?  This could be made more clear
Figure 2
A more thorough description in the caption is needed.  It should be made clear in the description that this is the GAN.  The input should be made more clear, the spectrogram output should be more clear.
Results
The results section should state the main result, rather than only refer to the figures and tables.
Why is mean accuracy reported, rather than F1 for the multiclass classification?  Some sample confusion matrices would be helpful in interpreting the results.  It is unclear from only accuracy which classifier is best.
Discussion
The authors should not comment on the method improving robustness or generalization because neither of these things were investigated in this study."
102	Conditional VAEs for confound removal and normative modelling of neurodegenerative diseases	More investigations of the network design could be informative (dimension of latent space, network complexity,....), as well as including a state of the art statistical analysis on the raw data.	The paper tackles an important problem which, if successful, would have a broad impact. However, there are critical concerns about the novelty of the contributions. I would appreciate if the authors could comment on those.	Method needs re-writing. It seems the main contribution seems very minor without properly linking to the baseline.
103	Consistency-based Semi-supervised Evidential Active Learning for Diagnostic Radiograph Classification	"This submission can be improved in the following places:

Revise and add discussions for Fig. 3 to provide a clear understanding of the results.

Fix the figure location issue by moving Fig. 1 to be closer with the text referred to it.

Add discussion of the limitations of the proposed approach and the directions for future research."	"Improving the readability of the paper: please expand some of the abbreviations or use descriptive short names.
Adding more experiments showing that these methods are broadly applicable is helpful."	"While providing a new framework, there is always a question that how the new framework perform on different datasets/modalities. The framework is tested on NIH-14 Chest X-Ray data, which is a challenging dataset. However, it is not clear how it would work on new datasets.
Regarding the active learning part, the authors only provided AU for the uncertainty sampling. They have shown that the AU works better than random sampling. However, it is not compared to other active learning algorithms. Therefore, the question comes up that whether AU is the best method for active learning.
The figures were all readable. However, the abbreviations made them complicated to follow. For example, for the Fig.2 and Fig.3 (which include the main message of the paper), one has to remember a series of not very well-known abbreviations to be able to understand the plots."
104	Consistency-preserving Visual Question Answering in Medical Imaging	"*	The attention vectors can help identify the important regions in image for answering a question. For a given main question, an analysis of when main and sub questions pay attention to same or different regions, will further help understanding the reasoning process of the VQA.
*	At inference time, for an image with DME scale of 1 or more, one can create multiple sub questions, while gradually questioning the entire image. Aggregating positive replies can create a weak attention map over the image highlighting the location of hard exudates. 
*	The authors can consider an additional question, ""is this region macula?"" This can help in identifying macula in un-annotated images and then further evaluating question: ""Are there hard exudates in the macula?""
*	The current scope of the study is very narrow, with a specific imaging modality highlight a specific disease. The authors should consider evaluating their method on another study to demonstrate the generalizability of their approach."	"Adding a paragraph to discuss related work would be helpful. The authors should be explicit about the contributions / novelty of this paper and consider discussing Wang, Peiqi, et al. ""Image Classification with Consistent Supporting Evidence."" Machine Learning for Health. PMLR, 2021."	"-First, this work lacks of novelty.
-The experiments are not enough, and it is not convincing only on the single dataset.
-This paper lacks a discussion on the experimental results and motivation analysis.
-Fig 2. occupied more than half of the page (page 4) - you could have saved this space to better discuss the motivation and results
-I suggest the authors could add baselines that recently proposed."
105	Context-Aware Transformers For Spinal Cancer Detection and Radiological Grading	"Please describe the structure of attention mechanism and explain how the attention is achieved.

Please consider to justify or change the baseline models."	"Method
In Fig. 1 the authors introduce STIR and FLAIR sequences, while later, in experiments, they talk about T1 and T2. Could the authors revise it for consistency?
Experimental results

At this stage, it might be useful to remind the encoder being used (ResNen18 as it stands from 3) and the way it was trained.
Results: it appears that the SCT (T1, T2) has lower performances than T1 or T2 alone. Could the authors comment on that?
Results: In table 2 the authors show the performances compared to the expert and report annotations with slightly different trends (e.g., the baseline outperforms the proposed method in fractures classification). Could the authors provide more details on how the tables should be read and how the results could be interpreted?

Discussion

I wonder, whether the T1+T2 trained method would require both sequences available at test time which might be a limitation of the method? Could the authors comment on that?

Overall:

I would suggest revising the format and better use of subsections and paragraphs, as some of the sections appear to be a bit lengthy."	"Please clarify if the method needs manual annotation of vertebra levels.
What is balanced accuracy in Table 3?"
106	Context-aware Voxel-wise Contrastive Learning for Label Efficient Multi-organ Segmentation	Please see my detailed comments above.	"The motivation that introduces unlabled information as complementary information for training is interesting. Sequentially, the proposed method of using contrastive learning for better feature representation also demonstrates helpfulness in the network training. 
However, many weaknesses have to be addressed:
a) the motivation to introduce the contrasitive learning is weak. 
b) Some typos, e.g. ""it still challenging"" -> ""is still challenging"" in Section 1.3.
c) Statistical tests are required to demonstrate the performance improvement."	"As mentioned in the weaknesses part, it would be good to add ablation studies on the effectiveness of ""context-aware"", as well as a comparison to DoDNet.
Some minor corrections
(1) ""a organ"" -> ""an organ"" 
(2) Introduction section: ""Alternative, one can design"" -> ""Alternatively, one can design"" 
(3) Introduction section: ""it still challenging"" -> ""it is still challenging"""
107	ConTrans: Improving Transformer with Convolutional Attention for Medical Image Segmentation	"Fig.1's notions and texts are too small to be read on print paper. Please consider resizing them.
There is an extra ""Q"" in the first ""LN"" of the SRCA module in Fig.1. Probably a miss type. The authors may remove it.
To improve Table.1, it would be better to include the categories in which the Methods fall. By adding a column to show whether the methods are Transformer-based or CNN-based."	Please see section 4 and section 5.	"Sec 1. P1: ""To alleviate such issues, considerable efforts are devoted to
enlarging the receptive fields by introducing effective sampling strategies [7,14], spatial pyramid enhancement [31] or various attention mechanism [28]"", how do spatial pyramid and attention mechanism in [28] try to enlarge receptive fields? also, it would be better to cite works in the medical domain if possible;
In the SRCA module, transformer features are only used as ""query"", that is to say, the transformer features and CNN features are not fused, the SRCA module essentially filter the CNN feature and is equivalent to a spatial attention module of the CNN features; however, the paper claim that SRCA module fuse the two-style features, which is not precise; please consider rephrase the presentation;
Ablation study, how do you ablate DAB and SRCA, in other words, what is the baseline version of your model? More detailed analysis of the ablation study is needed;
As mentioned in the weakness part, I suggest include the model size and FLOPs as comparison between other methods, as using two parallel encoders may significantly increase the model parameters; also, the design motivation of parallel encoder, DAB module and the SRCA module should be enhanced;
Evaluate the methods on 3D medical dataset and compared with, e.g. nnUNet3D."
108	ContraReg: Contrastive Learning of Multi-modality Unsupervised Deformable Image Registration	"It would be helpful if the author can provide some additional discussion regarding the auto-encoder used for feature detection. It seems to be a key part of the network, and plays a big role in determining the reliability of the contrast loss. What happens if the encoder just doesn't work well for a given modality?
I would really encourage the authors to test their method on a more real-world  example of multi-modal data alignment. Perhaps intra-operative to pre-operative alignment, as they motivated in their introduction. Or even CT to MR, which would demonstrate the method is robust to large nonlinear differences between the intensity spaces."	The reviewer suggests to the author to improve the clarity of the paper, specially the methodology, and to perform more experiement with published registration methods, to improve the quality of this paper.	"In summary, I think this is an excellent piece of work. I am very impressed with the clarity and density of the presentation and think this is close to what can be accommodated in 8 pages.
In fact, I have no concerns or further substantive comments.
My only minor complaint would be that the font size in the diagrams in Fig. 3 is too small and the legends are difficult to read."
109	Contrast-free Liver Tumor Detection using Ternary Knowledge Transferred Teacher-student Deep Reinforcement Learning	"Paper has good organization and easy to follow experiments used for getting the right results.
Results were explained in a manner where non-scientist person can understand the process of the method and result analysis used to prove the technique easy to follow."	"Here, there are some review comments of this reviewer:
1 The computational cost of the proposed approach isn't discussed. The approach should be computationally efficient to be used in practical applications. 
2 The proposed method might be sensitive to the values of its main controlling parameter. How did you tune the parameters? 
3 Have you considered the effect of noises on the performance of the proposed method? Please discuss how this would impact the results and conclusions of this study.
4 The practicality of the approach should be further discussed."	"More experiment details can be provided for the reproducibility.
The resolution of Fig 4 can be improved for visual comparision. The regions of interest can be zoom in."
110	Contrastive Functional Connectivity Graph Learning for Population-based fMRI Classification	"The paper proposes a method to encode functional connectome features by using contrastive learning to obtain embeddings that are then used as inputs to graph convolutional networks for disease classification. As brain imaging datasets are scarce, attempts to overcome overfitting problems are needed.
There are several issues in the paper.

Notations are difficult to follow and I found several errors with mathematical formulations. 
For example, First paragraph, section 2.1:
Errors start with the notions
P is defined as number of number of patients. What is cal_P refers to? 
G_i^j is defined in line 1.
Line 7, what is G_i referred to? Is P missing? 
And so on ....

The effects of random partitioning time-series into two sets need to be studies.
Though authors generalize their claim on small datasets, methods are tested only one dataset.
The details of dynamic graph classification (DGC) is sketchy and from the table 1, I doubt it is ineffective.
References to some existing work on unsupervised graph embedding methods for commectome analysis is missing. For example:

Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning
Ming JinYizhen Zheng[...]Shirui Pan
(2021),10.24963/ijcai.2021/204

Though comparison results are provided, how the performance matrices were derived - either from your own fair simulations or from literature has to be mentioned."	"In this paper, the authors proposed a constrastic learning framework for FC graph classification and showed improved performance in patients classification. Overall I found this paper to be pretty clearly written with good evidence of improvement. I only have a few comments

Why weren't the PCD features included in the KNN model? If you remove PCD features, will the current method still outperform KNN?

I am curious, if you train the network in a supervised-way, e.g., instead of considering two views from the same subject as ""attracted"", consider all the views from the same patient group as ""attracted"", will you get worse or better result?"	"They could have an encoder to extract imaging features from fMRI.
Some typos can be fixed: e.g. ""The framework overflow"" -> ""The framework overview""."
111	Contrastive learning for echocardiographic view integration	"Question: In the ""Inter-subject volume contrstive loss"", it is stated that ""we find a random subject i'from the batch in each training iteration which has a similar volume at phase p to form a positive pair"". How do you find this random subject from the batch? How do you guarantee that such sample is available in the batch for the given positive sample?"	Please see weaknesses.	"This is a well written manuscript that clearly describes the problem at hand and the solution presented as well as the logic behind it. It shows a good understanding of the problem and presents some simple additions to a known architecture (contrastive networks) for a novel application. 
It would be helpful to do a more in depth comparison of other state of the art methods to solve the LV volume from the 2 views the authors use and show the actual clinical impact of the error reduction the authors are showing. 
Nonetheless the results from the ablation study show that each part that has been introduced contributes to the performance of the network and it would be useful to the community to have this published."
112	Contrastive Masked Transformers for Forecasting Renal Transplant Function	It is difficult to reappear. The authors mentioned a lot method but the results were limited. It was hard to persuade the reviewers. More experiments will be better.	"They need to incrase the test set.
The authors need to discuss the proposed system limitations in addition when the system can fail.
I see the authors can use their own CNN instead of ResNet18 cause this pretrained network loss a lot of information in MRI images
The paper didn'y have figures enough to show how the proposed system works on MRI images and can show these images during the prediction stage over two years"	"Comparison with conference papers is not appropriate. Strong journal paper should be used instead
Visual assessment of the results is missing
Discussion is missing. How and why the method outperform others. Limitations should be given and why the approach not work well with certain cases"
113	Contrastive Re-localization and History Distillation in Federated CMR Segmentation	"Distribution shift is one of the major challenges in federated/distributed learning which is even more important when working on medical images obtained from multiple centers using different imaging modalities/sequences. The authors are attempting to solve a meaningful problem.
The description on CRL lacks clarity and the authors could have used equations to help describe the loss function etc. Also the terms used in describing the cross-attention transformers are not defined clearly. The parameter alpha was never clearly introduced. Overall the paragraph between page 4 and 5 needs to be rewritten with the assistance of Fig. 3.
The MD training procedure in 2.2 could be made much easier to follow if it is turned into pseudo code/algorithm.
Overall the authors could follow the following paper for the description of methods: Li, Junnan, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. ""Align before fuse: Vision and language representation learning with momentum distillation."" Advances in Neural Information Processing Systems 34 (2021).
The authors did not report structure-wise results and distance-based measures. Both could be put into supplementary materials if there is not enough space.
From the ablation study, the mutual information-based loss seems to have the most impact to the performance of the approach. It could be further proved by removing two model features per experiment.
The writing of the manuscript could be improved. Multiple statements are made without context or clear purpose, e.g. on page 4 ""In cross-attention transformer, the localized server distribution benefit client"", on page 5 ""The Seg_p is the regularization"". Also there are multiple typos, e.g. ""wit weight"" on page 4 should be ""with weight"".
The authors claim that ""For the first time, our FedCRLD enables the cross-center cross-sequence medical image segmentation possible"", which is not 100% correct. For example, the following paper has made an attempt on cross-center and cross-sequence problem, and it was done on a different structure as well:  Dani Kiyasseh, et al., ""Segmentation of Left Atrial MR Images via Self-supervised Semi-supervised Meta-learning."" In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 13-24. Springer, Cham, 2021."	"In addition to my above remarks, here are my specific comments:

Please condense the introduction for the methods section and add more details regarding the model architecture.
Please add more details and steps involved in label preparation, data preprocessing and how evaluation metric is calculated.
Authors should add statistical test results and standard deviation values for dice (if dice values are calculated for corresponding number of labels as specified in table 1, it is not a fair comparison)
A brief comment on the computational complexity of the method would be helpful.

For future work,
Authors could extend their analysis to see if the proposed method would be effective for CT vs CMR.
It would be good to see the upper limit of model capacity (to know the maximum range of heterogeneity the model can handle without affecting the optimization)"	"I think the manuscript could be improved from the following aspects:

A careful proof-reading is necessary to correct the grammatical errors present in the paper. To name a few: 
1)makes the segmentation is still a challenging task
2)causes the multi-center multi-sequence CMR has larger heterogeneity than regular studies
3) However, the heterogeneous server model replacing client model in FL directly causes the long-distance clients utilize worse optimization replace original optimization
4) Through minimizing KL divergence wit weight...
5) The data-sharing strategy makes the model are biased towards similar data...
Large chunk of texts (or texts expressing the same information) are repeating in the Abstract, Introduction and Methodology sections, while the actual methods are not clearly described (in terms of Fig 2 and Fig 3). The authors could make better use of the space for the essentials (add more detailed description of the methods and the U-Net used for experiments).
Qualitative comparisons among the methods are lacking. Besides quantitative comparisons (Table 2), qualitative comparisons (e.g., the output illustrated in Fig. 3) are helpful to visually comprehend the improvements of the proposed methods over previous ones.
It is unclear if the 3D U-Net used for comparison with the other methods (Table 2 [12,9,10,3]) are kept the same and the FL strategies are the sole variable for comparison. It is helpful if the authors could provide more detailed descriptions about the experimental settings.
Visual results (appendix) are compared with traditional deep learning methods instead of other federated learning strategies."
114	Contrastive Transformer-based Multiple Instance Learning for Weakly Supervised Polyp Frame Detection	"The reason why depth-wise 1D conv works for temporal modeling instead of FC is not fully explained.
If I3D is not fine-tuned on any medical dataset, it is recommend at least show what is the (expected) output of I3D. Otherwise, it is pretty confusing why it would work since pretrained dataset has nothing similar.
One of the key contribution, as stated in paper, is the selection of hard/easy example. However the description is hard to follow/understand. It is strongly recommended to expand Fig.2 for a much more detailed explanation. For example, how the hard abnormal snippets are formed is not directly shown in the figure."	please refer to section 5	"For weakness point (1), please cite previous work or share the reasoning behind why citation is not needed. 
For weakness point (2), please provide training details for all methods benchmarked on the new dataset.
It will be great to share the reasoning behind why combining multiple open-source datasets instead of applying the proposed method to each dataset and compared it with papers published accordingly."
115	Coronary R-CNN: Vessel-wise Method for Coronary Artery Lesion Detection and Analysis in Coronary CT Angiography	I think the paper is well written and explained. It was consistent and diagrams were appropriately explained. The authors have compared their work to previous published works too. However, it is lacking some details as mentioned earlier and I think including those details would make it an interesting paper. Some other work in the literature for example (Performance of a Deep Neural Network Algorithm Based on a Small Medical Image Dataset: Incremental Impact of 3D-to-2D Reformation Combined with Novel Data Augmentation, Photometric Conversion, or Transfer Learning, Automatic stenosis recognition from coronary angiography using convolutional neural networks) are missing. I would encourage the authors to include them as they see fit.	"(1) For vessel lesion detection, using the curved planar reformation image stacked from centerline seems to be a common preprocessing method. However, I think it is necessary to explain why this transformation is used instead of the original 3D volume of patch. 
(2) The proposed detection module seems to be a simple application of Faster RCNN to the lesion detection problem without modifications for medical images or lesion detection target.
(3) Ablation experiments are needed to demonstrate the superiority of the multi-task network in the second stage, i.e., is it more effective than performing plaque classification and stenosis degree regression separately?
(4) Why is it necessary to refine the localization of the region proposals in the second stage?
(5) What does the word ""vessel-wise"" in the title mean specifically?
(6) More details are needed for the ground-truth of stenosis degree. What is the doctor's annotation?
What are the advantages of the proposed regression method between [0,1] compared with performing classification directly?
(7) The term ""polar coordinates"" appear for the first time in session ""Ablation Experiments"", but is treated as a major novelty, which needs more explanation in the ""Method""."	The author mentioned arbitrary CPR length is acceptable. Did the experiments in this paper apply different length CPR inputs?
116	CorticalFlow++: Boosting Cortical Surface Reconstruction Accuracy, Regularity, and Interoperability	"Besides the weaknesses mentioned in Sec 5, here some other questions:
(1) About white-to-pial surface morphing, considering the thickness of the gray matter is itself not a ""big"" number, so a ""small"" deformation might be a large influence. I wonder if any constraints on the thickness are taken into consideration. 
(2) Question about registration of scans. FreeSurfer will do the registration, I wonder if your method will set a canonical template and mapping all inputs to this template."	Please justify and discuss whether the size of the brain have influences on the template generation. Also, if the test brain is larger than all the training brain, would the method still work? The current paper mainly validated the Runge-Kutta improvement, but not discussed the influence of the template improvement. The author claims the template improvement is a contribution, so it need some addition discussion to justify.	"The motivation is great in the proposed template reconstruction. However, in Fig. 1, the actual template does not make a huge difference from the convex hull template in visual inspection. This is perhaps because the average across all the training samples. Although the authors show some examples in Fig. 1, it is still hard to see which regions have indeed a merit using this template.

The white to pial mapping is a good approach, but this idea is already adapted in many classic surface reconstruction pipelines (e.g., FreeSurfer, CIVET, etc.). Please acknowledge this strategy in the manuscript.

RK4 generally requires more time and memory requirement. It is a surprise that the computing burden is negligible on switching Euler forward to RK4. In particular, memory consumption presumably remains unchanged from Table 1. Could the authors discuss more about this?

One baseline method is insufficient for validation. I understand the dataset is overlap with [15], but Table 2 does not appear in [15].

Why do the inference time and required GPU memory differ from [15]?

The current evaluation is performed under the assumption that FreeSurfer is ground-truth. Nevertheless, the surface reconstruction quality can be validated on other metrics such as manual landmark placement, geodesic distance of the landmarks on the reconstructed surfaces, etc. As this is a conference paper, there might be no enough room."
117	CRISP - Reliable Uncertainty Estimation for Medical Image Segmentation	I believe the paper presents a novel idea for a very relevant task nowadays. iven the page limit this might not be possible; however, I believe more comparisons to baselines and different latent space modeling and similarity metrics for contrastive learning for a journal version of the paper would strebgthen the findings of the paper further.	"One needs to compare k-nearest neighbor uncertainty based on the mask level similarity like dice.
The uncertainty formulation has major flaws, as mentioned in weakness point no. 2. The authors need to fix this. Additionally, they need to explain what kind of uncertainty the ensemble captures. Is it epistemic or aleatoric?
Because segmentation from unseen data deviates from the training set, it does not necessarily have to be anatomically not consistent. Authors can try to encode the prior shape in the latent space to derive uncertainty in the predicted shape than just plausible segmentation ensembles.
Uncertainty-error overlap was introduced in the metric section but never used later in the experiment."	"As I wrote in the weaknesses section, the only major weakness of the paper is the lack of comparison and discussion of the prior art. I would expect to see a summary of the more recent literature and comparison to some of them.

There are some details that are unclear to me.
1) It is stated that ""Edge"" is applied to the predicted segmentation map. By which method were segmentation maps obtained?

2) CRISP is also evaluated on the outputs of MC-Dropout and LCE as stated in the first paragraph of page 6. Which samples of MC-Dropout and LCE were used for uncertainty estimation? Do methods CRISP-MC and CRISP-LCE in Table 1 correspond to these experiments?
3) I didn't quite understand why domain shift is simulated in the test images as mentioned in the 2nd paragraph of page 6. If data augmentation was performed during training which is a standard approach while training networks, the simulated images may become in-distribution since they have been seen during training. What is the aim of applying data augmentation in test time? How would the results change if there was no this step?"
118	CS2: A Controllable and Simultaneous Synthesizer of Images and Annotations with Minimal Human Intervention	"The mentioned supplementary file is not provided.

Font size in figures is too small to be read.

Is data separated in patient-wise?

Why only use one image from each CT volume?

Why only 10 volumes are used to fine-tune nnUNet while 30 are used by the proposed method?"	"First, please explain whether the approach is applicable to all CT acquisition methods, or only certain types of CT where HU values are reliable.
It is not clear if the presented approach improves upon the lack of variability issue in M2I methods. It would be helpful to add an experiment or otherwise discuss this.
For the downstream segmentation experiment, I would have expected to see results where the model is pre-trained on the synthetic data and fine-tuned on the small amount of real data, as synthetic data is typically used to augment smaller training datasets.
Section 2.2:  in the section that states ""MSE between the HU value map and our synthesized CT images"", is the CT thresholded, or otherwise how is it post processed?
Some sections of the paper reference details in the supplementary material, which is not included.
Please also include some sample failure cases of the model. Is it possible that the HU maps and the generated CT do not correspond?
Small detail: please check grammar in the abstract (""in this study"" does not follow sentence flow)."	"Try to do more systematic analysis of why the proposed method should work. What is the role of the AdaIn GAN? What features do we need from an existing network? Is the method generalizable? Does it depend on HU values?

Do more thorough experiments
We need more comprehensive comparison tables showing the strength of the proposed method. Two box groups of box plots comparing the proposed method and M2I for lung and GGO segmentation is not enough.

More compelling visual examples
Visual examples showing why the proposed method generates more viable synthetic images compared to other existing method would be good - Perhaps in a scenario where the ground-truth annotations are scarce or it's hard to obtain such."
119	Curvature-enhanced Implicit Function Network for High-quality Tooth Model Generation from CBCT Images	"In Table 2, since it is an ablation study, a more clear descrition of the ablations would be ""No implicit functions, no Curvature enhacement"" and so on."	"The idea of using two modalities for 3D tooth construction and using IFN has novelty. Cross-validation would be much better for comparison with other methods and validation of the method. 
Although there is a section for ""implementation details"", the details of the method (especially network parameters) should be given.
The running time of the method should be given.
Also, since only the crown information comes from intra-oral data, the results of the construction of the root and crown can be given also individually."	"Please list  the contribution or highlight of this paper
How about the time complexity about your model compared other methods, because there is another segmentation step.
There are lots of the reconstruct 3D shape models, why do you only select these 2 models to compare with your models? The number of SOTA is so less, which can not demonstrate this is a hot topic or Meaningful research work.
After segmentation, each tooth is modeled by suface resconstruction, then whether it needs to be integrated into a full tooth image but not only one by one tooth model?"
120	DA-Net: Dual Branch Transformer and Adaptive Strip Upsampling for Retinal Vessels Segmentation	"1) How does the authors' method compare in terms of methodology and performance with the [Ref1]? Where [Ref1]: Danny Chen et al. ""PCAT-Unet: UNet-like network fused convolution and transformer for retinal vessel segmentation"", PLOS one, 2022
2) Please replace the ""Cytomegalovirus Retinitis [12]"" with a more relevant paper. The refereed work does not deal with this specific pathology.
3) The following sentence in section 1 is not very clear: ""In contrast, patches-level methods make the geometric features ... usually span multiple patches."" Please rephrase it.
4) In Fig. 4 (a) is the visualization of the strip convolutions correct? The authors propose a 4 pixel strip which seems to cover a significant area in the retina. Does each square of the strip cover that large area, or pixels, in the image?
5) In ASUB, the authors propose a straight line structural element to extract the context from the vessels. Could more complex vascular features, for example junctions, be approximated by more complex filters? 
6) In Table 1 are the differences in the metrics statistically significant? Also in the results are the performance improvements originating from better small vessel segmentation, or large vessel boundary detection?"	"There is no justification given why adaptive strip kernels are used in upsampling but not when feature extraction. They would also improves detection of vessels.
An ablation experiment to show if using dual branch improves performance instead of using only patch-level branch or image-level branch would improve the quality of the paper.
The authors stated that they resized all fundus images to 640x640 pixels. However, they did not provide any details if they crop images to be square before resizing; not doing it can change representation of vessels in images. Also, there is no information given if patches are cropped in overlapping fashion or not."	"However, I still have a question. Have you tried the STARE dataset [A]? It is also a widely used dataset to prove the effectiveness of the segmentation model. You should vaildate your method on it as well.
[A] Adam Hoover, Valentina Kouznetsova, and Michael Goldbaum, ""Locating blood vessels in retinal images by piece-wise threshold probing of a matched filter re- sponse.,"" in Proceedings of the AMIA Symposium. 1998, p. 931, American Medical Informatics Association."
121	D'ARTAGNAN: Counterfactual Video Generation	please check section 5.	"Novel technique is mentioned with SSIM score but no comparative analysis with any baseline is presented.
Reference to the EchoNet Github repository is missing. 
Too many and unnecessary abbreviations such as Ultrasound to US.
Related work either discuss simulators which are physical simulators and compute intensive or implementations of deep twin networks. No reference for cases where counterfactual queries are worked upon may be in domains other than medical imaging. 
Too much mathematical detail, for Definitions which are difficult to comprehend without much context. Overall paper is difficult to understand.
More explanation for the Abduction-Action-Prediction solution (discussed in Preliminaries) may be useful. 
Variable names should be described in Fig: 1 for a quick overview. Fig. 1 can be drawn wrt. the application on hand instead of a generic one.
Minimal or zero representation of models using figuratively, leads to a very lengthy and confusing description.
Poor sectioning and sub sectioning with casual paper writing. Also English needs to be significantly improved."	"The authors propose a novel model for counterfactual queries. However, it is not clear how it is compared to existing models. Is there a video generating model available? What is missing in the existing models for a similar purpose?
I have difficulty in understanding the details of the model, and a better clarification might be necessary. In Section 2, what are the relationships between $U$ and $X$, and how $E$ and $Y$ are related to $V$? In Section 3, how is a twin network used to generate the counterfactual samples? What is the objective function? What is the random variable $U_y$ in Fig. 1? How is the DAG utilized in the framework? How is the propensity score defined and used in the model?
The experimental results look promising. However, the authors might need to compare the proposed model with a baseline model, such as an existing model or an ablative version of the proposed model. Without comparison, it is not clear whether the metrics shown in Table 1 are good or not."
122	Data-Driven Deep Supervision for Skin Lesion Classification	"The methods are not novel. ERF and LERF methods are adapted from the literature. The paper reads like, authors utilized methods from the literature, on a novel application.
Presentation lacks of clarity at a few places.
The use of CAM from the last conv layer in order to determine the L_{target} in LERF may not be robust. CAMs are known to be problematic in finding the lesion area (or diagnostically critical area) in many skin lesion classification applications and therefore there is not guarantee that the LERF selection will also be from a diagnosticly critical area.
The isotropic nature of the LERF is not well justified. For skin images this assumption may not hold.  Similarly, the unimodel assumption may not hold for skin images. as seen in Figure 3, first and third row, skin lesions may contain multifocal areas of high response (or high diagnostic importance)
The inference procedure is not well explained. Do LERF have any significance during inference.
Page7, 2nd sentence: "" To neutralize variations..."". This statement is not clear. Please further elaborate on the issue and the proposed solution.
Overall, the table captions are too short and does not convey enough information in regards to the table.
What does V and R stand for in column C of Table 2
The message that the authors would like to convey via Figure 4 is not clear. Are CAMs after deep supervision better, more precise or ?
Conclusions section is very short and lacks of discussions."	"I appreciate the fact that the method was tested on multiple different datasets and the method shows consistent improvement in each case.
Adding confidence intervals to the presented results will add context to the presented metrics and give us a hint of the variability of the results.
Since the key idea is based on object sizes, I suggest adding some analysis by stratifying the test group by sizes of the objects and analyzing the performance dependence on object size.
It will be more convincing if instead of training and testing on the same dataset, you could train on one and test on another (for example on the Vitiligo private and public datasets)
For the LERF computation, 20 iterations are used. Adding some information on the relative variability across the 20 iterations (maybe in supplementary) is useful as it will show the robustness of the LERF computation.

Minor comment:

In the introduction, the citation of ISIC 2018 has not been added."	It is better to provide localization maps of other method such Grad-CAM, Score-CAM.
123	Data-driven Multi-Modal Partial Medical Image Preregistration by Template Space Patch Mapping	"This paper presents a method that improves performance on the challenging problem of partial volume registration. However, my concern is with the limited novelty in this work, and incomplete experimental details.
Specifically, can the authors:

describe why the method is not sensitive to the choice of template shape?
Describe the data split used in training. From my understanding, all images were used in training, in which case the network is overfit to the data, with supervised labels. This seems like an unfair comparison, and I don't believe this method would be feasible in practice.
How sensitive is the method to the choice of number of image patches selected?

Since the method is the combination of several existing works, it is hard to recommend acceptance without methodological novelty."	"Sec. 2.1: There are a few Template-Space Patch Mapping (TSPM) parameters that would be interesting to test. Most interesting would be the patch size. 16^3 is used as input, but larger patches might offer more context that would improve performance. Different patch sizes should be investigated, or at the least provide a discussion of this at the end of the paper.
Sec. 3: Regarding your data splitting for the cross-fold validation experiments, please verify that your training/testing splits were stratified by subjects. You want to ensure that there was not data contamination between the two sets, e.g. the training and testing sets both had patches from the same subject.
Sec. 3: One of the challenges in this approach is that the small FOV images may come from very different parts of the template volume. I am curious if your dataset had 3DRA images that were uniformly distributed throughout the template volume. Or, did one particular location of the anatomy have more 3DRA images? Some discussion about this would be interesting. This might also be interesting to see if your failure cases were examples that were from locations poorly represented in your training data.
Grammatical/typographical:
Sec. 2.1: ""The rest space"" -> ""The remaining space"""	"In general terms, this is a solid paper. It introduces a novel method that is an interesting and robust alternative for being an initialization for other multimodal registration methods.
The introduction and method sections are well detailed.
The experiments section is quite good, providing extensive information about the experiments. However, it would be good to provide more technical data regarding the datasets that were utilized in the experiments.
It would also be good to see time performance information
The conclusions section is quite short, but I feel that most of its contents are actually located under the Discussion subsection from the experiments section. I'd aim to combine Discussion and Conclusions under the Conclusions section.
The proposed future work, regarding both the performance improvements, as well as experimenting with deformable registration look like the adequate next steps for this work"
124	DDPNet: A novel dual-domain parallel network for low-dose CT reconstruction	"Please provide more details in section 3.1, such as parameters setting in the network.
In Fig.5, the improvement of visual quality of the DDPNet is not obvious. Although the tiny structure is easier to observe compared with other methods, its shape has possibly changed. Please give more explanations. In addition, as seen in the enlarged ROI of DDPNet, the entire ROI is shifted a little bit to the right, which indicates the inaccuracy of DDPNet. Please check if it is due to wrongly selected ROI or the drawback of DDPNet.
There are many typos in the paper. For example, 'more readable readable pattern' in Page 2, 'is proposed to makes' and 'grade-based discriminator' in Page 3, arrow direction from the fusion block to y_sin~img in Fig.2, 'signogram-domain stream' in Page 4, and so on. Please read the paper carefully and revise them."	"It would be better to clarify the difference between this work and most related works.
It would be better to compare the most related works to show the effectiveness of the proposed method.
It would be better to investigate the HU shift between the proposed one and the ground-truth as this limits the clinical value.
Key details should be provided for reproducibility."	The arrowhead in Fig2 should be pointed from y_{sim~img} to Unified fusion block.
125	Decoding Task Sub-type States with Group Deep Bidirectional Recurrent Neural Network	"The authors should add some explanations that describe the intrinsic relevance between the different external stimulus tasks and the properties of brain functional networks.
Why did the authors use bidirectional GRUs instead of unidirectional ones? It seems to be to learn temporal context information from both directions.
In the data preprocessing, the authors divided the fMRI into multiple time slices. The motivation for doing so is unclear, and the authors are advised to add corresponding explanations.
The multi-task interaction layer proposed in this manuscript is not introduced in detail. I suggest the authors to add corresponding diagrams to explain the mechanism of the multi-task interaction layer in detail.
The descriptions for data propressing are not clear. In the proposed data pre-pressing method, as shown in Fig2, the input T7-T1 refers to 7 different tasks? For the training phase, does the method need to choose 7 sub-tasks from different tasks? Or arbitrarily choose 7 subtasks?
In addition, the article lacks a description of the testing process. Does the proposed method need to enter 7 different tasks at one time during the testing phase?
The article also includes some errors such as ""Fig. 1. Confusion matrix for classification accuracy of 24 events"" should be fig 3. and ""Fig. 2. Classification comparison chart of 24 events"" should be Fig.4. It is recommended that the author make careful revisions."	"From the manuscript, the advantage of the group-wise functional brain states decoding method compared with the traditional method seems to be the improvement of the classification accuracy. I think the group-wise analysis method seems able to discover the internal relations between the sub-type functional brain states caused by different stimulus tasks. It is recommended that the authors add relevant experiments to analyze the ability of a group-wise manner to discover the intrinsic associations of subtype brain states.
The experiment results reported in table2 that some combinations of tasks can improve the accuracy. However, the possible reasons are unclear, and the author is advised to increase the discussion about the possible reasons for the improved classification performance caused by the group-wise manner.
What is the method of DSRNN in Table3? There is no citation in the manuscript. I suggest the authors add the introduction for the relevant methods and mark the cited references.
Also, the comparison methods in Experiment 3.3, such as SVM, MVPA, and SoftMAX, also lack reference citations.
The resolution of the pictures in the article is very low, such as Fig2. The color labels of different methods in the figure are not clear. the authors should improve the quality of the figures in the manuscript.
The experimental results in Table 3 require further analysis. The classification performance of the proposed method for class Emt and Lng is reduced. I suggest that the authors discuss the reasons for the decline in classification performance."	"Introduction needs to be carefully re-written; it is very difficult to follow and the motivation of the study is not well articulated.
What are the intrinsic correlations between brain activity states under different external stimuli? Why is the group-wise brain network state decoding strategy helpful to explore this interconnectedness?
Which steps in data pre-processing are referred to as ""Random combination"" and ""Signal extraction"" in Fig.2? Authors are advised to describe in detail in the manuscript.
Meanwhile, the multi-task interaction layer mentioned in Figure 2 is not introduced in detail in the article. For the proposed method, The multi-task interaction is more important than GRU, and it is recommended that the author explains it in detail.
The authors should check and revise the entire manuscript for grammatical errors and typos.
How is the multiscale manifested in the Multi-scale Random Fragment Strategy (MRFS)? The work seems to just randomly splice different fMRI sequences and does not use multi-scale MRI sequences.
In the experiment, the authors only reported the classification accuracy of brain network decoding, which was improved by the group-wise strategy. Whether the authors add some visual classification results in the revised manuscript? So that the readers can intuitively discover the interpretability of group-wise brain network decoding."
126	Decoupling Predictions in Distributed Learning for Multi-Center Left Atrial MRI Segmentation	"Some grammatical errors and unclear sentences: Page 1: ""has shown great potential"", ""the former"", ""the latter"" (no plural), ""the latters target multiple models, of which each for one center (denoted as local data)"" (unclear sentence), ""been proposed to for"". Page 4: ""the segmentation risk"".
In page 2, the authors state that ""Although the above methods have solved the problem of privacy and fairness"". I think that neither of these problems have been solved: issues about privacy can still arise given that it has been shown that images from the training dataset can still be allucinated from a trained model. As for fairness, the concept is very broad and should be better defined in this paper to better understand what the authors mean. Please edit accordingly.
Page 5: ""As q - 0, it is equivalent to CE loss which emphasizes more on uncertain predictions, and it degrades to MAE loss [3], which equally penalizes on each pixel, as q approaches 0."" This sentence is unclear. Please revise."	"The connection to [1] should be emphasized. Currently, [1] is only simply touched on. Please explain the main ideas of [1] and how they approach the two seemingly contradictory objectives simultaneously. Following this, please clearly explain how the proposed method follows the approaches of [1] and clearly list the proposed extensions / differences with respect to [1].
The method proposed a distribution adaptation network to used learned latent representation to generate an adaptation matrices. But it is unclear how the DA net yield such a high dimensional matrices? How about the computation and memory? It would be better to describe clearly or provide some visualization.
The method introduced a regularization term L_TR to minimize the diagonal elements of W_k^I for each pixel. The authors claimed that it is used to forces the matrices to modify the prediction as much as possible. However, the underlying mechanism is not clear.
There are some inconsistent descriptions in the paper: Introduction Para.5 ""our method decouples the predictions and labels"" and Sec.2.2 Title ""Decoupling Global and Local Predictions"". Please clarify it.
The method is evaluated on datasets constructed from three centers. Morphological operations is used to simulate the settings of Center C/D/generic data (unseen center) for Utah dataset. The method demonstrated significant superiority in Center C and D. But it seems that most of the training data come from the Utah dataset (35 for C,D and 15 for A,B. So this may be the main factor of the obvious improvement of the proposed method for center C and D, but not for Center A and B, as shown in Table 1. This make the experiment results less convincing. Please clarify.
Please also describe clarify the different of the propose method and the competitors in the introduction."	
127	Deep filter bank regression for super-resolution of anisotropic MR brain images	"The paper is overall well written, and makes mathematical sense. I only have a few questions hoping the authors can clarify.
Is there a reason why H1 - H_(M-1) and F_0 - F_(M-1) are modelled as 1D filters? Can they be 2D since, in stage2, y and d training pairs are 2D?
NIT: Can the authors clarify in text what is the exact value of M used in experiments? From Fig3, I assume M=5. Similarly with the order of the filters (k, I assume M-1?). How does changing the value of M and k affect performance of the model?
NIT: What is the runtime of the model to create a HR image? and how does this compare to B-Spline or SMOTE?"	"Fig 1: It would be nice to add the analysis/synthesis terms to this figure, and also the encoded/decoder analogy (as this is something that DL readers will appreciate). Maybe as boxes around the relevant parts?
""neural SR methods"", I am not sure I understand what this means, please explain.
Could h_0 be learned from the data, without using SMORE as a pre-processing step, or would this break the theory?
Why is a Gaussian used as part of the low-resolution generating process, and not the estimated slice profile? Would not the latter stay truer to the forward generating process?
As the method shares similarities with earlier work in SR - which casts the recovery of the HR image as an inverse problem - some of this work could be referenced in the literature review.
Fig 4: The Low Resolution scans would be better visualized using nearest neighbor interpolation.
Routine clinical scans are often acquired with a number of MR contrasts. It would be nice with a comment in the Discussion section if the model could be extended to this 'multi-channel' scenario."	"(1)	Please introduction the method in more detail.
(2)	Please conduct more detailed experiments to verify the advantages of the proposed framework with theoretical guarantee."
128	Deep Geometric Supervision Improves Spatial Generalization in Orthopedic Surgery Planning	"authours may wish to consider automatic landamark detection using deep convolutional neural networks for this task.
Liu, Wei, et al. ""Landmarks detection with anatomical constraints for total hip arthroplasty preoperative measurements."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2020."	"Section 1 - RQ1, why the choice to examine activation maps as a research question? This seems unrelated to the planning or surgical goals and does not seem relevant. Seems like this is for supplementary material instead a core research question
General - What is the accuracy of directly segmenting the graft fixation site using a FCN like U-net compared to using the proposed method which uses multiple learning models (Proxy + DGS). If this is already published, cite in discussion section for comparison
Section 2 - What value does task 3 (semantic segmentation of femur) add to the overall accuracy? Is segmenting the femur necessary for reasonable planning accuracy?
Section 2.3 - What is meant by the ""original segmentation task""? Is this directly segmenting the graft fixation point or is it referring to segmenting the femur as in task 3 of the meta learning?
Section 2.4 - What exactly did the medical engineer label in the X-rays? The surgeon marked the SP point, did the engineer mark the Blumensaat point and turning points or was this automatically calculated from the femur polygon?
Section 3 Fig 3 - Why is there a small increase in error for Shaft/Head ratios greater than 2.4?
Figure 4 - The legend on figure 4b seems to be wrong as from this plot it seems configuration A is the best.
Section 4 - What is the accuracy when only DGS is minimized? While this might be less interpretable, I would be interested in the accuracy difference as well."	"Among three model variants, the 'proxy' model is an improvement of (or equivalent to) authors' previous work [13]. I'm wondering how much performance improvement was achieved by hard parameter-sharing proposed in this study.
It would be great if the authors could more kindly explain the rationale for model variants 'B' Proxy + DGS w/o segmentation. Why did the authors' expect segmentation task may have chance to negatively impact the overall performance? Also, the current way of writing and naming is confusing because model variants 'A' Proxy model, inherently include 'segmentation' as one of three tasks. In my opinion, ""B) Proxy w/o segmentation + DGS and c) Proxy + DGS makes more sense. 
Is the multiplicative weighting term (lambda) in 2.2 the same as the risk-weighting (lambda) in 2.4? Keeping consistency in terminology would avoid any unnecessary misunderstanding. 
How was the initial (0.99) and decrease internal (0.01) of the risk-weighting determined? Have the authors performed any sensitivity test for it?
The results are impressive. What would be the next step to achieve full automation? How far the authors think is left to adopt the developed tool in real clinical setting?"
129	Deep is a Luxury We Don't Have	The proposed method seems to be highly related to GMIC. Adding more comparison with other methods can strengthen the results.	"This work adopts Performer equipped with different settings to detect the tumor region from the high resolution mammograph. However, many basic concepts are wrongly addressed, making the submission inconvincible.

The 'High resolution' concept has been disclaimed overall the submission. High resolution refers to the fine unit spatial distance instead of a large number of pixels.

The attention mechanism aims at dynamically searching out similar regions from the images, which is invariant for its variants. Then, the concept ""HCT focuses dynamically on regions based on their contents and not their spatial positions"" misleads the readers and should be corrected.

Since the authors argued that the AC is the contribution, please clarity the difference between the AC block and attention block in Performer [1].

[1] Choromanski, Krzysztof, et al. ""Rethinking attention with performers."" arXiv preprint arXiv:2009.14794 (2020)."	"Comments:

Need to include more related work that are highly important
[1] Woo S, Park J, Lee J Y, et al. Cbam: Convolutional block attention module[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 3-19.
[2] Kitaev N, Kaiser L, Levskaya A. Reformer: The Efficient Transformer[C]//International Conference on Learning Representations. 2019.
The authors need to introduce some related work on the efficient attention mechanism design.

Need more justifications about the novelty claims
The proposed Attention-Convolutional (AC) block of this paper is too close to the Convolutional Block Attention Module in Ref [2]. Therefore, the authors need to provide more justifications about the novelty claims.
This paper lacks a theoretical analysis and a complexity analysis of the proposed efficient attention mechanism.
The following references are hoped to be helpful for the authors to further improve the quality of this paper. Although Ref. [3] is only recently published, it is still easy for authors to refer to and follow up.
[3] Zheng, Lin, Chong Wang, and Lingpeng Kong. ""Linear Complexity Randomized Self-attention Mechanism."" arXiv preprint arXiv:2204.04667 (2022).

The evaluation needs to be enhanced in terms of baselines, datasets, settings, etc.
It would be better if the authors could add more benchmark datasets to verify the effectiveness of the proposed model.
The authors need to add more advanced baselines such as work based on an efficient attention mechanism (e.g., ref [2]) to further validate the effectiveness of the proposed model.

Need to check for grammatical errors and typos
The picture in table 1 is blurry, please provide a clearer version.
The editorial quality of this paper is largely unsatisfactory. It contains quite a lot of inconsistent/non-precise description, as also reflected in the above comments."
130	Deep Laparoscopic Stereo Matching with Transformers	"The paper is well-written: adequate background to the problem with references to state-of-the-art methods in learning-based stereo matching is given. The rationale behind the use of transformer networks is given and the proposed architecture is studied in terms of loss landscape, learning trajectory and accuracy. Finally, the experiments are adequately described while the results are clearly presented.

The proposed method is compared to the state-of-the-art using publicly available datasets. Mean absolute error has been used to compare the performance. I would suggest reporting either the variance together with the mean or report the RMS-error. Other descriptive statistics such as the 95th percentile will be useful to the serious reader.

Significant value can be added to the paper by including details on the running time and memory requirements of proposed architecture."	"In Fig. 1, a wrong disparity map is used. Replace it with the one associated with the stereo input. ""Transformer"" instead of ""Tranformer"" in Fig. 1.
4th line in Sec. 2, ""natural the stereo matching task"".
In Sec. 2.1, besides the number of layers, you will also need to maintain a relatively similar number of learnable parameters to make a fair comparison.
3rd line in page 4, ""we"".
In Fig. 4, it is difficult to see the value of the lowest point. Also, there need to be some quantitative measurements to justify your claim about the shape here. A larger set of hyperparameters should also be tried in order to remove the randomness in the loss landscape caused by a single set of settings. What do the axes stand for?
In Fig. 5, the numbers are too small to see. Also, the scales of the axes are different across different methods. It is very difficult to obtain any useful information from these plots. What do the axes stand for?
The DPI of Fig. 6 is too low. Also, this figure is pointless if you want to show the accuracy comparison because the models have not converged yet.
In Table 1, why not fine-tune the learning-based methods on this dataset to show those results also? The references to these comparison methods need to be made in Table 1.
The comparison with STTR is not fair because this method tries to predict an occlusion mask and does not produce valid disparities values in those occluded regions, as can be seen in Fig. 7. You should also try to compare the performance of the non-occluded regions.
In Table 3, the direction of the arrow for SSIM seems to be wrong and you did not explain what these arrows stand for.
DSSR uses a similar architecture as STTR, and you should explain this in the paper."	I just have a question other than the one above. In Fig 6, why are the errors lower on SCARED2019 than on the in-domain data?
131	Deep Learning based Modality-Independent Intracranial Aneurysm Detection	"This is a really well-written and interesting paper.
The data should be described in more detail, especially the spatial resolution is needed. It may be the case that the method just works well because the resolution off the CTA and MRA is similar.
The CTA in Fig 2 looks odd.
The ground truth used to train the vessel segmentation needs to be explained in more detail.
Another alternative for a modality agnostic detection method would be to use image-to-image translation techniques.
No true baseline comparison"	"Background: ""...methods are summarized in Table 1."" -> Table 2
Fig 2. could be enlarged to improve readability, in particular of the heatmap.
More details on the imaging parameters must be provided.
More details on the vessel extraction and its accuracy must be provided.
Impact of the vessel extraction on the detection performance must be investigated."	I would suggest including the processing time for each case, as it would make evaluation of the time savings involved clearer.
132	Deep Learning-based Facial Appearance Simulation Driven by Surgically Planned Craniomaxillofacial Bony Movement	"Introduction

The introductory paragraph about the use of FEA should be revised.  The standard of care seems to be primarily physician opinion.  FEA could be used.  Is FEA used clinically with widespread deployment for this indication?  That is not my understanding. The references do not refer to specific uses of modelling for CMF surgery.  Has modelling been specifically used for CMF surgery and shown to be successful?  If so, the authors should reference it.  If not then the paragraph should be largely revised.
Most of the second last paragraph can be put in the methods.  This paragraph is repetitive and not clear until having already read the methods.
Figure 1 - more detail is needed and more description in the caption.  There are numerous rectangles that are completely unlabeled.  Other than a pointnet++ feature extractor it is completely uclear what the other elements are.
Dataset
Include the spatial resolution of the imaging used.  Comment on how this relates to the errors observed.
How accurate were the registrations?
Implimentation
What boundary conditions are put on the FEA. How difficult are these to apply?
Results
The nose predictions appear to be the best.  Is this because there is little change in the nose after osteotomy?  How much displacement is there between the faces before and after surgery?
Ablation study
o	The finding on that CPSA was more effective than closest point is overstated.  The differences are small and the model has more parameters, these parameters may establish point correspondence or may do some other error correction"	"The facial deformities are complicated, the author could focus one or two kinds of main deformity to conduct and evaluate their method, as this work is really useful in clinical area. But the high accuracy is required.
The qualitative experiments were performed on upper/lower lips, this may be the key attention of the clinicians?
On page 5, "".... deep learning-based SkullEngine segmentation tool, SkullEngine segmentation tool [6],..."", repeated ""SkullEngine segmentation tool""."	"The paper is strong and well-written, thus there are no major issues (apart from the ones mentioned above dealing with the related works). Some possible improvements/suggestions are listed below.
The computation time required by ACMT-Net to complete a simulation significantly outperforms its FEM-based competitor. It would be interesting to report which is the part of the proposed methodology taking most of the computation time.
Authors should consider adding further details about the experienced surgeons who took part in the qualitative evaluation. How many of them?
Typos:

Faical -> facial
""Skull engine segmentation tool"" is repeated twice in 3.1"
133	Deep learning-based Head and Neck Radiotherapy Planning Dose Prediction via Beam-wise Dose Decomposition	"I believe the authors should aim to address the main weaknesses of the paper outlined above. In particular, the value-based DVH loss function should be re-evaluated and checked whether it is indeed equal to the average error. In case I have have misunderstood the corresponding paragraph, the authors should consider reworking it to improve its clarity. The experiments and results section should be updated to include all baseline methods. The authors should consider including dedicated experiments that evaluate the quality of the dose map predictions along the beam paths. Finally, I believe the authors should aim to more carefully discuss how their work fits into their vision of an ""AI solution for radiotherapy""."	This paper seems to take a promising approach.	"There are some concerns of this paper:

In recent years, many deep learning-based methods have been proposed to handle this dose prediction task, what are the limitations of the previous methods? Authors lack to discuss them in the introduction section.

There is no clear information about what network architectures are used in Global Dose
Network and Beam-wise Dose Network.

It is unclear how they realize the ""disassembling-then-assembling"" strategy at the network design level.

The experiment evaluations are a bit insufficient:

In Fig. 4, how about other views in the visualization results?

It seems that there is no clinical expert involved in the evaluations.

All comparison methods should be provided in the visualization of DVH curves (Fig. 5).

How did you implement the methods, which do not have the source codes in public, such as Lin et al. [5], Gronberg et al. [3], etc.? More information should be provided for this in order to guarantee a fair comparison."
134	Deep Motion Network for Freehand 3D Ultrasound Reconstruction	"Authors claimed that the method is really better than others. And this is clear from quantitative and qualitative data. However, explanation is weak about why the method works better than others.
I suggest authors to publish their GUI and also their results for other researchers
IMU and details of network implementation should be added to the paper."	"Fig.3 hard to interpret due to noisy sensors. Maybe an additional trend-line or visualization of the local variability can help the reader to understand how much the pose gets stabilized within adjacency of the neighboring frames. 
Statement ""All images were scaled down to 0.6 times their original size"" needs some clarification - why? Due to the input network tensor size?
Which PyTorch version is used?"	"The main focus of the paper is the usage of the acceleration signal of the IMU, so I will first focus my main comments on this part.
While the experiments do show that this is effective, I am a bit skeptical about the justification of some parts of the method.

I am not sure to understand Eq.2, and in particular why would removing the average signal would reduce the effect of the noise. Do you assume a noise model with a non-zero average? 
Also, what is the point of subtracting the gravity vector, since the output vector is enforced to have a zero-mean? It seems to me that the equation could be simplified into: Ai <- Ai - 1/N \sum_n An.

While I do understand the inspiration behind Equation 3 (v_i = v_{i-1} + a_{i_1}), I am not sure it really makes sense for feature maps.
Of course the network can be trained with this model, but I am wondering whether this really brings something compared to just concatenating them.

About the self-supervised step,
are some layers frozen or are they all adapted?
why doesn't the network prediction collapses to the IMU raw data, and completely ignore the original image content?

It must be noted that this step requires more computational power and is therefore not as easy to deploy (compared to just runnning an inference).

It would be interesting to study quantitatively how well the acceleration is estimated (for instance report correlations between accelerations).
Unlike what is stated in the paper, the bottom row of Figure 3 does not seem very convincing to me: I don't see any particular trend between the blue and red plots.

One key drawback of most current methods is the difficulty to detect ""turning points"" (having the user go back and forth to extend the covered area for instance), due to the implicit ambiguity of the direction of the probe movement.
Since this method uses IMU acceleration, it raises some hope that such points could be detected and estimated, yet this is not discussed at all.

General remarks

The notations are not very standard and make some equations a bit confusing:
angles are represented by E (instead of Greek letters like phi, theta, etc.) or sometimes with alpha,
inverse transformations with a * (instead of ^{-1})

accelerations sometimes with A, sometimes with a

The results are reported in a Table with only the mean and standard deviation, so no information on the distribution is available: we don't know whether one method is more robust or more subject to outliers than another one. Replacing it with boxplots or violin plots would give more information to the reader.

Future work is not discussed at all in the conclusion.
I think it would be interesting to see how the self-supervised step is able to help the generalization of the network (to new US systems, sonographers, motion speed, anatomies)
Besides, being able to detect turning points would be a big deal and would give much more impact to this approach.

In my experience, the performance of IMUs differ a lot across models. I think it would be important to report the brand and model used for the study.

The authors used a statistical test to prove the significance of their results, which is commendable. However, the t-test that they used assumes the results to follow a Gaussian distribution, which is neither guaranteed nor discussed. A better alternative would have been a Wilcoxon signed rank test, which does not assume a particular distribution.

Minor comments

The input of the network is not clear - are the pairs of images encoded as 2 channels?
Was the ResNet pre-trained (for instance on ImageNet)?
""Based on ResNet, LSTM"" -> LSTM modules are not necessarily based on ResNets
It would be interesting to know whether the authors came up with this architecture after a lot of trials (i.e. this is the best position for those LSTM modules), or whether they just added them here because it makes sense and provided better results right away.
What are ""loop scans"" exactly? I don't see any such motion in the 6 examples shown. Could you add one?
Figure3: the blue line hides the red one and makes the figure difficult to read. Consider adding some transparency (for instance apha=0.75 in Matplotlib)
It seems a bit suprising to only use a batch size of 1. Isn't it a problem for the LSTM modules?
Figure 5 is nice, but it would be even better with a legend to distinguish between the different colors (more visual than reading the colors in the caption)"
135	Deep Multimodal Guidance for Medical Image Classification	"to compare the model performance difference with different input features, the statistical analysis should be used. I am not sure whether the displayed difference is significant in statistics.
2.the model performance didn't work better than  direct fusion of two imaging modalities, which made the work not important enough.
multi-site external dataset (if exist) should be used to testify the generalization."	"The paper is concisely written and nicely guided through the text. The related work is introduced in an appropriate length for such a conference paper.
The figure and tables support the findings and contribute to the understanding.
Please consider the comments listed under 5."	"""Consequently, it would be advantageous to leverage the inferior modalities in order to alleviate the need for the superior one. However, this is reasonable only when the former can be as informative as the latter."" Luckily for the authors, this assertion is not true.  If it were true, the authors would be in trouble because their own data shows that inferior is not as informative as superior.  The problem with the assertion is that there are many use cases when inferior is not expected to replace superior 100% of the time; let's say both modalities are locally available, but we are trying to reduce usage of the superior modality due to cost, burden, or some other reason.  Using the inferior modality as a cheap screening tool that rules out, say, cancer without the superior modality most of the time (say in 90% of cases), with the other 10% of cases requiring the superior modality, would still make a large positive impact in many application domains.  This kind of cheap screening problem setting is not really considered here.
The description of the tradeoffs associated with image-to-image translators is mostly OK.  It's true that they don't deal with image-to-non-image translation very well or differences in dimensionality etc.  But an additional stated weakness is essentially ""what if it doesn't do image-to-image translation very well?""  That's a kind of non-informative critique- you could say ""what if it doesn't work"" about any hypothetical method.  The upside of image-to-image translators- that you can use the translated image for a multitude of different tasks, not just the one you did inferior-to-superior translation for- is not mentioned.  The current method trades off that flexibility with perhaps superior performance on a more limited job (i.e. the classification task).
To me, ""guidance"" is confusing.  The term has an active connotation, akin to active learners that are provided novel training examples optimally selected for them by an oracle as ""guidance"" to improve their representations.  Really, using more traditional terminology, the inferior and superior classifiers includ feature selectors that reduce the dimension of the input data in a way that is beneficial for classification; what you are calling ""guidance"" is really feature translation or transformation.
Table 1 is very difficult to follow, with notation that is only partially explained in the caption.  Where the rest of the notation is described in the text, it's in code, e.g. we need to translate from ""G(I)+I"" in the text to ""G(R)+R"" in the table.  Make it easier for the reader by writing out the row names in the table in English.
The discussion seems to take it as given or non-surprising that the inferior modality adds value on top of the superior modality in both applications.  How do we know that there is really independent information in the inferior modality, as opposed to just poor guidance / feature transformation from inferior to superior?"
136	Deep Regression with Spatial-Frequency Feature Coupling and Image Synthesis for Robot-Assisted Endomicroscopy	X	"Too much details are introduced in the first paragraph.
More introduction in the figure 1, 2 and its description.
More straightforward visualization of result, rather than the Signal direction only."	Please see above
137	Deep Reinforcement Learning for Detection of Inner Ear Abnormal Anatomy in Computed Tomography	"I think this is a very promising work that shows good performance in abnormal inner ear anatomy detection. The validation framework is excellent, and the proposal is sound. However, the good quality of this work is reduced by the lack of clarity in the methods part. Please, consider clarification of the methods in the following points:
	1.	Describe the distance between shapes properly. d_ji is the L2 norm of the difference between shape vectors b_i, b_j?
	2.	How do you ""normalize"" and ""merge"" the Q-values?
	3.	How do you calculate the STD from vectors of Q-values?
	4.	How does aggregation of variances across agents and Q-values validate the hypothesis of uniform distribution?
	5.	Describe Fig. 4 in the caption.

With these clarifications, I would strongly recommend publishing this interesting work."	"For the first measure based on PCA, a key limitation of the covariance matrix in classical PCA is its high sensitivity to anomalies meaning increased false positives. There are more robust PCA variants available even within one of the papers the authors cite: Amor et al. (2017) https://doi.org/10.1109/DISTRA.2017.8167682. This maybe worth mentioning in a discussion section.
The two methods presented PCA and Q-value based showed markedly different performance (fig 5e and 5f) for which the authors should try and provide any explanation.
Some statements on limitations of the approach and future research direction would be welcome.
The training regime is not presented and instead the reader is referred to the literature. This could be briefly outlined so the manuscript is self contained
There is no explanation for why 3 agents per landmark are used for the test on Synthetic data whereas 1 agent per landmark is used for the Real abnormality test. An explanation should be provided.
It's not clear where the 92 normal anatomy images mentioned on page 5 come from until one reads the results section on page 7 which is odd. The narrative order of mention should be revised.
A description for the normalisation of Q-value vectors on page 6 would be helpful. How are the values normalised?
The authors may want explain their specific contributions, if any, to CMARL and MARL.
The authors may want to improve the readability by breaking up some of the long sentences.
Some attention should be paid to grammar throughout the manuscript.
A range of voxel resolution on described on page 3 should be provided.
There is a typo on the first sentence of the Figure 1 caption, "" Set of landmarks used in small the Synthetic Set""."	"Add more comparisons to other methods. I was curious how a classification network would perform on this task.
It would be interesting to further explore how to better combine different abnormality scores and make a better prediction. For example, the way to choose the weighting factor, other clues from the model output when the abnormality exists."
138	Deep Reinforcement Learning for Small Bowel Path Tracking using Different Types of Annotations	See above	To make the manuscript more complete and to justify the conclusions that have been drawn, the authors should address the comments made. Specifically comments about statistical tests since without these tests drawing any conclusions is difficult. Moreover, some sentences need clarification and some definitions need to be added in order to make the paper self-contained. Please refer to the comments above for further details about the points mentioned here.	Overall, I think it is a good paper. The only concern is the way to evaluate the predicted paths as mentioned in the weakness part.
139	Deep treatment response assessment and prediction of colorectal cancer liver metastases	"Further clarifying the difference between the Siamese network approach of Jin et. al. and the proposed work would be beneficial.
Including a sentence or reference on the validity of the selected augmentations would help justify the choices made.
Was only one type of treatment used? How would the performance vary with treatment ... a quantitative evaluation of some sort."	see no. 5	"Given the size of the data is limited, please consider k-fold cross validation (or additional testing on independent data). All the data can be fully utilized through cross validation. Also, more evaluation metrics can be obtained to check the stability of the model across different fold.
It is not very clear how CT pairs were split into train/validation/test. Each patient can have multiple CT pairs so there might be label leakage if the train and test have images from the same patient. Please clarify how dataset was split.
How many pre-treatment CT scans available for each patient? If there's only one, the total number of cases for the prediction model is only 102 (not 400). Please clarify the data difference for the two tasks.
How was the liver metastasis segmentation evaluated? Did the segmentation performance impact the outcome assessment and prediction?
Did the TRA and TRP models share the parameters in the ResNeXt module? Or they were trained separately?
How was the sensitivity/specificity defined for the four-way classification problem?
I can see the potential value of the treatment response prediction model, but it is unclear the benefits of the treatment response assessment model for clinical application. If the goal is to alleviate the burden of manual liver metastasis segmentation, the segmentation model has already achieved this goal. As mentioned by the authors in the paper, RECIST is just a unidimensional assessment of the lesion based on human annotation. So some simple measurement of the lesion contours would be enough if the segmentation is good."
140	DeepCRC: Colorectum and Colorectal Cancer Segmentation in CT Scans via Deep Colorectal Coordinate Transform	MSD is used both to refer to the Medical Segmentation Decathlon and the Mean Surface Distance, making the analysis confusing.	"This approach appears to have a lot of potential and thorough validation is performed on a small in-house dataset with cross validation
To show more generalisability and further independent testing it would be valuable to show performance on Decathlon-Task08 which is reference for comparison. Even if the dataset does involve preparation -- which should make the task easier. Alternatively, releasing the code would make this easier to verify independently."	Please check part 5 and part 7 for detailed information.
141	Deep-learning Based T1 and T2 Quantification from Undersampled Magnetic Resonance Fingerprinting Data to Track Tracer Kinetics in Small Laboratory Animals	"A deep learning based MRF parameter mapping method is proposed which uses a sliding-window averaging to extract spatial features and combine the use of map loss and recon loss. With the proposed method, a 4-fold further acceleration is achieved compared with the baseline MRF method.
Due to CSF flow and large T1 and T2 property, T2 estimation is always not as faithful as parenchymal tissue under MRF parameter mapping framework. Besides, CSF normally is not of interests. Thus I suggest to further analyze on different brain tissue(such as WM, GM and CSF), which should better show how different methods or different acceleration factor data perform."	"In general, I think it is a good scientific paper, with sound experiments. My main concern is the low impact it may have, due to limited novelty of the methods and narrow application. It may not be the best-suited conference for this paper.
One of the stated objectives of the authors is to track T1 variations induced by Mn2+, and the training set includes images acquired before and after Mn2+ injection. While the authors do show T1 differences between time points, an even stronger experiment would be to train on pre-injection images and test on post-injection images (and vice versa) to see if the inclusion of both conditions in the training set reduces a bias in the fitting process; or conversely if training on control cases only generalizes to Mn2+ cases.
It could be clarified how the data was undersampled for training: were the same two spiral arms always used? Or were two spiral arms out of 48 randomly selected?
It is a bit surprising that the moving window does that much better than the feature extraction layer. It seems to me that the FE layer has the flexibility to learn a simple moving window. Does it mean that the FE is just more difficult to train? (t would be useful to state the kernel size - input and output features, width and height - of the FE layer)
It's not perfectly clear to me if C-2 contains exactly one consistency loop? (i.e. C-1 would not use the consistency module at all)
Similarly, does the fully sampled component (FS) correspond to lambda != 0, that is the loss include loss_recon?
There are no statistical tests, and it seems to me that no true difference exists between SW+U/SW+U+FS/C-2."	"In the abstract, I would suggest mentioning only the approach that yielded better results, to avoid confusion.
The implementation details should be described, i.e., training time, test time, etc.
Discussing the related works [3, 4, 5] would help to highlight the value of the contribution.
SW+U+FS was expected to yield better results, with the current tone of the manuscript, but it did not. Rephrasing or further discussion would clarify this.
Data augmentation approach could have been more exploited. The works [3, 4, 5] have not employed data augmentation."
142	DeepMIF: Deep learning based cell profiling for multispectral immunofluorescence images with graphical user interface	"Although cell detection is one manner for the cell-based analysis, while segmentaion based manner should be the preferred manner. Firstly, there is already model (deepcell) with promising performance, and secondly downstream analysis based on cell segmentation is far more interpreable compared with detection based manner.
Secondly, the classification evaluated in this manuscript is confusing to me. Is the NK/T deemed as positive, and macrophage as negative in the setting? Then how about other cell phenotypes?"	"For Table 1, could the authors also list the positive and negative distributions of each DI? It would be better to also include which markers are nuclear and which are non-nuclear.
Since cell detection and classification are two stages in DeepMIF, it would be better to also show the overall combined evaluation metrics to show how effective cell types have been identified from detection to classification.
For the external validation dataset, what's the performance for cell detection? Could we perform cell detection for the external validation dataset based on the trained model from the Immune T cell panel? As DeepMIF cell identification is a two staged process, only reporting the classification performance could not provide a comprehensive view of the performance of DeepMIF.
As performance between VGG16 and DeepMIF is very close, it might be better to perform cross validation to see if the performance of DeepMIF is statistically significantly better. Moreover, the authors mentioned that model 2 contains much fewer parameters than VGG16. What about the training time between model 2 and VGG16?"	This is a good paper bar the availability of the data.
143	DeepPyramid: Enabling Pyramid View and Deformable Pyramid Reception for Semantic Segmentation in Cataract Surgery Videos	"What is the criteria for choosing the networks to be assessed?
I missed the authors did not include the best results that have been reported on the CaDis dataset for a fair comparison even though they used an extended dataset.
I recommend the authors to show cases where their proposal failed and discuss the reasons for such mis-segmentation.
Minor corrections:
In Abstract ""Compbined""
In Fig. 1 ""Challenges in semantic segmentation for different in cataract surgery"" ??
In Page 2 the order of citations. ""Several network architectures for cataract surgery semantic segmentation have been proposed or have been used in the recent past [15,13,14,1,21]."" I recommend the authors to sort the numbers for a better presentation ""... have been used in the recent past [1,13,14,21].""
In Section 4 and in Table 1. are PSPNet+ and PSPNet the same model?"	"Explain more motivation why some exiting techniques are used and more analysis why they are effective.
Provide more details about the experiments."	"1) Abstract: The authors should mention the performance improvement achieved by their model compared to state of the art.
2) The main concern is that the authors need to justify why they include PVF in the decoder network. In most of pyramid view fusion networks used in the end of the encoder network. In a typical autoencoder architecture, the encoder extracts global context information from the input image, including the adjacent and class characteristics of the object. However, transmitting information to shallower layers will weaken the extraction of the context information due to the down sampling processes. Thus, many works proposed to use PVF in order to generate multi-scale features for each split using reduced pyramid pooling module. But here, the authors insert this layer in all layers of the decoder. The authors should explain their architecture in more details.
3) The Deformable Pyramid Reception (DPR) network is needed to more details. The proposed DPR is very similar to the network proposed in ""DefED-Net: Deformable Encoder-Decoder Network for Liver and Liver Tumor Segmentation"". Also, most of papers proposed DPR networks used them in the encoder networks to learn better context information of the input images than the Atrous spatial pyramid pooling.
4) It is very important in any medical method, to show the limitation of the work is something very promising, given that when reading the work we can glimpse a way to get around these limitations.
5) The experimental section is very limited. There is no ablation study to address the effects of each module on the proposed segmentation model. The authors gave an overview of the proposed model, but they did not mention anything related to each submodule's architecture. The article was missed a detailed description.
6) Missed References

Nandi, A., Lei, T., Wang, R., Zhang, Y., Wang, Y., & Liu, C. (2021). DefED-Net: Deformable Encoder-Decoder Network for Liver and Liver Tumor Segmentation."
144	DeepRecon: Joint 2D Cardiac Segmentation and 3D Volume Reconstruction via A Structure-Specific Generative Method	"""minimal radiation exposure"", MRI has no radiation exposure.
The input images (x) were motion corrected by a preprocessing step I assume (see Fig 1)? Please specify how this was done.
""resampling function that align"", it is not clear to me if this step performs some registration, or simply resamples the data? Please clarify.
The Settings paragraph of Section 3.1 is a bit cluttered because the five experiments are not in an enumerate{} environment. If space allows, please consider changing this for improved readability.
""3D DICE"", the Dice metric is agnostic to the dimensionality of the inputs.
It is surprising to me that linear interpolation seems to perform substantially worse than computing Dice on the original data, I would expect a very similar performance, or slightly better for linear interpolation (often used as a simple baseline in super-resolution work for example). Any ideas why this might be?
Fig 4: would be easier to interpret if each plot had a title (e.g., 'normal-to-normal', 'diseased-to-normal.')"	"There are some major concerns of this paper:

The general pipeline of this method is simple. It includes an autoencoder and U-Net network. There is not much novelty in the method and network design. Furthermore, the two tasks, i.e., image representation/reconstruction and segmentation, work sequentially from their workflow. It is unclear how they work jointly as they proposed?

In the proposed framework, it is unclear how it can realize high-/super-resolution 3D image generation (no technical description on it). There is also no evaluation on this task as they proposed.

In the 4D motion adaption, how to guarantee the smoothness and accuracy along the time space is unclear. From the supplemental video, we can see that the organ shape results look noisy and jittering.

What are the training steps on the comparison methods (e.g., DirectSeg, SemanticGAN, W+SegNet)? We can see that the proposed method has two training step settings. But it is unclear what is the setting for the comparison methods.

In general, the experiment and evaluation sections are a bit weak:
The evaluation only includes limited segmentation methods for comparison, more state-of-the-art deep learning-based 3D image segmentation methods should be included.
The evaluation only includes some basic reconstruction methods (e.g., Linear Interp, CPD) for comparison, but there is no comparison with state-of-the-art deep learning-based 3D image reconstruction methods.
The visualization results in Fig. 2 and Fig. 3 are difficult to see the difference between different methods."	The complete system used are a good ensemble of the methods with very good results of the system is very good. Nevertheless, the novelty is not the strong part of the paper.
145	Deformer: Towards Displacement Field Learning for Unsupervised Medical Image Registration	"The qualitative results in fig.2 are upside down and stretched. I prefer the qualitative result in the supplementary material instead.
Minor:
Avoid ""Title Suppressed Due to Excessive Length"". (The running head.)"	Please try to evaluate the methods over broad clinical registration scenarios, in addition to the brian image registration.	I think the author's work is valuable because it outperforms VoxelMorph which is one of the most popular benchmark in medical image registration. One very interesting thing in this paper is that encoder-decoder framework adding Deformer module outperforms both CNN and Transformer model. I think although this model does not have too many design on model, the results are enough provocative to researchers. I encourage the author to make the codebase open source in Github and thus people could build new benchmark in registration area.
146	Degradation-invariant Enhancement of Fundus Images via Pyramid Constraint Network	see section 5	The FPC shows superior performance as shown in table 2. However, the discussion on it is not sufficient. More experiments on it would make FPC more convincing.	"This manuscript proposes a pyramid constraint to develop a novel model called PCE-Net for fundus image enhancement.
The main idea of the work is to form SeqLCs and LPF from degraded images. The PCE-Net is to learn a degradation invariant model. The experimental results verifies the effectiveness of the model.
Suggestions and questions are as follows:

In ablation study, the authors didn't consider the effectiveness of the enhanced images for classification tasks.
How about the segmentation results on SE, EX, MA in enhanced fundus images?"
147	Delving into Local Features for Open-Set Domain Adaptation in Fundus Image Analysis	"Overall I think this paper is well presented. The model description is clear. Here are my comments.

Though the paper proposes IRS to reduce the computational cost, it seems to me it is still very expensive to train the model, considering a K-means clustering is extensively run to update the prototypes, which makes it even not feasible for large scale data. Please comment. In addition, I would recommend the paper to show computation time for model training.

It may be a minor issue or I misunderstand, in the experiment K^s=5 since there are 5 diseases in the source domain, \beta=1.5, so K^t=7.5, a fractional cluster number?

For the update of prototype, the paper proposes to use momentum controlled by \yita. A suggestion would be adding linear scheduling for \yita to make it variable instead of a fixed constant, this may improve convergence speed. Such approach is frequently used in existing models, such as mean-teacher exponential moving average."	"Since you have mentioned that ""they usually have similar global structures with uniformed appearances, such as the optic disks, vessels, and even common lesion patterns appeared in different diseases"", it may be a good idea to make use of the spatial information for OSDA methods on medical images."	The authors are encouraged to comment on the relatively strong assumptions of the proposed method: to which extent can the proposed method to be deployed in practice?
148	Denoising for Relaxing: Unsupervised Domain Adaptive Fundus Image Segmentation without Source Data	How clinically significant is the accuracy boost?	"It is suggested that authors should give specific definition of each utilized symbol.

It would be better if that each used symbol is mentioned in the whole framework figure.

Authors should compare with other label correction methods in unsupervised domain adaptation task or learning with label noise task, such as [*5], to demonstrate the superiority of the proposed label self-correction method.

[*5] Zhang P, Zhang B, Zhang T, et al. Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 12414-12424."	"Please address the points listed under ""weaknesses"". In section 3.1, a better description of the split into training and testing data on the source and the target domain would be helpful. Maybe a table could better visualize the different allocations."
149	Denoising of 3D MR images using a voxel-wise hybrid residual MLP-CNN model to improve small lesion diagnostic confidence	"Some suggestions:

It would be interesting that authors gave some information about the computational time required by the proposed method.
Summarize the research limitations and future research directions.
Extend the Conclusion with details on the method performance when compared to other tested techniques (in terms of PSNR improvement and SSIM)."	"Ablation studies need to be performed to evaluate the contributions from the MLP and the CNN.
How does the patch size affect the performance of the model?
Why only the central slice and its neighboring slices were extracted? Do the authors also did this for the test data?"	"This paper proposes a neural network with multiple residual MLPs and a residual convolutional subnetwork for MRI denoising. Compared with some other denoising methods, the proposed method shows superior results by presenting clear small lesions. The paper is well organized and has presented enough figures and tables to support authors' ideas.
The only suggestion is to consider conducting ablation study to show the contribution of residual MLPs and residual convolutional subnetwork parts for final denoising results."
150	DentalPointNet: Landmark Localization on High-Resolution 3D Digital Dental Models	"A discussion on the improvement of DLLNet to DentalPointNet can be given. Also, demographic information about the patients (age, sex, etc.) and the ratio of missing teeth should be given. The robustness of the method should be discussed because the FP in Table 1 is greater than in Table 2.
The term  ""DentalPointNet"" should be given in the abstract.
The results before/after data augmentation can be useful to show the effectiveness of data augmentation.
Is curvature constraint (>0.65 threshold) held for all landmarks?
In Table 1, bold parts are inaccurate for FP and FN."	See weakness.	"** Abstract

please remove the RPN abbreviation from the abstract since it has not been used further.

** Introduction

page 3 - first sentence: ""..., which significantly reduces false negatives on ...""; This sentence should be modified - first, ""significantly reduces"" is too vague and second, this reduction is compared with what?

** Methods

There are a few parameters that were set without explanation - selected 128 proposals, thresholds (0.65, 0.85), lambdas.
Landmark annotations from Fig.3 should be included in Figs. 4 and 5.
page 6 - first line of section 3.2 - DLLNet reference is number 10, not 9.

** Results

Table 1 - PointConv and DLLNet have shown 0% of FP and FN? Is this correct?

** Conclusions

Instead of ""significantly outperforms competing state-of-the-art methods"" please provide numbers or percentages to illustrate the performance.

** References

Do not use ""et al."" in the reference list. All authors should be included."
151	DeSD: Self-Supervised Learning with Deep Self-Distillation for 3D Medical Image Segmentation	Please address my concerns in the weakness part.	"Can you show the effectiveness of DeSD on other tasks such as classification?
Can you include comparison to other segmentation approaches?"	"Please refer to my comments in Section 5.
The baseline methods compared in this manuscript, which is somewhat limited. All three methods are SSL developed in natural images. I would also suggest the authors include more methods developed for medical images. Particularly,
a. SOTA segmentation method developed for medical imaging analysis (e.g., nnU-Net [3]);
b. SOTA SSL method developed for medical imaging analysis (e.g., Models Genesis [4,5], Rubik's cube+ [6], or other SSL methods evaluated in [7])
I would suggest the authors release the code and pretrained models to increase reproducibility.

[3] Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J. and Maier-Hein, K.H., 2021. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), pp.203-211.
[4] Zhou, Z., Sodha, V., Rahman Siddiquee, M.M., Feng, R., Tajbakhsh, N., Gotway, M.B. and Liang, J., 2019, October. Models genesis: Generic autodidactic models for 3d medical image analysis. In International conference on medical image computing and computer-assisted intervention (pp. 384-393). Springer, Cham.
[5] Zhou, Z., Sodha, V., Pang, J., Gotway, M.B. and Liang, J., 2021. Models genesis. Medical image analysis, 67, p.101840.
[6] Zhu, J., Li, Y., Hu, Y., Ma, K., Zhou, S.K. and Zheng, Y., 2020. Rubik's cube+: A self-supervised feature learning framework for 3d medical image analysis. Medical image analysis, 64, p.101746.
[7] Taleb, A., Loetzsch, W., Danz, N., Severin, J., Gaertner, T., Bergner, B. and Lippert, C., 2020. 3d self-supervised methods for medical imaging. Advances in Neural Information Processing Systems, 33, pp.18158-18172."
152	DeStripe: A Self2Self Spatio-Spectral Graph Neural Network with Unfolded Hessian for Stripe Artifact Removal in Light-sheet Microscopy	"Thank you for providing uncertainty measures - highly useful.
p-values are not so useful. See Wasserstein, ASA statement on P-values 2016.
""within a wedge in the Fourier space"": maybe clarify here that stripes are always perpendicular to the edge in LFSM images (not diagonals), assuming this is the case.
Fig 4 does not show any difference between filling the wedge and DeStripe - would a differently-zoomed image show a difference?
(2.1) 
""survival function"": what is this?
""Rayleigh distribution"": why Rayleigh, and what is its relevance here?
""this concentric annulus"": how thin or thick? Does it matter?
Fig 1e: It looks like all coefficients in the wedge are labeled as corrupted. Is this typical, or would the actual mask be more pixelated, with clean and corrupt labels intermingled? Perhaps clarify that at the end of 2.1 where the wedge mask is discussed. If pixelated, perhaps modify the figure to make this clear.
(2.3)
This section leaves me wondering why a more straightforward method, such as filling in the fourier coefficients with the MAP based on the annulus distribution, would not work. Can you explain why the complex GNN system proposed is necessary?
""also corresponding frequencies into account"": what does ""taking into account"" mean concretely?
(2.4)
How are lambda_x, etc determined? Presumably they are image-specific, based on the number of edges and steps in the image.
(2.5)
First term of eqn 7 (ie |Y-X|): Why do you strive to match corrupted sections of Y (the stripes), if these are what you are trying to remove, ie why is the first term used at all?
Fig 1, 2,: Sideways lettering is basically impossible to read. If it is important, perhaps the figure can be rearranged so that the lettering is horizontal.
Miscellaneous:
""prior one"": what does this mean?
Typos/grammar:
In general, an additional thorough proofing would improve the text.
-holds true -> hold true

barely exits -> exists. Also the grammar of the sentence wants correction
-LSFM once excites -> leave out 'once'
""B is the Bregman"" -> V (I think; there is no B in eqn 2)
-(in 2.5) are X and X_tilda the same thing?"	See weaknesses for useful improvements.	"p.4. ""which fall within the same thin concentric annulus A^r_k"" Maybe you could annotate this annulus (or an example) in Figure 1 to guide the reader.
p.4. after equations 2a - c) you mentioned ""B is the Bergman variable"" but there is no ""B"" in the formula. I suppose this is a typo
p.5. "" Note that we use complex-valued building blocks [20] for W1 and W2"". Just out of curiosity: What framework did you use? I remember that there were some issues with PyTorch complex numbers implementations. But maybe this has been solved by now.
p.5 ""s a result, by explicitly modeling the sample-only spectral response as a weighted combination of its uncorrupted neighbors on a polar coordinate, stripe-only Fourier projection is exclusively re-served as activation M  H(L+1), which can then be subtracted from the input stripe-sample mixture Y for striping removal."" I think I get the point, but I find this hard to understand when reading it. Maybe this sentence should be rewritten.
p.6 ""shrink(*) is the scalar shrinkage operator"" Maybe a naive question: I am not sure how standard this operator is but maybe you could add a reference here just in case.
p.6 ""Eq. (7) is to prevent the model from learning an identical mapping by quantifying isotropic properties of recovered X, where Pk is the corrupted subset of
Ark indicated by corruption mask M, and Qrk = {x  |x   Ark , x  / Prk }."" I think here a short explanation might be nice to help the reader understanding the general idea here."
153	Detecting Aortic Valve Pathology from the 3-Chamber Cine Cardiac MRI View	"I have no major points of criticism. Minor issues:

Please add units to Table 1. Also, it would be nice to give the resolution of the Cine MRI sequences to relate pixels to mm.
Did you perform an ablation study?
It would be interesting to see in which cases classification failed. Could you give an example image, maybe?"	"6) What is the sensitivity w.r.t. hinge detection? Practically this is a surrogate for the distance to hinges (since crop window is computed based on these), no?
7) Please provide the image resolution  and relate of the error in mm to the overall size of anatomy and subcomponents.
8) The sentence in Fig 4 ""The green curve shows the predicted an
AV-regurgitation curve."" seems incomplete - please correct this, message not clear.
9) The processing is based on 2D slices only where as jets may have 3D shape, please comment on potential sensitivity w.r.t. imaging limitations and variations in acquisition"	"In addition to the comments raised above, some comments follow:

The term ""curve"" may be difficult to understand when first reading the manuscript, especially in the abstract. To improve comprehension, consider replacing the term in the abstract (referring to blood jet or similar) and then properly define it in the main text.
Please provide further detail regarding your proposed networks. Are the number of levels and filters per level equal to the original U-net? What is the initial localization network's input size?
How are the curve-based heatmaps created? Perhaps a Gaussian convolved with a binary mask of the ground-truth curves?
Since the heatmap can present insignificantly low values, was a minimum threshold value used to ""stop"" the curve tracking (i.e. ridge detection) algorithm?
How is the orientation of the curve defined? A vector connecting the initial and end point (as suggested by Fig. 4), or perhaps a line fitted to all curve points?
How is the ""probability of being a pathological curve"" defined? From the text after eq. (2), it seems to be the estimated mean probability of the regressed map over the curve, rather than a true ""probability"" of being pathological (vs. healthy).
No reference to ""random forest"" is given in the ""Method"" section. Please add it there (including certain implementation details found in section 3).
Was a patient-disjoint split use to separate annotated frames into training, validation, and test sets?
Consider decreasing the length of the abstract (the first sentences are unessential).

Given the variable pixel spacing of CMR images (and potential resampling used during your pipeline), consider reporting the localization accuracy in mm.

Please correct English grammar/spelling mistakes/typos. A few examples follow:
p.1: where it reads ""leaflet hinge points"" should read ""leaflets' hinge points"".
p.2: remove ""of"" from ""assess of blood flow"".
p.5: remove ""an"" from ""predicted an AV-regurgitant curve"" in Fig. 4 caption.
p.7: where it reads ""utlises"" should read ""utilises""."
154	DEUE: Delta Ensemble Uncertainty Estimation for a More Robust Estimation of Ejection Fraction	This is interesting work which will add to existing uncertainty estimation methods, while providing a fast, scalable, single forward-pass solution, so should be of some interest. The application of regressing ejection fraction for cardiac echo is an important one, and the method is not limited to this - moreover it can run on pretrained models.  However the conclusions are nor clear to me as the compared methods all seem to be in the same ballpark (Figure 3 - predicted RMS vs % of eliminated data, all showing similar quality of levels of uncertainty (apart from DDR) and a similar test error and need for calibration, so it is not clear what extra value DEUE is then bringing in in clinical practice.	The references are good. There is so much work in uncertainty estimation that not all references could possibly be given but the paper has a reasonable selection.	"The definition of epistemic uncertainty in Eq. 2 and 7.

From the definition in the paper, the epistemic uncertainty in equation 2 is the expected squared error. However, the prediction error and the epistemic uncertainty can not be viewed equivalent to each other.  A prediction can be of high error and low uncertainty in the meantime, or low error with high uncertainty. Please clarify.
Ignorance of the prediction error \epsilon_x in Equation 7. While W* is the optimal model parameter, when the network converges, W will be very close to W, which makes the (W-W) equal to 0. Then U_L(x) will be equal to prediction error \epsilon_x
Is there any difference between U_L(x) and E(\epsilon_x^2).
During training stage, the network is initialized from different points, thus leading to multiple local optimal estimation of W*. How to obtain these different initializations? I am wondering the results of Eq. 7 maybe very high if they are extremely far from each other. Besides, how about computing the diagonal entries of the covariance matrix \Sigma directly from these initializations? Will this be similar to or quite different from the diagonal entries of \Sigma_M that computed from the converged Ws?

Experiments of this paper is inadequate.

From figure 3, the proposed method shows inferior performance compared to its competitors.
And there are no quantitative results to demonstration the advantage of the proposed method.
Table 3 is not convincing without any quantitative results of the memory and running time. It will be helpful to test the real memory cost and time cost during inference."
155	DGMIL: Distribution Guided Multiple Instance Learning for Whole Slide Image Classification	"This paper analyzes the binary classification problem as an example. For a multi-classification problem, how to choose negative slides to cluster at the beginning of each iteration?
The author divided each WSI into 512x512 patches without overlapping under 5x magnification. Is the performance of DGMIL affected at other magnifications? Does the clustering step take up a lot of training time if the patch is split under high magnification?
Why are the AUC results in Table 1(a) different from those published in the DSMIL article?
How to determine if a patch is positive or negative when calculating the metric of patch AUC? what is the threshold of positive score for determining a patch is positive or negative?"	"More explanation of some settings: 
(a) It would be more convincing to either illustrate the advantage of using Mahalanobis distance or show the proposed method is agnostic to the choice of distance.
(b) Authors are encouraged to conduct the experiment of using the feature of backbone for refinement.
(c) It would help to detail the refinement training setting, e.g., epochs, learning rate, etc, especially the definition of refinement convergence.
More explanation of the performance gap: the reported results in table.1(a) are significantly lower than that in DSMIL, e.g., the Slide AUC of Ab-MIL is 0.6612 in this paper but is 0.8653 in DSMIL. Authors use different settings of patch size (512 vs. 224) and magnifications (5x vs. 20x). It would be more convincing to run the proposed method in the same settings as DSMIL instead of reproducing other works to avoid unfair comparison.
As is figured out in introduction, many key-instance based methods may give wrong pseudo labels or select insufficient instances. The authors could further validate that the propose method solve this problem by more detailed ablation study or visualization.
Reference mistake:
(a) Mistakes:The reference of Loss-based-MIL and CHiknotew-MIL are mistaken.
(b) Missing discussion of [1,2].
[1] Xu, Yan, et al. ""Multiple clustered instance learning for histopathology cancer image classification, segmentation and clustering."" CVPR, 2012.
[2]Sharma, Yash, et al. ""Cluster-to-conquer: A framework for end-to-end multi-instance learning for whole slide image classification."" Medical Imaging with Deep Learning. PMLR, 2021."	"Please refer to the weakness part in Sec.5. 
-Justification of the permutation invariance in the latent space is necessary. 
-More related works need to be covered, and if necessary need to be compared. 
-Validation on more datasets is highly preferred.
-Also, some typos need to be corrected before publication. For example, in Eq1, what's 'iff'?"
156	Did You Get What You Paid For? Rethinking Annotation Cost of Deep Learning Based Computer Aided Detection in Chest Radiographs	For tables 2~4, the results will be easier to interprete if they are shown in line charts or bar plots.	"Fig.1 contains many symbols, and it's hard to read separately from the main text. It's recommended that the authors add necessary explanations to all the symbols in the figure for better readability.
In the Learning Objective, L_{cls} and L_{seg} using the same symbols for prediction and GT. Please distinguish the different output and GT.
Please add a brief explanation for L_{cls} in Eq.2.
Table 2: why the authors said the models trained with less than 12K training samples in total all performed similarly? There are clear differences between the models' performance as shown in the table under different dataset size. Plus, it's recommended that the authors replace ""Dataset Size"" to ""Training Dataset Size"" in the Table.
In ""Improving Performance by Mixing Levels of Granularity"", the authors mainly describe the observations without further explanation. It would be appreciated if more discussion or explanation of the observations are provided.
Fig 2: it's recommended that the authors also put results of using pure Bboxes in this plot, and discuss why the performance is the worst.
There are few related works to this paper. I would appreciate it if the authors could discuss the differences in findings between the following works and theirs:
[a] Zlateski, et al. ""On the importance of label quality for semantic segmentation."" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018.
[b] Luo, et al. ""Rethinking annotation granularity for overcoming deep shortcut learning: A retrospective study on chest radiographs."" arXiv preprint arXiv:2104.10553 (2021).
If there would be an extension of this paper, I would appreciate it to see: (a) a quantitative analysis of the cost of annotations under different scenarios; (b) what's the best strategy when the annotation cost is limited. The authors can also refer to:
[b] Ren, Zhongzheng, et al. ""UFO2: A Unified Framework Towards Omni-supervised Object Detection."" European Conference on Computer Vision. Springer, Cham, 2020."	"In this paper, the authors investigate the role played by granularity, quantity and quality of the annotations and conduct a series of experiments to identify the best trade-off between accuracy and reducing the annotation costs. A common trend in literature is to exploit NLP to automatically extract labels at scale: this strategy is current exploited by most state-of-the-art public datasets in chest x-ray classification (CheXpert, Chest X-ray14). It is cost-effective but inevitably leads to a certain level of noise, despite advances in the NLP labelers.  The results are, to the best of my knowledge, in qualitative agreement with previous literature in the medical and general CV literature. 
The most interesting contribution, in my opinion, is how to mix image-level and pixel-level annotations for both segmentation and classification, proving that lesion-level annotations can substantially improve performance regardless of the training set size (at least within the range considered in the paper, from 1.2K to 121K)
The impact of noise is, in my view, less relevant, since the noise model considered may be too simple. The practical importance of the findings greatly improved if the noise model was backed up by an analysis of the actual errors made by an NLP such as the one used in CheXPert (https://github.com/stanfordmlgroup/chexpert-labeler). Non-random, structured  or feature-dependent noise is likely to have a far greater impact than postulated (https://arxiv.org/pdf/2003.10471.pdf).  Previous studies postulate that, in CheXpert, the 'No finding' class is the noisiest one (https://arxiv.org/pdf/2103.04053.pdf).  This limitation is appropriately addressed by the authors in their conclusions. 
A few aspects should be further clarified:

Are the networks used in the experiments pre-trained on ImageNet?
The notation used in Section 2.1 - learning objective is not very clear, as y^n is used to indicate both the class level and pixel level predictions. It would be clearer to differentiate the notations
How is the segmentation network trained from image-level annotations only, especially in the experiments depicted in Fig.2 in which different images had different label granularity?
How were the types of abnormalities selected for annotation? Less categories are used than CheXpert or other benchmarks: is there a particular reason?
In Table 1, it is interesting to note that the distribution of different types of findings is largely different from CheXpert - for instance consolidation appears in 35% of the images, whereas in CheXpert it appears only in <7%. I wonder whether these differences due to the population or to the labelling method.
In Table 1, I would report also the number of cases with no findings
In Table 2, the authors report the average of three independent run. I would specify whether data subsampling was repeated for each round. Given the level of data imbalance it is likely that randomly selecting a small subset of the training set will result in an insufficient number of samples for the less frequent classes."
157	Diffusion Deformable Model for 4D Temporal Medical Image Generation	"As mentioned above, it would be interesting to do at least one down streaming task after generating 4D images, such as analyzing the cycle consistency of the  cardiac volume, to show that the advantages of generated images compared to registration.
In table 1, it is hard to tell if the proposed method outperform the other two types of voxelmorph or not. So including a statistical test and a significance score would be appreciated here.
In Figure 5 the authors compared the results of different /lambda values in the loss function, and picked an optimal one. It would be interesting to include two extreme cases - when there is only the diffusion loss or only the deformation loss in the model, and see the functionality of each individual loss."	See the above comments.	"I found the idea of using diffusion models as a generative model for the interpolation of the cardiac cycle really interesting. The authors provided promising results. The proposed method could be used for interpolation in datasets with missing images or in applications where a more continuous temporal variation of the images are needed. I have just two comments I would be happy if the authors consider to address in the manuscript.
1) My first question is, did the authors implemented also a generative model using GANs? The authors claim in the introduction that GANs may generate artificial features, but do they have experience for the application? What about other ways of obtaining a latent space or generative models not based on deep-learning?
2) My second question is on the selection of VoxelMorph as a baseline for the evaluation. Are there any evidence that this could be a competitive method for the application? What about alternative methods based on traditional image registration such as geodesic regression or EPDiff based time-dependent interpolation based on LDDMM?"
158	Diffusion Models for Medical Anomaly Detection	The paper is well written and clear to understand. However, I have some concerns about the experiments and the novelty. As discussed in above, it would be better if the author can compared with some baselines . Moreover, the novelty can be considered as insufficient given that it was proposed in previous paper [25].	"Overall, I understood the main ideas of the paper and found it very clear. There were some specific points I did not understand:

How is the binary classifier used and why is it necessary?
How does the iterative noising process induce specific anatomy details (Section 2, page 4)? It seems like noising would add random, non-anatomical noise.
For results in section 3, are the anomalies found indicative of certain clinical findings or annotations?
How does the proposed approach compare to non-synthesis based anomaly detection methods, e.g., density estimation or feature modelling? Please see ""Visual Anomaly Detection for Images: A Survey"" by Yang et al. It would be helpful to add to the related work section and discuss."	"I think some points could give great benefits and more credibility to the paper:

Please check you VAE implementation and training, looks like something went wrong there (put at least as much effort in your baselines as in your own method).
Another dataset with a predetermined hyperparamter and training scheme setting and a realistic evaluation with quantitative numbers would give a better estimate of the performance of the method (e.g. a similar setting to the MOOD challenge)."
159	Digestive Organ Recognition in Video Capsule Endoscopy based on Temporal Segmentation Network	This paper is written well to read and data design is also well explained. However, methodological novelty is limited. The paper shows how the existing methods are utilized to make high performance. It is better to emphasize methodological novelty as a MICCAI paper.	See weaknesses and justification for improvements.	"On the comparison with other methods:
There is a significant amount of literature on surgical workflow segmentation. While the application context is different, the problem formulation of supervised temporal segmentation is exactly the same. The most recent have publicly available code and can off-the-shelf be applied to this problem. Two examples:

Gao, Xiaojie, et al. ""Trans-svnet: accurate phase recognition from surgical videos via hybrid embedding aggregation transformer."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021.
Czempiel, Tobias, et al. ""Tecno: Surgical phase recognition with multi-stage temporal convolutional networks."" International conference on medical image computing and computer-assisted intervention. Springer, Cham, 2020.

No comparison with any of these widely available alternatives is a weak point in the paper.
There are a few other details that the authors could clarify:

The data videos are classified into ""normal"" and ""abnormal"". Can the authors provide more details about what type of abnormal videos are included? From figure 3, I understand that ""vascular"", and ""bleeding"" are two sub-classes within abnormal, but this is not sufficient to fully understand what type of data is included.

On the same note, it would be interesting to understand if the proposed method has different accuracy depending on whether the case is normal or abnormal. At present only global accuracy metrics are provided. Can the authors expand on this?

""We use segmental edit distance (i.e., edit) and the segmental F1 score at intersection over union (IoU) ratio thresholds of 50%, 75%, and 90% (F1@{50, 75, 90})."" - Please provide references here that fully describe how to compute segmental edit and segmental F1-Score. These metrics are not widely utilised in surgical action recognition, and a reference would be useful to the reader."
160	Discrepancy and Gradient-guided Multi-Modal Knowledge Distillation for Pathological Glioma Grading	Fixing the minor issues would make the paper better.	"Figure 1 is great, but I think it is a little bit cluttered. Numbering each loss function component is a good idea but should have been ideal if you also reference and explain them properly in the same order in the text, currently only L_{DCD}^{m} does it.

In the first stage of the Training of the teacher the fusion of the genomic features vector and CNN features is denoted by a F, what is F? Concatenation? Kronecker or Hadamard products? Addition? I guess the 80 dimensional genomic vector gets transformed to an embedding dimension equal to the multimodal (and unimodal) embedding vector before classification. Anyway this should be clearly explained (since it is the multimodal part of the method!), but currently it is not.

This might be a great contribution to computational pathology if the code to reproduce the experiments is released with proper documentation and explaining how to extend it for other genomic+pathology paired datasets.

Extending this framework and showing the potential for different tasks would be a great contribution for the community and for a prominent journal in the field.

Qualitative results? It would have been great to see few heatmaps (for the different approaches) with paired gene expression data and compare with what is known for Glioma prognosis.

Is the AUC the average one versus rest AUC of binary classification problems?"	"Authors should give more details on stage I training and explain the motivation of using mean-teacher simultaneously.
More explanation about DC-Distill loss should be given to facilitate understanding.
Authors should introduce the data preprocessing carefully. How to choose ROI? Working on which magnification?"
161	Discrepancy-based Active Learning for Weakly Supervised Bleeding Segmentation in Wireless Capsule Endoscopy Images	"This paper can be improved with analysis on training procedure convergency.

It can be further improved if the authors can show the current formulations of model divergence and CAM divergence are theoretically optimal.

Fig.1 can be improved to be more clear. For example, rearrange sub-figures into a more clear logic flow.

minor issues, a) Abbreviation GI needs to be declared before using; b) typo ""Tabel"" in section 3.2."	See the weaknesses.	"As discussed in the weakness part, the design of multiple propensities needs further justification, i.e. using multiple sub-decoders in active learning, which could be viewed as model ensemble, has been shown to improve performance in prior works (e.g. [1]). What would be the performance if we simply use three identical propensities? Please also cite the relevant papers.
[1] Beluch, W.H., Genewein, T., Nurnberger, A. and Kohler, J.M., 2018. The power of ensembles for active learning in image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 9368-9377).
The paper only mention that the three-propensity CAMs are generated by multi-threshold, how the threshold is determined?
Equation 4 maximizes the L1 distance between D_c and D_f, what is the intuition between this? Should D_f be a sub-set of D_c? Maximizing the L1 distance would hurt the model performance on the overlapping region between them;
Equation 5, the paper mentions that the goal is ""making the boundary of the discrepancy decoders always surround ...standard decoder"", how is it achieved?
Training steps (2) and (3) seems contradictory, on (2), D_c and D_f is supervised to be different, while in (3), they are supervised to be the same, what is the intuition behind and why this would not cause the model failure?
The writing needs to be improved and be more precise, e.g. the paper claim that the improvement is significant and other methods are ""far inferior to ours"". However, comparing the performance in table 1, the improvement is mostly on 10% data regime while for the others the improvements are marginal."
162	Disentangle then Calibrate: Selective Treasure Sharing for Generalized Rare Disease Diagnosis	"Eq. (1): please revise it. The magnitude of g* is not included. Also, use magnitude for gi.
For the GID module, how is sort implemented. Also, sort operation is not differentiable, so how does this affect the backpropagation? Please clarify.
For the DTC module, how is the mask generated? Please clarify. Also, provide visual example if possible.
The details of the network architecture are not clear. Please consider adding some details on the figure or add them in text."	"Don't risk desk rejection by modifying the writing template, read https://conferences.miccai.org/2022/en/PAPER-SUBMISSION-AND-REBUTTAL-GUIDELINES.html  carefully.
Use the supplementary materials for space.
Use a statistical test, e.g. Wilcoxon signed-rank test for table 1, taking into account multiple comparisons.
Some of the phrases in the paper were strange e.g.
'identifying rare diseases based on scarce amountof data is seen as inevitable.'
'studying automated rare disease diagnosis using an extraordinarily scarce amount of medical images is of far-reaching significance'
'selective treasure sharing (STS)' what does 'treasure' refer to here?"	"In order to make the paper stronger, I kindly ask the authors to reformulate the DTC module description (for example how to crop the lesion region and the normal region).
Also clarifying the experimental setup of each of the used methods might help avoiding any fairness issues and let the reviewers and future readers have a less biased feeling of the empirical potential of the method. As I mentioned in the weaknesses section, a comparison with a continual few-shot-learning method would have been nice to get the full picture. This is definitely not required for accepting the paper but in case of rejection, I strongly recommend to add this comparison."
163	DisQ: Disentangling Quantitative MRI Mapping of the Heart	It would be better to add some discussion or metrics to the concerns of disentangelement described above.	"In the abstract, I would suggest mentioning the proposed method is a pair-wise registration method.
In the abstract and introduction, the authors affirm that modern learning-based registration methods also fail on sequences with highly varied contrast, whereas the tested vanilla Voxelmorph did reduce the observed precision and existing work on deep learning-based methods for motion correction in T1 mapping [[1, 2]] also presented promising results. Unless proven otherwise in the study, I would suggest changing the tone on the limitations of the learning-based registration methods.
The units of the plot 1(b) should be shown.
The literature review did mention the classic motion correction method [22] and a robust PCA-based method [7, 21], but the cited learning-based methods [1, 16, 17, 18] did not address the issue of motion artefacts in myocardial T1 mapping as in [[1, 2]]. I would suggest opting for a more conservative tone as this issue has already been addressed. The useful contribution of the paper is in the disentanglement framework.
The population of the dataset of MOLLI acquisitions should be explained at least in terms of number of subjects, centres, cardiovascular conditions, number of acquisitions with pre and post-contrast, and the ethical clearance.
The implementation details should include the amount of computing time at least in a GPU.
The selection of t = 5 is not properly justified, the results with another selection are not presented in the paper, only mentioned that it was not sensitive. In case the selection of t where the T1-weighted has the lowest contrast still achieved the same performance, it would be a missed benefit of the model that is worth describing.
In Results, it should be specified that these are the results from the test set.
Table 1 may have a typo in ""Ori"", which was previously described as ""Org"".
Table 1 may be improved by including the execution time in each method. This may prove the efficiency of the proposed method compared to ""Groupwise"".
Further brief clarification on the implementation of ""Morph"" may be needed. I assumed it was the same Voxelmorph with the input pair as is.
In Results, I would update ""significantly"" by ""substantially"", as statistical significan was never quantified.
The presented results showed an improved performance in terms of precision, measured as the fitting parameters variability, but not necessarily accuracy, which may be validated in phantom experiments, as stated by the authors.
In Qualitative analysis, it should be mentioned it is a case study as one case may not me representative for all the test set.
In conclusions, to support the statement that the method is generic, further evaluation may be required, i.e., evaluating the performance in pre and post-contrast separately, in an independent dataset, etc.
It may be worth mentioning future studies will involve development or validation in extended datasets.
I would suggest ordering the references by their appearance.

[[1]] Arava, D., Masarwy, M., Khawaled, S. and Freiman, M., 2021, November. Deep-Learning based Motion Correction for Myocardial T1 Mapping. In 2021 IEEE International Conference on Microwaves, Antennas, Communications and Electronic Systems (COMCAS) (pp. 55-59). IEEE.
[[2]] Gonzales, R.A., Zhang, Q., Papiez, B.W., Werys, K., Lukaschuk, E., Popescu, I.A., Burrage, M.K., Shanmuganathan, M., Ferreira, V.M. and Piechnik, S.K., 2021. MOCOnet: Robust Motion Correction of Cardiovascular Magnetic Resonance T1 Mapping Using Convolutional Neural Networks. Frontiers in Cardiovascular Medicine, 8."	"This paper addresses the issue of improving cardiac quantitative MRI (qMRI) such as T1 mapping by proposing an image disentanglement method, called DisQ (Disentangling Quantitative MRI), to discompose cardiac qMRI images into their anatomical representation and contrast representation in the latent space. The idea is new, but the medical motivation is not convincing. Throughout the paper, the authors used the term ""cardiac qMRI"", but the shown images contain not only the heart but also other organs or tissues, so that it was very difficult to precisely assess the improvement of cardiac T1 map. For ex., it is difficult to assess real interest brought by the proposed method with respect to the original T1 map (Fig. 4) on the myocardium.
From a medical-imaging point of view, the ""contrast"" meant by the authors is not really a contrast problem. Instead, it is a signal loss problem. In this sense, such understanding of the problem may call into question the basis of the proposed method.
Another important issue in this kind of methods is the sensitivity to noise. This point was not addressed in the paper.
Some typos:

by substituted Lanatomy with: substituting
one common factors: factor
will be present: presented"
164	Distilling Knowledge from Topological Representations for Pathological Complete Response Prediction	"""distill the topological priors (i.e., Betti curve)"" Are these really
priors?
I do not understand why the authors need to write out Eqn. 2. It says
L_{RKD} = L_{R}, couldn't that have been easierly achieved with just
the statement that ""L_{RKD} is the cross entropy loss between the
softmax functions logits of the teacher and student model."" or
something similar?
What is ""we use the middle half slices""? I will assume it is slices
15-45 of the 60 available slices, is this correct? The authors should
clarify in the text.
Figure 3, currently, confuses me. Subfigure (a) shows, I think, the
3D 0-cycle, 1-cycle, and 2-cycle Betti curves. If this is the case
then these images should be captioned or be annotated (a1), (a2) and
(a3), with the captioning explaining things.
Now subfigures (b)-(d) show different filtration levels for the Betti
curves. But I do not know if this is just for two subjects one pCR and
one non-pCR? Or multiple subjects?
It bothers me that the scales of the x & y axes are completely
different. I think the x-scale should be the same in all three, even
if it only goes up to 50 filtration steps. I think the y-axis might be
better as a percentage rather than a total count. Just an opinion.
What is absolutely driving me crazy, and I only can only complain
about this because the authors provide the total counts, is the totals
for the 0-cycles and 1-cycles. If I have N 0-cycles, I can have at
most ((N * N-1) / 2) 1-cycles, because math. From Fig. 3(b) at
filtration step 15, there appears to be less than 100 0-cycles, maybe
below 50 0-cycles. From Fig. 3(c) there are north of 20,000 1-cycles.
(This is the case for both pCRand non-pCR.) I cannot square these two
numbers.
Can the authors explain the discrepency? Or am I on the wrong
wavelength.
Grammar
""which has been shown high"" ->
""which has shown high"" OR
""which has been shown to have high""
""implemented by the PyTorch"" ->
""implemented within the PyTorch"" OR
""implemented using the PyTorch"""	See the weakness section.	As mentioned in section 5, the authors should clearly determine the time complexity of the Betti curve over the previous ones. Also, they should address why they did not include the mentioned deep learning-based approaches using PH in their work. Those methods are more deep learning-friendly. Betti curves are not inherently feature-ready to use for classification/regression purposes. It is strongly recommended to better explain and illustrate the cubical complex filtration and show the appearance-disappearance of homology groups (when a node, cube, or square appears and dies).
165	Domain Adaptive Mitochondria Segmentation via Enforcing Inter-Section Consistency	"1)Please check the performance of the  NoAdapt for adaptation from MitoEM-R to MitoEM-H in Table 2.
2)It seem less informative to conduct ablation results onadaptation from VNC III toLucchi (Subset1). 
3) Discussions about the limitation of the study and future work are recomended. In general, the authors should emphasise more the real benefits of the methods found and give some general points and suggestions to authors taking these fields into account."	"Please avoid exaggerated language (""for the first time"", ""as a resuce"")
the term ""gaps"" is not clear at first. Please provide a better explanation early in the paper.
The evaluation would be better done object-wise and not segmentation-wise as has been done by Kreshuk et al, 2014; Dorkenwald et al, 2017 and others. Object-wise evaluations better reflect the performance as it relates to the downstream use-case
Please use a different color combination in Fig 2 to allow red-green blind readers to parse this figure.
The dataset pairings in the evaluation minimizes differences between paired datasets reducing the information that can be cleaned from it. In the interest of better evaluations please consider different pairings such as MitoEM-R + VNC III.
The work by Januzweski et al, 2018 (CycleGANs) addresses a similar problem for neuron segmentations and should be at least mentioned."	"It seems to me that there are a few inaccurate descriptions about UDA:
1.1. In the introduction section on Page 2 it is ambiguous and misleading to describe some of the unsupervised domain adaptation (UDA) approaches as ""... aligning segmentation results ... to make the prediction of mitochondria on the target volume similar with that on the source volume"". However, with UDA it is not the exact prediction of the image volumes from the two domains that are made ""similar"", but the distribution of features (or selected summary statistics of the features) extracted from the two domains with the domain adapted model. Though this alignment can be achieved via various methods, including aligning the output space, in the end we just need the adapted segmentation model that extracts aligned features for both domains. Could the authors rephrase and clarify the description? 
1.2 Related to 1.1, on Page 4 Methodology section under ""Prediction Consistency"": ""... we enforce the target predictions to be similar with the source predictions ..."" This description is confusing. I would suggest the authors rephrase it. For example, the described approach can be described as ""enforcing the distribution of target layout to be similar with the distribution of source layout"". 
1.3 In the ""related works"" section on Page 2, the description of UDA literature for EM applications is not accurate: The ""pseudo label-based"" approaches also seek to align the features of two domains in order to learn domain-invariant features, despite via entropy-based self-supervised training. Thus such definition overlaps with the authors' definition of ""domain alignment-based"" UDA approaches.  It is therefore confusing to categorize UDA approaches into ""pseudo label-based"" and ""domain alignment-based"".
Looking at Supplementary Fig 1, the authors designed two separate segmentation encoder branches for two image sections from the same volume with a certain z-step, respectively. Separate decoder branches are also depicted in this image.
2.1.  It is misleading in the main manuscript that these two separate segmentation decoders (including the classifier layer for segmentation) are labeled with the same color and denoted the same as ""Seg. Decoder"". A related concern is that the segmentation discriminators for each of these two decoders are also labeled the same in Fig1 and denoted the same in the loss term (Equation 2). 
2.2. There is no justification of why adopt two separate decoder braches. Since the layout of the segmentation masks between the two sections should be from the same distribution, it seems that one shared decoder (as well as a shared discriminator) can serve the purpose and the additional decoder complicates the model. I would suggest that the authors add their justification for such a design. 
2.3.Which exact network modules are used for inference? Can any of the encoder and decoder branch be used for inference?
""inter-section gap"" belongs to intra-domain gap, which has been studied in multiple reports (such as the baseline used in this manuscript DA-VSN and Pan et al ""Unsupervised intra-domain adaptation for semantic segmentation through self-supervision"" CPVR 2020). Considering the framework includes both inter-domain alignment and intra-domain alignment, I would suggest that the authors provide such elaboration/clarification to make the design easier for readers to relate to domain adaptation.
UDA have great promises for the MICCAI community, but one open question is how to avoid the heavy dependency of the target domain annotations as validation set for UDA model selection. I would encourage authors to provide insights on this regard in their future work and help the community seek a way to make UDA more feasible/useful in practice, e.g. clinical applications.
Minor:
5.1. Source domain and segmentation map are both denoted as ""s"". Although the former was denoted by upper case and the latter by lower case, it can be confusing to the readers. Could another letter be used to denote segmentation maps?
5.2. In Table 2, ""AdaptSegNet[4]"" -> ""AdaptSegNet[21]""; DANN reference is not [21] and is missing?"
166	Domain Adaptive Nuclei Instance Segmentation and Classification via Category-aware Feature Alignment and Pseudo-labelling	"(1) There are details missing in the method section, which affects the overall quality of the paper because the readers have no idea how the method works. It is better to make sure that all necessary infomation is provided, either by clear description in the paper or adding references.
(2) About the pseudo labels, I am wondering if the authors generate pseudo labels for all images/patches no matter what the probablity scores are. It may be better to only use the pseudo labels that have high probability scores (i.e., close to 1 or 0), which are more trustable.
(3) It is interesting to see how the method works in the reverse direction, e.g. CRAG -> Dpath."	"In the experiments, why Dpath is always used as the source domain in both the main paper and the supplementary materials? It would be interesting to know the adaptation performance from the other direction, i.g., CRAG -> Dpath.
Not clear what is the difference between Baseline+CA+PL and the proposal.
It is typically not trivial to learn the loss weighting parameter in Eq. 2. Would it be possible to share what is the weighting parameter after training and if any special optimization technique is used."	"What's the main difference between the proposed method and Ref.12, so what's the highlight of your paper.
There are a lot of papers about Zero-shot or one-shot image segmentation or classification, why do not you directly use these techniques and select the UDA, which more complexity than others?
How about the failures?
There are some grammatical and sentence expression errors in this manuscript, pls revise them carefully."
167	Domain Specific Convolution and High Frequency Reconstruction based Unsupervised Domain Adaptation for Medical Image Segmentation	Have the authors evaluated the accuracy of the domain predictor on other unseen test data? The predictor was only trained on a limited unaugmented data. Can the segmentation network still produces reasonable segmentation result when the predictor fails to predict the domain id on some OOD data? I am also interested about the source domain segmentation performance as the DSC by design can support multi-domain segmentation.	Check weakness section	"The authors should discuss the issues I mentioned.
Comparing the results between w/o DA and Intra-Domain, the domain shift is not very severe. So I encourage the authors to evaluate their proposed method on other tasks with more severe domain shift.
I recommend 5-fold cross-validation to verify the effectiveness of the proposed method.
I encourage the authors to visualize the reconstructed image at the inference stage to prove the effectiveness of the proposed HFR module."
168	Domain-Adaptive 3D Medical Image Synthesis: An Efficient Unsupervised Approach	"Please use the term 'data augmentation' instead of 'data argumentation'.
Please double check the notations and equations in page 4. Some are not consistent or not fully explained."	"In Fig 3 I would suggest adding a column with one of the supervised DA results, which I expect look much better. Currently, it is difficult to evaluate how much is ""lost"" by going unsupervised.
I would train/test a network without domain shift (e.g. train and test on TCIA) in order to get a feel of the best possible synthesis. This would provide an additional upper (upper) bound.
Fig 3. shows that even after DA, the synthesized images can be quite unusable. The authors may want to acknowledge this and comment on it: what's missing to get networks that are truly domain agnostic?
In the 2D VAE, I don't see why the fact that batch order follows the slice order should have an impact. I imagine that the VAE could be trained on a subset of shuffled slices (even from different subjects). What matters is that the concatenated code for a full volume (in a single batch, or split over minibatches) is computed during fine-tuning.
I did not understand the paragraph about ""impact of the amount of volumes"" at all. What does ""the first continuing training batch in the UDA process contributes more to the results"" mean?
The authors stop short of claiming that they provide a solution to unsupervised domain adaptation, and merely claim to ""explore domain adaptation for medical image-to-image synthesis models"". I completely agree that some problems are hard and stay unsolved, but in this case, a better paper would evaluate different unsupervised approach and discuss the factors that make the problem still unsolved. Is the VAE just not a good distribution encoder? Are there not enough training examples for the VAE? Are there more than intensity differences between the two domains (e.g., different pathologies, different anatomies) that makes matching distributions not a good surrogate form of supervision?"	"On major weaknesses:
M.W. 1
(a) From my practice, it is a typical approach to filter out such cases with missing modalities when creating a dataset, and in practice such cases are not rare. But the authors need to properly describe that point (maybe the description could be find in literature).
(b) Moreover, I have seen few works, that address the problem of missing modalities and use a synthetic experimental setup on BraTS dataset, stochasticaly removing modalities. Such work could enhance the authors message and method motivation. Unfortunately, I could not recover the papers due to limited time, but I believe this hit would encourage the authors to find these works and strengthen their message further.
Other comments:

It seems like contribution (1) is enough, and (2) and (3) are the implementation and evaluation details, respectively. The authors could formulate their contribution as a plain paragraph with one strong message (1) and supporting details (2) and (3).

Fig. 2 floats, no reference from the text is given. Besides, Fig. 2 is self-explanatory, but it still would be better to link it with text. What is the floating ""n"" in the caption?

major comment Why do not the authors train 3D VAE in the same fashion on small patches? As far as I understand, the only reason to discard 3D case is diminishing difference in distribution with the increase in size (e.g., equalizing anatomical structure). But reducing 3D images to small patches also solves the problem of equalizing distributions.
Moreover, the same procedure of learning structured (along one of the axis) latent representation for 2D images, as in [16], could be applied to learn a structured representation for 3D images, switching from 1-axis structuring to 3-axis structuring.
In my opinion, the authors should develop 3D method in depth as well as they do for 2D. It might underperform due to the lack of finetuning, as 2D case has.

Work [3] has been published as [https://dl.acm.org/doi/abs/10.5555/3304415.3304514], thus its citation should be replaced with the appropriate form.

In Sec. 2, par. 2D s-VAE for modeling 3D distribution, it would be more clear to use word ""call"" instead of ""nickname"".

How do the authors use the order of slices in their method? It seems like the order does not impact the training procedure...

Why do we need regularization of distribution to N(0, 1) with KL-divergence loss (Eq. 1)? It seems like the task does not require the exact form of distribution.

N(0, 1) is a multi-dimensional distribution, so the authors should replace 0 and 1 with the zero vector and ""identity"" matrix, respectively.

Why the authors use L2 loss to train a VAE (Eq. 1), while training a CNN for synthesis with L1 loss? In both cases, the task is the same -- image generation -- thus, this choice is unmotivated. Also, super resolution reviews (e.g., [https://arxiv.org/pdf/1902.06068.pdf]) indicate that L1 is perceptually a better choice. The authors might use it to motivate their decision.

In Tab. 1, the authors might indicate the unavailability of labels (e.g., in row 2, col 2) with a specific symbol (e.g., *) to enhance readability.

The authors should name their figures in Supplementary Materials starting from 5. Providing the explicit links to them from the text (e.g., backbone in Fig. 5) would also increase the readability. Hyperlinks between two files would be unclickable, but it still a visual improvement.

The authors could additionally report SSIM and PSNR for the ground truth (using the original modalities) as the upper-bound.

In Sec. 4.2, the authors could describe the procedure of [16] in few lines, so the paper becomes self-containing. Clarification ""sampling infinite number of 2D slices"" seems to be misleading.

Describing the results in Fig. 4 (b), the authors should specify the domain (target or source) of volumes that they vary. (Fig. 4 (b) caption indicates the target one.)

The use of term ""batch"" in Sec. 4.2 diverge from term ""iteration"" in Sec. 3."
169	Domain-Prior-Induced Structural MRI Adaptation for Clinical Progression Prediction of Subjective Cognitive Decline	"The authors argue that this work could be among the first attempts employing domain adaptation for SCD progression prediction. However, many existing domain adaptation methods can also apply to the same task. In this regard, it needs to thoroughly compare the existing domain adaptation methods in the literature.
In Eq. (2), the denominator should be $m\times n\times k$.
It is unclear why is the tensor $A$ interpreted as an attention map? It is just a rescaled activation map with the averaged value of the small volumes. Note that large activation values don't necessarily contribute to the task-related important features.
What is the rationale for using entropy loss $L_{E}$ over the bottom branch with unlabeled target data? According to the description, it doesn't even participate in the back-propagation.
In the test phase, the top brain model is used. This case assumes that after training, the model learned to map the samples of sSCD and pSCD to CN and AD, respectively. However, to this reviewer's understanding, no mechanism makes such a relation in the proposed framework."	"(1) The authors should give details of parameter settings of the proposed model and the competing models.
(2) The author should clearly show how the experimental data is divided into training, validation and test sets.
(3) Comparison with more transfer learning methods should be given."	Recommend the authors to compare attention maps obtained from the implemented attention module with other explainable AI method such as Grad-CAM - would be very interesting to see those results!
170	DOMINO: Domain-aware Model Calibration in Medical Image Segmentation	"Experimental and Results

The 'Ground Truth' section, it is mentioned that 11 tissue types are being used. These 11 tissue types should be listed here as well. SOme were listed, however, all were not. All 11 do appear in the figures, but they should all also appear in the text.

In the same section as above, the semi-automated segmentation routine used by the trained staff member should also be mentioned here. This helps to add to reproducibility of the pipeline, especially for training.

In the 'Evaluation Metrics' section, when the author says 'brain segmentation', I think the authors meant to write 'head segmentation'"	"Minor points

Introduction: ""interpretability"" is not really addressed, consider removing this term.
The way to compute the hierarchical class-based W matrix is unclear. The authors mention "" following this formula"", but I could not find any.
Why is S set to 3? What is the effect of changing this parameter?
Section 3.3. Unclear what the authors mean by ""due to its high variability between individuals"". All tissues would vary between individuals, wouldn't they?"	
171	Double-Uncertainty Guided Spatial and Temporal Consistency Regularization Weighting for Learning-based Abdominal Registration	This is a well written manuscript that introduces a novel formulation of dynamic regularization weighting, exploiting temporal information across the training steps and reports the state-of-the-art registration results for the 10 abdominal CT-MRI pairs. The main drawback is that the evaluation is missing an analysis of statistical significance of reported differences in performance between methods, which would allow to make firm conclusions.	"The average deformation fields and deformed images of N stochastic forward passes on the teacher model with random dropout were used to define the uncertainty. How to guarantee the mean field or the deformed image consistent with the ground truth field or image in the deformable registration? It would be helpful to discuss the mean field or images to justify the uncertainty computation.
The thresholds \tau_1 and \tau_2 are important to update the regularization weight and the appearance consistency terms. We noticed that \tau_1 is ten times of \tau_2 in experiments. Whether the thresholds were set empirically?
In Fig. 3, \lambda_{\phi} and \lambda _c seem to converge in the training process. How about the backbone network, such as the VM, used the converged weights? Variant II used fixed weights in the ablation study, while the selected values are not consistent with Fig. 3 (b, d).
It would be helpful to describe the loss functions, such as the temporal consistency regularization L_c."	Some of the parameter settings seem fairly arbitrary, for instance thresholds for focusing on the most uncertain predictions, and would necessitate further study; are results robust to these choices?  I also don't know what the baseline runtime is for these procedures to compare the newly proposed methods to, just that they are faster.
172	DRGen: Domain Generalization in Diabetic Retinopathy Classification	It would be great to at least speculate why is the performance so uneven across the dataset. Even better to conduct experiment to investigate this further. For example the dataset with degraded performance is the smallest one (with no grade 4). It should be easy to check whether the relative size of a dataset  or its imbalance are linked to their performance improvement, or lack thereof.	"The motivation to develop such a regularization term should be described in detail.
The theoretical guarantee of the influence of regularization term on the model training process should be presented.
The comparison experiments are not enough to prove the superiority of the proposed method."	"Table2: would be good to keep absolute numbers only in the total images column and express the remaining columns as percentage. In this way the comparison of class distribution is easier for the reader.
page5: coefficient gamma is not precisely defined, a precise definition would help the reader to avoid continuing comparison with the related work that present Fishr regularization.
page5: ""We adapted Fishr [24] loss to enforce invariance based on the difference in co- variance matrices as represented in equation 2"" I find the same equation in Fishr paper (eq. 4) what has been adapted?
Table5: Table 3 and 5 should use the same labels, i.e. instead of column Dataset Name, Testing Dataset
Table4: as above, the column Accuracy is misleading, should be average accuracy.
page8: in the discussion no possible explanation is given to the difference in performance Fishr - proposed method for Messindor. why has the proposed method for Messindor the lowest accuracy, if it was for Fishr the dataset with larger gains?
page8: in the discussion it should be given more importance to give possible explanations on why the improvement vs. Fishr is measured. I can only find ""This is can be attributed to seeking a flatter minima empirically"", this should be explained in more detail in my opinion to make this work useful to the reader."
173	DS3-Net: Difficulty-perceived Common-to-T1ce Semi-Supervised Multimodal MRI Synthesis Network	Good.	It's easy to be reproduced and this is very nice work. But the modality T1ce is not popular in the general diagnose. More details of T1ce imaging should be introduced for authors. Otherwise, the authors only gave some values of lamda parameters in the loss function. More tests of different values of lamda parameters should be discussed for other readers and users.	"The authors should add more ablation study to well characterize the merits of the proposed method. Especially, the constant in Eq.(2) that is set to 0.2 is not well explained, since it actually affects the weighting in the remaining loss terms. Besides, the hyperparameters for most of loss terms are set by 100, 1, 1, which is also empirically due to the large value range.
Addressing these concerns about DS^3-Net could further improve the  submission.
I would like to raise the score if these critical concerns could be well solved."
174	DSP-Net: Deeply-Supervised Pseudo-Siamese Network for Dynamic Angiographic Image Matching	The authors proposed a novel framework DSP-Net to automatically match the intra-operative X-ray fluoroscopic images to the dynamic angiographic images, which can provide doctors with dynamic reference images in PCI. The writting and organization of this paper is good. I suggest the authors to add the time cost of the proposed method for this matching task.	"P1 : References throughout the paper are marked in a confusing order in the text.
P2 line4-8: The sentences are too long and not easy for the reader to read and understand.
P2 line3 from last: Lack of theoretical basis. Can you add a recent paper with similar content for illustration?
P3 Fig.2.: Does 'gallery' have any special meaning? Can it be replaced by 'datasets'?
P3 line2: Can you give a simple explanation for this?
P3 line5: Can you add a relevant comparative test to confirm this benefit?
P3 2.1 line7: Can you give specific comparative tests or add a paper with corresponding conclusions?
P6 3.1 line6 74: The proportion of test sets is relatively small."	"As the structure outputs a binary number, the authors are encouraged compare the intuition compared with GAN.
More methods compared with non-rigid registration methids."
175	DSR: Direct Simultaneous Registration for Multiple 3D Images	"The mathematical proof in 2.2. looks elegant; however the outcome could be put in context better. To me it sounds intuitive that there is no dependency on the panorama intensities, since they are also reconstructed with a least-squares assumption, hence they will amount to the average value of all overlapping input volumes. Having a gradient iteration that is using the grid of that panorama image but not the intensities is nice - and the key idea of this paper it seems.
Please provide implementation details and run-times such that readers know whether this is only a mathematical feat or something useful for their medical image analysis problem.
The provided presentation video is very nice!"	"How are these panoramic image predefined even though the solution does not depend on this?
Other 3D CT or MRI cases would be very relevant examples to see if this can be applied to other imaging
Are the chosen transformations of +/-15 pixels and +/- 12 degrees realistic range. How did the authors come to these values?
Could authors include some samples of panoramic images (predefined) that they have taken and how these look after registration?
What is the estimated time for the proposed SE(3) pose optimisation?
Multimodality version or changing pixel intensities during simulation can be interesting to include especially to validate that the method is not dependant on intensity variation as claimed."	"The state of the art is presented simply and briefly: from direct pairwise registration (based on features or intensities) to multiple 3D volume registration in order to emphasize the main contribution in the field of bundle adjustment in registering an image collection. The contribution is conducted in two steps as explained in section 3: DBA based on panoramic images, which consists of an original way to solve the BA, and then the proof that this DBA, thanks to Gauss-Newton optimization, is independent of the image intensities. The contribution is straightforward but clearly presented and the demonstration of the independence to intensity is convincing enough. 
The experiments were realized on synthetic data (5 simulated sequences of 3D grey heart images) and real data (46 TEE, Transesophageal Echocardiography images). The data amount tested is quite limited, however, regarding the real data, this is probably due to the difficulty of collecting this kind of data. 
The proposed method is compared to four methods: the pairwise method, the Lie normalization method, the sequential method and the APE method.  This comparison seems relatively fair."
176	Dual-Branch Squeeze-Fusion-Excitation Module for Cross-Modality Registration of Cardiac SPECT and CT	"1) It is not mentioned how the ground truth transformations are generated. Are the SPECT and u-maps aligned manually? How are the ground truth transformations validated?
2) It would be helpful to add the original size and voxel spacing of CT images.
3) It would be nice to clarify (a few sentences) how the proposed method can be applied to a real case scenario, especially when CT and SPECT images are taken with different size and voxel spacing.
4) Is there any intensity augmentation used during the training? If not, it would be nice to mention that explicitly.
5) The squeeze excitation module has been used in several DL-based registration publications but not in the context of multi-modality registration. It would be relevant to cite a few of those. (not necessary for this conference paper)"	It would be great to clearly state that the paper focuses on rigid image registration and not deformable image registrations? Could there be potential deformations as well in case of SPECT vs. CT that might needed to be modeled. Also, authors should compare their method with at least one approach that specifically focuses on feature fusion.	"This is a very good submission with enough technical contributions and strong experimental validations. However, there are some minor problems need to be solved:
-- It is better to put subsection 2.1. Dataset and Preprocessing and subsection 2.4 Implementation Details to section 3. Results. It will be more concise to describe only the method design in the section 2. Methods.
-- Please enrich the caption of the Fig.1. and 2.
-- The difference in visualisation results in Fig.3. is not straightforward to see. Please use some signs or zoom-in window to highlight the different regions. Also enrich the caption would help to understand this figure too.
-- There is a third kind of registration encoder, which use a weight-shared encoder (means one encoder) to extract CNN from the moving and fixed images. I would suggest to discuss this third kind in the literature review part too.
[1]. Liu, L., and et al.: Contrastive registration for unsupervised medical image segmentation."
177	Dual-Distribution Discrepancy for Anomaly Detection in Chest X-Rays	The detailed suggestions are integrated to the limitation session: 1) the definition of the unlabeled data 2) the question about the training cases in model A 3) the detailed explanation of the experiments 3) concern about the robustness of this model.	"The full writing and abbreviations in the thesis should be consistent with each other. There are abbreviations in the front and try to use abbreviations in the back. The full writing and abbreviations should not be repeated.
How K was chosen, please explain.
There are some spelling mistakes, please note, such as fisrt time.
Regarding the selection of training data sets, when comparing with other optimal methods, use data sets that are common to other methods for comparison. Otherwise, the conclusion drawn is very likely to be overfitting. Please find the datasets used by the other best methods listed in the article, and do experiments to compare the results.
The code used in this paper needs to be open-sourced to demonstrate the reproducibility of the method, or to provide relevant proofs."	The paper is well written and novelty can be deemed as sufficient. There are a few issues can be addressed during rebuttal.
178	Dual-graph Learning Convolutional Networks for Interpretable Alzheimer's Disease Diagnosis	This paper proposed a new method to classify AD-related groups and outperformed than other methods. It's a good idea to provide more neuroimage information to make the interpretable method more substantial in Fig. 1.	"Please define abbreviations everywhere in the text wherever they are being used first.
What is n and sigma in equation 1. Similarly, at others places variables are being used without defining them.
How the kNN being used for graph generation? More detail is need in this context.
Graphs are being updated after each and every layer. What does this signify? Practically, graph is being formed on an initial data which is than carried forward to all the layers in GCN.
Context of equ. 4 is not clear. Please explain. How the lambda_1 and lambda_2 parameters are being tuned?
How the size of architecture is being defined? Will A' and S' actually correspond to identifying interoperability?
Did the model convergence? It seems very less samples being considered in each class. Comparison on other public dataset with large samples is required in order to check generalization of framework."	see above
179	Dual-HINet: Dual Hierarchical Integration Network of Multigraphs for Connectional Brain Template Learning	"For the hierarchical multigraph clustering, it will be helpful to visualize the assignment matrix to see if the learned clusters are biological meaningful.
How does the clustering loss L_d affect the performance? It is better to provide an ablation comparison regarding this.
Both sMRI and fMRI are available in ABIDE dataset, not sure why only sMRI was used. Morphological connectivity is usually computed as region-wise correlation of multiple morphological features, instead of using one single feature seperately. Learn CBT from multi-modal data will be more promising.
Further evaluation are needed to see if the generated CBT can better capture inter-subject differences and the associations between brain connectivity and phenotypes."	"Contents in the last paragraph of Section 1, the start of Section 2, and the rest parts are repetitive. It is like the authors have introduced their framework based on Fig.1 by three times, and some contents are overlapped with each other. It is suggested that you provide only a brief introduction of your method that focuses on the main idea in Section 1, while in Section 2 focuses on the detailed descriptions of each major step. Similar contents in the three parts of the paper should be reduced. 
In Fig.1, it is suggested to exchange the location of subgraphs E and F, so that the order of these subgraphs can be organized in a more natural way.
In the second line of the second paragraph in Section 3, what does that ""AD"" stand for? Should that be ""ASD"" instead?
It is also suggested to provide table to describe the contents in Fig.2 instead of figure, which can provide more accurate information between different configurations and the DGN. It is also unknown why they choose ABIDE I for validate their performance, since it is specifically collected for ASD disease. As this method is more like a general brain network analysis method, using only one dataset might not be sufficient to fully validate it."	Please refer to the weaknesses for details.
180	DuDoCAF: Dual-Domain Cross-Attention Fusion with Recurrent Transformer for Fast Multi-contrast MR Imaging	"It is not clear to me how to extend the proposed method to complex-valued and multi-coil MRI, which is more practical in real MRI reconstruction scenarios. (c.f. main weaknesses of this paper.)
Also, authors claim the proposed method is evaluated on fastMRI dataset, which is somehow misleading. Usually the fastMRI dataset is reference  to the raw complexed-valued multi-coil data, while actually only simulated data are used in this manuscript.
Extending the description of  residual swin transformer block (RSTB) would make this method more clear. For example, details like strategy patch merging and W-MSA/SW-MSA  in Section 2.3.
Is the TGR sub-module necessary in the proposed CAF module? It seems that the target of multi-contrast MRI reconstruction is to borrow information from reference image to target image, why the proposed method needs a target guided reference (TGR) sub-module?
I wonder why authors still need CNN layers instead of powerful non-local Transformer layers at the end of RRT and CAF modules in Fig. 1? In the context of Transformer, element wise addition and linear layers are preferred for image information fusion and feature extraction.
Minor Comments
Inconsistent under-sampling mask ($k_u$, Cartesian mask) and artifacts (Radial mask) in intermediate reconstruction result ($\tilde{x}_{u_1}$) in Fig. 1.
Content and contribution of section 2.3 is too weak for a subsection. This subsection should be further extended or simply merged/scattered into other sections."	"1)The authors should provide the analysis on the computational complexity of the proposed method.
2)The authors need to explain whether the proposed method has consistent performance for different modalities. 
3)The authors need to give clear explanations on whether Individual health status affects the performance of the proposed method. 
4)Font size of Fig. 1 is too small.
5)There are a few typos and grammar errors, such as ""the attention outputs is introduced by using one contrast image "", ""The TGR runs in the same way and thus form a"", "". In i-th image domain restoration block"", ""groups in out network"" and ""which means that learn the interaction between two different contrasts step by step is optimal""."	"This paper proposes a dual domain deep learning framework for MRI multi-contrast super-resolution. According to the results, the proposed method can generate superior results when compared with some other methods. The paper is well organized and has presented enough figures and tables to support authors' ideas. However, there are some major concerns.
In k-space CAF, feature maps are cropped into smaller patches and then embedded and input into RGT and TGR. Mathematically, each data point in k-space will contribute to every pixel in the image domain after Fourier transform. Please example the reason for cropping k-space.
It is very interesting to know the improvement of each recurrent block. Please consider replacing Fig, 3 to show results from each recurrent block.
In the ablation study, please provide more details on the network structure design for each experiment show in Table 2 as the statement of w/o CAF, w/o DD in the paper is confusing."
181	Dynamic Bank Learning for Semi-supervised Federated Image Diagnosis with Class Imbalance	"Some figures and equations are not well explained,like:
1). In Fig.1, it is not clear what is the \pi_1\cdots \pi_k. Additionally, it is better to simply  explain the overflow in the caption.
2). In Eq. 2, it is better to explain the function 1(\cdot).
It is better to simply analyze the reasons that why the proposed method has superior performance over the comparative methods in the experimental section.
The paper might miss some related works as follows:
[1] Laine, Samuli, and Timo Aila. ""Temporal ensembling for semi-supervised learning."" ICLR  (2017). 
[2] Shi, Xiaoshuang, et al. ""Graph temporal ensembling based semi-supervised convolutional neural network with noisy labels for histopathology image analysis."" Medical image analysis 60 (2020): 101624."	"According to the S_k of the experimental setting, the server side has a small number of class-balanced samples, which is a huge advantage compared to other semi-supervised FL algorithms in comparison. However, the proposed method only improves the training on the client side, without making full use of the server-side knowledge provided by the setup. Compared to the upper bound of performance, there is still a lot of room for improvement.

Author should introduce the analysis in Section 3.3 is performed on which dataset. Furthermore, accuracy is not suitable as a primary analytic metric due to the severe effects of imbalance, such as the large difference in numerical values between accuracy and F1.

The experimental results shown in Fig. 2 (d) are not intuitive. (1) When S_k=15, the standard deviation is reduced significantly compared to the adjacent values. What makes the method significantly improve the robustness at the selected S_k? (2) In addition, a larger S_k (i.e., more class-balanced samples on the server side) should not hurt the performance of FL, why is the performance apparently lower than 10 and 15 when S_k=20?

It seems the method requires the assumption of IID for data at server and clients. In other words, the method does not consider the data heterogeneity (shift on p(x)) among participants. In practice, due to the small number of samples on the server side, data heterogeneity will inevitably be involved.

Since the Dirichlet distribution is used to construct the imbalanced clients, authors should state the work is verified as a simulation of imbalanced scene to avoid potential ambiguity with ""real-world use"".

Does this work use the same threshold (h_m/h_c) for clients with different class distributions? If so, this may not be reasonable."	"When building the memory bank by Eq. 2, the threshold for the majority and minority classes need to be different. However, the class distribution may be unavailable and which class is the minority class may also be unknown.
When estimating the class prior for each sub-bank, it is assumed the class distribution of each sub-bank are not exactly the same. However, the sub-banks are split from the bank by random splitting, which may have the same class distribution.
If the data are distributed among client by a i.i.d. distribution, does the proposed method still outperform other methods?
The proposed methods require training on the server, but the baseline methods such as FedAvg do not. Could you please add this server-side training to FedAvg-FM and compare the results?
In Fig. 2 (e), it is interesting to see that with the increasing number of unlabeled clients, the accuracy of the proposed methods increase. Intuitively, with more clients, each client will have less number of samples and degrade the accuracy. Could you please explain more on this result?
Some related works on self-supervised federated learning [1][2] need to be discussed.
[1] Federated Contrastive Learning for Decentralized Unlabeled Medical Images, MICCAI 2021
[2] Federated Contrastive Learning for Volumetric Medical Image Segmentation, MICCAI 2021"
182	EchoCoTr: Estimation of the Left Ventricular Ejection Fraction from Spatiotemporal Echocardiography	"As said in the weaknesses, the applicative impact is in my opinion limited, given the performance of current segmentation and tracking methods. I also wonder if better impact would be reached by focusing on assessing the dynamics along the sequences, instead of trying to (slightly) improve the performance on a rather classic problem (estimating LVEF).
The network is supposed to select the most representative frames to estimate LVEF. I wonder how it behaves on cases with abnormal motion, and in particular little motion.
Writing could be revised on several aspects:

The Title and in particular ""spatiotemporal echocardiographic assessment"" may be revised to better fit what is actually proposed.
Abstract: the sentence ""However, according ..."" is rather vague and could be revised.
I have similar remarks for other parts of the paper: beginning of the Introduction, section on LVEF in the center of p.2
p.2: ""adopted"" should be ""adapted"""	"The database has been extensively used and now authors provide a new method and a good amount  of experiments,  that makes the paper strong and illustrative. I believe it can be relatively easy to reproduce and help for others to have access to your code and be able to compare the new ones.
Also, the contribution mentioned in the introduction is well written and gives a very good idea of the paper.  Perhaps it would be stronger if authors go deeper on the reasons why their method is better compared to the others and talk about specific disadvantages."	"It would be welcome to have a comparison of the different models' efficiency in Table 1. This may highlight one of the limitation of EchoCoTr or reinforce its usability in a clinical setup, where compute resources are limited. Efficiency could be estimated by looking at how much time it takes for each model to analyze all the videos from the Validation or Test set, or by using the standard FLOPs metric.
The data sampling (page 3, section 3.1)  looks well described, but EchoNet dynamic contains videos of arbitrary length. In the case where the video is longer than the clip covered by the sampling, how is the clip starting point selected ? And how do the authors handle the case where the source video is too short for the selected sampling method ?
The changes made to the UniFormer model are not very clear. The model is partially described at the end of page 4, but it would be welcome to know what differs from the UniFormer and what was taken as-is. If all the mentioned parameters are different from the ones used in the UniFormer paper, please clarify it.
Authors mention that their model can predict the LVEF with a good accuracy when it is given just one end-systolic and one end-diastolic frame. This is very interesting, but it should be stated that this is an important bias, as the model usually has to determine the position of these key frames."
183	EchoGNN: Explainable Ejection Fraction Estimation with Graph Neural Networks	"Minor style issues. For example, I would not start a sentence with a reference such as ""[18] did this and that"".
Typo: ""EF error and increase THE model's ability""
How could this method be used in longitudinal studies? e.g. progressive heart disease.
Table 1. Would it be possible to include running time?
Justify T_fixed = 64.
Rephrase ""necessitate the need"" (abstract)
What about other sources of bias / noise in the data, e.g. image protocol, intra-observer bias, etc.
The selection of the thresholds (unstated weights value and block width of 55) for ED/ES frame approximation is not explained (supp. Fig 3), but this selection will directly impact the aFDs. Can you please comment."	"Continuing my point from above. SInce the temporal relationship between the frames are learned using the Attention encoder + GNN, why do we still need the temporal relationship handled using 3D convolution in the video encoder? It would be conceptually cleaner/simpler if those are separated out. Would also increase the modularity/reproducibility.

Defining the edge/node relationships, why don't you write the equation for the last one (Ws) since that seems important?

One thing I was curious about was, for GCN, could you not leverage the sequential nature of the data and establish stronger edge relationships? Essentially bake in a stronger inductive bias. Perhaps you're already doing that?

Your model does have less parameters. But what about training time because of the 3d convolutions?

For table 1, what's the hypothesis on good performance of aFD on ED and not good on ES?

Another question I have is, EF is calculated using AP2 view US images as well. Do you think this work will translate well if used there as well?"	"In the supplementary material (Fig 2) it appears that coefficients for EF are low on the diagonal for 30%<EF<40% and 40%<EF<50%. Patients below 40% require medical care, and this data suggest that about a third would be misclassified as above 40%. There should be a discussion on this point in the main text. 
Generally, the Conclusion section is missing some detailed discussion on limitations and would benefit from a more specific outlook on future work. 
I would appreciate if the author could discuss as to why they think EchoGNN struggles with ES aFD in comparison to other methods. Also, are aFDs of 3+ frames not considered a poor outcome? Coming from a background of segmenting MR images, I view the temporal resolution of echo imaging superior and thought that the identification of ED and ES should be more accurate (especially when considering the closure and opening of the valves).
Small editing comments:

first line in Section 2: ConvoLutional Neural Networks"
184	Edge-oriented Point-cloud Transformer for 3D Intracranial Aneurysm Segmentation	"The overall presentation of the paper is great. I have some suggestions.
1: It would be great to provide the experimental results of the effectiveness of each proposed component.

Some experimental details are missing in the paper. For example, the details parameters of construction of the edge graph."	"Consider improving on the main weakness.
The last sentence of 3rd paragraph of Introduction says ""3DMedPT can still not perform well around the edge between vessels and aneurysms due to the less supervision and ambiguous features, where is extremely harmful for the clipping surgery process."" However, visualization of 3DMedPT results are not provided and its performance in segmenting edges are not clear.
The first sentence of section 2.2 ""Due to the ambiguous features generated from similar contexts, points around the edge are easily misclassified, which is harmful for the surgery process."" It is not enough to only say ""is harmful for the surgery process"" and it should be explained how it harms the surgery process."	Failure to address my concerns listed above might reduce my impression and marks. I am giving this result, considering the limited works in this domain and based on the expectation if these concerns could be satisfactorily addressed. Open source is also preferred by the community.
185	Effective Opportunistic Esophageal Cancer Screening using Noncontrast CT Imaging	"1) Clear description on the operation of global self-attention layer should given for better readability and reproducibility. 
2) The classification method should be described in details.
3) How to train the segmentation network should be described clearly."	"Minor comments:

It is unclear what the last 3 columns in Table 1 are; are these sensitivities? It seems Table 1 and Figure 3 are conveying the same information? It would be good to have Table 1 directly above Figure 3 in the manuscript, rather than be interspersed with Figure 2 which shows example output.
ROC curves should be plotted on square axes.
Figure 3 (ROC curve) needs error bars for the operating points (and preferably a shaded 95% confidence interval for entire curve)
It is unclear how the AI operating point was determined. This should have been done based on the training/validation results without taking into account the test set at all, i.e., a threshold for the output score should have been determined for the training/validation set and then applied to the test set. Please clarify.
The authors talk about detection and classification, but the performance evaluation is framed entirely as a classification problem (ROC, sensitivity, specificity) without localization or FROC (free-response ROC). Localization is implicitly included through a cutoff for the Dice score (with the reference standard), but subsequent performance evaluation assigns a single label (cancer, benign, normal) to a scan. Looking at the example output of the method (Figure 2), it would be possible, e.g., to have more than one false-positive region in a scan marked as false-positive.   In true detection problems, FROC analysis is important to evaluate performance, especially the number of, and types of, false-positive marks for normal scans.
The overlap of 0.1 for the Dice score that determines a true-positive 'hit' seems to be very low; why so low?
The sub-analysis by cancer stage is interesting but lacks statistical power (and needs error estimates). The dataset needs to be better described in terms of stage and lesion sizes in the 'Datasets' section"	I think it was a good study, with relatively complete, sufficient experiments and discussions. Further refinement of the language and supplementary charts will help this study to be published in a good journal.
186	Efficient Bayesian Uncertainty Estimation for nnU-Net	"""Efficient"" in the title is not well supported by the results.
nnUnet has been powerful on both 3D and 2D data. However, only 3D data is evaluated here. 2D data segmentation should be included."	No special recommandation to make.	"A paper with limited novelty is fine, as long as it honestly claims its contributions. Do not overclaim.

An extensive ablation study or study on important factors can improve the contributions of this paper. For example, why cosine lr fails for nnU-Net training? How do the hyper-parameters like the number of checkpoints, the gamma parameter, or training epochs affect the quality of estimated uncertainty?"
187	Efficient Biomedical Instance Segmentation via Knowledge Distillation	It would be more convincing to validate the proposed method on some additional datasets	I'd be curious to see if the student network can be trained without the affinity loss L_aff. This would enable distillation of a large network pre-trained on private data without access to groundtruth.	The authors should emphasize on how significant the performance gap between the student and teacher networks. The authors do include a table in supplementary materials, however, no further discussion is provided.
188	Efficient population based hyperparameter scheduling for medical image segmentation	"it seems that from the set of hyperparameters that were used in Table 1, there are none that are related to the architecture of the models - for example the activation functions or number of layers, or filters. Could you explain why these specific hyperparameters were chosen ?
explain why when the number of workers are low, that you do not obtain better results than the default setting .
could you provide test results on a held-out test set for the optimized models vs default model. It seems you have only reported validation results and not test results. I'm interested to see how much of a test performance gain can be seen if the model was retrained from scratch using the ""optimized"" set of hyper parameters found using this technqiue"	"since the search space of the hyperparameters is restricted by a range around the default values, I wonder how this method compares to simple grid search around the default values. This could be discussed in the paper, by e.g. comparing to grid search.
the authors mention an average performance increase of 1-3%, but the actual performance on the 4 datasets is not mentioned in the paper. Figure 3 gives some hints of performance on the validation set, but it would be helpful for the reader to see the performance e.g. in a tabular form. Also a comparison with original PBT in tabular form would be desired.
Figure 3 is mentioned before Figure 2 in the text
on page 3 the authors claim that their method reduces the training cost to 3%-10% of original PBT. Could the authors elaborate how they estimated this amount of reduction?"	"*Why there is no pbt best worker from scratch (W=27) in UNet Lung in Figure 2?
*Add significance test to the results. Repeat the experiments to remove the randomness of the parameter searching.
*Evaluate it in more challenges."
189	Electron Microscope Image Registration using Laplacian Sharpening Transformer U-Net	This is a nice methodological framework, tested on EM challenge data and generally compared well against SOTA methods, which however were designed for different purposes. Results seem moderately, but not significantly better. Deformations were first simulated then recovered, which is suboptimal. I would have thought that serial or cine registration applications would have been more interesting to investigate.	"Typos & proposed corrections to be made:

Page 1: Lv et al. [12] proposed (Is it correct?)
Page 3: STN, please indicate that it means ""spatial transformer network"" and cite it. Also,
At the end of page 2, the authors indicate that they add a Laplacian sharpening skip connection but they do not justify why until the description of feature enhancements. To ease the reading, I think it would be nice to motivate why you integrate such accesorial step in the architecture.
Figure 2: could you elaborate a bit more on the Caption. For a matter of completeness it is recommended to explain the meaning of the parameters in the propose architecture (i.e., W, H, C and so on) and how did you set them up.
Did you train the STN? How?
What is the size of the kernel used for SSIM?

Comments:

Cascade approach: Is there any theoretical reason to not train the cascade approach end-to-end? Additionally, there will always be an error and a bias implicit to the trained architectures. How does it propagate in this cascade approach? Is there any reason to apply the cascade iteration only once? Would it make sense to apply it iteratively until there is no significant change in the output?
The authors use images of size 448x448 to train the model. How does this relate to the receptive field of the network? Additionally, images are resized before the training. Is there any specific reason to do so? Were the images also downsampled before analysing them with the proposed stare-of-the-art methods? A comment on this might be important as in Figure 2, it looks like the proposed network is able to better resolve fine details, compared with the UNet. This might be first, because the resolution of the images was different on each approach, or because the proposed method is able to fine tune the results. Is this a consequence of using the cascade approach?
How does the proposed method compare in terms of hyperparameters and memory consumption?"	"The authors don't specify how many of the 125 images were used for training, validation, and testing.

A reference to related work which also used a cascade of registration networks would be appropriate. Appropriate would be Quicksilver by Yang et al. 2017 and Recursive Cascaded Networks by Zhao et al. 2019, though both differ from the simpler cascaded approach utilized in this work. Personally, I think Recursive Cascaded Networks with the difference that weights are not being shared and learning is not end-to-end (which is fine!) seems an appropriate reference.

Given that after an initial alignment, the motivation for long-range dependencies is less profound, would it be more plausible to use a standard CNN registration model for the second stage of the cascade?"
190	Embedding Gradient-based Optimization in Image Registration Networks	The methodology and particularly the training process with two optimisation level requires more explanations and clarity.	"In Table 1, it is not clear if the reported values are average or median or whatever values.

In Table 1 and 2, what is meant with ""
J
< 0% ? The number of voxels with a Jacobian determinant smaller than 0% ? Or the percentage of those so ""
J
< 0  in % "" ?"	"The methods section should be revised to clarify the reasons that led to the formulation of the procedure as presented.
The role of the regularizer in the lower-level optimization should be better emphasized. What form does it take after learning, what task does it have in the overall registration, what is its contribution to the results.
It is unclear what segmentations are used for dice determination - no segmentations are mentioned in the 3d case.
The role of lower-level optimization, specifically focused on in Tab 2, should be prepared and at least motivated in the methods section.
Minor changes:
The abbreviations for the segmentations in the heart are not used again in the following and can be removed (ie LV, Myo,...)."
191	Embedding Human Brain Function via Transformer	"If the embedding size was 64,  all fMRI signals of whole brain were represented the 64 vectors, or the fMRI signal of one voxel were represented the 64 vectors? or others?
When predicting ADHD, how to divide the training, validation and test data,  I mean that if author use some brains as training data, and the others as test data, or any other manner?"	"(1) Transformer shows a more flexible and expressive ability compared with CNN, so I'd like to see its application in medical image analysis. This work utilizes transformer in a basic way: switch CNN modules with transformer, which is not impressive. But applying transformer to fMRI is a good topic. LSTM-based or RNN-based methods show a good prediction results on HCP tasks, the authors could explore them and add some of them as comparison methods.
(2) Test on all tasks, evaluate with not only accuracy but also precision, recall, f1 score. Statistical results could be tricky, so always provide as much information as possible.
(3) Fig2 is redundant, provide some results from comparison methods."	"Some more comments are provided in the following.

Have the authors tried to train simultaneously the embedding and the specific task (despite the stated motivation of not being task-specific) ?

In 3.2. Since the methods is in two stages (train the embedding, then train the brain state prediction), it is not clear what some parameters refer to in the implementation details. Details should be given for both trainings.
Also, is there no early-stopping on the validation set?

Figure 1, keep t either columns or rows in both the 2D signal matrixx and the learned embedding.

In 3.3. ""But the performance gain is significant"". It reads as if a statistical test was performed but I think it is not.

In 3.4. Interesting analysis of the correlation of individual digits with individual stimuli. It would also be good to report the specificity of these digits, as some may just respond to any stimulus, while others may be more specific? I would mention the limitations of the ""interpretation"", it does not interpret the internal behavior of the transformer, it is just a correlation analysis between extracted features and stimuli.

""general, comparable, and stereotyped space"" is repeated in introduction.
In 2.3. ""We fix the weights of pre-trained model"" -> the pre-trained model
In 3.2. ""the size of two FC"" -> of the two"
192	End-to-End cell recognition by point annotation	"1 Please conduct experiments on multiple datasets to verify the effectiveness of the proposed method.
2 Please conduct multiple trials on the same dataset to get statistical description of the results."	Overall it is a good paper, but some definitions are confusing. What is the definition of the preset anchor point is confusing. Several variables, e.x. x, y, y* are not explained. How does the network get the center point (x,y)?	"The dataset is only divided into training and test sets. Does the method employ the early stop training?
Some cells in IHC slides were not fully membrane stained. How to define whether such cells are PD-L1 positive or negative? What is the standard? example: 50% membrane stained as PD-L1 positive.
""Regression models"" mean which models? It should be clearly described in this section.
""7times7"" is a latex error? ""The overall framework of the proposed model is depicted in Fig. 3"". I suppose it should be Fig .1.
Does Time/s refer to the average time per sample? Seconds or minutes?
What does 'IC' mean in Table 2? The various abbreviations in the paper need to be explained clearly."
193	End-to-End Evidential-Efficient Net for Radiomics Analysis of Brain MRI to Predict Oncogene Expression and Overall Survival	Please provide additional details about the patient population used and whether any cases were excluded and for what reason(s). Also, clarify whether all MRI scans were acquired pre-surgery. Please also consider redoing the overall survival prediction task as a Kaplan Meier analysis.	"(1) The paper should be written carefully;
(2) The proposed algorithm should be introduced seriously;
(3) The experiments should be improved."	"Page2: it was mentioned that some studies recently highlighted the limitations of the studied models. It will be very interesting to briefly state some of this limitation and related such limitations to the reported results by other studies. For example, while some studies reported the prediction accuracy above 90 percent, the best performance of the challenge is less than 65 percent. The authors can connect such a significant difference between the performance with the limitation/challenges of the task.
Section 2.2: please add a reference for the ""aleatoric uncertainty""
Section3.1: I assume the cross validation was done a subject-wise as there are several slices extracted for each subject. Please state this in the text to avoid the confusions for the readers.
Section3.1: The description of the preprocessing steps seems to be valid only for the MGMT dataset in which 16 slices for each subject's scan from Axial view are extracted. While the BraTS dataset was fed to the feature extractor with 4 channel volumetric images. Please specify the preprocessing steps for these two dataset and clarify whether the EffNetV2-T backbone is a 3D or 2D model.
An important question is about the 16 number of extracted slices. How these slices were extracted? Please also clarify how the final classification metrics were clalculated for each subject if it was based on 16 extracted slices?
In general, this confusion over 2D or 3D data classification should be addressed to also avoid the inconsistency when compared against other external references.
As an important comment, when performing the classification tasks over the whole image instead of the segmented area, there is always the risk of feature learning from irrelevant texture/spatial information. As there have been quite many methods introduced to somehow visualize the learned features over the image space, it would of great interest to include this analyses and investigate whether the model was successful in capturing the features from the target regions or irrelevant regions?"
194	End-to-end Learning for Image-based Detection of Molecular Alterations in Digital Pathology	Table 2: it's not clear what the percentage is in the first column for each result in this table, it is not defined.	See above. Comparisons with other end-to-end pipeline is important to justify the proposed approach. Specifically, the following GitHub makes it easy to run these end-to-end pipelines: https://github.com/KatherLab/HIA.	Authors can emphasize more on the strength of k-Siamese CNN instead of on an end-to-end approach over a two-stage approach, since experimental results did not support their claim (k-Siam vs Seg-Siam).
195	End-to-end Multi-Slice-to-Volume Concurrent Registration and Multimodal Generation	"I have only very minor remarks:
You mention X-Ray 2D-3D registration in the same context as slice-to-volume registration, however it is something fundamentally different, due to the accumulated data along a projection geometry.
You refer to the supplementary material several times I believe; make sure this is really optional to understand the manuscript (possibly rearranging figures & text accordingly)."	"I have several major comments:

It was mentioned that all volumes were resampled to given shape. Is it connected also to the MR volumes? What about the interpolation artifacts when the resolution of MR (or USG/Angio in other applications) is much lower than the related CT volume? It seems that the proposed method is dedicated more to low-quality 3-D to higher-quality 3-D instead of slice-to-volume registration.
The reported registration time is about 2s. For registering 3-D volumes the reported time is understandable. However, the motivation behind the article is to allow real-time processing during the multi-slice to volume registration. Such a registration should be much faster to be useful during real-time applications.
The main contribution of the article is the concept of using the generated image to perform the unimodal registration instead of directly performing the multimodal registration. The concept is interesting, however, was already explored in other works related to 2-D or 3-D registration. I suggest to more carefully review the literature and cite appropriate references.
Even though the concept is interesting, the article is relatively chaotic and the method description is confusing. There are some minor language mistakes that should be corrected (basic grammar mistakes, can be captured by automatic text screening tools).
The source code is not referenced or prepared to be referenced. The paper is not reproducible."	"In Fig. 1, the naming of the intermediate step as ""CT-to-MR translation"" is somewhat misleading. Probably, ""MR-to-CT translation"" is more intuitive with respect to the proposed approach.

Fig. 2 shows a specific case where the proposed method fails. Please discuss why the method fails and how often it fails.

Eq. 2: ""i; f _i \neq 0"" should be "" i: f_i \neq 0"".

Section 2.2 begins with a description of the creation of a 3D volume for the 2D sCT layers. The information that the intervening layers are filled with 0s and that in fact all layers (and not only those for which 2D sCTs exist) are fed into the pipeline is given at the end. It would be helpful if this information was at the beginning of the paragraph.

Related work is a bit dated and not state of the art, e.g. p. 2 ""Traditional Deformable Image Registration (DIR) methods like SyN [1], Demons [23,24]..."" with reference to methods from the late 90s is probably a bit too traditional ;) .

The paper should be self-contained, definitions of losses and measures should be part of the paper and not in the appendix."
196	End-to-End Segmentation of Medical Images via Patch-wise Polygons Prediction	"*	Provide a section on k, s parameter selection for each benchmark dataset
*	Report average and standard deviations of the performance in every table over parameter choices if no clear strategy is available to fix k and s beforehand.
*	Improve the structure of the paper with the following:
    1. The first paragraph of the introduction states that a second type of output representation is provided, which seems not correct the final output is based on a rastered image, the latter being similar to classical approaches. Therefore, it must be clarified why a classical approach could not learn an equivalent mapping.
    2. The introduction mixes related work and methods, without really introducing the problem that is addressed. The sentence ""Active contour methods"" is unclear and seems misplaced.
    3. The caption of Fig. 1 is using lots of elements that are only introduced on page 4. Please reformulate.
    4. Section 2: ""In another contribution, an attention maps to each feature map in the encoder-decoder block..."" This sentence is incorrect.
    5. Section 2 contains an enumeration of segmentation approaches and lacks conclusions on what is lacking in the literature and how the proposed approach could solve current issues.
    6. In section 3, the output domain of f_1 is \mathbb{R}^{C x H/32 x W/32}, please justify why ""32"".
    7. Section 3 states that ""The decoder f_2 contains two upsampling blocks to obtain an output resolution of s = 8"", but s is a free parameter with various values used in Experiments. Please clarify.
*	Typos are present, please verify the entire document (e.g. ""patche"")"	"The multi-label segmention is common in medical images. It will be a more efficient technique to extend the method presented in the paper to multi-class tasks.
The contour methods would be effective if the segmentation of the object can be represented by one closed polyline. What if there are more than one polyline, for example,  a tire which should be represented by two circles."	"The representation of a polygon with k vertices is not clearly described. The output is 2k + 1 for each polygon, then what does 2k and 1 means? Does it also mean all the polygon have k vertices? What is the k value then?
Where does the map M come from? How to choose suitable size for map M?
There is a discussion on the advantage of polygons that it allows one to be rasterized at any resolution. Could you further explain this point? why is it important for image segmentation task?"
197	Enforcing connectivity of 3D linear structures using their 2D projections	"In the introduction, perhaps clarify that only in some cases indeed 2D annotations can be used to train 3D models - in many other applications, 2D projections of 3D volumes would not give sufficiently reliable information to enable annotations, but the proposed 2D connectivity loss would still work.
Is Fig 3 the brain dataset only? please clarify in the caption.
Would it be possible to include results for Perc and PHomo with parameters selected in a similar manner as for the proposed method?"	"While the paper generally features a good description of training parameters,some details remain unanswered, e.g. what the window size for L_TOPO calculation is.
Fig 3: which use case does this describe?
Table 1: does bold mean statistical significance? I hope yes, but please clarify in description."	"Do Perc and PHomo also work on the projection of 3D output in combination with the MSE loss? Otherwise, it will not be fair to compare. Authors should provide more details on the baseline models for reproducibility.
On page 3, the authors claim that ""continuity of 3D structures implies continuity of their 2D projections."" However, that is irrelevant. One should look for how discontinuity in 3D can be captured in optimal projections in 2D. It may require having more than three canonical projections.
What is b? It is not explained anywhere in the text.
Why does MSE in 3D perform worse than MSE in 2D for the Brain and neurons dataset?
Since the data is highly sparse and most of the time, the error is a false negative, did the author apply any weighting for the cross-entropy? Why didn't they consider Dice as a baseline since it handles mild class imbalance pretty well?
Why has no 3D metric been reported? Since the main task is in 3D, authors should consider evaluation in 3D."
198	Enhancing model generalization for substantia nigra segmentation using a test-time normalization-based method	"Authors need to improve and extend Experiments and compare their proposal with SOTA methods.
Authors need to improve Conclusions. They only summarized the paper.
Minor corrections:
In Page 2. deep learning technique have achieved -> deep learning techniques have achieved
In Page 2. Inspired by the work of Alice -> Inspired by the work of Le Berre et al. 
In Fig. 1 prposed -> proposed
In Section Methods squentially -> sequentially
In Section 4 dice -> Dice"	"This was a solid paper, there weren't too many things to comment on, except for a few suggestions...

It may be helpful, as far as reproducibility, to include metrics such as repetition time and echo time for the T2-weighted images collected in-house.

At the end of the methods section, the authors mention proposing a post-processing re-threshold procedure, this should be explained. Maybe give a few sentences on what exactly was done regarding re-thresholding, post processing."	"Qualitative evaluation section can be improved by choosing examples to compare against the gold standard.
Some sentences can be phrased better specifically starting sentences of a Section for eg. Section 2.1, ""To further boost the SN segmentation..."". Similarly Section 2.2 Writing this in active voice will help the reader understand the essence of that section better.
Please explain how a large computation time can be handled during the inference stage moving forward."
199	Ensembled Prediction of Rheumatic Heart Disease from Ungated Doppler Echocardiography Acquired in Low-Resource Settings	"I enjoyed reading the paper and found it to be mostly well-written and easy to follow. But I was left slightly frustrated by an occasional lack of detail, especially with regard to the training/validation procedure.
For example, in Section 2, the authors give details of their dataset of 2136 Doppler videos. They also mention that 95 of these were annotated with view information, systole frames and left atrium segmentations. Later (Section 4), it is stated that the training/validation/test sets were 5108/1277/1510 images. Why switch from talking about videos to images in this way? I found this slightly confusing. Also, the numbers given in Section 4 are for the rheumatic heart disease (RHD) detection task, but no mention is made of the training/validation of the preprocessing steps (for which only 95 videos were available). What training/validation split was used here? And was there any overlap between these 95 and the data used for training/testing the RHD task? I.e. is it possible that some of the RHD test set had been ""seen"" before when training the preprocessing steps?
Regarding hyperparameter optimisation, as noted above the authors stated their final values and (at least for the RHD detection task) which data were used to optimise them but did not mention the ranges of values tested or the strategy used for optimisation.
For the model description, I also found the text slightly unclear. The first model uses 3D CNNs but the input data are 4D (64x64x3x16) so presumably one dimension was handled by defining multiple input channels? Which one? Later, for the transformer model it is stated that 3 colour channels are used so I presume the same approach is adopted for the 3D CNN model but please state this explicitly. And does every video clip have 16 frames, all of which are of the same view? This should be mentioned in Section 2 if so. Finally, the results of the 3D CNN and transformer models are combined using a ""maximum voting strategy"". I presume this strategy only makes sense when there are multiple A4CC/PLAXC videos for a subject? E.g. if there is just one of each and they disagree how does maximum voting help? What do you do if the votes are split equally?
The results of the statistical testing are also not clearly presented. What was being compared to what? In Table 2, there are two symbols (*, **) indicating p-values of 0.03 and 0.04 respectively. The second column from the right has both of these symbols - so how can a statistical test have two different p-values? I'm sure I have misunderstood something but this is because the results have not been clearly presented in my opinion. Finally, there are no asterisks in the rightmost column - does this mean no statistically significant difference was found on the test set?
Other more minor comments:

The Introduction mentioned that RHD is often associated with mitral regurgitation (MR) and cited some prior work on detecting MR. But then the rest of the paper is all about RHD. Is there a clinical need to detect RHD? Or MR? Or both? In the data used in this paper, what proportion of the subjects had MR and were there subjects who were either MR +ve/RHD -ve or MR -ve/RHD +ve?
The last sentence of the Introduction seems like it should have come earlier, not as the closing sentence of this section. Also in this sentence: ""life even"" should be ""life and even"".
In the introductory text of Section 3.2 I would recommend the authors explain why they are introducing two different models (3D CNN and transformer based) as I was confused by this at first. They could mention here that they will be used as an ensemble later.
In Section 4, the authors state that they use 5-fold cross validation for hyperparameter optimisation. But then which model was used for final testing? Were the best hyperparameters used to retrain using the entire training set?
The last paragraph of the Discussion highlights a very important point in my opinion. The authors might want to comment on what level of expertise is required to acquire images that are of good enough quality to be processed robustly by their pipeline. What evidence do they have that ""minimal training"" will be enough to acquire such images?"	Please refer to the comments on strengths and weaknesses.	"Suggestion:

If possible, move all implementation details in Section 3.1 and experiments of preprocessing to supplementary, which saves lots of room for more comparison experiments.
If possible, run ablation studies for preprocessing.
If possible, run comparison experiments with other methods.
If possible, provide inter-observer annotation results.

Future works:

It would be helpful to develop a single- or two-stage model, which would highly speed up the training and inference.
It would be helpful to develop a model with fewer parameters, which can be deployed on mobile devices, such as smartphones, and laptops without GPU.  With this setting, handheld devices can be connected to mobile devices and directly predict the probability after imaging.
It would be helpful if the authors could release the dataset in the feature, which would attract more researchers to contribute to this problem."
200	Estimating Model Performance under Domain Shifts with Class-Specific Confidence Scores	Probability calibration, especially in neural networks, is an interesting and important topic. The Authors should take a look at the current SOTA probability calibration work (many review papers) and go from there.	I think this paper would be a good fit for MICCAI. However, the contribution statement has to be fixed prior to acceptance and the results have to be presented as described above (see weaknesses).	"Writing:

Since the authors aim for American English, the correct spelling for ""labelled"" is ""labeled""
Please check all the indices in Fig 2 for DoC, ATC, and TS-ATC
Eq (4): ""d_y'"" - ""d_j""

Some other suggestions:

Abstract: ""be deployed or its performance"" - ""be deployed or if its performance""
Section 2 Method: ""achieve the goal"" - ""achieve this goal""
Section 2 Method ""all the training pairs for the case z of totally n pixels"": ""of totally n pixels"" should be rephrased.
Figure 1: there is too much content for a quite small figure. At least fill the entire page width, but I would recommend removing some details.
Table 1: Even though the best results are in bold, they are not easily noticeable.
Supplementary material: Section C appears empty because the table appears on the next page. Maybe rearrange the section placement."
201	Evidence fusion with contextual discounting for multi-modality medical image segmentation	"1) conceptually the idea is simple - train 4 independent encode-decoder networks (with slight modifications) , and learn the fusion (merging) of segmentations at the end. The paper frames it from the  Dempster-Shafer theory angle, but essentially presents a simple trainable merging approach. 
2) Evaluation/comparison is lacking modern sota approaches  (including nnUnet). Instead the method is compared to many U-net like methods of 2018 or older, which is not necessary. 
3) Since the brats2021 dataset is chosen for evaluation - why not report  the results on brats2021 official evaluation hidden test set (their server accepts submissions)"	"It will be helpful if the authors can discuss the limitation of the proposed method.
Some examples where the dice score is less can be visualized with possible explanation.
In results section, few sentences can be added to explain why the proposed method is useful if it is not showing significant improvement in dice score.
Few lines on how it can be used together with any state-of-art method?
Minor comments:
Typo in the first line of Page 5."	"The number of citation is disorder, such as number of the first citation is [23].
English spelling problems :""the to uncertainty""should be replaced by""to the uncertainty"".
""can be extend""should be replaced by ""can be extended""
The two important compared articles (""Peiris, H., Hayat, M., Chen, Z., Egan, G., Harandi, M.: A volumetric transformer for accurate 3d tumor segmentation. arXiv preprint arXiv:2111.13300 (2021)"" and ""Zhou, H.Y., Guo, J., Zhang, Y., Yu, L., Wang, L., Yu, Y.: nnformer: Interleaved
transformer for volumetric segmentation. arXiv preprint arXiv:2109.03201 (2021)"") are all published in arxiv. More comparison with published papers with peer-review should be given.
""2.1 Evidential segmentation module"" is same with subtitle ""Evidential segmentation module"".
Dempster-Shafer theory be combined into segmentation module. However, the Dempster-Shafer equation is not differentiable for optimization, how to deal with the model optimization?
equation 9a uses different fonts for Typesetting."
202	Evolutionary Multi-objective Architecture Search Framework: Application to COVID-19 3D CT Classification	"The author should describe the potential object in a clearer way, especially the formula 3.
The author should rewrite some paragraph and fix the typos, such as 8.57 hours instead of 8.57 ours.
The author should explain the experiment results in a clearer way. In fig 2, the axis title is missing and the conclusion drawn from three figures is missing in the title."	"The method seems relatively general and therefore I would have liked to see it being evaluated on a more varied set tasks, e.g. both 2D and 3D medical image classification.
I realize that the change to segmentation tasks might require more changes to the pipeline and should be reserved for future work (as mentioned by the authors).
Typo: Fig. 2a) ""ours"" -> ""hours"""	"As I mentioned in the weakness part, please highlight the difference between the proposed method and the other NAS acceleration methods, like [2].
Seems the authors did not provided the searched architectures for different datasets. Are the architectures are very similar or very different from each other? In addition, what kind of insights we can get from the resulted neural architectures?
In the abstract, the authors mentioned that: Recent works show that no model generalizes well across CT datasets from different countries. Basically, for my understanding, the authors searched three architectures for three COVID-19 datasets from three different countries. That would be great if the authors could search for one unified architecture which provide superiority over all datasets.
Minor issues: I suggest to make all 'i.e.,','e.g.,', and 'et al.' in italic. Method [16] in table 1 should have a name.
Typos: section3.3, relu6 should be relu. Please also check other parts."
203	Explainable Contrastive Multiview Graph Representation of Brain, Mind, and Behavior	"Much more details should be provided. e.g.  hyper parameters, how edge of graph was defined? Was time-window used for dynamic FC? why distillation is necessary?
I don't think the model can find ""cause"" instead of correlation. And granger causality is hard to applied in resting fmri with large number of node.
22 major ROIs for fmri is not enough.
sex classification is not that good compared to current FC based result.
where the ""structural connectivity"" come from?
ablation study is necessary for this model."	"This paper used GCN for contrastive learning of structural and functional multimodal data and used the model results to analyze the strength of the structure-function coupling patterns between functional connectivity, structural connectivity and behavioral performance.
In general, this paper is comprehensive and technical sound that will surely inspire future research along similar lines. The main message is brought across fully supported by the presented results. However, I still have a few major concerns before possible publishment. (1) In the introduction, the authors mention several reasons why the previous method is not applicable, and in the article authors should emphasize how the method proposed in this paper solves these problems and why it has advantages over the previous methods. (2) Authors should add the parameters of the methods. It would be better to add some necessary arguments for Equations to make them easier to understand. The overall schematic illustration needs to be clear and easy to understand and highlight the innovative points of the model. (3) The first experiment used the causal explanation model to obtain the regions that play an important role in the classification, and the authors should further show and analyze them. (4) In the second experiment, three different FC data were used to assess whether the analytical approach of the role of SC is the innovative approach of this paper. Please add some details of this experimental approach or relevant literature."	"Several minor limitations are as flollowing:

the authors should testify the fusion of other modalities in HCP dataset for the sex discrimination to demonstrate its generalization of the proposed framework, such as fmri and dti, or smri and fmri.
there is no colorbar in Fig 4, and it is not clear how to compare them.
the used parcellation atlas is not detailed enough for fMRI."
204	Explaining Chest X-ray Pathologies in Natural Language	Spelling error p 4 p 4 radiolographic	"- Contribution is limited. Presented a new data collected from already public dataset.
- I could not find any explaining factor in the paper. Traditionally, explaining means that authors will explain how their results/method they used in not a black box but an explainable. (Title of a paper is misleading)
- Previous methods are used to set the baselines results. Authors claimed they proposed a new method. 
- all baselines are from previous studies including DPT is inspired from previous SOTA method. DPT leverages a DenseNet and GPT-2.
- This paper lacks a discussion on the experimental results and motivation analysis.
- - The results lacked visualization or statistical analysis."	The clinical evaluation should be performed by a consensus of clinicians.
205	Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation	"Overall, I think this paper has merit on the proposed method and adequate experiments. However, the paper can be improved in a few ways.

Some details about Inter-class Separation are missing in Sec 2.2.
How to select the subset of F_l according to the predictions' confidence? Is there any threshold?
What is the detailed structure of the attention modules?

What is the starting value of lambda and the specific warming-up function?
Backbone selection. Why do the authors use different backbones for LA and ACDC? In addition, the authors should demonstrate whether all the comparisons were conducted with the same backbone.
It seems from both Table 1 and Table 2, the performance increase is very limited under 8 labeled settings. It's difficult to assess whether the model is significantly better than the baselines. It would be helpful to provide the variance or testing models under more labeled/unlabeled settings.

A few other minor comments:

What do the red arrow and orange block represent in Fig.2?
Some of the notations in Fig.2 do not correspond to notations in the paper, e.g. X_{L} and X_{l}, Y and y. Some descriptions are missing, e.g. P_L, P_X.
What do 'y' in Eq(2) and 'y_c' in Eq(3) represent? The reviewer thinks they might be typos. Please double-check all the notations and equations."	"More comprehensive and relevant literature should be discussed, i.e., other strong perturbation SSL and prototype-based SSL.
Sensitivity analysis is needed since there are many hyper-parameters."	"Regarding to the first contribution, ""fewer labels"" and ""blurred targets"" are two common challenges in medical image segmentation task and existing semi-supervised methods always aim to address these challenges. In addition, the idea of using ""constrain the pixel-level smoothness and inter-class separation"" to address these challenges is not new, as the authors mentioned in the Introduction section (paragraph 2) that existing methods did the same thing.

For the second contribution, using adversarial perturbations for medical image segmentation has appeared in previous works (e.g., [A], [B]), which is not the first in this paper as claimed.
[A] Adversarial Perturbation on MRI Modalities in Brain Tumor Segmentation, IEEE Access, 2020
[B] Towards Robust General Medical Image Segmentation, MICCAI, 2021

The proposed framework is more like a combination of two existing works and has limited technical novelty. For the two parts of the proposed method, the ""Pixel-level Smoothness"" part is basically the work from [13] and the ""Inter-class Separation"" part is very similar to [1]."
206	Extended Electrophysiological Source Imaging with Spatial Graph Filters	"Did you try other metrics to evaluate the performance of the method? To me, when looking at Fig2 (3rd row), the MNE method gives a more suitable result than the proposed method that eventually covers a quarter of the hemisphere.
I do not understand why on real data, the proposed method gives such a sparse solution compared to other. This is counter-intuitive when regarding at the results on synthetic data.
On real data you mention the highly diffuse activation of other method, but on synthetic data your method is the most diffuse.
There is too much supplementary material, better write a journal paper to be more comfortable, this would allow you to give more details in the method, or limit your paper to synthetic data analysis. As it is, it is frustrating to have only partial information."	"Will start for more details about the main weakness of the paper.
The proposed method includes a,b, and l . The authors they do not mention a lot how the tuning of this parameters affects the results and how susceptible to error is based on a suboptimal selection of the parameters. Furthermore when coming to comparison between other methods with less parameters (e.g. I think sloreta has only 1 regularizer) is even more important to include the influence and easiness of tuning them.
Other comments of less importance

Fig. S.5.1 I n the case of 10 db. The performance difference between the proposed method and all the others seem significant while from table 1 this is not the case (From auc the method is not even optimal). Could you elaborate on this, might be an indication that the metrics are not optimal and some metric of mutual information - or correlation might be better?
""We set the length of EEG to 1 second"" - Not very clear to me you mean the whole EEG was 1 second? Why such a small choice is it much more difficult the the graph setting will be used in bigger segments?
In case where you have two distinct areas of activation (simultaneous) e.g. left and right temporal lobes, instead of 1. How would this affect the performance of the method? Especially the ""Forcing"" of neighboring signal (which helps in the case of single area of activation, as well as the low rankness when you have multiple ""areas of activation""  (e.g. temporoparietal network)
Define tr  (is trace ok but you need to define it)"	Please specify details of the computing platform, programming language and parameter settings used in this study.
207	FairPrune: Achieving Fairness Through Pruning for Dermatological Disease Diagnosis	"I generally enjoy reading the manuscript. Although there seems to be some intuition given in the text, I still feel that the proposed approach ends up being a series of engineering steps without clearly showing why the objective of Eq. 4 is equivalent to fairness. It seems that the authors refer to fairness as the accuracy difference between demographic groups. If so, please explicity define because there is broader definition of fairnes which I don't believe this artical is pursuing.
Some technical questions in case I missed something. 1. Why \Delta E (change in objective function) has to be identical to accuracy drop? 2. Why can we ignore the third-order terms in Eq.1 if \theta is not near zero?
Minor: bold the accuracy columns in Table 1"	"The word 'significantly' is widely used in the results and discussion sections. Since no statistical tests were performed I would replace it with 'substantially'.
It would be interesting, maybe as future work, to show how the method would generalize to a multi-class setting, for example if we performed the classification for each skin tone class separately.
There is a minor error, in Table 1 for DomainIndep the calculated differences Diff are wrong, they must have been copied and pasted from AdvRev and not replaced with the actual values."	"I enjoyed reading the paper and found the central idea of using pruning for fairness to be very interesting. The idea is simple (as many of the best ideas are) but as far as I know this has not been proposed before so I believe this paper has a high degree of novelty compared to other MICCAI submissions. I believe the paper would be a good addition to the MICCAI program. The comments below are aimed at improving it still further.
The paper is generally well-written, although see below for minor comments/suggestions. The introduction sets the methodological context quite well, and there are a good number of relevant papers cited from the computer vision literature. However, I thought that the review of papers on fairness in medical imaging was slightly limited. The authors cite Larrazabal et al [13] which is certainly relevant, but there are other equally relevant papers that were not mentioned. In particular I would highlight Abbasi-Sureshjani et al (https://doi.org/10.1007/978-3-030-61166-8_20), Seyyed-Kalantari et al (https://doi.org/10.1038/s41591-021-01595-0) and Puyol-Anton et al (https://doi.org/10.1007/978-3-030-87199-4_39). They could even distinguish between papers that assessed bias (Larrazabal et al, Seyyed-Kalantari et al) and those that also applied mitigation strategies (Abbasi-Sureshjani et al, Puyol-Anton et al) to make the discussion more relevant to this paper. Also, although the specific idea of using pruning to promote fairness is novel, a few papers have analysed the impact of pruning on fairness and so these could also be mentioned (https://doi.org/10.48550/arXiv.2009.09936, https://doi.org/10.48550/arXiv.2201.01709).
In addition, as noted above I think it would be useful for the authors to state the approach they used to set hyperparameters for their model (& comparative approaches?) and the data used.
Other minor suggested edits:
*	Section 1, para 1, line 5: ""rthe"" -> ""the""
*	Section 1, para 1, line 8: ""turns to perform"" -> ""performs""
*	Section 1, para 1, lines 10-11: Put dataset details in brackets. Also, I think ""ISIC 2018"" should be ""ISIC 2019""?
*	Section 1, para 1, line 16: ""with different"" -> ""from certain""
*	Section 1, para 1, line 18: ""biased"" -> ""bias""
*	Section 1, para 2, line 9: ""proxy of"" -> ""proxy for""
*	Section 1, para 3, line 13: ""Besides"" -> ""In addition""
*	Section 2, para 3, line 3: ""regularizing"" -> ""regularize""
*	Section 2, para 3, line 4: ""sensitive related"" -> ""sensitive attribute related""
*	Section 3.1, para 1, line 1: ""Given"" -> ""We define""
*	Section 3.2, para 1, line 6: ""row i and column i of second"" -> ""row i and column i of the second""
*	Section 3.2, para 2, line 6: ""and is biased"" -> ""which is biased"". Also, I think it should be ""biased against"" not ""biased for""?
*	Section 3.2, para 2, line 7: ""In the coordinate"" -> ""In the bottom illustration""
*	Section 4, Baselines section, line 6: italicise ""DomainIndep""?
*	Section 4, Ablation study section, line 9: ""consistent"" -> ""consistently"""
208	Fast Automatic Liver Tumor Radiofrequency Ablation Planning via Learned Physics Model	"It is unclear how much of the architecture is different from that of DeepONet
Please clarify how inputs of the branch network are put together
in relation to the trunk network, is it only the (elapsed) time used as input? if not, is that the set of queries starting from 0s? please clarify.
Overall, it would be great to validate this approach on a phantom model with known material properties, geometry and heat sinks.
Are there 14 (text) or 15 (figure) convolution networks?"	"(1) The formula 1 seems not right as the advection term (a_vp_bc_bvgradT) and R(T_b-T) should be in seperated equations (see eq(2) and (3) in ref[2]). Please check it and make sure the reference solutions are right, otherwise the presented results may be wrong.
(2) The authors used 4mm grid for simulation, which seems not reasonable -4mm grid will make the geometry of liver and vessel distorted, thus the simulation was affected. For example, Fig 3 shows that the vessels are even not continuous, how did the authors simulate the blood velocity and make it right?
(3)  The method part is not easy for readers to follow and the following information should be clarified:
a. it is not clear weather the input images are 2D or 3D. I guess it was 3D but the  figure 1 showed the input images were 2D.
b. For the trucnk network, it is not clear weather the input was a time point or  multiple time points.
c. What is the spherical source and what's physical meaning of the radius of the spherical source?
minors:
typo->section 2.1 : for a(an) advenction-diffusion-reaction problem."	"The 4mm grid seems quite coarse. How would this affect results? 
A figure to illustrate the details in section 2.2. would be very helpful. It is difficult to visualize the approach through text only.
Please further clarify what electrode position means in this application. Is it the depth of the needle? Coordinates of the tip of the needle?
From Table 1 the superiority of the proposed method over the Sphere method is not clear. The latter does not account for the heat sink effect of blood vessels which I believe is the benefit of the proposed method over the Sphere method. Would there be a measure/metric to demonstrate that? Is AE sufficient as a comparison metric?
Is it possible to understand in what situations the Sphere method is producing better AE % compared to the proposed method (in Table 1)"
209	Fast FF-to-FFPE Whole Slide Image Translation via Laplacian Pyramid and Contrastive Learning	"The paper is well written and the idea is interesting because it has not been used in digital pathology before.
The quality of the paper would increase if a deeper analysis of the LP method is included e.g. show what the internal representations in the generator look like, the masks, etc. If space is an issue, this can go in the Supplemental material."	"T_m should be defined similarly to c_h.
In the memory bank section, how is FF patches' size decided? 65535.
In the dataset section, what is the relationship between images at 512x512, 1024x1024, and 2048x2048? Downsampling or independent?  If the images in 1024x1024 are downsampled from 2048x2048, why are there only 4K images in 2048? Please clarify.
What is the batch size when training/testing fastFF2FFPE, AIFFPE, and vFFPE?
Fig2. could add a zoom-in view to show how fastFF2FFPE overcame the artifacts.
I assume the results are validated by the statistical tests. Please clarify."	"The paper makes several assumptions from the reader, who may or not be familiar with these. As such, it would be good that the details were clarified and not left to educated guesses or having to search for these. Examples
""steps, e.g., dehydrated, saturated with formalin and stained with dyes, whereas FF slides are produced in ultra- low temperature freezers with liquid nitrogen. ..."" 
There are well defined protocols to obtain FF and FFPE, please add references for these.
""However, there are many artifacts in FF slides and variations between FF and FFPE slides (see Fig. 1.a)."" Which artifacts exactly? Please add a list and if these are visible, add arrows to illustrate these in the figure.
In Figure 1a
What is the green line around the samples? I make an educated guess that the background has been removed, but readers should not be making educated guesses.
Fig 1b uses FF patches as input and Isynth as output, but Fig 1c uses Iin and Isynth, there should be consistency or if the terminology is correct, the caption should explain the differences.
Fig 1b ises Lrec, Lcl and Ladv, which are explain many pages afterwards. Same with M,N.
Fig 2 The visualisation of the results is rather useless if I do not know what I am looking for. I do not even know if the proposed method is supposed to be better, why not add the accuracy metric for each case so that the reader see if the proposed method is better."
210	Fast Spherical Mapping of Cortical Surface Meshes using Deep Unsupervised Learning	"The proposed method utilizes existing spherical CNNs [29]. The central idea of spherical deformation is based on Spherical Demons that employs diffeomorphic deformation over a stationary vector field using the scaling and squaring approach. This is also used in [24] for spherical CNNs-based surface registration. Although a combination of several metric losses is incorporated, this paper technically reads an application of what was proposed in [24]. A hierarchical optimization has been widely adapted in classic surface registration. I think the paper needs some acknowledgement of the previous studies. In terms of their methodological descriptions.
Local charts of the velocity field are presumably predefined. To learn deformation fields, spherical CNNs need to support rotation-equivariance as initial mappings are not necessarily aligned. As such is absent in [29], rigid alignment and possibly data augmentation may be required for the proposed framework. In particular, [29] defines a spatial filter on an icosaherdral mesh with a specific order of neighborhoods, where the performance may depend on the alignment of initial mappings. The authors claim that the IAP is used due to its popularity, but the proposed method perhaps works because the initial mappings are already roughly well aligned?
Please provide more in-depth rationale of the average convexity for this work. The average convexity roughly captures overall sulcal patterns but offers a rougher representation than other fine geometry such as mean curvature. Fine features might work better to capture local deformation.
As the authors pointed, FreeSurfer has an extra step after spherical mapping that unfolds self-intersection. This is another bottleneck in FreeSurfer. Even if the proposed approach models diffeomorphism, the resulting deformation can practically have self-intersection since the re-tessellation introduces sampling errors and the deformation field is defined on an icosahedral mesh.
The experimental results also need to be interpreted carefully. For example, if some method uses a conformal mapping, large distortion in distance or area is somewhat expected. Indeed, an isometric mapping is not possible, so any mappings have their pros. and cons. depending on which metric they optimize over. Since spherical mappings have several types (conformal, area preserving, etc.), I think a more fair comparison in this work would be to show performance of individual component (loss). This also lies along what the authors claim in the introduction.
What was the step size for the scale and squaring approach? What is the memory requirement for the proposed method? It looks like the reported runtime of the proposed method exclude that of that of the IAP. This is misleading as the IAP is preprocessing in this work.
This paper cites about a third of papers from a specific group, and some of them seem not strongly related to this work. Please focus rather on relevant studies. There are yet other studies for spherical mappings. For instance,

Choi et al., FLASH: Fast Landmark Aligned Spherical Harmonic Parameterization for Genus-0 Closed Brain Surfaces
Nadeem et al., Spherical Parameterization Balancing Angle and Area Distortions"	"How is the coarse-to-fine mesh exactly generated? ""reparameterize it again using an icosahedron with higher resolution"" may not be sufficient and perhaps a reader would like to know the differences between the coarse and fine meshes after reprarmeterization. What properties are maintained after the transform?

I am wondering if the authors have done any toy experiment with ground truth, although not reported in this paper. I understand that this method operates without supervision; but such an analysis would have been helpful.

How much of a change is expected at each iteration? Is there a way to control how much to update per iteration (e.g., learning rate) so as to transform the mesh faster or slower?"	More than 800 cortical surfaces were processed by the method. Is there any failed case? If so, what is the reason for the failed cases.
211	Fast Unsupervised Brain Anomaly Detection and Segmentation with Diffusion Models	"One of the main disadvantage of DDPM is its slow sampling speed. But the authors claimed in abstract that DDPM can achieve log-likelihoods that are competitive with transformers while having fast inference times.

I really donnot undertand why a transformer[16] need about 10 mins for the inference in one 2d image. If so, the training process will be extremely crazy. Can you give me more explanations about why?

More convincing Visualizations for both the observation (as mentioned before) and segmentation. At least, from the supplementary material I get limited information from the visualization. What do you want to emphasize in Fig. 1?

For BRATS, there are three tumor labels, which one do the authors use? which region of the reported DICE score is for?

Minor:
What is the 'f' in tables 1 and 2? 
The m in page 5 is referred before definition."	"In addition to the above major comments, please find below some minor suggestions:

The authors mention that the proposed approach may be useful in 3D neuroimaging applications in the introduction, but all the experiments are done in 2D. It may be helpful to provide a discussion about this at the end of the paper.
Although a video is provided in the supplementary material, it would still be helpful to the readers to show some example images together with their segmentations in the main manuscript.
Small typo: ""using at diffusion model"" in the introduction."	"One thing that I think could paint the results in a better light, would be to present detection results instead of segmentation. In critical cases where detecting possible anomalies in time is more important than getting a good delineation, a good detection metric paired with a small execution time would be required. For example, a set of anomalous and normal images could be gathered and then the detection rate of anomalous regions could be used instead of Dice. That would help recontextualize Tables 2 and 3.
Another aspect that I think could be improved is the methodology section. There is an abuse of math notation that is mostly lifted from the original papers but removing some key details and definitions. As a consequence, the summarized explanations are hard to follow and confusing. I would have preferred a high level explanation (which is partially given) with a limited use of notation complemented by the reference to the original papers for the interested readers. One example of this is the explanation of Lcodebook as ""We used the exponential moving average updates for the codebook loss"". Without checking the original paper and only reading the explanations on the manuscript it is impossible to understand what it is referring to.
Finally, this is more of a personal opinion, but I think that it would be interesting to use classical unsupervised segmentation approaches to compare where possible. They are simpler, before the deep learning revolution they were extensively used, they are fast (especially if implemented in GPU) and they can also give good results. That would further help contextualize the idea of unsupervised lesion segmentation."
212	Feature Re-calibration based Multiple Instance Learning for Whole Slide Image Classification	I would have liked to see a more in depth analysis on further datasets but I understand the limitations of space.	"As mentioned before, I guess a contrastive loss (or loss with weighted samples)  provide with us the current improvement and still is not limited to binary classification. So, I suggest authors to explore if this statement is correct.
PEM and  PMSA abbreviations have been introduced many times."	It will be better if the authors include qualitative results. The authors should also revise the result validation section as much information is not available. The results should be validated by pathologists as well. The authors should include stepwise results of figure 2. The quantitative analysis part also needs to improve. Include ROC instead of tabular format.
213	Feature robustness and sex differences in medical imaging: a case study in MRI-based Alzheimer's disease detection	"Why did the authors restrict their analysis to sex dependency only? It would be interesting to see how age-dependency plays a role in such a framework.
The paper would benefit for a more exhaustive analysis.
The author claims that deep-learning reach lower performance compared to the  hand-crafter feature and logistic regression method. However, the method used have been proposed in a pre-print submitted recently (February 15, 2022) - Without saying that given the date this probably goes against the double-blinding submission process. The performance of this network is below current state-of-the-art for AD classification, therefore all the discussion might be not adapted for more advanced deep-learning methods."	"-I suggest adding the main quantitative results to the abstract

Denote MRI and HC before table 1
Review the format of table 1
The validation and final test sets are not clear, I suggest adding a table with the subsets used in each validation instead of figure 2.
I recommend considering the age of the study subjects as a covariate. Brain morphology can change according to age.
One of the challenges in the classification of AD or MCI is the automatic classification between MCI/HC, justifying why this test was not performed."	"The authors could mention why they mix FreeSurfer and SPM to obtain regional volumes normalised to ICV (I assume this is related to the better consistency in ICV estimation with SPM?).
Could the authors explain why the classification performance is higher for women than men, even when no women were part of the training set?
Even though nothing is significant, it seems that the CNN is more affected by sex differences than the logistic regression, could you comment?"
214	Federated Medical Image Analysis with Virtual Sample Synthesis	"Major comments are as follows:

More details on experiment settings should be provided, especially on non.i.i.d. distributions.
More realistic datasets shall be used for evaluation. MNIST-like datasets are quite different from clinical data, which makes the experimental results less convincing.
It will be interesting to show the training curves when using VAT. Because data synthesis via VAT between local and global models can affect the convergence."	Besides the concern in the weaknesse, it is interesting to see how this method works on imbanlanced data.	"Fig. 1 can be improved, the middle distribution part does not deliver intuitive and direction information.
It is not clear how to calculate the direction r_l and r_g, better give a clear formulation.
Better perform repeated experiments with mean and std.
Better add the visualization of virtual samples or draw the distributions with and without virtual samples to demonstrate the effectiveness of using the VSS.
Better add the training curve, given current results, the model seems has not converged.
Besides performance comparison, current additional studies are weak."
215	Federated Stain Normalization for Computational Pathology	"I think that the original motivation for the paper is reasonable, but much of the evidence provided to back this up is somewhat convenient/selective. There are many stain-normalisation techniques and data augmentation techniques out there which are orthogonal to Federated Learning and it is not clear what inadequacies these might have that BottleGAN does not (and why). 
This is always the risk when combining techniques (GAN, SSL, FL) - that you obfuscate where the unique benefit is coming from.
What is unfortunate is that there might be something useful and interesting in this work, but it has not been sufficiently teased out and rigorously tested."	"1: Since the proposed BottleGAN network can explicitly transfer staining style, what is the advantage of integration of BottleGAN into WA-based FL? 
2: In this paper, it said that BottleGAN network to learn staining style transfer with linear growth, it is suggested to add a training time for comparison with other GAN network. 
3: In 3.2, author said the proposed architecture is entirely independent of the size of the input image, how to make it, it needs to make clarification?

In the experiments, this paper is to solve stanning style normalization, why choose IOU evaluation criteria for performance evaluation?There are no statements in the manuscript."	"The idea to apply federal learning paradigm to computational pathology can really be of great practical importance. In this regard, the authors offered solution for the major obstacle: how to solve problem created by different privacy-protected staining-styles protocols in laboratories supposed to cooperate in creating large datasets necessary for deep networks training.
The new original architecture of the Bottle-GAN staining-destaining network is also novel and creative contribution. Its demonstration for different staining styles transformations to a reference staining style and, afterward, destaining is impressive.
Thus, to make proposed concept closer to the application in practice, authors should try to evaluate it on more datasets. In particular, in my view, it is important to verify whether assumption on availability of a shareable public dataset of reference stained or destained whole slide images (WSIs) owned by a server is met in practice. It is not certain whether this assumption can fulfilled in some real-world privacy concerned scenario. It is also important, since it is not totally clear to me, whether publicly shareable datset have to be composed of destained WSIs or reference stained WSIs or both?"
216	FedHarmony: Unlearning Scanner Bias with Distributed Data	The discussion about privacy-preservation and scanner bias is interesting and this particular domain-adaptation approach is interesting. As noted above, it might improve the paper to go into some more detail regarding hyperparameter settings as well as to more highlight the methodological novelty of the approach. Finally, some discussion as to how the approach could be used with other image data types (e.g. rsfMRI) and abnormality outcome prediction (beyond just predicting age) would be helpful.	Not much to add. My main thoughts are in the strengths and weaknesses boxes. This is not a topic I am deeply familiar with, although I have a good understanding. The main feedback would be to run more extensive simulations to identify the boundaries between where the strategy is and isn't effective.	The authors should consider extending their evaluations to broader neuroimage analysis tasks, such as segmentation, disease prediction, or image-to-image translation.
217	Few-shot Generation of Personalized Neural Surrogates for Cardiac Simulation via Bayesian Meta-Learning	"The compared method should be discussed and analysed to emphase the contribution of this work.
the contribution should summarize the contribution of the the work effectively."	"Fig. 1 is confusing: Are the four cardiac surfaces D_c or D_x? On the right, \hat{D_c} and \hat{D_x} are not explained anywhere in the paper. The KL notations in the middle are not correct either.
On page 4, what is the graph CNN model used here? The description of this GNN is very vague and more detail is needed.
In Table 1, comparing metaPNS and PNS, why metaPNS typically has much better CC/DC but much worse MSE?
There is a typo in the title."	"Refer to weakness.
1) It is better to review the terminology and make it more consistent with those widely used in meta-learning.
2) Add some discussion on motivations of why use Bayesian meta-learning over other methods like MAML."
218	Few-shot Medical Image Segmentation Regularized with Self-reference and Contrastive Learning	"I do not understand why performance of PANet and SENet is much lower than the proposed method. As far as I understand, performance of PANet at least be similar with their model without any regularization.
Evaluation setting is limited. Authors uses only big organs which are relatively easy to segment. In addition, they use 2D slices from support and query volume after being carefully matched. To make a general framework applicable to various organs, entire process need to be done with 3D volume. Then other organs can be also used for evaluation."	"The paper is well written and organized, however, the heavy notation used in Section 2 could be improved to ease the reading. For example, there are too many indexing terms. In Sections 2.2, 2.3, and 2.4 the indexes k and j are not used in any of the equations but are still present in many of the variables, omitting these indexes could reduce clutter. Also, the equations for the background class and target class c are the same. The authors could present only one of the equations and just treat the background as another class.

""Specifically, our method achieved the best segmentation performance on each abdominal organ which were significantly better than the second-best method (SSL-ALPNet) (77.65% vs. 73.02% in terms of average DSC)""
No statistical significance tests are provided to support this claim. The reviewer strongly suggests the addition of some marker (i.e. \dagger) alongside results that are significantly better than the baselines in Table 1.

Authors could also discuss the application of the proposed method in interactive image segmentation as a future work. This could be a great addition to the conclusion of the paper."	"In table 2, it would be better to provide another ablation study where the contrastive learning method is used but without self-reference.
Regarding the contrastive learning, it would be better to cite some other works using contrastive learning to compare with the proposed method. Especially if the proposed contrastive learning method is different from existing works.
This would be a good paper, if the authors can demonstrate that:
This paper proposes the novel self-reference setting. As a self-supervised method, it outperformed the setting that uses contrastive learning without self-reference (the required addition ablation study). Moreover, by combining the self-supervised method and contrastive learning, better performance can be achieved.
Otherwise, if using contrastive learning alone can achieve a better performance than using self-reference, considering the limited novelty of the contrastive learning approach and limited added value of the self-refrence, the paper would be weak as a conference paper.
Given the additional ablation study, I would be willing adjust the score to accept."
219	FFCNet: Fourier Transform-Based Frequency Learning and Complex Convolutional Network for Colon Disease Classification	"Dealing with colonoscopy images is very challenging and the method proposed in this work can be useful, 
-yet regarding the image classification; (i) it is better to consider a sequence of frames, (ii) flat lesions are among those polyps that need to receive more attention,
-There should be a section to demonstrate the effectiveness of this network through methods such as GradCam
-The method should be compared against more advanced network such as transformers which has the FFT as an embedded feature."	"Illustrate the advantages of the proposed complex network over previous methods that are utilized to mine complex information. Also, supplement some ablation study on it would be even better.
Add some visualization results to justify the improvement of the proposed method. For example, what kind of hard samples will be better recognized in the proposed method."	See weaknesses.
220	Fine-grained Correlation Loss for Regression	"1.more comparison experiments and ablation studies should be designed to demonstrate the proposed method.

The authors  should  improve the level of the presentation."	"The samples that cause outlier predictions probably were outliers in the training set too. Could the authors comment on this assumption and explain what will happen if we simply exclude the outliers from the training set and then train the network using a correlation-based loss? And how the results will be different compared to the proposed method that splits normal and outlier predictions before calculating PLC? Although excluding some samples leads to a smaller training set, probably those samples were not so informative.

The network was trained using a NIVIDIA 2080 Ti GPU, which has 11GB of memory. Since the input size was 320x320, I was wondering whether a batch size of 160 could fit in the memory. Is there any chance that 160 is a typo, where the correct batch size is 16?

Could the authors please comment on why in Table 1, the NIN and SoDeep methods yield much worse AE and RE while their performance is comparable to the other methods for the other cases?

It is mentioned that ""to the best of our knowledge, these medical regression studies mainly focus on learning the mapping among input and output for individual samples, but ignore the learning of the structured relationships over the dataset and among the samples."" However, we know that during training phase, the network looks at the whole training set at each epoch, meaning that the trained model considers all samples together. Could the authors comment on how a model trained using regular loss functions completely ignores the relationships among the samples?

I appreciate the way that the authors introduce the correlation-based loss functions only when the network passes epoch #30 (ignoring the first few epochs). Probably because they pick the top 10% of samples with the largest difference between prediction and ground truth as outliers in each iteration. Therefore, in the first few epochs, it seems the differences are too noisy to let us select the correct top 10% since the network is not stable yet, and even non-outlier predictions may show large differences with ground truth.

""reduce the distribution discrepancy at at global level."" -> Please remove the extra ""at"""	"Suggestion:

Use accurate words to describe the method.
Evaluate the proposed method with public datasets and other modalities.
Run experiments for hyperparameter a.
Run experiments with different margins."
221	Flat-aware Cross-stage Distilled Framework for Imbalanced Medical Image Classification	See above	"Major: Flattening local minima is a clever idea yet there is still no guarantee that local optimum of the second stage can be found in the flatten local minimum of the first stage. It will definitely help to have at least one of the following (1) more theoretical analysis to prove this point  (2) Visualization of the loss landscape in different stages with the proposed methods (3) experiments on more datasets with more variety of data distributions. It is also not clear how rho (radius of neibouring area) is selected and what's the effect of rho.
Minor: There are some typos in the last few pages of the manuscript. For example (1) Page 7, row 3, Ours(RS) and Ours(RS) should be Ours(RW); (2) Page 8, Table 3 should be Fig. 3."	See 5.
222	Flexible Sampling for Long-tailed Skin Lesion Classification	"Overall the paper is very well written and the presentation is clear.
Extensive experiments and comparisons are conducted to validate the presented method.
The only major issue that requires more in depth analysis is the selection of the anchor points. The paper reads like authors assume that the initial embedding representation of the samples are distributed around a mean anchor point (or in more general sense unimodal gaussian) However, no evidence is presented if this assumption holds. Did the authors conduct any analysis on this issue?
In the text, it is not clear, how do the authors calculate the entropy for further selection of the samples. Figure 2 makes things more clear but the reviewer thinks that authors could have explained this more clear in the text.
It would have been good if the authors can show how many new samples are introduced at every step of the curriculum for an example run."	"focus on one main technique and give detailed analysis/motivation/discussion
OR
give the detailed ablation study to verify the effectiveness of each module, and discuss/analysis the relation between them. If the pages are not enough, I think SOTA comparison can be shortened, comparing with SOTA resampling methods is enough."	"My main comments are:
1) The results of some of the methods in the comparative study are relatively close to the proposed method (RW and ELF). However, these results do not seem to be explained or discussed in the paper. If the authors have additional experiments or different metrics to show the improvement of their method with respect to the baselines, I would recommend including them as well. See section 3.3.
2) Table 2  only highlights when the proposed method performance of the proposed method when it does better than the rest. I suggest properly highlighting the best results to show the advantages and weaknesses of the proposed method properly.
3) Section 2.1 is slightly challenging to read. Although it becomes clear later in the paper what x, v represent and the purpose of equation 1, I recommend rewriting that subsection to improve the overall clarity of the manuscript."
223	fMRI Neurofeedback Learning Patterns are Predictive of Personal and Clinical Traits	"The motivation for the methods section needs to be further clarified.
NF data needs to be further compared and discussed with the performance of resting-state data that are commonly used."	Great piece of work! The focus on the amygdala is clear and well defined. This opens opportunities for further research into fear/anxiety disorders. The method is neuromodulatory and uses fMRI. Though details on the acquisition and data quality control are missing.	"Not sure how to properly interpret the MSE results. They seem to be arbitrarily scaled.
What's the importance of the results regarding to traits prediction. Are they clinically useful?
The description of the method does not seem to be very clear, maybe because the pipeline is fairly complicated and the space for this paper is somehow limited"
224	Free Lunch for Surgical Video Understanding by Distilling Self-Supervisions	"** General **
The list of key contributions is a bit redundant with the previous paragraph. Some space would be saved then for the display of some sequences of surgical phases. I am not requesting this for the paper acceptance.
Sometimes ""et al."" is written in italic and sometimes not. Can you make it consistent?
Section 2.1.
L6: what is the meaning of the index ""i""? 
L6: what is the meaning of the index ""+"" under ""k""?
P4, Figure 2: the ""x"" at the bottom of the figure should a \nu instead. Otherwise, you need to change  Eq 1 and the first line of the first paragraph in Section 2.1.
I would appreciate a small motivation of the use of moment-updated encoder. This motivation is written in [10], but I think that the paper will gain in clarity.
Section 3.1
Can you provide the same type of information than Cholec80? e.g., the type of surgeries and image size.
Also, reporting the average length of the videos can give an idea of the dataset size.
P6, Section 3.1, L3: ""We sample the videos into 5 fps"". I think that the explanation should be mentioned, even if it is for computational reason.
P6, Section 3.2, L8: ""by self-supervised learning approaches"": you can add a reference to the next section 3.3, to make clear what are these approaches.
Table 1: Usually, the best value is in bold. However, this common practice is not followed for:
Recall on Cholec80: Ours - MoCo v2
Precision on M2CAI16: Ours - MoCo v2
Table 3: similarly for Recall, where the third configuration of the ablation study, and not the second, gives the highest recall.
Tables 1, 2 and 3: can you center the metric names?
Tables 2 and 3: Can you add in the legend that the results are given for Cholec80?
** Typos **
P1, Paragraph 1, L1: require - requires 
P2, Paragraph 1, L6: corrupted images reconstruction - reconstruction of corrupted images
P3, 2nd bullet point: dataset to improve - dataset improves
P3, Section 2, Paragraph 1, L2: Section. 2.1. -Section 2.1.
P3, Section 2, Paragraph 1, L3: illustrated - presented
P4, Legend of Figure 2, L4: model ,i.e., - model, i.e.,
P4, Last Paragraph, L5: formulate - formulated
P5, Section 2.3, Paragraph 1, L7: i.e.sim - i.e., sim
P5, Section 2.3, Paragraph 2, L4: the the -the
P6, Section 3.2, Paragraph 1, L1: frames - frame
P6, Section 3.3, Paragraph 1, L5: the performance of self-supervised training on ImageNet outperforms that trained from scratch-  to reformulate (""performance...outperforms"")
P6, Section 3.3, Paragraph 1, L7: motivate - motivates
P6, Section 3.3, Paragraph 1, L9: outperform - outperforms
P7, Section 3.4, Paragraph 2, L1: We conduct ablation study - We conduct an ablation study
P7, Section 3.4, Paragraph 2, L4: model ,i.e., - model, i.e.,
P7, Section 3.4, Paragraph 2, L4: can not - cannot
P8, Paragraph 1, L2: fine-tuning - fine-tune
P8, Paragraph 1, L3: approaches - approach
P8, Figure 3, Legend, L3: scratch, - scratch.
P8, Figure 3, Legend, L4: MoCo v2.- MoCo v2, respectively.
** Future works **
It would have been interesting to have an idea of the performance of the proposed method on videos from other surgeries, such as cataract surgeries. Cataract-101 dataset: ""Cataract-101 - Video dataset of 101 cataract surgeries"", K. Schoeffmann et al., Proceedings of the 9th ACM Multimedia Systems Conference, MMSys 2018, pp. 421-425, 2018."	"Would be good to add ablation studies on the effect of hyper-parameters Tau and Lambda.
mAP is metric often used in computer vision community for evaluation of action recognition models on long videos. Might be good to add this metric to the paper for completeness.
The method should be applied to a more updated approach for video action recognition, i.e. 3D convnets which are outperforming 2D CNN based models."	"The paper is well written and presented. It proposes to distill the abundant knowledge in the general computer vision datasets learned using the current self-supervised approaches to the surgical domain. The proposed two-stage training approach to preserve the semantics in the first stage and distill the knowledge in the second stage helps it obtain improved results. However, Fig. 3 gives some counter-intuitive results where the results using less percentage of labels are better than using all the labels. For example, results using 20% labels achieve more than 90% accuracy on the Cholec80 dataset, whereas they reach 87% accuracy from the 100% labels. The authors should thoroughly check the correctness of the results in Fig. 3.
Moreover, it also provides an insight into the results (if we assume that the results are off by some margin) that the underlying task is relatively straightforward. It shows that the methods reach more than 80% accuracy on the Cholec80 dataset using as few as 5% of labels. The authors should discuss these points or apply their approach to some more complex surgical data science tasks to evaluate their approach."
225	Frequency-Aware Inverse-Consistent Deep Learning for OCT-Angiogram Super-Resolution	"In page 6, the authors set a parameter a = 0.7. However, I could not find this parameter in any formula of this paper.
The evaluation metrics PSNR and SSIM need the ground truth of OCTA image. Please give the way of getting the ground truth."	"This paper is essentially a super-resolution task, and unpaired super-resolution methods should be added to the experiment for comparison.
The paper may consider proposing indirect image quality evaluation metrics. The paired metrics used in the current paper are not convincing.
The resulting image of the non-rigid alignment is missing in the paper.
The purpose of the paper is to enhance the whole 6x6-mm image, but the comparison result image of the whole 6x6-mm image is missing in the experiment."	"I think the architecture presented in Fig. 2 could hardly reveal inverse-consistency and may lead to readers' misunderstanding.
I get confused about the cross-entropy adopted in feature distribution loss and f() in Eq. 5. In addition, the value settings of b1 and b2 are also puzzled me.
Some mini comments: 
(a)	clinis in the second sentence of the introduction should be written as clinics. 
(b)	In the sentence ""Weights of each components are set as a = 0.7, ..."", what does a represent? 
(c)	UnpairedSR using pseudo pairs in the first paragraph of Section 3.3 should cite the reference [14].
Compared methods seem somewhat limited and all of them are unpaired learning frameworks. I think the authors could supply some supervised learning methods due to have paired images.
In ablation studies, does the method wo HFB indicates that authors replace HFB with mere high-frequency components? Please clarify it.
Only adopting PSNR and SSIM for evaluation is limited. It is of great clinical interest if authors prove that their approach could benifit subsequent vessel analysis or diagnosis tasks."
226	From Images to Probabilistic Anatomical Shapes: A Deep Variational Bottleneck Approach	"It would be good to validate the proposed method on an additional dataset, preferably a public dataset.
Section 3.1 will need further clarification - it is not clear how the ground truth meshes were generated, by who? radiologists or people without training? how many?
Figure 4 is too small, almost unreadable."	"In the method part, it would be more clear that the last layer of the encoder is explained in details.
The paper mentions the proposed method is self-regularized. It is not clear to the reviewer why it is self-regularized and this part is not discussed. Also, why the comparison work is not self-regularized is also not discussed. Some discussion can make this more clear."	"(1) To address weakness #1, it may have been a good idea to perform ablation studies for the two different contributions separately.
(2) It would be good to clarify whether the dataset is public/private and to include more descriptions if it is private.
(3) It would be good to explain figure 4 (i.e. shape accuracy) a bit in the main text."
227	FSE Compensated Motion Correction for MRI Using Data Driven Methods	FSE is often used as a multi-slice 2D FSE, and motion corruption can happen in both in-plane and through-plane directions. Any methods considered in both directions would be more practically useful.	Strongly recommend the authors to bolster their work by performing experiments on prospectively motion-corrupted FSE data.	"Evaluate the method on real MRI data with real motion.
Add PSNR in Table 1."
228	Fusing Modalities by Multiplexed Graph Neural Networks for Outcome Prediction in Tuberculosis	"Other than the basic fusion techniques, perhaps a high-level summary of multi-modal learning approaches (e.g., for text and images) would be useful to provide more context.
To understand the data better, please provide the prediction class frequencies for each data modality.
If possible, it would be informative to see which relationships between modalities are most important for each outcome."	"Authors demonstrate the performance of Multiplex-GNN on only 1 dataset (Tuberculosis). This is perfectly sufficient for MICCAI, would probably be hard to fit more experiments into the page limit. But for a journal extension, I would really be curious to see the performance on more datasets. Maybe also strong baseline methods from ""conventional"" ML on the d-AE joint latents, especially Gradient Boosted Classifiers (e.g. XgBoost).
The presentation of the multiplexed-graph framework is extremely condensed in this work. This may be hard to avoid within the page limit of MICCAI, but for a journal extension, I would appreciate a much clearer introduction (almost tutorial-style) into the theory
I believe citations 3&4 refer to the same book. Probably good to check&merge."	Could you please comment on possible ways to incorporate interpretability/explainability into your model? Since this is a clinically relevant problem, this aspect would also be important for the practitioners
229	FUSSNet: Fusing Two Sources of Uncertainty for Semi-Supervised Medical Image Segmentation	"There were a number of aspects of the proposed framework that I liked. It is interesting to investigate the effects of using both aleatoric and epistemic uncertainty. I also like the way that epistemic certain/uncertain regions are treated differently by the semi-supervised learning. I think there is a novel methodological contribution in this paper, but I don't think it came across that clearly in the authors' summary of their contributions at the end of the Introduction. E.g. they highlight the different treatment of certain/uncertain regions as a contribution, but in Section 2.3 they mention that [14,17,20] have already done something similar. It would be useful to clearly state the authors' contributions in the context of the most closely related work. Currently it is difficult to do this in the Introduction because a lot of the most relevant work is only discussed/cited in the Methods (i.e. Sections 2.3/2.4). I think the paper could be organised better by having a Related Work section to more thoroughly review the literature, then the authors' contributions could be more clearly stated in this context.
I also have some concerns about hyperparameter optimisation. There are a number of hyperparameters in the proposed framework but there is no information about how they were chosen, what data were used for optimisation and (sometimes) even what the final values were (e.g. what was the value of lambda in Eq 1?). This is all important information to help the reader interpret the results. E.g. for the left atrium (LA) dataset there is mention of training and validation sets but no test set. Does this mean that the validation set was used for hyperparameter optimisation, and that these are the results reported? Or did they use the 54 cases of the challenge test set at http://atriaseg2018.cardiacatlas.org/? This was not mentioned in the paper. Finally, what hyperparameter optimisation was performed for the comparative validation models and the ablation study experiments?
Some of the differences in performance, especially on the LA dataset, seem to be quite small, and I believe that the test (or validation) set was only 20. Could some statistical significance testing be included? Also, I presume the results presented are for a single run of all models? What would happen if the authors simply reran all their experiments with a different random seed - would they still see the same differences?
Other minor comments:

Can the text in Fig 1b be made bigger? It is quite hard to read at the moment.
P4: ""Thus, we do not enforce consistency constraint in certain areas ..."" I know what the authors are trying to say but I think this choice of words is misleading - it sounds like they are saying that they do not treat different areas differently, which is directly contradicted by the following Eq 1. I would suggest something like: ""Thus, we do not enforce the consistency constraint in areas considered to have low epistemic uncertainty ...""
It would be interesting to see some examples of the epistemic certain/uncertain areas. E.g. is this just highlighting boundary regions as uncertain? I understand that the authors are constrained by the page limit but some insight into this would be useful, even if it were just a sentence in the Discussion."	"On page 4, the authors said, ""The CE loss and focal loss focus more on pixel-level features, while Dice loss and IoU loss care more about shape information."" This is not true; Dice and IoU do not care more about shape; they are still a pixel-level loss. This and the next sentences need correction.
Eq. (3) the softmax would be on the channel dimension, right? If so, the index j should be out of the softmax function. softmax(\eta_i^(t))_j.
Unnecessary acronyms and symbols can be avoided for better readability, e.g., MIS, EMA. Also, please introduce ASD and HD before using those acronyms."	"Abstract: ""... outperforms the state-of-the-art ... by large margins"" -> consider a more quantitative statement such as ""by up to XX% DICE"" or similar.
Fig 1: some text is too small to read, especially some boxes on Fig 1b; please enlarge
Fig 1a and text indicate an iterative nature of the framework. The paper is lacking convergence criteria and discussion about the added complexity (runtime, memory) for training, especially in light of the comments regarding other methods computational cost issues (MC sampling)
Fig 1a: not clear what pretraining entails (first box in Fig 1a): what is the training objective, what data is used, etc. I suppose it is a fully-supervised training of the V-Net directly on the segmentation task using the selected training datasets that are also used later in the training? Please clarify.
page 4: ""four classifiers share the encoder but differ in loss functions"": what about the decoder? Why is only one forward pass required (or only one forward pass for encoder, but four for different decoders (?))? This part needs clarification
page 5, eq. 4:  What is the value of \omega? And right below: ""weighted sum of ...: how are L_sup and L_unsup weighted against each other? Are these hyperparameters difficult to tune, and where the same used for both tasks?

The paper highlights several times that AU is modeled in logit space, but it is not well motivated why and if it helps

More details on experiment section
FUSSNet appears relatively complex to train. Therefore it would be great to compare not only final performance metrics, but also time until convergence to those results for each method
it would be interesting to see the impact of increasing the number of labeled datasets in training (or changing the ratio of labeled vs. unlabeled training datasets)
page 5: Unlabeled data not truly unlabeled, because data is preprocessed (crop) based on labels.
page 6: sliding window patches are very small (16x16x4 pixels) -> can the authors motivate why? memory limitation or does a small window iprovide benefit in any way? Same setting used for comparsion methods? Maybe this very small context explains the many disconnected regions in some thumbnails in Fig 2.
""theoretical upper bound"" (used to describe performance with a standard training strategy using all training data) -> this term is not adequate here. Please use ""fully-supervised upper bound"", ""fully-supervised reference result"", or similar."
230	GaitForeMer: Self-Supervised Pre-Training of Transformers via Human Motion Forecasting for Few-Shot Gait Impairment Severity Estimation	"It is a good job that shows us the potential of pre-training models in this field. It could be better if the paper can describe the proposed model in more detail. Besides, Fig. 1 is somewhat simple and is very similar to the picture in the referenced previous work POTR. Redesign the picture in a different way could be better. And I suggest that ""z0"" should not be put in the same box as ""z1,z2,...,zT"" for it will not serve as input to the decoder."	"In contrast to good ideas, the explanation of the proposed design is not thorough.
Compared to the cited paper [1], the novelty of this paper is incremental.
[1] Lu,M.,Poston,K.,Pfefferbaum,A.,Sullivan,E.V.,Fei-Fei,L.,Pohl,K.M.,Niebles, J.C., Adeli, E.: Vision-based estimation of mds-updrs gait scores for assessing parkinson's disease motor severity. In: Medical Image Computing and Computer Assisted Intervention (2020)"	"It is an interesting paper. The paper develops models to predict MDS-UPDRS gait impairment severity and these models are first pre-trained on public datasets to forecast gait movements.
Comments are below:
Illustrate and discuss the misclassifications from the proposed method? And also suggest possible ways to make it better? Any discussion to make the model more interpretable would add value.
Report areas under roc as well and areas under the pr curve for all methods. 
What is the threshold used for all classification methods? How was it decided? Is that threshold optimal?
Provide more description on the subjects in the study such as their age, gender and severity of Gait Impairment etc."
231	GazeRadar: A Gaze and Radiomics-guided Disease Localization Framework	"The methods section can be restructured for clarity. It is currently confusing as to how the GAF is pretrained and what was the input data for pretraining.
Standard metrics such as IoU, mAP can be used for comparison.
As a baseline, a simple model that does a basic multiplication of the radiomic image with the gaze map and then thresholded for localization can be used to show the benefit of a deep learning model."	It would be ideal if the authors can demonstrate that the proposed method performance increase is significant.	The authors present the global-focal student-teacher architecture for disease localization based on radiomics information and visual attention features. The authors explore an interesting topic related to eye-tracking. The developed model can be of practical importance, as it improves the results of disease localization using easily collected eye-tracking data. The authors provide a solid ablation study but the paper lacks clarity.
232	Geometric Constraints for Self-supervised Monocular Depth Estimation on Laparoscopic Images with Dual-task Consistency	"In the first paragraph of Introduction, the authors stated that ARAMIS and RAMIS have become the preferred approach for laparoscopic surgery. This reviewer agrees that RAMIS has become preferred, however, ARAMIS is still questionable nowadays. The provided references for this statement do not provide enough clinical evidence that ARAMIS has become the preferred approach yet. Therefore, it is better for the authors rephrase this statement.

There is probably a mistake in Fig 1. Given the only input of Source Image I_{t'}, can the Scene Coordinate Network generate both Scene Coordinate of I_{t} and I_{t'}? Can the authors double check this figure and make corresponding corrections as needed? Also, in the paragraph below the Fig 1, please change ""I_{t} and I_{s} are the input of pose estimation"" to ""I_{t} and I_{t'} ..."". Please describe what is D(p_t) function below Equation (1). Furthermore, regarding Equation (1), how the empty pixels (those ones do not projection) are addressed?

Given that Scene Coordinates can be generated from the predicted depth of the image, can the authors provide a detailed explanation why a separate Scene Coordinate Network has to be explicitly trained? I understand that an Ablation Study has been provided, but an insightful discussion on this matter is currently missing, e.g., why this additional branch will benefit on the smooth surfaces? In addition, are Scene Coordinates ranged? Are they normalized? How the performance of Scene Coordinates get impacted when a different camera (with diff. focal lengths, etc.) is used?

What are the unit of the rotation errors in Table 2? In radians or degrees? Can the authors describe how the errors were computed?

Minor comments:
Please use ""Siamese"" instead of ""siamese"", and there are a few places where ""Siamese"" got mis-written (e.g., Abstract, etc.).  In Page 3, change ""To overcome the challenging"" to ""To overcome the challenge"". There are also other places with noticeable grammar mistakes and typos, please perform proof-reading on the paper."	"I don't understand Equation (3). Are you really searching the t' for which E becomes minimal? Does that mean you're selecting the neighboring frame with the lower loss? Or is that a typo? Either way, I think this equation should be explained or removed.

Fig. 1 implies that only Source image T_t' is given to the Scene Coordinate Network, however the network predicts both ^c'S_t' AND ^cS_t - is there an arrow for the second input missing? I think this network would require both images?
I find section 2.2 relatively hard to follow. I had to read it a few times before I (think I) understood. I understand this is likely because of limited space to describe a complex topic but here are a few things I stumbled accross:

The term ""world coordinate system"" is used, however as far as I can tell the system never really uses any consistent world coordinate system? Everything is done in (local) camera coordinates. If I understood this correctly, then I suggest removing the term ""world coordinate system"".
""from the camera coordinate system c to camera coordinate system t'"" should this be c' instead of t'?
""the camera system coordinate c"" -> the camera coordinate system c (?)
I think ^cP (introduced for Fig. 3 and in the text on page 5) represents the same as ^cS_t(p_t), correct? As the latter representation is used in Equation 5, maybe replace ^cP by ^cS_t(p_t) (and similarly for ^c'P)?
I think you could maybe drop the superscripts c, c' and c'->c? As far as I can tell they're always consistent with the subscripts t, t' and t'->t and this would be one less thing to have to understand/interpret. I think it's clear from the text that camera coordinate system c is where the camera is at at time t and c' for t'.

Minor:

""and scene coordinate prediction with novel consistency loss functions under a camera coordinate system."" The ""under a camera coordinate system"" wasn't clear to me at this point - maybe remove here, or explain?
Fig. 2 referenced before Fig. 1
""I t and I s are the input of pose estimation to estimate the transformation matrix"" -> I_s should probably be I_t'?
If Fig. 4 represents absolute values, could you show the range instead of ""low"" and ""high""?
Since you show rotational errors in Table 2, I assume that SCARED includes ground truth poses? If so, I would mention this in 3.1 where SCARED is introduced (since the ground truth depth is mentioned).

Language (suggestions only, I'm not a native english speaker):

""...the smooth surface causes the photometric error to reduce even the corresponding positions between adjacent frames are inaccurate."" Words missing? add ""for"" and remove ""are inaccurate""?
""We predict the scene coordinate prediction as an auxiliary task."" predict... prediction (maybe ""estimate the scene coordinates"" instead?)
""an unified"" -> ""a unified""?
""siamses"" -> siamese""? (2x)
""depth estimation had become"" -> ""depth estimation has become""
""To overcome the challenging of pose estimation"" -> the challenge of
""Then the minimized photometric error can be deformed as"" -> can be formulated as (?)
""in the previous researches"" -> in previous research?
""two consistency"" -> two consistencies? Is this the same as ""dual consistency""? If so, maybe replace with the same term everywhere?
""an weight mask"" -> a weight mask
""ours w/o scene coords represent s the proposed method was evaluated with the siamese optimized pose process."" -> ?
""on test datasets overall the whole scenes."" -> on test datasets over all scenes. (?)"	"Only the encoders' structures were described. It would be better to also provide details of decoders.
""siamese"" was spelled wrong several times.
Page 3, wrong notation ""I_s"".
Ambiguous sentence on the top of Page 8."
233	Gigapixel Whole-Slide Images Classification using Locally Supervised Learning	"""
You must edit the paper at least with Gramerly. ""the golden standard for many""
Gold standard
""WSIs is challenging due to their enormous image sizes.""
I in WSI stands for Image; then image is repetitive. 
WSIs are
""which processes the entire slide exploring the entire local and global information that contains""
think about rewriting the sentence, specially contain
""Duan et al. [9, 8] proposed to train each module by minimizing intra-class and maximizing inter-class distances to improve the data separability.""
8 is a survey and the main contribution of 9 is much wider than Fisher idea.
""LK = Lcls(H(FK(xK-1)), y)""
consider semicolon "","" after equations that continue by where"	"Include the results of the SOS method for the LKS dataset.
Add discussions around batch size, number of locations (10) you use for the RFR model
Reference your equations
Introduce the abbreviations
Improve the writing"	"In 2.1, what is necessity to use feature reconstruction unit?
In RFR, the detailed parameters are suggested to give.
At the same position, how does random ensure that most positions are ergodic? The corresponding proof was not found in the following text. And, the reason choosing cropped size is memory?
In LKS dataset, why the magnification was 4x, which different from other two dataset. Meanwhile, suggest to explain the reasons for choosing 4-5 magnification.
How are the number and location of instances determined?
The difference between different number of Module block is minor, however, no specific analysis and verification were given.
For results, it is suggested to add statistical analysis of significance, or cross-validated."
234	Global Multi-modal 2D/3D Registration via Local Descriptors Learning	"""Despite all these advantages, to the best of our knowledge, these approaches have been applied the medical domain only to a limited extend. ..""
-> An example of MR/UR registration based on SIFT descriptors could be: https://doi.org/10.1007/s11548-018-1786-7. There are others.
""Multi-modality and multi-dimensionality Different modalities exhibit different visual appearances and also emphasize different structures. We overcome this issue by jointly training two distinct networks (a 2D model for US and a 3D one for MR) to produce cross-modality descriptors.""
-> the two networks will provide descriptors for each modality, but are the extracted features the same? Probably not, at least due to the different physics behind each modality. This could be discussed."	"It would be nice to see how the algorithm can be generalized to different ultrasound scanning setting, e.g. different depth, frequency, etc.
The dataset sample size is a bit small, which is why the authors used cross-validation. But it would be nice to have a larger set for a proper validation."	"The goal is to improve multi-modal registration. The work focuses on  US and MRI, of abdominal images. It is challenging due to the noisy data acquired with US and also because the abdominal part is highly deformable. 
The proposed method is based on keypoints extraction and matching. The main contribution is the adaptation of the LoFTR algorithm to multimodal data and with imprecise ground truth. 
First of all, two models are trained (U-Net) : one for 2D images and one for 3D data for estimating cross-modality descriptors (matching score are computed by scalar product between features extracted with the 2 differant models). Then, problems about the imprecision of the ground truth by considering data augmentation.
The approach is validated on 16 patients with ablation studies.
Questions :

""In order to not consider the ultrasound geometry but rather the structural features, we further augment the ultrasound data by cropping it in a random polygon shape"" : it is unclear what is really done and more important why."
235	GradMix for nuclei segmentation and classification in imbalanced pathology image datasets	"The authors comapre to CutMix, which shows quite disappointing performance. The authors shall explain why CutMix behave worse here, otherwise I'd assume the way the authors apply CutMix is problematic.
In the evaluation of nuclei classification, besides the F1-score for each type of nuclei, the overall performance is also suggested.
I'd also suggest to split the data into train/test multiple times, and report the mean/std. Then the results would be more convincing."	"This work targets an important problem; it is difficult to generate large, representative training data for nucleus segmentation. It proposes a synthetic data generation pipeline to increase the number of nuclei of rare class types in imbalanced datasets. The experimental evaluation shows performance improvements.
The experimental evaluation is limited because it uses a single deep learning method (developed by the authors). While there are performance improvements, they are relatively modest (around a few percent on average; Tables 2 and 3). The authors should include another network (e.g., Hover-Net) to show if the performance improvements from the proposed method are due to the specific deep learning network used in the paper or other networks can benefit as well. For example, nucleus segmentation performance numbers from the Hover-Net paper [2] (Table III in [2]) are generally higher or equal to the performance numbers obtained by the proposed method (Table 2 in the paper). The Hover-net paper uses real data only.
The authors should also provide a better comparison of their approach to other approaches ([a] and [b]): 
[a] Gong, Xuan, et al. ""Style consistent image generation for nuclei instance segmentation."" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2021.
[b] Hou, Le, et al. ""Robust histopathology image analysis: To label or to synthesize?."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019.
Both [a] and [b] use in-painting and nucleus generation via simple segmentation or deformation of real nuclei as part of synthetic data generation steps. Both works also implement variations of an approach in which an end-to-end training approach is used that integrates training of a task specific network and the synthetic data generation process. The goal is to guide the synthetic data generation and model training to generate training data that can better optimize the task specific model. That is different than the approach used in this paper which separate synthetic data generation and training of the segmentation model. Readers would benefit from a comparison to those approaches."	"This work would be more appreciated if the authors compare between CutMix and GradMix in terms of the methodology. CutMix was not described in the introduction, so without understanding CutMix, it was difficult to follow the result when comparing CutMix and GradMix.
In Figure 1, I recommend to avoid using blue color for rare-class. It was difficult to see rare-class nuclei because hematoxylin-stained nuclei are also blue.
What is the definition of ""major"" and ""rare""?
Figure 3 should show more instances of rare-class nuclei, because the novelty of this work is to classify rare-class nuclei when imbalanced. I only see one instance of miscellaneous."
236	Graph convolutional network with probabilistic spatial regression: application to craniofacial landmark detection from 3D photogrammetry	"Overall a sensible approach to an interesting problem, with evidence that it gives good performance.
What is the required accuracy of the placement for clinical application, and does the system achieve that?
Minor points:
Section 2.2 Unnecessary precision. Sufficient to say the average number of nodes was 8000+/-4100
""unitary surface normal"" -> ""unit surface normal"""	"There is no justification for the orders of polynomials used (k=3 and 7). Where these decided on empirically?
The authors may want to comment on the large standard deviations in their results on average landmark detection error.
Section 5 conclusion. There is a typo in the first sentence. ""We presented a novel graph convolutional neural network to for the automatic identification..,"""	"1) More related works should be reviewed and compared.
2) The detection results should be assessed clinical experts to confirm the clinical feasibility of the proposed method.
3) Quantitative evaluation results should be provided."
237	Graph Emotion Decoding from Visually Evoked Neural Responses	It would be better if they could provide the conclusions by summarizing their works and their interpretation. Also, to convince the model interpretation of which emotion scores and the regions in the brain are important for decoding emotions would be needed. Additionally, since the number of the subject is small it would be better to perform an analysis on the other dataset to show the reproducibility.	"Overall this paper is interesting and the authors sort to increase the accuracy of a difficult prediction problem using a simple way. The proposed emotion-brain region architecture is novel, and very straightforward to understand, i.e. the biological interpretation of the model is very clear and understandable. However I do have some questions and concerns about the experimental design and the results.
1, it is not clear what value were authors trying to predict, emotional score for audience like me is quite vague, and subjective, authors should clearly indicate how the emotional values were measured.
2, how the input vector was generated was not clear, emotional stimuli (from vedios) and fMRI signals are both time series, did authors segment the time series into pieces? How? and what is the length for each time window?
3, is the output (the emotional score) a vector or a scaler? If it was a vector, then how the accuray was measured? If it was a scaler, what does it represent in a time series window?
4, the prediction is based on a fMRI voxel signals but not a functional brain network or graph. However two methods authors compared (GCN and BrainNetCNN) are both graph based, how authors ran the two models using the same input is unknown. More information is needed.
5, I don't think the models authors compared are most state-of-the-arts methods, authors should compare their methods with more recent methods."	Please refer weakness
238	Graph-based Compression of Incomplete 3D Photoacoustic Data	"(1) The significance of the proposed method should be stressed by providing more explanation on PA data compression. Specifically, what is the data size you used in the article? If the (W,H,D) in Section 2.1 are not large, so is such compression valuable?
(2) I noticed a conventional CS method (BP algorithm) is used here. I wonder if there is other state-of-art CS can be used here? Because the sampling ratio is not very low here, meaning regular CS methods are sufficient to recover the signals confidently.
(3) Words and presentations should be chose more carefully. For example, in Section 2.1, is the equation ""sigma X_=P_inc"" an accurate formulation? Besides, the authors claimed ""As medical data tends to be sparse, ..."", general medical data is sparse in what sense?
(4) In Equation (3), only the largest $\lambda$ is used, what if we keep top N largest values?
(5) In Figure 3, I observe the margin between ""DCT-only"" and ""GBC-only"" is narrower compared with the one beween ""Prop-full"" and ""GBC-only""? If combined with the concern about the rationale of PA data compression. This point further determines the merit of this paper."	"My primary concern is how the undersampling and reconstruction is performed.
Usually in PA and other tomographic applications, the undersampling to save measurement time is performed in the measurement space (here the PA signal, which is a time-series). Reconstructions are then performed in the image domain, there is no undersampling done here in the reconstructions. If I understand the paper correctly, the authors state that an undersampling is performed in the reconstructed PA image (section 2.1). This does not make sense to me. The undersampling needs to be performed in the measurement space. 
Additionally, the undersampling would not be random and follows a pattern that pixels on the 2D scanning domain are omitted, while for all measured points the full time series is available.
This needs more clarification and motivation. Maybe I did not understand it and the undersampling is just done to save memory of storing the image? But then the whole discussion on compressed sensing (which concerns measurement space) is misleading.
For the construction of the graph, it is not entirely clear how we go from the voxel grid to the graph. This possibly assumes the random omission of voxels? These would be then excluded in the graph? Maybe a figure could help to illustrate this. In equation (1), could you explain the role of \sigma?
I would ask for some computational details. How long does the encoding part in the compression take and also the decoding?
Could you add some absolute numbers, MB of image before and after compression?"	"Including more comparisons with popular image/video compression standards and other learning-based methods will make this approach more convincing. For example, AVC [27], HEVC [20], ref A, and ref. B.
Ref A: Pengfei Guo, Dawei Li, and Xingde Li, ""Deep OCT image compression with convolutional neural networks,"" Biomed. Opt. Express 11, 3543-3554 (2020)
Ref B: Zhihao Hu, Guo Lu, Dong Xu; ""FVC: A New Framework Towards Deep Video Compression in Feature Space"" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 1502-1511

Authors may clarify the relation between Q and compression ratio (CR)? What's the definition of the compression ratio? From Fig.3, under same Q, the proposed method always achieves a smaller compression ratio."
239	Greedy Optimization of Electrode Arrangement for Epiretinal Prostheses	"I think the work could benefit from some examples of image ""reconstructions,"" i.e., it would be interesting to see examples of how the optimized electrode positions might be leveraged to create better quality image perception."	If accepted and published at MICCAI, I think that the authors will not receive the feedback and citations they would get in case they submit and publish this work in a more relevant community. As citations in this paper clearly shows, no prior work has been published in MICCAI, IPMI, IPCAI, Medical Image Analysis or TMI, which are the main conferences and journals relevant to MICCAI. I think that this work will have much more impact if published in a more specialized community.	please see above.
240	Hand Hygiene Quality Assessment using Image-to-Image Translation	"The manuscript is well written and easy to read. I have two minor remarks for improvement of the manuscript: On page 5, please add a reference (website, or publication) regarding the MediaPipe and finger-web. In the second last paragraph of the Introduction, there is an extra ""used"". 
Please see below two more general comments/suggestion

Have you considered to compare your method with using 2D statistical shape models for hand contours? Such shape models might also be useful to generate a test dataset for the hand translation model.
I believe the strength of the proposed method is in the improvement of methods introduced by Kampf et al to document, analyze and compare data from much larger populations and multiple settings. Therefore, I would have enjoyed seeing one or two of the statistical evaluation graphs as proposed by Kampf et al. For example, in your reference [5], Figure 1 or 2 show the frequency of untreated skin using a color map. Having a similar image (even just for one of your task categories) in your manuscript would show a potential application of your proposed method."	"p.4 Is the mapping function R^s -> R^t to signify a mapping from s-dimensional real space into t-dimensional real space? I found this a bit confusing
p.4/5 The loss function is not well explained. Why is the chosed formulation more robust? This would be nice to briefly explain.
p. 5 The experiments with the standardized template space are not well explain and not motivated from my perspective. Why is a registration-based approach not a good option here? This should be explained I think. What was the idea behind the triangle/trapezoid mapping approach?
p. 6 Evaluation metrics: Why are U-Net and U-Net++ the baselines to compare against? Of course it depends what you want to compare. So here it seems you would like to argue that the Attention Gates are an important addition to model. However, isn't the more important question if a deep learning-based approach is better than a baseline method? Is there maybe a more traditional method that you could compare against?
p.7 Results: Are the difference in Dice and IOU between the models practically relevant ? Can you comment on this?
p.7 ""Thus, to avoid overfitting, we chose to use the model trained with eight epochs for translating the lab study data to hand templates"" This is a bit of an unclear argument to me. Why eight epochs? Did you inspect the reasons for overfitting?"	Please address the issues discussed in the main weaknesses section.
241	Harnessing Deep Bladder Tumor Segmentation with Logical Clinical Knowledge	"The paper should be compared with more diagnosis rules embedded methods, for example ""Integrating Diagnosis Rules into Deep Neural Networks for Bladder Cancer Staging"".
2.Describe the details of the methods by which clinical knowledge is integrated into the network."	"The method is novel and provides an original way to use clinical data. The paper and the figures are clear. The authors provide comparison with other state-of-the-art methods. 
The authors should provide more information about some parameters. In particular they should explain how the parameter beta was obtained. Was the value obtained empirically? (The narrow range 0.9-1 should be justified. The value 0.01 in the Lsegment formula is not justified. 
Were the experiments (groundtruth) validated with clinicians? There is a lack of information regarding to that issue."	"If the author is the first one who applies this idea, it will be a good seed with more analysis for the top journal paper.
-May be adding the wight for equation one be adding a before Lsegment and tuning the two available a and b will enhance the segmentation accuracy.
The data for bladder segmentation is imbalance. I think you should select a loss function that can face this challenge, such as Tversky loss, which will be better than cross-entropy."
242	Hierarchical Brain Networks Decomposition via Prior Knowledge Guided Deep Belief Network	"In this paper, authors proposed a novel prior knowledge guided DBN (PKG-DBN) model to identify the hierarchical and complex task functional brain networks (FBNs). Specifically, they enforce part of the time courses learnt from DBN to be task-related and the rest to be linear combinations of task-related components. By incorporating such constraints in the learning process, the PKG-DBN model can simultaneously leverage the advantages of data-driven approaches and the prior knowledge of task design. Extensive experiments on both HCP Gambling and Language task fMRI data have well supported the proposed ideas.
In general, this is a novel and interesting work and the paper is well structured. There are only a few writing problems need to be improved. For instance, how the overlap rate is calculated is not clear. For Table I, in addition to the average overlap rate between DBN inferred RSNs and their corresponding RSN templates across subjects, the standard deviation should also be reported. The figure resolution need to be improved."	"In this paper, the authors propose a prior knowledge guided version of DBN for brain network discovery. By imposing task paradigm information, the discovered brain networks are able to match better with the RSN templates. However, there are a couple of comments I would like to make.

It is a bit confusing to show fig 2 as the authors mentioned that DBN has lower loss but the authors mentioned that the model can be overfitted. Did this happen to all the subjects? How did the author infer that the model were overfitted? In my understanding, the authors concluded the overfit by observing an early turning point of the mismatch loss in PKG-DBN. However, could the authors plot similar task paradigm mismatch plot for the DBN model?

How were the hyperparameters chosen in the experiment?

Similar as I wrote above in the weakness. As we already have access to the task labels, why don't we switch to the supervised method entirely to estimate the brain networks?"	"In this paper, authors propose a novel prior knowledge guided DBN (PKG-DBN) model hierarchical brain network decomposition. Specifically, they enforce part of the time courses learnt from DBN to be task-related (in either positive or negative way) and the rest to be linear combinations of task-related components. By incorporating such constraints in the learning process, our method can simultaneously leverage the advantages of data-driven approaches and the prior knowledge of task design.
In general, it is novel and interesting. The proposed points are well supported by the results. I only have a few concerns/comments.
1) A few Figure resolution is relatively low, such as Fig.2.
2) The model parameters should be clearer.
3) The language need to be improved with native English speaker. There are a few language errors in current version."
243	Histogram-based unsupervised domain adaptation for medical image classification	This paper is a good paper but I would try to improve Table 1 a lot and also the grammar could use some more editing. Otherwise good job.	Experiments are insufficient and the motivation is unclear. The convergence of the model should be demonstrated to help analyze performance. Experimental details should also be supplemented.	Motivation and intuition is clear. The paper writing is good.
244	How Much to Aggregate: Learning Adaptive Node-wise Scales on Graphs for Brain Networks	"The major concern is the novelty. Despite the contribution is elaborated and interesting, it seems a small improvement of the graph-heat method.
Please be specific which Amyloid tracer (there are several in the ADNI dataset), also which ADNI group? All?
The results using the cortical thickness are a bit puzzling. From Fig.3 it seems you only used AD patients at the baseline (correct?) , yet the results in Table 2 are relatively good for the models even if not good when using other features. This is surprising as at baseline in the ADNI dataset, AD/Control/MCI are not so different except for the cingulum, hippocampus and some areas. Old works on voxel-based morphometry and the ADNI dataset had a lot of difficulties in this. It is quite impressive that works so well even with SVM. So, is the cortical thickness discriminant or not?
The normalization with the cerebellum for amyloid-PET is known, though depending how it is done it makes a lot of difference. Subtracting the mean value? the max value? Z-score between healthy subjects and each AD subjects?
Moreover, it is surprising that degree as features performs so badly with SVM. There are other studies even using t-test and simple features as degree, centrality, etc which says the opposite (e.g. Elsheikh et al. Front. Hum. Neurosci. 2021). Can you please check how you use the degree? As an average degree? Kept as local features?
Minor:
The style of the paper is sometimes clumsy and presents many typos, majorly due to hurry or distraction, or long sentences (e.g. page 2 ""...Graph Diffusion Convolution (GDC) [14], however,...""  .  Furthermore, the figure 2 has an imprecision in the left image (scale s1 is not as wide as s2, despite the caption tells the opposite). Some symbols are used for different things (I.e index I used both for the convolution layer in eq. 8 and the sample Y in the eq. 7). The performance listed in table 2 is not particularly sharp (in some case the SVM is even better, as in ""All Imaging Features"" section, or the ""GDC"" generally is very close), etc"	Please refer to the strengths and weaknesses of the paper.	Some as the weakness.
245	Hybrid Graph Transformer for Tissue Microstructure Estimation with Undersampled Diffusion MRI Data	"1) The presentation in 2.3 mainly focuses on the mathematical aspect of the RDT module. Adding more explanations about the difference between RDT and standard Unet and the meaning of the notations in terms of imaging parameters will be helpful.
2) More details about the definition of \theta is needed. It is not clear how symmetric q-vectors with respect to the origin and the norm of the q-vector are considered in the definition."	"The authors may better clarify what the new contributions in their network design are. Is it the new combination of existing building blocks or are these building blocks also novel?
The authors have not mentioned how the threshold of theta was determined.
Information about standard deviations can be added in Tables 1 and 2.
The authors may consider application of the proposed method to other datasets in the future, especially those with patients."	One thing to consider, when evaluating accuracy of multi-compartment models, you may want to consider the underlying tissue/csf volume fraction.  When a voxel is mostly CSF, the ICVF value has little effect of the model fit, so in my opinion, it's not quite fair to penalize that.  You can see in Fig. 2 row 3 that the largest error values are in the ventricles, where ICVF is not very meaningful.  One thought is to use a weighted average with weighting by (1 - Fiso) when summarizing error across the image.
246	Hybrid Spatio-Temporal Transformer Network for Predicting Ischemic Stroke Lesion Outcomes from 4D CT Perfusion Imaging	"General
In this paper, the authors tackle AIS lesion prediction directly from CTP raw images, i.e., without the conventional way of estimating perfusion maps. This is an important topic since such maps can discard information that may be learnable from a data-driven approach. The novelty of the work lies in the methodological approach, by using CNN and Transformers to extract and combine features from the multiple time-steps, and its application to CTP and AIS lesion prediction. The Reviewer considers the method novel, but there are concerns related to the evaluation. Please, check the detailed comments below.
Comments/questions to the authors (not in order of importance)
1) How many transformer blocks are used in the model? Could the authors clarify it, please? Also, it would be interesting to see ablation studies where this number is changed. That would also inform about how much we can gain from further temporal context aggregation.
2) The proposed method is evaluated in an in-house dataset, which makes it hard for future work to be built upon or to compare results. There are public datasets that could be used, for example, ISLES 2017 [2] provides MRI perfusion data with the raw temporal acquisitions, and it could be used to compare with SotA. While it is not CTP, the proposed method should be possible to be applied in ISLES 2017 without modifications. Could the authors comment on why such a dataset was not used, please?
	a) Moreover, ISLES 2017 has a hidden test set, which would allow mitigating some of the evaluation concerns mentioned next.
	b) The authors state ""sets a new state-of-the-art for lesion outcome prediction from raw 4D CTP data"". But, since the data is not publicly available, such a claim cannot be challenged or claimed so strongly.
3) The proposed work raises some concerns about the evaluation procedure.
	a) The authors use 10-fold cross-validation for evaluating the model. This is a valid practice, given the size of the dataset. However, the Reviewer assumes that the reported metrics are the average of the validation fold of each run. In this case, results may be overly optimistic, since there may exist the risk of overfitting. Again, this is a valid method to find hyper-parameters, but having an external test set would be desirable.
	b) The authors crop the images such that only the brain hemisphere with stroke is kept, which poses 2 main issues. First, results may be overly optimistic because some potential regions of False Positive detections are removed before analysis. Second, it does not represent a fully automatic real-world scenario, where the affected hemisphere of the brain is not known since manual annotations do not exist.
	c) The authors detect a strong statistical difference in the results in terms of DSC when comparing with baselines. But, none to small difference exists when we consider volume. This is of course possible, but it would be interesting to see more discussion on why this is the case.
4) The authors cite [3], but do not compare with. This work was proposed in the context of MRI perfusion but seems possible to be used for CTP, or at least partially. One of the contributions of [3] is to model the temporal aspects of the raw perfusion images as channels of CNN. This would be a simple, yet, valid CNN-only model to compare against.
Further comments (suggestions/extra comments on future work) - NOT intended to be addressed during rebuttal

The method considers 2D slices of the CTP, which neglects the 3D nature of the images. It is understandable, however, that it is computationally demanding. Indeed, the authors will try to address it in future work. Besides the proposed future direction, it may be worth taking a look at methods that decrease the computational load of the 3D CNN, too, such as [4].

This is more of a detail. The authors wrote, ""paired t-test with p<0.05"". The p-value is indeed used to check the statistical significance, but such a threshold is referred to as significance level, usually defined as alpha.

The Reviewer believes that the proposed method is interesting. Still, it would be good to see an extended version of the work that considers more validation, more datasets (including public ones), and a comparison with more SotA methods.

References
[1] Carion, Nicolas, et al. ""End-to-end object detection with transformers."" European conference on computer vision. Springer, Cham, 2020.
[2] http://www.isles-challenge.org/ISLES2017/
[3] Pinto, Adriano, et al. ""Enhancing clinical MRI perfusion maps with data-driven maps of complementary nature for lesion outcome prediction."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2018.
[4] He, Junjun, et al. ""Group Shift Pointwise Convolution for Volumetric Medical Image Segmentation."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021."	"For fair comparison between methods, I believe all methods should chose their own optimal training settings, e.g., epochs, learning rate, batch size, rather than fixed for all methods as presented in this manuscript.
When it comes to spatio-temporal learning, commonly used approaches include the well-known LSTM/GRU, nowadays ConvLSTM and ConvGRU. I wonder why the authors opted for transformer over ConvLSTM/ConvGRU in the first place?
As the transform is the main technical contribution, I am curious how this would compare with ConvLSTM/ConvGRU, which is not presented in this paper."	"1, Some details should be provided: after the CNN encoder, what is the feature map size for each slice and how are they sent to the transformer and decoder? Let's say there are T slices (at T time points) that lead to T feature maps, the output of the transformer will lead to T feature maps as well. How to combine the T feature maps obtained by the transformer with the decoder to obtain a single segmentation?
2, I think the comparison of the model sizes and average runtime do not take much space and they can be put in to the manuscript. 
3, They authors should consider to compare the fusion strategy with some other methods. The transformer here is proposed for fusion the information from the T slices. Is there any alternatives for this purpose? Some comparisons are necessary. 
4,In figure 3, it seems that the images had been skull-striped and cropped. However, such preprocessing was not described in the manuscript. 
5, Does it really necessary to use the raw 4D CT perfusion images as input? What happens if using transformer-based network for the perfusion parameter maps?"
247	Ideal Midsagittal Plane Detection using Deep Hough Plane Network for Brain Surgical Planning	"In general the paper is nicely written and easy to follow. It would be great to share code and allow others to reproduce results and apply method to other use cases.
The comparison to state of the art would require more justification. From me, the conclusion can often not be fully understood, e.g., ""Consequently, they commonly have the disadvantages
of low efficiency, poor accuracy, [...], leading to unsatisfactory and suboptimal results."" However, some of the approaches give for their application promising results and not so sure if I agree with such conclusion. Another example is ""In addition, insufficient number of landmarks will also make the plane fitting unstable, which ultimately leads to the poor robustness and accuracy of
the plane detection based on the location of landmarks."" Again, I can not fully follow the argumentation as a plane fit does not require a lot landmarks and I would argue that usually enough 3D landmarks can be detected. At the end, ground truths is extracted from annotated landmarks.
Figure 4 deserves further explanation. How have the cases been selected? What can be observed in detail? How can the difference be explained?
Very nice to split the data in different levels of pathology (>5mm and < (=?) 5mm)"	"P1 Abstract: For Abstract, data types and forms are not mentioned, which can be appropriately supplemented. Let the reader have a more comprehensive understanding of the work of the paper.
P2 line1-7: It's not that the explanation for ""manually delineated guideline"" isn't clear. The specific way of manual labeling leads to bad clinical effects. I think this part of the explanation can be modified to give readers a more intuitive feeling.
P2 Fig.1. (a): What is the specific impact of this deviation on the operation.
P5 Fig.3. line2-4: There is no specific explanation for this part. Can you use a small amount of space or formula to show it in the text?
P5 2.2 (2): Such voting process can greatly improve the training efficiency, but will it affect the training effect to some extent if all pixels with a space median value of 0 are erased? Is it possible to add comparative tests to verify that the training efficiency can be improved while ensuring the improvement of recognition accuracy?
P6 2.4 line1-2: Can you give a relatively specific explanation, or add paper citations with corresponding explanations?
P6 2.4 l: Can you explain the rationality of setting 0.1? Or add experiments to illustrate.
P7 line8-10: The relationship between GT and Landmark is not clearly expressed.
P7 line11 3:1:1: The proportion of test sets is relatively small.
P7 3.2 Table1: The comparative tests are a little sparse. The latest comparison is 2019. Is there an updated method to compare with the method proposed in this chapter?
P9 References: There are relatively few references."	The paper is well written and understood. It would be better that if the authors would detect the deformed beizer curve and estimate the midline shift. The midline shift is an improtant predictor for predicting the sevearity of the brain. The application of hough transform is interesting to see, but tracing the deformed curve is equally important.
248	Identification of vascular cognitive impairment in adult moyamoya disease via integrated graph convolutional network	"The major issue in the methodology section, is its descriptions of the node-based constraint item, especially the similarity constraint mechanism. The motivation to design the fusion of imaging and non-imaging features in such manner (using imaging feature as node and the similarity between two non-imaging features as edge) is not clear to me, as you can just simply concatenate them together for constructing the diagnosis model, which can also be effective and can fully utilize the two features. The computation of loss2 is also missing, which makes it difficult for readers to reproduce this work.
Table 1 shows the ablation study, where the last line seems to be the results of the proposed method, but it is unclear what the meaning is for that (1) and (2), which is also not clearly stated in the methodology and experimental sections. Therefore it remains confusing to me why the last line can achieve such a high performance compared with the others. Also, in some lines the performances from the testing set seem to be better than those from the validation set, which is also against my expectation, and it is suggested that the authors explain more about this.
The authors claim that they use 156 subjects for training, which might be sufficient for some conventional machine learning methods such as SVM, but definitely not for GCN. Limited training data will result in overfitting of the model training, which can hinder the credibility of the proposed method. To solve this problem, the usual way is data augmentation, from which there are already some ways available in this field. I am wondering if the authors realized such issues and if they have any solutions to them.
In Section 3.3, the authors provide the biomarker interpretations, from which they found some ROIs which are highlighted for VCI diagnosis, but it remains unknown if such findings are valid when there are no related references included in this paper to support them."	"1)At the beginning of the abstract part, it is suggested that the author briefly describe in one sentence what the existing methods for VCI research are usually and what are the shortcomings, and then lead to the research of this paper.
2)In the second paragraph of the introduction, the research on VCI using rs-fMRI and DTI and their shortcomings are introduced respectively. What is the traditional research on multi-modality data fusion, how to fuse multi-modality data, and what are the similarities and differences between the research in this paper and the previous multi-modal fusion research? The author can briefly explain.
3) In the third paragraph of the introduction, what is the traditional multi-modality method of the graph, and what are the advantages of the model proposed in this paper. The author can briefly explain.
4) In Section 2.2, graph construction based on rs-fMRI and DTI, the authors should indicate whether a fully connected graph or a sparse graph is used. In addition, whether fully connected graph or sparse graph should be reflected in Fig. 1 and Fig. 2. Besides, it should be stated whether a uniform graph structure was used for all subjects, or different graph structures were used between subjects. The authors can briefly explain how the FN matrix is calculated.
5) In section 2.3, how the input and output dimensions of each layer change, the author should give a brief description.
6) In the experimental parts of 3.1 and 3.2, because the sample size is small, the experimental results obtained by only dividing the training set, the validation set, and the test set cannot explain the performance of the model. And please clarify the usage of validation set in this work. More importantly, the author should use the cross-validation method and repeat the experiment some times to finally obtain the generalization ability of the proposed method.
7) In Section 3.3, please supplement the explanation of how the self-attention pooling mechanism preserves the important brain regions of the subjects."	The description of the method section is not clear.
249	Identify Consistent Imaging Genomic Biomarkers for Characterizing the Survival-associated Interactions between Tumor-infiltrating Lymphocytes and Tumors	"What is the magnification of the WSI?
The meaning of gamma, lambda, t, and beta in ImQCM should be explained.
Were there any duplicate patches across the two categories for the selected 200 patches with the largest tumor or TILs ratio?
The red/blue number labels in Fig. 2(b) patient A seem to be inverted. The black bounding box in Fig. 2(b) Patient B appears off."	"The paper is fairly well-written and the few suggestions that I have a on style and clarity:
1) The font in the figures is TINY! I suggest that the authors print one of their figures in Letter or A4 and try to read the figures. I have a 27 in monitor and had to maximise to be able to read. In some cases (fig 3) just making the figure larger would help, but it would be better to increase the font when the figures are produced. In Fig 1, the jet colormap for the 2 step is not good, better to change to greyscale so that the contrast between classes is clear. Again, try to print in a printer that is not colour to notice this would not print well.
2) The captions are just too short. Every caption should help to make the figure self-explanatory so that a reader can turn to the figure and understand it without having to refer back to the text, i.e. which are groups 1 and 2 in Fig 3? Add some insight in the caption, e.g., ""it should be noticed that ..."" 
3) With the tables, it is easy to get lost in the numbers. Please use bold to highlight the best results. Also use the caption to indicate which is the proposed method, this is normally added as the last one, especially since in the text all other methods are numbered 1-9, but in the table these appear in positions 2-10
4) The majority of the cases are censored, thus very few (10%) are used. What is the implication of discarding so many? What is the criteria to censor?
5) there are many typos and bad English (""patients, 46 of the them are non-censored patients..."")"	"Two minor Comments:

put space after commas and before '('. [e.g., in page7, methods(i.e.,OSCCA,
DGM2FS,SALMON)]
make labels in Fig. 2 and Fig. 3 clearer by bigger fonts."
250	Identifying and Combating Bias in Segmentation Networks by leveraging multiple resolutions	Additional experiments (drop Group H, and downscale Group H) could be helpful to examine the systemic bias due to image resolution.	"The authors illustrate the problem of image resolution bias in two different tasks. I've found the paper extremely well written, and portrays an important message for the MIC community. The paper though lack of details for methods and imaging setting used.
My concert regards this work relates to the first experiment. Which is the rationale of mixing adult and children's data for cortical segmentation? It is not specificized which image contrast is used for that task. Children of below 9 months may have quite different MR contrast (T1 and T2 are swapped).
Why not explore the issue on either children or adults only?
I think there is an error in the 80 scans split (adults children) as 50/10/30 is 90?
On the top of scale augmentations, which other augmentations were used in the scenario? Gaussian noise?
Which was the exact interpolation used for method c) ?
Would super-resolution techniques instead of interpolation do a better job?
I acknowledge the value of adding an additional downstream task. But is not clear to me the rationale for the task in classifying adult vs children based on cortical GM. Were those measures normalized from intra-craneal volume? It would have been more interesting to explore this task on patients vs healthy controls population for instance (AD vs controls).
I did not understood why adults overall seems to not be affected by the traiing/network strategy in the results, this resolution bias is then just and issue according to the targeted volume to be segmented?
I do not understand how in the classification task Table 1, first column, suddently classification improves up to 0.89 at the lowest resolution. But overall I've found this task not so pertinent to explore how bias propagates further."	"When training your models, I would have implemented early stopping to finish training rather than the fixed number of epochs the authors used. This could lead to either underfitting or overfitting. This is especially important as the authors used different architectures for their models.

Would be beneficial to include visual results of the model performance (and bias on low-res data)

It would be nice to assess the correlations between Dice, Hausdorff etc with volume bias

There are several available open-source (SOTA) CNN-based hippocampal segmentation models, the paper and the field in general would strongly benefit from comparing them to your augmentation or VINN models.

The figure labels are a bit crowded"
251	Identifying Phenotypic Concepts Discriminating Molecular Breast Cancer Sub-Types	The authors apply the TCAV method on a multi-modal training image dataset. This application is an important contribution towards interpretable identification of breast cancer sub-types. Figure 3 is a particularly good illustration. However, the font size on the radial diagrams is too small.	"This paper performed image classification on molecular sub-type classification for breast cancer patients. The proposed pipeline consisted of the classification using ResNet-18 pre-trained on ImageNet, the latent features were then extracted from the intermediate layers and clustered using k-means. Phenotypic Concepts were formed and weighting on these was calculated for each sub-type category.
Strength of this paper lies in the pioneering application of concept calculation on medical images for breast cancer patients, which is one step closer to find the co-relation between phenotypic traits and microscopic categories.
Weakness of this paper is really minor and some typos can be corrected during the proofreading process."	"Below, I comment on the paper in more detail:

I think important references to previous works are missing to justify the contributions and methods. For example, the paper [a] (details below) is an extremely relevant reference that should not miss in concept-based interpretability works. In this paper the authors explain the limitations of learning concepts directly in a model's latent space, without applying any transformation to make the space centered and orthogonal with respect to concepts. They demonstrate that an important limitation of CAVs is that two very different concepts may have very similar vectors (in terms of their cosine distance) simply because of how the latent space is organised (e.g. skewed and far from the center).

[a] Chen, Z., Bei, Y. & Rudin, C. Concept whitening for interpretable image recognition. Nat Mach Intell 2, 772-782 (2020). https://doi.org/10.1038/s42256-020-00265-z
Similarly, I think a discussion of related works based on concept attribution would have made the related work richer and more interesting. The work in [b], for example, shows how the binary concepts in CAV can be quantified and transformed into continuous-valued concepts. I think that knowing this work may have even further helped the authors to find ways to improve the current state-of-the-art methods and produce more insights about their work. The bidirectional scores proposed in [b] can illustrate if a given concept is responsible for an increase or a decrease of the probability assigned to each class, hence the authors would have been able to clarify not only what concepts contribute to the identification of phenotypes, but also in what way.
[b] M G, V A, S MM, H M. Concept attribution: Explaining CNN decisions to physicians. Comput Biol Med. 2020 Aug;123:103865. doi: 10.1016/j.compbiomed.2020.103865. Epub 2020 Jun 17. PMID: 32658785.
In addition to [a] and [b], the work in [c] proposed alternative scores then the TCAV score to overcome some of the limitations of TCAV.
[c] Yeche, Hugo, Justin Harrison, and Tess Berthier. ""UBS: A dimension-agnostic metric for concept vector interpretability applied to radiomics."" Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support. Springer, Cham, 2019. 12-20.

As a cascade effect, I think that by not including the reference in [a] the authors missed to addressed an important limitation of CAVs, namely that distant concepts may be pointed at by similar vectors. The authors could have addressed this limitation by adding, for example, an analysis of the cosine similarity between the vectors for each concept.
I think that more in depth discussions about the concept footprints would have been useful. What do the vectors look like for patients with similar footprints? How far/close are they in the latent space? Can it be that similar patient share similar footprints? Is it possible to identify concepts that generalise to the entire set of patients?
The quality of the figures is to improve. The text is too small to read."
252	Implicit Neural Representations for Generative Modeling of Living Cell Shapes	"The references are cited randomly in a disturbed order.
The best results in Table 1 are not annotated.
Section 2 only describe the proposed method."	"Discussion: it would be good to hear about the dynamics of new protrusions in the cancer cell case. This looks like a bifurcation or other non-linear state change. How does the model generate this effect?
Tables: uncertainty measures are good, thank you.
I wonder if in the cancer cell case there is a better measure than sphericity, one that quantifies the existence and characteristics of protrusions.
(2.1):
Re sigma in L_code: is this sigma related to the covariance matrix of the multivariate gaussian? If yes, how is it extracted from the covariance matrix? If not, a different notation would be clearer, since sigma routinely refers to std dev. Does it correspond to the std dev of the initializations?
(2.2):
Did the sine activation work any better than ReLU? Is the ""low frequency bias"" of ReLU an issue in this context?
(2.3):
""70%"": Is 100% preferable? Was 70% chosen solely based on particular RAM restrictions? Does 3 GB max out the GPU's memory?"	"It is unlikely that it will be possible for a more thorough evaluation to be completed against other methods during the rebuttal period but I would encourage the authors to consider this for any further journal paper. I would suggest considering at minimum whether this approach performs well against a classical level set approach, and how suitable the generated samples are for downstream tasks.
Further discussion of how architectural choices were made should also be provided (section 2.2). What process was followed to arrive at this architecture?"
253	Implicit Neural Representations for Medical Imaging Segmentation	Please check part 5 for more detailed information.	"The motivation looks like not reasonable. Compared with typical UNet, the method also still uses different resolution features without decoding layer-by-layer. It also needs big memory resource. Moreover, use some FC layers to incur more parameters.
The baselines are not medical segmentation related methods.
IF-Net has big advantages in reconstructing and smoothing. The work follows it, but doesn't  show the advantages in segmentation task."	"Section 2.1, ""we learn a continuous segmentation (occupancy) function si for each organ i \in {0, 1, . . . ,R} where R is the number of OARs."": Do we need to learn a continuous segmentation function for the background? If not, the value of i should start from one, not zero.
Section 2.1, Equation 3: Is the Dice loss calculated inside the sigma symbol? If so, the loss value could be very large. Do we need to normalize it to a fixed range such as [0, 1]?
Section 2.1, ""s_i are the predicted organ occupancies and ground truth segmentations"": ""ground truth"" -> ""ground-truth""
Section 2.2, ""The data sampling parameters are K=4000, L=4, and M=5000."": What's the unit of L? It is better to use another variable to indicate the padding size since L has been already used for the loss function in Equation 3.
Table 1 (left): Why do the upper and lower parts of the table list different sampling strategies? Why not evaluate all the four sampling strategies in both the upper and lower parts of the table? And the inconsistent number of sampling points of different sampling strategies may also make the comparison unfair.
Section 3.4, ""Although boundary sampling segments well around organ boundaries (see Table 1 (left)) ..."": This conclusion is hard to see from the number reported in Table 1."
254	Improved Domain Generalization for Cell Detection in Histopathology Images via Test-Time Stain Augmentation	"*This ensemble method may work well when each detection result tends to under estimation (there are false-negatives but few false positives). However, when each detection result tends to over estimation (has false positives), the final results also contain false positives, and thus it is not always right to improve the detection performance. How to control this?
*TTSA is defined in the caption of Fig.1. It is common that an abbreviation is defined in the main text when it first appears."	"I would recommend the following for future work:

The method should be explored for other applications apart from cell detection.

The significance of (4) and (5) should be proved by comparing it with the direct usage of random source image.

If (4) and (5) are necessary, different types of distance metrics should be explored to select the optimal one."	"The central part of this concept is fusion strategy that operates on the conjecture that model's predictions based on various source-target domain mixtures are likely to be suppressed if the mixture ratio is not right. Towards that, certain number of detected cells per group are fused based on their confidence scores. That is based on an assumption that improper mixtures will have poor confidence scores.
The hyperparameter of proposed concept is the number of detected cells with high enough confidence scores to be fused together. It set to 3 in the only experiment conducted in the paper. But no justification based on sensitivity analysis is provided.
Many sentences are extremely long what occasionally makes paper hard to follow."
255	Improving Trustworthiness of AI Disease Severity Rating in Medical Imaging with Ordinal Conformal Prediction Sets	See my weakness section.	"The evaluation of uncertainty estimation is biased. The radiologist checked only the cases with high uncertainty for potential issues, but the cases with lower uncertainty might have issues as well. Cases from low uncertainty regimes should be sampled as well and presented to the radiologist, and he should not know whether the presented cases were indicated as having ""high uncertainty"" or ""low uncertainty"" by the algorithm.
Ordinal APS algorithm, which is the proposed method, is only described in pseudo code without additional explanations. It will be helpful to describe the algorithm also in a paragraph text. The following was unclear in the pseudo-code:
Initialization: looks like in the beginning S is an empty set. It is therefore not clear how y' is selected.
How the pseudo-code makes sure the same y' is not selected twice?

Table 1 in appendix:
Inconsistent ""grading class"" order. E.g. ""Ordinal CDF"" appears first in the ""coverage"" row but last in the ""Size"" row. This makes results interpretation confusing.
The rows description of ""Coverage"" is too large visually and goes beyond the described rows.
It is not clear what the ""count"" row means in the stratification by Set size.
The title talks about stratification by true stenosis grading label, but looks like there is also a separate stratification by Set size

Figure 5:
Inconsistency in the chosen cases for radiologist's inspection - there are 5 stars in each category in the plot but in the description looks like only 4 points were taken from each category.

It is stated that ""LAC provably has the smallest average set size but achieves this by entirely ignoring conditional coverage, and does not consider ordinal information"". However, results did not showcase any limitation of the LAC algorithm and it was shown to perform similarly to the proposed method.
Minor grammar issues:
    - Page 4: the sentence ""In practice, therefore approximate Ordinal APS...""
The last sentence on page 6 is cut.
              
The first sentence in ""conclusion"" section: ""into"" should be removed"	See weaknesses section for the detailed comments.
256	Incorporating intratumoral heterogeneity into weakly-supervised deep learning models via variance pooling	"It is not clear why the authors chose the specific five cancer types from TCGA. They said the choice of these types was based on the number of patients. However, there are some other cancer types in TCGA with more patients than some of the selected cancer types.
The sentence below is difficult to understand. Please consider rephrasing them:
- Since the number of patches is different across WSIs, we cap - by random subsampling- the number of instances in each bag at the 75% quantile for that cancer type. The 75% quantile of what?
According to the results in Table 1, the increase of c-index is only 5.3% for the Deep Sets method on the COADREAD dataset, instead of 10.4% mentioned in the text. Please double check.
4 In conclusion section, no need to provide the full form of MIL again.
The number of patches (i.e., 75% quantile) used for training and the number of projection vectors are two key parameters. Sensitivity analysis of them should be performed.
Not clear how to incorporate VarPool into DeepGraphConv."	"Related work. I suggest the authors add more introductions about the selected three MIL architectures in the related work.
Implementation details. I suggest the authors provide more implementation details on incorporating proposed modules to different architectures.
Experimental settings. I suggest the authors do more ablation studies and explain their choice of some hyperparameters or network configurations.
Cross validation. Instead of randomly splitting the data 10 times, a more straightforward way to show the stability and generalizability might be 10-fold cross-validation.
Baseline experiments. For MIL architectures without attention mechanisms, it is not clarified whether the mean pool module is used or not. If yes, the performance gain from ""MeanPool"" to ""MeanPool + VarPool"" is more important to prove the effectiveness of the proposed variance pool module. If not, I suggest the authors add another baseline with only ""MeanPool""."	Could the authors expand on the ideas of interpretability as mentioned above?
257	INSightR-Net: Interpretable Neural Network for Regression using Similarity-based Comparisons to Prototypical Examples	The authors could provide an additional dataset, where regression is indeed the task to solve. Additionally, it could make sense to adapt their approach to ordinal classification. Both suggestions would make sense for an extended journal version of the work.	"Compare the proposed method with appropriate baselines.
Choose a reasonable data set that can demonstrate the validity of the method.
The authors should also clarify how the hypermparameters were chosen."	It would be interesting to include 'simple' methods such as linear regression in order to better highlight the benefits of the proposed approach.
258	InsMix: Towards Realistic Generative Data Augmentation for Nuclei Instance Segmentation	"Suggestions:

I did not fully understand how the background perturbation works, i.e. what do the authors mean by shuffling with a ration of alpha? Could you elaborate or provide a formula?"	"Comments:

(p.1) ""some studies [10,12,21] proposed to regress the distance map [...]"" there are more interesting and novel approaches than referenced [21] I would suggest substituting it with https://doi.org/10.1186/s13640-020-00514-6 and https://doi.org/10.1109/ICCVW54120.2021.00081

the text about the steps of method on p.3 is redundant
(p.4) ""The parameters [...] are determined by cross-validation"" - please provide further explanation
par2.2 (p.4) the background perturbation needs further explanation and/or reference to be properly proven (especially the last sentence in that paragraph)
The results in table 2&3 would be more reliable if they were cross-validated
it is unclear if the results presented in Table 2 were obtained with TAFE or Hovernet, please provide this detail
The results in Table4: the claimed increase in AJI value should be confirmed with proper statistical analysis, for now the increase is questionable
background perturbation in not proven enough to have positive impact on the results"	Please see the weakness section.
259	Instrument-tissue Interaction Quintuple Detection in Surgery Videos	I would suggest that the authors try to address the mentioned weaknesses. In particular, the authors should formulate all operations in the STAL and STAM, with a detailed description of convolutional and fully connected layers and feature maps' dimensions.	A literature review on both the generic object-object interaction graph representations and particularly tool-tissue interaction graph representation should be added. It should be clarified that a similar approach was proposed by some earlier works in the surgical domain, and the authors should distinguish their approach in comparison to these works.	"For me, this paper is more a ""pipeline"" paper, where several known methods have been stacked together for a specific application. Per se, that is not a bad thing, not every paper has to invent an absolute novel algorithm. However, I am not so impressed by the results, they seem more minor to me, compared to previous works. For me, it is hard to judge what ""effect"" this better results have on the surgery and the authors should comment on that. Another limitation is the dataset, which seems an in-house one:
""we build a cataract surgery video dataset""
""labeled frame by frame ... under the direction of ophthalmologists""
With this little information, it is hard to say how valid this dataset is. In summary, the paper is borderline for me.
Minor comment:
Mean Average Precision (mAP) should be defined earlier in the manuscript (actually it should be defined when it first appears in the text)."
260	Interaction-Oriented Feature Decomposition for Medical Image Lesion Detection	Please address my concerns.	"""We adopt a 3x3 convolutional layer (Conv3) on input features of the module to avoid the optimization effect of the localization branch."" What is optimization effect?
""The proposed IOFD network superiors the state-of-the-arts algorithms"" -> ""The proposed IOFD network surpasses the state-of-the-art algorithms"", ""two modal datasets"" -> ""datasets in two modalities"", ""valid set"" -> ""validation set""."	"Section 2.2, ""Firstly, We adopt ..."" -> ""First, we adopt ...""
Section 3.2, ""In particular, the recall is equal to the precision, which means the number of detections is equal to the number of ground truths."" What does this imply?"
261	Interpretable differential diagnosis for Alzheimer's disease and Frontotemporal dementia	"Introduction

When mentioning that other methods may be ""based on handcrafted features that may not fully exploit the image information"", it would be helpful to then give a few examples of features that these methods may be missing. It would also be helpful to also mention a few features that most methods take into consideration. After that, you could mention why this could be problematic (although this last part is not necessary).

Materials and methods

To help with reproducibility, you should talk about which methods you used for preprocessing. For example, for inhomogeneity correction, did you use FSL's inhomogeneity correction, ANT's inhomogeneity correction, or did you create your own intensity inhomogeneity correction algorithm? This should be done for each step of the preprocessing pipeline.
You mention that you used AssemblyNet for segmentation, however you don't give any details about the hyper-parameters used for this. You should mention this in a few sentences.
The author should include hyper-parameters used for the SVM, it may seem obvious, however you should write the number of hyperplanes chosen.

Interpretation of deep grading map

In the first paragraph, instead of saying 'regions around hippocampus', you should say ""regions included in the hippocampal formation"". I say this because, based on your Figure 2, it seems that the hippocampus, along with other areas around it, are highlighted in the grading map."	"I suggest adding quantitative results to the abstract
Justify why subjects with MCI were not included
What MNI space was used?
Indicate what software was used for pre-processing
What kernel did you use for the SVM? And how were the parameters optimized?
-In tables 1 and 2 I suggest adding the standard deviation, how many repetitions were made for the results of table 2? Same case for tables 3 and 4. Give details about the data used, standard deviation (if it is the case) and number of repetitions.
It is not clear how the data was partitioned in validation and final testing. Can you clarify this?"	"Integrating multi-modality MRI data (e.g., diffusion tensor imaging, functional MRI) might be helpful in further improving the classification results.
Probably the authors can use these open-access data to demonstrate that their method is useful for the detection and differentiation of dementia subtypes in the early stage, i.e., MCI."
262	Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis	"The abbreviations when first occured should be explained with full name.
It's not very clear for the weights for the losses."	please see above	"1, extending this method to learn subject-dependent masking matrices would be interesting.
2, It is unclear how the initial ROI embeddings were obtained. Given the limited size of labeled datasets, using self-supervised methods for pre-training may help to improve the tasks evaluated in this paper."
263	Interpretable Modeling and Reduction of Unknown Errors in Mechanistic Operators	"In the introduction, the authors claim that there has been ""substantial interest"" in combining deep learning with prior knowledge of imaging physics. However they only cite two examples. More review of the prior work is necessary.
Similarly, the method presented is not contrasted with prior approaches in the literature. Such a comparison would help evaluate the method's novelty.
More discussion of the sources of error and how they affect inverse problems in general would be helpful for assessing the applicability of the method.
The comparison to the DAECGI method given in the introduction is not very clear. It is important to make the comparison very understandable, as the present method seems to be a direct extension of the DAECGI method. It seems that the major difference between the two is that the presented method uses the both the original forward operator and the derived forward operator in inferring the latent vector z. The authors seem to claim that this is the innovation that allows for the sources of error to be identified.
The SOM method applied to the latent vector seems to me to be a post hoc method to add to the interpretability of the algorithm. From the description in the text, it seems like the SOM clustering is central to the generative model. I may not fully understand but it seems like the SOM is applied after inference. If this is the case, any clustering algorithm could be used. The SOM method is presented in the text alongside the generative model but it is not clear that the SOM plays a role in the generation of the forward operator. If it does, it needs to be clarified. If it does not it should be presented in a separate subsection.
I'm not sure I understand the motivation for using a U-Net as a generative model. It seems that any autoencoder architecture would be as applicable as a U-Net. Some motivation should be given for this choice. Furthermore, as the U-Net uses convolutions, it is not clear why the convolution operation is appropriate for the forward matrix H.
In equation (5) the authors add a hyperparameter to the loss function. With the beta parameter I don't think the equation is quite equal to the ELBO and I'm not sure that the bound in the equation still holds. I think the addition of the hyperparameter is fine and makes sense to control the degree to which the KL term effects the loss, however I believe some care should be taken to ensure that it is described in a way that is mathematically correct.

As the model is fairly complicated, I think the training procedure could use explicit description. The main text details how the forward and inverse problems are solved but I am unclear on how the U-Net and variational inference network q(z
Hi, Hf) are trained. Are the networks trained simultaneously with simulated ground truth forward operators and operators with errors? What are the settings used for the hyperparameters beta and lambda_reg? What are the architectures used for each component. These details would be helpful in the understanding of the overall method and increase its reproducibility. If there is not space in the main text they should be provided as supplemental materials.

The authors claim the cyclical inverse estimation procedure ""is expected to continuously reduce the error."" I'm not sure what this means. If the error is guaranteed to decrease they should explicitly state that. If not, they should also explain why the approach they have chosen is appropriate. This step seems important to describe in detail as it seems possible that a cyclical update such as this might not converge without some guarantees or some amount of appropriateness to the task. Convergence plots could be shown for these cyclical updates to verify the procedure is working as intended.
I am unable to understand what is being depicted in Figure 2. The images are small and difficult to read. In addition the visualization of the forward operator is not human interpretable. Instead the authors should choose to display some metrics that might more informative.
Similarly, Figure 5 is very difficult to interpret. I am not sure how to read these images.
In figure 4, the reconstruction error of the different methods is presented. It may be helpful to show the simulated and inferred u's if they are interpretable. This would show that the model accurately reconstructs the underlying latent cartographic source data and that the inverse solutions are as expected.
The paper contains spelling errors and awkward formatting such as ""variatioanl,"" ""gird,"" and ""Real Data : ."" The manuscript should be more closely spell and formatting checked."	"In this manuscript, a new model was proposed to reconstruct images by relying on the forward operator and residual errors simultaneously.
It is tough to see that the proposed model is more interpretable. The interpretability was not demonstrated in the manuscript. I suggest removing this word from the title. Otherwise, solid results demonstrating interpretability should be supplemented.
The evaluation of the model is not complete. More quantitative results should be added to justify the advantages of the proposed model.
The numbers and fonts in the figures are too small. It would be better to have larger sizes."	see point 3,4,5
264	Interpretable signature of consciousness in resting-state functional network brain activity	Authors performed the experiments using more data. The variation of functional patterns and associated activities across runs within same subject also could be examined for clarity.	"The paper only adopted an existing method without any improvement and the references listed in the paper are not recent literatures, which weakens the novelty of this paper.
Varol, A., Salzmann, M., Fua, P., & Urtasun, R. (2012, June). A constrained latent variable model. In 2012 IEEE conference on computer vision and pattern recognition (pp. 2248-2255).
The adoption of three atlases in this work is not well explained and ambiguous.
While the method can reveal the differences in network activities among different consciouness states, the inferred spatial patterns and dfifferent network activaties are group-wise, which can not be used, at least in current work, for subject-level recognition and thus limits the clinical application of proposed strategy."	"This was a solid paper that was very thorough, so there aren't too many changes to be made.
Methods

In the 'Atlases learned from the data' section of the 'ROIs definition' section, there was mention of using 'nilearn'. This should be referenced by (Python), along with the version.

You should write out the definition of the acronym CIVMR before using it. I believe it's (Center for In Vivo Microscopy), however, I am not sure where the 'R' comes from. This is based on the authors reference #8.

A suggested word change: Maybe instead of using the word ""perfect"" in the last sentence of the 'Discussion' section, use the phrase ""best tool thus far""."
265	Intervention & Interaction Federated Abnormality Detection with Noisy Clients	It is a very good approach and computational complexity of approach has to be mentioned	The paper represents the FADN task as a structural causal model, and identify the main issue of  recognition bias. Specifically,  two novel strategies are proposed to handle the applications of FL in abnormality detection.  The motivation is reasonable, the extensive experiments designed on the benchmark datasets demonstrate the efficacy of the proposed method. To further improve the quality,  the authors should provide more details of how to design and incorporate the strategies into FL model. Specifically,  model training is crucial to employ the proposed model,  more details and discussions are necessary. For instance, what is the  added computational cost compared to the common FL model.  How to define the training objective and whether it can be trained more efficiently?	"Ideas for an extended journal version of the paper:
It will be interesting to see the effect of different data partition strategies used to distribute data among clients (in the training stage, where data is fed into both local and global models- Section 3.1). Can a smarter partition (such that the confounder client is pre-identified) mimic the advantage of the SCM and interaction steps?
The authors may want to explore and clarify the difference in the source of noise- making the labels noisy versus making the underlying images noisy. While both will impact the final prediction accuracy, they may potentially affect the structural causal model's assumptions differently. They may alter the modularity or independence of each node via unobserved variables and add confounding association effects on top of causation effects.
In the interaction strategy (equation 5), while a good regularization is achieved, any truly exceptional (but legitimate) data points might be missed and the overall differentiating power of the local client will be suppressed. In such cases, the lambda_local value may be low and the global features will take precedence in the mixing. To counter this, it will be good to discuss this in the context of larger training data sets, more clients, or introducing semantic counterfactuals to increase the variety picked up and learned by the model.
In the ablation experiments, doesn't removing intervention effectively make the interaction redundant? If so, is this the same as a base federated model with no intervention-interaction component?"
266	Intra-class Contrastive Learning Improves Computer Aided Diagnosis of Breast Cancer in Mammography	"The marked lesion in Fig 1 is very hard to see. Please darken the contour to mark the region.
How do the Hard Negative mining examples vary as the learning progress. For instance, as separability improves, is it correct to assume that the triplet loss margins improve and the harder to distinguish (like the benign cases experiment) becomes closer in terms of their l2 distances in the embedding space? Can the authors comment on what they observed?
Authors use this LCL + NCL + classifier loss to improve their accuracy over the baseline when trained. But the details on how the overall weighted combination of the losses (i.e., were the losses weighted equally or something else) is missing. Further, during training, was there a training policy in place? I would assume one batch had lesion anchors, positive and negative, and the other would be normals? I would suggest the authors provide more information on that. I recognize that space is limited so it could go in the supplementary section.
In table 3 in the supplementary material, were the differences statistically significantly different?"	The authors can more clarify the usage description of the in-house dataset.	Overall, I congratulate authors for their work breast cancer, which is the leading cause of women deaths. The technique is well explained. The formulation of LCL and NCL is intriguing. The results presented in the paper shows the merits of this approach. However, The evaluation on independent test set in a prospective manner is important for testing the clinical feasibility.
267	Invertible Sharpening Network for MRI Reconstruction Enhancement	"1) The specific reasons for the increase in performance fore reconstructing MR images from undersampled data by replacing an existed network with an invertible network is unclear. More explicit explanations and rigorous experiments would be needed. In particular, it seems that the proposed model was built by simply changing the existing CNNs with a pre-existed invertible network.
2) The experiemtns were conducted only with cWGAN and InvSharpNet models and lacked comparison with other deep learning-based MR image reconstruction methods. To show the effectiveness of the invertible networks, more rigorous experiments would be needed by applying them to other deep learning-based MR reconstruction methods and compared with other methods.
3) Although adiologists' evaluation showed outperformed than the baseline in Table 1, it is hard to see the performance increment in Fig. 2 and 3. Especially, the difference between cWGAN and InvSharpNet seems to be very minor. In addition, the quantitative metrics that did not follow the trend of the radiologists' evaluation. Though the authors mentioned about this, but still it is not clear. The quantiative metrics are widely used for evaluating the reconstruction performance in various papers. Please provide more supporting results and discussion about this."	"(1)	Please fully investigate the relevant literatures and compare with them in experiments.
(2)	The generalization validation part is not very clear."	"Overall it is an interesting paper, and discussion on the visual quality is an important topic.
More insights on the InvSharpNet will strengthen this paper, for example:
-More discussion and analysis of how is this different from a loss function is necessary. Given that additional training and parameters are needed, is the proposed methods overweight the loss function? (such as perceptual loss etc.)"
268	Is a PET all you need? A multi-modal study for Alzheimer's disease using 3D CNNs	"1) MCI vs. NC is more challenging than AD vs. NC, why not conduct experiments on MCI vs.NC? And the paper lacks the description of MCI. 
2) The experimental results from Table 2 (CN vs. MCI vs. AD) cannot completely conclude that a single-modality network using FDG-PET performs best. Is the opinion too arbitrary?
3) In terms of quantity, the data with three labels is not balanced, how do you solve this problems?"	An argument against the presented findings is that a ResNet is not the correct model to learn structural pattern associated with AD. Indeed, current classification framework obtained better classification performance compared to results for AD classification using MRI readout only presented in this paper.	"(1) The authors design three fusion strategies to show the comparison results, and the results show that  FDG PET alone is sufficient for AD diagnosis. However, whether do the results rely on the current classification models? Therefore, more SOTA classification models should be investigated with only using FDG PET. 
(2) The current comparison experiment is comducted on one dataset. To effectively the effectiveness of  FDG PET, it is recommended to emply on more datasets.
(3) When using PET and random MRI, the model still obtains promising performance. However, whether does the  random MRI introduce incorrect information? More discussions should be included."
269	iSegFormer: Interactive Segmentation via Transformers with Application to 3D Knee MR Images	"Some details shoudl be added on what specific pretrained Swin transformer has been used. 
In page 6 the training set has 1521 images, which should not be correct since only 407 volumes are supposed to be used, with 3 images from each."	"There are a few places where acronyms are missing definitions or should have been defined earlier. For example, on page 2, it would have been nice to define ""STCN,"" and I believe the acronym for CNN (following paragraph) should have been defined earlier, as CNN was used in the second paragraph of the intro.
In page 3, you assert that U-Nets are not memory-efficient due to their symmetric encoder-decoder architecture. While I understand what you're saying, I'm not sure that symmetry between the encoder and decoder is sufficient for a network not to be memory efficient (in theory, I could imagine there could be such a thing as a ""memory-efficient"" symmetric architecture). There are works in the literature that support your point (i.e., that networks with a smaller decoder may be more efficient) - perhaps you could elaborate on this or cite one of these works (e.g., Paszke et al.'s ENet from 2016).
In Fig. 1, the arrow going around ""interaction loop"" seems to be going in the opposite direction of what is indicated by the rest of the flow diagram. Also, I believe there may be a typo in the iSegFormer architecture graphic, but I'm not sure. If this is for segmentation, shouldn't the output size be H x W x N_cls? (as opposed to H/4 x W/4 x N_cls)
In the first paragraph of your experiments section, it is unclear why you choose to focus on cartilage segmentation exclusively instead of femur or tibia segmentation. I understand your reasoning for choosing one for a short conference paper, but you may want to justify this decision in a larger paper.
I wasn't sure why slices per second were the metric of choice for speed instead of seconds per slice? Slices per second seem to imply that the process of loading new slices is implicated in the measurement of speed, although this seems like it would be independent of network architecture. If what we're trying to measure is the speed for the network to make a prediction on a single slice (I could be wrong), it seems like seconds per slice would be a more intuitive metric?
The cross-domain evaluation experiment was interesting and felt a bit strange at first. I didn't expect a model trained exclusively on COCO+LVIS data to generalize to medical imaging, let alone three separate tasks. You may want to discuss your reasoning for conducting this experiment in more detail for a larger paper. Would we expect to encounter a scenario where we need to train a model on non-medical imaging data and translate it, without fine-tuning, to the medical imaging domain? On an unrelated note, I appreciated how you still included results demonstrating that CNN-based models generalized better in this experiment than the transformer-based models.
In the cross-domain evaluation first paragraph, you say you trained models (in the previous experiment) using 1,221 slices. This seems to be at odds with your earlier statement that you used ""1521 training slices, 150 validation slices, and 150 testing slices."" Perhaps one of these numbers is a mistake?
In a larger paper, I think your ablation experiment merits more discussion. The HRNet32 model seems to outperform the Swin-based models in several ablation configurations, although the Swin-based models seem to achieve the best results overall (pretrained on ImageNet-21K with fine-tuning)."	"Minor comments:

Caption of table 1: say 'difficult cases' rather than 'hard cases'
Table 1 shows performance for 2D slices in terms of number of clicks required to obtain a pre-determined level of performance in terms of IoU. Table 3 shows 3D segmentation performance using different #2D slices to start propagation into 3D. it is unclear to me, however, which 2D segmentations were used to start propagation; the ones with 85% IoU or the ones with 90% IoU, or even 2D 'ground truth' segmentations? How does the location of the 2D starting slices impact the end result of the 3D segmentation?
Why use Dice to evaluate the 3D segmentations instead of IoU which was used earlier for 2D when determining #clicks? I'd like to see IoU (which I think is preferred over Dice but I know Dice is used extensively) along with Dice.
Related to the clinical utility comment above under weaknesses, could you comment on ""how good is good enough"" for this segmentation problem? There usually is a substantial inter-observer variability when determining a reference standard for segmentation problems and it would be good to know how your method compares to the variability in the reference standard itself, e.g., if the average IoU for the 'ground truth' of 2 clinicians is only 0.6 then your requirement of 0.8 or more may be making things unnecessarily difficult."
270	Joint Class-Affinity Loss Correction for Robust Medical Image Segmentation with Noisy Labels	"More detailed descriptions should be given to elaborate how P' is computed. Why does it only reveal intra-class relation? And how will its reverse version captures inter-class relations?
More ablative experiments should be conducted in terms of without intra-class relation learning, without inter-class relation learning, without class-level loss correction, and without affinity-level loss correction.
Since P' is normalized, it may be not necessary to normalize for P_re'.
Fig. 2 should be improved. It is not easy to be recognized from the figure about how P' is derived.
Eq.1 is very similar with GCNs. Does it perform in an iterative manner or just one-step inference?"	"(1) There are typos in the paper. For example, in equation 1 the P(k1) should be P'(k1). On Page 7, ""results in Fig. 3"" -> ""results in Fig. 4"". On page 8, ""curves in Fig. 4"" -> ""curves in Fig. 5"".
(2) Additional ablation studies are needed to illustrate the benefits of Class-Affinity Consistency Regularization.
(3) The writing of the paper should be improved. For example, it is necessary to clarify the ""inter-class"" and ""intra-class"" concepts. Also, why would the affinity label reduce the noise rate (see Fig.1, page 2)?
(4) For completeness, it is better to also compare the proposed method with [15] and [20] in the experiment."	"Please include a computational complexity analysis of the proposed method.

For the visualization comparison (Fig. 4 in the main paper and Fig. 1 in the supplementary material), it would be good to include some discussions on how the proposed method outperforms other comparison methods."
271	Joint Graph Convolution for Analyzing Brain Structural and Functional Connectome	Please refer to sec.5	"1)	The coupling strength seems show a similar pattern with a typical spatial pattern of structural strength. The authors should test the correlation between coupling strength and raw SC/FC strength across nodes.
2)	Whether coupling strength is stable in adult brain network is still a question. The authors should use HCP data to reproduce the analysis.
3)	The coupling strength should be corrected by a random label task.
4)	The influence of network thresholding should be taken into account."	"1) Please analyze why only the edges between the corresponding ROIs are included as the inter-network edges. 
2) Since different partition of the data may lead to significant different average performance of the n-fold cross-validation, at least 5~10 times of n-fold cross-validation is recommend to get a relatively objective assessment of the method. It may be better to include the standard deviation of the cross-validation results in the comparison, not only the mean.
3) Please explain why the correlation of the predicted and real age is pretty low (~0.38) comparing with the results in other cohorts (Infant: ~0.85, Adult:~0.89).
4) Why the coupling strength between SC-DC learned in the gender prediction and age prediction is similar should be simply analyzed. It is interesting to know why the gender related-feature is highly correlated with the age related-feature (time-variant)."
272	Joint Modeling of Image and Label Statistics for Enhancing Model Generalizability of Medical Image Segmentation	"Title is hard to make sense of before reading the paper.

Bayesian segmentation or BayeSeg is a very general name. Consider coming up with something more specific that actually describes what the model does.

The Introduction is repeated somewhat needlessly in the Methodoly section.

I find what the image is decomposed into somewhat unclear. E.g. what is the basis and contour? Some intuitive explanation could help.

Similarly with the graphical model, what is the line and boundary? What is the reasoning behind these variables?

What is the matrix D_x in practice, can you give an example?"	"Just a typo to correct on page 6: replace ""underwent cardiomyopathy."" with ""suffers from cardiomyopathy"".
I don't have any other comments for this manuscript.
For future work (beyond MICCAI), it would be nice to test on larger pools of data, to see whether these benefits still reproduce (or could be mitigated by brute force ""no data like more data"") :-)"	"The author proposed a  deep learning-based Bayesian segmentation framework that decomposes an input image into components of contour and basis. The segmentation label is then inferred from the contour component, which varies less across different MRI sequences.
Unlike conventional Bayesian methods, the framework is implemented with neural networks, where three CNNs were trained to infer the posterior distributions.
The author evaluated the proposed solution on public databases, significant improvement in segmentation accuracy was observed.
I believe the proposed method is a novel approach and addresses an important challenge in the medical image segmentation field.
The theoretical analysis of the framework design seems to be relatively thorough. And the use of CNNs for posterior distribution inference avoids some technical challenges that could be difficult for conventional Bayesian methods. 
Quantitative evaluation of the proposed method was done properly.
Even though the paper is relatively well written, I found some detail about the network training is missing, which could be helpful for the reader to understand the proposed method. The author used 3 CNNs to infer the posterior distributions. However, it is not clear whether the three networks were trained together in an end-to-end fashion (which seems to be what the text implies) or trained separately or in an alternating order with individual ground truth computed from ground-truth segmentation(which is easier for me to understand). I believe the latter is more plausible because, in an end-to-end setup, there is a lack of regulation to force the two ResNets to focus on either high-pass or low-pass components.
Another missing detail is how to generate the final segmentation labels. As the U-Net outputs distributions, a post-processing step is needed to convert the distribution to segmentation labels.
Finally, as the contour and basis decomposition remind me of the high-pass and low-pass filters, it might be good to include a baseline U-Net trained on high-pass filtered images to demonstrate the strength of the proposed method."
273	Joint Prediction of Meningioma Grade and Brain Invasion via Task-Aware Contrastive Learning	"This is very good work based on a fairly large data set from one institution.
I think future work based on this might benefit from:

validating with prospective data not in training and testing data.
adding other brain tumor types
the addition of rCBV calculated from MR DSC. If MR DSC was collected for these studies during contrast administration, then rCBV could be added as an input as it has is known to identify invasion in other brain tumors ]L. S. Hu et al., ""Accurate Patient-Specific Machine Learning Models of Glioblastoma Invasion Using Transfer Learning,"" AJNR Am J Neuroradiol, Feb. 2019, doi: 10.3174/ajnr.A5981."	"There are instances in which the methods section could be made clearer. Most significantly, further rationalization for the contrastive loss and alignment of the task-common representation should be provided in addition to details on how that alignment is performed. Furthermore, a rephrasing or additional discussion of the purpose of the auxiliary classification loss should be added for clarity.

In section 2.1, the authors mention that they adopt a convolution layer and an average pooling to realize feature disentanglement. What is the rationale behind this?
In section 2.2, the authors mention that they align the task common feature to the task specific feature. My understanding for task common features and task specific features is that they dont intersected with each other. Why do the authors think the task specific features can be transformed from task common features? Can the authors justify this?
If the task common features can transform into two different types of task specific features (one for invasion, one for grading), why don't we use different features in the beginning of the feature extraction step? My concern is that since we want to generate two task specific features, why do we need to generate them from task common features?"	"The paper is interesting from a clinical point of view and the proposed method is simple yet not trivial. 
But the rather weak statistical analysis does not make the paper completely convincing. Generalization is evaluated by splitting the dataset randomly in training/validation three times (and only mean AUC on validation is reported). Repeated cross validation could have been used for this purpose. This makes the evaluation of the improvement of the proposed method over sota or simpler approaches difficult as the numbers (e.g. AUC) are quite close. Significance tests (e.g. DeLong's) could have been used. Some reported metrics like accuracy are not really relevant for such imbalanced datasets, other ones (MCC) could have been considered and confidence intervals should be given as well."
274	Joint Region-Attention and Multi-Scale Transformer for Microsatellite Instability Detection from Whole Slide Images in Gastrointestinal Cancer	"It will be nice if the author can visualized the token attentions on top of the WSI image to show what has been attend to by transformer. 
Cropping patch from image region and extract CNN features is quite intuitive and standard. I would suggest authors simplify the method in this part and give more details on other aspects such as how Epos is designed - is it in WSI space or region image space? Is Epos different between x20, x5, and thumbnail? If so, how?
Section 2.2 is kind of duplicated to 2.1. It can be simplified or combined with 2.1 to make more room for other more critical contents.
Minor: 
Some of the references seem to be missing in the paper, for example:
... state-of-the-art method DeepSMILE [] by 0.18%.
... MSI detection methods for comparison [].
Typos:
... where c is the number of feature map channels.."	"I appreciate the work done by authors in this paper, and I think it is in general a solid work.
The authors should clarify better the classification goal of the method (number of classes): my understanding is that it is a binary classification problem positive or negative for MSI, which I beleive is not clearly stated in the paper.
There are repetitive sentences in the paper that can be removed, for example the showt summary of RAMST is reapeted in the abstract, the introduction (page 2 last paragraph), introduction again (page 3 second paragraph), method last paragraph
Page 4 line before eq (1), M'{fwus} should be X^P{fwus}"	Nothing
275	Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification	Please see 5.	"The main contribution is to proposed a kernel attention transformer. However, such motivation is not clearly presented in the paper. The authors claimed that ""It makes the KAT be able to learn hierarchical representations from the local to global scale of the WSI, and thereby delivers better WSI classification performance."", but we could not understand such novelty in the Fig. 1 to see why such kernel attention is useful for WSI classification.
Also, the authors didn't explain Fig. 1 clearly in the methodology section. For example, how to choose the anchors on the WSI and how to perform soft-masking and generate anchor-related masks shown in Fig. 1(g).
In table 2, we could see KAT cannot perform better in Gastric-2K dataset compared with TransMIL in terms of accuracy, sensitivity and specificity in different tasks. Also, it is not clear if results in Endometrial-2K datasets are significant better than baselines. Also, the proposed KAT is not the fastest method and thus it is not easy to understand how the KAT could be efficient and effective.
Std values should be reported as the authors said they used 5 fold cross-validation. ROC curves should also be presented and statistical tests could be done to test curves of each model.
Attention-based MIL model should be discussed and compared because they have been widely used in WSI classification and diagnosis. 
""Data-efficient and weakly supervised computational pathology on whole-slide images."", Nature BME, 2021.
""Whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks."" Medical Image Analysis, 2020."	"""is the most appreciate for the dataset."" - ""appreciate"" should be ""optimal""?
Very little worth adding - this is a great paper and I won't waste our time trying to find any further niggles."
276	Key-frame Guided Network for Thyroid Nodule Recognition using Ultrasound Videos	"1.The number of frames to be taken around a keyframe has been empirically selected as 32. It is recommended to give some explanation on this choice, in terms of the computational cost, performance, etc. 
Besides, I am curious what if the difference of two detected keyframes frame index are no more than 32? Would there need to take some actions to handle such cases. 
2.As illustrated in Figure 1, frame-index of the detected nodules is hard-encoded as a feature component of the nodule information. Is this kind of hard-encoding suitable? Would the variation of ultrasound video lengths effect this feature component?"	"Section 2, Key-frame localization: As the Faster-RCNN is not introduced before, an explanation about the main characteristics of this network and why you select it as your detection model is missing in the manuscript. Also, it is not clear if you adopt it to your aplication or if you used it as it is proposed. Please clarify this point.
Please explain what IOU similarity is.
Section 3, Dataset: Please clarify how the dataset's size has changed after data augmentation.
Section 3, Experimental results, Table 1: It is not clear if the listed state-of-the-art methods also used the same dataset as you or not. please clarify this.
Section 4, Conclusion: As there is no information regarding the computation time, it is difficult to evaluate the performance of the proposed approach in the context of real-time application inside clinical settings. I highly recommend you to add a discussion regarding this point and also focus more on the clinical significance of the proposed approach."	"The Motion Attention module only has incremental innovation as compared to the similar temporal attention module in [2], using motion speed to replace video brightness as the radiologists' attention indicator.
Because the baseline methods may preform worse on the self-collected dataset, additional experiments should be conducted to compare the baseline methods with the proposed method on other datasets, e.g., the used datasets reported in the baseline method papers.
The authors should provide a complete description of the data collection process, such as descriptions of the experimental setup, device(s) used, image acquisition parameters, subjects/objects involved, instructions to annotators, and methods for quality control."
277	Knowledge Distillation to Ensemble Global and Interpretable Prototype-based Mammogram Classification Models	In my opinion this is an excellent work and I do not have any suggestion for improvement.	Although this paper is well-written and efficient, there lacks of important ablation study regarding the hyper-parameters, such as the hyper-parameters in Eq. (1)-(3) and the number of prototypes. I understand such experiments might be missing due to length constraint of the MICCAI conference. However, I think it would be better if the authors could provide more detailed ablation studies in the supplementary file.	"I would encourage the authors pay attention to the PDF formats.
The authors include several weight parameters (alpha, beta, lambda 1/2), it should also be critical to show the hyper-parameter tuning analysis. Loss curves for each term could also be favorable.
The current project is a binary classification problem. The authors could consider studying a multi-category classification task and see how the ProtoPNet++ perform, especially on some public datasets competitions (CheXpert, ROCC etc.)."
278	Landmark-free Statistical Shape Modeling via Neural Flow Deformations	"The innovation of the paper is weak
How about the time complexity about your model compared other methods.
Comparative experiments are slightly inadequate
There are many expressions in the full text that I cannot understand
Such as the last two lines in page 7, and so on. Pls revise the manuscript carefully."	To improve their paper, the authors should i) compare their approach with shapeflow ii) detail more the implementation parameters of their method iii) the computation times of the method	Authors should provide more details that allow fully reproduction of the method. The method is very well described in a general overview. It is elegant, but more details are needed to fully undersand the math background.
279	Layer Ensembles: A Single-Pass Uncertainty Estimation in Deep Learning for Segmentation	"The computational analysis (such as number of floating point operations, computation time, and speedup) is desired to show the efficiency of LE vs DE.
It's unfair to only apply STAPLE on LE.
In Methodology/Ensembles of networks of different depths part, the authors claim that LE is equivalent to DE, which is not very solid. More evidence or theories are needed to verify this claim."	The idea presented in the paper is very interesting and finally challenges the standard ensembles which are state-of-the-art for uncertainty estimation for segmentation. For this reason, in order to show the benefit of the proposed approach in a strong way, i would like to see more comprehensive compariosns to multi-headed networks as well as runtime comparisons. This way the paper can be strengthened.	"Other noise corruption in segmentation difficulty evaluation. In addition to Gaussian noise, the evaluation could be further evaluated on convolution with a filter kernel or intentionally corrupt the boundary region to mimic more challenging segmentation scenario with lower uncertainty.
As another the-state-of-the-art, it will be great to also compare with MCDropout.
The randomisation of data in testing can be improved by cross validation.
In MnM result of Fig.3, there is a range (0 to 0.1) where DE-LV is lower than LE-LV. It will be great to add more discussion or justification on this issue."
280	Learn to Ignore: Domain Adaptation for Multi-Site MRI Analysis	"the data sampling algorithm is only mentioned in the supplement, it would be nice to explain the data sampling in 1-2 sentences in the paper, so the reader doesn't have to switch to the supplement.
did the authors try to train the center points based on target and source domain instead of just the target domain? This would be an interesting comparison.
in contrastive learning the projection subnetwork (layers after the ResNet that project the feature vector to dim 128 in this paper) is usually discarded after the contrastive learning, as it leads to some information loss (see SimCLR paper), could the authors explain why they chose to keep these layers?

Chen, Ting, et al. ""A simple framework for contrastive learning of visual representations."" International conference on machine learning. PMLR, 2020."	As stated above in strengths and weaknesses.	"Remove the first parameter for classification loss.
Add some discussions about influence of the key parameters on the classification results."
281	Learning Incrementally to Segment Multiple Organs in a CT Image	I would suggest to the author to do a down-to-earth explanation of everything before  writing formulas. What are you trying to do, intuitively? Also, they should try not to leave anything to the imagination of the reader.	Please refer to Section 5 - main weaknesses	"The method is of high interest for the medical image analysis community. The submitted paper is innovative and very well written. The following comments could be taken into account for further improvements.
Main comments:
1- In Sect. 2, the multi-teacher single-student knowledge distillation (MS-KD) framework proposed by Feng et al. should be mentioned as related work in the paragraph entitled ""MOS with partially labelled datasets""
2- $L_{Marg}$ and $L_{Exc}$ from Shi et al. [21] should be explicitly defined and linked to Eq. 1 and 2. 
3- You should explicitly write the global loss and mention how all the sub-losses are weighted
Minor comments:
4- The last sentence of the paragraph ""Framework of IL"" (Sect.3. 1) is unclear and should provide the definition of $\Theta_{t}$.
5- Dice and Hausdorff distance comparisons between the proposed approach and existing incremental learning methods - especially MiB [2] - could be confirmed using a statistical analysis through t-tests
6- The comparison with and without memory module report the same performance in both configuration. Your assumption to explain the instability (various field-of-view) should be confirmed by using other datasets. 
7- Be careful with the differences of writing between Fig.3 and Eq. 4, 5 and 6 for $l_{mem}$, $l_{same}$ and $l_{opp}$
8- $b$ (background) should be defined in Sect. 3.1 and not in Sect. 3.2 as it is used in both Eq. 1 and 2"
282	Learning iterative optimisation for deformable image registration of lung CT with recurrent convolutional networks	"I like Figure 2. However, the authors do not explain what the ground truth reference is that they are using and how they got this ground truth. Without this information it is not clear whether or not this visualization is biased or not.
Pg 6. The authors mention that their method produces smoother transformations and less folding compared to Adam optimization.  The authors should report how much folding occurs in both the Adam and the L2O approaches.
Fig 3. The authors should state what the shaded regions of the graphs represent and how they are computed."	"Major point:
(1)	It will be better to more clearly discuss the relationship between the hidden states in this paper and the first and second momentum in Adam optimization.
(2)	It will be better for the authors to add comparisons of running time.
Minor point:
(1)	Eq. (1) should be improved.
(2)	One more comma at the end of the first sentence of the caption of Fig.1.
(3)	'For each sample coordinate a dissimilarity cost...' should be improved.
(4)	'...are used to asses the registration accuracy...' should be improved."	The authors should improve the motivation behind the development of the L2O method. Perhaps it would be interesting to look at the actual registration outputs and the diffeomorphism of the displacement fields.
283	Learning Robust Representation for Joint Grading of Ophthalmic Diseases via Adaptive Curriculum and Feature Disentanglement	"Dear Authors,
I read your manuscript with great interest and I found it of excellent quality. Also the results are quite impressive and opens the field for further improvements.
I have no major concerns for this manuscript.
The only suggestion I have is to improve the introduction in order to better state the unique challenges are associated with this task and to provide a deeper overview of the study."	"The abbreviations (YAW) and (DETACH) are quite unrelated to the full words, so consider make it clearer.
In the introduction, redefine (DR) and (DME) again.
In Methods, Fig. 3 is referenced before Fig. 2. Reorder the figures in the paper.
In Fig. 3, please put details of the network architecture, e.g., feature map sizes, and size of fully connected layer, or mention them in text.
For all the tables, please, put vertical separators between different datasets and diseases to make the tables more readable."	the dataset should definitely be properly introduced.
284	Learning self-calibrated optic disc and cup segmentation from multi-rater annotations	"Just some minor problems:

references should be checked, such as the redundancy of 20 and 21;
typo such as ""This enable us to supervise DivM..."""	"The three contributions listed in the introduction sections are all about the methodology. I would suggest the authors to reorganize this part and add experimental validation as one of the contributions.
What are the number of recurrent steps used for each fused label sets in Table 2 and 3? Is it needed to tune this hyperparameter for each fused label set?
For future work, it can be valuable to extend this self-calibration approach to the multi-rater challenge in other imaging modalities, such as MRI and histopathology images.
Minor: 
4a. Formatting: Space is missing between the text and the square parenthesis for citation
4b. Grammar: On Page 2 ""... which aware the uncertainty .. "" -> ""... which is aware of the uncertainty ...""
4c. Page 2 ""potential correct label"" -> ""potentially ..."""	"There are multiple language errors in the work. Please consult with an expert. Few examples:
""Comparison with SOTA"" instead of ""Compare with SOTA""
Table 2: ""Calibrated"" instead of ""Calibrate"", ""Not Calibrated"" instead of ""No Calibrate""

It is stated that the calibrated segmentation and multi-rater expertness converges to an optimal solution. However, since the update (equation 2) is a form of alternating minimization algorithm, looks like it might also end up in a local minimum and not necessarily a global minimum. It will converge to a global minimum if W and V are convex measures, but it is not clear if this is the case.

Evaluation metrics: it is not clear why ""self fusion"" is used as an evaluation metric when it is part of the method itself. Also, the given evaluation metric of ""Diag"" is actually another method for combining multi-rater annotations and should be compared to as well. On the other hand, the common simple multi-rater evaluation metrics ""Random"" and ""Average"" are missing.

It is stated that ""self-calibrated segmentation consistently achieves superior performance on various multi-rater ground-truths"". In most cases this statement holds, but not always.

This sentence is not clear as there is no reporting of AUC: ""The performance improvement is especially prominent for OC segmentation where the inter-observer variability is more significant, with an increase of 1.83% and 1.72% AUC over current best method on REFUGE-MV and RIGA-MV, respectively."""
285	Learning shape distributions from large databases of healthy organs: applications to zero-shot and few-shot abnormal pancreas detection	I suggest the author explain contributions more clearly in this paper. 	Add more related works.	"I would have wished for a comparison with other shape encoding approaches, especially with shape models.
Releasing the code would further improve the importance of the paper."
286	Learning towards Synchronous Network Memorizability and Generalizability for Continual Segmentation across Multiple Sites	"Most of the comments are already expressed above in the section highlighting the limitations of the work. I add below a couple of minor comments that the author might also like to address.
Minor comments:

Training on sequential data streams is not necessarily privacy preserving. Differentially private mechanisms would be needed to demonstrate this. Also, streaming data often poses more of a privacy risk (data-not-at-rest) than centralising the data at rest.
Authors fail to recognise the existence of Federated learning as an approach. While Continual Learning is an important area of research, the justification for CL vs FL in a medical setting is not provided or asserted.
Would be interesting to see the behaviour of the method when the task is actually changing between sites, eg. if the model starts learning to segment prostates then learns to segment livers, does it still remember how to segment prostates?
Would be good to see a comparison to the model performance if all the data was co-localised as the optimal performance. In this setup it is hard to know if the demonstrated performance is good or not."	The paper is overall solid, with a little of concern on its novelty (little connection between the DG and CL solution, seems like they are just independent methods combined to solve their own problems). Please see my weaknesses sections for detailed concerns.	"Very nice work. Well written and organised with a novel approach. 
Some suggestions of further work can be: i) the application of the idea in further internal and external datasets to capture the behaviour of the learning approach in different domain shift effects of external cohorts. ii) test the learning approach in different segmentation networks to capture the variation of the results compared with SOTA learning approaches (CL, DG etc)."
287	Learning Tumor-Induced Deformations to Improve Tumor-Bearing Brain MR Segmentation	"I'm not convinced that the inter-hemisphere symmetry validation is very insightful. For such big tumours it seems to be near impossible to attain true symmetry (since one would have to majorly deform the image to eliminate the tumor). The reported symmetry improvement is very small. Is this meant to capture the effect of tumor-induced deformation on the rest of the brain only?

Small comments:


Please clarify the use of the parameter tau when defining the displacement field and stationary vector field. I don't see why this scaling factor is needed when def(q
P) is given by the network, which would hopefully just return the correct deformations directly. Why does it make sense to have a user adjust the degree of deformation?

In the introduction, consider rephrasing ""grows a tumor in the atlas is still highly complex and challenging"" and ""warping (grow a tumor in) the brain atlas to reflect the tumor and spatial changes in the patient image"" because these phrases can sound very similar to what you're doing. Also avoid criticizing ""these approaches require a good tumor segmentation"" because your method does too.

Let the tuple... before eqn (1): there are two errors, it should be D_i(q_j^(i)
j= [1...J] - i.e., subscript (i) and the ... between 1 and J.

Section 3.2, ""Our method, including the baseline, significantly outperforms plain SAMSEG"" - I wouldn't use terms like ""significantly"" if you're not doing a statistical test.
Figure 5 is hard to understand without ground truth segmentations, which I understand may not be available. Can you use arrows or boxes to highlight the main areas the reader should look at and give more information about these areas in the caption or main text?
Some typographical errors, paper needs a grammar and spelling check"	"The method relies on initial tumor segmentation. How sensitive the method is to this initial segmentation?
Please provide a measure of variability for the provided Dice scores.
What does the author mean by weighted-mean-Dice? How was it calculated?
The authors should quantitatively evaluate their method on real data and compare it with some of the state-of-the-art methods."	While a realized that adding the comparison with another method would be a lot of work, I would recommend at least adding to the discussion of the results (ex if there are any insights into why SVF representation performs worse in experiments 3.2; SVF guarantees a valid diffeomorphic field and adds implicit regularization ; would a PDE model-based method would perform similarly ?)
288	Learning Underrepresented Classes from Decentralized Partially Labeled Medical Images	"The algorithm 1 is too simple to represent the method, I can't understand the method only by the code provided in algorithm 1. I suggest the author to include a flowchat/figure in the paper to better present their method.
In table 2, is the prototype based inference adopted to all the method, is the energy calculated for all the methods during inference? Or just FedFew? What is the difference between 'FedFew w/o EBM' and 'NN (MLC w/ FSSL)', and what is the factor that bring so much improvements between these two methods?
Why (Cc + 1)-dimensional vector important?
'We sample 10 negative examples and 10 positive examples to simulate the class imbalance for UCs'. What's this?"	"major points:

Section 3.3: a 0-th class is used to determine whether the image contains any CCs. Then how would one know if an image contains any CCs if this image is from the UC clients?
Section 3.4: Doesn't passing meta data from local servers to the parameter sever violates the privacy policy?
Table 2&3: The FSSL step seems effective for common classes but not useful for the underrepresented classes, which might be the main focus of the paper?
minor points:
""We first train an MLC model for CCs Cc"". It's hard to understand what is CCs Cc.
Table 1: what are RN and DN?
Table 2: it's recommended to explain A,P,R,and F in the table caption."	As shown in the weaknesses.
289	Learning with Context Encoding for Single-Stage Cranial Bone Labeling and Landmark Localization	See above	"The qualitative evaluation results should be reported.
More relevant methods should be included in the comparison evaluation."	"The authors should clarify if any pre/post processing was performed for  the images and the labels, along with any augmentation strategies that were utilized (if any).
Moreover, the authors should specify the list of all the weighting parameters, from which the final values were empirically selected.
Finally, the authors should consider adding visualizations of  the images and its corresponding predicted segmentations and landmarks for all the methods compared."
290	Learning-based and unrolled motion-compensated reconstruction for cardiac MR CINE imaging	"This is very nice work.
I would recommend improving the competing results with elastix for a more fair validation."	"This paper is written very well. So, I recommend it for publication. However, 
one minor remarks: if I am nor wrong, references should be numbered consecutively in order in the text. so, please check the offical template of the MICCAI."	"The current reconstruction method is using CG-SENSE. With the progress in DL based reconstruction, authors might want to undersampled MRI reconstruction using deep learning based techniques.
In future work please consider testing on some prospectively undersampled data.
Although computational metrics are important indications of the method's performance, would recommend having radiologist ratings for future work."
291	Learning-based US-MR Liver Image Registration with Spatial Priors	"The whole system is lack of technical novelty as described in the weaknesses part. Moreover, considering the whole registration workflow consists of several procedures, the errors may be accumulated to reduce the final performance.
The motivation of the proposed initial alignment is not strong enough. Considering the registration is conducted preoperatively and in 3D space, in my opinion, the initial rigid alignment is not a challenging problem and can be achieved by conventional registration networks. The proposed initial alignment may be tedious.
The authors used coherent point drift to register the surface point clouds from MRI and ultrasound images. The surface point clouds were generated from the segmentation results, which may contain incorrect segmented boundaries. However, the CPD is not robust to noisy point clouds, thus may result in inaccurate alignment.
In data and material section, the authors mentioned ""from 26 subjects..."", ""Our MR dataset consisted of 67 liver abdominal..."", ""In these datasets, we have 18 subjects..."", I'm confused with these descriptions.
In FCN-Based Tissue Feature Extraction section, why the authors call the segmentation as feature extraction?
There is lack of comparison between baseline and current state-of-the-art registration methods."	It would be great to compare this approach to alternatives.	-
292	Lesion Guided Explainable Few Weak-shot Medical Report Generation	"Too long introduction is unnecessary, since ZSL is not highly relevant to this paper.
Problem setting is unclear.
Some symbols lack corresponding explanations and some symbols may be reused. Authors need to check them carefully.
Fig. 2 does not show weight imprinting scheme and is lack of explanation of virtual, solid lines and color.
There is a lack of discussion of some important parameters."	Please address my questions listed above.	"Please comment on the implementation of this methodology between two classes where feature overlap is not significant.
The BLEU scores of the the new methods outperforms other ablation methods but is still moderate when looking at the absolute value. Are there strategies to improve the absolute score?"
293	Lesion-Aware Contrastive Representation Learning for Histopathology Whole Slide Images Analysis	See above.	"As the WSI labels are weak supervision, it would help to provide more analysis about the risk of overfitting the noisy pseudo labels.
To what degree the class collision problem is mitigated should be discussed and analyzed, e.g., the T-sne visualization.
More explanations of settings:
(a) The expected distribution is queue refinement strategy, where the similarity is 1 and 0 for same and different pseudo-labels, respectively. However, the ideal similarity between instances with different labels should be -1 under cosine similarity.
(b) The threshold in eq.5: As the instances in mini-batch are randomly samples, how to understand the average KL divergence?
(c) The same length of lesion queue for each class: As the class imbalanced problem (metioned in section 4.3), the update of each queue may be inconsistent under such settings.
More details of experiments:
(a) As the authors also split a  validation set, it would help to explain how to use it in self-supervised learning for early stop?
(b) The authors should clarity the evalution protocal, i.e., fixed or fine-tuning.
(c) The authors are encouraged to explain why the MLP is of fc-BN-ReLU-fc (BYOL style) instead of  fc-ReLu-fc (MoCo v2 style), as the MoCo v2 is chosen as baseline.
Better experiment analysis in section 4.3."	"For the title, ""class-aware"" is more suitable than ""lesion-aware"".
For fairness, author should at least compare the representation capacity of the proposed method with a ResNet50 trained with WSI-level labels.
Authors can discuss the effect of queue initialization on the performance.
Eq. (3) and Eq. (4) should be reformulated.
Typos like ""MoVo"" and ""z_k = f_q(v_k)"" should be corrected."
294	Lesion-aware Dynamic Kernel for Polyp Segmentation	"Overall, the paper is very well presented and easy to follow. Some points for possible improvement below.
It would be helpful to explain the clinical use case better. For example, is this an approach to better segment the same polyp once detected in subsequent frames by using a the dynamic kernel? Or do you think the kernel will adapt to a particular colonoscopy? Also what is the segmentation need for polyps in terms of clinical utility?
When making claims, it would be good to have experimental basis/support. For example, in the LCA description the section ends with ""which significantly improves the feature contrast and benefits to detect conceal polyps."" But where is this demonstrated?
The new dataset should be described fully including size, data labelling procedure, etc.
The experiments on public data are commendable and rigorous. What limitations to testing the proposed method do these datasets present? E.g. do you need videos to demonstrate the full utility of the method?
Perhaps if the method is meant to apply/adapt better to temporal information (video) then it would be relevant to also ref works which incorporate the temporal features. Several of these in MICCAI, e.g. Puyal, et al. 2020 and Wu, et. al 2021."	Please see Section 3 &4	"One aspect the authors should try to highlight is how their methods differ from prior work that introduced the various components brought together in this work. While putting various components together is commendable, it would be nice to see where authors have introduced changes to bring these components together. If authors can highlight these main modifications (e.g., after contributions in the introduction section), it could help bring out the impact of this paper.
While authors explain various components of their method is detail, some further details critical for reproducibility are required. Some aspects that were unclear are below:

Were all the other architectures that authors compare against also trained in the same way? I.e., using the same datasets and splits?

Why is K=1? Were other Ks tried? Was this K chosen via experiments with the validation dataset?

Can authors elaborate on how the binary cross entropy and dice loss were combined? I.e, are these weighed equally throughout training?

How are recall, specificity, precision and accuracy defined? As in, how do authors decide that a particular lesion is correctly identified? Is this done via some threshold on the Dice score or IoU? If so, please describe. It seems that the Dice scores for held out datasets are quite a bit lower than datasets used for training while accuracy remains fairly unchanged. So understanding how accuracy, etc. are computed might help understand this disparity.

Finally, while all the metrics shown in evaluation are useful to see, it may also be nice to include how many parameters each of the architectures are using in order to solve the segmentation problem."
295	Less is More: Adaptive Curriculum Learning for Thyroid Nodule Diagnosis	"The paper writing can be further improved.
(1) In the last paragraph of Introduction, the "";"" before ""(2)"" should be changed to ""."" so as to unifying the formatting.
(2) There should be punctuation after each formula.
(3) In the first paragraph of 3.3, the left colon for the phrase ""less is more"" should be corrected.
(4) The backbone names in Table.2 are better changed to uppercases, such as ResNet, VGG, and DenseNet."	"1- ""Let u and s be the average value and standard deviation of elements in Qh"" You may clarify the intended value to avoid mis-understanding the confidence value vs the number of elements in the queue Qh
2- I didn't get how can Ci (confidence) will be limited from 0.5 to 1, since as per Algorithm 1, the confidence uses the probability after SoftMax which can go outside the 0.5-1 range?
3- Could you please elaborate more how can the probability produced from the DNN represents the confidence of the samples
4- I mean, that T=the two queues Qc and Qh use the same shared prediction probability, however they are by definition representing two different concepts. 
5- What is the network that generate yi and yi' in Equation 8? How this is related to P of Algorithm 1 and Equation 1 and 2. I understand that there is a network that should estimate the Ci independently from the backbone network, but the architecture of this network is not clear"	"Proofreading is needed, e.g., in Page 5, ""we propose to embeded"" -> ""....to embed"""
296	Leveraging Labeling Representations in Uncertainty-based Semi-supervised Segmentation	"About the parameter selection, in the ablation study, the authors say that ""we report on b=0.1 in all experiments for a fair comparison."" However, according to Table 4, when b=1, the method achieves the best performance. Then why b=0.1 for all experiments instead of using b=1? If b continues to increase, how does the performance change?
While saying ""the proposed uncertainty estimates are more robust than those derived from the entropy variance, requiring multiple inferences strategy"", it would be better to also compare the uncertainty maps.
The low computational cost is claimed as one advantage of the proposed method. It would be better to also compare the training time required for different methods (not only the number K).
In the ablation study, this paper compares with threshold strategy variant and entropy scheme variant. More explanation is desired. Why the proposed method is better than others?"	"This paper proposes a labeling representation-based uncertainty estimation algorithm for semi-supervised segmenation. It obtains better performance than SOTAs on left atrium segmentation from 3D MR volumes in two different settings. The topic appropriates for MICCAI and the technical novelty of the paper is somewhat novel. Its contribution is moderately significant and the coverage of the problem sufficiently comprehensive and balanced. However, following I have some minor questions that the authors should address to improve this work:

I strongly recommend authors to release the source code along with the submission, since the learning based projects are typically open-source oriented to facilitate a fair assessment of the performance of the proposed methods for the community.

It is mainly improvement of mean teacher, why you design it, what is the main differences from mean teacher.  Can you give the motivation much clearer.

A suggestion: A similar uncertainty-aware method: mutual teaching for GCN, you can learn something from it."	Please see the answers of Q5.
297	LIDP: A Lung Image Dataset with Pathological Information for Lung Cancer Screening	"Define LIDP. What does it stand for?
It is very unclear what do the authors refer as 'samples'. Please clarify.
It is sobering to see negative results published. Having another dataset for nodule classification is of great use to the community."	"The following details limitations in the manuscript that could be better if detailed or eliminated to allow for more room to detail the dataset further.
*Claims made should be cited:
DNN models have ""become one of the main computer assisted techniques for early
screening of lung cancer"" - please provide a reference to this. Currently DNN is leading research approaches but in terms of actual products themselves has there been a study demonstrating that DNN products are dominantly used in lung cancer screening?
Page 2- ""Data with a score of 3"" - what does this mean? This should be detailed further.

""the classic DNN model"" - page 3 top - this is simply too general as there are many different DNN networks. There is no ""classic"" model. This should be removed from the paper unless more detail could be provided.

Reference [20] - Wu, G.X., Raz, D.J.: Lung cancer screening. Lung Cancer pp. 1-23 (2016) - appears to be incomplete.

Section 3.1 - length of a nodule is introduced here and never mentioned elsewhere - do the authors mean diameter?
ADJUDICATION - section 3.1 - The contour ""was checked twice"" seems like a very unscientific way to establish a contour
The choice of patient age >18 is strange since most approaches either target age >50 for screening or > 35 for incidental findings.
*
""Maligancy of 3"" is not clear - Does this mean the malignancy was detected 3 years later?
Section 3.4 Visualization of the dataset does not add much to the paper - consider removing this section
Consider removing or abbreviating Section 4 - there are simply not enough details to really warrant the multiple experiments. It leaves the reader with multiple questions that are not sufficiently covered in the manuscript and simply weakness the manuscript as a whole.
How were operating points selected?
How was the data split? What was the split??
How was the training and check point selection performed?
What architecture was used?"	"*	The introduction should be revised, as well as all references (the paper starts with the 18th reference).
*	Be consistent when creating acronyms, e.g., whether they start with a capital letter (Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI)) or not (Low-dose computed tomography(LDCT)).
*	Add reference for the Project Bell.
*	Explain the reason for the second sentence in point 3.1 (""When collecting...""), it is important for less experienced readers. 
*	Why do the legends in the diagrams (LC015) differ from the actual name of the dataset (LIDP)? 
*	- It would be interesting to train with the LIDP dataset and test with the LIDC-IDRI.
*	Please increase the quality of the discussion in point 4.3."
298	LifeLonger: A Benchmark for Continual Disease Classification	"Authors should provide more insight into how cross-domain incremental learning could be beneficial by e.g. showing results of an experiment showing that subsequent domains could benefit from previous learned knowledge when fine tuned.

Rather than cross-domain incremental a commonly tackled continual learning scenario is domain incremental learning, where the domains are usually more related than in the proposed benchmark. Examples for such work are:
- Perkonigg, M., et al. ""Dynamic memory to alleviate catastrophic forgetting in continual learning with medical imaging."" Nature Communications 12.1 (2021): 1-12.
- Gonzalez, C., Georgios S., and Mukhopadhyay, A.. ""What is Wrong with Continual Learning in Medical Image Segmentation?."" arXiv preprint arXiv:2010.11008 (2020).
- Srivastava, S. et al. ""Continual domain incremental learning for chest x-ray classification in low-resource clinical settings"" (2021).
It might be worth to include this domain incremental setting into the benchmark.

A more detailed discussion of the results could help to gain insights into the results. For example the differences in performance of class incremental learning on different datasets is large. Why is CI learning on TisseMNIST only reaching 32.0, while on BloodMNIST 67.7 is possible? Could authors offer an explanation/intuition for that?

The benchmark presented is limited to 2D on relatively small images, which is an important step in defining benchmarks for continual learning in medical imaging settings. However, a 3D version of such a benchmark is needed to truly evaluate the potential of different CL methods."	"the definition of the cross-domain learning setting is confusing. Commonly in domain adaptation, different domains are given by e.g. data from different hospitals, different modalities or scanners, but the task (e.g. classification of blood cells) stays the same. I wonder if it is needed to have a model that can classify such different images as CT scans and microscopy images.
how did the authors define the split into different tasks? Does the order of learning the different tasks/ classes make a difference?
In Figure 3 it should be '...right column indicates the average accuracy....'
it would be interesting to add an upper bound like joint training on the whole dataset, similar to [1], to the comparison.

[1] Van de Ven, Gido M., and Andreas S. Tolias. ""Three scenarios for continual learning."" arXiv preprint arXiv:1904.07734 (2019)."	"When outlying the contributions, I suggest that the authors focus on the first of the three contributions listed. As previously stated, scenarios similar to the proposed ""cross-domain incremental learning"" have been explored in the past, so I would not deem this to be a separate contribution. The third point is ""We explore task and class incremental learning scenarios of continual learning, to respond well to new labels i.e. diseases for multi-class disease classification."", and it is unclear how this is different from the first contribution where the different scenarios are already presented.

I suggest that the authors rephrase the definitions for the continual learning scenarios in the abstract, as the definitions in the main text are much more understandable. One could also argue that there are four, not three, scenarios as ""cross-domain incremental learning"" can be ""domain-aware"" or ""domain-agnostic"".

What does ""fine-grained cross-domain incremental learning"" stand for?

The explanations for well-known concepts in continual learning, such as the introduction of catastrophic forgetting, take up too much space. I would suggest that the authors instead focus on the necessity of a unified benchmark dataset to drive forward continual learning research in the medical imaging community.

When introducing continual learning strategies in page 5, I would suggest that the authors combine the subtitles with the text, e.g. instead of ""\textbf{Regularization methods} They reduce"" ""\textbf{Regularization methods} reduce"". This would save space and be easier to read.

In the definition of the ""average accuracy"" metric, ""t"" is used for both the number of tasks until a certain task as well as the index of the last task. I would suggest that the authors use a different symbol for the number of tasks.

The titles in Table 1 are too close together, making the table difficult to read. I would also suggest that the authors separate Table 2 and Table 3 into different pages.

Please explain directly why Table 3 only has 3 rows (excluding the lower baseline and several methods). Also, please explicitly state that Table 3 contains results for the ""Cross-domain incremental learning"" scenario.

When stating ""we train the model [...] with the option of early stopping in the occurrence of overfitting."", please explain the early stopping strategy.

""by combining regularization term with the classification loss"" is missing a ""the"" or ""a"" before ""regularization.""

""toward classes, associated with the most recently learnt task."" I would suggest removing the comma and using ""learned"".

""clinical practise"" -> ""clinical practice"""
299	LiftReg: Limited Angle 2D/3D Deformable Registration	I think this is a very interesting paper with not only novel methods but good experimental results. In the future, it would be good to have more of a description of real applications of this work.	An evaluation on real 2D X-ray images should be included to better motivate the clinical usability of this work. In the subsequent publications on this work, a comprehensive evaluation based on real clinical 2D data must be included. Another assumption made in this paper is the presence of X-ray calibration parameters for DRR generation purposes. This may not be a valid assumption in a clinical setting. The authors should amend the conclusion section to include aspects regarding the calibration parameters as well. In general, the illustrations are of poor quality and are not well connected to the mathematical expressions within the text.	The paper needs to justify the use of the PCA to serve a population-based deformation model. Also the use of Lift3D module needs to be justified and compared with standard 3D reconstruction layers. The paper should also make clear how the training/validation/testing division was done and whether they used different patients.
300	Light-weight spatio-temporal graphs for segmentation and ejection fraction prediction in cardiac ultrasound	"The naming of the 4th row of table 1 is misleading (calling it ""GCN - Regression""). This row is basically a CNN video encoder plus EF regressor (does not have GCN layers). The GCN branch is a parallel path to EF regressor and does not directly contribute to regressed EF results.

The author can add an ablation study to show in what extent the GCN path is contributing to the regressed EF results (reporting EF regression with and without GCN path in the multi-frame GCN - Fig 1 and Table 2). If the ablation results supports the paper's claim and verifies that the GCN is improving the EF regression results, I'd change my review rating to ""accept"".

The paper could include power analysis to show whether the differences across reported results are statistically significant."	"It wasn't clear how exactly is the ED/ES classification is used ultimately. In the multi-frame GCN setup, the output is (40 X 2) keypoints for ED and ES. Is the graph also composed for 2 frames only - the ED and ES? And is the information simply concatenated to the feature or used explicitly to form the graph from 2 frames only?

Your main figure (figure 1) - has B, C, W, H variables which I didn't seem defined elsewhere. What is B by the way? Batch size?

In 2.1 (Encoder) section - you say the original image/video needs to be compressed to meet the input requirements. This could perhaps be phrased a bit better. Because you're not just compressing - you're trying to learn relevant features that are useful downstream, right?"	"Major comments:

Current experiments make it difficult to correctly assess the exact contribution of the GCN head. It is missing comparison of the proposed model with MobileNetv2/ ResNet18  without the GCN part.

In the EF prediction, the comparison between your EF regression against the SOA volume based methods is unfair.

Again regarding the EF prediction, why the authors think that the whole  video results worse? Moreover, the best results need to be highlighted in bold in the Table.

Why are the SOA networks different in the EF prediction and segmentation task?

Minor comments

(pg 1) ""EF describes the blood volume pumped by the heart  in each cycle"": That is the stroke volume. Ejection fraction is as defined in pg 4.

More details are needed on the graph convolutional point decoder. What are spiral connections?

Figure 1, Please state what are  B, C, W in the legend.

""Q1 How accurate, efficient and robust is segmentation of a single frame"", Fig 2: "" the GCN (with MobileNet2 backbone)  is more robust"" :   Add the Hausdorff distance should be added to Table 1), also, quantify the number of outliers.

Details of the machine settings are important when reporting timing.

Looking at the processing time per frame in Table 1, it seems that the authors managed to reach real time. Does it still hold even when adding the image preprocessing time/loading time of the network? If so, I would add it to the manuscript since it is an important achievement."
301	Local Attention Graph-based Transformer for Multi-target Genetic Alteration Prediction	"I assume all the results are validated with a statistical test; please clarify.
Sensitivity analysis or relevant discussion should be added for the trade-off of using LA-MIL and T-MIL."	"While multi-target prediction is very useful as it allows predicting different phenotypes through one single model. However, it would be interesting to see if these tasks are helping each other for more effective prediction. The author could perform additional experiments to train and predict each task individually using the proposed method and compare with the multi-target prediction results.
For results in Table 2, while T-MIL and LA-MIL contain standard deviation, the existing methods do not. Are these results generated through predictions on the same set of cross validation partitions? If not, please ensure that the compared methods are trained and tested on the same set of training and testing data for a fair comparison.
For the visualization in Fig.3, could the author also include visualization for T-MIL as it seems to have similar performance as LA-MIL?"	Nothing
302	Local Graph Fusion of Multi-View MR Images for Knee Osteoarthritis Diagnosis	Some details are required to display. Please see the the main weakness.	"The authors should do a more comprehensive related paper review. If possible, I hope I could get a very detailed explanation in the rebuttal.

When the ""PD"" abbreviation first appears, you should give it the full name (proton-density-weighted sequence). And, in table 1, in this sentence ""LGF-Net (No PT)"", does the ""PT"" mean ""PD""?

Some specific implementation details of the ""Projection"" step in the ""Knee Graph Construction"" could be introduced. Dose the projection or ""multi-view slices alignment"" utilize slice registration or an external positioning hardware during data sampling?"	"The proposed approach clearly outperforms the current available approaches for grading OA using multi-contrast knee MR images.
Please clarify why did the authors defined crop size of the knee region by given fixed mms? is it related to the details of WORMS?
For the future work, it would be interesting to see the effect of individual pipelines within the knee graph construction. for example, how crucial is to have an accurate segmentation or how bad bone segmentations could be and the approach will be still performing well."
303	Localizing the Recurrent Laryngeal Nerve via Ultrasound with a Bayesian Shape Framework	It would be better to explain more about the relationships among the surrounding organs.	"It would be better if the dataset could include negative cases, so that the accuracy is more meaningful.
For ultrasound images, it is better if authors could discuss how to select qualified frames from all images, because each sonographer could have a different acquisition habit. And this is a protocol that could be used for a multi-center study."	"Besides the above comments, I have some further suggestions.
The authors can test their method in some other public localization datasets to improve reproducibility.
There would be more structure priors in 3D images, the author can consider extending the work in 3D images."
304	Local-Region and Cross-Dataset Contrastive Learning for Retinal Vessel Segmentation	The overall paper is well organized and the writing of the paper is great. I have no further comments regarding the presentation of the paper.	"It is essential to provide a detailed comparison with [1] and clarify the contributions of the paper.
[1] Exploring Cross-Image Pixel Contrast for Semantic Segmentation. ICCV 21.
The term ""cross-dataset"" confuses me a lot. Would it be better to use ""cross-image"" like [1] because the contrast is conducted over different images rather than different datasets?
I wonder whether it is possible to formulate the two strategies into only one formulation like [1]? Are there any advantages to compute them separately?"	"I suggest revising the paper writing to be more objective and application-motivated. A reasonable and novel application is a kind of novelty.

It is recognise that the weights for contrastive loss are small (i.e., 0.001). Have the magnitude of L_ce, L_lc, and L_gc been studied? How do these tiny contrastive losses contribute to the model training?

Please keep the naming consistency, like ""local-region"" and ""local intra-region"". Additionally, ""easy vessel samples"" and ""easy region-level vessel samples"" only appear once in Figure 1, they are called ""high-quality samples"" in the main text.

Language needs polish - some sentences and words can be more precise. For example, the first sentence of paragraph three in the Introduction says ""For addressing the feature discrimination..."". It can be revised as "" To extract discriminative features..."" or ""To enhance the features' discriminability...""."
305	Longitudinal Infant Functional Connectivity Prediction via Conditional Intensive Triplet Network	In this reviewer's opinion, the manuscript would greatly benefit from the deletion of figure 4. The space available from this deletion could be used to further expand section 2.3 (even a modification of the accompanying figure 2). More detailed literature review on other Deep Learning attempts would also be useful for any reader interested in diving into the topic of longitudinal FC prediction, regardless of the scope (infant, adult, Alzheimer, ...). Reporting of other numerical measures would vastly improve the credibility of the model. Some ideas would include network measures comparisons between real and predicted.	"The influence of brain node partition should be taken into account.
The statistical significance of comparison between the performances of three prediction models should be tested."	"in the loss design, where is the loss about encoder and ID extractor? e.g the similarity between subjects?
It's hard to follow how ICM module works. According to Fig2, will there be big difference if input times are 100 and 101? or is there and difference between 101 to 200? Dose the module simplify the regression problem to classification problem?
For the compared method MLP and MWGAN, what their hyper parameters are? Since training GAN is not easy, it's hard to conclude your method is better.
I'm not sure if the fmri data is preprocessed well, the development pattern of  two representative individuals in Fig.3 were quite different."
306	Long-tailed Multi-label Retinal Diseases Recognition via Relational Learning and Knowledge Distillation	If possible, provide the ground-truth locations of diseases in Fig.3 in comparison to the CAMs.	"The class activation maps are not useful to highlight the obtained results.
For example, In Figure 3 (a), (c) and (d) the highlighted areas are not specific to the diseases.
Why CCT-Net method was not explored using ResNet-50 backbone?
Which results are obtained using healthy images as input?
What about the CAM in healthy images?
The code and some model weights are missing, so the reproducibility of the paper would be impossible!"	"Please refer to the ""main weaknesses"" part."
307	Low-Dose CT Reconstruction via Dual-Domain Learning and Controllable Modulation	"Demonstrate the need for complex designs of dual-domain networks.
Make a more comprehensive comparison, including inference time and complexity of reconstruction networks.
P.2, ""Besides, caused by mismatched resolution between the heterogeneous sinogram data and CT image..."": The authors raise the problem of mismatched resolution in dual-domain learning. It is interesting to see how the proposed method solves this problem. Maybe the author can explain it here?
Experiments: The paper uses an iterative way to reconstruct CT images. How does its model size compare to other methods?

Some suggestions: 
(1) P.3, ""Each DDB is composed of a PI block that transforms ..., and a PS block ..."": Abbreviations should state the full name on the first occurrence.
(2) P.4, ""Different colored rectangles are used to represent different branches and different operations in each branch."": ""branches"" -> ""sub-branches""
(3) P.4, Figure 1: The colored rectangles which represent full connected layers and adaptive average pool layers look too similar. Please consider changing the color.
(4) P.5, ""the image details can be gradually preserved and simultaneously avoid the introduction of mottle noise and streak-like artifacts caused by FBP."" The argument will be more solid if the author can provide some examples of image degradation caused by FBP.
(5) P.5, ""Each controller block is composed of two convolutional layers with an ReLU in the middle."": ""an"" -> ""a"".
(6) P.7, ""We use real clinical dataset authorized..."": ""real clinical dataset"" -> ""a real clinical dataset.""
(7) P.7, ""The dataset contains ten patients, in which..."": ""in which"" -> ""of which"".
(8) P.7, ""the edge of the tissue reconstructed through the network is relatively blurred but showing the powerful denoising ability"": ""showing"" -> ""shows"".
(9) P.8, ""Although the PSNR value is not the highest, but..."": remove ""but""
(10) P.8, ""For fairly comparison, ..."": ""fairly"" -> ""fair"".
(11) P.8, ""which demonstrates the effectiveness of integrating both domain ..."": ""domain"" -> ""domains""."	Perhaps a more thorough discussion explaining the results: why is your strategy a good choice even if it does not outperformed in certain databases; how did the controllable modulation helped in the visualization of certain features?	"The authors may apply their algorithm to the full dimension dataset which has been scanned in CT or CBCT machines. The computational efficiency and memory requirement are exposed to the real clinical data. 
Dual domain network was proposed a few years ago. In previous publications, authors tried to link the info between image and projection domain. A more comprehensive evaluation of priors are expected in the introduction.
Subtle improvements compared with existing algorithm. The real utility of the method is not clear."
308	Low-Resource Adversarial Domain Adaptation for Cross-Modality Nucleus Detection	The paper is well-organized and well-written. Also, the experiments are carried out extensively. It is a pleasure to read such a paper.	"When defining y^S_i in page 3, shouldn't it take into account the pixel size in images?
The authors define a new approach for the differentiable data augmentation but I strongly recommend them to include a graphical illustration of this process (Section 2.2) to really understand what they are doing. In the limit, it could be in the supplementary materials.
When explaining the data augmentation process, the authors sometimes speak about differentiable data augmentation and others, about invertible. Although these two properties are important to integrate the process in the training, I would specify in the text why you need both features and when they are used.
Please, correct me if I am wrong, is the augmentation applied to the source image the same one as for the target one? The question raised from this sentence ""To address this issue, we apply data augmentation to both real and translated images before feeding them to the discriminators and conduct the augmentation when training both generators and discriminators""
Please correct the following sentence: ""... data distribution ONLY if the augmentation contains invertible transformations...""
Please correct: ""training data can posE serious challenges""
Please, elaborate more on this sentence as it is not clear to what you are referring ""This target task-based augmentation can also reduce the effects of data for which the discriminator has no access to the labels.""
What is E in Eq 1,2,3? Are you referring to the estimated mean value? please, define it."	"Presentation of the qualitative results needs to be improved, as the images are too small and hardly readable on print.
It might be useful to add the size of each of the four validation data sets (in terms of number of images) to the header of Table 1.
It would also be interesting to see comparison between the proposed method and the reference ones for the less extreme case, when more training images are available."
309	LSSANet: A Long Short Slice-Aware Network for Pulmonary Nodule Detection	"Full writing and abbreviations in the paper are confusing and should be consistent. If the abbreviation has been abbreviated in the front, please keep the abbreviation in the back, instead of using the full letter and the abbreviation back and forth.
There are English spelling errors in the paper, please correct them carefully.
The method proposed in this paper is based on the pre-training model of the current best method as the benchmark model. It cannot be clearly stated whether it is the advantage of the method in this paper or the overfitting of the original method."	Generally speaking, this paper does not have many weaknesses. The motivation and paper writing are all good. It would be better if the authors could be more careful about typos.	
310	MAL: Multi-modal attention learning for tumor diagnosis based on bipartite graph and multiple branches	"Beside the comments I mentioned in section 5 (weaknesses), I have the following comments.
Abstract is not very informative, no information about the datasets and performance accuracy.

The selection of the axial slices from CT and sagittal ones from MRI is not demonstrated. Would it make difference if these slices are reversed. Also, which MRI sequences used and would it be better to use all sequences of MRI?
Authors mentioned that they used the average value of CrossEntropy loss, PMSLoss, and PiTSLoss. Did you explored different weighted losses of these terms?
So many abbreviations are not defined before using (e.g. CT, MRI and others).
Many typos appeared in many places e.g. ""topk"", ""Experiment and Result"", ...
References are a bit old. Below are some suggested references that authors may use to update their reference list:
https://doi.org/10.3389/fgene.2021.690049
https://doi.org/10.1016/j.media.2022.102444
https://doi.org/10.1016/j.compbiomed.2021.104836
https://doi.org/10.1007/978-3-030-87199-4_10"	"1)	Page 1, Introduction, Paragraph 2, ""Multi-modality methods can improve the diagnostic accuracy by multi-modal data analysis and model construct ... "" should be ""Multi-modality methods can improve the diagnostic accuracy by multi-modal data analysis and model construction ... ""
2)	Page 2, Paragraph 1, ""The performance of these multi-modal methods are considerably improved..."" should be ""The performance of these multi-modal methods is considerably improved..."". Besides, please provide references with numeric results. 
3)	Page 2, Paragraph 2, ""... for feature fusion to segment brain tumor"" should be ""... for feature fusion to segment brain tumors"".
4)	Page 3, Fig. 1, Please explain the meaning of the green line. How to obtain the interested regions and crop them from original images?
5)	Page 4, Paragraph 1, ""patches with 8x8 pixels"" should be ""patches of 8x8 pixels"".
6)	Page 5, "" 3 Experiment and Result"" should be ""3 Experiments and Results"".
7)	Page 6, Paragraph 1, ""The benign and malignant labels of each patient"" should be ""The benign and malignant labels of patients"".
8)	Page 6, Paragraph 6, ""Result of different methods"" should be ""Results of different methods"".
9)	Page 7, Table 1, the font of the first line is not the same. 
10)	Page 7, Paragraph 2, ""it still can reveal ..."" should be ""it can still reveal ..."" 
11)	Page 7, Paragraph 4, ""Compare with doctors"" should be ""Comparison with doctors"".
12)	Page 8, Paragraph 2, ""The diagnostic accuracy and efficiency of doctors have greatly improved ..."" should be ""The diagnostic accuracy and efficiency of doctors have been greatly improved ..."" 
13)	Page 8, Conclusion, ""The attention learning and multi-branch strategy in MIL are used ..."" should be ""The attention learning and multi-branch strategies in MAL are used ..."""	Adding comparison result with the current SOTA method could further strengthen the paper.
311	MaNi: Maximizing Mutual Information for Nuclei Cross-Domain Unsupervised Segmentation	"The second paragraph of the introduction mentioned multiple methods, but did not properly cite them. The authors are invited to carefully cite these papers.

Regarding the related works, the paper only presented methods using adversarial training and pseudo-label. However, more methods should be mentioned, such as those using mutual information (2,3), using metric learning (6,7,8), and using co-training based frameworks (9,10)

As mentioned previously, the authors used JSD instead of InfoNCE or MINE [11] to provide a stable estimation of MI, however, the authors do not verify this claim experimentally in the ablation study. Moreover, only one negative pair is used per positive pair, and the paper claimed this would not decrease the performance. However, no evidence has been shown to support this hypothesis. I would like to see experimental elements for both claims.

The projection head is 1 x 1 convolutional layer. Parallel works usually used nonlinear heads. The authors may want to explain the motivation for this choice.

The Word ""Actual"" on the top line of Fig. 1 should be ""ground truth""?

The discriminator's structure should be explained clearly.

There is no ""source only baseline"" for Table 1.

As mentioned previously, the values reported in Tables 1 and 2 should include the runs in the authors' own codebase/environment. I encourage the authors to provide further results on these settings.

For the presented tables, the authors can bold the best performing values in order to provide a better visual understanding.

For the ablation study, the authors only tested MI loss weight and the different sampling/pooling methods. For table 3, I would like to see the values with a weight higher than 1, to see if it helps the performance.

The authors should provide the mean and std for each experiment to show the variance of each method."	"1) Please include a computational complexity analysis of the proposed method.
2) For the experimental results in Table 2, [27] also reported the performance under the nuclei classification and detection. Please also report the results under these tasks."	Please refer to main weaknesses.
312	Mapping in Cycles: Dual-Domain PET-CT Synthesis Framework with Cycle-Consistent Constraints	"a) The authors should state the motivation more clearly. To the best of my knowledge, this is the first work to synthesize CT images from PET. I wonder why the authors use PET, which is more expensive to acquire and has limited available data, to generate CT images that are much cheaper and easier to obtain?
b) The authors should give a more detailed review of existing cross-modality synthesis methods, especially for CT synthesis.
c) ""Training four networks in Stage 2 will bring more computational costs with little benefit compared with the case of only training image domain networks."" The paper lacks the analysis of the concrete performance and computational cost gaps induced by training two/four networks. It would be better to add a corresponding ablation study or a comparison experiment.
d) To validate the superiority of the proposed method, the authors should compare it with state-of-the-art methods for CT synthesis.
e) In the second paragraph of ""Ablation Study"" section, ""necessity of employing tje secondary"" should be changed to ""...employing the secondary""."	see p5	"In the Introduction, second sentence: ""PET records the consumption of glucose in organs, revealing their metabolic characteristics and thus potentially the disease status"". This is not accurate. Only FDG tracer records the consumption of glucose in organs, but there are other types of PET tracers as well.
In the abstract and introduction, the authors claimed that ""existing works perform learning-based image synthesis to construct cross-modality mapping only in the image domain, without considering of the projection domain, leading to potential physical inconsistency"". This is not correct. Please refer and cite this paper for accurate literature review:
Shi, Luyao, et al. ""A novel loss function incorporating imaging acquisition physics for PET attenuation map generation using deep learning."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2019.
In Section 3.1, image volumes were sampled into 2D slices and then split into training/validation/testing samples. Did the authors make sure that the training/validation/testing slices are from different patients for each set? If so, please mention that in the text.
This method is based on 2D image processing. However, volumetric information in 3D data is essential for the majority of medical tasks. I can understand that expanding this work to 3D processing can be challenging due to increased computation workload, more complex forward/backward projection and limited data. However, if the authors can show that the proposed method is better than a 3D U-Net or a 3D GAN (with a typical 32x32x32 3D patch size for example), it can make this work more impactful and more convincing for real clinical applications (optional)."
313	Mask Rearranging Data Augmentation for 3D Mitochondria Segmentation	"An effort should be made with regards to reproducibility and evaluation. More specifically, the authors should provide a better description of the range of hyper-parameters considered, the number of training and evaluation runs, validation split and validation results, etc. In that sense, I recommend to follow the code of good practices proposed by Dodge et al. (""Show your work: Improved reporting of experimental results"", 2019).
If possible given the short review processing time, I recommend testing the method on other datasets as well."	"It will be better to review previous studies on  learning the mapping from instance masks to real EM images. There are previous methods on synthesizing 2D images.
It will be interesting to show the results by models training only on synthesized image."	"Page 2 Paragraph 2, the authors stated that ""... aims to synthesize enough diverse mitochondria EM training data ..."". The authors designed mask rearrangement techniques in order to generate diverse images. The proposed mask rearranging strategy takes into consideration the size, number and relative spacing of the mitochondria in real images, which makes a lot of sense. But there are concerns as follows:
1.1. Key details and supporting evidence of mask rearranging strategies are missing: (1) the authors studied the actual size distribution and distance distribution of mitochondria, but none of these statistics are included in the manuscript to support the design of two described strategies. (2) key details, including the mitochondria number and size distribution in the synthesized images, are missing. This also leads to the following questions: (a) are there other characteristics of the mitochondria images  (e.g. the overall density of mitochondria, ) needs to be matched in order to synthesize realistic looking mitochondria images? Whether it matters or not to match these characteristics of mitochondria to those of real images in order to improve segmentation performance?
1.2. The relative location of mitochondria and biologically important structures in the background is ignored.  Unlike natural scene images, or even fluorescent images only targeting and imaging specific cellular components, where a foreground  instance can appear in various locations in the scene and even unlikely locations (this can be justified since the natural scene image can be synthesized, like an advertisement image), the background in mitochondria images, however, has biological meanings. One question is, when the orientation and localization of mitochondria are changed by the rearrangement technique, how are the background filled? Do the synthesized biological structures in the background look realistic? Does it matter to have realistic background in order to improve segmentation performance? 
1.3. Evaluation of the quality of generated images is missing. It is common practice and standard to have experts evaluate image quality. In addition, considering the abovementioned concerns regarding the mask rearrangement technique, it will be highly valuable if experts with domain knowledge on mitochondria EM images can validate the generated images in terms of how realistic they are.

On Page 7, the authors showed that the proposed approach ""can improve the segmentation performance by alleviating the false and missed detection cases, respectively."" This is a generic statement and not very clear to me how the two examples in Fig. 2 can fully demonstrate how the proposed method improves segmentation. It would be great for the authors to provide more insights and reasoning in this regard as well as potentially show more examples. In addition, study failure cases: What type of failure modes are there? Is there any type of failure mode with the proposed method which are note present with the baselines?

In Section 3.2 ""Learning from limited training data"", it is described that ""... decreasing the training volume on the z-axis, which are one-half, one third, one-fourth, and one-fifth, respectively"". How does the selection of sub-volume affect the results (for example, which ""half"" of the volume)? It seems that here only one consecutive block along the z-axis is selected, but does it preserve more data diversity to select multiple consecutive image blocks spread along the z-axis, each of which have smaller depth (i.e. smaller number of image sections in z-axis)? For example, to reduce a 3D volume of 100 images by half, could it preserve more example diversity to select No. 1-25 and 51-75 image sections, instead of No. 1-50 image sections?

Confusing method description:
4.1. On Page 3, the authors stated that ""we apply 5 times downsampling with 3x3x3 convolution of stride 2, so the maximum downsampling rate is 32."" But in Supplementary material Figure 1, it showed 4 downsampling operations. 
4.2. In Section 3.1, the size of each training example d by h by w is 32 by 256 by 256, but why it is changed to 33 by 306 by 306 on Page 7 ""Learning from limited training data""? I suggest that the authors explicitly clarify and explain the reasoning of their design choices.

For future work, I would also suggest the authors consider more recent generative models for conditional generation of images, such as instanceGAN (Mo et al, ICLR, 2019), SPADE (Park et al, CVPR, 2019), SEAN (Zhu et al, CVPR, 2020) etc.

Typos and grammar: 
6.1. Page 2 Paragraph 2 ""... obtaining considerable accurate segmentation annotations"". Would it be ""... obtaining a considerable number of accurate ...""?
6.2. In Page 2 Paragraph 2 and a few other places in the text, ""perceptual realistic 3D EM images"" could be ""perceptually realistic...""."
314	MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation	"In Fig. 2, the Reviewer sees that the two backpropagation updates are both ""Maximize L-seg"". Should one of them be ""Minimize"" because of the adversarial nature of the training process?"	"What is the baseline compared in Table 1. The authors should give clearly states. Is it just a encoder-decoder segmentation method?

The results on prostate segmentation should be in the main text, or the tittle should be cardiac segmentation instead of medical image segmentation.

Page 6, there is a missing right parenthesis in eq (5)."	The idea of expanding style space with additional noise and search for harder style composition is interesting and somewhat noble. Although Fig. 1 shows visualizations of Mixstyle and Mixstyle-DA, it would be helpful to add few more visualizations from other baseline methods.
315	MCP-Net: Inter-frame Motion Correction with Patlak Regularization for Whole-body Dynamic PET	The order of citation numbers in the main text is random.	"It looks like both B-ConvLSTM and MCP-Net seem to have very very slight difference in performance (0.9513 +- 0.014 vs 0.9523 +- 0.014 SSIM, 0.6390 +- 0.1005 vs 0.6197 +- 0.1032 torso NFE etc.). So, it is difficult to envision the scale of improvement; how much error is actually tolerable clinically by any method?
Following this train of thought, the authors mention that MCP-Net achieved ""lowest remaining spatial mismatch"" and for ""significant motion in the hand and bladder, ..., MCP-Net still has the capability to reduce error."" But this error bound is hard to define; ideally, we want perfect registration between reference/source, so what is the tolerable level of error that we can get before this model will be practically (clinically or pre-clinically) useful?
The data sub-section of the paper could use some clarity. From 27 subjects, how many volumes were used? If they each have 19 frames, then why were they resized/reshaped to 128x128x256 and not 128x128x19? Was there some volumetric resampling that occurred?
I'm curious to know the instances where MCP-Net failed in registration? What caused its failure and how much was the fitting error?
Perhaps B-ConvLSTM and MCP-Net could be used in conjunction in an ensemble registration framework?"	The authors should think of some ways to get more ground truth (or close to ground truth) data to make the results more convincing.  Even though the motion could be non-rigid, at least some measurable gross motion could still be useful.
316	Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction	"See weaknesses.
typos:
Section 2.2: 
reconstruct x from y as (accurate as) possible
Therefore, the task of under-sampled medical image reconstruction (is) to reconstruct
the posterior distribution."	"The order of references in the text is right? Does it match the template? please check it carefully.
In page 2, paragraph 2: In this paper, We design our method. Maybe ""we"" is correct.
In the discussion part, the weakness of the proposed method is somewhat less. Please give more discussion on the proposed method, if possible.
This manuscript is well constructed, so I recommend it for publication."	"-DDPM can be very slow, which may limits its application. The authors did not discussion/mention this point. Accelerate the DDPM needs further investigation.
-More benefits of using DDPM, rather than current state-of-the-art unrolled network should be mentioned. I did not see any comparison in the experiments."
317	Mesh-based 3D Motion Tracking in Cardiac MRI using Deep Learning	"Minor point:
(1) One more period in the title of Section 2.3. 
(2) In Section 4, ""...propose a image-base...'' needs to be improved."	"Sec. 3 (Tables 1, 2, 3): From the text in Sec. 3 it appears that quantitative motion correction evaluation is calculated at only two time points (end diastole and end systole). It would be interesting to report the values at different times in between as well to verify quantitatively that the method works well across the full cardiac cycle.
Sec. 3: Based on the results of the ablation study to include different views of the heart (Table 2), it appears that a major source of performance improvement comes from having two or more views (the addition of the 2CH and 4CH views). While comparison to the three other registration methods using SAX only images (Table 1) is very strong, and the results in Table 2 demonstrate that SAX only imaging for the proposed method shows improvements over the comparison methods, these results raise the question about the other methods also being improved using multiple views. This may present a technical challenge for the other methods, but it might be worth discussing."	"This is a very interesting work with some good innovation (especially the mesh-to-image rasterizer part mentioned above). The paper is also well written. It is definitely worth being published in some form. Though considering the novelty required in MICCAI, I am not totally convinced to recommend a full acceptance. Cardiac motion tracking is a very wide field with numerous previous methods. The authors tried to separate this work from those ""voxel-wise deformation estimation"" methods by working in the mesh tracking area. This is a good strategy and vertex tracking is definitely beneficial in many ways. But you cannot help compare it to those voxel-wise tracking methods because they have been used for a long time. The hardest job for the authors is to show the value of this new method and justify its value. But it feels too weak for me.
For example, in the initial introduction the authors briefly mentioned those voxel-wise methods [17,18,2,27], which are pretty recent but are also very limited. They are two MICCAI papers, a CVPR paper, and one journal, which are way less than enough to represent those previous works. Later in related works, the authors introduced more, but they were mostly registration-based ""conventional"" methods, which, again, are just too limited to cover the 3D cardiac motion tracking field. This is not to mention the overwhelming amount of deep learning-based tracking methods nowadays. Also, in the experiments, the authors compared the method to FFD, dDemons (representing the conventional methods), and U-Net (representing deep learning methods). And the conclusion was ""Experimental results show that the proposed method quantitatively and qualitatively outperforms both conventional and learning-based cardiac motion tracking methods."" This makes it a rash conclusion. As said above, conventional methods come in way more categories and FFD and dDemons barely suffice to represent the deformation kind. The same goes for learning-based methods. So the justification of proposing this method is not strong enough for me."
318	Meta-hallucinator: Towards few-shot cross-modality cardiac image segmentation	"The inputs to each network shown in Fig. 1 are unclear and the training process also lacks clarity. The authors could consider adding an algorithm box to better explain the pipeline.

Using only mean teacher (MT) significantly improves the performance from 14.0% to 49.9%. MT is a semi-supervised learning technique, but the result seems to show that MT benefits the cross-modality adaptation to a large extent. Could the authors provide more explanations on this result?"	What is the difference between generators in GAN and hallucinator in this paper?	"More badly case analysis
More experiments on different types of distributions shift."
319	MIRST-DM: Multi-Instance RST with Drop-Max Layer for Robust Classification of Breast Cancer	"The related work of this paper is quite long and can be shortened significantly to make more space to explain the proposed approach.

Generally speaking the term multiple instance is used for semi-supervised (SS) learning methods. However, RST itself is a SS approach, so the use of multiple instance with RST is not clear."	See weakness.	"There is a typographical error: CIFA-10 instead of CIFAR-10.
In Table 2 the highest values for every row should be made bold. Specifically in the 'No Attack' scenario, the baseline has 0.831 f1-score over the MI-RST that has 0.830. I understand that the comparison is focused between RST and MI-RST but in my opinion making a lower value bold is confusing to the reader.
Fig. 2 has low resolution and I would recommend a high resolution version of it to be added to the manuscript.
The name multi-instance is a bit misleading since it could refer to multi-instance learning which is not related to the paper. An alternative could be multi-adversary.
The word 'significantly' is used widely for the discussion of the results but there are no statistical tests performed so I would replace it with 'substantially'.
To my understanding, all methods were trained using the same hyperparameters but that could be unfair for some of the baselines. I would recommend using the exact hyperparameters stated in the papers of the comparative methods to achieve a fair comparison of all approaches.
An interesting experiment for future work would be to train on one of the two datasets and test on the other to see if the method is able to generalize better after robust training. That would show the benefits of the method in a more realistic scenario than adversarial  attacks.
Another experiment for future work would be to test whether it generalizes well in larger datasets or if it is really tailored to smaller datasets and the multiple instances are not required when a larger dataset is present."
320	Mixed Reality and Deep Learning for External Ventricular Drainage Placement: a Fast and Automatic Workflow for Emergency Treatments	"The paper presents a very nice integration of existing methods to solve a specific and important problem. It demonstrates the validity of the method in a rigorous manner. Two question remains:

The comparison between blind and navigated catheter insertion may be unfair compared to a real surgical procedure as the surgeon is usually informed by the resistance of the anatomy to insertion of the catheter.
In a real procedure, surgical draping may invalidate registration. The registration procedure presented cannot be performed after surgical draping."	"The MR guidance is a good way to assist the surgeons to accurately perform the EVD placement. To achieve better results, I recommend the authors to
1)  improve the smoothness of the MR guidance.
2)  analyze the brain shift during the EVD placement."	"The solutions introduced in the workflow lack of technical novelty
As the point cloud-based registration relies on the facial features, registration validation should also consider points in regions that are not in the covered facial area but are often used in patient-image registration in the OR for fair assessment, such as the tragus and points on the head.
No statistical tests were performed to confirm the results
The manual EVD technique/protocol used to perform targeting as a comparison to the proposed method should be clearly elaborated. It is not clear if the participants are restricted to the holes as ROI for trajectory planning only. If yes, it is not the most ideal experimental condition.
Although it is good to use the Foramen of Monro has a target for accuracy assessment, the EVD procedure does not necessarily need to target this point, but rather the catheter should in the ventricle at the level of the Foramen of Monro to ensure the pressure measure is valid. However, other protocols also exist depending on the position of the patient.
Some discussion regarding the tolerance of accuracy will be appreciated for the particular application.
The anatomical variability is lacking for the user tests, but it is key to demonstrate the robustness of the proposed method."
321	mmFormer: Multimodal Medical Transformer for Incomplete Multimodal Learning of Brain Tumor Segmentation	"The paper is well written and explained.
The results should be presented to BraTS 2021 dataset or similar other dataset in the extended version of the work.
There should be a subsection to discuss the complexity of the proposed architecture in comparison to SOTA."	"There is some redundancy in the text in the last paragraph of introduction and beginning of the methods section. This could be shortened to provide more / clarify the implementation details.
Suggest writing the loss terms in terms of outputs from shared-weight decoder and convolution decoder with outputs at every level in Equation [9]. If the first term is the summation of losses from modality specific encoders + shared-weight decoder, does it imply that loss will be greater when more input modalities are available?
Please provide model size in terms of model parameters, memory requirements and training duration for 1000 epochs.
From the description in experiments and results section: 'For a fair comparison, we use the same data split in [21] and directly reference the results', but comparison with ACN model in [21] is not provided in Table 1. Please update Table 1 with missing information.
For Fig.2, suggest to provide all input MRI sequences to help see what lesion information is available in each input modality and the lesion information could be provided as magnified insets. There is less tumor heterogeneity in the example shown, if space permits please include another example with tumor heterogeneity.
Similar to Table 1, please update Table 2 with results from ACN in ref [21].
It is hard to interpret results in Table 3. The drop/improvement in segmentation performance for a specific tumor type could be explained by how much of a shared representation is present across modalities and how well the model is able to capture this shared information; and better capturing long range interactions with transformer modules. Please try to dissociate these factors for the ablation studies in Table 3. Maybe, use the ablation experiments with all input modalities to understand the contribution of transformers for modeling long range interactions and for missing modalities, group them with number of missing input modalities. This would be representing the data in Table 3 differently, as you already have the results for various models.
This might shed more light on why you see more improvement for enhancing tumor even though its size is usually smaller than the core and whole tumor. It might be that most of the improvement could be driven by better learning of shared information than modeling long range interactions with transformers (probably not discernable from your current results, this would require a configuration without the transformers).
The sample size of 285 is small for large models. Following similar training and validation folds as SOTA models enables fair comparison. Suggest considering CV-splits on a few subsets of input configurations to understand the variability in model generalization.

[21]  Wang, Y., Zhang, Y., Liu, Y., Lin, Z., Tian, J., Zhong, C., Shi, Z., Fan, J., He, Z.: ACN: Adversarial co-training network for brain tumor segmentation with missing modalities. In: International Conference on Medical Image Computing and Computer Assisted Intervention. pp. 410-420. Springer (2021)"	"Basically speaking, I understand that you try a new way to solve the problem, although I don't know why you use it. If solid reasons are provided, it would be nice.
Besides, the validation is limited, maybe due to the paper length limitation. There are several algorithms on this topic, more comparison would be  persuasive."
322	Modality-adaptive Feature Interaction for Brain Tumor Segmentation with Missing Modalities	"Significance test is needed. Since the proposed method has small improvement in comparision with RFNet, significance test is recomended.
Disccusion about the limitations of this method is needed.
Means-> Mean  in Table 1"	"1) The GN illustrated in Fig 1 should be detailed in this paper.
2) The motivations and the advantages of the computational method of edge shown in Formula (2) should be detailed, and some additional experiments should be conducted to demonstrate the advantages of this computational method."	"Recently, Wang et al. proposed ""ACN: Adversarial Co-training Network for Brain Tumor Segmentation with Missing Modalities. MICCAI 2021"". Similarly, Azad et al. has published ""SMU-Net: Style matching U-Net for brain tumor segmentation with missing modalities. MIDL 2022."" The authors should compare the proposed method with these previous works to demonstrate the main difference.
Net-MFI is applied with multi-modal code to the BraTS 2018 dataset which is not the latest BraTS dataset. The authors should apply to recent BraTS datasets such as BraTS 2020 and BraTS 2021."
323	ModDrop++: A Dynamic Filter Network with Intra-subject Co-training for Multiple Sclerosis Lesion Segmentation with Missing Modalities	"The authors defined a good problem to be solved based on the real clinical setting, however, the proposed solution was not good enough to address such a problem.
Authors should consider a more convincing approach on feature scaling from missing modality setting to full modality. Features from the dynamic head can be treated differently based on their confidence.
The experiments should be designed more carefully, since the problem was inspired from a real clinical perspective, the design of the experiment should follow this. The inclusion of the modalities involved in the experiment should be re-considered and discussed."	"-The authors divide the ISBI dataset with a 4:1 ratio for training and testing, meaning that a single patient was kept for testing (5 patients in total). This makes the results obtained much less meaningful and should at least be acknowledged as a limitation.
-No indication of how the UMCL dataset is split into training and testing is given.
-A previous MICCAi paper from 2020 employed ModDrop in the context of MS lesion segmentation and should be cited as well: 10.1007/978-3-030-59719-1_57
-The validation set is not mentioned for both datasets, how is the binary threshold optimized?
-Regarding the SSIM loss, was the windows size of 11x11 voxels chosen empirically or optimized somehow? Similarly the values of a, b and g in the overall loss function could be optimized. 
-Fig 2. The choice of the colors could be improved (eg. swapping FP and TP, as green is commonly seen as a positive outcome).
-The Discussion section is lacking the limitations of this study and future research directions."	Fig 2: The figure should be enlarged as its difficult to see the results - could be presented in the Supplement as a full page or half page Figure. Also, the description/interpretation of the results - along with the GT should be improved. It is unclear what the sub-image on the top right of each results image represent. Please clarify
324	Modelling Cycles in Brain Networks with the Hodge Laplacian	See above.	"p.6, Table 1: It is understood that p-values are shown here, and it is guessed that
the \pm refers to a standard deviation, likely, from repeating the simulation.
If this guess is correct, than, well, contents of this table are incorrect.
It is not viable to perform this kind of arithmetic on p-values. Even more, p-values
are not Gaussian distributed. Please, revise.
Suppose Table 1 was corrected. Please, describe exactly how you demonstrated that ""the proposed method ... outperformed all (other) measures."" How do you compare two (or more) sets of p-values?"	"Table 1 in the paper is not well tabulated as there are no bold numbers to see the difference. The fact that just positive connections are chosen to construct the final network should be explained more, as those connections are important and some studies may use the absolute value. 
There is a paper related to the cycles using the notion of persistent homology which is not mentioned in this paper as a related paper in medical brain network analysis "" A Univariate Persistent Brain Network Feature Based on the Aggregated Cost of Cycles from the Nested Filtration Networks""."
325	Momentum Contrastive Voxel-wise Representation Learning for Semi-supervised Volumetric Medical Image Segmentation	See Section 5 - in general, the results are really convincing. If the method details and paper are made clearer and the main novel contributions are carved out better, the overall quality would significantly increase.	"Please consider all points listed under ""weaknesses""."	"In the experimental perspective, I would recommend to add more contrastive learning state-of-the-arts and cite more contrastive learning methods for segmentation performance, such as:
[1] Wang, Wenguan, et al. ""Exploring cross-image pixel contrast for semantic segmentation."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.
[2] Hu, Xinrong, et al. ""Semi-supervised contrastive learning for label-efficient medical image segmentation."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021.
[3] Lee, Ho Hin, et al. ""Semantic-Aware Contrastive Learning for Multi-object Medical Image Segmentation."" arXiv preprint arXiv:2106.01596 (2021).
I also have a concern on the innovation. According to the Figure 1., only unlabeled dataset is only contributed to the high-level contrastive loss, is it possible to also use the labeled dataset to calculate high-level contrastive loss?
For the contrastive loss section, is the positive pair defined voxel-by-voxel? What is the benefits of using your proposed contrastive loss to the pixel-wise contrastive loss? It will be great to add experiments and provide more clarity on your innovations."
326	Morphology-Aware Interactive Keypoint Estimation	"This submission tackles the problem of fixing the errors in automatic keypoint estimation from X-ray images for vertebrae applications, which can be improved from the following perspectives:

Revise the explanation and discussion in the experiment section, especially the part for Fig. 4.
Discuss why the proposed method is close to RITM in MRE for increasing the number of user interactions.
Discuss how to select the one keypoint to correct so that the information can be used to correct all other keypoints, does it matter which keypoint is selected?
Discuss the limitations and directions for future research."	In general, the writing tone and style can be revised to help with understandability of the implemented methods. Despite the fact the backbone network architecture is borrowed from an existing method, the authors should better explain the general information flow inside the network and the associated training process. Aspects related to the gating network and the morphology aware loss were hard to follow given the lack of proper explanations regarding the underlying requirements of those components and their added benefit. Although appreciating the effort in comparing the developed landmark detection algorithm to the existing interactive segmentation networks and despite the fact that the authors have performed this comparison on the basis of heatmaps (for consistency), one may argue that this is not an adequate evaluation given that those methods were inherently developed for completely different applications.	"General comments:
Introduction:

One clear example of where key points are used clinically (ideally with reference to the tasks evaluated) would be appreciated - the motivation is currently very general and superficial
Since the authors work on X-ray projections, I would encourage them to include further citations citation for medical images/X-rays: Bier et al., MICCAI 2018, https://doi.org/10.1007/978-3-030-00937-3_7; Kordon et al., MICCAI 2019, https://doi.org/10.1007/978-3-030-32226-7_69
p. 2: ""Also, RITM ... "" - The authors may want to summarize the main idea behind RITM shortly (and introduce the abbreviation), furthermore, the terms ""HRNet-W32"" or ""hint fusion layer"" mentioned later in the text may not be clear to a majority of readers and could be shortly explained.

Method:

p. 4: The formalization of Eq. (1) doesn't seem to fully hit the nail on the head, and should be revised. The line above the equation kind of predefines n as a point with a user interaction. In the equation itself, there is then an additional condition, i.e., n \in {l_1, l_2, ...}. From my perspective, the subset of ""interaction landmarks"" should be more clearly defined (as it is defined now, it may refer to any landmarks...). Potentially, the authors want to define a subset of adapted landmarks L_adapted \subset L_all.
In the same equation, I would expect to see the position described by the ""user interaction"" itself, not the ground truth position. This can be synthesized by employing the ground truth...
Using a cross-entropy loss for heatmap regression (alone) strikes me as relatively unusual - did the authors also experiment with L2-loss/L1-loss?
""We apply the global pooling method on the feature maps to aggregate the most activated signal per channel; the resulting vector is in Rdw. Specifically, we adopt the global max pooling layer, which selectively retrieves the important interaction-aware features for each channel. "" - this is phrased rather complicated, wouldn't it suffice to formulate something along the lines of: ""We use channel-wise global max-pooling to obtain channel-wise activations. These are further processed by two fully connected layers (I presume - from the equation) and a sigmoid activation function to form per-channel gating weights for the main network."" (Eq. 2 doesn't really add more clarity to the paper)
p.5: The authors state that a subset of landmarks were selected to add the morphology-based loss. How where these landmarks selected? Was there some threshold of variance? This should be explained at least shortly and is currently rather vague.
It may not be clear to every reader that the soft-argmax function is an essential ingredient to get the morphology-based loss working. I would encourage the authors to make it more clear how the heatmaps can be used to derive a morphology-based loss.
p. 6: The authors should clarify whether/that a patient-wise split was applied to the data

Evaluation & Discussion:

There is no guarantee that an annotated point will actually end up where the user wants it to be, correct? Were there any border cases observed? This should be shortly discussed.
""when comparing the performance of Vertebra-focussed and each model ..."" - not clear, there also seem to be some repetitions in the results section. I would encourage the authors to double-check this section again.
I don't quite understand Fig. 4: What is the difference between the methods with and without manual revision? Does this just mean that one landmark is moved? If this is the case: Couldn't this also be reported for the ""Vertebra-focussed model""?
Fig. 5 is not fully clear - what are the values mentioned below the images (Initial, After, delta)? Why is there gain for one image but ""delta"" for all others?
The authors only report the mean error, here, a more detailed analysis of at least variance across images / repeated training etc. would be expected. Additionally, an analysis of failure cases would be desirable.
The ablation study is not very clearly described. E.g., how are low/high variance differentiated? Are all adjacent points included? Also, the discussion of these results is a bit superficial. I am missing an ablation that only uses the morphology aware loss (without the interaction guidance).

Minor comments & typos:

Abstract: additions such as ""as shown in Fig. 1"" should not be contained in the abstract.
p.2: ""motivated by SE block"" - abbreviation SE should be introduced.
p.2: ""vertex points on the cervical vertebra have limited deformation"" - this is rather unclear - what do the authors mean here?
p.2: ""... user modifications than manual revision"" - than > compared to
p.2: ""Adding the proposed gating network on the model ..."" - unclear - how is a network added ""on"" the model - to? combined with? What is this ""model""? (see also on p. 3)
p.4: ""are filled with zero matrices"" - why not simply ""are filled with zeros"" or ""contain only zeros""
""It allows all pixel positions in the feature maps of the main network to attend to each significant pixel position with respect to the user interaction information."" - I don't quite understand what the authors want express here.
p.5: ""the ones that rarely deviates ..."" - typo
p.5: ""as the criterion to apply the proposed loss"" - What is meant here?
p.6: ""... to achieve the MRE under 3"" - measurement unit of 3 should be mentioned
p.6: Why were the thresholds selected like they were? i.e., 3/5/10?
p.6: ""the images with high error than ... "" - grammar
Fig. 4: ""reivision""
References: Capitalization seems a bit off."
327	Moving from 2D to 3D: volumetric medical image classification for rectal cancer staging	"In this work, the authors investigated the performance of various types of approaches for aggregating 2D features into 3D features for colorectal cancer staging. Specifically, network structures, loss functions and aggregation functions are tested. The authors adopted the approach where they first determine one factor accroding to the best performing model and gradually add more factors into consideration one at a time while keeping perviously defined factors fixed. It's understandable they chose this approach but it would be better if they could do more rounds of selctions or try all possible combinations.
There are a few minor problems:

How long is the interval between MR imaging and surgical resection for the patients in this study?
Variations in scanner types should be taken into account.
One big challenge for moving from 2D to 3D is the computation requirement. It would be helpful to include an analysis on number of parameters of different network structures.
Attention weighting is implemented as softmax in this paper. It's better to call it softmax weighting/pooling to avoid confusion."	This paper is ready with a bit of modification to publish in the top journal.	To distinguish the T2 stage from T3 for rectal cancer is a big challenge especially over MRI dataset. This paper employed a reasonable CNN model for this issue.  The experimental results are fantastic. However, some aspects of dataset, CNN architecture and experimental analysis are required to be added and revised. Moreover, some evaluation figures such as ROC, Accuracy and loss should be necessary in the experiment section.
328	MRI Reconstruction by Completing Under-sampled K-space Data with Learnable Fourier Interpolation	"The authors have attempted to solve a challenging problem in MRI reconstruction research, which is commendable.
As claimed by the authors, it outperforms all the benchmarking algorithms in terms of visual and qualitative evaluations. Further the time less time taken for training and testing is a big achievement. However, the only concern to strengthen the above statements is validating the hypothesis for larger datasets. If this could be addressed, this proposed methodology will be a huge success in this area of research."	"- [Novelty] Whilst the paper has indeed a strong motivation, the novelty is appreciated as incremental. This is  due to several reasons.

Firstly, there is no intuition on the interpolation strategy that is somehow connected to a naive zero filling interpolation.  That is then refined through a couple of CNNs. Therefore, it is hard to appreciate the level of novelty in this work. 
Secondly, the paper needs a revision on the technical description. What is the norm for both terms in (4)? Why is there not a weighting parameter in both terms in (4)?   Overall, the technical description is limited and no intuition is provided.
Also, authors should introduce all notations, for example the Fourier operator in (1).

- [Experimental Setting] the experimental setting is not well-explained which can be perceived as not more than case studies which demonstrates only limited advantages in both qualitative and quantitative terms.

Authors are using ADNI. However, they are not explaining from which ADNI version is taking the data (e.g., ADNI-2, ADNI-GO etc.). Moreover, ADNI provides data in 3D, why did authors not show the applicability in 3D? (at least from the text it seems like they are taking 2D slices). In the medical domain, data is usually given in 3D. Authors need to clarify this part. 
Baselines comparison,  Authors highlight in several places new optimisation schemes such as deep unrolling and PnP methods. However, their technique is not directly related to such techniques or novel in that research line. Authors mention [14]  as a PnP technique. However, [14] still builds upon ADMM. Author should notice the difference between deep unrolling and PnP methods. Why not include ISTA-net[*] (another deep unrolling technique with higher performance than ADMM-net see [] ) or a PnP method. The suggested work of [] is advised to revise. Therefore, a more stronger baseline could be good to explore or justify.

[*]Zhang, J., & Ghanem, B.  ISTA-Net: Interpretable optimization-inspired deep network for image compressive sensing. CVPR, 2018
[**] Wei, K., Aviles-Rivero, A., Liang, J., Fu, Y., Schonlieb, C. B., & Huang, H.. Tuning-free plug-and-play proximal algorithm for inverse imaging problems. ICML 2020.

No ablation study in parameters or intuition of the model is provided. For example, with a smaller and bigger K.

All in all, the current paper has good motivation but the technical description and intuition needs to be strongly improved as well as the experimental setting."	"The paper is globally well written. 
Please add in Fig. 3, 4, 5, 6, 7 the true images. That will help the readers to visualize the differences between the true images and the reconstructed images. 
Define all operators and variables used in equations 1 and 4.
In section 3.1 results, the authors wrote ""The best results were emphasized in bold and the second to the best results were marked in bold"". Bold appears two times in the sentence which is not matching what we can see in the tables. Please correct the typos.
Discuss the limitations of the study?
Discuss the performance of all methods in challenging subjects (worst cases, tumor areas, etc)?"
329	mulEEG: A Multi-View Representation Learning on EEG Signals	"In this manuscript, the authors proposed to combine EEG time series and their spectral representations to enhance the effectiveness of the feature extraction. It has been widely proven that the fusion of multiple features positively contributes to the classification. However, it seems inefficient to combine the EEG time-series and spectral powers. In general, limited information can be mined from EEG time series. I would like to suggest combining spectral powers with other types of features, such as functional connectivity.
Only one channel was used in this study. Why did you use one channel only? More channels could lead to higher performance. In addition, different channels were selected for different datasets. The same channels should be used. If there are not the same channels, the channels located in the adjacent area should be selected.
As the study's primary purpose is to have a good representation, it is therefore expected to have detailed comparisons in representations. However, only indirect results were shown in the manuscript.
The number of training samples is 31. How did you set a bitch size of 256?
It states ""an initial learning rate of 3e-4"". Why did not you use the format of 10e-4?
Typos: e.g., contrastve on Page 5"	Overall the idea suggested for multi-view seems convincing.	The author claim that they design an EEG augmentation strategy for multi-view SSL. It is better to show the effectiveness of using augmentations from baselines.
330	Multidimensional Hypergraph on Delineated Retinal Features for Pathological Myopia Task.	This is an interesting work to provide better prediction performance of the degree of pathological myopia, using multimodal hypergraph learning model. It would be interesting to see the performance of the approach in comparison with other CNN based methods on RGB images.	The paper is well structured and well written. I suggest that the authors consider answering my questions in the weaknesses part mentioned above, in order to improve this and future submission.	"Given that the performance between the standard CNN-based approach and the proposed hypergraph learning approach is so similar, I think it would strengthen the paper to investigate the interpretability of the hypergraph, to show that it is indeed an advantage of this method.
Also, I found the reference to ""multimodal"" to be a bit confusing.  Specifically, the proposed algorithm only processes retinal fundus images - it does not take into account different modalities during the prediction.  I appreciate that the authors extract multiple anatomical features, and input this to their hypergraph model; however, I would not refer to this as multimodal.  Perhaps ""multi-dimensional"" would be more accurate."
331	Multi-head Attention-based Masked Sequence Model for Mapping Functional Brain Networks	"*	Major comments
o	For future works, it would be more interesting to extend the current 2D-based approach to 3D by utilizing the state-of-the-art 3D-CNN model. As the authors described in the conclusion, applying spatial attention would make the MAMSM more complete. In the same vein, extension to 3D can be helpful.
o	It would be helpful to describe the procedure of other methods for obtaining FBNs such as GLM, and SDL.
o	It can affect to use of neural network-based techniques to obtain final functional brain networks (FBNs' W) rather than simple LASSO regression, as other papers used. As we already know, a simple neural networks-based regression model also can be added to the LASSO regularization.
o	I am wondering if the ""token embedding"" in the paper is only for applying masked methods? The token embedding was commonly used for the embedding of input word to vector with fixed length (e.g., 512) in NLP, but in MAMSM, it seemed it just meant the input signal with discrete and continuous masking. 
o	Continuing from the above comment, it would be good to clearly explain the meaning of ""latent features"" from MAMSM. It seemed that it indicates the output of multi-head attention, but other readers can be confused if it is the trained attention scores.
*	Minor comments
o	In the first sentence of the Abstract, I think it is better to change ""brain functional networks (FBNs)"" to ""functional brain networks (FBNs).
o	In Fig. 1., it seemed to be needed to describe more detailed components of the figure such as what is ""CLS"", ""SEP"" in (c), indicating ""n"" is a number of voxels (or vertex) in (a).
o	In the ""3.3. Reconstruction Loss"" of the results section, the further explanation of MAMSM can be placed in the method section, instead.
o	In the conclusion section, it would be possible to remove the word ""model"" in ""MAMSM model"" because MAMSM have already ""model"" in the last alphabet ""M""."	"a) Include ablation studies as mentioned in Q5.
b) Include more subjects in HCP S1200 release.
c) More evaluations can be done on other tasks like LANGUAGE and WM.
d) The authors did not mention the value of K used in Eq. 5. Consider the new loss function is one of the main contributions, it would be better to add more ablation studies with different value of K.
e) The MAMSM model has three transformer encoder layers and each layer contains six head attention, how does these number determined? 
f) In Section 3.1, what is attention scores? There is no illustration when it firstly show up.
g) What is the meaning of tokens in Section 2.2.3? Is it a scalar value in a fMRI time point? Or it is a vector?
h) In Section 2.2.3 , how the original value (I think it's a scalar) can be replaced by [mask]?
i) What is the difference between discrete and continues masks? What is the meaning of CLS, M, SEP in token embedding in Fig. 1.
k) The optimization function Eq. (6) is denoted by L2 norm, but all variables are matrices. The authors need to clarify the L2 norm and Frobenius norm."	"Major Concerns:
1). Validations
The method proposed in this work is very novel. But the comparisons could be biased. For instance, the authors provide comparisons of the proposed method with SDL and ICA, which are canonical unsupervised methods. To further validate the proposed method, the author will be asked to validate the performance of the proposed method with other supervised methods.
2). Limited contributions
In this work, the authors only provide spatial similarity as the most important standard to evaluate the performances. Obviously, in Fig.1, (d) further training, the reviewers are very curious about the hierarchical organizations of brain networks. But, in the results section, there are only conventional task-evoked brain networks, without any hierarchical structures, reported and compared with peer methods. 
Here are some references for future validation:
Sahoo, D., Satterthwaite, T.D., Davatzikos, C.: Hierarchical extraction of functional connectivity components in the human brain using resting-state fMRI. IEEE Transactions on Medical Imaging pp. 1-1 (2020). https://doi.org/10.1109/TMI.2020.3042873.
3). Artifacts of identified brain networks
In Fig. 5, the authors provide the results of the other detected brain networks compared to the ICA. The reviewers are afraid that some networks would be artifacts. For instance, in Fig. 5(b), an identified network in the second column and row seems to be an artifact that occupies a large area of white matter. Moreover, the reviewer noticed that the figures are generated arbitrarily in Fig 4. Specifically, the results of GLM are mapping with T1 weight images, but other results are mapping with different background images.
Minor Concerns:
1). Mathematical formula issues
In Eq. (4), the authors need to provide more details of cos.
In Eq. (6), the authors denote all variables as matrices, but the L2 norm is applied to generate the optimization functions. In fact, it should be using the Frobenius norm instead.
2). Quantitative Comparison Issues
In Figure 4, obviously, according to the color bar, the intensity of GLM results is larger than the proposed MAMSM. Therefore, the reviewers are very curious whether the spatial overlap can leverage the spatial and intensity influence."
332	Multi-institutional Investigation of Model Generalizability for Virtual Contrast-enhanced MRI Synthesis	"This paper proposes a deep learning framework to investigate the generalizability of the proposed method through comparing MRI contrast synthesis results from some single-institutional models and a joint-institutional model. According to the results, joint-institutional model outperformed single-institutional models in both internal and external testing sets. The paper is well organized and has presented enough figures and tables to support authors' ideas. However, there are some major concerns.
Authors should provide data description in detail. Are all T1w, T2w, and CE-MRI images from the same institute obtained using the same MRI scanner with the same pulse sequence? Do all institutes use MRI scanners from the same manufacturer? These descriptions will help readers better understand the difference in MR images from different institutes.
Another suggestion is to consider adding a reader study to evaluate the quality of synthesized images."	"Discuss the limitations of the study?
Discuss the performance of all methods in challenging subjects (worst cases)? 
Do the pathologies are preserved after generating the virtual contrast-enhanced MRIs?"	"A very nice study just some minor concerns:
1)Need of statistical analysis about the significance of the testing cohorts so the authors can strengthen the final conclusion about the training in different cohorts and generalization.
2) More details about the institutes and the difference of the cohorts' modalities, resolution quality needed.
3) minor typos (conclusion needs capital C, reference of L1 loss or equation is needed)"
333	Multimodal Brain Tumor Segmentation Using Contrastive Learning based Feature Comparison with Monomodal Normal Brain Images	"It would be interesting if  the proposed method is compared with other contrastive learning methods such as MoCo (https://arxiv.org/pdf/1911.05722.pdf ) and SimCLR (https://arxiv.org/pdf/2002.05709.pdf ) instead of SimSiam network.

Usually, contrastive learning methods require a large number of negative samples to avoid the mode collapse, the author used SimSiam which use only positive sample.  Then the proposed method should be compare with other the contrastive learning methods which only uses positive samples such as BOYL(https://arxiv.org/pdf/2006.07733.pdf ).

The Normal Appearance Network performance depends on the reconstructed monomodal normal brain images from IntroVAE. It would be better to show qualitative and quantitative analysis of the generated samples.

Please also explain if the segmentation backbone and Normal Appearance Network were trained from scratch or these are pre-trained models.

The author should also include the hyperparameter sensitivities for the proposed method in the training details"	"General
This paper focuses on the task of brain tumor segmentation in MRI, and the main idea and contribution is the use of normal brains to contrast with the brains with tumors. To that end, the authors learn an IntroVAE from normal T1 scans and generate a normal brain at runtime. During training, the authors propose a loss that aligns the features of normal regions between normal and tumor scans, such that features of the tumor are enhanced by an attention map. The paper is well-written and the method is novel. However, there are some concerns such as dealing with the ""Whole Tumor"" class only or the evaluation procedure.
Comments/questions to the authors (not in order of importance)
1) The proposed method has a lot of components, such as the segmentation network, but also the IntroVAE for generating normal brain images.
	a) There are no details about the training of IntroVAE. It is also unclear how much the segmentation network is affected by this model. Could the authors add supplementary material with the recipe to train the IntroVAE, please?
	b) Related to the previous question, could the authors provide some images with the reconstructed images, especially in the tumor region, please?
	c) Could the authors release the source code, please? It would help the readers in implementing this approach.
2) The evaluation procedures raise some concerns.
	a) The authors use five-fold cross-validation. While this is a valid approach to searching hyper-parameters, it is fair to assume that the reported metrics are the average of the held-out folds, used for validation during each training. Therefore, it can suffer from over-fitting and the results may be overly optimistic. In other words, it would be desirable to have an independent test set.
	b) Related to the previous question. BraTS makes a validation set that can only be evaluated online with hidden ground truth. Could the authors submit their method and the baseline and provide such results, please? This wouls allow to compare with the state of the art.
	c) The authors employ Wilcoxon signed-rank test and report p-values to compare across methods. However, the significance level must be mentioned, otherwise, the reader does not know the condition to verify the hypothesis.
	d) No comparison with SotA is provided, other than comparisons with the baselines. Although the authors only tackle the Whole Tumor class, comparisons could still be made in these classes.
3) The authors only report the metrics about the binary whole tumor segmentation. However, in BraTS, the tasks also include Core Tumor and Enhancing Tumor.
	a) The proposed paper would be stronger if the authors extend the method to the other classes. Why did the authors choose to tackle Whole Tumor? Could this be made clear in the manuscript, please?
Further comments (suggestions/extra comments on future work) - NOT intended to be addressed during rebuttal
1) The proposed method is interesting and new. Here are some suggestions for an extension of the work that the Reviewer considers would make it a stronger work. As an extension of the work, it would be good to train the network with Core Tumor and Enhancing Tumor and report these metrics, too. Also, a comparison with the SotA in BraTS 2019 is necessary to show the benefits of the proposed work. Finally, the authors propose to generate a normal brain from the patient's acquisition, which is interesting. However, it is not clear how much the method is sensitive to this. A simple experiment would be to replace the generated brains with an Atlas of T1 sequence, registered to the patient's scans, and check the differences.
References
[1] Tian, Yuandong, Xinlei Chen, and Surya Ganguli. ""Understanding self-supervised learning dynamics without contrastive pairs."" International Conference on Machine Learning. PMLR, 2021."	"*	Page2, ""anomaly detection based methods are unsuitable ..."" This statement needs to be reformulated. In fact, there have been quite many studies in the field of unsupervised anomaly segmentation of brain tumors. However, most of them relied on single modal MR images such as FLAIR in which at least a part of the tumor appears with hyperintensity patterns. Although the segmentation problem is simplified in such works, some interesting results have been reported. Therefore, this is recommended to reformulate the statement.
*	Page2, second paragraph, line8: ""in this was, tumor regions.."" this sentence is a bit confusing and is hard to follow. Please rephrase it.
*	Page2, second paragraph, line11: ""tumor brain images"" please reformulate this and the rest in the whole manuscript to ""brain tumor images""
*	Page3, first paragraph, the last three lines: Does this framework function as the same for the testing phase? If so, how the pathological slices will be defined? Otherwise, if in the testing phase, all the slices are analyzed, this should be briefly explained to avoid confusion. 
*	Section 2.1: It was stated that T1 sequence is the most commonly used brain imaging modality, and therefore, the normal appearance model was developed based on T1 images. In addition to the conventionality, it would be interesting to investigate how other sequences perform in capturing the appearance of healthy images. 
*	Another important comment: Since the performance of the whole pipeline depends heavily on the IntroVAE, It will be of great interest to quantify the performance of the IntroVAE iteself. For example, some post processing steps can be conducted on the residual images between the original and the output of the IntroVAE and compare against the segmentation labels. This would be beneficial to quantify the functionality of this piece of the model.
*	Page 6, line2: Please describe briefly how histogram matching was performed (e.g, one image per modality was randomly chosen and used as reference or ....) . 
*	Regarding the external comparison against nnUNet: please specify if the nnUnet is trained on 2D or 3D. 
*	It will be very good to add another column in table 1 and show the best performance reported over the same BraTS dataset.
*	Page8, line9: briefly describe how the attention maps were calculated and visualized."
334	Multimodal Contrastive Learning for Prospective Personalized Estimation of CT Organ Dose	I believe this is a relevant paper for the community and proposes a novel methodology that is worth publishing. Please, consider the weaknesses already pointed out to improve the clarity of this work.	"why not perform contrastive learning directly in the scout image domain ?
I disagree with the unsourced comment that ""Existing contrastive learning methods are heavily reliant on the availability of extremely large training sets. Especially in medical imaging, training data are limited making the contrastive learning methods less effective."".
section 4.3 should be renamed results"	"Abbreviations are used before their definitions, also notations (l \in {1,2,...L+1})
Some abbreviations should be reminded in Fig. 1. Some are defined after the figure.
In Introduction, ""in medical imaging, training data are limited  ..."" It is not always true, there are a lot of unlabelled data for instance in histopathology. 
In introduction, sentence to fix: ""... in order to maximize and minimize respectively...""
In section 2., it is mentioned that the scout and Dw should learn similar representations. First it is not the image and Dw that lear representations, but even more I don't think the representations should be similar. The image contains much more information than the Dw. 
More information could be provided for the model description, such as the number of filters, filter size, initialization for the 1D convolution.
Equation (3) should be better motivated. It is also not well defined. Should it sum for all organs, or maybe one loss for each organ? Why are there L+1 organs?
Also the notation l is used in the previous equation for the contrastive loss, and also for ""leaky"", maybe other notations should be used.
It is not clear whether the data originates from a single institution or more (plural in 4.1).
Below table 1 (4.2), ""l \in L"" should be ""l \in {1,...L}"" or L+1 that's not clear from before.
In Table 1 it is strange to name the model ...CL (noCL)"
335	Multi-Modal Hypergraph Diffusion Network with Dual Prior for Alzheimer Classification	Refer to the detailed weakness.	The authors could make changes to address the issues shown in the weakness.	"The authors need to solve the following issues:
1.For the introduction, improving predictive uncertainty is not clear.
2.The proposed method is reasonable, yet hypergraph diffusion module is not clear, more in-depth analysis will be better.
3.Some typographical and grammatical improvements should be made, such as the performance of ""ours"" in Table 2."
336	Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training	"Here are some suggestions for improving the paper:

Apart from vision-and-language downstream tasks, can this pre-trained model be useful for vision-only or language-only tasks as well. How to use the pre-trained weights for vision and language tasks separately?
Please elaborate on how to determine masked 15% words in the input text. The word ""opacities"" is masked, but most words in a sentence do not carry such a critical meaning, such as ""chest"", ""radiograph"", ""shows"", etc. Did the authors apply any specific process for effective masking choice?"	This is a good submission in general. Please consider revising the paper according to the weaknesses.	The proposed contribution point should have experiments to prove that it is effective. Try to supplement relevant experiments to make the paper more solid.
337	Multi-modal Retinal Image Registration Using a Keypoint-Based Vessel Structure Aligning Network	"Overall the paper is well written and is of interest to the MICCAI community. A few comments to further improve the paper.
(1) All test images are narrow-field. The transformation can be approximately modeled by homography. I suggest the authors to test on more challenging case including ultra-widefield retinal images (for example, PRIME-FP dataset https://dx.doi.org/10.21227/ctgj-1367) as secondary results.
(2) The paper compares the proposed method with other deep learning based method such as SuperPoint and GLAMPoints. Please also consider other conventional methods for retinal image registration, such as vessel-based, intensityb-based or traditionally keypoint-based methods. 
[Vessel-based] Registration of multimodal fluorescein images sequence of the retina
[Intensity-based] Maximize mutual information between multi-modal images
[kepoint-based] A partial intensity invariant feature descriptor for multimodal retinal image registration
[keypoint-based]  Alignment of challenging image pairs: Refinement and region growing starting from a single keypoint correspondence
(3) The synthetic images are obtained using CycleGAN. The generated images may contain artifacts. How robust the proposed method is regarding such artifacts?
(4) Will the IR-OCT-OCTA dataset be publicly available?"	"I would like to congratulate the authors on the great description of the system, the dataset creation and the experiments. I consider this paper to be a good contribution to the field, even if not highly innovative. The main weaknesses I see are mostly related to the synthetic data utilized for the training of the method.

Synthetic data is created using homographies. This works mostly for images with a narrow field of view, more complex warping, or if possible the utilization of an eye model would provide more realistic transformations, particularly when working with standard FOV images.
Color retinal dataset utilized contain only low resolution images (576 x 720), when it's almost been a decade where high resolution images (at least 2500x2500) are widely available, so such low resolution images should not be considered enough anymore. I think the authors should aim for an upgrade of those images towards that direction.
Additionally, while not strictly necessary, a nice piece of data to show the performance of the registration method in single mode images would be to utilize the FIRE dataset for testing, as there are results of competing methods readily available."	"In the abstract, it is suggested to write quantitative experimental results.
What is the meaning of the colored points in the image in the rightmost box in Fig. 1?
What are the shortcomings of the existing methods introduced in the Introduction?
What does x_ai and x^_pi represent in Formula (2), and what does d(x_ai, x^_pi) represent?
What do N and M mean in Formula (3)? Do the M on the left and the M on the right have the same meaning?
How are the hyperparameters in the framework determined?
What is the reason for using different assessment measures in different experiments?"
338	Multi-Modal Unsupervised Pre-Training for Surgical Operating Room Workflow Analysis	"One of my main concerns is that a lot of details were missing/unexplained in the paper:

First, it only becomes apparent relatively late in the paper that the authors are talking about workflow analysis from room cameras, not e.g. from the daVinci endoscope
Some abbreviations are never introduced, e.g. SwAV, Bi-GRU
The term ""prototype"" is also never defined, nor is any intuition behind the idea explained.

There are also some other open questions/concerns from my side:

The results of the different methods for segmentation appear close together, can you comment on the statistical significance of the difference in results?
The way I understand sec. 3.1., you are pretraining the network to produce similar features for the depth and the RGB images, here it would be interesting to see if the final network actually needs the multimodal data in the end/how the network would fare with only one modality.
How are the modalities merged? Is the input two channels? If yes, how exactly does it work for pretraining? Is one channel just set to 0?
Did you also perform the analysis for K for the segmentation problem? On what data was K selected, did you use a validation set?? Seeing the results in table 3, wouldn't it have sense to consider larger Ks?"	"-Except for the weakness, I'd like to see more details about the training process. Because the pretraining process in this paper is for a better initialization of the backbone encoder. How's that backbone utilized for different task, i.e., activity recognition, semantic segmentation, what the specific settings need to be applied, a few lines of explanation will make it better to understand.
-Ablation study about the multi-modal data and the pretraining process
-Multi-modal pretraining baseline to be added."	"I think it would interesting to use [4] as a baseline, as the proposed model share many concept with [4] and the model in [4] can be applied into more data because it only relies on a single modality. 
[20] used DeepLab V3+ as baseline, it would have been more interesting to use the same baseline. Similarly to workflow experiment, it would be interesting to see the performance of the semantic segmentation model when 100% of labeled data was used."
339	Multimodal-GuideNet: Gaze-Probe Bidirectional Guidance in Obstetric Ultrasound Scanning	Hard to find room for improvement here, at least for me. I am not sure if there are any theoretical contributions (see previous comments), but the innovative approach already makes for a good paper on its own.	"Can the authors make some comment on the practical significance in terns of diagnostic accuracy of the improved error rate of Gaze vs Multi-modality versions of the network? It is not clear what the significance of the statement that ""the error of Multimodal-GuideNet* is within 10 pixels"" is."	In the experiment, all data were downsampled to 6 Hz. What is the rationale for it? Is it due to the computational constraints?
340	Multiple Instance Learning with Mixed Supervision in Gleason Grading	In Table 1, there is one extra horizontal bar between ATMIL and TransMIL. And the results of ATMIL is very impressing, with the consideration that it only use slide-level supervision. Can the proposed framework also taking advantage of the strategies used in both ATMIL and TransMIL.	The authors used both instance-level and slide-level labels for Gleason grading, which is interesting. To generate labels, the authors adopted super-pixel, which is a reasonable approach. But, the improvement made by the proposed work is only marginal. It seems that the effect of masking is larger than mixed-supervision. The authors may provide an extended discussion on this matter.	"Although the idea of mixed supervision is interesting, its effectiveness is not impressive. In the ablation study, the performance improvement mainly comes from the masking and position encoding, rather than the mixed supervision. Without the masking, this work with pixel-level labels (92.67%) performs worse than SOTA work [9] using merely slide-level labels (93.73%).

Except for the mixed supervision, the novelty of the entire work is not so adequate for the conference. In fact, simple masking strategy [4] and sinusoidal position encoding are widely used in computer vision.

When datasets are small, classification methods may gain significant supervision benefits from additional pixel/instance-level labels, such as the SICAPv2 dataset with only 155 slides used in this paper. But the recent Gleason scoring dataset (e.g., PANDA [1]) has more than ten thousand of WSI, where slide-level labels are enough to train huge models. The authors should discuss potential limitations of the algorithm, and analyze whether significant supervision advantages can be obtained on advanced datasets.

[1] Myronenko, Andriy, et al. ""Accounting for Dependencies in Deep Learning Based Multiple Instance Learning for Whole Slide Imaging."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021.

Unclear meaning of limited in so-called limited pixel-level labels. Does it mean that only a small number of samples with pixel-level labels are collected, while most of the samples only have slide-level labels? The authors do not indicate how to extend the algorithm to the so-called limited cases, nor reflect such a setting in the experiment.

The produced instance-level labels (not the predictions) should also be added into Fig. 2. This is critical to illustrate the advantage of the instance-level labels over the inaccurate pixel-level labels, as an important basis of the mixed supervision."
341	Multi-scale Super-resolution Magnetic Resonance Spectroscopic Imaging with Adjustable Sharpness	"Please see Sect. 5. Besides, there are a few typos and errors:
1) Introduction states ""Furthermore, the adversairal loss was incorporated to ...."", was -> is. The tense of the related work.should be consistent.
2) ""such that S best approximates the high resolution ground truth ... "", the text below the fig. 1 could't be followed.
3) In Sec. 2.2, ""where m is the type of metabolite"" should be ""... types ..."".
4) In Sec. 3.3, ""To ensure fairness, we set the same layer number and latent size"" should be ""... numbers ..."", and ""... sizes ..."".
...
Please check the overall paper carefully."	"1: The writing of the paper can be improved. The description of how the proposed method can deal with the multi-scale input is unclear.

: The utility of the T1 and FLAIR images is confusing. The author doesn't demonstrate the effect of the T1 and FLAIR images. Have the authors studied on using only LR Met images?
The ablation studies are insufficient. For example, the ablation study of the pixel loss, structural loss and adversarial loss to explore the effect of different loss function."	"As the proposed method was a multi-modal based super-resolution model, the author could discuss the influence brought by the different images in the proposed method.
While Fig. 3 and 4 showed the relation of the sharpness to the parameter lambda, the effect of Equation (5) was still unknown.
The visual comparison to the current blind SR method could be provided."
342	Multiscale Unsupervised Retinal Edema Area Segmentation in OCT Images	"192 and 96 /192 seem to be a bit arbitrary   - why these? would e.g. 128/192 or 128/256 have made sense?
Fig 2: It seems that the GT has a small layer at the bottom ""extra"" compared to MUIS - nnUnet has it. Any reason for this? Is this layer an artifact, or essential? is MUIS focussed on elevation?
Any reason for answering sometimes 'no's in the reproducibility checklist?"	Along with the comments presented above (Q#5), I would like to ask the reason behind setting the number of training epochs at 20. This number seems quite small, and I would suggest to incorporate an ablation study over varying number of epochs.	"The abstract is not concise enough. Mechanisms of the two stages should be summarized in the abstract to bring out the novelty.
Is the proposed network only applicable to edema? Edema is considered as a lesion of relatively large scale. How about other types of lesions of very different scales, such as hard/soft exudate and hemorrhage? Do they also benefit from scale-invariant regularization?
More explanation is needed for eqn 5. What is R^(-1)F(R(X))? Do the modified DCCS obtain images of multiple scales? How many different scales? What is R^(-1)? Do you scale the features back? Why so? Since this is considered the only novelty of the proposed network, more explanation is needed.
How scalable is the network if multiple lesion classes are considered?
In a real clinical setting, a diseased image often contains multiple lesion types. How much degradation is expected on a dataset of this nature?
Based on table 3 for ablation study, the most noticeable improvement comes from si. The improvement from ms is not considered significant. For this reason, I consider scale-invariant regularization the only novelty of the proposed network. In addition, multiscale fusion of CAM is a necessary step for combining CAMs from multiple scales, not something considered a novelty.
How is ""MUIS w/o ms, ac"" done? Which CAM is selected for segmentation?
The authors included a section explaining the original DCCS, but should avoid copying the words directly from the original paper, e.g. ""category-style latent representation in which the category information is disentangled from image style and can be directly used as the cluster assignment.""
Failure analysis missing.
What are the data sizes for training, validation and testing? Are all 85 OCT volumes involved in training? The validation process has to be clarified."
343	Multi-site Normative Modeling of Diffusion Tensor Imaging Metrics Using Hierarchical Bayesian Regression	"Per suggestion above, please work on improving the clarity and organization of the paper. 1) A brief introduction should be provided in Introduction about the Hierarchical Bayesian Regression framework for general audience. 2) Introducing 16pDel at Methods is deemed unnecessary. 3) All the abbreviations for the ROIs should be fully spelled out in their first occurrence. 4) Provide Supplementary materials.
Page 2: Citation(s) are needed for the statement ""These deletions increase the risk for a myriad of neuropsychiatric disorders, including neurodevelopmental delay, autism spectrum disorder and attention deficit hyperactivity disorder.""
Table 1: Fix typo for ""Sire 1""
Fig. 3: The texts are hard to read. Also I was wondering whether the analysis for the polynomial fit was performed and may be shown (if not, should justify in the texts more clearly), to compare the three fitting approaches."	"Unclear why the study involves the evaluation of rare genetic copy number variants.
The paper seems to be poorly prepared and very rough, e.g., equation, table and reference.
Although the authors used HBR normative modeling to infer the distributional properties of the brain's WM microstructure based on large datasets from healthy subjects, there is lack of results in age analysis, or across lifespan.
The discussion about the association between research motivation and genetic-related analysis is underexplained."	Investigate the points listed in the main weakness comment.
344	Multi-Task Lung Nodule Detection in Chest Radiographs with a Dual Head Network	It will be interesting to have a discussion on extending the proposed DHN and DHA to another stream of object detectors, namely the YOLO family of models.	It is suggested to add more experimental parts and describe the implementation details of the model as clearly as possible.	"Major:

'Considering the properties of the pulmonary nodule, we propose to use deformable convolutions [6] and the gIOU loss' - this needs more details of what properties of the pulmonary nodule lead to the use of deformable convolutions. The authors mentioned: 'Applying deformable convolutions allows more dynamic focus on particular regions in the image where the size of the nodule might be small .' We know that FPN can help leverage the nodule of various sizes, but how can the deformable convolution help detect the nodules of small sizes?

The dual-path network contains global classification and local location prediction with a confidence score. Is it possible for the framework predicts several bounding boxes from the local path but with the negative prediction from the global path, or positive predictions from the global path but no bbox generated from the local path? How to deal with this? 
Generally for the nodule detection, we have a prediction of the lesion and the corresponding location. What is the exact output of this network, and how to use the result of this paper to guide clinical practice?

In the experiments, this paper compared the CONAF [19]. Why are only the classification metrics shown for CONAF? Additionally, in the introduction, both references [13] and [19] are discussed, so what about the performance compared to Li et al. [13]?

It is reasonable that the author compares the performance with similar attempts in the experiments by comparing the CONAF [19] in Table 1. However, by reviewing the purpose of this paper - 'identify pulmonary nodules on chest radiographs', should this paper also compare the nodule detection performance with the state-of-the-art methods [1, 20, 26] (global) or [10,12,15,17,23] (local) cited in the introduction?

Minor:

A typo in ""default setup 'utilizs' a Smooth L1 Loss "" of the last paragraph in section 2 on page 3.
It is recommended that Table 1 be placed near the corresponding text on page 7."
345	Multi-task video enhancement for dental interventions	"For Fig. 1, can the authors explain a bit why Fig.1 is equivalent to the architecture in Fig. 2?  The traveling path of Fig.1 is a bit confusing, and I would like to know the path of information flow from input to the final output.
There are some inconsistencies regarding the uses of subscripts and superscripts for several symbols. For example, the same symbol O has different layouts of subscript and superscript at the 6th line of Sec. 6, Eq. (1), the 2nd and 8th  lines in ""Problem Statement"". Similarly, for symbol B, the appearance of B in the 4th line in ""Problem Statement"", Eq. (2), and the 2nd and 3rd line in ""Training"" are different. For the last two lines in ""Encoders"", in Fig. 2, H has subscript t-1 -> t, while the symbols in those last two lines do not have them.
Please carefully go through the text and make the style of all symbols consistent. If there are different ways of using the same symbol, please consider adding some more notes to explain to the readers.
Fig. 2 is a bit confusing because the symbols are in the same row as the modules and at some places, the arrows between two modules are not shown. Please consider making the data dependencies and information flow clearer.
Fig. 2, in terms of the method design, why do authors choose to use f_t^{1, 2, 3} as one input to produce h_t^{1,2,3} instead of the feature maps after the channel attention? It looks like for the other two tasks, the feature maps after the channel attention are used instead.
Fig. 2, why do authors only upsample the binary mask output to the finer-scale level? Why not also upsample the deblurred image as input features?
For Sec. 3, ""Noise, blur, colorization"", is only camera C_2 used for this frame-to-frame training. Is camera C_1 also involved?
Fig. 3, are images from C_2 only used for color mapping training? Are those not directly used during image deblurring training other than this color mapping training?"	Overall the paper is in good shape and more discussion on how these 3 tasks are related is appreciated.	Please see Weakness section
346	Multi-TransSP: Multimodal Transformer for Survival Prediction of Nasopharyngeal Carcinoma Patients	"Given the limited number of patients (only 384), the paper may consider cross validation.
Is overall survival defined by duration in terms of weeks, days, hours or minutes?
The ""text data"" (age, BMI, dose) is either numerical or categorical, rather than free-text. Strictly speaking, it should be called structured data rather than text data."	"If you did not observe death for a subset of patients and treat the time to the last follow-up as the OS time: Do you check for patients that had successful treatment and were therefore not showing up any more?
For the error you state (e.g., MSE), please indicate the unit (years, months, ...)
Please indicate how the segmentations were obtained (e.g., manual, manually by an expert, (semi-)automated method, ...)
Space embedding: I do not understand the self-learning part of the spatial matrix, since I assume the location information is a value you can simply retrieve from the slice position?
Please add more details about:
Demographics information of your data
The text features you used, since it contains relevant information for the target task"	"The paper is overall well-structured.

Details of the method can be improved as mentioned above. In particular, since one of the key novelty lies in the use of the transformer, this part of the model needs to be explained in good detail. Discussions on why the particular input to the transformer model was chosen is also essential. Why was the particular 2D expansion of text chosen? Has this been shown to be effective in earlier works?
The paper will greatly  benefit from obtaining performance results on more datasets (e.g. other types of cancer) or different data splits of the same data. It is really hard to compare and contrast different methods based entirely on one run of one-split of the dataset."
347	Multi-view Local Co-occurrence and Global Consistency Learning Improve Mammogram Classification Generalisation	"Would swapping the CC and MLO provide the same results? It seems only the projection aspect might make a difference.
Could this be translated to other application areas. It seems most other applications don't have similar views, so the application area might be narrow.
Could full n-fold cross validation results be included. I would suggest that the term ""significant"" is only used if relevant p-values indicate this.
It would be good to see a detailed discussion on why the authors think their model preforms that much better that existing SOTA approaches."	"1) Provide additional details regarding the implementation of the model so the model can be reproduced.
2) Fix the problem mentioned in the weakness
3) There are a lot of grammatical errors such as 
generalisation-> generalization
right hand size of the image -> right hand side
optimiser -> optimizer
area under the precision-recall curve (AUC-PR) -> the area
...."	It would be interesting to demonstrate the gap of generalization. That is, for a dataset A, compare the performance of models trained by the dataset A with that trained by ANON 1.
348	MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep Models for X-ray Images of Multiple Body Parts	See weakness.	"The result section can be enriched with more in-depth discussions of the results, rather than simply listing the results.
The column widths of Tables 2 and 3 need to be adjusted for clearer display.
The texts in Figure 2 are too small."	"-As accepted by the authors, MUSCLE is proposed as a ""proof-of-concept"". It is not optimized for any single task and no evaluation is done to compare against other methods in the literature that could make it clinically useful. 
-Since the proposed framework is quite elaborate with many components, it would very helpful to make the source code public."
349	NAF: Neural Attenuation Fields for Sparse-View CBCT Reconstruction	As described above I assume for the Chest and Jaw datasets projections were truncated left and right. This may highlight another advantage of your method, but is confusing if not explicitly mentioned in the article. I would recommend resimulating the dataset with a wider detector or a different parameter setting to avoid this. Alternatively this could be briefly discussed.	see weakness	"1 The authors are expected to provide details on the network architecture.
2 The performance of the method may be further improved by fine tuning the parameters within the network. The current incremental image quality may not support the statement of 'clinical use'.
3 Computational efficiency is not provided."
350	NerveFormer: A Cross-Sample Aggregation Network for Corneal Nerve Segmentation	Please address the weaknesses.	"The paper presents a transformer based network for nerve fiber segmentation in CCM images. The transformer based part contains an intra-image local spatial attention and inter-image attention. The proposed method outperformed other state-of-the-art methods on two public CCM datasets. Ablation study shows the effectiveness of the prosed two blocks. The paper is generally well written, but could be improved by addressing the following comments. 
-""The TDA enables the proposed DEAM to learn more crucial information in a single CCM image"". How ""crucial information"" is learnt? Requires more intuitive explanation. 
-A_hqk and W_h in equation (1) was not explained. Explain how the sampling offset is determined. 
-Please explain Norm in equation (2). 
-What does ""xN"" mean in Fig. 2. N also represent the number of elements in page 5 line 4. Are they the same meaning?
-Parameter setting, e.g. H in equation (2), N in Fig. 2. H is also the height of image. 
-Please perform statistical test when claiming one metho is better than the other in Table 1 and 2. 
-It would be interesting to see the visual result of Backbone+TEA and Backbone+TDA in figure 3.  This helps in intuitively understanding the effects of each block. Pick one of CS-Net or TransUnet, or use two rows of examples instead of four, if requires more space."	"The paper demonstrates superior performance on two training datasets. But I am not confident about its generalizability, and I believe the two source methods (deformable DETR and external attention), which the proposed approach inspired from,  should be included for a more comprehensive evaluation. Plus, I wish the authors could illustrate the novelty of TDA if it is a major contribution.
I cannot recommend acceptance based on current manuscript. But if the authors can address all these three major concerns, I would be inclined to accept the paper.
Besides these three major concerns, here are two minor suggestions.

Fig. 1 and Fig. 3 seem redundant to me. It may be better to merge them and provide more discussion and analysis in the saved space.
For published papers (e.g. Ref. 21), it would be better to cite their published version instead of arXiv preprint, unless there is significant difference between these two versions."
351	NestedFormer: Nested Modality-Aware Transformer for Brain Tumor Segmentation	"There should be section or subsection to discuss key highlights of proposed architecture which make it better than other SOTA.
There should be thought presented on extension of the architecture to any other medical dataset where multiple modalities are available."	I suggest to provide an nnU-Net baseline, which can be done out-of-box, to help the reader access the relative performance gains.	"In the encoder you named the feature embedding layer, why not 3D conv? 3D conv is easier to understand in terms of feature extraction by the encoder.

For reproducibility, the version of the library used should be specified.

Why did the authors not perform cross-validation in their experiments with BraTS2020? The dataset may be randomly divided, but any bias may not result in a correct evaluation.

The results in Table 2 show that the NestedFormer has a larger HD95 value. There is no discussion of the results with higher accuracy in terms of Dice and lower accuracy in terms of HD95.

Why is it that only limited combinations were evaluated in the ablation study in Table 3?

6.The font size in Figure 2 is small and the resolution is low.
Figure 3 caption is incorrect.
Figures and tables must be placed on or after the page on which they are referred to."
352	Neural Annotation Refinement: Development of a New 3D Dataset for Adrenal Gland Analysis	See the list of weaknesses above.	"In Table 1, it would be great if the author could show the upper bound, e.g., models trained on good annotations. As a repairment, one could consider an ensemble of multiple models to reduce segmentation noise and uncertainty. Other heuristic methods to remove noise, such as hole filling/connected component analysis might be other baselines methods.

Discussion on the limitation might improve the quality of this work. For example,

x. The proposed approach incorporated CT intensities value to improve the reconstruct quality. Absolute intensity values contain tissue information. However, this might not work well for MR images. 
x. This approach works for regular shape (e.g. organs) and might not work for abnormal tissue such as lesions due to shape heterogeneity or small structure.

Some typos / grammar errors
x. In Introduction, ""Unfortunately, such datasets are difficult to obtain in part because human annotations are known to be imperfect"". Isn't it because that expert annotations are expensive?

x. In Introduction,  high-frequency artefacts include false positive/negative.
x. In Introduction,  MLP appears without a full name.
x. In Sec. 2.2, 'by changing the input of...', 'changing' should be 'including'?
x. dice -> Dice
x. 'lower bound' -> 'minimum'; 'upper bound' -> 'maximum'"	"It would be nice to see how this method applies to 2nd data set, especially for very sparse annotations such as lung vessels or very tiny structures.

It would be nice to see the comparison between this method with GNN based approach.

It would also be nice to see how much human labour has been saved, for example, the authors could ask human experts to refine the annotations and compare the time cost.

In table 1. what are the parameters no. of NeAR and Seg-Unet? It would be nice to see the real comparison of the perforamnce

In table 1, what are the stand deviations?

It would be nice to assess how realistic the synthtic distortion is and how much it affects the downstream tasks. For example, if human make 30% distortion of the ``ground truth'' and the classification network is robust when trained with 30% distorted data, then the refinement might not be needed or someone should focus on improving the model. It would be nice to add analysis of the effect of the distortion on the downstream task.

refinement of data vs refinement of model. It is necessary to add comparisons in Table 2 with model-focused methods, for example, more appreance focused augmentation plus bigger model with better representation learning ability might simply improve the classification result and maybe refinement on dataset is not necessary.

Could the authors add connections between the proposed method and NeRF (neural radiance field)?"
353	Neural Rendering for Stereo 3D Reconstruction of Deformable Tissues in Robotic Surgery	In spite of the paper being difficult to read, the results are impressive. More clarity in writing as detailed above, along with a  comment on using the neural rendering technique in conjunction with arbitrary conventional reconstruction techniques to improve performance would improve the paper.	"1) In the supplementary materials, some areas are occluded throughout the videos, such as pulling, cutting, and tearing cases. How to accurately reconstruct these areas, and perform the evaluation?
2) The authors point out that they perform the evaluation following Ref. 10. While, the evaluation method in Ref. 10 is only applicable to the situation where occlusion areas change frequently. So, how to achieve the reference of these areas which are occulated throughout the whole video?
3) Stereo endoscope captures both left and right images, but only left images are used in the proposed method, can the method be applied to right images? If it does, how to evaluate its performance on right images?
4) Some format issues:
- Equation (1): What is the relationship of M_i and M_j? If M_j is a subset of M_i, M_j can be 0 (M_j is tissue).
- Equation (1) and (3): The meaning of variable j is unclear, please give the interpretations or references. 
- Equation (2): Is D_i [u,v] similar to {[[D_i}]] (i=1)^T used in Ref. 10? If it is, why {[[D_i}]] (i=1)^T is named as ""coarse depth maps"" rather than a reference, considering that it has been respectively used as a sampling guidance and depth supervision in subsection 2.4 and 2.5?
- Equation (4): What is I_i [u,v]? I suppose it could be the images with the tissue information only, if it is, please give the definition explicitly.
- Subsection 2.5: ""we firstly find residual maps ..."", what is D_i? Is D_i  equivalent to {[[D_i}]] _(i=1)^T?"	In Section 2.5, the equation (3) and (4) are not well explained. Equation (3) is too dense, maybe consider dividing them and explaining each clearly and also make sure the meaning/definition of each parameters are described in the text. For example, the T in (3) and I in (4) were not defined in the text.
354	Neuro-RDM: An Explainable Neural Network Landscape of Reaction-Diffusion Model for Cognitive Task Recognition	"The ""weaknesses"" section details my major concerns and constructive suggestions.
Here are some additional minor comments:

The use of ""functional neural imaging technology"" in the abstract is awkward. I would suggest rephrasing.

The characterization of fMRI as allowing us to characterize coupling mechanisms of brain functions on to of the structural connectomes is also overstated (page 1). FMRI quantifies hemodynamic changes, which are not necessarily linked to the structural connectome.

The motivational comparison on pages 2-3 seems unfair. The one-particle trajectory is generated from a PDE, so naturally embedding a priori knowledge of this process into the model will perform better. The key question is whether such a model would perform well if the data were NOT generated from a PDE.

It is unclear to me why an ANN is an appropriate model for the ""Reaction Process"". I would think neuronal firing patterns are more complex than a linear model with sigmoid activation.

The similarity between test and retest in Fig. 5 may indicate that the model is learning a ""mean"" representation for the attention weights, rather than capitalize on subject-level differences. I would suggest the authors explore this phenomenon."	"If one already has the model of differential equations, what is the benefit of re-expressing it in a neural network (other than for the beauty of doing it)?
How much error is there between the trained NN and the original PDE model?
although from figure two I think I can intuit how the diffusion-reaction model of equation 1 has been translated into the neural network, but perhaps a little more step-by-step explanation explaining the methodology to take the first model to the second would be convenient. to be able to generalize the idea.
I suppose that the generic idea of decomposing differential equation models into fundamental elements with counterparts in neural networks is in principle generalizable to other models. Although I understand that this goes beyond the purpose of the article that is presented, may I ask the authors to what extent they believe that this idea can be developed? Is it possible to affirm that any model of partial differential equations can have an equivalent counterpart in neural networks?
Add the PDE model to figure 1"	"I would encourage the authors to improve the clarity of Fig.2 to clearly indicate the input BOLD signals and outputs, to aid readers in parsing the methodological description

Several error bars in Fig 3 appear to be cut off. Perhaps the range on the y axis can be changed to prevent this

Clarifications for the following would be helpful:

a) Are the regional connectivities thresholded to generate W? If so, what is the threshold? In practice, how would this threshold affect the optimization of the Neuro-RDM and generalization.
b) What statistical test is used to compare the performance of the methods pairwise in Fig. 4?
c) ""we truncate the long time course of BOLD signals (usually includes more than eight functional tasks) into a set of segments where each segment primarily covers one functional task""
From my understanding, this implies that the time series was broken up into segments of a fixed length. How was this chosen, and how would the choice of this length impact performance?
c) Definition of a latent brain state: For the purposes of this paper, it seems like the intrinsic brain states and cognitive tasks are used synonymously. Therefore it is unclear whether this method could be applied for a study of dynamic functional connectivity in a more broad context, for example for resting state-fMRI data. In this space, several models such as the sliding window or dynamic conditional correlation aim to address the problem of identifying the ""latent brain states"" from the regional time series. It would be great if the authors could discuss if and how their method could be applicable beyond the specific task evoked fMRI paradigm examined here.
d) Scalability: From my understanding of the experiments, the Neuro-RDM was tested on Functional Connectomes generated by grouping regions into 8 subsystems such as the DMN, CEN etc. A natural question to ask here is whether the method would scale to using finer parcellation schemes such as the Yale functional atlas, which is also the scale at which several GNNs developed for functional connectivity work with"
355	Noise transfer for unsupervised domain adaptation of retinal OCT images	"In Section 2.1, it is stated that the pixel values between the restyled image and target image is (often) still different, which is fixed by further histogram matching. The histogram matching is empirically shown to further improve performance somewhat (Table 1, supplementary material).

However, it is not clear whether the (final) restyled image should necessarily actually share the same pixel value distribution as the target image, since the true source image (assumed with noise from source device distribution removed) and the true target image (assumed with noise from target device distribution removed) might well have different distributions. Indeed, the noise addition formulation would appear to have the restyled images be (true source image)+(source noise)+(added estimated target noise), which is then adjusted to fit (true target image)+(target noise). Theoretically, it might be more proper to first subtract estimated source noice, if the source is known. This might be considered.

Related to the above, the clipping of values outside the interval [0,255] after noise transfer would appear a potential source of signal loss at its extremes. It might be clarified if such out-of-bound values apply to a significant proportion of the signals, in general.

In Section 2.2, it is stated that for SVDNA style transfer, when a target domain is required, one image from that domain is randomly chosen as the source. While Figure 2 suggests that images from the same domain do have relatively similar noise characteristics, it might be clarified whether the adapted data does in fact mimic the intended target domain well. SVDNA to Bioptigen in Figure 2(b) appears clearly separate from Bioptigen, for example, and SVDNA to Cirrus/Topcon do not appear to be shown.

Moreover, Table 1 in supplementary material suggests that the histogram matching alone contributes the majority of the performance improvement, with noise transfer alone contributing next to nothing. As such, the natural comparison would seem to be against histogram matching/normalization methods such as CLAHE etc.

In Section 3.2, it is not clear why CycleGAN required a different training methodology.

While Table 1 in the main text claims comparison of SVDNA to supervised trained models from 8 teams, only three results are presented. If only the best model from the 8 teams was used, this might be clearly stated."	"Improve the Algorithm 1. Including the cohesiveness between the text description and the algorithm.
Make sure that all variables used are clearly defined."	"Minor issues:

""For more details on the feasibility of the used values of k see supplementary figure 1"" in Page 4 should be modified to ""For more details on the feasibility of the used values of k see supplementary figure 2"".
""for examples of content distortions achieved by an optimized CycleGAN see supplementary figure 2"" in Discussions and Limitations should be modified to ""for examples of content distortions achieved by an optimized CycleGAN see supplementary figure 4"".
The last line in Table 1 of the supplementary material seems inconsistent with the results of Topcon (left) in Fig. 3. Please check."
356	Noise2SR: Learning to Denoise from Super-Resolved Single Noisy Fluorescence Image	"Some suggestions:

It would be interesting that authors gave some information about the computational time for proposed method.
Summarize the research limitations and future research directions.
Describe the computing system (hardware) used in experiments.
Extend the Conclusion with details on the method performance when compared to other tested techniques (in terms of PSNR improvement and SSIM)."	"Model: The paper can be viewed as an extension of N2S, predicting a random set of  pixels. It'll be great if the paper can plot the performance vs. number of pixels to predict, showing the interpolated results between two methods.

Experiments: The proposed method will be more convincing if evaluated on different image modalities."	"What does super-resolved mean? Authors need to define SR in the text.
In N2N, one can use patches to use training, i.e. can be trained using less samples so how does using sub-sampling module help boost performance?
As commented in the weakness above, authors could benefit from providing more ablation details.
Please add time comparisons to better understand more clearly if there is any added value to this method"
357	Non-iterative Coarse-to-fine Registration based on Single-pass Deep Cumulative Learning	"1) The definition of large displacement is not clear. It would be helpful to mention the predicted displacement distribution in the testing datasets explicitly.
2) It would be helpful to report the total number of voxels in each dataset in Table 1 in addition to the NJD.
3) The ablation study in Table 2 is inconclusive. Up to what amount of L the improvement in Dice would saturate or degrade?
4) For the extension, I would recommend applying the proposed method to a dataset with large deformations, such as chest CT scans (e.g., DIR-Lab 4DCT). I would recommend comparing your results to the study of [Hering2021CNN].
Hering, A., Hager, S., Moltz, J., Lessmann, N., Heldmann, S. and van Ginneken, B., 2021. CNN-based lung CT registration with multiple anatomical constraints. Medical Image Analysis, 72, p.102139."	"1) The parameters such as \lambda should be detailed in the ablation study.
2) Other datasets such as OASIS should be included in the experiments.
3) Multi-iteration scheme registration experiments using the same proposed architecture should be conducted in the ablation study."	"1- The authors should compare their proposed network with the state-of-the-art qualitatively. This would help to highlight NICE-Net's registration performance.
2- Limited clarity: A description of the training results would enhance the paper clarity. It is also preferable to include a brief description on the training implementation details (e.g. How many image pairs were used?)."
358	Nonlinear Conditional Time-varying Granger Causality of Task fMRI via Deep Stacking Networks and Adaptive Convolutional Kernels	"There is a growing interest on convergent cross mapping (CCM) (Siguhara et al. Science 2011) as an alternative tool to address non-linearity in Granger causality and estimation of different lags. Moreover, Granger and Siguhara causality (as well as dynamical causal model (DCM)) have been criticized to mere temporal correlation tools, and perturbation based approached should be more relevant. These aspects should be mentioned.

In several papers investigating convergent cross mapping and Granger causality, it has been observed that the amount of noise added during the creation of the synthetic data (e.g. using the STANCE tool) can deteriorate the estimation of the causality directionality. In the paper, it is reported the defined relationship but not this critical detail. This is not a small thing, as you might be using relatively clean data  with your ground-truth and slightly noisy data in reality, with the latter leading completely wrong causality estimation. How sensitive is the tool about this aspect? Or for the simulated data the reported noise (0.1) is already big, and therefore your simulations are even noiser than real data?

It is not clear the network topology of the synthetic data. The STANCE has some kind of DMN simulation and other nucluei, but in the presented paper it seems to be a simple couple-triple relationship. The authors are asked to clarify this. Ideally you should have networks with at least 5 nodes as in (Smith et al. Neuroimage 2011, or Crimi et al. Neuroimage 2021). Couple analysis is completely
a different story than multivariate.

This also raises the further question about indirect causality. With convergent cross mapping a time series should be able to reconstruct even indirect causality (Ye et al. Nat SciRep 2015), is the same for your case given the conditional estimation and non-linearity? Or would you need a propagator settings as described in (Crimi et al. Neuroimage 2021).

It is not clear why the authors use up to 5 lag causality. BOLD signal is very low-temporal resolution, generally causality study with those data don't go beyond 3 lags due to this (depending on the data 1 lag is the max useful). Other times the order of lags is estimated by the  Akaike information criterion, but I haven't seen this here. I cannot grasp what you are trying to convey in Fig.5 regarding lags
and inhibition/excitation, please clarify better.

One of the advantages of Granger causality is that is computationally less demanding of DCM and therefore more suitable for brain-wide analysis rather than few ROIs. Yet, you use it in a scenario with 5 areas and task-based. Then, why are we even using Granger causality? We could stick with DCM, which is a physiological tool ideal for task-based studies and less prone of finding temporal correlation rather than causality.

""the method captures richer information than prior methods"" is clearly an overstatement. You haven't proved the superiority compared to CCM for nonlinearity, or to propagators for indirect connections.  Without saying that since you focus on task-based with few areas justifications against DCM has to be made.

Minors: some capitalization in the bibliography is lost (""granger"", ""keras""...), notation in Sec2.1 could be improved e.g. ""time point of Yt into Yt..."" Then there is \hat{Y}, please clarify or rephrase, for example at the second row  of this section."	"In the original Granger Causality modeling, the corresponding statistics of the significance can be estimated for testing whether incorporating X can ""significantly"" improve the reconstruction of Y. The reviewer would suggest performing similar investigation to derive the statistics of Granger Causality in the current non-linear setting, if possible.

While the idea of using a fixed number (six) 1*2 filter to model the time-varying causal relationship at specific time lags is intuitive and working, the reviewer suggests improving this approach into a more integrated framework."	See the weakness part
359	Nonlinear Regression of Remaining Surgical Duration via Bayesian LSTM-based Deep Negative Correlation Learning	It would be worthwhile to validate the method on other datasets, such as Cholec80. The surgeon experience labels are not necessarily present in other datasets, so testing the method without predicting surgeon experience could also be considered. From now, the method only estimates the remaining time to the end of the surgery. As different tools are needed for different phases, preliminary tool preparation is required in surgery, and the network does have the ability to classify phases, predicting the time remaining in each phase could be a potential development for this BD-Net.	"MAIN WEAKNESSES

Uncertainty evaluation
All the uncertainty-related results (Fig. 2(A) + suppl. Fig. 2) are restricted to selected surgeries. This makes it difficult to understand the performance of the average case and not just the best.
SUGGESTION: E.g. all pearson r's could be presented in a single plot in the supplementary material.

Qualitative evaluation
The example surgeries in Fig 2(A) are all ca 5 minutes long and are thus probably rather easy examples.
SUGGESTION: It would be more insightful to also include more difficult examples (e.g. longer surgeries) and failure cases.

Variance of predictions is maximized
The DNCL loss explicitly enforces the model to have high-variance predictions. How is this compatible with uncertainty estimation? The model should be encouraged to have low variance when it is ""certain"" about the remaining time. The qualitative results indicate that the model does have low-variance predictions in certain cases and that uncertainty decreases over time (as expected).
One explanation could be: Since negative RSD predictions are implausible and thus discouraged during training, the predictions automatically have lower variance when the predicted RSD is lower. However, this could mean that uncertainty predictions look better than they are. The DNCL objective might make it difficult for the model to give meaningful, input-specific uncertainty estimates and instead simply give higher variance with higher RSD values. Fig. 2(A) actually suggests that this might be happening.
SUGGESTION: How do the authors justify the use of DNCL for uncertainty estimation? Has this been done in previous work and if so, how is it justified there? This should be discussed more in the paper.

Missing ablation
SUGGESTION: It would be very helpful to include an ablation without both DNCL and Bayesian LSTMs (i.e. only phases/expertise) since it seems like these two components are most effective if used together. So using only one could potentially degrade performance. E.g. without dropout in the LSTMs, the variance-maximization might be difficult to achieve.
Adding this ablation would make the combined contribution of ""DNCL + Bayesian"" clearer.
Additionally, this ablation would be very similar to CataNet but would be more comparable to the proposed model since the same training and evaluation scheme was used.

Results possibly not completely comparable to SOTA
It is not clear from the paper if 6-fold cross validation was used like in CataNet. Since the SOTA results were directly copied from the CataNet paper and not reimplemented, the scores might not be entirely comparable if all 81 videos were used for training. However, adding the missing ablation from 'Weakness 4' would alleviate this problem.

REQUIRED CLARIFICATIONS

The open questions from the reproducibility section could be clarified.

MINOR COMMENTS

There are some typos: ""annotaitons"" (section 1), ""incorporates"" (section 2)
In the abstract, the phrase ""we deeply learn"" is a bit unusual. Maybe ""we learn a pool of deep, decorrelated ...""?
What exactly is meant by the ""bias-variance-covariance tradeoff"". Maybe the authors can elaborate this in more detail in the paper.
The LSTM visualization in Fig. 1 appears to be based on Colah's blog (https://colah.github.io/posts/2015-08-Understanding-LSTMs/). Maybe this should be referenced.
The term ""end-to-end"" in the caption of FIg. 1 might be misleading since the model is not trained end to end.
Adding statistics regarding the durations of videos would be especially helpful for RSD tasks. What is the mean, median, standard deviation? Or maybe a complete plot of durations could be added to the supplementary material if space permits this.
The hyphen in the evaluation metrics ""MAE-ALL"", ""MAE-5"" and ""MAE-2"" appears to be formatted as a minus in the paper. Maybe using ""\text{}"" in math mode or not using math mode at all when not necessary would make this look nicer.
By naming the ablations by their missing components, I sometimes found it difficult to interpret the ablation results. maybe by representing the ablations with checkmarks it might be easier to follow which components are used and which are missing (e.g. in the style of Table 4 of https://arxiv.org/pdf/1904.07601.pdf)
In section 3 (experimental setup), the authors state that videos range from 5 to 20 minutes. However, Fig. 2 (A) shows two examples with durations of less than 5 minutes. Maybe the authors could provide more precise min and max durations."	"To improve the readability, the authors are suggest to detail the following contents.

Describe the training and inference procedure by inserting an Algorithm, so that the dropout, ensemble, uncertainty estimation, etc. can be clearly introduced.

Explain more about Eq. (5).

Introduce how the default hyperparameters are selected, which hyperparameters are sensitive and should be finely tuned. Given some comparison of different hyperparameter settings if possible."
360	NVUM: Non-Volatile Unbiased Memory for Robust Medical Image Classification	"It is better to explain the following points:
Between Eq. (2) and Eq. (3), what is the definition of \pi_c?
In Eq. (3), why utilizes z_k^i-log(\pi)? It is better to show its motivation.

Since this paper aims to solve the noisy multi-label problem with class imbalance, it is better to individually provide the experiments results on noisy multiple labels, class imbalance, and noisy multi-label with class imbalance.

It is better to discuss the relationship to the following related works, which also generate the predictions using the similar form as Eq. (3), like:
[1] Laine, Samuli, and Timo Aila. ""Temporal ensembling for semi-supervised learning."" ICLR (2017).
[2] Shi, Xiaoshuang, et al. ""Graph temporal ensembling based semi-supervised convolutional neural network with noisy labels for histopathology image analysis."" Medical image analysis 60 (2020): 101624."	"The author should rethink the gradient analysis of the proposed regularization loss.
The author should consider the change of the label distribution and the multiplier of Jacobian matrix during training phase.
The authors should consider to provide more exhibitions to highlight the role played by the proposed method."	"Please add a discussion about the potential size increase of memory module.
It's better to evaluate the proposed method on different noise levels by adding multiple level of noise to a clean multi-label dataset. This is important to explore the noise tolerance of the method.
I also suggest that exploring the class prior distribution estimation under different noise levels, since the prior might heavily degenerate when severe noise in training set."
361	On Surgical Planning of Percutaneous Nephrolithotomy with Patient-Specific CTRs	"Revise sentence in section ""Constrained Inverse Kinematics for CTRs"": ""has been addressed with by"".
Figures need to be more legible and larger.
Need more commentary on the results and its implication."	"The paper presents a neat engineering solution to personalized CTR design for PCNL. It has a lot of potential for clinical application. An image like Fig 1. with the CTR path and tube segments overlaid would help clarify the contributions.
One question I had was whether 5 h for optimization is a clinically viable solution. From section 5, it was unclear whether the 5 h was for all the runs of the algorithm or for one run (and then repeated twice more). Is the runtime feasible for every patient?
How does the proposed method compare to learning-based methods such as [1]? These tend to be faster than traditional optimization approaches at deployment time.
[1] Liang, Nan, Reinhard M. Grassmann, Sven Lilge, and Jessica Burgner-Kahrs. ""Learning-based inverse kinematics from shape as input for concentric tube continuum robots."" In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 1387-1393. IEEE, 2021.
Comparisons to previous works are needed. For example, what would the error in reaching the calyx be using the optimization technique in [1] or [2]?
[2] Morimoto, Tania K., Joseph D. Greer, Elliot W. Hawkes, Michael H. Hsieh, and Allison M. Okamura. ""Toward the design of personalized continuum surgical robots."" Annals of biomedical engineering 46, no. 10 (2018): 1522-1533.
What is the path to clinical deployment after this? Is it realistic and helpful to calculate the particular length of each section and custom make the CTR for each patient, or is it more realistic to expect an algorithm to need to optimize over a set of pre-made tubes?
While the paper presents the clinical goal as being able to reach the calyx accurately, it is unclear how the ability to reach those points translates to the ability for total stone removal. The cases all differ greatly in how closely the ellipsoids resemble the kidney stones they encompass.
Additionally, any discussion on how closely the paths match the constraints imposed by the anatomy would also help understand the clinical benefits this method could provide. It is usually desired that the robot does not exert much force on the anatomy to avoid damaging the tissue. Does the robot sufficiently match the whole path so that is the case?
More discussion in general on the strengths and limitations of the proposed method would be helpful.
Minor grammar note - ""2 cm"" instead of ""2cm"""	This paper could be more strength by comparing by literature works. in addition, the novelty of this paper need to be improved. However the objective is good but what is new your method compare to other methods? i.e. in terms of accuracy of or time assessment, is there any improvement?
362	On the Dataset Quality Control for Image Registration Evaluation	"Quality control and reproducibility of data used for registration evaluation is of vital importance to the MICCAI community. I believe this paper has the potential to make a very significant contribution to the field. At present the paper has a very significant weakness in that it does not properly test the key hypothesis, however if the authors address this weakness I would change my recommendation to strong accept. If the paper included further data on reproducibility and accompanying software to enable deployment on other data I would make an even stronger recommendation.
As detailed in my answer at 5, the paper needs to be slightly restructured and more experimental data added to properly test the hypothesis that variograms provide a reliable way to evaluate the quality of fiducial points used in these data. The authors need to provide a three arm experiment showing that points identified as suspect are identified correctly, i.e. there are significantly more suspect points in this sample than points identified as OK or potentially suspect.
Further to that the authors should provide a more formal description of the algorithms used to classify points from the variograms so that the work could be reproduced or applied to other data.
Other comments:

Page 1, para 2: ""FRE are uncorrelated [8], in practice, TRE is approximated by FRE, and these two terms are often used interchangeably."" You can't claim this without some evidence. If people are using the terms interchangeably they are wrong and need to be put right. Putting this sentence in your paper risks people citing your paper as evidence that TRE and FRE are equivalent. I think you should just delete the second part of the sentence, or say something like ""however a surprising number of researchers fail to understand this.""

Page 2, para 1: ""they may contain FLE"". I think you should reword this. The source of errors of localising a target and a fiducial are not necessarily related. I think you should try something like ""they will contain a localisation error, i.e. the TRE is itself only an estimate of the true error""

page 2 para 6. Maybe a bit more on why you've chosen the variogram over other methods? What other methods are there?

Figures 2 to 6 encode really important information using only differences in colour. Please consider using non colour cues (i.e. different shapes like squares and crosses) and/or a colour blind friendly palette (https://doi.org/10.1038/nmeth.1618)"	Please address issues in point 5.	"after review, average score [1-4] of Cat 1 (problematic): 1.4
after review, average score [1-4] of Cat 2 (atypical): 2.4
after review, average score [1-4] of Cat 3 (normal): ?
For the problematic pairs, the average score of 1.4 ([1(poor), 2(questionable), 3 (acceptable), 4 (good)] generally confirms the variogram category.
-> How many problematic landmarks were eventually noted 3 or 4, if any?

""Both datasets have manually annotated corresponding landmarks on pre-operative Magnetic Resonance (p-MR) and intra-operative Ultrasound (i -US) images [20, 30].""
-> since the landmarks are part of the datasets themselves, better cite the original papers here? [16, 29]. The other papers are mostly stressing the use and importance of the annotations

Eventually less than 5% of all landmarks were of poor quality. These landmarks have to be improved, but this low figure eventually remain a good news!"
363	On the Uncertain Single-View Depths in Colonoscopies	"Honestly I really like the work, but its quality is currently not sufficiently reflected by the manuscript in its current form. In my opinion, only two main issues should be addressed to address this:

Clear and complete writing: The authors should add a more complete description of the proposed approach, that would enable reproducibility of the results. Also, some related work regarding aleatoric and epistemic uncertainty could have been included. Furthermore, some more discussions on the limitations of the proposed approach would be of great value to the manuscript.

Complete and extensive evaluation: Currently, the evaluation is a bit meager at times, especially since the differences with respect to the state of the art is minor. Including more different applications here would really help strengthen the generalizability of the work.

Finally, some minor issues:

Pre-requisite -> prerequisite
""... single-view depth estimation in colonscopy""
An exhaustive analysis seems a bit too much.
I would refrain using perfectly.
MC dropout is a practical and scalable approximation of VI, but not the most reliable.
""Illumination"" in section two on bayesian preliminaries.
18 networks for endoscopy is quite expensive, perhaps Variational inference could still have been applied by making only a few crucial layers bayesian."	"Please revise the literature review of Single-View Depth Learning to provide a more precise summary of the existing works.
On Page 7, ""However, we observe that it successfully generalizes to the real domain..."". Does the network generalize well because there is not much appearance difference between the synthetic dataset and the EndoMapper dataset? Do we expect bad generalization ability when applying to a real endoscopic dataset with a different surface appearance? How many colonoscopic procedures are included in the EndoMapper dataset?
Please address the typos in the paper."	"Authors should reference Liu et al. 'Reconstructing Sinus Anatomy from Endoscopic Video- Towards a Radiation-free Approach for Quantitative Longitudinal Assessment' on pg. 4 where they describe their network's dual outputs of depth and variance (I believe this reference outputs a mean depth map along with the standard deviation).
Are the rotation and translation of Eq. 5 the predicted relative camera motion? If so, please clarify in text.
While minor, it should be pointed out that according to the manuscript guidelines (https://conferences.miccai.org/2022/en/PAPER-SUBMISSION-AND-REBUTTAL-GUIDELINES.html), the 'paper itself must contain all necessary information and illustrations by itself' and not require readers to refer to references to understand, for instance, the evaluation metrics. I understand space is a constraint, but if authors are able to include in parentheses, for instance, what AUCE stands for where it first appears in text, that would be helpful.
To clarify, for evaluations shown in Table 2, the COLMAP reconstruction is used as ground truth?
Since the model trained with supervised GT already performs fairly well on real data, to what is the teacher-student architecture contributing to domain transfer?
The sentence ""Note that, in general, teacher-student depth metrics outperform the models trained with GT supervision in the synthetic domain and with self-supervision in the real domain"" is confusing and it is unclear what the authors are trying to claim. Are authors claiming that synthetic supervision for teacher-student models generates lower depth errors on synthetic data? Should this be shown in Table 1?
Fig. 4 caption - should b) say teacher-student instead of self-supervised?"
364	One-Shot Segmentation of Novel White Matter Tracts via Extensive Data Augmentation	"It is suggested that the authors provide an overall pipeline of the framework, which can help the reader to make a clearer view of the major works in this paper.
Descriptions in Section 3 are too complicated and unclear to understand how they manage to demonstrate the validity of the proposed method, there are many Abbreviations jumping everywhere in that section, and I strongly suggest that they write Section 3 and organize all the terms there in a more comfortable way.
The authors seem to try augmenting the training data for WM tract segmentation, by implementing cutout to the one-shot data. I believe that the authors should further claim why they think that cutout is the most appropriate to this task, as there are a lot of researches in data augmentation for segmentation, many of whom are even for one-shot task. The authors have discussed none of the augmentation literature, which is also surprising to me, and they also haven't compared their method with the alternative augmentation methods available. 
Besides, it is also curious to know what is the upper bound of the WM tract segmentation performance, when using normal number of training samples instead of one-shot manner to construct the segmentation model. In this way, it is clearer to know if this one-shot segmentation has reached its limits, or there remains some spaces for further improvements."	"For the training process, the authors used the original HCP data. To mimic a more challenging and realistic scenario, they decided to downscale the original HCP data and use this downscaled data to further train the model with respect to the new wm tract. This simulates that the data for further training was obtained differently than the already annotated data from the first training. However, it would be interesting to show the performance of the network even if the further training was also based on the initial data (in this case, the original HCP data).
It would also be good to directly compare the one-shot scenario with data augmentation to the few-shot scenario without data augmentation, both in the context of the IFT. This would display on whether few-shots may eventually become some kind of obsolete."	"This is an interesting paper about using transfer learning to predict white matter tracts in dMRI data. Given the fact that in the field the studies of analyzing white matter are limited by the number of available tract segmentations, the proposed method can be useful to generate potentially more based on existing data without requiring tons of manual seminations.
One minor comment that I have the private dataset used for testing has very similar quality with the HCP data. Wonder if the authors have any comments about applying to it to low quality, clinical style dataset."
365	Online Easy Example Mining for Weakly-supervised Gland Segmentation from Histology Images	"*In the fully supervised learning, PSPNet was described as 'ours w/o OEEM'. However, this is not contribution from this paper. The description 'ours' causes miss-understanding.
*In ablation study, there is a mIoU in CAM. CAM is a not mask data, whose pixel has a value. How to compute the mIoU?
*What does the SEAM CAM indicate? The reviewer could not catch up the setup of the ablation study. Please clarify it.
*In the paper, several metrics were proposed, and only the empirical conclusion was described. Please add the discussion why the l_normal was the best.
*Citation [15] and [16] are the same paper (duplicated)."	"(1) It is recommended that the authors describe the method more specifically and clearly, and present not only the common weakly supervised methods but also the special features used for medical images.
(2) It is suggested to add more experiments to validate the methods in this paper."	"I commend the authors for an interesting take on glandular segmentation with image levels with OEEM and its variants.
My main concern is whether the evaluation of SEAM, including the training procedure are sound. Based on Table 2, SEAM and the proposed fully supervised (PSP+Res38) report a significant difference in performance. They both employ the same backbone to produce pseudo masks for segmentation training, yet the reported scores vary. I hope the authors can clarify. 
By the authors own admission, MIL methods are commonly employed for this task. However, no recent MIL methods were included in the evaluation. This can potentially better support the argument.
Based on the current results, it serves to suggest the technique is not model/backbone agnostic. (see prior comments regarding UNet+ OEEM etc)"
366	Online Reflective Learning for Robust Medical Image Segmentation	Please see the weaknesses. I have included the suggestions there.	"a) what is the exact edge detection algorithm used to produce the sketch? cite the paper or the implementation of the method, or explain better
b) In Figure. 2: what is ""Label values"" block? is it a ground truth label or just and integer index of the class ? 
c) How do you create the ""Heatmap""? explain better.  Is the Heatmap a single channel image?  It seem you create it by  multiplying  each probabily channel by the corresponding index  = 0 * p(0) + 1 * p(1) + 2 * p(2).... is that correct?
d) what is the reason to combine (add) the sketch and Heatmap into a single channel image?  alternatively you could have concatenated them into 2 channel input to the Synthesizer. 
e) what is the ""normalized Heatmap H_att"" on page 5, how is it produced? where is it on the Fig 2?"	"Fig 1, the construction of the input to the image synthesis network is not that clear to me. Is it a multi-channel input or a single-channel input, which combines the heatmap with the edge map?
Page 4, ""in default"" - ""by default""
Page 5, 3), please explain what is I? Identity matrix?"
367	OnlyCaps-Net, a capsule only based neural network for 2D and 3D semantic segmentation	Authors could use more metrics for more fair comparison such as Hausdorff, accuracy, sensitivity and specificity.	I do not have any particular remark to do on this paper: references are given, formulas are clear (except (1)) the notion of capsule network is given with its comparison versus standards CNN's. Perhaps that a comparison with separable convolutions (used in some CNN's) should be done since the concepts seem to be similar.	"The work does not address any clinical problem directly. However, it is interesting from an implementation point of view. However, it is not sufficiently justified, especially when looking at the results, which are numerically comparable to existing state-of-the-art methods.
In the absence of significantly better results than state of the art, the contribution would have been complete if the author had provided an in-depth analysis of the impact in terms of computational cost and memory of this type of model.
An analysis of variability through graphs (e.g. boxplots) and more comparison images for 2D and 3D segmentation results would have provided more insights into the discussion of the results obtained.
One of the aims of the work is to reduce the memory footprint of a MatwoCapsNet-based capsule network (cost 2z^2) to z, where is the achievement of this aim demonstrated?"
368	Only-Train-Once MR Fingerprinting for Magnetization Transfer Contrast Quantification	It is difficult to reappear, but the content is very interesting.	The authors have attempted an important problem in the field of saving training time and computational needs for MRF acquisition schedules. The approach of using a bi-LSTM seems appropriate. However, there are some improvements related to explainable AI and moderating of claims which might make this manuscript more impactful. These are outlined in the strengths and weaknesses sections.	"1: The dictionary matching method should be included as one of the baseline methods.
2: The in vivo data is acquired with 4x acceleration. Any reconstruction to reduce the undersampling artifacts before the parametric mapping?
3:  How was the Gaussian noise level determined? Please clarify."
369	Opinions Vary? Diagnosis First!	"Authors propose an interesting idea, well explained and that could have many uses not only in the particular application field that they have chosen. Therefore it has some merit in it.
Experimentation also shows some good results but fails to allow to definitely conclude that the way to measure and consider multilabel by means of the expertness analysis in diagnosis network. Some results show a decrease in performance that is not correctly discussed."	see section 5	"In section 2.1, a private dataset is used to show the diagnosis performance of segmentation masks with different qualities. The details of this private dataset should be given. Why not use the public dataset REFUGE-2 as section 3?
Which diagnosis network is used to show the performance of segmentation masks with different qualities in Figure 1?
The result of reference [26] based on ExpG in Table 1 is different to Table 2."
370	Opportunistic Incidence Prediction of Multiple Chronic Diseases from Abdominal CT Imaging Using Multi-Task Learning	Please assure that scans from the same cases are not used for training and testing.	This paper designed a multi-task low-label learning method for opportunistic incidence prediction of multiple chronic diseases from abdominal CT imaging. A multi-planar 2D CT processing method is designed to extract useful information for five diseases, which reduces the dimensionality of the volumetric 3D data and outperforms 2D single-plane approaches. The proposed method achieve outperformance in 5-year incidence prediction of CKD, DM, HT, IHD and OST.	"I enjoyed reviewing your paper, well done on your work. I have some points to kindly raise:

How do you discard the predictions of the secondary disease? Not clear to me.
The optimal parameters chosen - how did you conclude this, trial and error? or the strongest associations/lowest losses drove these choices?"
371	Optimal MRI Undersampling Patterns for Pathology Localization	"I find the the network training details in the Supplementary material. It it better to include some important part in the paper.
Even though the results look promising, how to validate the  learned undersampling patterns working well?  Will it miss some important information?"	"There are few minor grammatical issues; I would suggest the authors to thoroughly check and revise them.
There are too many footnotes in the manuscript, which could have been easily written inside the main body text.
In Table 2, it is not clear which null hypotheses the p-values are referring to.
In Eq (2), Y_hat is not defined.
After Eq (3), c_{1,2} should be c_1 and c_2.
Page 4: Covariance is calculated between two variables; hence, it is not clear what the authors referred to as 'covariance of the pixel intensities'."	The authors propose a new paradigm for accelerating MRI which takes account into the downstream pathology localization tasks. They need to provide the implementation details about IGS and LOUPE optimization. They can compare more advanced MRI under sampling methods like pg_mri (with greedy policy search) instead of Center or FastMRI.
372	Optimal Transport based Ordinal Pattern Tree Kernel for Brain Disease Diagnosis	Due to my limited expertise in this field I cannot comment further.	"The authors may add a more detailed description of how OT is implemented to compute the OT distance between sub-trees.
The authors may add more controlled experiments on a different combination of methods.
The authors may further justify the relevance between the OPT and brain network analysis."	"Besides the question in Section 5, here are some other questions:
(1) How to guarantee that two graphs have the same number of levels in the form of your tree?
(2) If the same region has multiple nodes, how to choose which one is the root in the tree structure?
(3) Evaluation of classification results is expected to have precision, recall and F1 score."
373	ORF-Net: Deep Omni-supervised Rib Fracture Detection from Chest CT Scans	"Please address the questions I listed in the weakness section. 
In addition, I would suggest to reformat the equation 1, following the best practice of   matrix , vector and scale format."	See above	"Figure 1 should be improved about how the label assignment's input and output. Current figure is a little confusing.
Section 3.2, ""Note that, We enable ..."" -> ""Note that, we enable ...""
""As shown in Table 1, By simply ..."" -> ""As shown in Table 1, by simply ..."""
374	Orientation-guided Graph Convolutional Network for Bone Surface Segmentation	"It might be worth including the inter or intra rater variability for the test set as it will allow the readers to better understand the performance of the proposed neural network.
It might be worth assessing the impact of data augmentation on the overall performance of the proposed neural networks."	"It would help the reader if the authors explain what they mean by 'bone connectivity' or 'bone shadow' where it is first mentioned. Perhaps a figure would help.
Also, it would help the reader to understand the effect of disjoint segmentation if authors describe how that would affect CAOS procedures. What level of discontinuity would affect the results? Looking at fig 3, row 2 method MFG-CNN the slight discontinuity does not appear harmful if guiding the surgeon is  the goal. Perhaps explanation of the severity of the consequences can help the reader better understand the significance of the solution. 
Pg 2 ""next we propose utilizing orientation as an ..."". Can you please clarify the orientation of what with respect to what? Also in pg 3. ""By tracing them in a specific orientation"". It is not clear what orientation means here. 
2.1. Please clarify what portion of this is the authors' novelty and which is existing work.
Pg 3. ""Existing bone segmentation networks only utilize..."" Please provide reference.
Please read the text to correct some typos. 
The dataset consists of images from 'healthy' volunteers (I assume this means healthy from an orthopedic point of view). Additionally, in the introduction, application for orthopedic surgery is mentioned. I'm curious to know whether and how images from patients requiring orthopedic surgery would differ from the dataset used. If it does differ (e.g. non smooth bone surface?) how the performance of the algorithm would be. 
In what situations does this method not work well?"	"Overall, this paper was well done with detailed methodology and experiments described clearly.
Major revisions:
1) The clinical motivation for the task is not well-explained in the Introduction. Only the first two sentences introduce the clinical problem but the unmet clinical need is not clear and should be described more fully to allow the reader to understand the clinical relevance.
2) Limitations of the proposed approach should be acknowledged and described in the Discussion.
Minor Revisions:
Abstract:
-Second last sentence: ""in"" is missing before ""vivo""
-Second last sentence: Define all acronyms on first use (US has not been defined)
Introduction:
-Paragraph 2, Sentence 2: The sentence beginning with ""Note that the difference..."" is very unclear. Please rephrase and clarify to strengthen the rationale.
-Paragraph 3, Sentence 3: Replace ""good"" with ""better"".
-Last sentence: ""in"" is missing before ""vivo"", as in the Abstract.
Discussion:
-Ablation Study should be with Experiments and Results rather than in the Discussion section."
375	Orientation-Shared Convolution Representation for CT Metal Artifact Learning	This is excellent work. It would be even better if the authors also discussed the limitation of the proposed method more in detail.	"This paper addresses the metal artifact reduction problem in CT reconstruction. The main method is based on a recent method called DICDNet. In this work, based on the prior fact that metal streak artifacts are rotational because of the back-projection algorithm along different angles, the authors improve the existing DICDNet by integrating an orientation-shared convolution representation into their network design, which can reduce rotationally symmetric metal artifacts better.
The overall structure of the manuscript is clear and the method has been compared with state-of-the-art methods. Not only qualitative results are displayed, but also quantitative evaluations are performed. The clinical feasibility is also demonstrated by the experiments on clinical data. These are all the good points from the paper.
However, in Fig. 5, we can see that compared with NMAR and DuDoNet, large streaks between metals are not reduced, which is a major disadvantage of the method. The authors should find a way to overcome this limitation.
Since the backbone method, i.e., the DICDNet, has already been published. This work is an improved/modified version. Hence, the contribution is incremental."	"""Furthermore, it is difficult to collect the sinogram data in realistic applications [9]."" This sentence is factually incorrect. In order to reconstruct a CT image this raw data is necessarily acquired. I believe this sentence is supposed to express the practical difficulty of acquiring this data if one does not collaborate with CT manufacturers or builds an own CT. Please reformulate this sentence if this is the intention. This is especially important because classically approaches in projection-domain where favored since they allow dealing with the rotational streaking artifacts easier, before they occur. In addition for deep-learning approaches different prior works have shown that correcting metal artifacts in projection domain is advantageous e.g. in https://ieeexplore.ieee.org/document/8331163.
I don't understand equation 5. Why are all the terms multiplied element-wise by ""I""? Doesn't that make the term superfluous? From equation 9 I get the impression that ""I"" serves to direct the loss at image areas which are metal artifact affected. If that is the intention ""I"" could just be introduced in equation 9 and described like this.
I also don't understand equation 9. Isn't the objective minimized if X + C * M = Y ? That would just mean that any additive artifact model automatically minimizes this model. So simply setting up a global residual connection in a network where Y - A = X, where A is predicted by a network (A = f(Y)) would automatically minimize this. What am I missing here?
I am generally unconvinced about the necessity to use the presented prior knowledge. The rotational artifact structure stems from the fact that it is caused by inconsistency in projection domain, between different projections which causes such artifacts on circular trajectories. Therefore, projection-domain (sinogram-based) methods implicitly also use this prior knowledge. Even better, if more complicated trajectories are used, which can cause different artifact patterns, the methods in projection-domain would still exploit this appropriately.
The evaluation should be improved by providing results on measured data. The method description should be simplified, e.g. by providing a sketch of how it works. In addition an ablation study should be done to show that all aspects of the method are relevant."
376	Out-of-Distribution Detection for Long-tailed and Fine-grained Skin Lesion Images	"An ablation study on the susceptibility of the model against the selection of grouping is missing.
It is not clear how did the authors train the prototypical networks. How did they decide on the feature level to be picked for extracting embeddings?
Equation 3 is not clear. Why do the authors put d_{xpi} through the network again? Assuming that ""f"" represents the model, why and how would the output of f(d_{xpi}) be y_i?
What type of images is the in-house dataset composed of. Dermoscopy images, clinical images?
The authors did not compare their model against the algorithms from the literature that were designed explicitly for OOD detection for skin images. Some examples are;
-M. Combalia, F. Hueto, S. Puig, J. Malvehy, and Veronica Vilaplana. Uncertainty estimation in deep neural networks for dermoscopic image classification. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 3211-3220, 2020.
-Abhijit Guha Roy, Jie Ren, Shekoofeh Azizi, Aaron Loh, Vivek Natarajan, Basil Mustafa, Nick Pawlowski, Jan Freyberg, Yuan Liu, Zach Beaver, et al. Does your dermatology classifier know what it doesn't know? detecting the long-tail of unseen conditions. arXiv preprint arXiv:2104.03829, 202
-On Out-of-Distribution Detection Algorithms with Deep Neural Skin Cancer Classifiers. Andre GC Pacheco (Federal University of Espirito Santo, Brazil)*; Chandramouli Sastry (Dalhousie University, Canada); Thomas Trappenberg (Dalhousie University, Canada); Sageev Oore (Dalhousie University, Canada); Renato Krohling (Federal University of Espirito Santo, Brazil) 
-Torop, M., ""Unsupervised Approaches for Out-Of-Distribution Dermoscopic Lesion Detection"", arXiv e-prints, 2021.
-Subhranil Bagchi, Anurag Banerjee, and Deepti R Bathula. Learning a meta-ensemble technique for skin lesion classification and novel class detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 746-747, 2020."	The authors provide most of the implementation details except for the values of hyperparameters (lambdas). Providing these additional hyperparameters in the revision would facilitate the reproducibility.	" There are several modification that could allow the improvement of the paper.
 Method:
 - Could the authors detail how the lambda is selected as it does not clearly stand out from the paper?
 Experiments:
 - could the authors detail why 20 classes were chosen as OOD? Is there any ablation having been done?
 - Could the authors revise ISIC class labels as it might be not clear for unaware reader?
 - Could the authors specify how the precision, recall and F1 are calculated for multi-class classification in imbalanced dataset?
 - Could the authors motivate the choice of the ResNet34?
 Results: 
 - Is it possible to report the classification metrics (prec, rec, f1) for the tail classes?
 Mixup Ablation
 - could the authors clarify/discuss more their choice of M-T strategy for further experiments?
 Conclusion
 - The authors state ""OOD techniques are still far away from clinical deployment"". Could the authors provide further details of such conclusion?
 Syntax and writing:
 Introduction:
 - in ""There are two shortcomings in this aspect - 1) ... 2)"" could the authors revise the syntax of the items 1) and 2) to facilitate the reading
 - The main contribution appear to be somewhat drawn in the text. Could the authors consider outlining it differently?
 "
377	Overlooked Trustworthiness of Saliency Maps	"For the relevance experiments, the authors fine-tune the network and enforce the condition that the image looks within an \epsilon of the truth image while also optimizing for the saliency map to look identical. The result ends with an adversarial attack where the class changes, but the saliency maps stay the same.
For some saliency maps, e.g., Grad-CAM, which back propagates the highest output class probability, is it correct to think the network is somehow learning the saliency map invariance? What would happen if you took the Grad-CAM of the truth class, i.e., atelectasis, in the authors' example? How would that change the Grad-CAM? Can relevance be established then?
Further, can the authors comment on the real-life scenario where such an attack can happen? For both methods, there was a need to fine-tune the network for a few epochs to enforce this saliency relationship and create the adversarial image. Without fine-tuning, if you have a white box or a black-box attack, isn't the expectation that the performance of a regular model drops, and the saliency will also be meaningless? What would be the outcomes if an adversarial image was run through this model without the fine-tuning of the model eq 2-4 and using a standard attack?"	"There are some minor typos in the paper: page 5, ""Each adversarial image xp is is"", ""optimizerfor"", ""Adversarial images xs for evaluating resistance is""; page 8, ""The propose properties"".
In terms of the existence of transferable saliency map attacks, it would be interesting to see a more theoretical analysis of the methods and results obtained (something in the line of Ancona et al. [https://arxiv.org/pdf/1711.06104.pdf?ref=https://githubhelp.com]). Maybe to consider for a future journal version of the work."	"Please refer to the weaknesses of the paper.
Although this work has solid with experimental evaluations that is valuable for the MICCAI community, how to apply the concepts and results to improve the trustworthiness of saliency maps in medical image application is unclear."
378	Parameter-free latent space transformer for zero-shot bidirectional cross-modality liver segmentation	The lack of baselines and alternative approaches renders this approach interested, but not well connected with the literature. It is uncertain if these approaches would be superior to established techniques.	The evaluation is not adequate. More experiments should be done to compare the proposed method with other related works.	"Adding different comparison experiments and using the common shortcomings derived from different backbone experiments to show that the problem proposed to be solved in the article is a problem that exists in many current studies. The use of only one backbone method does not effectively indicate whether it is a problem of the backbone itself or a common problem of existing studies.
Optimize the common feature representation of the proposed potential space to better balance the features between the two modalities. The current latent space formed by the a priori knowledge of liver intensity seems to be more biased towards the feature representation of CT images and less expressive for MR images, which makes the segmentation from CT to MR to a greater extent than the segmentation from MR to CT under the same conditions, and also the segmentation from CT to CT to a greater extent than the segmentation from MR to MR"
379	Patcher: Patch Transformers with Mixture of Experts for Precise Medical Image Segmentation	"The technical novelty of the Patcher encoder is not clearly explained.
It seems that the MoE-based decoder tends to be over-claimed. It just utilizes an attention block to weight features at different CNN layers."	Please make the sub-subsection titles in Section 3.1(and others if applicable) in consistent formats.	"As the main contribution of the paper is the creation of a new neural network architecture, it would have been welcome to compare it against established benchmarks in the field of medical imaging. Comparing this new architecture with SOTA methods on a new dataset, or a dataset where other state-of-the-art technics had not been applied yet, does not let us know whether the model is globally better, only excellent at the selected 2 tasks, or whether the other methods were not applied with the same amount of efforts. To clear up any doubt, please evaluate your model on a segmentation benchmark where some other technics have already been applied. As an example, [1] does not report the same result for UNet depending on training conditions.
For the ablation study in Table 3, putting the relative increase/decrease in accuracy would be more enlightening than the absolute results achieved by each combination. Try to answer: ""What is the relative increase in DSC/IoU when swapping SETR decoding method with ours?"". Absolute results are helpful, but are not enough to get the full picture.
[1] Huang, C. H., Wu, H. Y., & Lin, Y. L. (2021). Hardnet-mseg: a simple encoder-decoder polyp segmentation neural network that achieves over 0.9 mean dice and 86 fps. arXiv preprint arXiv:2101.07172."
380	Patch-wise Deep Metric Learning for Unsupervised Low-Dose CT Denoising	"The paper proposed a novel unsupervised learning approach for low-dose CT reconstruction using patch-wise deep metric learning. Experiments confirmed that the deep metric learning plays a critical role in producing high quality denoised images without CT number shift. However, there are some concerns as follows:

The sample size in this study was limited and the one-time split could not verify the generalization ability of the network well.
Cross-validation should be conducted for further validation.
Authors should provide more detailed descriptions of the pipeline in section 2.1. The description should include how the GAN is applied to obtain output images from noisy input images, how features are extracted from these two types of images and whether two generators have the same parameters. Moreover, authors did not present how to combine output images with low frequency images to obtain final results.
In fig. 3, using high frequency images as the gold standard is beneficial to present the difference images.
In table 1 and table 2, the title should be written before the table."	"The approach needs more ablation study to demonstrate the improvement in performance with CT numbers shift.
The approach needs to be compared to more existing state-of-the-art approaches, which include both supervised and unsupervised methods.
The motivation of maintaining the original CT number is questionable.
The motivation of the objective function is also questionable. More experiments should be conducted to show how it works.
The approach needs more detailed description of the model blocks, datasets and experiment implementation to access better reproducibility.
The related work needs more content about deep metric learning and the related methods applied to the medical image processing.
I wonder whether the image of domain Y can be called HDCT, because the model does not use any HDCT image, but only denoised LDCT images."	"Perform the quantitative comparisons with a larger and more modern dataset.
Justify the choices for the windows, or choose more conventional windows.
Perform more comparisons with non-GAN state-of-the-art methods."
381	PD-DWI: Predicting response to neoadjuvant chemotherapy in invasive breast cancer with Physiologically-Decomposed Diffusion-Weighted MRI machine-learning model	"Sec 3: the the -> to the
scale_pos_weight -> please explain to which method/component this hyperparameter belongs, and what it does.
Addressing weaknesses 2 and 3 would make the paper stronger."	It is interesting to see the model was only based on breast DWI data, but it would be useful to investigate a way to combine different imaging sequences, particularly DCE-MRI. DWI sometimes suffers from insufficient image quality, and a combination of DWI and DCE will either improve the performance or the reproducibility. Different feature selection approaches with and without clinical information need to be further investigated to improve the generalizability of the model.	"(1) May consider the other feature selection and machine learning modeling methods, not only ANOVA and XGBoost.
(2) The work selected 100 features with highest ANOVA F-values for the modeling. I don't think it's an appropriate decision, usually a feature need at least ten observer samples.
(3) Whether most samples all meet the rule of Fig.1?
(4) Lack of radiomics feature repeatability analysis in the new maps.
(5) Please add the discussions why the new maps-based radiomics models all got the optimal performance."
382	Personalized Diagnostic Tool for Thyroid Cancer Classification using Multi-view Ultrasound	"Section 3: Please clarify how many patients are included in the dataset.
Section 3: It is not mentioned how many images were included in the dataset after data augmentation. Please clarify this.
Section 4, Table 1: Are the results in this table showing the performance of each approach on one common dataset, or each approach is tested on a different dataset?
Section 4: Proposing a personalized diagnosis tool that is one of the main claims of this paper needs to be more discussed. What are the main advantages of having such a tool in comparison to the other methods while the performance of the proposed approach is slightly better than the state-of-the-art?"	"The authors should conduct more experiments to demonstrate the effectiveness of the proposed mothed.
The authors should explain why the reproduced AdaMML has poor performance.
The description of the data collection process should be completed, such as descriptions of the experimental setup, device(s) used, image acquisition parameters, subjects/objects involved, instructions to annotators, and methods for quality control. The authors should clarify if the 4529 sets of multiview US images were collected from 4529 patients."	The authors should provide more details of experimental implementation and computation complexity for the proposed method.
383	Personalized dMRI Harmonization on Cortical Surface	"1) It will be helpful to compare with the standard cifity surface-based analysis. 
2) The diffusion MRI data from HCP has a very high spatial resolution. For standard clinical data, the resolution will be much lower. Then the partial-volume effect will be a significant problem for surface based analysis. Adding more comments or a solution to this problem will certainly improve this work."	"The authors stated ""Alternatively, surface-based registration can alleviate some of this anatomy misalignment problem for dMRI harmonization, but it is still insufficient to resolve this challenge."". Please explain why surface-based registration is not sufficient to solve cortical mismatch or at least provide reference for the statement.

Are the improvements in Table 1 significant? Please provide the p-value of the test.

Is there any explanation that HCPD harmonization task is always better than HCP harmonization task?

The authors only harmonize b=3000 shell, then how FA and MD was calculated? Only from b=3000 shell or from the unharmonize b=1000 shell?"	"Recommend authors to avoid colloquial language
Evaluate on more datasets"
384	PET denoising and uncertainty estimation based on NVAE model using quantile regression loss	"The assumption that the output distribution follows the logistic distribution should be justified.
The authors mention supplementary material but none was available.
From the authors' point of view it is a good thing that the proposed approach has a larger variance but would the clinicians, i.e. the users, think the same way?"	"It would really benefit the paper to present example cases or datasets to highlight the performance of the model in a research/clinical context (in an ROI or lesion-based approach), as well as other voxel-based metrics to indicate performance. While PSNR or SSIM are important metrics, a smoothed denoised image might provide high quantitative values on these metrics while missing important high-frequency information or small lesions for example.
Page 1, Abstract: Not all abbreviations were first defined in the abstract section. For example, PSNR and SSIM.
Page 2, Introduction 2nd paragraph: While the authors have included the description and extension of VAE and NVAE to provide a better understanding of the improvement, the Unet-based model literature was relatively lacking. It would be equally important to provide a bit more context rather than briefly mentioning Bayesian neural networks and suDNN.
Page 2, Introduction 2nd paragraph: The authors used tumor identification as an example to describe the urging need for the novel technique to improve the PET denoising and uncertainty estimation. However, the dataset used to test the model of interest in the study was a brain imaging PET data.
Page 5, Dataset: It would be great to include more detailed information on the PET data acquisition such as the dynamic framing, different types of corrections (transmission, scatter, random) applied, etc.
Page 5, Data analysis: Based on the manuscript, the authors have reported using 200 training epochs and a 0.01 learning rate. How do the PSNR and SSIM change over different training epochs and different learning rates? Do the authors expect any further improvement?
Discussion: Would this model still perform better than the other models for different PET tracers?
Figures with a color scale bar should indicate the unit."	"Explain what is meant by "" group of low-quality and high-quality training pairs"".
Include a reference or an expression for the structural similarity index measure (SSIM).
section 3.2 is confusing because it describes the usual VAE loss but it refers to it as the loss used in this work.
Was there a reduction in variance shrinking?"
385	PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation	"This paper presents a network architecture that combines the design of convolution and transformer blocks so that both local and global visual representations are captured. The motivation is clear and experiments validate the effectiveness of the proposed approach.
I have the following concerns.

The relationship to prior work is not well analyzed. In particular, In ICCV 2021, there is a paper named <Conformer: Local features coupling global representations for visual recognition>, which delivered a very similar idea to this work. The authors shall add discussions on this topic. In addition, it is helpful to provide some visualization (or other analytical results) showing how the mixed architecture helps recognition - please also refer to the above paper for examples.

The improvements on both datasets (BCV and ACDC) are marginal compared to the baseline and other competitors. While I understand that the baselines are already high, but, I strongly suggest the authors to provide additional results to support that the improvement is solid (e.g. by qualitative or other quantitative studies). This is very important considering the small volume of the studied datasets.

Overall, introducing a novel architecture is helpful for medical image analysis. The paper is well written and I recommend weak acceptance."	"Recently, many transformer model paper for medical image segmentation have been made. However, most of them do not make fair comparison. I am curious why the author have cited UNETR but do not show its performance. And why the author followed the paper setting of nnFormer(https://arxiv.org/pdf/2109.03201.pdf), where it outperforms nnUNet in BCV/Synapse dataset, but in this PHTrans paper, nnUNet is better than nnFormer. And also, in UNETR (https://arxiv.org/pdf/2103.10504.pdf), its results are better than PHTrans, so I suggest author add detailed results of UNETR(https://arxiv.org/pdf/2103.10504.pdf) and Swin UNETR(https://arxiv.org/pdf/2201.01266.pdf) because I think those two transformer models are SOTA right now.
Back to framework of this paper, I think it lacks novelty. The most novelty part in PHTrans is adding Conv Block parallelly in ""Trans&Conv Block"". Too many parts of PHTrans are referenced from Swin Transformer and UNet(encoder-decoder) architecture.
Moreover, some SOTA models such as CoTr and nnUNet are also trained from scratched, they outperform certain transformer models which have pretrained models. So I think PHTrans does not need pretrained model, which is great, but that is not enough."	"The manuscript will benefit from a clear definition of Volume-to-Sequence (V2S) and Sequence-to-Volume (S2V) operations;
The definitions of W-MSA and SW-MSA are not given;
The authors declared PHTrans w/o ST essentially have the same architecture as nnU-Net. While we observe a difference between table 4 (PHTrans w/o ST have DSC=87.71 and HD=14.37) and table 2 (nnU-Net DSC=87.75 HD=9.83). Can the authors provide some intuition about what caused such difference, particularly in HD?"
386	Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography	"The proposed method is called PICTURE and is compared to unsupervised learning and OVERWIND from reference [2]. The methods are compared on both phantom and in vivo liver scanning data using CNR and SR (strain ratio), two common metrics of comparison. No simulations are performed where the ground truth of displacement is known.
The main contribution of this paper is the development of a learning-plus-physics based approach to displacement measurements that does appear to perform well in these limited test cases with some exceptions where it performs worst.  As negatives, the amount of testing is rather limited, there is no sensitivity analysis to the parameters, and there were no simulations performed with known ground truth. This is therefore a good but early contribution that needs more analysis to prove its benefits with confidence"	Paper is very good and in good shape. Other than that I recommend to look into hard-coding the physical properties into the network (see weaknesses)	"In Eq. 11, how do you warp I2? How does data loss constrain the displacement W?

In Eq. 12, the axial strain is constrained to be near the mean, while the strain in other directions is constrained to be zero, and all first-order derivatives are constrained to be zero. As a result, the strain is the same for all frames, which is not the case in reality.

In Eq. 14, how do you select the window, especially the background window?

The authors should describe detailed information about the dataset, such as the number of sweeps for experimental phantom and the number of patients for in vivo data.

The unsupervised results in Fig. 1 are a complete failure, which questions the validity of data loss and smoothness loss. The authors should analyze the reasons for the failure and conduct further other ablation experiments.

The authors should show more cases in Fig. 2.

Why does the EPR histogram of PICTURE (Fig. 1 in the supplementary material) range beyond v_emin and v_emax, with two peaks?

Can PICTURE be applied to out-of-plane displacements?"
387	Physiological Model based Deep Learning Framework for Cardiac TMP Recovery	"This paper would benefit from further improvements in validation. The outstanding question that all inverse ECG papers should answer is how  generalizable is this approach to different subjects. This is particularly sensitive  for data-driven approaches that require  training and  testing separately.
More importantly, the authors should include noise of some form in the data.  Otherwise, the results will always be good, but not realistic.
I would encourage exploring the ECGI database in  EDGAR (https://edgar.sci.utah.edu/).
As a minor comment, it would be useful to clarify what are the axis in Figure 4(b)"	"proofread and fix grammatical errors e.g. 'is verify' on 6th last line of abstract; 'those powerful' on 2nd last line of abstract; 'a imprecise' after eq 9; 'a precise results' after eq 9; 'to instead the' in conclusion;
proofread and fix typo/formatting errors e.g. text after eq 1, text after eq 4
fig 4 right part is not clearly legible"	See above.
388	Physiology-based simulation of the retinal vasculature enables annotation-free segmentation of OCT angiographs	"Overall, this is a novel and original paper. The proposal is interesting and the use of synthetic data for training demonstrates advantages on pre-training, as well as providing a way to train vascular segmentation on 3D OCTA.
However, the description of the experimental setting is lacking details. Specifically, the reproducibility issue described above is important, in my opinion, because of the following reasons:

Adding an stopping criteria significantly changes the settings with respect to those reported in [16]. The authors should clearly report what this criteria is, otherwise the amount of training is difficult to evaluate, and, as a more relevant issue, depending on the specific setting, some networks may have stopped way earlier than others, inducing biases in the comparison.
Considering a fixed number of epochs, and comparing networks trained with different dataset sizes, induces a bias regarding a different level of overall training of the networks. This is because the number of network updates depends on the total number of minibatches, while the decaying learning rates (i.e. the strength of updates) depends on the epoch number (using the poly rule reported in [16]). Thus, larger datasets imply a larger number of updates with larger learning rates, i.e. more training. If the networks have not reached their full potential, the comparison may be biased.

Related with this topic, there is a potential issue with the results reported in table 2. On the one hand, the trainings with 32 vs 320 vs 3200 synthetic datasets imply completely different training settings. While larger datasets could imply larger diversity, the question arises on what would happen if the 32 images where presented the exact same number of times, with the exact proportion of learning rates as the for the 3200 images dataset (i.e 3200 over 200 epochs vs 32 images over 20000 epochs with decaying learning rates accordingly). Moreover, the case of the synthetic + finetunning (320 images over 200 epochs + 32 images over 200 epochs) vs real (32 images over 200 epochs) may also be imbalanced wrt the refinement level.  This is something that worth look into, as it is not guaranteed that the networks have reached their maximum refinement (and potential of the data) when the training stopped, nor any note has been provided regarding this limitation, nor regarding any actions taken to prevent this potential bias in the comparison.
It is important to explicitly report if augmentation was used, and their details.
As a minor detail, the authors should add the cite number after Liu et al. in page 3 (i.e. [14]), and after Ma et al. in page 6 (i.e. [16]). Otherwise, the citing style is not coherent."	"This is overall well-wrritten and holds a lot of potentials to tackle an important clinically relevant problem. There are a few statements like ""Qualitatively, we find our segmentations to be superior (see figure 4). "" Can the authors comment on this and how generalized this superiority of results hold for different example images."	"It is necessary for the author to explain in detail the number of samples, size, sampling method, etc. contained in the dataset, which is helpful for other researchers to conduct repeated research.

In section 3.1, the authors only stated that they extended a method by Schneider et al. However, for a vessel tree, how to determine the parent or child tree and how to select the root node, I think it is necessary to give an example to explain in detail, which is a key step in the simulation.

In Section 3, the authors describe that they have performed image deformation to simulate the typical curved shape of the retina. However, the details (e.g., method and setting) of how you exactly perform the deformation are not given.

I have doubts about the operation of three-dimensional deformation of blood vessels, including how to confirm the connection point of SVC and DVC, and whether this bending deformation conforms to the actual retinal anatomy, all of which need to be verified.

By observing the examples shown in Fig. 3, I feel that there still have room for improving the model. The synthetic data is still different from the realistic data in terms of the noise level and vessel intensity distribution.

Since the authors use the ROSE data for evaluation, it would be more intuitive to also compare with the OCTA-Net developed for ROSE data.

In the introduction part and Fig. 1, the authors declare that their method is the ""proof-of-concept of 3D segmentation of OCTA images for the first time"". As far as I understood, this is an inaccurate statement. The authors should better review the literatures published on IEEE-TMI, IEEE-JBHI, MICCAI and ISBI for the methods developed for 3D OCTA segmentation and analysis."
389	Point Beyond Class: A Benchmark for Weakly Semi-Supervised Abnormality Localization in Chest X-Rays	I think the symmetric consistency has less to do with the point label and may be also used with full-labeled data.	"Overall, I believe this is a good paper with clear motivation and reasonable design. It may be stronger if:

the result is effective on at least another dataset and the code is released
Improve the caption of Fig 2 for better presentation."	"Suggestions to answer my concerns:

Clearly state that Bearman ar al.'s work was done on the PASCAL VOC dataset and that the data was not CXR images.
Slightly nuance the mAP improvement claim in the abstract e.g. give the average improvement for each detector arch. or when using a certain %age of labeled data.
Regarding the claim of having produced a ""publicly available benchmark"", either state that the code used to produce the results will be made public or change the claim to having produced a ""new method"".
 
Only a couple typos, good job:
4 Experiments, subsection ""Dataset"": this subsection should rather be called ""Datasets""
Caption Table 2: WSOD - WSSOD
Overall ""chest x-rays"" appears often even after the acronym ""CXR"" has been defined. You can probably replace a few occurrences
 "
390	Poisson2Sparse: Self-Supervised Poisson Denoising From a Single Image	"1) The assumption of noise to be poisson distribution is very restrictive. It will be good to devise an approach where there is no assumption on noise. Or specifically , when the signal strengths is comparable to noise.
2) It will be good to observe the behaviour of alpha and check it susceptibility for different type of noise."	"More experimental detail and better quality of data is needed.
The optimization and denoising needs to take place in the projection/k-space domain, rather than the reconstructed volume domain.
A comparison to the original ISTA optimiser needs to be shown, to demonstrate the superiority of the neural network based modification.
The advantage of this method to standard supervised learning approaches needs to be explained and experiments need to be conducted to show it."	"1)	The method overview presented in Fig. 1 should be more clearly described to highlight the innovation of the proposed method.
2)	Why the structure of autoencoders is described as a recurrent neural network requires further clarification.
3)	In the experimental section, the format of table titles of Tab. 1 and Tab. 2 seems to be unconventional.
4)	As the proposed method is an extension of iterative optimization methods, using deep neural networks, it would be better to compare it with the iterative ISTA method. When it comes to practical applications, computational efficiency should also be considered in the experiments."
391	Pose-based Tremor Classification for Parkinson's Disease Diagnosis from Video	It would be nice to see the final results when OpenPose shows wrong results or the results of applying other recent pose estimation approaches.	The authors should better identify the support that has the proposed tool and how this characterization may impact Parkinson following. Also, the authors should study the OPenPose outputs to determine the level of noise, similar to tremor.	"The authors should already specify in the abstract which type of movements / posture the videos should contain - the method does not require any type of video.
The authors should specify (or at least estimate) the detection accuracy of the OpenPose 2D human pose detection framework.
Noise in the predictions of the 2D joint locations could be interpreted as tremor by the proposed framework. The authors should discuss how this issue could be addressed.
The authors should specify the exact inputs and outputs (size and format) of the proposed model for reproducibility.
The authors should discuss the multi-class classification performance in more detail."
392	Position-prior Clustering-based Self-attention Module for Knee Cartilage Segmentation	"I think the proposed method is a flexible module, and I think it should be applied for more organ extraction problems (including the ""health"" knee data).

The proposed method is an improved/modified self-attention idea, and the authors should focus on the comparisons to the self-attention or attention-based approaches. E.g., describing more about the improvement to [10] or its follow-up papers, or comparing with some different attention-based papers and their follow-ups (Attention U-Net: Learning Where to Look for the Pancreas).

Adding 3D visual comparisons for 3D segmentation.

I have a concern. The experimental data has a very large size in 3D, and the GPU has 11-GB memory, how did you implement your method on a 3D network with 4 down-sampling and 4 up-sampling layers, and also the batch size is 4?

I suggest that if you used the Erode operations, you should clearly say it, and avoid some vague implementation descriptions in the sub-section ""Position-prior module""."	N/A	The use of PCAM clearly improves the performance of the cartilage segmentations. PCAM includes 3 submodules, as a future study, is it possible to experiment the effect of individual submodules to the performance gain?
393	Predicting molecular traits from tissue morphology through self-interactive multi-instance learning	"On page 3: ""...we use k representative tiles with high attention scores to fine-tune CNN encoder f_res..."": It is not sufficiently clear how this f_res is fine-tuned, since it is pre-trained on ImageNet. It would be better to also give a brief description here.
On page 4: k^1 and k^2 are better to be k_1 and k_2."	"One question about dataset: why spliting training and test sets can alleviate the small sample size problem?
More experiments:
(a)  Quantitative ablation experiments for investigating the different parts of selected instances (i.e. the attention tails, supplementary tails and negative tails) are necessary.
(b) Analysis of hyper-parameter sensitivity, e.g., the (defined/sampled) number of three kinds of tiles, the weight of adversial training and the chosen of L_final/L_init.
(c) Since selected instances are further used in fine-tuning the backbone+fc, one can directly use the output of fc for instance selection, which seem more reasonable than attention as it is unconstrained.
Visuliazation: 
(a) T-sne: Instead of increasing the number of top tiles, it would be help to visulize the lowest-attention ones, as this is the difference between inter and adInter training.
(c) As attention score is generated after softmax, it would be help to clarify how blue-green-yellow-red/blue-white-red colour map are defined."	"I am concerned with the statement: ""The training and test sets are generated using bootstrapping for 10 folds  ..."", to my understanding this scheme would let the same samples to fall in both training and testing sets, since bootstrapping is performing sampling with replacement. In this case, there will be some bias in the presented results.
The authors state that they did not utilize a validation set in order to select their models and instead they just utilize the last snapshot of the model for testing. This is quite tricky since in fact there is no certainty that the different models converge in a similar manner or even that their convergence is stable. Hence, it could be the case that a competing method would reach similar performance with the proposed with a proper train/val scheme.
I believe that AUC can be quite cryptic in terms of classification performance, considering also the fact that it is quite low in some cases and there is the extra concern of data leakage to the test set. I would suggest the authors to complement it with additional metrics like BACC, F1, Sens, Spec as well as the ROC curves."
394	Predicting Spatio-Temporal Human Brain Response Using fMRI	Please see the comments on the main weakness of the paper.	This paper can further improve the comparative experiment.	"a) The details of how LSTM and GRU-D are implemented for comparison are missing. At least, it should be included in the supplemental material.
b) It's better to include more visualizations in Fig. 2 and Fig. 3. For example, the time series in Fig. 3 is from one of visual networks. What about the other ROIs?
c) I think the prediction performance varies from region to region. In which ROI, the proposed model performs worse? It's interesting to exploring the performance degeneration. 
d) Does the number of parcels have an effect on the model's performance?
e) What is the limitations of the proposed model?
f) Correct the grammar errors."
395	Privacy Preserving Image Registration	"Regarding the motivation of the work, the authors could make a bit more effort to come up with a realistic use case. The SSD metric itself is mostly suited for intra-patient registration, and this would pretty never require your PPIR. For inter-patient, atlas- or multi-modal scenarios on the other hand, this might become relevant, mostly in academic & research scenarios similar to federated learning, i.e. where sophisticated registration-based anatomical atlases can be added to, with data from multiple privacy-preserving sources. The limitation of simple SSD metrics & gradients might be overcome to do something like pre-processing on each party's computer (self-similarity, a modality synthesis GAN etc.).

Please improve the presentation of the results a bit. Show the computation times in total (not per iteration), also showing the time of the original method (it won't be 0.0 seconds as in the table now), to allow to estimate the relative performance loss. Maybe you can also print the required network bandwidth for the original scenario, i.e. if one were to send the pixel data for each iteration in an unencrypted fashion.
Can you come up with anything more innovative than subsampling? Are there specific mathematical properties of the encryption methods that lend themselves to more specific algorithm changes during the registration?
If the overall description of the MPC and FHE approaches can be further improved for people not familiar with such encryption methods, this becomes a really nice manuscript that bridges the gap between two otherwise quite disconnected technical domains."	Authors are suggested to apply for other tasks, i.e., segmentation, detection.	The authors should provide clear evidence motivating the proposed algorithm.
396	ProCo: Prototype-aware Contrastive Learning for Long-tailed Medical Image Classification	Please see the weakness.	"Small Issues:

In the title, I think Medical Classification should be Medical Image Classification, unless there are additional experiments presented (e.g. clinical text, audio, video)."	"Why the calibration factor reflects the difficulty of each category? Why add the factor with the prototype? Please provide more details or proper references.
Kappa metric is a popular metric for an imbalance dataset. The author should consider reporting this metric.
As stated before, the hyperparameters gamma and E are important for ProCo. The authors should put more words on them."
397	Prognostic Imaging Biomarker Discovery in Survival Analysis for Idiopathic Pulmonary Fibrosis	The only minor point is that the physiological meaning of the  novel C36 biomarker should give more explanations.	I would like to congratulate the authors for their impressive work. My only comment is that comparison should be performed both with standard methods and with other recent techniques.	"The proposed framework follows a two-stage procedure. In stage 1, self-supervised is used to train a ResNet to extract features from image patches. In stage 2, patch-descriptors are clustered and a ViT is trained to predict risk scores of survival. Overall, the proposed approach is interesting and combines interesting ideas from unsupervised deep learning and transformers to predict survival of patients with idiopathic pulmonary fibrosis. However, there are some issues with the paper that should be addressed.
Major issues:

Additional baseline should be included in the experiments (see above).
How sensitive is the proposed approach the various hyper-parameters, in particular the number of clusters K?
CT images are 3D images, yet, image patches seem to be 2D. In which plane are the patches, and how is the third dimension treated?
In section 2.2, the authors mention sequence length N, but it is unclear how it relates to the patch-descriptors computed in section 2.1. Does N correspond to the number of extracted patches? The experiments seem to indicate otherwise, which does raise the question how exactly this sequence is defined.
In section 2.2, the authors write that ""queries within the same cluster can be represented by a prototype"". Please clarify what query and prototype are? Are queries patch-descriptors (from step 1) and the prototype the cluster centroid (from K-means)?
In section 2.3, the authors discuss how novel biomarkers can be discovered, however important details are missing? First, how are ""existing biomarkers"" defined? What is the measure of correlation? What does ""relatively far"" exactly mean? How is the p-value to measure ""predictive of mortality"" computed, and has multiple testing be considered?
The ablation study in table 2 mentions two entries, which are not sufficiently explained. ""w/o contrastive learning"": How is the ResNet from step 1 trained in this setting? ""w/o attention pooling"": How are per-patient predictions formed in this model?
In table 1, how can the proposed model (ResNet-18 and ViT) have less parameters than the ResNet-18 model?

Minor issues:

Please provide more details about the datasets, in particular about the follow-up period and the amount of censoring. A Kaplan-Meier curve would be helpful.
At which time points was the IBS evaluated?
Please add the Kaplan-Meier curve as a lower-bound of the IBS to table 1.
The Cox-loss has only been re-discovered by ref. 18, but was originally proposed by Faraggi D, Simon R. A neural network model for survival data. Stat Med 1995, which should be the preferred citation."
398	Progression models for imaging data with Longitudinal Variational Auto Encoders	"What disease stage groups are included in the training and test group, respectively, and why? How does input of different disease stage groups influence the result?
I like the style and the amount of information delivered through Figure 1, but I think more contents can be added to it to make the model more clear. For example, the authors could add symbols besides the lines and arrows in the middle figure to make it clear. During training, what parameters have actual meaning and what parameters are from a Gaussian sampling?
In algorithm 1, it would be good to elaborate the simulation and approximation part in the main text, or refer to some citations so that readers would know how this process is simulated.
In the result section 3.2, the authors claimed that the minimum dimension to capture the dynamics of structural MRI is 16. Could you provide citation where this number comes from?
In Figure 3, only the synthesized images are shown, and it doesn't seems to be clear enough for a T1 MRI image of the whole brain. Also the resolution (809680) does not seem high enough. Could the authors provide a sample original image in Figure 3 for comparison?"	I don't have much to say. The paper is very well written, the problem challenging and interesting, and the solution smart and appealing.  I would like to read something about the use of interpretability with the parameters estimated by the method in a clinical application, maybe within the last line of the conclusion. I believe this is a great paper for Miccai.	Please check the weakness section. A thorough suvery of related work and the comparison to existing methods, both traditional and deep learning based methods are necessary for demonstrating the effectiveness of the proposed method in the work.
399	Progressive Deep Segmentation of Coronary Artery via Hierarchical Topology Learning	Adding evaluation results using public dataset will clarify the effectiveness of the proposed method.	"1) The evaluation does not properly analyse a vessel-level view as adopted by the scoring methodology used in the 2008 and 2012 MICCAI challenges. This evaluation is especially crucial as the correctness of the stenoses segmentation is much more important than the correctness of the ""healthy"" part of the vessel. Hence, the authors should make an effort to provide results on cases with stenosis and justify objectively.
2) The author should pay attention to the expression in the figure. The distance map proposed in the method is dependent on the surface of the chamber, but the figure shows the center of mass. 
3) The author should analyze the specific reasons why the method brings improvement, rather than engineering the method and boasting the advantages"	"References are incomplete. The key points and cube-connectivity branches in HTL module come from existing algorithms in the field of computer vision, which are applied to coronary artery segmentation in the paper. Reference <1> has proposed a similar approach to cube-connectivity branches. The paper adds a channel on the basis of 26 neighborhood, but the main idea is similar .
<1> Qin, Y. , et al. ""AirwayNet: A Voxel-Connectivity Aware Approach for Accurate Airway Segmentation Using Convolutional Neural Networks."" 2019.

The experimental results in this paper are very intuitive, and it will be better if the following supplements can be made in the future. It is suggested to add some intermediate result diagrams in the supplementary material in the future to show the interpretability of each branch, such as distance factory diagram and key point diagram. According to the network design in this paper, it may be helpful for some clinical research fields that want to directly output the centerline or key points of coronary artery."
400	Progressive Subsampling for Oversampled Data - Application to Quantitative MRI	"""network architecture hyperparameters e.g. number of layers and hidden units, is a task-dependent problem"" - NO. As long as your dataset is large enough, more layers etc. ALWAYS increases performance. This is not task-dependent. The balance between hidden layers and number of features is task dependent though.
NAS terminology.
This paper ""only"" performs a (smart) hyperparameter optimization (which also falls into the broad category of AutoML, which notably is also the terminology used by AutoKeras [20]). Both the supplementary materials and the code imply, that the following, critical NAS characteristics are missing: changes to the topology (e.g. skip connection), different choices for layers (see also NASNet, DARTS, such foundational papers are completely missing from the related work).
This issue can be easily fixed by replacing neural architecture search (NAS) by Hyperparameter Optimization (HO) or AutoML.
Evaluation.
The framing of the challenge makes a clean evaluation difficult, as some biases are engrained in the challenge framing.
As is, I would strongly recommend to exclude a subset of signals (for some direction + b-value) from the selection (1st) network manually, but include them in the second as outputs only and only report results on these ""calling them test signals"", otherwise your network is encouraged to recreate measurement noise. I understand this will be out of scope for a rebuttal. At a minimum, we would need to understand inhowfar the noise characteristics of the different signals differ. I.e. I believe, the direct comparison in Table 1 is invalid.
Example for SOLUTION: New Table, which reports the performance grouped by b-value (as a proxy for SNR) and whether the ground truth signal was in the input or not.
Table 1 does not have to list results for 8 different values of M (I'd be happy with 4 to make space for a second table, also M>=100 is in my opinion irrelevant for the superresolution aspect).
Generally, the writing is often unclear/hard to follow/imprecise.
To just understand the text, most of the paper has to be reread multiple times. To illustrate: ""PROSUB has an outer loop: steps t = 1, ..., T where we simultaneously perform NAS and RFE, choosing the measurements to remove via a score, averaged across the steps, whilst simultaneously updating the network architecture hyperparameters.""

Long (check), convoluted (check), number of verbs (6 in one sentence) with parts that is not even a proper sentence (steps... where)

""PROSUB is not limited for subsampling MRI data sets"" -> ""PROSUB is not limited to subsampling MRI data sets""
""We determine this by (i) by constructing"" -> ""We achieve this by (i) constructing""
""Recursively over steps t = 1, ..., T RFE prunes the"" -> ""Recursively over steps t = 1, ..., T, RFE prunes the"""	"This works appears as an excellent engineering paper, which sets some best-practice standards. 
My biggest concern is with the importance of the task it sets out to solve. This concern may be adressed by explaining it clearer in a rebuttal and also adding explanatory sections in the paper. In addition, the evaluation leaves a very one-sided impression. All the comparison methods are variants of SARDU net. It would be more convincing if other methods were used. E.g. one simple comparison could be a simple linear model where each volume is predicted by a linear combination of the set of subsampled volumes. The set of volumes for the subsampling could be determined e.g. by random search. Alternatively I expect a look towards compression methods and dictionary learning methods should yield strong alternative baselines."	The authors have used a meaningful approach to solve the dual problem over a standard dataset, especially improving the performance of a previous winning submission and related versions. The authors are requested to look at the strengths and weaknesses section for further comments.
401	Prostate Cancer Histology Synthesis using StyleGAN Latent Space Annotation	Clinical application of this approach is not clear. I would suggest the author to provide some evidence to show that this approach can be beneficial to some clinical tasks.	"It'd be great it was possible to validate the method on public data (PANDA dataset).
Also, some technical details are missing, for example, how GAN was trained? More details on the training process would be useful to share, ideally the code."	"A general comment: I think not all choices or ideas are clearly explained in the paper. It would be good to check the manuscript in general and try to explain all ideas and assumptions as clearly as possible.

Could you briefly explain why you used annotated samples in the first place to train the model? Was the annotation necessary to have a balanced dataset for the unsupervised training part? (Maybe I have missed it while reading)

p2. ""Finding the latent point from an image - whether real or synethetic - is known as GAN inversion. While recent work improves GAN inversion [19,7,22], we found these approaches not pixel accurate on histology."" Could you explain this a bit clearer? What was the problem with those images?

p2 ""Considering all the points sharing the same category, we apply principal component analysis (PCA) to describe the variation of these points within a unimodal latent cluster."" What was the (geometric) intuition behind this idea? Could you briefly explain it?

p.6 ""Each latent point was truncated toward the mean of entire latent space using factor of psi=0.6 [15], reducing the number of unrepresentative features within the cluster while preserving diversity."" Also here, could you briefly explain why you do this?

p/.6 ""In addition to the Z channel, the StyleGAN network has a random noise channel that influences the layout of features within the image [8]. In generating images within a category, the noise channel was fixed so that the layout of glands, nuclei, etc, in the images would remain fixed while the classification of the images changed."" A few, brief explanations here would help: What are the different components of the model, what kind of information do (we think) they capture?

p.7 ""Second, the GAN provides a quantitative approach to comparing Gleason grades. Gleason patterns are often arranged from least-cancerous to most-cancerous tissues. The categories confused in Figure 2 (b) follow this same scheme"" This is a very interesting finding. Have you tried to visualize the geometry (e.g. via dimensionality reduction) of the latent space to see if it fits your interpretation?"
402	PRO-TIP: Phantom for RObust automatic ultrasound calibration by TIP detection	"(1) The designed phantom consists of nine cones. The relationship between the number of cones and the calibration accuracy should be analyzed.
(2)The author said ""we simulate Gaussian and speckle noise of various scales"". Please add more explanation.
(3) The author uses 4 pairs of tips to produce a calibration hypothesis. Whether the more pairs of tips, the higher the calibration accuracy?
(4) Weather the proposed calibration phantom is suitable for various ultrasound probe? For example, the linear probe and the curve probe."	Explaining the augmentation method	"Overall, this paper was interesting and well-written, solving a useful challenge; however, the following need to be addressed to properly understand the work.
Major Revisions:
1) Introduction: Although thorough and clearly written with the limitations of current methods described, it is not entirely clear how the proposed approach will overcome the limitations of previous methods and why the need for new phantom rather than combining image-based detection with any of the existing phantom approaches.
2) The paper should acknowledge the limitations of the proposed approach and discuss.
3) Approach: Details of the CNN implementation should be provided, most notably the dataset split.
4) Approach: Data labelling - Details of how the tracked sweeps and label map are registered are not provided. This is critical to explain, as this affects the ground truths used for training and evaluation. Were the automatic segmentation maps validated in some way prior to use?
Minor revisions:
-Fig. 1: as the two sub-figures (a and b) are extremely similar, including both does not add much value. It would be better to show a photo of the physical phantom in (a) instead.
-Experiments and Results: given the large disparity between the average and median distance metrics (~10mm vs. ~1 mm), a rationale for the outliers should be provided, as well as a normality test on the data to aid in the interpretation of the results"
403	Prototype Learning of Inter-network Connectivity for ASD Diagnosis and Personalized Analysis	"We advise the authors to assign an editor to revise the paper, examples of such improvements would be:

Abstract, ""by comparing to competing"" -> ""by comparing with competing""
..."	"Additional Points and Clarifications:

In section 5-Personalised FC Analysis, the authors base their analysis on randomly selecting two ASD and two controls subjects for identifying the top 5 ROIs with the largest variation. However, since this sampling was done only once, it is unclear how stable these variations (and thus the selection of the top ROIs) are.

How is the positional encoding matrix e in Eq. 1 calculated?"	"Detailed comments are in order of appearance in the paper.

The paper makes use of the self-attention mechanism and multi-headed attention modules. However, the original paper proposes such structure is never cited and should be included: Vaswani et al., All you need is attention, 2017.

I am not sure what the variable z_r^L represents. I think that z_0^L is the summary vector representing the entire FC networks. But where do z_r come from? This needs to be defined/clarified. Perhaps labeling where these variables appear in Fig. 1 would also be helpful.

For the 5-fold cross-validation setup, are the partitions done subject-wise? If so, it is confusing because this setup is mentioned immediately after discussing data augmentation, so it is not clear if the split is performed just on the augmented data as a whole, or whether it is by subject. And if not by subject, this would greatly inflate classification performance, since the augmented data per subject is highly correlated.

Related to the cross-validation setup, is the pre-training done using the same partitions as the classification training (i.e., same test set is left out the whole time)? While the pre-training does not make use of labels, the same network is being used to learn the classification in step 2, and thus the test data needs to be left out the whole time.

As mentioned in the paper, the training is performed in 2 steps - pretraining of the transformer reconstruction network, then learning of the classification model. How does the performance change when trained in 1 step, end-to-end? There is no additional data added in the pretraining as far as I can tell, so I am wondering about the advantage of pre-training vs. end-to-end training, since the classifier is trained in step 2 in an end-to-end model. Comparison to 1-stage training would strengthen the case for 2-stage.

The hyperparameter settings, e.g., for the lambdas in the loss function for classification learning are given in the supplementary. However, I wonder how these parameters were chosen? Was any tuning involved, and if so, was a validation set used, or is based on the testing set? Clarification on if any tuning was performed (for the proposed method and other methods) and if so how this was done would be appreciated.

For the hyperparameters such as in the loss function, how sensitive are the results to the choice of parameter settings?

In Table 1, the AUC means are around 0.6-0.7, but the standard deviations (or standard error? please clarify) are reported in the range of 2-4 - are the decimals off and this should really be 0.02-0.04? The overall means are also reported as proportions, but the standard deviations appear to be in percent. Please check values.

For the main classification performance results, while some sense of variation is given, there is no statistical significance testing reported - this could help strengthen the case for the proposed method."
404	Pseudo Bias-Balanced Learning for Debiased Chest X-ray Classification	important topic has been studied, and not needing a labelled bias info makes the approach practical; but, I am missing the discussion on the impact of the method in no-bias case, and when multiple forms of bias are existing in the data.	"1- In problem statement: Source-biased Pneumonia (SbP): ""We then sampled 5, 000xr% pneumonia cases from NIH and the same amount of healthy cases from MIMIC-CXR. Here, the data source became the dataset bias, and health condition is the target to be learned."" Do you mean here that the subset of data can cause this bias? as I see that you randomly sample the exact number from each set/class. I didn't get how this can mimic a bias.
2- In problem statement: Gender-biased Pneumothorax (GbP):Here it is much straight forward. However, I just wonder why didn't you consider a direct bias from the label? Instead of having this sort of indirect bias through one of the covariables?
3- In algorithm 1: Are fB and fD independent networks? Does not share weights?
4- In Page 5 ""Giving f(x) the softmax output of the model, denoting fy=j (x) the probability of x being classified to class y = j and th the parameters of model"" It would be better if th is included in the f function definition.
5- In table 1: G-DRO looks having best results with ground truth bias label. Is that comparison performed on an independent datasets? If used on the same dataset so the GT bias should be available for your model as well.
6- The improvement in results looks marginal."	Consider superimpose the class activation maps to the x-ray images to see: based on which part of the image, the biased/debiased model is making the final decision. This may help the readers get a more intuitive understanding.
405	Radiological Reports Improve Pre-Training for Localized Imaging Tasks on Chest X-Rays	The authors study the effectiveness of existing text-supervised methods and compare them with image-only self-supervised methods. The authors did a good job of evaluating the text-supervised methods to contrastive methods and in-domain and cross-domain transfer from classification methods. The results and justification look reasonable and might be useful to the community. It would be interesting to see the results for the deeper backbone for UNet and the higher resolution of the input image.	Please add a detailed analysis that why the text-supervised method is better and how the radiological reports help to improve the performance. For instance, show some cases with original images and radiological reports, analyzing which parts of the radiological reports helps. Or, using some grad-cam method to show the attention regions of radiological reports. Meanwhile, please show analyze the differences in the performances of these self- and text-supervised methods in natural and medical images.	"Could you explain more on the sensitivity of pre-training methods to the size of the downstream tasks shown in Fig. 1? If I understand this figure correctly, the results shown is the performance on 1\% or 10\% data of the downstream task relative to the full data. However, it is not clear why this metric is important. Providing an example could better explain the importance of this sensitivity.
It could be better to show the results of combined contrastive and text-supervised learning. Does this combination improve the performance? If yes, does it always outperform each of them in the studied datasets?"
406	RandStainNA: Learning Stain-Agnostic Features from Histology Slides by Bridging Stain Augmentation and Normalization	"Providing computation times for the whole pipeline with and without preprocessing would have been useful.

This might be a great contribution to the computational pathology community if it is included in a library and distributed to researchers to use it. Or include the method in actively developed computational pathology libraries such as[4,5]

I think the method is indeed useful, but for me its lack of statistical analysis and comparison with more recent methods makes a borderline accept decision needed.

[4]https://github.com/TissueImageAnalytics/tiatoolbox
[5] https://histolab.readthedocs.io/en/latest/index.html"	"Comments:

The subsection 'Virtual stain normalization template' consist of not clear description, please rephrase
sigma_j is not defined (p.4)
(p.4)""The empirical results suggest [...]"" - meaning that there is no proof, please expand on that
(p.5) please provide a proper reference for the MoNuSeg dataset
(p.5)""we generate different virtual templates for images that vary at every epoch during the training"" - this is not described enough, please expand
on p.7 please double check if there should be SN1 & SN2 or maybe there should be SA1 & SA2 instead
the proposed ablation study is questionable - please reconsider with proper testing."	Please refer to the weakness section.
407	Real-Time 3D Reconstruction of Human Vocal Folds via High-Speed Laser-Endoscopy	"Improve the lit review to show how your method is different from existing methods, especially [21].
Crystalize your naming of the different parts of the method so that they are described the same in Fig 1, in their text descriptions and in Table 1.
Add a quantitative comparison with [21].
Explain what labeling error is being evaluated in Table 1."	It is a very well written manusctript presenting scientifically sound (and potentially amenable to cilinical workflow) framework/pipeline for real-time 3D reconstruction of vocal folds geometry. My main reservation regarding the proposed framework is that while the reported results seems sufficient to support the conclusion that the obtained 3D reconstruction is visually appealing and can be provided in real-time, they do not appear to satisfy the requirement for quantitative accuracy and robustness that would be required for clinical application in diagnosis of laryngeal and voice related disorders. Providing justification for the error measures used in the study and interpretation of the results in the context of the accuracy required for clinical applications, would improve the manuscript.	"Overall, the paper reads well: adequate background to the problem is provided with reference to the state-of-the-art methods, methods and the results are presented well.
The described reconstruction pipeline is very similar to the one presented in reference [20]. Without explicitly describing how this paper differs from [20], the authors contributions are difficult to identify.
The authors assume a calibration between the camera and the projector. How is this calibration estimated? How are the camera intrinsic parameters estimated? How good are the estimates? This information is crucial to the reproducibility of the paper.
The authors use epipolar lines to constraint the correspondence search. If a centroid of a projected dot hits an epipolar line, it is considered a potential match. However, with errors in calibration, practically, the dots do not exactly hit the epipolar lines. Therefore, some distance measure (between the centroid and the epipolar line) with a threshold has to be considered. What distance did you use?"
408	Recurrent Implicit Neural Graph for Deformable Tracking in Endoscopic Videos	I have to admit to being a little out of my depth here. The paper is generally clearly written, with lots of technical detail.	"I don't quite understand the sentence ""Each of these layers can be thought of as a new initialization: no weights are shared between each ph or g."" Does this mean there are multiple g? Or does 'each' only refer to the ph? In this case, this may be cleared up by writing ""... are shared between ph_1, ph_2 and g""? Maybe the word ""initialization"" could also be replaced, since this already refers to choosing initial weights in the context of neural networks?
""with the difference being that we add in a relative positional embedding to let the network select based on position."" I did not understand this sentence, because Equation (1) uses the absolute pixel positions p_i and p'_i (and the distance between them), but this sentence instead mentiones ""relative"" positions. Can you explain what you mean by ""relative positions""? To what are they relative, and how are they used exactly? Or is this refering to the p_i - p_j in Equation (4)? In this case, maybe you can move this sentence to the next paragraph?
I assume the gamma in Eq. 1 and Eq. 5 are not the same? If so, could you use a different letter?
""We set the base offset to be the barycentric estimate, helping similarly to how a skip connection helps learn the residual in CNNs. We perform barycentric interpolation on the Delaunay triangulation of the refined neighbor node
displacements"" -> I had to read this a few times. Maybe you could start by saying that the base offset is set to an interpolation of the refined displacements in the neighborhood, and then go on to saying how it's done (via barycentric interpolation)? Otherwise the two sentences sounded to me like you were performing two different steps.
""The information at the query point is broadcast to each of these neighbors, run through two graph convolutions and then pooled."" and later: ""We first broadcast information from q to each neighbor."" - This is the same concept twice, maybe only mention once?
Suggestions for Fig 2:

If I understand correctly, these are two ""unrolled"" steps of the RING network? Maybe the difference between the two boxes would be slightly clearer if the titles would be changed to: RING (at time t-1) and RING (at time t) or similar.
In both boxes, h_q^(t-1) is used. I assume this should the t-2 in the first box and t-1 in the second?

For the SCARED dataset, the ground truth is calculated - could you show the errors (as images) of each sample in the supplementary material? That would be interesting to see (are the errors larger on the tools, or distributed equally etc.)
Minor:

""we calculate new features for and refined displacement estimates for each match."" -> remove first ""for""?"	My only complaint is, as mentioned, if the 2/3 of the method is from a previous method, those could have been more brief. Then, there could have been more space for experiments etc.
409	Reducing Positional Variance in Cross-sectional Abdominal CT Slices with Deep Conditional Generative Models	This paper should illustrate more on the new clinical application and technical motivation. I also have a question regarding the experiment part. Fig. 4 shows spaghetti plot of muscle and visceral fat area longitudinal analysis. But muscle and visceral fat may change overtime. Some important detailed illustration is missing here, or in the introduction. See detailed comments in the weakness of the paper.	"The colors in figure 2 are quite faint.

The paper lacks extensive quantitative evaluations from other registration and generative model based baselines."	Authors may use other methods for comparison such as CycleGAN and Pix2pix.
410	RefineNet: An Automated Framework to Generate Task and Subject-Specific Brain Parcellations for Resting-State fMRI Analysis	"In addition to the above problems, the author should rearrange the sturcture and content of the paper, especially in method and result sections. there is only a Section 3.1 in Section 3, and acctually there are various results in section3.1.
The authors should rehearsal expression and description, i.e., in ""The  coherence term S uses the pearson correlation coefficient with each mean time series"", Whose and whose relevance is this?"	"The paper is well written. the results are convincing, particularly when trained for tasks, the cohesion measure for resting state is still higher using the proposed method than the original one.
Maybe one super minor comment: the RefineNet by itself is very shallow and may not be called ""deep"" neural network."	"1) Please improve the clarity of this paper, especially the method section. 
2) The framework of the proposed method illustrated in Fig. 1 can be improved. Some key words or symbols in Fig. 1, e.g., network weights, ne(v),A_v^j,... are not explained clearly in the main content. It is better to place the definition of s_{v,p} after mu_1,...mu_P and near S.
3) It looks like the experiments did not include all the subject in the respective datasets, please explain the subject exclusive criteria clearly to improve reproducibility.
4) It is better to describe the method of how to select the best hyper-parameters in the proposed method. For example, how to determine that epoch= 5 and I=20. 
5) To validate the effectiveness of the proposed method, is it possible to design another comparison to make sure the better accuracy comes from the co-training strategy?
In the comparison, first, obtain the brain parcellation with the state-of-the-art individualized parcellation method; then adopt the corresponding deep learning framework used in the three tasks to get the prediction results; At last, compare the results with the ""Combined"" proposed in this paper.
6) An extra ""a"" in the first paragraph of section 2."
411	Region Proposal Rectification Towards Robust Instance Segmentation of Biological Images	See the details above.	"The weakness I mentioned above might be helpful for the authors to improve their work.
I would suggest the authors test on more challenging datasets and if possible give some failure cases of this method. As shown in Table 1, the proposed method does not beat the baseline on every metric. It's interesting to see and analyze some failure cases."	"The authors propose an RPR module to improve object detection and thereby improve segmentation quality for better volume and shape estimation. The paper is well written.

Given that the improvements expected are small, mainly along excluded areas near the boundary of the object, average precision as the only evaluation might be insufficient. Consider including more metrics like Dice coefficient and Average surface distance / Hausdorff distance.
The improvement in AP for segmentation task when using mask RCNN with RPR was <2% in Table 1. However the improvement in IoU in Fig 4a is >20%. Please provide the IoU plot after non max suppression.
From Fig.5,the proposed module improves the regression of bounding boxes but the segmentation AP improves only marginally. Suggest looking into other segmentation metrics to understand if the proposed module improves sensitivity. Also, the edges could be weak, making it hard to segment; i.e., good bbox regression might not always improve the segmentation quality."
412	Region-guided CycleGANs for Stain Transfer in Whole Slide Images	"Please refer to my comment in the weakness section. In addition, 
(1) the authors claim that the original CycleGAN fails for the task. Is there any reason for the failure? Is the failure attributed to the discriminator? The discussion would help to motivate the work.
(2) It is suggested to include a discussion on the mis-localizing DAB by the proposed method in figure 2."	"I would recommend following for the future work:

Extension to other applications
Is it possible to make the approach less dependent on the annotations?"	I made some suggestions to help improving the experiments of this paper. Please see the Weaknesses section.
413	Regression Metric Loss: Learning a Semantic Representation Space for Medical Images	"I found EQ.6 somehow ambiguous, given a test sample x_t,  and its label y_t, why the distance only computed on semantic representations fi of training space. what is the relation between f_t and f_i? In fact, Do we need f_t during testing? or maybe f_p is f_t?
In supplementary: the radius r used in Fig1. is less than 1. It is unclear why \epsilon=10. Is it a step? It is unclear how the performance is evaluated between r and r+10? while r\in{0,1}.
In proof of Lemma 1. What is D? In the case of closed geodesic, the authors claim that the uniqueness is violated. Could the authors elaborate a bit more? Does this condition breaks the bijection property and so global isometry becomes not true?Does this means obtaining multiple representations f in the manifold that match a single label representation - like multiple-to-one correspondence?
The ablation on full CAC datasets are somehow difficult to interpret. For example, we observe that not using the mask m was better than using m for same values of sigma and alpha, and when sigma is infinite (the linearity case), the results look comparable to non-linearity. why this behaviour?"	"In section 1, the claim << To learn a meaningful representation space for regression, a loss should be able to ... margin in loss functions >> is central to justify the methodology but is clearly not enough justified or even simply illustrated by a convincing example. I would recommend the authors to make cristal clear the pertinence of this claim in medical imaging before developing the methodology;

In section 1, the sentence << It guides the deep learning model to learn a low-dimensional manifold that has the same semantic meaning as the label, ... >> introduces the notion of << semantic meaning >>, which is not described and far to be obvious when talking about regression. What is the meaning of this notion?

In section 2.1, is a $d_t$-dimensional vector, a one-hot encoding representation of different labels? If this is the case, why binary vectors would live in Euclidian spaces?

It is impossible to know from Eq. (1) what is actually optimised. The authors should use a  $\hat{\theta} = \arg\min_{\theta} ... $ like formulation of the problem.

What is $l'$ in Eq. (2) and where is it used later?

Just before Eq. (2) how the sample pairs are selected in a training batch?

If I understand well, the loss is computed for a whole mini-batch. Could we use it to compare a single $f_i$ to a $y_i$ (which leads to the information that is usually backpropagated and then averaged into the mini-batch)? Maybe an algorithm explaining how to train a neural-network using this loss would help understanding how this loss can be used in practice.

The paper contains many typos. The authors should use a spell checker before submitting the paper, eg:
<< Various clinical risk or measurement  ...>>
<< Resent studies ...>>
<<...  and E a Euclidean ... >> 
..."	"The authors should better pay more attention to the details, 'cause there are some typos in the context. E.X. ""Mean Squared Error (MES)""."
414	Reinforcement Learning Driven Intra-modal and Inter-modal Representation Learning for 3D Medical Image Classification	"1) May be discuss some qualitative results. 
2) In table 2, please make the best performing number as bold.
3) May be add another dataset in future."	"Please properly motivate the use of RL for classification and show how it addressed the mentioned challenges.

2, This paper is not properly motivated. After introducing related works, the authors suddenly jump to the discuss of the novelties. I am kind of lost why RL for classification. Why it can be used to address the mentioned challenges?

Please properly formulate the problem before introducing the details model structure.

Are the comparison methods the state-of-the-arts? If not, please compare with the top methods for the open challenges."	"(1) For the proposed model, how to obtain the final classification results? 
(2) Compared M2Net with the proposed model, they all adopt the modality-specific network and shared network. There, what are the main differences and its advantages? More discussions should be included. 
(3) The dataset looks small, with only 165 subjects. This is a huge limitation in medical imaging, and it is an issue of the data for other researchers, thus the small sample issues and some related works should be discussed. It is also expected to discuss where the proposed model fails to predict some samples."
415	Reinforcement learning for active modality selection during diagnosis	This is a very well written and interesting paper , nice work !	"The class predictions given the current state are computed using support vector machine classifiers. However, the authors do not evaluate the classifier's quality, thus may lead to some bias in the final results. Intuitively, the reinforcement learning algorithm can probe these classifiers during training, which can learn some shortcuts from them. Further, a classifier needs to be learned for each superstate, resulting in many classifiers.
In Fig. 1, the authors can show the meaning of each point and explain why the number of points is different for reinforcement learning and the population-based method.
In Fig. 2, the authors can explain how the Dice coefficient is computed.
The authors can elaborate on why the modality selection can be formulated as a Markov decision process (MDP).
Typically, reinforcement learning has a parameter gamma in the MDP formulation, and the authors choose to set the gamma to 1. The authors can give some discussion on it.
The authors can give a more precise description of s_{n+1} given s_n and a in section 2.2.
The authors can further refine the paper writing and organization, e.g., abstract, notations."	"Please consider increasing the opacity of the plots in Fig. 4 for better visualization.
Please comment on the high similarity between GLS curves across individuals of all groups (and why higher importance was given in the third group).
Small typos were found throughout the manuscript (e.g. in page 3, twice appears ""an MDP"" rather than ""a MDP""). Please revise it."
416	Reliability of quantification estimates in MR Spectroscopy: CNNs vs. traditional model fitting	"This work gives a nice warning about the potential pitfalls of relying completely on deep learning for MR spectroscopy.  There may be value in discussing the link between this observation and similar observations that have been made in the context of MR imaging:
Antun et al, On instabilities of deep learning in image reconstruction and the potential costs of AI. PNAS 2020
Chan et al, Local Perturbation Responses and Checkerboard Tests: Characterization tools for nonlinear MRI methods. MRM 2021.
There is very likely a similar underlying principle"	"A few specific comments:

Regarding estimation of the different types of uncertainty, one approach the authors don't consider is to train the network specifically to output an estimate of uncertainty cf. (Tanno et al NeuroImage 2021). The authors might consider that when extending this work.

Figure 7 it's a little hard to decipher.  Could be reorganised to make clearer what is experimental result and what is ground truth."	"As perhaps a naive question (but one which could be addressed in the paper), what if one simply simulates an extremely large range of concentrations such that one is sure that these concentrations will never be reached. Would this perhaps reduce the variance but increase the bias? It would be interesting to see the model retrained with different data where the parameter bounds are increased.
As mentioned, the study is restricted to a single CNN architecture which was found optimal after a hyperparameter search. If expanded to a journal, I think it would also be worth it to reproduce the architectures of previously published studies. Furthermore, I think it is even worth it to discuss whether a CNN is the optimal architecture for inference from spectrograms (see e.g. https://towardsdatascience.com/whats-wrong-with-spectrograms-and-cnns-for-audio-processing-311377d7ccd, which applies to sound spectrograms but some of the criticisms are transferable). It is possible that an architecture more tailored to the data may alleviate some of the problems noted in the paper.
In general, the DPI/resolution of the figures are quite low, and detract somewhat from the paper while reading/zooming in. For the future, I would suggest to remake the figures in a higher resolution."
417	Reliability-aware Contrastive Self-ensembling for Semi-supervised Medical Image Classification	Maybe in the future, it is also interesting to see the results of the segmentation tasks.	"(1) There is a typo in ""Input of min-batch of images"" in Fig.1, please correct it.
(2) How many training iterations are operated between the update of the parameter of the weight function and the network parameters on two datasets?
(3) Which dataset is used to conduct the ablation study to investigate the role of each component in RAC-MT. Please specify it in the manuscript.
(4) Please discuss the limitation of the proposed method and the possible solutions/future directions in the manuscript."	"It could be better to replace the word reliability with words such as importance. The learned weight function is novel, but is not designed for data reliability.
Could you also include comparison with contrastive learning based method such as SimCLR on the skin dataset, following the protocol of contrastive pre-training and fine-tuning with labeled data?
From the ablation study in Fig. 2, without using the weight function, the simple combination of CST-MT (consistency loss and contrastive loss without weighting) already achieve a good accuracy, which means the improvement by the weighting function is not as effective as it appears. Could you explain more about this result?"
418	ReMix: A General and Efficient Framework for Multiple Instance Learning based Whole Slide Image Classification	"I am bit concerned the choice of baselines (ABMIL,DSMIL) is not very indicative of the generality of this method; though I do understand the limited space in the manuscript can be a factor. To expand on this, from a technical standpoint, DSMIL implicitly computes prototypes and thus LA could be directly applied with without resorting to K-means cluster learning as a prior step. Is there a reason only these baselines were employed?
It would be interesting to see whether ReMiX can work with recent methods such as TransMIL [1].  Especially given that ReMix-DSMIL and it's variants had marginal improvements on CAMELYON-16 over the baseline."	"The proposed model contains two parts, reduce and mix. Though it somewhat looks like interesting, novelty is still limited. Reduce part uses K-means on the patches' representations to obtain K clusters as prototypes to represent the bag. Such similar idea has been proposed in the previous WSI work, like WSISA. In addition, Mix part includes latent augmentation and the technique behinds such part does not sound novel.

WSISA: Making Survival Prediction from Whole Slide Histopathological Images, CVPR 2017

Though two baselines are compared, it is not clear how are they implemented. Does the author implement the baseline models following the original setting? Also, several new WSI-related MIL methods are not compared. 
CLAM: A Deep-Learning-based Pipeline for Data Efficient and Weakly Supervised Whole-Slide-level Analysis, Nature Biomedical Engineering, 2021.

It is not easy to use the proposed model in practice. From Table 1, we could see different augmentation strategies achieve different results. How to decide the one for use is not clear and easy.
Another concern is the lack of interpretability of the proposed model. Many MIL-based models show attention maps on patches/instances, it seems there is not applicable for the proposed model to have such visualizations."	"The training budgets comparison may be over-claimed. The success of reduce and mix depend on the high-quality pretraining, which would demand a lot of resources and time. However, the comparison in Table 2 excludes the time and memory demand of pretraining, which is not fair to SOTA baselines. Moreover, compared with the training budgets, the resource cost of inference is more significant for the practical applications.

The summary of existing WSI works that reduce the resource consumption may be improved. For example, [1] randomly samples specific number of patches as an augmentation of bags, and [2] utilizes the attentive regions with a sparse tree. Authors are suggested to discuss the differences or advantages over these works.
[1] https://dblp.org/rec/conf/cvpr/HashimotoFKTKKN20
[2] https://dblp.org/rec/conf/aaai/0013ZCHHY21

For some WSI tasks related to regional proportion (e.g., HER2 scoring), the reduce step may lose necessary spatial information of massive patches, which would restrict the performance. Authors are suggested to discuss potential limitations of the method.

In abstract, ""the descent performance of deep learning comes from harnessing massive datasets"" is a little confusing. Please check the description."
419	RemixFormer: A Transformer Model for Precision Skin Tumor Differential Diagnosis via Multi-modal Imaging and Non-imaging Data	"However, there are some places that are not clearly expressed.
In Section 2.2, ""When DWP is turned on (based on p > Tp, p  [0,1])"", what does p mean here? How is the p obtained?
What are the global features and local features?
In fig. 2, what does patch token mean? It's never shown in the main text.
Section 2.3 is very confusing. 
For example, the authors mentioned gc or gd are generated by GAP layer, gm is generated by LN layer. But in the formulas below, it's zx and gx' that are generated by LN and GAP. I don't know if the gx' here represents the same gc, gd, gm mentioned before, or it means something else. 
Also, I suggest to add the notations lc, ld, and lm(if there is one) to the figure 2."	"1) Although the chosen problem is exciting, technical novelty is still missing. Authors use state of the art Swin Transformer for a given task.
2) The proposed architecture seems to have multiple branches for each modality, however the computation complexity, number of parameters are not discussed. 
3) In table 1, the second best performing network would be Inception-comb. With respect to this the performance exceeds by 5.5% in terms of average accuracy and not 12 %.
4) Augmentation might not be clinically sound. 
5) Experiments sound a bit weak.
6) May be please add significance test to check the significance of the results."	To demonstrate the contributions in addressing missing modality, it is suggested to give details of percentage of missing data. Besides, experiments should include validations on different percentages of missing data and modality.
420	Removal of Confounders via Invariant Risk Minimization for Medical Diagnosis	"The manuscript is pretty good. I'd suggest the Authors to use more intuition and examples to explain their concepts and keep the formal description only after that. Especially for a future extension to a journal article, I invite the author to present the topic of removing confounders also from a more historical perspective - which is the one related to linear models - to guide the audience in this very interesting topic.
Minor: it is not clear why the proposed method is specifically called ""ReConfirm""."	"The paper presents a modified  invariant risk minimization (IRM) to remove the effect of the confounders. The proposed method were applied to NIH chest X-ray classification tasks where sex and age are confounders. The experimental results outperforms baseline CNN models trained under the traditional empirical risk minimization framework.
This work proposed a modified IRM framework to accommodate class conditional variants for NIH chest X-ray classification tasks , where the invariance learning penalty is conditioned on each class. This work designed a strategy for optimally splitting the dataset into different environments based on the maximum violation of the invariant learning principle.
However, the comparative studies seems very limited. I would like to suggest:

More datasets and backbones like transformer can be added to verify the generalization ability of ReConfirm.
More existing methods could be included for comparisons."	"A big limitation of the paper, in my opinion, is the lack of comparison to invariant-feature learning methods from the domain adaptation literature (e.g. [22]). This is especially the case as the experiments done in the paper are in cases where the confounding variables are known in advance.

A discussion on the limitations of the invariant risk minimization framework (e.g. Rosenfeld et al. The Risks of Invariant Risk Minimization, ICLR 2021) would have been useful. In particular, it is unclear why the proposed method should work if all confounding variables are not taken into consideration during the training."
421	RepsNet: Combining Vision with Language for Automated Medical Reports	"Few other questions need to be addressed:
There is no mention of questions and answers in the IU dataset?
Where is misalignment defined before generating a heat map?
How to address data bias as abnormal patients being way more than normal patients?"	The transfer of the developed methodology to clinical practice remains uncertain. What are the limitations of your approach? How would this approach integrate into a clinical routine? What are the difficulties? What is the uncertainty associated with the prediction?	"Please improve the manuscript clarity, i.e., the results discussions, motivation, etc.
Please check the consistency of notation system, i.e., the decoder part Equation 5 with conditional inputs {y, \had(X), \had(C) } and Fig.2  with inputs  {\had(X), \had(C), \had(Q), \had(Y) }"
422	Residual Wavelon Convolutional Networks for Characterization of Disease Response on MRI	Looking at the main weaknesses noted above, the greatest improvement that can be made to the paper is to expand the range of data and clinical tasks used in order to explore how RWCNs compare to ResNets under a wider range of conditions. I realize this would be difficult to accomplish during the rebuttal period, so it is not something I would insist on.	"(1) ""We also present the theoretical basis for the technical advances offered by wavelet activation functions being utilized within our unique RWCN formulation"". This is only the theoretical description of the corresponding method in the paper. It is the workload, not the contribution.
(2) The innovation point is only the proposed RWCN method, the innovation is single, and it is suggested to add innovation points.
(3) ""To our knowledge, no previous work has specifically examined the properties of residual skip connections within WNs in conjunction with CNNs."" The description of is not accurate. As early as ten years ago, there was an article on the combination of CNN activation function and wavelet. The article only focused on the architecture of wavelon integrated into convolutional neural network, abbreviated as WN. Pay attention to full literature research before expression.
(4) VGG was proposed in 2014 and RESNET was proposed in 2015. These two methods were proposed seven years ago. It is suggested to conduct a comparative experiment with the latest in-depth learning method proposed in the current three years. At least compare the effect without combining wavelet with that after combining wavelet, so as to highlight the advantages of your method.
(5) There is too little data in the experimental verification part. For the results after training and testing, multiple groups of experimental comparison should be carried out in the verification experiment. If the image display is required due to the length limitation of the article, the list can display as much data as possible."	Very nice study. Just include the source code please.
423	Rethinking Breast Lesion Segmentation in Ultrasound: A New Video Dataset and A Baseline Network	"The spatial resolution of the dataset is down-sampled to 300x200. It is concerned that such down-sampling affects the reconstruction accuracy.
Additional results about the relationship between the number of memory frames and accuracy would help understand the effectiveness of the proposed spatially, and temporally decoupled transformer module."	"The proposed temporal decoupled transformer split the memory, and query key into s^2 non-overlapping patches. The relationship between the accuracy and the s value would provide a deeper understanding of the temporal transformer and local similarity.
The dynamic memory selection scheme employs cosine similarity to calculate similarity among each frame. Diverse similarity metrics such as Euclidean distance could be an alternative option. It would be good if the reason for choosing cosine similarity is recommended.
 "	"My first major concern about the paper is the baseline model for comparison. The reviewer assume that the baseline is STM, however, it is hard to align the results of STM with the ablative results. I will suggest that the authors clarify the baseline and then incrementally demonstrate the contributions of the components proposed in the article. This is essential since the segmentation framework has been explored in other places.
My second concern is about the generalization of the method. Is the method also applicable to other medical video segmentation tasks? What makes it unique to US video segmentation?
Memory-based networks have been explored in medical segmentation like Quality-Aware Memory Network for Interactive Volumetric Image Segmentation. Thus it should be carefully discussed.
The ablation study is not sufficient. Some detailed investigation of memory hyperparameters (e.g., K in Eq.5) should be performed."
424	Rethinking Surgical Captioning: End-to-End Window-Based MLP Transformer Using Patches	The authors did not promise to release the code, which might be somehow disappointing to the community I suppose.	"Table1: why does ""Ours"" does not have FPS (Frames per second?)
especially in robotic surgery. --> why is it especially for robitc surgery a problem? Isnt it also a problem for e.g. minimally invasive surgery?
------ 
In Table 1. SwinMLP-TranCAP, Swin-TranCAP, V-SwinMLP-TranCAP i think its better to say SwinMLP-TranCAP-Encoder (likewise for the others)  for FE Column.  Add a X to Det. for same group 
------ 
""Self-sequence and AOA originally take the region features extracted from the object detector with feature extractor as input. In our work, we design the hybrid style for them by sending image features extracted by the feature extractor only"" --> doenst this modification make the AOA and Self-sequence method less capable?
------
How does the window size of 14, instead of 7 influence the results and what is the rational for this change to the  baseline?
------ 
Revisit sentence:
""The shifted window and multi-head MLP architecture design make our model less computation"" 
""Replacing the multi-head attention module with a multi-head MLP also reveals that the generic transformer architecture is the core design instead of the attention-based module.""
------ 
Avoid using these terms in academic writing:

extremly simple,
very compute heavy
https://www.scribbr.com/academic-writing/taboo-words/"	"-This is a great work, however I'd like to see more discussion, such as the difference between Swin's encoder and this work's is the multi-head MLP by group convolution. How that differs from the conventional convolutional network?
-The decoder is standard transformer structure, and can we replace it with the other sequence modeling methods, such as LSTM, GRU?
-For the video model, is there any fundamental improvement compared to the 2D-based method? Can we simply use 2D-based method to handle the video caption?
-The comparison should be complete and fair, the work has a larger window size compared to the twin transformer, I think it is better to evaluates win with the same setting in the table 1. Otherwise, I will be confused about if the window size affect the performance or the modification boost the performance.
-Inference speed, FPS problem as indicated above.
-In the ablation table 2, why not show the patch size of SWIN-MLP, but the vanilla transformer? Also, it seems that the patch size as 16 works better, why do we finally choose patch 4 in the table 3. The param column in table (b) does not show the difference between this work and swin transformer, is there any explanation?"
425	Rethinking Surgical Instrument Segmentation: A Background Image Can Be All You Need	It is a nice paper to read and follow.	"The authors should provide some comparison with other data augmentation approaches.
They have also to highlight what is the novelty, especially in comparison to AugMix."	"Given that this is a MICCAI paper with limited space, some of the comments are about removing content that is not strictly necessary.

A small section (~1/3 pg) is dedicated to blending which is really not needed given how common the technique is generally in film.
Eq 1 in particular doesn't help explain the method and Eq 3 is largely a copy of Eq 2, meaning that all three could be more readily expressed as Eq 3 with a sentence defining H_i and \Theta.

Other small comments:

The number of foreground stills is giving in Section 3.1 but not the number of background stills. This is stated in the introduction, but briefly restating it in Section 3.1 would be good to remind the reader of the quantity of annotated data used.
For Fig 3b, it would be nice to have the Dice for the vessel sealer vs the other instruments shown separately. As it stands, a ~1.6% improvement seems small, but one imagines that this may be because of the prevalence of the instrument in the Testing dataset.

And one big comment:
I stated that Table 1, despite how it looks superficially, is actually a strength, but there is, I feel, a way to make it even better. Firstly, it lacks an idea of a reference bounds. It would be improved by a row that shows an a priori reasonable lower bound on performance. (For example, this could training with the same few stills but without simulation, which would should just how much using simulation-based data augmentation as training a U-Net with only a dozen or so images is probably going to fail miserably without the nuanced data augmentation procedures suggested in the paper.) It would also be improved by a row indicating an a priori reasonable upper bound as well, such as using the same large database but with on-the-fly simulation. This latter part is important to the optics of the paper as it would show that the method presented does outperform the argument ""just get more data.""
Typos:

""minimal human efforts"" - ""minimal human effort"""
426	Retrieval of surgical phase transitions using reinforcement learning	Please see Weakness.	"Title matching content:
No, the manuscript describes a method to correctly predict the phase transitions and that itself is a ""refinement"" task over the initial set of predictions rather than a ""retrieval"" task.

Abstract summarizing content:
The abstract mentions the use of the reinforcement learning paradigm but did not fully justify the rationale behind their choice.
The abstract talks about two configurations with full and sparse number of video frames, however, the paper does not elaborate on the usefulness of the two settings beyond just computational efficiency
The abstract mentions the ""comparable computation cost"" of the new method but it is not specified in any form in the paper. It will be good to provide numbers for the computation cost as well.

Motivation:
The motivation for the work is short although clear and matches the goal provided in the abstract. However, it will be great to provide more details about the implications of ""erroneous phase transitions"". The readers are left looking for more details on the said problem.

Novelty in contribution:
The novelty lies in adapting Deep Q-Learning Network (DQN) and Gaussian components, but more details on the rationale behind DQN use would have been better.

Knowledge advancement:
The work provides a method of reducing erroneous phase segmentation by the use of multi-agent DQN and Gaussian smoothing. The work outperforms the re-implemented SOTA work as baselines for the Cholec80 dataset but not for the in-house dataset. 
The weak performance (~10% less from baselines) on the in-house dataset is justified by the processing of less number of frames (~20%) which is encouraging but it's defined on only one phase transition. More details on the dataset and for more phase transitions would have been better.

Positioning with existing literature:
The manuscript mentions recent works in the field of phase recognition but misses out on mentioning other papers such as MTRCNet-CL, Surgical phase recognition by learning phase transitions (Sahu et al.), etc.
Related references are covered for DQN, phase recognition, datasets, but missing for LSTM, ResNet.

Method description and rationales:
The method is aptly divided into three modules with sufficient details and provides enough purpose for all the modules.
The clip size in the Average ResNet feature extractor is set as 16. It will be nice to see the results for K=8 or K=32 to justify the K=16 choice. For example, the K=8 setting might make the predictions less noisy or more refined.
The DQN Transition Retrieval subsection does not provide the reasoning for choosing the RL-based network compared to standard CNN/RNN based methods or possibly share some results to ascertain why RL is necessary.
The second FC layer after DQN is 50-dim in size and is mapped to ""2 Q-values Right and Left"". This part is confusing - is the final output vector is of dimension 2 or 50 and how is it divided into vectors responsible for ""Right"" and ""Left"" action. 
An example of a sample input and output feature dimension through the DQN + LSTM/FC setting makes it easy to comprehend which is not provided.
Without giving the input dimension, it is important to mention the dimension of the downsample features.
In the DQN subsection, two characteristics provided are similar as the position of the agents defines the nearby clips that will be used for training. There is a scope for further clarity in the text.
RMI initialization is not clear from the text. Is there another ResNet trained with transition indices as the final output? More details should be provided on RMI as reported results are better in ""RMI"" setting than in ""FI"" setting. 
The motivation behind the third Gaussian composition module is clear.
The loss in Algorithm 1 is not explicitly specified and there is no mention of the type of loss used. 
The data structure used for the replay memories is not specified - Is it a tensor? What is the size of the replay memories? Is the sampling random or sequential? The author refers to the RL paper [12] but it should mention small details as mentioned above.

Standalone figures and tables:
The problem statement in Fig 1 is clear and summarizes the goal of the paper.
The architectural diagram in Fig 2 is clear and legible but missing small details like an arrow pointing from DQN to its expanded view.

Reproducibility of the experiments:
The basic hyperparameters are presented. Is there no weight decay used during training?
Are both ResNet-50 and DQN trained end to end? This is not clear from the experimental setup section. 
The maximum number of steps for agent exploration is mentioned as 200. Is it because the network converges by 200 steps? Is there a lower and upper bound on the number of steps where convergence starts or stops? 
The manuscript should mention the maximum number of episodes used for training which is missing from the text.

Data contribution/usage:
The method is implemented/evaluated on a publicly available dataset - Cholec80. The paper uses the recommended train/val/test splits in the original dataset paper.
A private in-house sacrocolpopexy dataset is used for evaluation however it's focused on only one phase transition compared to multiple phase transitions in Cholec80.

Results presentation:
The results in the tables are clear but the best results must be highlighted in bold.
The results are specified with mean and std which makes it easy to comprehend with others.
The manuscript stresses the improvement in the performance based on the metrics - Event and Ward Event ratio which is promising but did not discuss further the case for Sacrocolpopexy where TeCNO/Trans-SVNet in spite of having a much higher F1-Score than TRN does not have better Event/Ward Event ratio. 
The manuscript mentions TRN21/41 FI for Sacrocolpopexy in the results and discussions but the results are provided only for TRN21/81 FI. 
To maintain uniformity, the manuscript should have presented the results for the RMI setting for Sacrocolpopexy which is missing, and no rationale is provided. 
One of the SOTA, Trans-SVNet, is said to be reproduced in this paper for comparison, but the reported numbers on the Precision and Recall metrics for Cholec80 are ~8-9% less than the published performance. This raises the question of the quality of the baseline used and reported in the paper.

Discussion of results and method justification:
The results for Sacrocolpopexy are not ""slightly"" under the baselines, the word manuscript used is misleading as the difference between TeCNO/Trans-SVNet and TRN is ~ 10% under F1-Score/Precision/Recall metrics.
The improvement in the performance for Cholec80 is clearly mentioned and the reasoning provided. However, it is important to know which part of the TRN is largely responsible for smooth transitions. Is it because of DQN or the gaussian composition? For example, the caption for Table 1 is confusing - the results for individual phases do not perform Gaussian Composition but the Overall F1-score is after applying Gaussian Composition. What is the reason behind this?

Clinical relevance of the proposed method and obtained results
The clinical relevance of the work done is not discussed.

Conclusion:
The paper presents significant insights for research continuation
Reference is adequate but ResNet and LSTM are not cited.

Arguable claims:
The videos are center-cropped but might miss out on surgical activities or motion patterns happening around the video frame. Most works resize the video frame rather than center cropping. The performance might also be stunted due to center cropping. (page.5). The manuscript should provide reasoning behind this.

Manuscript writing and typographical corrections
We implemented the standard DQN training framework for our [netwrok]: [network] (pg: 3)
We perform a Gaussian composition of [of] the predicted phases: [] (pg: 4)"	"MAIN WEAKNESSES

Offline method only compared to online methods
The authors propose an offline approach but only compare their results with online methods. Since offline phase recognition is a considerably easier task than its online counterpart, it is not clear if the performance gain is because of the effectiveness of the proposed RL method or simply due to the easier task.
TeCNO and Trans-SVNet should be fairly straighforward to reimplement as offline methods (especially TeCNO).
SUGGESTION: I believe the authors should compare with offline methods (e.g. offline TeCNO, offline Trans-SVNet) to demonstrate whether the proposed RL formulation is competitive with the standard frame-classification formulation.

There are several open questions regarding the method design. These questions partially indicate limitations of the proposed task formulation:
The paper does not mention what happens if a phase does not occur in a video. If the model cannot handle this case and always predicts all phases, this would be a major limitation.
The authors state that their method guarantees contiguous phases. However, what happens if the start and end points of one phase are predicted to be within another phase (e.g. f_1b < f_2b <_f2e < f_1e). Due to the Gaussian composition, this would result in the 'outer' phase to be split into two segments. How is this case handled? Has this happened in any video?
Is the constraint ""f_nb < f_ne"" somehow enforced by the model? How would the model behave if this constraint was violated? Has this ever happened?
For the FI approach, the authors state that transitions are initialized at the ""average frame index"". For short surgeries and late phases, this average frame index likely often lies outside of the range of the video. How are these cases handled? Or is it rather a relative frame index is measured (i.e. the average progress of the surgery in percent)?
IDEA: Making the agents predict ""f_nb > f_be"" might be a way of handling missing phases and might kill two birds with one stone. Not sure if this is a good idea.

The way windows are traversed by the agent might make the task unnecessarily difficult.
How are the windows traversed by the LSTM agents? If they are traversed sequentially, then the most relevant frames are likely somewhere in the middle of that sequence. The LSTM might forget relevant information or it might be difficult to remember their exact location.
E.g. if the agent is currently at the correct location, the LSTM would have to remember that the transition happened at exactly the middle of the sequence. This seems like an unnecessarily difficult task. Adding a positional encoding or traversing the sequence from outside to inside might be alternative strategies.
Why was this sequential strategy (supposedly) chosen? Did the authors test or consider other traversing/encoding strategies?

Why was DQN chosen?
The standard DQN algorithm is quite old and many improved or different RL approaches already exist? Why did the authors not opt for more modern RL methods like PPO[1], SAC[2], A3C[3] or HER[4]?
SUGGESTION: The authors should explain why they chose DQN or consider a more modern RL method.

The authors modify the baseline methods but it is not clear if this modification improved or hurt performance.
Modifying the baselines to be more comparable to the proposed approach is definitely a valid approach.
Nevertheless, the original approach should still be reported to understand how this modification affected performance.

[1] https://arxiv.org/abs/1707.06347
[2] https://arxiv.org/abs/1801.01290
[3] https://arxiv.org/abs/1602.01783
[4] https://arxiv.org/abs/1707.01495
REQUIRED CLARIFICATIONS

The open questions from the reproducibility section and 'main weakness 2' could be clarified.

MINOR COMMENTS

Apparently a mistake happened in the supplementary material. All plots show the results of the same video.
Why was the RMI approach not evaluated on the sacrocolpopexy dataset?
It is quite cumbersome to understand how big the receptive field of the model is in terms of seconds. If I understand correctly if would be 2L16 / 2.4 seconds (with a window size of 2*L feature vectors; the averaged ResNet producing 1 feature vector from 16 frames and an initial framerat of 2.4 fps). Maybe this could be made clearer in the paper? Or if I am incorrect, the correct receptive field could be given.
Typo: ""netwrok"" in Section 2.1"
427	Revealing Continuous Brain Dynamical Organization with Multimodal Graph Transformer	Please see comments on the main weakness of the paper.	"some details should be added to make the paper easier to understand.
the preprocessing of SC is not mentioned.
if the findings in the paper could be validated in another dataset, the method may be more reliable."	"a) The paper writting should be greatly improved.
b) Ablation studies should be included to evalute each part of the proposed method."
428	Rib Suppression in Digital Chest Tomosynthesis	Please see above	"This is an interesting study. However, more analysis is required, especially on the network's robustness to varying sizes of patient anatomy and disease types.

The paper mentions M2D and M3D are trained separately before F; But it would be interesting to see how the joint training of the three networks works.

It looks like the proposed TRIPLE-Net is almost on the same level as M3D (PSNR), both Doctors A and B rated M3D predicted images higher. The authors should add an explanation. Perhaps, a domain adaptation approach would be feasible with a larger clinical dataset.

To make it more reliable, the simulated DCTs should be quality checked.

The paper doesn't report the per scan execution time of the proposed model."	"The CT data is orthographic on sagittal and coronal orientations but entirely perspective on axial planes. However, once the X-ray beam source and detector rotate a complete circle around the object, the integration compensates both directions and turns the collected voxels equidistant on axial planes. Please clarify whether the projection and simulated datasets were made on an orthographic or perspective scene?
Experiments on simulated 3D CT datasets need to explicitly clarify whether the full HU scale is used and normalized to (0, 1), or a specific window is applied to the data. This matter will affect the evaluation metrics L1, L2, and the intensity range of 2D projections.
On the other hand, L1, L2 norms, and PSNRs metrics are somehow related. Perhaps, other interesting metrics would be performing the maximum-intensity projection (MIP) on 3D rib segmented data compared with the 2D rib segmentation artifacts in Dice Score.
Assuming that the proposed method can separate the rib artifacts in the image domain, the bone shadows still presented, given the results in Figures 3 and 4. It poses another question: Is the artifact well-observed if one performs gradient transform on the rib-suppression images?"
429	Robust Segmentation of Brain MRI in the Wild with Hierarchical CNNs and no Retraining	"It would be beneficial to include conditions under which the proposed framework doesn't perform well. This will help the reproducibility of the model and manage data accordingly.
Why Gaussian mixture model has been employed in step(b) of SynthSeg+. 
Can we use General mixture model instead considering the MR noise distribution (Rician) and possibly other unknowns."	"The authors only provide comparison in terms of Dice parameter. Other metrics should also be provided. 
The authors could provide a fair comparison with other state-of-the-art techniques (in recent challenges) using public databases so the benefits of SynthSeg+ could be better displayed.
A deeper discussion on related Works should be provided, as the authors focus mainly in SynthSeg as a precedent."	No
430	RPLHR-CT Dataset and Transformer Baseline for Volumetric Super-Resolution from CT Scans	"Further Questions:

It is not clear to me why a modeling of Long-range (!) dependencies is an issue for volumetric SR. Can you elaborate on this argument for the transformer network a bit more?
For the dataset analysis you are saying that ""We use PSNR and SSIM to access the changes in the similarity of three slice-pairs"". However, the PSNR should be computed between a noise free image and the noisy representation. Not as a measure for image quality between CT sets. Can you explain the reasoning behind this measurement?
Can you elaborate on the difference for the match slice between the thick and thin CT a bit more? How have you handled motion in the dataset etc. ?
where is the external test set coming from? is this also public? are the parameters the same? are the patients the same?
benchmark: I was a bit confused, that two of the used benchmark algorithms are changed (cite: ""For ResVox, the noise reduction part is removed. For
MPU-Net, we do not use the multi-stream architecture due to the lack of available
lung masks.""). By removing parts of the algorithm, the network is changed and it is not longer the originally proposed algorithms.  Can you comment on this?

Structure of the paper:

there are a lot of graphics in the paper (and supplemental material), but most of them are not mentioned/explained in the paper, and the caption is to short. Therefore, the reader has to figure out the findings by himself. (examples are: Fig. 1. ""Summary of our RPLHR-CT dataset"" is not exactly what i see there. Fig. 2 what are the colorful boxes in B)TAB?. )
the abbreviation on the heading (RPLHR) is never introduced!"	"1)	More image quality metrics comparisons
2)	Some bar plots from Figure 3 will be very valuable in the table form in supp. material"	None
431	RT-DNAS: Real-time Constrained Differentiable Neural Architecture Search for 3D Cardiac Cine MRI Segmentation	"*	More thorough comparison experiments should be done. For example, other latest CNN and NAS based methods should be compared as mentioned in the main weaknesses of this paper
*	Although this paper is not difficult to read, the writing quality could be further improved by careful proofreading. There is one typo in the formulation (2)."	"Slightly larger text in Fig. 1 would be more easily readable.
Minor language issues:

""In the past a few years,"" -> ""In the past few years,""
""each paths is formed by"" -> ""each path is formed by""
""the performance each path can be"" -> ""the performance of each path can be""
on page 4: ""L_{up}"" -> ""L_{ub}""
""After the all the parameters are fixed"" -> ""After all the parameters are fixed""
""metohd"" -> ""method"""	"In terms of the computation time, how long did it take to run these hyper-parameter searches in the RT-DNAS method?
Once the architecture has been settled on, the authors find which optimal path had the highest latency to accuracy trade-off? Can they describe/quantify if it was more Conv or skip connections, downsampling/upsampling? It would be exciting for the readers to see what optimized this final set of layers and overall network architecture was."
432	RTN: Reinforced Transformer Network for Coronary CT Angiography Vessel-level Image Quality Assessment	"The first step in the processing pipeline, i.e., centerline tracing, may suffer from image artifacts as well. I don't see how this is addressed by the suggested method.
English spelling and in particular grammar should be carefully double-checked once again. (""We follows ..."", ...)
It's not entirely clear to me, which problem is solved at all. Are all centerlines processed at once and then a final quality score for the whole image is computed?
The definitions and equations on page 4 are hard to follow: how do I have to imagine an instance embedding Z_L[1:n]? Is this a family of values?
 . As an improvement one could think of using the following convention for mathematical notation: scalars as regular small letters, vectors as small letters in bold, matrices as capital letters in bold, sets as capital letters with \mathcal, functions also with domain and value range ...

PRID: from the explanations I can't relate the reinforcement learning strategy to a Markov decision process as stated I the paper.

Minor things
 . The number of discarded instances seems to be a free parameter of the system. How shall it be chosen in practice? Isn't it data dependent?
 . The term ""follow-up"" on page 1 might not be used in the right way as ""follow-up"" most often refers to examinations carried out days or months later.
 . ""the coronary artery""? I'd rather speak of ""the coronary arteries"".
 . I'd recommend introducing the abbreviations again in the main text not only in the abstract.
 . ""SOTA performance"" for ""state-of-the-art performance"": I never heard this abbreviation.
 . Sentences shouldn't be started with abbreviations.
 . Abbreviations are use inconsistently: e.g., ""Fig."" vs. ""Figure"".
 . Word repetition ""MIL aggregators in MIL methods""
 . The abstract speaks of ""above two modules"" while there's been only one introduced before. I don't understand this."	"The proposed method classify the quality of the vessel by first removing the instances that not relevant for determining the quality of the vessels, then using the remaining instances to decide the quality of the vessel.

The authors did some augmentation on the training dataset, could you please provide the volume of the dataset after augmentation?
Different coronary arteries have different size and different length, the proposed method used the same number of cubes (n=19). Could you please add the details on how do you deal with the different lengths?
Table l compares the results of the proposed method with state-of-art methods which also includes the results with/without PRID. Table 2 compares the results with different discarding numbers. It seems T-MIL without PRID has higher accuracy than the PRID with less amount of discarding number of instances. Could you please help me understand this?
The authors provided supplementary materials for the propose method but lack of reference or description of the contents in the supplementary materials. Please consider add one or more reference sentences to the Figures, especially the Figure1. And Based on Figure1, it will be difficult to claim that the remaining examples are mainly concentrated in the front of coronary artery since index 2,4,7 has low frequency and, only 1,3,5,12,17 has a frequency more than 0.5. So, I would like to suggest the user to rephrase the claim/conclusion on Figure1. Instead of saying that the main concentration was on the front of the coronary, the conclusion could be that the assumption that only limited instances play important role in the VIQA is correct."	please address the concerns and questions above.
433	S3R: Self-supervised Spectral Regression for Hyperspectral Histopathology Image Classification	BR and CR are two reasonable pretext tasks for HSI image. However, the relationship between image masking and these two tasks is not direct, so the introduction of MIM model is abrupt. More theoretical analysis and experiments on the necessity of MIM for BR and CR are welcome.	"Could the authors provide qualitative assessment of the proposed method? For example, how does S3R method help in exploiting the morphological characteristics in a sample tissue image. Highlighting the spectral bands and showing the result of this approach can help better understand the learning strategy.

Next, the authors claim that S3R forces the network to understand the inherent structures of HSIs. Could the authors expand on this idea?

Can the authors provide an insight into the failure modes and if it could be expanded to datasets from other modalities?"	"In the implementation details, some parameters, such as batch sizes of all methods, are missing, and the details of the fine-tuning stage are missing (freezing parts of parameters, the structures of the classifier, etc.).

In Fig. 3, it would be nice to see the ""ground-truth"" coefficient Visualizations and then compare the similarities between the two proposed methods.

It would be interesting to see the quantitative results and illustration about ""S3R converges at least 3 times faster"" from the abstract."
434	S5CL: Unifying Fully-Supervised, Self-Supervised, and Semi-Supervised Learning Through Hierarchical Contrastive Learning	"First, address my major concerns above. 
Second, please proofread the paper and correct typos and minor issues."	"(1) After a fixed number t of epochs e, the classifier can be applied to Z1U , yielding pseudo-labels. It may not be a good way , it is suggested to use the predicted confidence level to determine whether pseudo tags are acceptable or not, referring to FixMatch.
(2) Generally speaking, in the combined loss function, adjusting the weight of the hyperparameters is very important, has a significant impact on the model. It is suggested that the value of parameters should be more fully explained or ablation experiments should be provided.
(3)	Figure 4d shows the ablation study of pseudo-labels. It can be seen that the effects of  pseudo-labels are not always positive. Especially, pseudo-labels in Lc and LL are even worse than no pseudo-labels. So, a more detailed analysis is recommended.
(4)	In the experiment for Munich AML Morphology, there are only comparison result of fully-supervised and self-supervised methods, but no self-supervised algorithm. It is suggested to provide the results of a self-supervised algorithm for comparison on this dataset."	"How is the proposed different from the related work (e.g. [25])?
Why different there were different batch sized for the labeled vs the unlabeled paths? How the batch size was tuned?
Not clear how the error bars on Fig 2 were calculated. Details are missing from the text.
More information on the classification task could help the reader (e.g. 9-way and 11-way classification). There are not so much details in the text.
a),b),c) are missing from Fig. 2.
Some references are missing (HED augmentation, Macenko's method)"
435	Sample hardness based gradient loss for long-tailed cervical cell detection	See 5.	"(1) It would be great if the author can provide systematic way to determine the hyper-parameters used in their method.
(2)  It would be preferable if the author can publish their dataset and the code upon the acceptance of the paper, so the work can be fully reproducible."	"1.Fig2 provide a visualization example about how the grad-libra loss works. But the weights after F are illustrated unclear by colorful circle. Expressing the weights after  F as concrete math number may be better.
2.It is better to provide visualization results of cell detection which can make your paper more impressive."
436	SAPJNet: Sequence-Adaptive Prototype-Joint Network for Small Sample Multi-Sequence MRI Diagnosis	"this paper proposed to use Transformer and additive-angular-margin loss for small sample multi-sequence MR image classification. However, this paper has several major flaws and fails to illustrate their point:

The first component, Transformer model, is claimed to filter intra-sequence features and aggregate inter-sequence features, based on attention mechanism. But I didn't see any details about how to achieve these 2 goals in section 2.1. The overall writing quality is poor and difficult to follow.
Similarly, neither the section 2.2, prototype optimization strategy, illustrates how to approximate the intra-class prototype and alienate the inter-class prototype. For example, the query sample q^a and support sample s^b corresponding to which modalities, the loss_2 should be explicitly expressed like loss_1, and why you choose the additive-angular-margin loss instead of the ordinary cross-entropy loss for classification.
Fig 2 is very ambiguous and lacks sufficient explanation. E.g., what are the vectors besides the local significance block, how to aggregate into global correlation (through concatenation, pooling, or others), where does the support prototype come from, where are the 2 stages in Prototype optimization strategy, and how to approximate intra-class prototypes and alienate inter-class prototypes.
The experiment section lacks the explanation of 3 modalities of data (SAX, LAX, and LGE).
Some minor issues:
a.	P2, 1st paragraph, the hyperparameter p's explanation is ambiguous.
b.	Section 2.2 mentions robust classification. So what kind of attack methods you are dealing with?
c.	Section 2.2 2nd paragraph says ""two outputs of the SAT ..."" what does the two outputs referring to?
d.	Still in Section 2.2 2nd paragraph, ""The former is reserved for ..."". After this, where is the latter one? What's the purposes of two stages here?
e.	Table 3 didn't explain what's VOT."	"This is an interesting study addressing important questions associated with deep learning using real-world medical imaging data, such as MRI. The proposed method appears to be able to overcome 2 critical limitations of clinical MRI: small sample size, and multiple sequence acquisitions. However, there are a few questions about the paper that deserves further attention as seen below.
1)	The method is largely unclear. For example, in section 2.1, how 'the SAT accurately extracts features and generates prototypes that mimic a doctor's overall assessment of ...'? How are 'these representations aggregated and translated into new semantics'? Further, how are the parameters 'p' and 'delta' set and what do they mean?
2)	In section 2.2, how are the positive and negative sample pairs calculated; what does the 'supervisory information' refer to; and what is the relationship between 'pre-prototype' and 'prototype'? In addition, while there is an equation (#2), adding additional explanation for the 'additive-angular-margin loss' would help.
3)	In Fig. 2, how was the 'filter intra-sequence features' step done? What's the usage of 'padding' here - expand the cropped images to 90x90? How to 'approximate intra-class prototypes' and 'alienate inter-class prototypes?
4)	In Experiments and Results (section 3), the output of the proposed and comparison networks are not defined; multiple abbreviations, 'PAH', 'IIM', 'LGE' et al, are not defined at first use.
5)	Also in the above section, it says that 'different sequences of the same patient, although ..., were not spatially aligned'. Is this beneficial or harmful, and why? Related to this point, in image preprocessing, the MR images do not seem to be normalized, in any dataset. How would that impact network performance, and why is that preferred?
6)	In Fig. 3, it is unclear what 'abnormalities' are supposed to be seen despite the use of arrows in the panels. Including an example of normal image would also help.
7)	In section 3.3 (page 8), it is unclear what this part means: 'As shown in Fig. 4, compared with the baseline method, it can be seen that with the reduction of training samples, the performance of the SAPJNet is better, and its performance loss is smaller in the five training'. In Fig. 4 (bottom plots), it appears that the sample size is increasing from 20 to 40 instead of 'decreasing' as the network performance increases. Please verify."	Please address the issues listed in weakness.
437	SATr: Slice Attention with Transformer for Universal Lesion Detection	Please refer to my weakness part.	"The writing needs improvement and there are a few typo/grammar errors, e.g. Page 4 ""overfittin"", Page 5 ""Kernal"", Sec 3.2. full training results should be in Table 1;
Please provide more justification on the design of SATr block as discussed in the above weakness section; and/or provide ablation result on, e.g. using key-slice as query and all-slice as key and value;
Provide analysis on model size and flops to see the computational overhead introduced by the SATr block;"	Overall, I think this is a very good paper. If the authors could offer some useful feedback, that would be lovely.
438	Scale-Equivariant Unrolled Neural Networks for Data-Efficient Accelerated MRI Reconstruction	"1) The specific reasons for the increase in performance by combining the scale-equivariant CNNs and an unrolled architecture is unclear. More explicit explanations and rigorous experiments would be needed. In particular, it seems that the proposed model was built by simply changing the existing CNNs with pre-existed scale-equivariant CNNs.
2) The experiemtns were conducted only with vanilla, scale-eq, and rotation-eq models and lacked comparison with other deep learning-based MR image reconstruction methods. To show the effectiveness of the scale-equivariant CNNs, more rigorous experiments would be needed by applying them to other deep learning-based MR reconstruction methods and compared with other methods.
3) Although the quantitative results of the proposed network showed better performance than the baseline in Table 1 and 2, it is difficult to see the performance increment in the presented Fig. 1. Especially, the difference between Vanilla+ and Scale-Equivariant+ seems to be very minor. Please provide figures that can show the effectiveness of the proposed model."	"(1)	Please fully investigate the relevant literatures.
(2)	Give more qualitative comparison results."	"The topic of designing networks that work better for smaller amount of data is important, especially when considering limited possibilities to obtain ground-truth references in medical imaging.
A curious result is that the authors achieve better performance with the geometrically constrained network than the vanilla version. This is curious, usually slightly worse performance is observed under additional constraints of the network (here it is by including a scale invariance) I would have welcomed a short note on this in section 5.
As mentioned above, one of the aims of the authors is to provide data efficient learning. Sadly a study on the influence of data size is missing. The argument is only underlined by eliminating the need for data augmentation, but that is done from the same data set anyways, so it does not reduce the amount of data."
439	Screening of Dementia on OCTA Images via Multi-projection Consistency and Complementarity	"I recommend to replace section 3.2 by ablation study, authors can put extented experiment to supplyment materials
Authors should provide some examples of different projections of OCTA.
Better to provide FLOPS/model size/memory-usage information
It would be more persuasive if the dataset can be publicly avaliable"	The authors should provide experiments on hyperparameters to demonstrate the reliability of the selected parameters.	"Some minor typos should be noted, e.g., epochs is 200-> The number of epochs is 200.
The experimental content is not rich enough, and the analysis should be further strengthened."
440	Scribble2D5: Weakly-Supervised Volumetric Image Segmentation via Scribble Annotations	"Please check spelling and grammar
page 3 - ""In this way, we have a Label Propagation Module (LPM)
to generate 3D pseudo labels from scribbles and images for ROI segmentation
and static boundary prediction, respectively"" - This sentence is confusing.
page 4 - ""Specifically, At the"" - At is capitalized
page 8 - ""... methods and reduce the performance gap..."" - reduce should be ""reduces""
Text in figure 1 is too small to read. The caption should also be more descriptive, describing the overall architecture.
In equation 3, Volume_out should be (1-u) instead of u
Equation 4 has duplicate L_seg term. One of these terms should specify that the mask is refined"	The paper was technically sound and easy to follow overall. The authors exploited low-level boundary evidence, mid-level super-voxel evidence to achieve high-level segmentation.	"There are some concerns of this paper:

It misses some related references:

Zhao, T., & Yin, Z. (2020). Weakly supervised cell segmentation by point annotation. IEEE Transactions on Medical Imaging, 40(10), 2736-2747.

Luo, X., Hu, M., Liao, W., Zhai, S., Song, T., Wang, G., & Zhang, S. (2022). Scribble-Supervised Medical Image Segmentation via Dual-Branch Network and Dynamically Mixed Pseudo Labels Supervision. arXiv preprint arXiv:2203.02106.

Zhang, K., & Zhuang, X. (2022). CycleMix: A Holistic Strategy for Medical Image Segmentation from Scribble Supervision. arXiv preprint arXiv:2203.01475.

It is unclear what is the major advantage of the proposed method compared with the above related work.

In the comparison, they only include two scribble-based segmentation methods. More state-of-the-art weakly-supervised methods need to compare, such as:

[UNetD] Valvano, G., Leo, A., & Tsaftaris, S. A. (2021). Learning to segment from scribbles using multi-scale adversarial attention gates. IEEE Transactions on Medical Imaging, 40(8), 1990-2001.

Zhang, K., & Zhuang, X. (2022). CycleMix: A Holistic Strategy for Medical Image Segmentation from Scribble Supervision. arXiv preprint arXiv:2203.01475.

As for the mask-based segmentation comparison, they only include two U-Net methods. More recent methods should be included, such as:
UNetD [Valvano et al., 2021]
PostDAE [Larrazabal et al., 2020]

ACCL: Adversarial Constrained-CNN Loss ... [Zhang et al., 2020]

For the VS and CHAOS datasets, the scribble generation is not real from doctors or clinicians. So the evaluation results on these two datasets are not very convincing.

More visualization results and qualitative evaluation should be provided."
441	Scribble-Supervised Medical Image Segmentation via Dual-Branch Network and Dynamically Mixed Pseudo Labels Supervision	"Page 6: Table 1 is barely legible. Please improve the layout.
There are a few parts of the paper that I find unclear. Page 6-7: ""we trained networks with partially supervised and semi- supervised fashions, respectively. We used a 10% training set (8 patients) as labeled data and the remaining as unlabelled data, as the scribble annotation also takes similar annotation costs [29]"". Page 8: ""3) the proposed approach dynamically mixes two outputs to generate hard pseudo labels for two decoders training separately"". Please re-phrase/clarify these parts.
Please consider adding supplementary materials with more qualitative results (as Fig.3)."	The proposed dual-branch network is similar to the mean-teacher architecture, I prefer more discussions about the differences and strengths of the proposed algorithm compared to the mean-teacher architecture. Besides, the authors should add more experiments on other datasets with scribble annotation(such as PASCAL-Scribble Dataset) to improve the persuasiveness.	
442	SD-LayerNet: Semi-supervised retinal layer segmentation in OCT using disentangled representation with anatomical priors	"1). Manually tuning parameters
Although the reconstruction loss proposed in Section 2.2 is very promising, it would be challenging to determine all lambda values in reconstruction loss for self-supervising. If all lambda values for reconstruction loss are manually designed, the promising segmentation results of proposed SD-LayerNet should be arbitrary.
2). Further validations
In Table.1, the authors provided a methodological validation of proposed method with other two peer methods. These results demonstrate that the performance of proposed methods is better than other peer methods, given the reported segmentation errors and standard deviation.
In addition, it is more interesting to show the validation of time-consuming of proposed SD-LayerNet with other peer methods."	As shown in the weaknesses.	"Overall, the paper provides some insightful contributions for semi-supervised retinal layer segmentations. The proposed method provides a possible way for reducing the annotation cost in layer segmentation tasks. The design of the self-supervised tasks in this paper could also inspire future researches in how to incorporate anatomy priors. The main drawbacks lie in that the intuitions behind some design choices are unclear, e.g., the textual factors, and the final algorithm involves too many hypermeters, which may be tricky for tuning.
Some mistakes:
The notation of the textual factor generation branch seems to be wrong in Figure 1, i.e., conv-t or conv-m?"
443	SeATrans: Learning Segmentation-Assisted diagnosis model via Transformer	See above.	"The author is suggested adding reference citation for those state-of-the-art methods compared in Table 2 and supplying some analysis of the efficiency difference between proposed methods and recent transformer models.
Meanwhile, If the authors can provide some visual comparisons of the results from proposed method and recent models, the quality of this paper can be further improved"	"I believe the authors should provide references for those ""commonly used segmentation-assisted diagnosis techniques"" in the second paragraph of ""Experimental Settings"".

I believe the authors should add some info about the distribution of data. The sensitivities reported are the lowest among all the metric and I wonder if it is related to the imbalancy in the data.

I believe the authors should perform some statistical analysis to establish whether the improvement reported by their method is in fact statistically significant."
444	Segmentation of Whole-brain Tractography: A Deep Learning Algorithm Based on 3D Raw Curve Points	Please find my questions and concerns in the weaknesses section.	"According to the above major questions:

Perhaps the authors can provide more details of the data sampling. especially the non-major bundle and give examples of the long vs short fibers, the single vs branched fibers.
It would be great to compared with other methods using the same dataset.
Ablation studies can be performed for example by removing the attention layers, or using interpolation other than randomly selected points."	Q5
445	Self-Ensembling Vision Transformer (SEViT) for Robust Medical Image Classification	My main concern is the the comparison between ViT and SEViT is not fair. If the authors can supply a fair comparison, that would be helpful.	"Please see the weaknesses. Other detailed comments are as follows:

In Section 1, the authors state [11], [20], [22], etc, that enhances the robustness of ViTs. However, none of these approaches are compared in the experiments.
It seems the idea is from [22]. More explanations about the improvements would be helpful.
The contribution 3 can be removed since it belongs to experimental results.
Many parameters do not be well defined, such as threshold for KL-matrix.
Some visualization results are suggested to be added to support the results.
The reasons for the noticeable performance drop in MLP number = 12 for X-ray in Fig. 3 (a) should be explained.
It is recommended to compare the proposed model with other state-of-the-art methods."	"The paper demonstrates a good aspect of the evaluation of Transformer model. To further improve the impact on the MICCAI community, the authors can discuss the motivation, challenges, and potential application of how the adversarial attacks are influencing medical image analysis.
Detailed constructive comments are listed along with the above."
446	Self-learning and One-shot Learning based Single-slice Annotation for 3D Medical Image Segmentation	"It would be good to clearly define terms such as one-shot, as this is not very clear and the use of the term seems slightly different from other works.

I would suggest to replace 'inference' with 'test-time' throughout the paper, as inference is something slightly different than what the authors meant here. The use of the term 'inference sets' is misleading, and should be changed to 'testing sets' or 'evaluation sets'.

The introduction may need a reference or sentence to explain what 'human-machine disharmony' means.

Unclear what the authors mean by 'enormous semantics' in Method section.

From the definition, it is unclear whether the authors assume all volumes to have the same number of slices D.

I would suggest to rename 'Featuring Module' to 'Feature Extraction Module', and 'Reconstructing Module' to 'Reconstruction Module'."	"Alternative methods in Table 1 - do the results for experiments (3) to (5) come from the papers [40, 16, 37], or did the authors reimplement or run those methods themselves? In particular, I'm a bit wary of the results for the pseudo annotation results for the CT liver dataset, as this doesn't seem to be an extremely hard problem and the reported results (Dice 0.63) seems very low. I would have expected this method to do better.

3D vs 2D U-Net in Table 1 - Both of these use a single annotated slice, and it's unexpected to me that the 3D U-Net does so much worse, given that it could provide additional contextual information. Is there an explanation, e.g., does it overfit more than the 2D U-Net?

CHAOS vs LiTS liver CT scans - I would think that they would be quite similar given that they are both CTs, but the experiments in Table 2 in which the training and testing datasets are different indicate that it is difficult to transfer from one to the other. Is the difference in field of view, contrast enhancement, etc?

Additional small comments:

Abstract - ""our new framework achieves better performance with less than 1% annotated data"" - to me this phrasing implies that the proposed framework with less than 1% annotated data outperforms fully annotated 3D U-Net training, which is not what the authors are actually trying to say. Consider rephrasing to something like, ""when less than 1% annotated data is available, our new framework achieves better performance than several baselines"".

Related work - This method is quite similar to conventional patch-based segmentation along with a learned distance function - the authors could consider adding this to their related work.

Introduction - I didn't understand the motivation to avoid human-machine iterations, as I'd think this would be fine as long as any computations were fast enough.

Consider editing the self-learning loss in equation L_sche to incorporate the representative slice pairs from the screening module training stage (eqns. 2-4). As written, the loss reads as though the self learning operates on all pairs of neighboring slices and that the representative slice-pairs are unused."	given the limited amount of space, the paper is self-containing and clear enough. Some more evidence, as written above, would have been nice.
447	SelfMix: A Self-adaptive Data Augmentation Method for Lesion Segmentation	"1) Better writing is needed, especially for the equations.
2) More details are needed for the ""fusion"" step. 
3) It should be explained more clearly why it is distortion free."	"Clarification on the selection of non-tumor regions. 
For liver lesion and kidney, I guess the non-tumor regions should be the liver or kidney tissues. Otherwise, if the synthetic images use completely background pathes, they would not realistic. However, I didn't see a clarification on the selection of non-tumor regions.

Demonstration of sample synthetic images. 
Unfortunately, there is no more synthetic images to give the reader more ideas how good the proposed SelfMix is compared to cutMix and CarveMix. Good and bad samples are all helpful.

Results 
It would improve the quality of this work if the SelfMix is done in 3D and training the segmentation models in 3D. 
In Table 3, the number of using 100% data for UAD drops compared to the one achieved by 75%, why? 
How cutMix is implemented in Table 2? Are the tumor allocated on random positions?

Presentaions. 
Fig. 1 is nice but it can be improved. I could not see the difference between CutMix and CarveMix. Image content is a bit different. Please crop the image carefully.

Too many typos. 
x. in Sec. 2.1, Fig 2 -> Fig. 2; multi-steps -> multiple steps ; date -> data"	see the  main weaknesses part
448	Self-Rating Curriculum Learning for Localization and Segmentation of Tuberculosis on Chest Radiograph	"This paper presented an interesting research topic and proposed an effective ranking function for scoring images. The experimental design and evaluation of the data are satisfactory and the conclusions are justified. The manuscript is written in clear and concise English. However, the paper can be further improved by (1) referencing the latest state-of-the-art curriculum learning papers published in the last three years and compare your proposed method with the state-of-the-art methods; (2) using multiple datasets to validate your proposed method; (3) using more evaluation metrics such as sensitivity and specificity; and (4) rectifying some typos such as ""6000 case"" should be ""6,000 cases"" (Page 3), ""9600 samples"" should be ""9,600 samples"" (Page 5), ""achieve 4943"" should be ""achieve ""4,943"" (Page 7)."	"1) In the paper, AP50/mAP50 was mentioned several times but I couldn't find their definitions. Please add one or two sentences of definition in the text when it was first mentioned.
2) Page 4 design of the ranking function: this was probably the most important part of the paper. However, there was no justification on why it was designed this way. Please at least provide high-level intuitive justification as to why this would be the optimal design.
3) In Table 2, the results using SRCL was only slightly better than straight-forward training in the AP metrics but not in AUCs, which made me wonder how much the proposed SRCL really helped improving the results. One additional experiment that could be helpful would be to compare the SRCL in the paper to a different self-ranking algorithm (for example a very naive algorithm that only looked at classifier output probability and its difference to ground truth labels). If it could be shown that the proposed self-ranking algorithm was superior to a naive self-ranking algorithm, I think the results would be a little stronger."	"The authors mentioned the TB studies in the literature, and one is the lack of radiologists' annotation. The mentioned study - Chexpert - is indeed one of the largest CXR datasets, and annotations are automatically extracted from radiology reports using an NLP approach - which contains mistakes. Then, a group of radiologists at RSNA went over some portion of the dataset for manual checking and used this subset as ground truth. But, as far as I remember, this large CXR dataset does not contain TB patient data.
I would suggest authors go over the manuscript to provide a better-organized manuscript.
I think the method is not compared to the ""without SRLC approach"". What would be the results of the Mask-RCNN+Resnet50 backbone architecture trained with the same training CXR dataset without the SRLC approach and tested on the same test set? Then, we would have a better understanding of how much SRLC has contributed to the learning process."
449	Self-supervised 3D anatomy segmentation using self-distilled masked image transformer (SMIT)	"I'm willing to increase the rating if the authors could clarify the following two aspects.

What is the reason that [CLS] token in the image reconstruction task carries global information? It makes sense for image classification task (as stated in [24]), but it is not appropriate to directly borrow this assumption to image reconstruction task (pixel-wise task) without justification. What is role of [CLR] token in an image reconstruction task? Why does global image embedding matter for an image reconstruction task?
It remains unclear the conclusion of Fig. 5. 1-layer seems to produce better reconstructed images than multi-layer decoder, but does a lower MSE loss mean better representation? Results for fine-tuning 1-layer vs. multi-layer decoders should be presented along with the reconstruction quality.

Here are some suggestions for improving the paper:

The authors assembled several CT datasets for pre-training by image reconstruction. One possible issue is the image difference across these CT datasets, such as contrast enhancement, because restoring pixel intensity can be deeply influenced by imaging protocols. This domain gap might make image reconstruction more challenging to accomplish. Please comment on this.
For target tasks, have the authors applied the same pre-processing to Dataset I (CT scans) as the one used in pre-training? What about pre-processing for Dataset II (MRI scans)? How to address the domain difference (in terms of data) between pre-training and fine-tuning? The pre-processing steps for the target tasks should be included in the paper."	See the major weakness above.	"It would be better to provide more descriptions of the proposed method in the caption of Fig.1.

I would suggest briefly discussing the finetune difficulties among different downstream tasks (e.g., MR organ segmentation and CT organ segmentation).

Please provide more details on the split of the dataset, such as the ratio of train, validation, and test."
450	Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion	"It does not seem easy to see that the experiment compared to OpenPose has a significant meaning. It is recommended to compare with more recent pose estimation approaches or to train state-of-the-art approaches with the given data.
While training the RGBD keypoint detection framework, the magnitude of the difference between RGB-based and depth-based errors is ignored, which has room for improvement.
The training data was composed of only a small number of patients (3 patients), but it would be good if the performance change could be shown according to the amount of training data.
Several typos need to be corrected.
Page 3 Given a RGB -> Given an RGB
Page 5 (i.e., AMASS[25], ... -> missing the close paranthesis
Page 6. 3.1(1) inconsistent double quotation marks"	"The authors should consider addressing the following points.

The proposed methodology of using attentive fusion is novel and appropriately combines the RGB and depth heatmaps for multi-modality 2D human pose estimation. The authors obtained better results than the state-of-the-art RDF model. However, The RDF model uses the Resnet-50 backbone features from various modalities, whereas the authors have used a more accurate and high-capacity HRNet model. So whether the improvement is coming from a better backbone or the proposed attentive fusion is currently unclear. For a fair comparison with the RDF model, authors should consider either using the same backbone model or using their proposed fusion method in the RDF model.
The authors have used the ""Depth Keypoint Detection branch"" to estimate the 2D keypoints from the depth image. However, a depth image provides a better 3d representation encoded in the depth values. As the final aim is to estimate the full 3D mesh, using only the spatial 2d heatmaps from the depth might not fully utilize the full potential of the depth image. The authors should consider exploiting more appropriate 3D features from the depth image for the 3D mesh predictions."	"Methods
*	The loss function is shown in Fig. 3, however, it not explained anywhere in the main paper. Maybe it's definition and explanation can be moved to supplementary entirely, as going back and forth between supplementary and main paper is tedious for the reader.
Experiments and Results
*	It is not clear on which dataset the baseline methods were trained. To they use SLP, SLP + proprietary data, or are off-the-shelf models used? None of the baseline methods except RDF use the SLP dataset in their original models, so the comparison in terms of methodology would not be entirely fair for off-the-shelf models. 
*	Following up on the previous point, general purpose pose estimation methods, such as OpenPose, need to generalize to a much greater variability of poses, compared to only lying, in-bed poses in SLP and the proprietary dataset. Maybe other works focusing on in-bed poses [1,2] could be considered for a comparison.
*	The work in Ref. [36] in the paper was followed up by the same group with Ref. [15] in the paper. Why was [36] chosen as comparison? Particularly considering that [15] also uses RGB-D modalities and shows very encouraging results. 
*	It seems like the PVE-T-SC metric is missing from Table 1.
*	In Table 4, why are only results for the MI dataset presented, what about CT or MRI? Why was the head pose omitted from the evaluation?
Conclusion
*	An interpretation of the results in terms of clinical significance is missing. For someone unfamiliar with the topic, the presented metrics show superior performance of the approach compared to other methods, but their clinical interpretation is not clear. It would be good if authors could comment on the clinical significance of the results. Does the approach already satisfy the demands of a clinical application? Which error ranges would be considered acceptable?  While I like the clinical evaluation in section 3.3, this is especially true for these metrics. 
*	Following up to the previous point, if the results are not yet satisfactory for a clinical application, limitations and areas for future research should be discussed as well.
[1] Yin, Y., Robinson, J. P., & Fu, Y. (2020). Multimodal in-bed pose and shape estimation under the blankets. arXiv preprint arXiv:2012.06735.
[2] Clever, H. M., Erickson, Z., Kapusta, A., Turk, G., Liu, K., & Kemp, C. C. (2020). Bodies at rest: 3d human pose and shape estimation from a pressure image using synthetic data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 6215-6224)."
451	Self-Supervised Depth Estimation in Laparoscopic Image using 3D Geometric Consistency	Overall a well written paper that contributes to the field of 3D reconstruction stereo endoscopic images. The paper would benefit by having the points made above addressed concisely	"A weakness for me is that this method is quite general and not really specific to computer assisted surgery/MICCAI community. I feel like it could have been submitted to a mainstream computer vision conference and validated on the larger datasets those communities use, can the authors explain why they have submitted this paper to MICCAI?
The motivation for depth-from-mono is not well established, many laproscopes now are stereo- the authors should improve this motivation in the introduction. The authors also do not really explain how they propose to use a depth from mono method to solve the type of applications they suggest in the introduction. Without a known object, a mono system cannot predict true depth. The authors should explain how they see depth from mono being used.
I have concerns about the accuracy of the proposed method. Although it is clearly better than the previous monocular methods from mainstream computer vision, the error is still very large. Perhaps too high to be useful? Is this simply off by a scale factor since it's using a mono method? Can the authors comment on this?
The authors could/should have compared with some previous medical depth from mono papers for a stronger comparison. For example 'Self-supervised Learning for Dense Depth Estimation in Monocular Endoscopy', Liu et al, MICCAI 2018, this paper has code available and would be a stronger comparison.
Is this statement actually correct: 
'However, all of these methodologies employed left-right consistency and smoothness constraints in 2D, e.g. [3],[5], ignored the important 3D geometric structural consistency from the stereo images.' I see the self-supervised methods as implicitly using the 3D whereas this is more explicit about it. The authors should clarify this point unless they disagree with my assessment."	Please refer to the weakness section.
452	Self-Supervised Learning of Morphological Representation for 3D EM Segments with Cluster-Instance Correlations	"I realize these are impossible to achieve within the rebuttal timeline and would still recommend acceptance. However, if the authors want to improve the next extension of the method, I would recommend the following:

As noted above, I think it would be important to evaluate if the learned representations have to be retrained when a different dataset is used for input.
Besides the evaluation on a different dataset, preferably a mammalian one with more difficult classes, I think it would be interesting to compare more directly to the methods of [15] and [20]."	"Writing: The paper can be clearer if it can directly attribute modules to previous works and emphasize on the proposed new pseudo cluster label supervision. In its current form, it's unclear to novice readers which is this paper's contribution.

Experiment: The paper only provides ablation study comparisons. It's unclear how it compares with previous neuron subcompartment classification methods or state-of-the-art point cloud contrastive learning methods."	"In general, the paper is well written and easy to follow. The motivation is clear. The motivation is clear to integrate the feature correlations at the instance level and cluster level. However, it misses the quantitative ablation study for the cluster/instance level contrastive learning. Although BYOL and SwAV can be seen as the instance-level and cluster-level methods, the training set may be different from the proposed method. It is better to add a comparison of the proposed method w/o L_{CIL}/ L_{IIL}.
Others:

For the cluster initialization methods. Do KMEANs and DBSCAN have a large difference?
Why choose K-means instead of the Sinkhorn-Knopp algorithm?
Does the choose of /beta affect the performance?
How to set K?
How to choose the size of the memory bank?"
453	Self-Supervised Pre-Training for Nuclei Segmentation	Please address the comments listed under weaknesses of the paper.	"The setup of ablation studies in this work is pale and weak. I don't find its current version convincing to justify the contribution of each individual design. A more comprehensive ablative setup is suggested, together with a more insightful ablation discussions.
It might be more preferable by the community to denote Vision Transformer as ViT."	"(1) The paper could add some explanations for illustrating why the proposed method will not fall into trivial solutions.
(2) Additional ablation studies regarding the losses are necessary.
(3) To make a fair comparison, it is better to also adopt Vision Transformer in InstSSL and some other methods.
(4) It would be better to also compare with other generic self-supervised learning methods, such as MoCo, BYOL, SimCLR, etc.
(5) It seems like the font of the paper does not follow the MICCAI paper template. It is suggested to fix this."
454	Semi-supervised histological image segmentation via hierarchical consistency enforcement	"The datasets used in the experiments are relatively small regions of whole slide images. Provide the computation time (inference and training) and the average computing time for the instances (glands, cells). Providing the computation times can help evaluate the feasibility to apply this method directly on whole slide images in the routine work of pathology laboratories and researchers in computational pathology.

Please elaborate why the learnable HCL of equation (4) is ""capable of providing a more reliable prediction as guidance for the student model"". Did you test this experimentally? Such statements should be accompanied with evidence to some degree at least. Is this the first row of the ablation results table? If so, please refer to it properly in the sentence.

The writing can be improved in some parts: In section 2, the last paragraph that introduces the notation, should be the first, I think.

Figure 1. Is quite cluttered and difficult to understand at a first glance, making it simpler probably would draw the attention of more readers."	"Part of the optimization process is based on the measurement of uncertainty from the teacher's network output, but the explanation is limited and there are no additional experiments that validate its importance. It will be advisable to give more information to clarify and validate the design choice.
In addition, I am concerned that the model is sensitive to the number of layers employed for HCE and learnable HC losses."	Please refer to the weaknesses above.
455	Semi-supervised Learning for Nerve Segmentation in Corneal Confocal Microscope Photography	The proposed method is interesting and achieves an improvement in corneal nerve segmentation. However, this improvement is limited with respect to other methods for corneal nerve segmentation. I suggest to the authors to investigate about the clinical relevance of this small improvement and about the generalisation of the proposed method to other types of images.	The framework is a very complex framework e.g., three stages, pertaining with two networks type GAN, fine-tuning, retraining etc., but the Ablation Study is very short, and the table is not convincing enough. For example, what is the influence of plain model with using pseudo-labels, dropping any training stages, what about the other combinations. Based on my understanding, in this complex framework both model design and dataset mixing shall play a role and Ablation Study shall distinguish them clearly.	"(1) It's unclear why gated convolution is used in coarse repairing network in the perspective of its difference to refine repairing network.
(2) It may be improper to use the term ""supervised "" in Table 2, as the ground truth are simulated from existing centerline, which are not real careful annotations and may be even worse than pseudo labels. Therefore, this manner is not able to demonstrate the proposed method is better than supervised method. Please use a weaker statement or provide stronger evidence.
(3) Please provide analysis of computational considerations such as data efficiency and time efficiency.
(4) Please provide description about what knid of preprocessing steps are needed to deal with noise and difference in background brightness.
(5) Is there any reason to propose this framework for nerve segmentation? Is there any special design for this narrow object? Is it also suitable to solid objects?"
456	Semi-supervised learning with data harmonisation for biomarker discovery from resting state fMRI	"The paper missed many recent studies on multi-site brain disease diagnosis, including population-based graph neural networks, domain adaptation, and domain generalization methods.
The proposed method applies the harmonization operation over the FC values, processed through a Pearson correlation function, rather than the raw BOLD signals. It is wondering how the performance would change when the harmonization is applied to the BOLD signals first and then to construct their respective FCs, feeding into the EDC. It would be interesting to compare them from different viewpoints.
Regarding model training, no loss term involves the site-related parameters of $\gamma_{iv}$ and $\delta_{iv}$. How can those parameters be optimized?
The authors raised an issue of inconsistency in diagnostic criteria among sites. However, the proposed method doesn't handle the issue at all.
A critical concern about the performance is that the reported performance values are too high compared to the existing work on the same dataset, e.g., ABIDE, in the literature. In particular, the accuracy of the competing method of ASD-SAENet is approximately 10% higher than the one reported in the original paper. How could the authors explain this?
The authors raised an issue of inconsistency in diagnostic criteria among sites. However, the proposed method doesn't handle the problem at all."	"With the goal of helping smaller studies to be able to augment their data set, I think it is imperative to show the stability of the determined site-invariant biomarkers. A simple leave N-Sites out approach should be able to do this?
It would also be good to see how much training data was actually going into the individual models.
I also spotted a wrong highlight in Table 1 for USM. Maybe use only one number after the decimal point, as this table was pretty hard to read in some areas.
I would not call putting the code online, although extremely important, a major contribution in the introduction."	"Comments are listed in order of appearance in the paper. The most major concerns are marked with (M).

The paper states ""Unlike more complicated alternatives like ComBat, our approach of removing site differences allows biomarkers to be easily derived via computing saliency scores [10] since the implementation is based on linear layers."" The proposed generalized linear model for harmonization is the same as that used in ComBat, but ComBat estimates the parameters in a different way. Thus I am not sure what is meant by the quoted statement, as after the resting-state fMRI data is normalized in ComBat and then applied to say a classification task, feature importance could be attributed to the proper ROIs using the normalized data.

In the data harmonization definition in eq 1, should M_{jv} be M_{iv}? The design matrix for covariates of interest (eg. gender, age) should only depend on the specific subject (i.e. subject j at site i).

In eqs. 4-6, it seems that the subscript i has disappeared, but is needed to denote all the subjects from different sites.

In Sec 2.4 motivation for SHRED-II, where ComBat is applied before using the proposed model, the sentence ""For very small datasets (< 50), we propose a two-step variant of SHRED"" makes it sound like ComBat is only applied in the small dataset cases in the experiments. However, I think that the intended meaning is that the two-step variant is proposed to further improve prediction in small dataset cases. Please clarify/reword to make this clearer.

(M) For training of all the deep learning models, how many epochs were run / what criteria was used to determine when to stop training?

The loss appears to be largely dominated by the harmonization reconstruction term, with a hyperparameter that is many orders of magnitude higher than the other loss hyperparameters. I'm wondering how much of an effect the VAE parts of the loss have then, i.e., how important is the VAE modeling compared to simply the shared representation and included data harmonization?

(M) To reiterate the point in question 5. it would be helpful to see comparisons to other data harmonization approaches, e.g., applying ComBat and then single site training.  I am also curious how the results would look if the proposed model were applied to single site data, so that there would be harmonization of age and gender factors. This could further demonstrate the advantage of being able to include more data in the semi-supervised learning approach (if this performs better).

For the supervised and semi-supervised (without harmonization) methods, it appears that the extra data that is used for harmonization (age, gender, site) are not included in the model. This then is not exactly a fair comparison, and goes back to the point above about needing to include other harmonization comparisons or at least compare other methods that also consider age, gender, and site (e.g., as inputs to the DNN).

For the ASD-SAENet results, how might the authors explain the seemingly much higher performance reported here than in the original paper proposing the approach, which also used the ABIDE individual sites for testing?

Table 1 is a bit hard to read - consider adding more space between columns or vertical lines to better separate values.

Some noted typos:
p. 5 ""constraints"" -> constrains"
457	Semi-Supervised Medical Image Classification with Temporal Knowledge-Aware Regularization	"An ablation analysis on batch size is needed to verify whether the results are sensitive to it.
It's better to visualize the cluster across different training iterations. This will help the authors understand how the proposed IPH module works.
A fair comparison between the proposed method and other medical image classification methods [22, 36] should be conducted."	I appreciate the thoroughness of the analysis (different datasets, several state of the art methods, ablation study).  The organization and presentation of the paper is easy to follow. But the paper lacks qualitative analysis to demonstrate the mechanism behind the components (IPH) of proposed method. It would be excellent if the author can provides additional analysis on this point in their future work.	please see above comments.
458	Semi-Supervised Medical Image Segmentation Using Cross-Model Pseudo-Supervision with Shape Awareness and Local Context Constraints	"More theoretical explanations for the framework architecture or how this design sufficiently addresses the lacking of dataset issue, i.e. overcoming variability and complexity issue of medical images, is needed.
For the comparison experiments, the choice for the number of training samples, 7 or 14, needs to be justified. Would this be clinically relevant, as a larger number of training sets could be reasonably obtained for the segmentation tasks targeted? Overall, the proposed experiments showed inferior results compared with the baseline ones using full supervision.
When the local context loss is used, the DSC increases substantially for RV, while only slightly for Myo ad LV. An explanation for this difference is helpful."	"The paper is well written, the idea is simple and clear, the key is how much the author's contribution helps on performance gain. It will be nice if the authors can elaborate a bit what exactly is new compared to citations [4] and [9]. Also elaborate a bit on the ablation studies, specifically in Table 1,  what exactly is the ""Baseline""? what is ""UE"" (the authors mentions only U1 and U2 for the unsupervised loss), what is the impact if no threshold is applied to the entropy?
There is also a conflict when the author describes the implementation and experiments. The author claims that they use only 7 labeled samples to generate the results in Table 2. But in section 3.2, the author states that during training a batch size of 24 is used, 12 labeled, 12 unlabeled.12 is more than 7. This is confusing"	"The unsupervised loss function should be better justified.
Could state the robustness of the method with respect to rotations or whether they have to be aligned and in the same pose. The preprocessing necessary for shapes."
459	Semi-Supervised PR Virtual Staining for Breast Histopathological Images	As said before, the dataset not seems to be free and public	"Comments:

(p.3) ""It is a pity for traditional unsupervised method to discard the information of consecutive slides completely"" - this is unclear, please explain/rephrase
the description of generator on p.4 is vague. If the specific architecture is not crucial and the metod was tested with several different architectures yielding similar results it should be clearly stated in the paper. Alternatively, the specific architecture used should be give in supplementary materials.
Nash equilibrium (p.5) is not properly introduced of referenced - it could be misleading for non-expert readers - please provide short explanation or reference
(p.5) ""[...] auxiliary classifiers are used to introduce attention for finding the focus region [...]"" - the auxiliary classifiers are not fully described, please provide further explanation in either mian text or supplement
(p.5) I believe the Nvidia model 3090 should be RTX instead of GTX, please double check on that
Results presented in Table 1 are not cross-validated and based on overall only 8 WSI slides, this poses a question about reliability of these numbers
slide-level performance and ablation test has only qualitative results, that might be biased
the metod works on 5x magnification (big FOV) with such magnification clinical usefulness of such processing is questionable, please provide some confirmation or reference/citations"	"Dear authors, 
very interesting work. I would go further on the ""brown"" color extraction for PR+ annotation and will add an explanation by pathologist about their perception error in the brownish grading.
Proof read again ; ""Refenence"" instead of ""Reference"" and Paragraph 2.1 and 2.2 are not always very clear at first reading."
460	Semi-Supervised Spatial Temporal Attention Network for Video Polyp Segmentation	At one point the authors say they annotate every 11 frames. But in the supplemental materials the figure says every 5 frames. This should be clarified.	"Figure 4: if possible, please mark the ROI in the original images in order to present a better understanding of the part that should be segmented for the readers.
Section 4, Concluion: A stronge evaluation over the proposed approach is presented; however, the main advantages of the method over the state-of-the-art in not discussed. I recommend to highlight this point in the manuscript.
Section 4, Conclusion: There are no significant evidence (such as computation time) in the manuscript to evalute the feasibility of the method for the real-time clinical application. I recommend you to add this point along with more explanation on the clinical need for such a system."	Qualitative results are shown with different models and the segmentation performance of the proposed approach is good.  It is better that some failure examples of this approach is shown with the reason.
461	Sensor Geometry Generalization to Untrained Conditions in Quantitative Ultrasound Imaging	"The description of the introduced symbols should be improved. Some notation is defined but never used (e.g., W_probe), other is never defined (e.g., R and omega in equation 1). Then, do y and I_s represent the same thing (i.e., output image)? In 2.4, authors say that y represents the ground truth, while in 2.1 y_i is defined as ""organs and lesions"". Definitions should be more consistent.
The implementation details at the end of 2.4 are very difficult to follow, since the notation is quite heavy. In particular, it is not straightforward to me what the apices p, l and r associated to the inputs x indicate.
Authors claim that the method provides ""accurate AC image"", but more details should be provided to support this claim. How do PSNR and MNAE allow to assess the method accuracy? Equations provided in the supplementary material B use a different notation than the one in the paper, thus are not really helping with the understanding. Moreover, measurement units should be reported.
In 3.1, authors refer to the ""aggregated dataset"" but they never define what it represents.
It is not clear how the Deep-All baseline method is exploited for evaluation. Are the proposed modules used together with Deep-All in the ablation study? If so, how are the different modules interfaced? When authors refer to ""the NN"" in section 3.1, do they mean the Deep-All model?
The description of in-vivo experiments should be improved. In particular:
1) How do authors define which is the AC related to lesions in in-vivo experiments? Do they extract lesion area from the generated images via segmentation? 
2) To which approach do results related to the ""ablated baseline"" refer to (among those in Table 2)?
3) Since the paper reports an evaluation using in-vivo data, ethical approval must be mentioned.
4) I would suggest moving the details about the geometry of the probes used in in-vivo trials to section 3.2, to group there all the information related to such experiments.
As for the data-augmentation part, it would be interesting to report the data distribution after VSG-aug is used and compare it with the one in Table 1. This would allow to understand if the tested conditions fall within the data distribution obtained when using the augmented dataset.
Images can be improved for better clarity. Fig.1 should include the acronyms relative to each ""module"" to facilitate understanding of what is what. The font size in Fig.2 is too small, making it impossible to read, but it is important since it is the only way to know the dimensionality of the different parts of the architecture.
Typos:

Euclidian -> Euclidean
Each resolution subnetworks -> Each resolution  subnetwork
adjust -> adjusts"	"The authors mentioned they used ""biomechanical property that is set to cover general soft tissue characteristic"" with  a reference. It would help better clarification of the paper to mention these mechanical properties and how they were implemented in the simulations."	"There is a recent highly related paper that the authors have probably missed and I recommend to cite:
A K Z Tehrani I Rosado-Mendez, H Rivaz, (2022). Robust Scatterer
Number Density Segmentation of Ultrasound Images, IEEE Trans. UFFC (TUFFC)
The authors should simplify the paper. The DSA module should be explained in more details. Why using B-mode in DSA and why B-mode of one imaging setting is used.
Overally, some part of this paper is not clear."
462	SETMIL: Spatial Encoding Transformer-based Multiple Instance Learning for Pathological Image Analysis	see 5.	"I believe authors should discuss the memory usage of the current approach and also the fairness of the experiments against methods that use a small proportion of patches.

To this end, they can compare their method against the following work, which also uses all the patches. Here is an example:
""Neural Image Compression for Gigapixel Histopathology Image Analysis"" 
David Tellez*, Geert Litjens, Jeroen van der Laak, Francesco Ciompi
Also, this is necessary to benchmark their work against end-to-end methods. Here is an example:
""CNN and Deep Sets for End-to-End Whole Slide Image Representation Learning""
Sobhan Hemati, Shivam Kalra, Cameron Meaney, Morteza Babaie, Ali Ghodsi, Hamid Tizhoosh"	"Fig. 2. Sub-figure (A) - Color meanings is not considered. For a while I was confused with yellow green blue color code for 3x3, 5x5, and 7x7 windows. However, right below that the same colors have been used for showing unfold without any relevance to the windows size.
= Concat(T2Tk=3(Ei), T2Tk=5(Ei), T2Tk=7(Ei)),
it needs period (.) after this equation.
""achieve inferior performance compared to other methods [1, 9, 8]."" should be other MIL methods
Table 1.needs patch percentage from each WSI"
463	SGT: Scene Graph-Guided Transformer for Surgical Report Generation	"How to define the Interaction Representation X^r, i.e. how to extract it from the raw image?
Why only use X^r to calculate the relation memory M, instead of including X^v?
The more intuitive explanation about why extract 'interaction' can make the graph change from heterogeneous one to homogeneous one? Meanwhile, why graph induced attention can capture the 'local' attention?
If the scene graph is first generated by [9], I am wondering whether it shall require the extra annotation information. Is the 'interaction' annotated by the experts, as the original dataset does not contain such information?
For the dataset, EndoVis18 actually show limited variety within each sequence, therefore 3 sequences for testing are relatively insufficient. The cross validation is better to be conducted when the dataset is small."	It is recommended to use self-contained tables and figures' captions to transmit the information more clearly.	In general, the proposed approach is novel and interesting, and the experimental results could demonstrate the effectiveness of the proposed method. However, there are some unexplained parts and typos (as listed in the weaknesses).
464	Shape-Aware Weakly/Semi-Supervised Optic Disc and Cup Segmentation with Regional/Marginal Consistency	"How would the method handle multi-class setting? I have troubles to understand the motivation of Eq (2), and I absolutely do not see how it could be generalized to multi-class. Which is a problem here since we have three classes to segment, so I am a bit lost.
For equation (6), would it be possible to use ""default"" or ""safe"" values for unlabeled samples?"	See the weaknesses.	"It can be added in the abstract that how much the performance of the proposed method is better than that of SOTA.
Why is the data size set to approximately 1:2 for the weak supervised set and the test set allocated in the UKBB data set?
In each batch of the training data, must the proportion of labeled data and unlabeled data be 1:1?
In the experiment, what is lambda in Eq.(9) set to? Why?"
465	Shape-based features of white matter fiber-tracts associated with outcome in Major Depression Disorder	"Specifics about analysis:
There are two major issues with the analysis. These both stem from the fact that this is a relatively small sample with many covariates. You do not have much statistical power to detect any associations, except for very strong effects. There are two things that you do that I believe are inflating the association results, and this is not a standard way to do association analysis.
1) I do not understand the point of adding noise to the bundles in section 3.3, it sounds like an attempt to do some regularization because of the excessive number of covariates compared to the number of samples. This is fine in machine learning and statistics, where you are solving a prediction problem, but this does not seem correct in the setting of doing association, it actually might result in double dipping and inflated association statistics.
2) I understand the point of doing leave one out cross-validation, but it makes more sense for prediction. In this setting, for the each of the 1000 models run, I would have wanted to see two models. One with only standard covariates and the other with the standard covariates and then also the ones specific to the particular bundle (local or global). Then the authors should perform a likelihood ratio test whether the model with the shape data is significantly better than the model that has only the standard covariates. This is a standard approach for this kind of analysis. The results seem to be the adjusted R squared on the case that is left out in cross-validation. This R squared is using the standard covariates with the shape data, so it doesn't reflect only the contribution of the shape data, but also the standard covariates, and is thus very likely inflated and not truly representative of the association the researchers seem to be after.
Comments on English language:
1) This sentence in section 3.3 is wrong w.r.t. English language: ""We did not added more landmarks..."".
2) It sounds weird at the start of section 3.3 to say that you ""performed"" two linear models. I would say that you either fit two models, or that you defined two models. Defined sounds more correct here, since you furthermore fit a bunch of times for cross-validation."	Interesting findings in the splenium of the corpus callosum, the right optic radiation and the right thalamo-occipital fascicule. How would this fit with major depressive disorder?	"The first sentence is a bit misleading. Depression is a wide spectrum of mental illness, some minor, some mild and some dangerous. Please specify that you are referring to major depression, or treatment resistance earlier.
Riemannian is upper case Riemannian
Diffusion image processing: please report which tools you used. 
Capitalization in bibliography: 7-t in bibliography should be 7-T or 7T, mri should be MRI, etc"
466	ShapePU: A New PU Learning Framework Regularized by Global Consistency for Scribble Supervised Cardiac Segmentation	"*	As stated above my major concerns are about the quantification of the time savings possible using the proposed approach. Is the time/accuracy trade of arguable? 
*	Besides of being a steady standard for medical images segmentation, is there a reason to stick with U-Net? There are several modifications and/or alternatives published with demonstrated superior segmentation performance (e.g. nnUnet). 
*	How do you explain the drop in performance when combining cutout and pu without including shape in your ablation study? Please add a short discussion on this to the respective section.
Minor:
*	There seems to be a typo here: ""We randomly divided the 45 images into 25 training images, 5 validation images, and 20 test images."" Either you used 50 images or a different train/val/test split.
*	Table 1: I assume the significance is always given in comparison to the previous model? A clarification in the caption or text would be appreciated.
*	Figure 3: The images are pretty small. If there is enough space, larger images would be appreciated
*	Table 2: I assume HD is HAUSSDORFF DISTANCE. Please add it to the caption. 
*	Is there a reason for not highlighting statistically significant differences in Table 2 and 3? For consistency I'd suggest adding these indications here too.
*	Figure 4: It would be interesting to also in include the corresponding scribble annotations"	"Authors provide with enough insights of their methods and a good number of experiments with comparison against other methods. Perhaps when they open their code they will be completely understood.
An explainability analysis is documented so, the reader can understand the advantages of their methods"	"With reference to my above comments regarding weaknesses, here are my comments:

Please clarify the equations for the EM section and the PU loss section.
Why should it be shape and not texture? When the cut-out is performed the model could perceive it as change in the shape or in texture since the texture information is missing in the cut-out region - hence the term 'shape' is quite misleading. Also, how helpful is cutout? Would be helpful to see the results in extreme cases? e.g. distortion or non-circular MYO?
Provide more details regarding the shift in characteristics between the two datasets and a brief disc on how they were reduced. Samples from the datasets could be included in the suppl. material.
If possible, it would be good to see the cases where the method fails or gives lower performance.
The results report greater improvement in HD values (btw, average or 95th percentile of HD?) than dice - why is this? was the boundary smooth with the proposed method that led to lower HD? Authors should discuss the reason behind this given that the improvement in HD is much better than Dice)."
467	Show, Attend and Detect: Towards Fine-grained Assessment of Abdominal Aortic Calcification on Vertebral Fracture Assessment Scans	"A good piece of work which has some future work left.

Applying the proposed algorithm on a larger dataset.
Testing another option rather than using a pre-trained network.
Explore how the difference between expert assessment and the outcome of the proposed method can be reduced."	"First, I'll elaborate on my first point in the weaknesses. The authors note the reported results in the previous publication, but write them off by saying they ""report their best results."" It is unclear what is meant by this statement; in most cases, we want to present our best results in a publication. If the authors are suggesting that the previous paper might have cherrypicked a beneficial test/train split or random seed, then it would be helpful for the authors to replicate this result (e.g., demonstrate the range of possible results using the previous publications methodology and show that the reported value is the top of the range). This would demonstrate the limitations in the previous paper and show that the baseline model is implemented correctly.
I'll also list the issues/errors I have found with the evaluation here.

The metrics reported in table 2 are computed one versus rest and then averaged, which can be deceptive in a three-class problem. The accuracies here are substantially lower than the actual three-class accuracy, which can be directly calculated from the confusion matrices in figure 3 (about 73% for the proposed model and 56% for the baseline).
There aren't any confidence intervals or statistical tests showing the stability of the results or the significance of and differences.
The second example AAC24 score in figure 1 is calculated incorrectly. The value should be 2, not 0.
The confusion matrices in figure 3 are transposed. As displayed, the counts for the ground truths do not match the breakdown in the body.
The metrics for the proposed model in table 2 do not match the confusion matrix in figure 3. Most are close, but do not match exactly (for example, the Moderate PPV should be 37.5 not 40)."	"Section 4: A few important details of the implementation were not explained, which hinders a full comprehension of the model and of the obtained results. For example:
    - how many training epochs were performed?
    - what was the threshold correlation value used in early stopping?
    - which layers used dropout regularization and what was the dropout alpha?

Section 4.2: Data augmentation transformations should be applied carefully to medical images. Shear transformations are not always desirable, as they may distort spatial relations between anatomical structures. Also, the authors should clarify further the data augmentation ranges, e.g., ""[-10, 10] pixels""

What was the reasoning behind the choice of ResNet152v2? Did the authors test other backbone architectures? The results analysis could be improved with a comparison of different CNN encoders.

The paper need to be thoroughly proofread. Some minor issues include:
  - page 1, line 16: add a comma after ""out of these""
  - page 2: define AAC-24 before in the text (e.g., after ""Kaaupila 24-point scoring method)"". There is a definition later on section 2.1.
  - section 2.1: no need for ""rd"" after 1/3 and 2/3. Also, if the authors find the space for it, please rewrite the third phrase in 2.1, using ""more than""/""less than"".
  - section 2.1, 2nd paragraph: repeated ""the"" after ""Furthermore,""
  - In Fig.1b, there seems to be a typo. There is no calcification (as described in the text), but the left column of AAC-24 reports a 2 for L3 Ant, and the score is 0.
  - section 2.2: after [3] -> ""followed"" and consider removing ""as [4]""
  - section 2.2, line 4: add comma after ""as a whole""
  - ROI acronym is not defined. Also, it can be used in section 4.2
  - near the end of section 2.2: ""scores AAC-24"" -> ""AAC-24 scores""
     - section 3, line 3: ""prepossessed"" -> ""pre-processed""
     - section 3, line 6: ""conventional"" -> ""convolutional""
     - section 4, line 1: remove ""of"" after ""comprises""
     - section 4.3, line 17: start sentence with ""In each fold,""
     - section 4.4, line 2: ""as compared to"" -> "", instead of a""
     - section 4.4, line 4: ""sum up all"" -> ""sum of all"" / remove comma after ""Since""
     - section 4.4, line 7: ""comprises of"" -> ""consists of""
     - section 4.4, line 15: remove ""one each""
     - ""vertebra"" is the single form, ""vertebrae"" is the plural form"
468	Siamese Encoder-based Spatial-Temporal Mixer for Growth Trend Prediction of Lung Nodules on CT Scans	"8-	It is better to add some examples of images where the algorithm fails / succeed to classify correctly the output, a lake of images makes the quality of the discussion section poor. Illustrations improve the quality of the paper.
The discussion section is poor, and it would add to the quality of the paper to study more the strength/weaknesses of such method.
The framework is simple and seemed to give good results though the components are not innovative and have been used before which make this paper more suitable for other conferences rather than miccai."	It is suggested that other signs of pulmonary nodules and clinical information of the case be used in this paper, which is closer to the doctor's clinical diagnosis process. In the experimental part, more comparative experiments and ablation experiments should be done to show the superiority and effectiveness of the method more comprehensively and accurately.	"Lack of clarity: Better explanation of the figures: it would be helpful if authors could make the figure captions, especially Fig.3, more clearly. Abbreviations in the figure should be explained in the caption, for example, MLP, FG1,etc.
For future work, I would recommend to apply the proposed framework on more diseases and more image modalities, such as lesion variation in cine MRI, cervical cancer lesion variation in time serie colposcopy, etc."
469	Simultaneous Bone and Shadow Segmentation Network using Task Correspondence Consistency	No additional recommendations. US segmentation is a challenging task and this proposal will increase (not so much, but at least a bit) the current performance.	"The number of subjects is small. This can result in low anatomical variability, and it is unclear if the subjects are healthy controls or patients.
It is not clear how Section 2.1 is related to the segmentation ground truth.
For the curvilinear probe, the shadows should not be in the vertical direction parallel to the length of the image. Instead, they should follow a fan-shape along the transmission direction of the ultrasound beams. This can affect the validity of the trained network.
Some discussion regarding the requirement of accuracy vs. the impact of the application will be appreciated.
The term ""mutual information"" used in the article is not appropriate since it does not refer to the more commonly used information-theory-based metric.
Figure 5: (d) and (e) are not consistent between the first and second rows regarding the surface and shadow segmentation.
It will be good to mark which anatomical structures are shown in the figures.
For the Ablation study in Table 2 and 3, it is unclear if the improvement with CTFT is statistically significant."	The paper can be improved by collecting data from disease subject.
470	Skin Lesion Recognition with Class-Hierarchy Regularized Hyperbolic Embeddings	"In this paper, a method for skin lesions embedding and classification based on hierarchical class relations encodng and  hiperbolic embedding is presented.
The two main contribuions are:

They use hyperbolic geometry, instead of Euclidean, for the image embedding. 
. They incorporate to class distance a distance based on the hierarchical relations between classes.
Five comments that may improve the paper:
1) I consider questionable the data augmentation that they employ. In an application where color is essential for the diagnosis, it does not seem adequate to incorporate color transformations in the augmentation.
2)  An explanation of the dynamic range and justification of the values of the different metrics in Table 1 would be desiderable. For example, they do not justify that 50% accuracy is reasonable for 65 classes or the maximum possible value of the mistake severity or if 1.X is a good value for HD-k
3) It would be interesting to highlight in the text the main contributions of the paper with respect to the literature.
4) Perhaps the theoretical explation 2.1 could be substituted by more details in the method.
5) Figure 4 is impossible to visualize."	It would be nice if a concrete contribute list is provided in the paper.	"The introduction succeeds at stating the problem and the related work. It is also well-motivated and clarifies the paper's contributions.
It would be helpful to see the standard deviation on all the values in table 1 to evaluate the proposed method's performance further.
The authors mention more implementation details in the Appendix, but they seem to be missing.
Can the authors further clarify the default ""0"" parameter used in Eq. 3. It seems with 0, Eq. 2 simplifies a lot, and it is not clear how this affects, for instance, the expressivity of the network.
I believe there is a typo on the abstract on the word ""provably."" It seems that the authors meant ""probably."""
471	SLAM-TKA: Real-time Intra-operative Measurement of Tibial Resection Plane in Conventional Total Knee Arthroplasty	"Firstly, I would like to comment that the accuracy of the results you have presented in the manuscript is impressive. However, I have a few questions regarding the method used to create these results:

From my understanding, the algorithm requires as input an initial state, comprising of an estimated C-Arm pose for each fluoroscopy image, and initial 3D pin poses. According to equation (3) the state also contains a set of 3D back-projection points for each contour edge point and each fluoroscopy image. I'm unsure how these 3D points are initialized? As mentioned in equation (1) and (2) the relationship between 2d contour points and 3D back-projected point is only proportional.
Would the algorithm not also require the tibia and pin contour points from the fluoroscopy images? I believe in figure 3, these are named contour observations? If so, how are these contour points determined? How would the accuracy of the contour detection influence the overall results?
In your simulation experiment (section 4.1) what was the resolution of the fluoroscopy images? The differences in your results compared to the other methods are noteworthy. Could you discuss your results and why you think that your method outperforms the comparative methods?
The colors-dots in the legends of the images in Fig 3 are too small to identify
According to the supplemental material the tests seem to be performed all with N=2 (number of fluoroscopy images). This information should be added to the manuscript. Also, how would change in N effect the results, i.e. N=1, N=10?
Despite the good results, I have some hesitation about the clinical feasibility and applicability of the proposed method. For example, the following sentence in the manuscript ""... the tibial resection plane estimation can be completed without much influence on the CON-TKA procedure."" gives the impression that the method can be applied at the cost of the runtime of the optimization. To the best of my knowledge, fluoroscopy imaging is not standard of care during a conventional TKA procedure in many clinics. Therefore, setup and obtaining the images should be considered part of the proposed method and clinical feasibility and applicability should be considered including these extra steps and additional radiation exposure. 
In the introduction you mentioned that your proposed method can be used as replacement of systems which are actively navigating the tibia resection block and/or tibia resection. Arguments against the computer-assisted navigation system includes: extra procedure steps, time, complexity and instruments. Although I agree that these are well established and published disadvantages of many computer-assisted TKA systems, I believe it would need to be evaluated if your proposed method can in fact eliminate these concerns. As mentioned above, procedure time and complexity might be influences by the use of fluoroscopy. Furthermore, while computer-assisted system are designed as a ""one-step"" navigation, your proposed method would need a repeat alignment of the tibial block when an error is detected.
Lastly, you mentioned also that no significant improvement of mid-to-long term functional outcome was detected using navigation. However, in the publication you refer to, the authors did measure a significant improvement of tibial component alignment compared to conventional method. Your method also aims to improve the tibial component alignment. I would therefore be interested to get your thoughts on if and how your system might have the potential to overcome this mid-to-long term problem."	"The authors present an important method for improving the CON-TKAs by formulating the cutting plane estimation method as a SLAM method, and demonstrate the effectiveness of their method by the in-vivo experiments. Some suggestions are give below.
The authors should add the clinical evaluation or add the corresponding discussion in the manuscript, and should carefully proofread their manuscript to correct the existing typos, e.g., ""gold standar""."	"More proofreading is needed to correct the typos, e.g. ""gold standar(d)"".
Enlarge the legend font size in Fig.3"
472	SMESwin Unet: Merging CNN and Transformer for Medical Image Segmentation	"The authors examined their work on three datasets. They obtained the best results for two datasets (GlaS and WBCs). However, the results for MoNuSeg was inferior to UNet++. And, there is no explanation for this result. The authors may provide an extended discussion on their results. 
Such difference may arise from the different characteristics of the datasets. All three are similar in a sense that the method needs to segment objects that are circular or elliptical, in general. The ones in MoNuSeg may be the smallest. This may affect the performance of the method. If so, it indicates that the method has difficulty in dealing with small objects. The authors may investigate their results from this perspective. 
Also, the comparative methods are not the state-of-the-art methods for the three datasets. The authors may compare their method to the current SOTA methods for these datasets.
For these datasets, DICE and mIoU may not be the optimal choice for evaluation metric. PQ would be an alternative, in particular for nuclei segmentation, which has been widely used these days."	"1: I feel that there are too much text used for the superpixels generation. Current version has not give enough arguments and motivations for using it.
2: In MCCT, it is a bit mathematically intensive; providing more conceptual description would be helpful for the readers to understand it.
3: Pointing out the limitation and future directions for the further development of the method could help some readers sometimes."	"As the T1, T2, T3, T4 all have different number of channels and number of patches/tokens, how are they concatenated together?
In section 2.3, please double check the dimension of all matrices mentioned. The Ti should be in the shape of dCi; if W_Qi is in the shape of Cid, the shape of Qi should be d*d.
Could authors provide more insight illustration on the reason why they think CNN analysis on raw images cannot deal with its 'influence of constructed defect and noise'.
In Eq 5, as Fi are in different size, how should they share the same Mk?
The CNN generated features are only used as 64/(64+128+256+512)=0.07 of the T_sum in attention mechanism. I am wondering if this can be a significant contributor to the final results. And also, swin transformer with CCT should be compared with MCCT to prove the CNN branch is useful. Also, in original CCT paper, they used four skip connections, while authors of this paper used three and one CNN branch. It is better to provide more experiments and illustrations on this: replace one or add extra one."
473	Sparse Interpretation of Graph Convolutional Networks for Multi-Modal Diagnosis of Alzheimer's Disease	"It is unclear why the loss in  EQ. 3. is called mutual information. In my knowledge, the mutual information describes divergence between two probabilities. EQ. 3 looks more like a conditional probability based on joint attention matrices. Could the authors clarify more this ambiguity?
The regularization/cross-entropy loss in EQ. 5 need to be checked. What are the ground truth and predicted variables?
Please check the mathematical notation in section 2.2. n and N look referring to the same thing.
The authors have chosen K=10 but they didn't justify this choice or discuss the effect of K smaller or larger on the performance of the GCN model.
The authors might also compare their method with existing Graph attention-based convolution networks which are close to SGCNs.
The presentation has some English typos to be checked in (e.g., section 2.3)."	"Dataset sample size in each of the modality is less in number. How to make sure the convergence of proposed method?
Edges in functionality connectivity are estimated based on weighting the Gaussian similarity function of Euclidean distance. What is the intuition of selecting this metric for evaluation.
More detail about mutual information loss is needed.
Why the interpretability is being called as sparse. How the proposed method make sure that the sparsity is being imposed on interpretable coefficients?
Interpretation results need to be discussed in more details.
Grammetical errors need to be checked throughout."	"The research work in this paper is innovative and of practical significance. It will be better if the following questions can be considered.

Since the work in this paper is based on multimodal data, it would be better to analyze or learn the relationship between multimodal data more deeply.
Several more datasets can be used to test for greater persuasiveness."
474	Spatial-hierarchical Graph Neural Network with Dynamic Structure Learning for Histological Image Classification	Even though the flow of the text is good, there is a relatively large number of grammatical mistakes, mostly in the use of articles such as 'a', 'an', 'the'.	"In the first paragraph, citations started with [6,28] it is better to see an order in the references [1, 2] instead.
Please see the weakness section."	"The paper is well written and well organized. The method is clear and the comparisons are sufficient. However, I have some suggestions:

please, could the authors clarify how dinamically change the structure of graphs? what is meant by dynamic change? do it change during epoch?
In subsection 2.3 it is written "" Specifically, each hierarchical sequence is tokenized and attached with positional embedding as the input of a Transformer encoder consisting of Multi-Headed Self-Attention [26], layer normalization (LN) [4] and MLP blocks."" How the tokenization of graph in ViT happens? Please, clarify
further information on reproducibility should be added, for example the early stop criterion for the choice of the best model
with particular reference to BRACS dataset, in the cited paper, as HACT, the results are given in terms of F1 measure, why did the authors enter the results only in terms of AUC? AUC as Accuracy do not take into account the imbalance of the dataset, which is particularly evident in BRACS. Instead, F1 measure is adapted in the case of imbalanced dataset and I suggest to added also this measure
the comparisons were performed on the same dataset for all methods? HACT used a previous version of the dataset, thus I would make sure of this.
the reference to CRCS dataset is missing. Moreover, the reference to BRACS is not [22], but the following:
Brancati, N., Anniciello, A. M., Pati, P., Riccio, D., Scognamiglio, G., Jaume, G., ... & Frucci, M. (2021). BRACS: A Dataset for BReAst Carcinoma Subtyping in H&E Histology Images. arXiv preprint arXiv:2111.04740."
475	Spatiotemporal Attention for Early Prediction of Hepatocellular Carcinoma based on Longitudinal Ultrasound Images	"[Major comments]

The main drawback of the proposed method is its novelty in technical aspects. The authors mention the difference between the proposed method and nonlocal attention in section 2.3, but the difference might be marginal. Please provide clear and additional explanation on the novelty of the proposed method.
Some experimental conditions are not clear. (1) Why do the authors set N to 3 ? What does it mean from the clinical benefit point of view ? (2) Were the ultrasound images captured using the same ultrasound diagnosis machine ? If not, are there any impact on the performance ? (3) The ablation studies include ""(iii) STA_HCC without the age-based PE (w/o age-based PE)"". How do the authors define PE in this case ?  I think the authors do not use the PE used in the vanilla transformer since the experimental results shown in Fig. 3 and Table 1 are different.

[Minor comments]

The term ""longitudinal"" is confusing. In medical ultrasound images, the term ""longitudinal"" uses to indicate the direction of the image plane in general. It might be good to provide a note about the definition of the term."	"Usually I understand ROI to mean bounding box. Consider renaming ""ROI attention"" to ""spatial attention""
My understanding is the ""Transformer [18]"" baseline does not use age-based PE? It seems that your model w/o age-based PE does worse than [18], which suggests that maybe [18] could outperform your model if you just add age-based PE.
Would be a little stronger with multisite data."	"There are major weaknesses in the evaluation of the method:
The information of and hepatocellular carcinoma is kind of localized in MRI slice. Compared with reference [22], why do you think taking advantage of the gating mechanism would help the localize focal areas? Could you add more experiment to show that the necessity of ROI attention in this task? For examples, adding results without sigmoid or ROI attention in Fig.4 to intuitively show the improvement of location.

There are sentences which require some references.
-In page 2, .... (US) is currently the most common imaging modality for diagnosing and monitoring HCC "" requires a reference.
-In page 8, "" bidirectional LSTM (BiLSTM) "" requires a reference. 
-In page 6, ""We chose SE-ResNet50"".... requires a reference.

Typos:

The dataset collected from...--->> The dataset was collected from...
-.. track and focus the same lesion...--->> track and focus on the same lesion...
-..used to establish a appropriate positional...--->> used to establish an appropriate positional..."
476	Spatio-temporal motion correction and iterative reconstruction of in-utero fetal fMRI	"The authors have pursued an important problem - motion mitigation in fetal fMRI. 
The strengths and weaknesses are outlined above."	"My major concern is the comparison of baseline methods. Authors only compare their proposed method with some simple interpolation-based method, which can be much improved in my point view. Fair comparison with baselines would make this manuscript more convincing. Check the ``main weakness'' for more details.
While This manuscript is well organized and easy to follow, there are still several things unclear to me.
What does  the metric ''L'' mean in Fig. 3 (a)? I do not find any definition all through the manuscript. Also, SSIM metrics require a reference image, but obvious there is no such a ''ground-truth'' in this dataset.
 Fig. 2. shows a Reference volume, but the main text does not have any description of the reference volume. Also, the ''s2v'' in Fig.2 is not properly abbreviated.
Language-related errors that can be easily fixed.
''..., whereas the proposed 4D iterative reconstruction did recover the entire brain.''"	"1: Iterative reconstruction of the fMRI data with 96 volumes using the low-rank and total variation constraints can be computationally demanding. Please comment on the computation complexity.
2: How the downsampling and blurring operators in the reconstruction equation are designed should be clarified.

The motion operator is missing in Eq. [7].
In the current algorithm, the motion is estimated prior to the iterative reconstruction. The curiosity is whether the motion parameters can also be iteratively updated during the reconstruction."
477	Stabilize, Decompose, and Denoise: Self-Supervised Fluoroscopy Denoising	"Literature needs to more comprehensive about fluoroscopy denoising.
Do comparison with other SOTA methods."	"Robust Alignment[1, 2], which combined RPCA and image registration, has been studied to deal with stabilizing the video where sparse corruptions exist.
Robust image alignment aligns the batch of images together, and simultaneously doing RPCA so that the alignment is robust to the sparse corruptions.
Using robust alignment, we could acquire aligned images, and corresponding low-rank background and sparse foreground.
It can be viewed as the combination of Stage 1 Stabilize and Stage 2 Decompose in the proposed paper.
It should be clarified about the difference between the proposed method and robust alignment, and also the reason why the proposed method is a better formulation.
Also, robust image alignment handles all image data at once, where the Stabilize step of the proposed method finds the transformation parameter for each frame. The robust image alignment can find a better registration parameter based on better accurate low-rank subspace.
Training a smaller student network using a large teacher network to reduce prediction time is a widely used technique, but I can't find any information about speed improvement or execution time w/ and w/o student-teacher network.
Optical flow is estimated from the network implemented by PWC-Net. Was it newly trained on this fluoroscopy video, or was it a pre-trained network?
In the example videos (in supp.), I observed the blood vessels are moving with the heartbeats, whereas the background body does not.
Is it okay since we are finding a representative translation parameters using KDE (where the pixels for blood vessels are more sparse compared to the pixels of the backgrounds)?
It is difficult to investigate better performance in Figure 4. Consider adding arrows to emphasize.
Consider adding the following recent RPCA paper in the Introduction:
Han, Seungjae, et al. ""Efficient neural network approximation of robust pca for automated analysis of calcium imaging data."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021.
Followings are minor comments.

Are other denoising methods (Noise2Void, Noise2Self, ... ) trained in XCA dataset the same as Self2Self?
In the citation of RPCA methods in the introduction, need to change the order of the last two papers (currently [... 32, 34, 33]).
Typo in the Table 1 caption. denoiseor -> denoiser
In Table 1, there are Ours+N2V, Ours+N2S, Ours+S2S. Isn't the proposed framework include Denoise step? Consider denoting as 'Ours w/ N2V'.
Not sure only one proficient radiologist is sufficient, without consensus, for qualitative validation of the methods.

References:
[1] Peng, Yigang, et al. ""RASL: Robust alignment by sparse and low-rank decomposition for linearly correlated images."" IEEE transactions on pattern analysis and machine intelligence 34.11 (2012): 2233-2246.
[2] Zhang, Xiaoqin, et al. ""Robust low-rank tensor recovery with rectification and alignment."" IEEE Transactions on Pattern Analysis and Machine Intelligence 43.1 (2019): 238-255."	"This paper proposed a three stage framework for denoising including stabilizing using optical flow, decomposing by proposing masked Robust Principle Component Analysis (RPCA), and denoising by using a simple self-supervised network. Evaluated on one private dataset and one clinical dataset (a radiologist to rate denoised image quality).
The paper is easy to follow and understand, the writing and formation could be improved (such as adding numbers to Fig. 4). The novelty is limited except a contribution to RPCA (based on Inc-PCP algorithm) to solve non-overlapped areas, the first two stages are data processing stage and the denoising stage leveraged an available self-supervising network, overall it does not have a significant contribution.
The experiments are not strong, suggest to use more publicly available benchmark datasets and avoid to use private dataset. The comparisons to other methods are unfair since the proposed method used optical flow to stabilize inputs for motion data, but other methods such as Self2self proposed for the additive white Gaussian noise (AWGN) data. More recent works suggest to discuss after the year of 2020 and compare if possible."
478	Stay focused - Enhancing model interpretability through guided feature training	"If the proposed study focuses on the XAI point of view, it is an opinion that it may be better to focus on the proposal of eCDF-Area metrics for visualization methods that try to explain the output of models such as Grad-CAM, SmoothGrad, SmoothGrad-CAM++, etc.
The training scenario in Figure 1 should be expressed more concretely in terms of the input and output of the model."	"Perform further experiments as (or similarly) than the proposed in the point above
Further justify (and or add a reference) to certain statements. For instance ""As preliminary experiments have shown, common models that determine if a
certain instrument type is currently visible (instrument presence detection) lack focus on the instruments themselves and often decide based on features in the background."" Where has this been shown?
Improve figure captioning. For instance, add description of what the graph shows in Figure 3, specially on the horizontal axis; and improve the caption of figures including what the reader should extract from them."	"Thank you for the work performed for this paper, I have some points to kindly raise:
1.1 Table 1: 0% for the clipper, for automated masks? Why is that, do you know?
1.2 Table 1: I notice the Scissor performed less optimally compared to other instruments, any thoughts around this?
1.3 Thoughts about the combo providing the best outcome? Do you think this is because a model needs to see more images not similar to the ideal in order to train more generaisable? 
1.4 Why only train with false negatives for guided feature training?
1.5 Figure 1 - I assume you first train the model then retrain? Sorry this needs to be a little clearer unless I have misinterpreted this.
1.6 Protocol for the video/cine stack - not sure this is clear 
1.7 My concern is probabably re the different combinations of original/modified datasets - what/how is an optimal value chosen - see previous comment - I don't see the ratio/size given for each?"
479	Stepwise Feature Fusion: Local Guides Global	"Writing issues:
1) Don't capitalize State-Of-The-Art.
2) ""attention dispersion"" is invented in this paper. If you propose a new term, please define/explain it in detail. Otherwise, please follow common terms.
3) In the caption of Fig.1., ""(a) is the ..."" => ""(a) the""
4) In the caption of Fig.1., ""emphasized features"" is an awkward term. 
5) In Section 2.2, (such as... etc.) please don't use ""such as"" and ""etc."" at the same time.
6) In Equation (1), what's Ci, C and i? Please define them.
7) In ""Stepwise Featgure Aggregation"", ""information interacted by .."" is an awkward expression.

Fig. 2 is not clearly explained. Esp., as transformer attention is pairwise, you have to select a query point and visualize the attention of all pixels with the query point. What are the query points used for the attention maps in Fig.2?"	"For equation (1) and (2), it would be better to explain the meaning of notations, like C and F_i.
In table 4, it seems the last sentence is not complete. (""the CVT is"")"	"For the Experiments:

In order to better explore and present the impact of LE and SFA separately, I'd suggest the author adding another group of ablation study for several settings: LE + parallel fusion, simple upsampling + SFA, LE + SFA and baseline without LE or SFA.
Keep mDice scores and remove mIOU scores, and add Hausdorf Distance as an extra evaluation metric for a more comprehensive segmentation evaluation.
Double check the data augmentation methods in papers [9, 14, 16, 25] from which the scores in Table 2 and 3 are refer. As I mentioned above, the different data augmentation will affect the robustness and generalizability of the model. It's even better to reproduce the models in those papers using exactly the same experiment settings, especially data augmentation, and compare the final scores in Table 2 and 3.

Other issues in paper writing:

From the paper, it's not clear how the author generate the attention maps at different scales. Please briefly introduce how the attention maps in Fig.2 is generated.
Check through the paper and fix the minor writing issues mentioned in ""main weakness"", as well as correcting some other spelling and grammar typos."
480	Stereo Depth Estimation via Self-Supervised Contrastive Representation Learning	Please refer to 4 and 5.	"More ablation studies may be needed to understand the contribution of each components of the proposed method. For example, how does the proposed momentum supervised contrastive loss compared to vanillar CRL loss/ MOCO loss?
It is unclear how the hyper-parameters are chosen such as the temperature \tau, and the weights for L_pe.
Can the authors explain more about multi-scale disparity estimation in the decoder?"	"The legend in Fig.1 is too small. 
Reference is expected to be added for DispNet.
The equation (3) is confusing. I understand losses, Lmo and Lpe are applied at different stages of the model training. But the equation suggests both losses are applied at the same time."
481	Stroke lesion segmentation from low-quality and few-shot MRIs via similarity-weighted self-ensembling framework	"The motivation of the article in investigating ways of using a large dataset to train a method on a small dataset on a different problem is interesting and may have impact. This could justify an article only on this problem without the goal of learning to apply in a situation with images with a lower resolution.
Although there is potential in the objective, the idea that by learning to distinguish between necrotic and enhanced tissue based on T1, T2, FLAIR and specially T1C may help to distinguish ischemic tissue, using a set of MRI sequences that differ partially is not obvious. So providing the rationale of the adequacy of the method is a must. Also, it is important to show that it is also consistent. This could be argued by more compelling tests. For instance, 1) the authors could have opted to present a test in the blind test set of SISS; this would allow comparing with state of the art methods on this dataset. An improvement over these would give indication on the strength of the proposal. This could be accomplished by not reducing the resolution of the images. 2) To give evidence on the consistency of the approach, the authors could test in another problem, for instance in SPES that distinguish between the core and the penumbra of the ischemic lesion. An improvement here could allow arguing the methods had potential to transfer information about the complexity of the brain tumor to another less complex problem but more complex than SISS.
The authors should improve the description of the method. Figure 2 should have all components defined and explained, or referenced to the respective article for detail. Also, the authors should indicate the number of feature maps, the input and output stages and, the places where the upsampling is performed. The training process is not adequately explained. The authors should explain when SDU updates the parameters. It is not explained if it is at every batch or every epoch. Also, when the samples of the glioma dataset are used and using which proportion? This is not explained in the article.
As referred in the weaker aspects above, the authors should review the tests. The rationale for the selection of the methods should be clear. Also, a comparison with state of the art methods on the problem would strength the paper. The training of each method should be described. Also, the baseline should be included in table 1."	"-Rather than splitting the dataset in training, validation and testing once, a nested cross-validation could be considered to evaluate the method over the entire dataset.
-Fig1. B has a typo. To be employed BY our method.
-Throughout the manuscript, the authors mention the voxel spacing without indicating the unit. I assume this is mm and it should be specified. 
-Fig.2 could be improved. Instead of repeating ResBlock and DiscernBlock, a legend showing the color-block correspondence could be added. The text is quite small and difficult to read. 
-I understand the limited number of pages available, but the Introduction is rather long and there is no Discussion section. Rather than discussing the results in the Experiment section, I would do that in a separate Discussion section.
-It would be interesting to add a time comparison between a manual annotator and the automated method proposed. This would strengthen the contribution of this work and its applicability.
-Evaluation. ""Four matrices"" is probably a typos. This should be ""Three metrics"" as only three metrics are listed.
-Conclusions: ""Our further work will improve the lesion segmentation accuracy and quantify the lesion volume"". This is a bit vague, the authors should rather mention how they think the lesion segmentation accuracy can be further improved."	"""pyramidal structure"" is better than ""coarse-to-fine"". ""coarse-to-fine"" usually describes an architecture that first explicitly processes a coarse version of the image and then fills in finer details.
a little better to rename ""accuracy"" to ""precision""
is stroke lesion segmentation really needed for ""rapid stroke diagnosis""? seems this is more of a tool for clinical researchers. maybe can be used for radiological reporting but not for diagnosis.
no related work section"
482	Structure-consistent Restoration Network for Cataract Fundus Image Enhancement	"(1) The text lacks clarity and needs important English editing. The authors also need to be more precise, to do not overinterpret their results and to dampen their conclusions. E.g., ""intuitively"" lacks clarity and not scientific.
(2) Other comments mentioned clearly in the strength and the weakness section of the papers."	"The parameters showed in the figures should be explained. In Fig 1, three kinds of loss fuctions, LR, LH, Lcyc, their meanings of symbol are not found in the instruction.
A detailed description of quantitative metrics of restoration and segmentation in Table 2 is required. It's better to clarify the reasons and advantages of choosing those metrics."	The author may consider discuss more about the loss function, as mentioned above.
483	Super-Focus: Domain Adaptation for Embryo Imaging via Self-Supervised Focal Plane Regression	"It is unclear how the decision on about which focal planes are missing (and need to be generated) is made. This requires some sort of data alignment. From what I could deduce, this step was performed manually. In either case (if it was automated), this needs to be mentioned explicitly.
It would be interesting to obtain more information about the acquisition hardware: same/different manufacturer, model, etc.
Section 4.1. ""... 4 (uniformly) randomly selected planes ... "" I find somewhat difficult to interpret how selection can be random and uniform at the same time. Please clarify.
From the description in Section 4.2 the reader can get an impression that the stacks were shown to the experts in this particular order: 50 real, followed by 50 simulated, and then 20 copies. Was this the case? Or were the stacks shuffled before being shown?"	Probably UMAP of the embedded features from the autoencoder can help us to have a better understanding if there is any domain shift or not.	"In addition to the suggestions mentioned earlier, some minor comments:

Table 4 appears before Table 3. Consider swapping the labels.
Fig. 3: mention that within one panel the pairs are real/generated, respectively. Otherwise, one could think the two left pairs are real and the two right pairs are fake.
Please carefully go through the references again. Some of them are incomplete or lacking page numbers and the like (e.g., 9, 12, 13, 32)."
484	SUPER-IVIM-DC: Intra-voxel incoherent motion based Fetal lung maturity assessment from limited DWI data using supervised learning coupled with data-consistency	Apart from the points described above, it might be interesting to perform more extensive experiments and comparisons to other models, also classical non-DL methods. The authors only focus on one unsupervised approach.	"Thank you very much for this high quality submission. The authors find my comments in the ""strengths"" and ""weaknesses"" section."	"This paper proposed a SUPER-IVIM-DC to alleviate the need to acquire DWI data with a large number of ""b-values"" by constraining the DNN training process through a supervised loss function coupled with a data consistency term. However, the SUPER-IVIM-DC is similar to the published work IVIM-NET [3] [11]. Compared with IVIM-NET, the correlation results of SUPER-IVIM-DC has no significantly improvements (0.239 vs 0.242)."
485	Supervised Contrastive Learning to Classify Paranasal Anomalies in the Maxillary Sinus	I think its a decent paper with a minor novelty and application to a new dataset. However I think the presentation is still lacking and can be improved. In particular IMO the introduction is a bit all over the place and the general structure/ clarity can be improved. Furthermore the Figures 2&3 should be revised (especially Figure 3).	Although the paper is well-written, the major novelty is trival.	"It is probably early to conclude and generalize that contrastive loss + cross entropy loss is better for medical images and those with limited amount of labels.
It'd be helpful to include one or more experiments with similar characteristics/problems for more generalizable conclusion.
More examples and visualizations to help understand the benefit of the proposed method would be good. For example, in which cases do the methods fail and succeed compared to the others? Can we get some insights into why?"
486	Supervised Deep Learning for Head Motion Correction in PET	"Language problems need to be checked, many long sentences are not clearly expressed and are confusing.
The experiments section needs more clarifications:
The subject group contains patients with normal cognition, cocaine dependence, and cognitive diseases. I would like to know if any strategy was used in splitting the training and test sets? For example, keeping a diversity of patient types in each subset.
In the ablation study (Table 1), the test MSE value for ""more data, FWT, Deep Encoder and Normal sampling"", which is located in the second row, has a much higher value than the others. Can the authors explain this?
The results In fig. 2(a) are for subject 1. Also, there is another subject 1 in Table S1. I assume they are different subjects since one is for single-subject experiments and one is for multi-subject experiments. But referring to them the same name still causes confusion.
In section 3.2, the authors claimed that subject 2 has a mean MSE of 0.02. But Table S1 shows the mean MSE of subject 2 is 1.114. They are contradictory. I assume they are also different objects as I mentioned above. The confusions need to be addressed.
""The results for Subject 2 (mean MSE 0.02) show that the network is capable of accurately predicting motion from training subjects even though the motion relative to the reference frame was never used for training."" Does this mean that in the experiment in figure 2, the moving images of subject 2 have been resampled and different from the images that are used during training? The detail of the experiment settings needs to be clarified."	"I wonder if the authors would like to explore the low number of degrees of freedom in the rigid body motion problem. The proposed network uses a lot of parameters and dense connections, which are useful for dense prediction problems. But the 3D rigid motion has 6 parameters after all. For example, the 3 translation parameters can be estimated from the difference in the centre of mass, if the centre of mass can be properly estimated by a neural network.
Also I wonder if the performance of the proposed model can be easily improved by doing like one MLEM step to replace the 3D cloud images in Fig1 as the input. As far as I remember, the first few iterations of EM give more or less what looks like the brain without the details. This is of course at the cost of time in preparing the input data, but it may be a worth trade-off.
Typo:
Page 3: 'algorithn'"	"Abstract:
The authors state ""However, to date, there is no approach that can track head motion continuously without using an external device.""
Well this depends on how you define continuously and if you only speak for PET. In MRI, 3D Navigators can be placed in a regular MPRage sequence every TR, so giving you a head position approx. every 2 sec, hence you should rephrase this
You use the Polaris Vicra, but throuhgout the paper there is no comment regarding if the Polaris has been validated wrt attenuation issues in the field of view of the detectors, so to speak are you acquiring worse image swiht the tracker on?
You explain already in the abstract that ""the output [of your DL method] is the prediction of six rigid transformation motion parameters."". Also later on this is stated, but in whcih coordinate system are these motion parameters reported? In scanner coordinates or in Polaris position coordinates? I am assuming in scanner coordinates, but that information is completely missing.
Intorduction:
I really like the introduction and the emphasis on the clinical application.
You state "" average head motion can vary from 7 mm [1] in clinical scans to triple this amount for longer research scans."". As someone who regularly works with 90 or 120 minute PET research scans, I find especially the seond statement hard to believe. I would tone that down.
Regarding the usage of tracking devices the drawbacks you mention are ""HMT is
not generally accepted in clinical use, since it usually requires attaching a tracking
device to the patient and additional setup time."" Well, by now there are also several mrtkerless tracking solutions on the market even MR compatible ones and setup times are not an issue anymore. So I would rather put the emphasis on extra cost of acquisition or on the capabilities of actually running event-based reconstruction methods based on high frequency external tracking, especially in the HRRT setup that due to its geometry restricts which reconstruction algorithms can be used.
I also disagree wiht the statement ""Other systems like markerless motion
tracking [11] are still under development and have not been validated for PET
use"". That's simply not true anymore, see e.g. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0215524
Methods:
Section 2.1
I have a comment on the choice of tracer. While FDG is by far one of the most clinically used tracer, it is also rather stable and one cannot compare kinetics to e.g. C11-based tracers for dopamin or serotonin. So while your population is interesting and diverse, their FDG images will still look very similar, I guess.
I ma a bit confused about the measure of motion. You state ""The average intra-frame motion of eight points forming the vertices of a cube [7] was used to summarize (mean+-SD) the overall motion of the brain throughout the entire scan to be 12.07+-7.12 mm."". Could you add an illustration of this? Is the cube located in the center of the head or center fo the field of view, I am a bit confused about this. Also an average of 12 mm over the acquisition across 25 subjects seems to be really large to me.
I think there is a typo in the description of the acquisition. You write ""All PET imaging data is 30 minutes acquired 60 minutes post injection."" This I would understand as you having a 30 minute acquistion, aka 1800 1-second frames. But below you keep using 3600 frames. So I am assuming that you switched the two numbers around and that it should say ""data is acquired for 60 minutes 30 minutes post injection"".
Also what is your regular framing for the 60 minute scan? 5 or 10 minute frames?
Section 2.2
You state ""The encoders effectively reduce the 3D image data volumes down to a vector of size 128."". That reduction seems really extreme considering the 6 degress of freedom you are trying to measure. Did you try with a larger input, e.g. by using a different network for pre-training?
Can you confirm that t_ref is the last second/1-second image in the whole scan?
As stated already in the intro, how was the HMT tracking data transformed into the 3D cloud, aka PET scanner frame of reference coordinate system? Is that cross-calibration provided by the HMT system, so all tracking is already provided relative to scanner coordinates or was this transform performed by you?
Section 2.3
This section confused me even more regarding to what the reference time is, is it now always the last timepoint or does it vary?
""we calculate the relative motion transformation matrix from the Vicra data"" I guess, here I would like more details and maybe a reference to understand it
Section 2.4
Here you introduce t_ref = 3600 which does not match with an image acquisition of 30 minutes
Results
I guess what you call theta is often referred to as RMS in the MRI motion correction community, see e.g. https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.27705, but that is calculated wrt a point of reference, e.g. a point cloud center when using a markerless tracker or center of the marker when using a marker-based tracker. What is your point of reference?
Just as a side note, when you are the only PET site in the world with an HRRT and using the MOLAR reconstruction, then anonymization kind of goes out the window if the reviewer has decent PET expertise.
Regarding FreeSurfer by now you probably want to replace reference 4 by Fischl B. FreeSurfer. Neuroimage. 2012 Aug 15;62(2):774-81.
Section 3.1
Your results on motion prediction where you state that ""accurate motion
prediction performance error to be 0.035+-0.073 (mean+-SD of corresponding
dataset)."" is quite impressive. But what was the overlal range of motion for that single subject? Can you add that?
Section 3.2
Figure 2, these plots are way too small, try and make axes descriptions only once and enlarge the plots. Also please comment in the caption on my question in which coordinate system this is shown.
Again when you state results for the other subjects such as "" Mean MSE for Subject 3 is 0.74 and for the failure case in Subject 4 is 6.33."" Please also give the overall range of motion for these subjects over the scan as reference.
Discussion
You write ""While our initial model results indicate capabilities of predicting motion of magnitude ~1mm, our current pre-processing reduces input image resolution to ~10mm3, which may limit the model's ability to detect motion with smaller magnitudes."" But as a common cutoff for motion when deciding whether to do motion correction of the PET data is 2-3 mm, this is not really useful in practice. can you comment on that?"
487	Suppressing Poisoning Attacks on Federated Learning for Medical Imaging	"This is a well motivated and presented paper that proposes a new method for federated learning that aims to address poisoning attacks. The work is discussed in context to related work and empirical evidence is presented to support the usefulness of the proposed method.
The limitations of the proposed framework are not discussed. There is a claim that the proposed method works when the proportion of clients experiencing byzantine-failures is less than 50% but this is not sufficiently justified. Moreover, there is no discussion about the proposed methods limitations with respect to the data/class distribution among the nodes that are participating in the federated learning network. These issues make it hard to assess the practical impact of the proposed method."	"Federated learning is indeed a good solution to enable a multi-center collaborative training process. However, in this paper, the only connection between medical images and FL is the dataset employed in the experiments. My suggestion is to change the motivation, making it closely related to the clinical aspect. For example, the authors can provide some specific attacks that commonly appeared in FL with medical images, then analyze why existing techniques will possibly fail in such cases.

Compared with SOTA works in this area, a major issue of this work is the lack of theoretical analysis. It would be better to discuss why the parameter-free algorithm is robust in abnormal detection, and why the distance space is more appropriate for detecting malicious clients.

To make the experiments more comprehensive, it would be better to analyze the performance under the different percentages of malicious clients (e.g., 10% to 40%) and different numbers of clients (e.g., up to 30 clients). Moreover, methods like RFA or SparseFed should be included as two strong baselines. The current version of Section 4.4 Results and Discussion is not very insightful, which only demonstrates the good performance without discussing why baseline methods fail and why the proposed method works well."	Please refer to the comments.
489	Surgical Scene Segmentation Using Semantic Image Synthesis with a Virtual Surgery Environment	Plots could be more interpretable than the tables but I guess it might be a bit tricky to convert them into plots.	"The first figure on https://sisvse.github.io/ is much more informative than Fig. 1 in the paper.

A table or paragraph introducing the main features of the dataset could be added, for example, total image numbers, organ types, instrument types, min/max instrument number in a frame, etc. Thus, the readers can understand the value of your contribution."	"What does X and O mean in table 1? A better notation could improve the clarity of the table.
Why do the authors choose to do class-balanced frame sampling? Surgical scene segmentation is an imbalanced problem by nature and data imbalance can be handled by the framework (focal loss, data augmentation, ...). Class-balanced frame sampling throws away a significant part of the data that could be used to improve the results. This should be at least discussed in the paper.
The authors should not only publish the datasets but also the synthetic data generation pipeline which could be transferred to other (medical / surgical) applications.
Were the medical professionals trained before creating the manual virtual synthetic data in unity? How do the authors make sure that the simulated execution resembles reality?
The tables on page 7 are cluttered, maybe it would be helpful to only present the most relevant results in the paper."
490	Surgical Skill Assessment via Video Semantic Aggregation	"i) What is the architecture of the transformer for which findings are reported in Table 3?
ii) What do we learn about the semantic groups in the HeiChole dataset? Do they emphasize only the instruments, how consistent are the groups based on visual examination?"	Please see weakness.	"A discussion on using inverse kinematics for supervision while they do not provide accurate or sensitive enough supervision could be discussed.
It is interesting that the proposed model uses clustering to aggregate semantic features, therefore does not need supervision. However, a note on how the K is somewhat arbitrarily chosen (with a belief that these features will relate to tools, tissue, and the background) should be added and discussed in the light of the resulting clusters which only loosely relate to these elements (Fig 4)."
491	Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer	Please refer to previous comments.	"The submission instructions must be followed correctly. The paper ID must be added at the beginning of the submission.
The legends in Figure 4 are not consistent with the reported results in Tables 1 and 2.
How is the annotation generation process performed? Is it a completely automatic process?
Do the questions and sentences follow natural language templates?
It is not clear why the performance drops when using temporal features. Are there any additional insights?"	"-Except for the weakness, I think the author can give more details about the motivation behind this VQA task as well as its clinical benefits. For now, the system can only act as a second option to roughly clarify student's confusion. I am wondering if there is any intra-operative applications that could use this system.
-The effect of the decoder is unknown. As far as I am concerned, the transformer decoder, unlike the auto-regressive decoders can predict sentence at once. Can you give more explanation about the decoder's choice and can we replace this decoder with the other methods, such as LSTM, GRU?
-Also, will you public the dataset?
-Can you compared to the other captioning methods?"
492	Survival Prediction of Brain Cancer with Incomplete Radiology, Pathology, Genomic, and Demographic Data	"This work introduces a multimodal fusion model built on top of a state-of-the-art deep learning model for improving survival predictions.
The model architecture and the optimization strategy are well motivated. The authors have provided all the necessary justification for each component.
It will be very helpful for the reader if the authors could explain their reasoning behind reconstructing the embedding of the data instead of the data itself.
The authors could perform cross-validation to select the best hyperparameter. This could improve their performance."	"The authors are providing a solution to a highly meaningful and practical problem in clinical research, which is learning features from multi-modality dataset under the condition that data points from each modality could be missing on subjects due to various reasons. Providing solution to such a problem would help researchers take advantage of data that used to be discarded.
Since many features are added into the comparison, more analysis should be needed to identify which features are the most important for improving the performance of the prediction model. For example, training with modality dropout seems to bring substantial difference when compared with the corresponding models without dropout. Such factors need to be highlighted and supported with statistical tests.
The extraction of radiology image features is described with limited details. For instance, it is unknown how the segmentation of the 3D tumor volume was conducted. Also the extraction of 2D features seems arbitrary. Overall this step could result in over-parameterized model that is prone to over-fitting.
The extraction of pathology image features is described without necessary details. Since the cited method would generated three types of features which are from CNN, GCN, or the combination of the two networks, the readers would need more information to know which way to follow if they would like to replicate the work.
The introduction of the reconstruction following modality dropout seems to bring very limited value in the prediction, which to some degree is expected since the reconstruction is based on existing features from other modalities which doesn't bring in additional information to the combined model. More work may be needed to justify the introduction of this feature."	"There are some method descriptions which can be made clear

What is modality dropout? Why was it used in the previous works? How are the modalities dropped?
In what way is your method ""optimal""?
Why do you use radiomics features in addition to the ResNet features? How does the performance change if you do not use the radiomics features?
What is the source of variation in the results in Table 1? Are there different splits being considered here? The table description can be made clearer.
How are the single modalities reconstructed from the mean vector embeddings? What is the architectural design of the model used for this? What is the decoder structure?
Do the experiments clearly demonstrate your answer for Q3? To me, it looks like both strategies perform somewhat similarly and more experimental validation is necessary to clearly answer the question.

There are some typos/grammatical mistakes.

All the experiments were ran on a -> All the experiments were run on a
Chen et al chen2020pathomic should be a reference
Some prior studies already shown -> Some prior studies ""have"" already shown"
493	SVoRT: Iterative Transformer for Slice-to-Volume Registration in Fetal Brain MRI	"The authors present a novel and interesting approach using transformers for the slice to volume motion estimation in the reconstruction of fetal brain MRI. I've found the paper interesting but many aspects appear as preliminary and I have major concerns with the choice of the experiments. My general comment is that as a work of slice-to-volume registration it would have been valuable to illustrate experiments according to the level of motion. This is lacking to me as to understand the value of the simulated images.
It is unclear to this reviewer wether this method aims at provide a 3D reconstructed image or only an initialization as for other classical inverse problem reconstruction. Formulation in equation 1 solves the data term inverse problem, but it is unclear to this reviwer why no regularization is included then. If the final goal is really to provide also that SR image, classical inverse problem could have been used as for SOTA methods also. But if the goal is to use this approach as to further initiaiza a more classical SVR it would have been good to illustrate then how the different initializations (SVoRT, Planet, SVRnet) influence a classical SVR recon (inverse problem+regularization). So, how SVoRT potentially improves SVR is not proven.
It is unclear to this reviewer the experimental setup. Authors mention FeTA dataset, and then registration to a brain atlas and resampling to 0.8 mm isotropic. Which is the rationale behind this step? Was this done by FeTA or this is something needed/specific for this study?
Authors mention simulation of 3 stacks in random orientation, do they mean orthogonal? Or really random? Often in a real fetal acquisitions orthogonal views are generated (sometimes not perfectly). Please clarify the definition of random views.
From the 12 left testing cases (which GA? Were they normal or pathological?) it says 4 different samples were generated for each. What does it means? 4 different levels of motion? This is a crucial point to understand the indudec motion for generating testing examples. Would have been interesting to see results vs level of motion for instance.
It seems then in the experimental setup that yes a SVR is used further for real cases. Which SVR method is applied with which regularization technique?
This reviwers wonder also the parameter setting of the SOTA methods, if any, how this was choosen? Do they also have outlier rejection scheme?
Would the authors compare with manual initialization or classical motion estimation methods slice to volume for comparison purposes? At least for the two illustrated real cases. Certainly those methods might be more time consuming but would be interesting to illustrate the added value.
I am not sure I understand what does it means the study with one stack only. There is then no super-resolution in that case. In practice, due to the in-plane through plane resolution differences, multiple stacks are acquired for fetal MRI. I would have found the assessment more meaningful if starting from 3 stack up to 6 for instance.
Is real data at 3T? A gap/slice thickness of 2mm is the smallest one often seen in fetal acquisition that may go up to 4 mm often (also depending on the field strength and in plane resolution).
Did the experiments with real data with SVRonly use some classical slice to volume or multi-scale slice registration as often used?
Could the authors evocate hypothesis on why SVROnly seems even to work better that with the two SOTA initialization? I've found this weird overall. Would have been interesting to see the type of low-resolution stacks acquired as to illustrate the level of motion."	"This work produces a novel solution to a challenging clinical problem. The SVoRT construct architecture to predict transformation and volume simultaneously. The estimated volume as auxiliary task provide 3D context to improve the accuracy of predicted transformation. This work utilize transformer to encode spatial correlation of the input sequence. Specially, during volume estimation, this paper consider some wrong slices, resulting in artifacts in the reconstructed volume. They proposed addition SVT to predict weight of slice, where represent the image quality of the slice. 
I am curious about the necessity of transformer module in this framework. What are the benefits of transformer compared to other RNN (lstm)?
Can the estimated volume of the network output be used as a preliminary reconstruction result? How about quality of the estimated volume? If the quality of estimated volume is good,  the estimated volume as a reference volume for reconstruction."	"Unclear why using both y_hat and y (concatenated) and not only y_hat as the input of ResNet, please provide an explanation (or experiment) on the why

Please mention dataset type (synthetic v.s. real) in Figure 3
Minor comments: sec 2.3. to b*ridge, sec 2.3: << Previous works >> have demonstrated, please cite, sec3.1: << Learning rate of 2 x 10 4 and linear decay for 2x 105 iterations >> unclear please give decay rate for how many iterations and please mention total number of iterations(or epochs)"
494	Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI	"Please refer to the weakness section. My main comments are:
1) How do DDDDDD-O models perform? This is important for understanding/ validating the necessity of combining  Swin Transformer and deformable attention. 
2) Attention score based explainability is not unique to this method but a property for all Transformer-based models. Have the authors checked how it compares to gradient-based model explanation methods?"	"To strengthen the article, the experimental part needs to be refined by including proposed benchmarks and other transformer-based models.
Explainability claims need to be elaborated too (or removed from the title)."	"I suggest the authors to write the motivations behind their designs more clear, especially in the methods parts. Why and how the design leads to performance gain? Which design reduces the computation cost? I can see the experimental results confirm the claims (good performance with limited computational cost + explainable fastMRI), but more insights of the design will make this paper more solid.
The most interesting part of this paper is the claim of ""explainable fastMRI"", but it is not convincing enough about how the proposed method provides explainability. Detail explainations/motivations on this should be mentioned in the introduction & methods parts.
More comparison with SOTA methods, especially those including DC layers should be demonstrated. Also, it is interesting to see if the performance improves when including DC layers."
495	Swin-VoxelMorph: A Symmetric Unsupervised Learning Model for Deformable Medical Image Registration Using Swin Transformer	"The dice evaluation has been done using Freesurfer segmentation has ground truth. In an extended version, it would be interesting to see also the results on a dataset with manual annotations (such as the IBSR or the MICCAI multi atlas 2012 challenge dataset).
Using these data only in the test would also help reinforcing the results regarding the generalization."	The authors should improve the figures and make Figure 3 easy to understand.	"One of my main concerns with this work is a lack of precision in the explanation of steps which are essential for the whole apparatus to work well and be credible. For example, the authors state in the 'symmetric loss terms' 2.2 subsection: ""where phMF and phFM are difffferentiable and invertible in a bidirectional fashion"". How is this guaranteed?This can  be precisely formulated in mathematical terms."
496	Tagged-MRI Sequence to Audio Synthesis via Self Residual Attention Guided Heterogeneous Translator	"For pair-wise disentangle training, the authors selected a few channels for the feature to denote the mean and variance. Please explain how these channels were selected and why.
I suggest the authors should also justify the necessity of GAN.
I believe it may be hard to enlarge the dataset used and explore more word samples, even sentences. But it would be more interesting if more samples can be included for this study."	It is difficult to reappear, but the results are promising for applications.	"The authors should include a discussion how the model would scale up with an increased dataset size.
The reconstructed samples are very distorted which is probably (partly) caused by the use of the Griffin-Lim algorithm for waveform reconstruction. Maybe the authors can discuss more recent approaches, e.g. MelGAN [1].
The authors should the implementation of the baseline method, Lip2AudioSpec, in more detail.
Basic information about training, e.g. batch size, should be included in the manuscript.
The ""self-trained attention network"" seems to only generate masks by blurring the residual frames. Why is this complicated approach chosen, and could a simple technique maybe even improve the performance (e.g. filling the area of white pixels in the binary residual frame with simple computer vision techniques)?
The authors should discuss the very related field of generating speech from real-time ultrasound images, e.g. [2].
[1] Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de Brebisson, Yoshua Bengio, Aaron C. Courville, MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis, Advances in Neural Information Processing Systems 32 (NeurIPS 2019), 2019
[2] Jing-Xuan Zhang, Korin Richmond, Zhen-Hua Ling, Li-Rong Dai, TaLNet: Voice reconstruction from tongue and lip articulation with transfer learning from text-to-speech synthesis, Title of host publicationProceedings of the AAAI Conference on Artificial Intelligence, 2021"
497	Task-oriented Self-supervised Learning for Anomaly Detection in Electroencephalography	"1)	What's the criterion in defining the range of amplitude scaling factor, e.g., \alpha_l and \alpha_h in the second paragraph of page 3, ""Generation of self-labeled abnormal EEG data""? Also applies to the scaling factors for frequency scalar factor. Is this range different for different types of anomalies?
2)	Please compare with method [25], and indicate the major advancement of the present method in comparison to [25]. Noticed that there are some results presented in Fig. 3. 
3)	In general, what's the dimension for the shortcut branch from the output of the 1st convolutional layers to the penultimate layer?  Does this design need to change for the detection of different anomalies of EEG disease?
4)	Regarding the methodology discussed in page 5, what's the performance if you retain the classifier head instead of using Mahalanobis distance on the extracted features??
And what's the criteria of determining the threshold for anomaly detection?
5)	As for the experiments, please indicate the disease present in these datasets."	"I think the authors need to justify how to and when should one to use the task oriented SSL rules? When do they apply?
I would highly recommend improve the baseline. The current unsupervised anomaly detection baselines are too old to be effective."	"Choose and discuss more complex anomaly similation combining frequency and amplitude abnormal.
It is suggested to provide 2D t-SNE to visualize the features of normal, simulated abnormal and abnormal features.
Comparision to supervised ( with fully and partial training label ) methods.
As a classifier is trained, it is interesting to see the performance of directly using the classifier to detect abnormaly EEG data.
Another simulated based method in medical imagning is highly related to this paper but has not been cited:
(NormNet:) Label-free segmentation of COVID-19 lesions in lung CT, TMI, 2021."
498	Task-relevant Feature Replenishment for Cross-centre Polyp Segmentation	Some aspects need to be clarified as stated in the weakness section.	"The formulation of TCLoss is not intuitive; it is understandable that I(p+) should have lower entropy than I(p_di), but TCLoss essentially encourages the network to be uncertain on I(p_di); how is that benefit the overall method and what if we only penalize I(p+)?
For PAAL module, why pixel-level adversarial learning is applied rather than image-level? Domain is usually defined on the image-level, how does the network can differentiate the domain of a pixel?
The TRFR module is very similar to a feature pyramid network (FPN). Does TRFR really replenish the task-relevant feature, or is just simply aggregating feature from different scales? Please discuss the different between TRFR and FPN and consider citing relevant works."	The authors should redraw Figure 2 and make it easier for the audience. Through literature search is missing currently. The clinical motivation of the paper could be improved. I feel that CBAM can further improve your model performance. Please give it a try.
499	TBraTS: Trusted Brain Tumor Segmentation	The only remarks that I have is that some references relative to U-Net variants are missing, and some sentences are not well constructed; please reformulate them.	"Besides subjective logic theory, many methods have been proposed to assign belief of mass, for example, Shafer's model [1], Evidential KNN [2], and Evidential neural classifier [3]. Why did the authors choose subjective logic theory instead of other methods to assign belief of mass? The discussion and comparison between different belief assignment methods should be interesting and meaningful. 
[1]. Shafer, Glenn. A mathematical theory of evidence. Princeton university press, 1976.
[2].Denoeux, Thierry. ""A k-nearest neighbor classification rule based on Dempster-Shafer theory."" Classic works of the Dempster-Shafer theory of belief functions. Springer, Berlin, Heidelberg, 2008. 737-760.
[3]. Denoeux, Thierry. ""A neural network classifier based on Dempster-Shafer theory."" IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans 30.2 (2000): 131-150.

why did the authors only choose two modality data as the experiment dataset? And the Gaussian noise was added to which modality? Why only add noise on one modality but not modality together?

Fig 2 a) is hard to follow. The performance of Dice is not as good as others. The authors should give some explanation for it.

In Fig 2 a), the most stable method is UE. The authors should give some explanation for it.

The results of Fig2 are obtained during training. Where are the results of the test set?

Fig 4 only visualize the uncertainty comparison with the slice containing enhanced tumor (class 4) results. The brain2019 dataset is a three-class segmentation task. The authors should offer an example slice with three class information included to show the model's reliability better.

There are some methods that use the Dempster-Shafer theory for uncertainty quantification in medical image segmentation tasks. Some discussion beyond those methods is necessary, i.e.,
[1]Huang, Ling, et al. ""Evidential segmentation of 3D PET/CT images."" International Conference on Belief Functions. Springer, Cham, 2021.
[2]Huang, Ling, Su Ruan, and Thierry Denoeux. ""Belief function-based semi-supervised learning for brain tumor segmentation."" 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI). IEEE, 2021."	Considering elaborating why the proposed/used uncertainty estimation method is more effective, in medical image segmentation (e.g., brain tumor), than the existing ones. In your experience, is there a conscious difference between different estimation methods? Such kinds of discussions can provide insights to the community and increase the contributions and impact of this paper.
500	Test Time Transform Prediction for Open Set Histopathological Image Recognition	"Ablation study: Train a simple model with the same data split as in the paper. The train data would comprise all classes in the closed set in addition to the open set as a single class. Comparison with this experiment would show the benefit of the proposed approach.
Tables 1 and 2: Add columns for average ACC and AUC over all three splits.
It is not clear which loss function was used for classification and transform prediction.
What is the form of ground truth labels and expected network output for the transform prediction task?
Was thresholding applied to softmax probability to decide whether the input image belongs to an open set or closed set? If so, what was the value and how was it obtained?
Do you think the trained model will generalize well to the unseen dataset from another domain?
What do values in parentheses mean in Tables 1 and 2?"	"1.The Abstract should address more novelties of the proposed model and detailed results for attracting the readers.
2.With respect to the transform space in T3PO, there are seven appearance (i.e., Identity, Brightness, Contrast, Saturation, Hue, Gamma, Sharpness) generated by appearance transform. How does this different appearance affect the performance of T3PO? Does generating more appearances improve the performance of T3PO? 
3.Reference 3 citation format should delete ""conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence""."	"I would have liked to see a more detailed analysis of the classification outputs for the ""known"" image regions, especially in comparison with the state of the art. The reported metrics are a good aggregate value of performance, but in detailed images such as the histopathology ones, it's usually the rare classes / limited pixels that contribute the most in semantic understanding."
501	Test-time Adaptation with Calibration of Medical Image Classification Nets for Label Distribution Shift	"Please highlight the assumption that all test data samples are accessible simultaneously.
It's assumed that, after converting multi-class classification into multiple binary classifications, the resulting binary classifications are conditionally independent. This is a strong assumption that need careful justification. Please elaborate on that.
The proof of Eq.(1) is not convincing, because the connections/assumptions between y_i and y_{-i} (given x) are completely missing. Will Eq (1) still hold? Under what assumptions?
The clarity on Eqs 1-3 should be significantly improved. For example, the main idea is not clearly stated; the terms related to ""expected"" or ""training"" can be confusing. Eqs (1) and (3) are not easy to understand without the supplementary materials."	It is suggested that the authors should provide the comparison between the tackled label distribution shift problem and long-tail problem. Moreover, the comparison between the tackled problem and test-time adaptation task should also be discussed.	"I find the paper valuable and well written, but I can't see the advantage of test time adaptation in a medical setting. Let me explain better with a simple example. COVID detection or severity like the example in the paper:
I would like to have a model that, given a single CT scan image, can detect if covid is present, the severity and the areas that lead to that decision without influencing how many other positive cases. I suppose that the goal is to train models that can extract features related to the pathology. Why my model will be influenced at test time if I trained it in a balanced manner?. Maybe you can explain better in the paper or in response to that review what is the advantage."
502	Test-Time Adaptation with Shape Moments for Image Segmentation	The paper uses shape descriptor based losses to perform TTA of a segmentation model that is trained only on the source domain. This is a good way to add shape priors to the model, while spending significant training time only on the source domain dataset. The benchmarks are good and show the improvements due to the method. One thing that is missing here is - How does the approach do when the DNN is adapted using the proposed loss function on the entire target domain data? This would show how much better the proposed method is compared to [2]. Why was this not done?	"We found the following mathematical inconsistencies:
The index variable n is used first at page 3 in the Method section to identify an image I_n, whereas it is only defined later (page 4) as the index variable identifying a slice of the target image, therefore in equation 2 it is unclear what the domain omega_n should be (which by itself is also never defined).
Equation page 3 bottom: u and v prime are not defined, are they the components of the centroid as defined at the top of page 4?
Table 1, Class-Ratio: what is the domain omega_T in this equation, should it be the lower-case t (target domain) defined a little later?
The weights ny_k in the formulation of the weighted shannon entropy should probably be after the summation symbol, not before?
Generally, the notation of scalars, vectors and matrices is not consistent. First, bold fonts are used, later i.e., for the mu after it is stated to be vectorized not anymore.
Result presentation:
Table 2+3: The Dice score is unitless. The ASD should be converted to mm which allows a much better comparability over different datasets. It is unclear why TTAS_RC and _RD are proposed methods but the TTAS_R under the heading of ablation study? The ablation with all three moments would be interesting as well?
Minor linguistic and structural inconsistencies are: 
[..] variations in image modalities ... without in 
[..] Standard DA methods, such as [18,17,5,22,18] ... duplicated reference and order 
[..] is unavailable during training  ... available during training 
Page 4, subsection Test-Time adaptation and inference ... it is described how the centroid and distance to centroid moments are estimated but at this point it is unclear how the class ratio is estimated.
Page 6, estimating the shape descriptors ... it would probably be much cleared to shortly describe how R prime is created instead of referencing to [1]
Other: 
I wonder how the results would change if a Dice score-based loss is added to the CE loss in the pre-training on the source domain and the oracle as it is standard in state-of-the-art image segmentation to use a combination of both."	"It is hard to understand the main idea of this work for the first time. The authors should use more detailed and organized descriptions to state what are shape moments and how to use them. By the way, I am not sure whether the citation of [14] (section 2) is correct or not. I did not find the explanation of ""Shape moments"" in [14]."
503	Test-time image-to-image translation ensembling improves out-of-distribution generalization in histopathology	"The author should compare with https://proceedings.neurips.cc/paper/2021/file/a8f12d9486cbcc2fe0cfc5352011ad35-Paper.pdf, and discuss their difference.
The authors should provide any compared metrics like forwarding counts or test time when compared with other top methods."	"I would suggest adding details of how mapping between different domains was learned with a simultaneous classification task. The inference pipeline is clear, however training is not.
Page 2 ""UDA methods needs unlabeled data from the target domain"". Do you mean availability of domain labels?
Please add reference in the last sentence of the last para of Introduction section.
Section 2.1: a random latent latent code -> redundant latent
Add details on training the model. Due to page limits, this could be added to the supplementary doc.
Give reference to Supplemenaty Figure 2 and Table 1 in the main manuscript.
Add details of baseline method. Also, it is not clear what ""Base + XYZ method"" (Table 1) really means.
What are those values in Table 1."	Execution time comparison, and performance comparison with (i) TTA + Stain Normalization; (ii) TTA + Stain Augmentation [ref. 21] are suggested (please see weakness for more details as I am not going to repeat.)
504	TGANet: Text-guided attention for improved polyp segmentation	"As shown in weaknesses part, the authors may need to add the ablation studies only using the label attention for further discussion.
Since channel and spatial attention introduce additional parameters and flops, the authors may need to report the influence on FPS when adding different components in Table 3.
The authors may need to show how to determine the label of size classification for the cases with one or more polyps."	"Fig. 1(C) looks very twisted. The layout and arrow lines can be better designed.

There should be a section number ""2.6"" before ""Joint loss optimization"""	"More comparisons with recent works. The proposed ""Feature enhancement module"" should be compared with recent attention modules, e.g., CBAM, Non-local.
What is the definition of ""small"", ""medium"", and ""large""? Please show the specific measurement for these different groups.
Showing the performance if removing the labeling attention module but just keep the ""Num polyps"" and ""Polyp size"" tasks. I'd like to see the improvement of the labeling attention module besides the improvement from the multitasks of ""Num polyps"" and ""Polyp size""."
505	The (de)biasing effect of GAN-based augmentation methods on skin lesion images	"Cross validation result need to be reported rather than considered testing on some fixed data.
More details are needed for unconditional and conditional GANs.
Results should be evaluated on other bigger dataset.
Discussion section need to be added as currently results section looks week from discussion point of view.
Please mention about parameter tuning inside GANs fully.
Compare the results of classification with other state of the art work on same dataset.
In Table 2, mention standard deviation along with mean value.
Grammatical errors need to be corrected throughout the manuscript."	"Overall, the authors seem to confuse artifact removal with bias removal, which are not necessarily the same thing. The authors appear to use the term ""debiasing"" to refer to the removal of particular artifacts such as short hairs, but this isn't really debiasing the data. One would assume that debiasing would somehow be equilibrating the frequency of certain features causally known to be separate from the task at hand between the two classes."	It would be interesting to provide some significance metrics when comparing different parameters. Also, the title can be misleading as it seems generic. It would have been better to mention that it is about bias on skin lesion images.
506	The Dice loss in the context of missing or empty labels: introducing Ph and 	"The article talks about the dice loss in the context of missing and empty labels. However, the introduction doesn't explain the need behind it and the relevant use cases for each one of them.
The introduction talks about general papers about the dice loss, but mentions only a handful of works about the topic of missing or empty labels. The relevant papers are cited much later in the methods section. This makes it difficult to follow through the article.
In the introduction the reduction dimension ""phi"" is mentioned without any background or formula to understand what it is referring to. It becomes clearer only later.
DL_CI and DL_BCI are mentioned in section 2.1 but are not used or compared to in the experiments. Their function is therefore not clear."	"(1) The writing of this paper should be improved to make it easier to understand. In particular, it would be better to additionally use plain language to explain the insights and intuition.
(2) Additional experimental results are required. For example, it would be helpful to provide the results by other $\epsilon$ values. Also, it is necessary to provide results of marginal Dice loss and leaf Dice loss."	"As mentioned above, I would recommend experiments with real empty labels, to show that it reduces false positives in cases with only background, e.g. patches that do not contain a tumor, cases with and without a lesion etc.
Other segmentation losses could be briefly mentioned in the introduction
The main difference between missing and empty labels and their implications should be discussed. False positives to take into account in the case of empty labels? It comes later in 2.2 and 3, but it would be good to explicitly mention it in the introduction.
In 2. I would remove ""most general case"", as it is in the case of segmentation.
The intermediate part of eq. (1) could be removed."
507	The Intrinsic Manifolds of Radiological Images and their Role in Deep Learning	As I have mentioned before, I am happy with the paper as is, but it lacks the openly available code.	"Show some evidence about the ID estimation is correct/ exact. Rule out all other potential possibilities to affect the relation between test accuracy and intrinsic dimension and domain to finally make a strong and clear statement.

Fig. 4 right is pretty, but the 3D version is hard to view which color is above another. Maybe reduce the \alpha or show the lapping information in a better way."	
508	The Semi-constrained Network-Based Statistic (scNBS): integrating local and global information for brain network inference	"My main constructive comment is about the experiment's section. The Authors should at least provide more ground to the choice of presenting only ""synthetic"" experiments and should at least provide perspectives for future experiments.
My second main constructive comment is about discussing the limitations of the proposed method. The Authors should spend more effort on this side and characterize within which limits the method is expected to perform and underperform."	It would be nice to make the picture a little bigger and add performance comparison with the other inference methods to the result part.	"In this paper the authors present a novel method (scNBS) to statistically compare the functional connectivity matrices from different conditions/populations trying to increase the statistical power which is greatly reduced when conventional edge-based methods are used. The paper is interesting and well-written, I enjoyed reading it, the figures are appropriate to convey the message and help the readers to follow the pipeline. I only have some minor points and suggestions:

I would try to give more details and revise the section related to the generation of the synthetic data as at the moment it lacks a bit of clearness;
Would it be possible to adapt this method to structural connectivity matrices? This would make the method even more applicable and general and I would mention this in the discussion;
Please carefully proofread the manuscript as there are several typos across the different sections;
I would suggest the authors to make their code freely available upon acceptance of the manuscript, as this would allow other interested researchers to explore and use this novel method."
509	Thoracic Lymph Node Segmentation in CT imaging via Lymph Node Station Stratification and Size Encoding	The proposed approach improves the LN segmentation performance for the corrected test data of actual esophageal cancer patients.  The effectiveness of the original idea is shown in the experimental evaluations.  It is better that some failure examples are shown with the reasons as far as the segmentation performance is not perfect.	For the CT scans, the slice thickness may have an effect.  It is unknown if these were contiguous slices or overlapping slices.	"To overcome the difficulties of segmenting visible lymph node (LN) from CT images, a novel LN-station-specific and size-aware LN segmentation framework is proposed, which can explicit utilize the LN-station priors and learn the LN size variance. Two-stage learning process is proposed, thoracic LN-stations are segment and then grouped into 3 super lymph node stations firstly. A multi-encoder deep network is designed to learn LN-station-specific LN features; secondly, to learn LN's size variance, two decoding branches are proposed to concentrate on learning the small and large LNs, respectively. Validated on the public NIH dataset and further tested on the external esophageal dataset, the proposed framework demonstrates high LN segmentation performance while preserving good generalizability.
This study provided a meaningful approach, but there are several weaknesses:

In a word, this paper is not well organized, and the writing is needed improved thoroughly. And the English of the manuscript should be significantly polished before further consideration for accept.
The architectural settings are rarely discussed. The reason of choice of nnunet blocks should be explained. And the structure of nnunet block should be described simply.
The images offered in the manuscript are low-quality. It is recommended to improve with high-quality images. Moreover, it is better to repaint some illustrations with unclear intentions. For example, such Fig 2, the contents are too small."
510	TINC: Temporally Informed Non-Contrastive Learning for Disease Progression Modeling in Retinal OCT Volumes	"Th presentation of VICReg is incomplete and lacking a clear motivation for the three losses. I had to read the original VICReg paper in order to understand this paper.  Specifically std(z, epsilon) and ""d"" are not defined.    I suggest to provide a short introduction for the three loss terms, for example: the first loss is for reducing the distance between related presentations (e.g. invariance to data transformations),  second loss is for increasing the variance of the different elements in the presentation to prevent norm collapse, and third loss is for de-correlating the different elements in the representation, to reduce redundancy.
In the original VICReg method, the batch of n patients is composed of n images, which undergo two separate transformation, t1 and t2.  The following sentence was unclear to me: ""Given a batch of n patients with multiple visits, let visits v1 and v2 be the n tuple of time points randomly sampled from each available patients' visit dates within a certain time interval.""   Does this mean that for each patient we randomly sample a pair of images from two different points? In other words, does tuple=pair, and each sample contains a single image?
In Table 1, the vanilla VICReg had ""representational collapse"". The authors mention that in medical images a larger area-ratio crop should be taken. It would be interesting to see the results of VICReg with larger area-ratio crop but without the two visits input. Was there still a representation collapse? I suggest the authors add this result, to demonstrate the contribution of two-visits input.
To assess whether the difference in the performance (AUROC, PRAUC) is statistically significant, I suggest to add p-values based on bootstrapping.
I did not understand this sentence ""This can be explained by the fact that in Barlow Twins, the fine-tuning reached the peak validation score within 10 epochs, same as the number of linear training epochs."" Did you try increasing the number of epochs, maybe the fine-tuning takes longer to converge to a better model?"	"This is an interesting paper. The authors propose a  modified loss function which they call TINC (Temporally Informed Non-Contrastive Loss) to be used with VICReg to predict whether an eye is going to convert to wet-AMD within 6 months time-frame
Comments are below: 
Include and discuss in a section to explain misclassifications. Are there images which were classified correctly by other methods and not by the proposed method. If yes, why? And also vice versa, why the proposed methods works better based on images as example."	"I would suggest a more comprehensive study, e.g., more datasets if possible, cross-validation on one dataset. Consider the time is limited during rebuttal. I will not say this is a mandatory requirement, but the authors may consider it.

Improve the writing to make the readers easier to follow. As the weakness 2, the readers may be confused about that. So other parts including: 
Is the evaluation metrics (AUROC, PRAUC) computed based on volumes level or eye-level? Without this information, it is not friendly to readers who is not a expert on OCT. 
2)"
511	TMSS: An End-to-End Transformer-based Multimodal Network for Segmentation and Survival Prediction	"Hausdorff distance can also be included to evaluate the secondary output of segmentation.
Ablation studies to show the effect of patch size.
Ablation to test the effect of using only PET-CT image (minus EHR).
Range for intensity normalization can be included i.e. was the whole CT HU window used to rescale to 0-1 or similar specifics.
Were any other EHR records identified (besides smoking and alcohol) to have missing data and was any imputation used for the same?"	See the wekness section for details	"Evaluation

As to my understanding, the authors have chosen to use a 5-fold cross-validation and have further optimized their parameters using the OPTUNA framework.
- In this combination, the authors might have partly done a circular analysis, namely they might have found parameters which perform well in this specific setting.
- I would recommend to instead initially split off a hold-out set, which then the authors conduct their final analysis on, as only this would rule out the opportunity of statistical dependence of the model training and the later testing phase, makes the results more reliable and thus creates a clearer incentive for others to use the work later on, which would be in the authors' as well as in the community's interest.

Further, I was not able to find any measure of confidence:
- As the authors have conducted a cross validation, depicting the variances across different folds would have created a first impression on the confidence of the achieved results.
- If the authors aim for a statistically more sound evaluation, I would recommend to also employ methods such as bootstrapping [1] and statistical testing, which would make it easier for the community to understand the value of the proposed method.

Minor

I would like to recommend to make the text in Fig. 1 a bit larger to facilitate reading.
The table formatting in Tab. 1 felt somewhat old-fashioned. I would like to recommend [2] for this.

References
[1] Efron, B. (1987). Better bootstrap confidence intervals. Journal of the American statistical Association, 82(397), 171-185.
[2] https://people.inf.ethz.ch/markusp/teaching/guides/guide-tables.pdf"
512	Toward Clinically Assisted Colorectal Polyp Recognition via Structured Cross-modal Representation Consistency	"Did you perform any image augmentation in order to increase the number of images and avoid possible overfitting?
Please clarify if the presented results on the Table 1 and Table 2 are on the same datasets or not.
I highly recommend you to add an explanation regarding the real-time clinical application of the proposed approach."	"The introduction is a bit verbose (avoid using the same sentences for both abstract and introduction)
The survey of the state of the art should be improved. Limits in the state of the art have not been clearly highlighted. Open challenges the authors want to tackle are not listed.
The listed contributions are not clear to me. The authors state that they introduce
a general framework of multi-modal learning for medical image analysis, but only one dataset is considered for the experiments. Point (3) is not a contribution.
The authors should try to clarify why the introduced contributions allow them to overcome the state of the art (without just presenting the results).
The clinical rationale behind the work is not clear. The authors write ""Despite the enhanced imaging, endoscopists rely on WL images before they change the light mode to detect the possible polyps, which means that the WL images may fail and lead to missing detection"". What does this sentence mean? If the clinicians have the possibility to exploit NBI, I don't see the point of developing algorithms that work with WL. The issue can be that some centers do not have NBI endoscopy."	Please provide more attention and vision transformer information in medical image domain, rather than general computer vision domain.
513	Towards Confident Detection of Prostate Cancer using High Resolution Micro-ultrasound	"(1)	This paper proposes a micro-ultrasound PCa detection learning model that is robust to weak labels and OOD samples. The weak labels and OOD samples are common problems in medical classification tasks. Therefore, it is better to provide the comparison with the DNN model that are proposed for solving the problems of weak labels and OOD samples.
(2)	The used clinical evaluation metrics are important parts of this manuscript. Please add some references.
(3)	About the accuracy and calibration error, please add the standard deviation.
(4)	In section 3.3, the heatmap can provide good biopsy targets, which is critical for the adoption of precision biopsy targeting using TRUS. This found is important, but maybe it is better to add the comparison of heatmaps by using different models."	"This reviewer would encourage the authors to minimize the use of acronyms, which would enhance the overall readability of the paper.
Figure 2(c), comparing to its (a) and (b) counterparts, lacks a legend."	The paper is very clearly written. The motivation for adopting their framework appears well informed by the clinical problem and data available for training. I eagerly look forward to performance comparisons with standard of care TRUS and mpMRI-fusion biopsies.
514	Towards Holistic Surgical Scene Understanding	The paper is very strong, both on PSI-AVA and TAPIR aspects, in combination even stronger. Needless to say, 8 radical prostatectomies is not much data, and it is arguable whether this covers a large range of variability, especially on the highest annotation level (surgical worflow deviations, anatomical anomalies causing backup/recovery workflow steps etc.). Annotation effort on PSI-AVA level must be enormous - nonetheless, if any way possible, it would be great to further increase this number, e.g. towards a journal extension, or maybe make this a yearly growing challenge, similar to how BRATS dataset size has increased over the years.	"TimeSformer and Swin transformers are two of the state-of-the-art models recently introduced in the computer vision community for video analysis tasks. For completeness, authors need to compare TAPIR with such methods to show their method is actually the state of the art.
It would be interesting to see how TAPIR performs on other publicly available datasets like Cholec80."	"The structure of the transformer is not shown in Figure 2. This is not conducive to the reproduction of the model.
All tables have no bottom border. It's not pretty.
The authors used too few methods for comparison to well verify the advanced performance of the proposed method."
515	Towards performant and reliable undersampled MR reconstruction via diffusion model sampling	"Please add the std statistics to table 1.
If possible, would be nice to compare with the work or at least cite: compressed sensing mri with deep generative priors. They have their code published."	"The detailed pretraining details and motivation of generating 2 consecutive slices should be provided.
Since the current method is bulit on U-net, the authors can discuss if designing more advanced backbone via NAS [1] [2] can help to improve the inference efficiency and performance.
[1] Huang Q, Xian Y, Wu P, et al. Enhanced mri reconstruction network using neural architecture search[C]//International Workshop on Machine Learning in Medical Imaging. Springer, Cham, 2020: 634-643.
[2] Yan J, Chen S, Zhang Y, et al. Neural Architecture Search for compressed sensing Magnetic Resonance image reconstruction[J]. Computerized Medical Imaging and Graphics, 2020, 85: 101784."	"The detail of the refinement step in the coarse-to-fine module could be more elaborated: Is $epsilon_theta$ the same as the one used in the k-space guidance module and the coarse sampling step? How does $x_obs$ involve in the conditioning? How to choose $T_refine$?
The author should provide the objective function of training the neural network of $epsilon_theta$.
The author should elaborate on 1) how many samples are used to train the network $epsilon_theta$ and 2) how much time it takes to train the network."
516	Towards Unsupervised Ultrasound Video Clinical Quality Assessment with Multi-Modality Data	"It is not clear what makes the three types of images to be of low quality. Is it because they are the wrong planes for making the measurements?
It is not clear how you partitioned the images in the training and testing datasets, i.e., did images from a given subject appear in both the training and testing dataset?
Figure 2: Can you add meaning of the labels of the columns to the caption.
Figure 2: It is not clear in this figure what makes the images to have high-quality and low-quality. Perhaps a short description in the caption would help. You described the difference between the TVP and TCP, but that is not obvious. Perhaps arrows on the image to relate to the description.
Was the study performed with Institutional Ethical Review Board approval?
Since the labeling of the images is dependent on the frozen frame, who did that?
You had a total of 611 video clips. Was each clip from a different subject, i.e., did you image 611 fetuses?
Tables and figure 3: The differences between the reported performances is clear, but it is not clear if they are statistically significant different."	"In section 2, the authors state: ""The definition of quality assessment in ultrasound is different in that it needs to factor in clinical context"". This is not entirely true, as quality assessment for ultrasound content may also focus on image clarity and definition, for example, to provide insight in equipment/technology development. Consider rewriting this sentence to accommodate this.

The final sentence in section 3 (before 3.1) and the one immediately before the ""Spatial zoom-in module"" subsection could be written just once. They are repeating the same idea. Also, in section 4, the second and third sentences could be condensed into one.

The parameters of the Farneback algorithm and the median filter should be described. The same applies to the fully connected layers in D_v

In section 4, the loss weights should be defined as w_adv, w_rec, and w_gaze, following the notation in Eq. (1).

How exactly was the image-based approach implemented? Was this done with the model proposed by the authors? If that is case, it is not clear how a single image input would be processed. This should be further explained.

- The paper needs to be thoroughly proofread. Some minor writing issues include:

Abstract, line 8: end sentence after ""temporal information"". Begin new sentence with ""The model...""
Abstract, line 9: ""anatomy-specific annotations, which makes...""
Introduction, line 2: ""free radiation"" -> ""acquisition process, which does not use radiation"".
Introduction, line 9: add comma after ""labour-intensive""
Page 2, line 1: ""spaces""
Page 2, line 3: add comma after ""error""
Section 2, line 2: add comma after ""proposed""
Section 2, line 15: ""considers""
Section 2, line 16: add comma after ""gain)""
Section 2, line 22: ""checks (if) images""
Section 2, line 23: add comma after ""[16]""
Page 2, last line: ""limit"" -> ""limits""
Consider renaming section 3 as ""Methods"" or ""Methodology""
Subsection ""Spatial zoom-in module"", line 5: ""on (the) overall""
Subsection ""Bi-directional reconstruction"", last sentence: ""The structure of the discriminator DV is similar to that of encoder""
Page 6, line 8: ""exemplar"" -> ""example""
Subsection ""Quantitative results"", line 5: add comma before ""thus""
Avoid repetitions such as ""clinical quality for clinical tasks""
Page 6, last two lines: add comma after ""single-modality video reconstruction""
Subsection ""Ablation study"", line 4: ""achieved by (the) inclusion""
Subsection ""Gaze prediction"", line 2: remove ""can""
Subsection ""Gaze prediction"", line 7: ""approximate"" -> ""approximately"""	"There are two major issues with the current submission as follows:
1) The size of the dataset is very small. While the details of dataset and imaging settings have not been mentioned, I think only 300 video clips for training a 3D model with several components is quite insufficient. 
Please explain the detail of your stopping criterion because your method includes adversarial training.
In Fig. 1 (b), it is also unclear how the label is created to calculate quantitative indexes.
Please also explain whether cross-validation and data augmentation is used in the training step or not.
2) The clinical application of the proposed method seems to be minor because the method is computationally expensive. I would think that the clinicians can easily look at the images to see whether TVP exists or not. It is not something hard to distinguish or user-dependent or even the user does not need to be very experienced. I would think the task is not challenging enough."
517	Tracking by weakly-supervised learning and graph optimization for whole-embryo C. elegans lineages	"Besides the comments given in the box #5, there are other points that deserve additional attention:

It seems that the proposed method is not listed on the Cell Tracking Challenge website. It is therefore questionable whether it really ranked first at the time of paper submission, especially when its predecessor (i.e., Linajea or JAN-US in the Cell Tracking Challenge terminology) achieved practically the same scores as those reported in Section 3 (2nd paragraph).
It is unclear whether the cell state classifier can deal with temporal gaps in cell detections, and thus correct imperfections of the cell detector employed.
The description of the evaluated performance measures is unclear, which makes the numbers listed in Tables 1 and 2 difficult to interpret. For example, the last reported value of FPdiv in Table 2 (i.e., 0.046) does not give any integer number after being multiplied by 18 (i.e., by the number of evaluated configurations). The formal definitions of individual measures need to be provided. Furthermore, please clarify where the 16% reduction of manual curation comes from (Section 3, 3rd paragraph) when comparing Elephant and the proposed method.
There is no information about the annotation protocol followed for the nih-ls dataset. Furthermore, it is unclear whether multiple manual curations of the Starrynite results for the mskcc-confocal dataset were prepared and subsequently fused to reduce the subjectivity and error-proneness of the final reference annotations.
The numbers listed in Tables 1 and 2 are mostly similar across the evaluated approaches. Are the differences statistically or practically significant?
As the proposed method shall be used a baseline for the nih-ls dataset, it is unfortunate that the cell state classifier did not account for apoptotic celss. How much would the performance of the proposed method improve when training the ResNet18 backbone for that class too?

Other remarks:

Table 2: There is a typo (cls -> csc) in the table caption.
To this reviewer's understanding of the Cell Tracking Challenge format, two test sequences per dataset can be downloaded from the Cell Tracking Challenge website (i.e., the test data is public).
The FP decrease from 3.7 to 2.5 in Table 2 does not seem to be dramatic. Please tone this claim down.
Check the spelling of Linajea across the paper."	"The main result of this work for me is that the proposed method achieved top-raking performance on the Cell Tracking Challenge data. Please, by the way, specify this explicitly in the corresponding paragraph of the Results section. StarryNite, while being often used for benchmarking, is known to be rather sensitive to the input data; meaning that its performance on data that differ from their own data tends to be significantly worse.
It is interesting to notice that the modified method and the original one seem to prioritize different validation measures. The same holds for ablation study presented in Table 2. This is something that needs to be discussed by the authors.
Judging from the two established measures of lineage tracing (DET and TRA), the improvement provided by the modified method is very marginal. I do realize that it is impossible to assess its statistical significance due to low number of data samples. But this observation does somewhat undermine the value of this work. An alternative way to show the worth of the improved performance would be to demonstrate its positive impact on the downstream analysis; which is missing in this manuscript.
""In this regard, our improvement in TRA over Elephant should mean that our method entails a 16% reduction in manual curation effort as compared to Elephant."" I find it difficult to understand where the 16% are coming from; taking into account that the corresponding tracking scores are 0.979 and 0.975.
Please explain the meaning behind the underscored entry in Table 2.
""By adapting the cost function D one should be able to modulate this depending on respective application-specific needs."" I find this sentence somewhat speculative. Otherwise the authors have to explain how this can be achieved.
Please report the value of the l parameter for completeness."	See list of weaknesses.
518	TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers	Not much more to add.  I like the idea and I can see it being useful in some applications. As I note under weaknesses, the most obvious way to feed a whole-brain tractogram into CNNs etc would be to create a tract-density image (streamline count at each voxel); an experiment with this as a baseline would highlight the benefits of the proposed approach better.	"Authors may explore the embedding in higher dimensions. The proposed workflow should be easily extended to that. It is interesting to see how emending affect the performance and will be a good ablation study.
Comparison between CAM and ViT's attention seems necessary to demonstrate the interpretability of TractoFormer.
Since the proposed method is able to obtain 2d grid representation, it is interesting to see using other kinds of conventional data augmentation techniques in computer vison."	Q5
519	TransEM: Residual Swin-Transformer based regularized PET image reconstruction	"How is the variance-bias analysis?
Fig2. looks like the bias is really high from the ground truth
Fig4. why does DeepPET show a different slice?
Typos:
Page 4: ""and The LayerNorm"", ""The Whole process"""	"1) The authors should present a concise explanation about Patch Embedding layer.
2) The authors should state the choice of the hyper-parameters more clearly, such as the choice of patch size M."	"The introduction structure is good, but be sure to make more link between the different methods you talk about. You still can refine a bit the state of the art / the explanation of each pros and cons for each category to be more accurate :

""represented by filtered back-projection(FBP [2]) and maximum-likelihood expectation maximization(ML-EM [3])"". Why ""represented"" by these 2 methods ? What does it mean ?
""solve this problem well"": what problem ? I think you meant the modeling of physical properties, but it is not clear whether it is that or the noise problem.
Examples on post processing: talk about the more common one : gaussian filter.
I don't know this paper : ""Machine learning in PET: from photon detection to quantitative image reconstruction"". You may only cite [8] from Reader et al., which is a wide overview of deep learning in PET reconstruction.
Gong et al. [10] is not a unrolled network from what I know, maybe I am wrong.

Maybe you should skip the part with historical methods and focus more on the DIP learning methods, or be sure to be very clear and accurate.
The equation in the method part requires slight modifications to be very clear :
I was a bit confused about equations (4) and (5). I did not know the FBS algorithm, but when reading it for the first time, I understood that theoretically, it was equivalent to the optimization problem (2) because you say ""used to split the objective function"". I did not understand directly that it corresponds to the equations of an iterative algorithm.
""where the goal is to perform the pixel to pixel fusion"" : you should replace it by something like ""can be viewed as a pixel to pixel fusion.
""The hyper-parameter a was learned from training data"". Did you mean from the validation set ? Otherwise I do not understand how you can learn it.
The fact that the number of unrolled blocks is hand-crafted should be said at the same time the other hyperparameters are learned from training data.
Ambiguities which need to be rephrased :

Prior is ambiguous : you can talk about prior information as additional or anatomical information, used in kernel methods. But prior in the Bayesian point of view can be used for maximum a posteriori, which is a category of Penalized Log-Likelihood (PLL) methods. Please do not merge these 2 different methods as one called ""prior-incorporative methods"". Anatomical information enable to improve the image quality by adding information. Penalized Log-Likelihood (PLL) methods are used to decrease the noise, directly in the optimization process.
What do you mean by ""ablation study"" ? Removing RC ? ""ablation"" is a medical term which should not be used in your case.

Number of counts in your table is smaller than 1.
Figures 2 and 4 show normalized images, which does not enable the reader to do a fair comparison between the different methods. You should show them with the same contrast."
520	Transformer based feature fusion for left ventricle segmentation in 4D flow MRI	Please clarify whether the LV was annotated for all the 30 frames, or only for a subset?	The authors have well proved the superiority of the proposed model. However, I would suggest adding the results on a single modality (2D MRI without velocities) with U-Net. These experiments can prove the effectiveness of 4D flow MRI against conventional CMR.	"The figures in the paper is not clear enough, the font in all figures is inconsistent with the text, and there is some ambiguity. It is recommended to revise all figures carefully.
""dividing feature maps into patches leads to loss of spatial information"" should be given detailed evidence to prove this point through ablation experiments.
The paper mentions ""a learnable positional encoding sequence"", however the detailed descriptions are not given.
The typesetting is not neat enough. It is recommended to use ""latex"" for typesetting.
what the meaning of ""in 91 182 annotated pairs""? is it a writing error because of the large space in one number? Please check it.
The sentance ""KE was normalized to EDV as recommended by other researchers"" is too casual and not rigorous, please explicate this point by citation who recommend or which paper inspired you to do like this.
""we did not employ any data augmentation methods to enlarge the dataset"", in future I suggest you to conduct some data augmentation like nnU-net to improve the performance of the proposed method.
""All of those Pvalues are larger than 0.05, which confirmed that there is no significant different..."" is different from the ""Pvalue was computed using Wilcoxon-signed-rank test. P<0.05 indicate a significant difference between two variables"". Please check which one is wrong."
521	Transformer based multiple instance learning for weakly supervised histopathology image segmentation	I suggest adding the discussion about chosen hyperparameters, discussion of the weakness of the approach, and possible failure cases.	"It would be better if the authors further explain how Swin Transformer consider relations between instances. E.g. in Method, the statement ""Similar features get high attention weights while dissimilar ones get low attention weights, which leads to an improvement in distinguishing foreground and background"" need further explanation.
The image size and patch size needs further to be clarified. I wonder whether the H&E refers to original size(3000) or downsampled size(256). Also, is it necessary to downsample the images from 3000 to 256 since only 4*4 patches are fed into Swin Transformer.
Some details of the method need further explanation or correction. I wonder how multi-scale features are fused through fuse layer. In Fig.1, the ""Swin Transformer Block"" is actually two successive Swin Transformer blocks. Also, the Decoder is mixed with the ""structure of decoder"" so the clarity can be improved.
The paper uses F1 and HD as metrics. But there are many other popular metrics such as dice coefficient for semantic segmentation and it would be better if these metrics are adopted."	"1.In page 3, the author defined Y_n \in {0,1}, and in the last line of page 4, the author defined, where the author confused this confusion between Y_n with \hat(Y_n).
2.The author did not specify whether as the probability of the pixel  in the th image being positive or negative.
3.The full name of CAM was not given when it first appeared in the paper."
522	Transformer Based Multi-task Deep Learning with Intravoxel Incoherent Motion Model Fitting for Microvascular Invasion Prediction of Hepatocellular Carcinoma	If possible, I strongly suggest to increase the samples to test the proposed method with a proper dataset. Although, cross validation is really useful for this small dataset (together with removing of outliers or selection of relevant features) I am a bit concern about possible overfitting or other problems related with this kind of very reduded dataset.	"Some points in the method are not really clear:

The author claim that Compact Convolutional Transformer (CCT) enable the transformer to work with small dataset of medical images but this is not proven in any experiment.
The IVIM Model Parameter Fitting Task not clear:
How is it self-supervised?
The Ground Truth assessment is also not clear
What would be the results if the method from [21] will not be modified to use ResNet-18?
The training, evaluation, testing data split are not explained.

No results are shown in the paper.
The results shown in the supplementary materials should appear in the paper. I know that space is limited but table 1 and 2 could be merged into 1 single table for example.
Minor points:

Table 1 and 2: best results should be highlighted for better clarity.
ref 22: typo: gd-eob-dtpa-enhanced??
[19,21] are 2 studies so in intro:
a recent study -> recent studies
page 4: both two task-specific embeddings is passed to the task shared transformer block
-> both task-specific embeddings are passed to the task shared transformer block
What is ViT?
Parameter is Eq 1 are not introduced.
p7 spacial => spatial"	"This paper aimed to perform simultaneously IVIM parameter model fitting and MVI prediction using a multi-task learning method based on transformer. The originality of the work is to combine the advantages of convolution network and transformer to jointly achieve IVIM model parameter fitting and MVI prediction, which allowed the authors to obtain better results than those obtained when performing one single task (fitting or prediction).
However, methodological motivation of the work was not appropriately justified. The authors motivated ""the parameter fitting of the IVIM model based on the typical nonlinear least squares method has a large amount of computation, and its accuracy is disturbed by noise."" But, the paper didn't deal with these two aspects. On the other hand, the authors claimed that their method performs better IVIM parameter fitting; but throughout the paper, the authors have never shown IVIM parameter maps, which is an obscure point.
A few typos::

""embeddings is passed"": are.
Fig. 1, ""Average polling"": pooling."
523	Transformer Based Multi-View Network for Mammographic Image Classification	"1.The English expression of the article needs to be improved.
2.The abstract should state the novelty of our method and why it is useful.
3.The article needs to strengthen the logical ordering, and clearly describe the algorithm process and working principle.
4.The article needs to add relevant experiments to prove the superiority of this method."	In general, the idea is interesting. However, more detailed descriptions should be provided for the shifted window based cross view attention block. More ablation studies should be provided, such as (1) the experimental results for Swin-T (feature concatenation) should be provided; (2) shifted cross-view attention vs cross-view attention.	Adding experiments on other tasks can further strengthen the results.
524	Transformer Lesion Tracker	"It seems the largest gain is achieved from SSS, but it seems the motivation for SSS is reducing computation cost. Further discussion why SSS improve performance is appreciated.

It is not clear why affine transformation is used. For lesion deformation, it is more like a non-rigid registration task. Adding reasoning on why affine is used is appreciated.

Add some discussion about dense tracking/object tracking is appreciated."	"please don't introduce abbreviations in the abstract
The Learn2Reg image registration challenge paper is a good reference for several registration methods!
Questions:
Why are the images resampled to 2mm for the deeds algorithm? It can also handle larger images and
maybe the registration quality is better with a 1mm resolution."	
525	Transforming the Interactive Segmentation for Medical Imaging	"There are several points which should be more clear and consistent.
(1) The descriptions of click encoding and label assignment are not specific enough:
The authors state ""$\phi_{index}$ refers to an indexing function that simply extract the features from the dense feature map"", but it is unclear what feature extraction means here.
Also, in the statement ""$\phi_{CE}$ refers to a projection from the category labels to high-dimensional embeddings"" it is unclear how the projection between the category label and the high-dimensional embeddings works.
(2) It is confusing in which steps the user interaction (clicks) is used.
On the one hand, in Sec. 2.3 it reads ""After stacking 6 layers of click encoding and label assignment modules, we adopt a linear layer to read out the segmentation labels, and train it with pixelwise cross-entropy loss."", so it seems that clicks are involved during training.
But on the other hand, according to Sec. 2.1 (""allow end users to refine its own output by incorporating feedback during inference""), the clicks are only used during inference, but not during training.
(3) In Fig. 2, subfigure D1 (Lung Cancer), D3-1 (Pancreas), and D3-2 (Pancreas Cancer), there are several curves which almost do not change with increasing number of clicks. In subfigure D2 (Colon Cancer), the green curve even drops when there are more clicks. This is strange since all compared methods are interactive methods, so their results are supposed to improve if more clicks are used. Is there an explanation for this phenomenon?"	"The paper has a good organization and the method is explained with enough detail. 
Now the next step is to use real data that the system can test. And also a usability evaluation with final users."	"This paper proposes TIS (transformers interactive segmentation), a new network architecture to allow for the refinement of segmentation inferences from a (multi-class) segmentation network (\Phi_ENC
\Theta_e) using clicks (i.e. xy positions and labels) encoded through a click encoding network (\Phi_REF
\Theta_r). In these experiments the segmenter is a U-Net but can in principle be any type of encoder decoder. The click encoder (the main contribution) is transformers based and takes as inputs both the click coordinates and the (vectorized) encoder output. This step is followed by  a ""label assignment"" mechanism whose purpose is to learn to balance the contribution of the clicks wrt similarity to ground truth labels.

Validation is performed by simulating automatic clicks in regions where the encoder failed based on the ground truth, and evaluating the improvement with increasing numbers of clicks. The authors provide a comparison to many other interactive segmentation methods (which I am not familiar with) and demonstrate significant and consistent improvement over SOTA.
The paper is well written, the proposed architecture is is quite innovative (although I am no expert in interactive segmentation), and results are very convincing, with consistent improvement improved by both the click encoding mechanism as well as the label assignment.
I  am missing a technical aspect. I fail to understand why the label assignment brings such an addition to the click encoding only, and why click encoding only  perform so poorly. As I understand the label assignment includes the click encoding step. However based on the results illustrated in table 2, label assignment leads to a +8% dice in cancer segmentation. Results using click encoding do not seem to benefit at all of increased number of clicks. (-1% Dice between 5 and 10 clicks), while impressive results are achieved using
I however strongly recommend to accept this very interesting paper."
526	TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation with Transformers	"In last para of Introduction, I would suggest to link the terms with the figure. For example, the terms such DIFA and MSA are mentioned with no background on where these terms come from (though they are later described in the overall method). 
The authors should also highlight the limitations of the approach with examples where the proposed method fails to work. Similarly, information on required training resources and how these compare with other methods should be included."	The proposed method looks interesting and the validation is also proper. Kindly, include the computation details of the proposed methods as well as the methods used for comparision including the hardware details.	The proposed attention modules seem to be much heavier than the baselines. The model computations and time consuming may be considered for comparison experiments.
527	TranSQ: Transformer-based Semantic Query for Medical Report Generation	"How long does the model take to train on the dataset?
Were there instances when the model did not perform adequately? What were the main reasons for failure and your best guess as to why?
Bipartite set matching with a vision transformer runs into issues when detecting small objects (see DETR Carion 2020). Does your model have similar issues as well? What is the smallest finding that it did not do well on?"	It could be interesting to explore the stability under small jittering of the image which would cause the VIT to differently tokenize it.	"Strengths:

The paper is well-written and easy to follow.
The proposed approach is interesting and novel.
The experiments on two benchmark datasets show that the proposed approach can achieve state-of-the-art performances.

Weaknesses:

Some important implementation details are missing.
Although the authors report a very good performance, it is not clear to me which part of their method is responsible for it. In particular, the proposed model incorporates the powerful Vision Transformer (ViT), which is pre-trained on large-scale datasets, and there is no experiment to show how much improvement is brought by the existing ViT and how much improvement is brought by your proposed approach.
So I wonder if previous works adopt the ViT as the image feature extractor, can they achieve better performances than your approach?

The novelty of the idea is limited.
Although the authors claim that ""they make the first attempt to address the medical report generation in a candidate set prediction and selection manner"", in my opinion, this idea is very similar to the existing retrieval module in medical report generation [1]. What are the main differences between this work and previous retrieval modules?

Some implementation details are missing.
How to initialize the semantic queries, and how to ensure that these K queries have different latent topic definitions? Did you visualize them to prove it?
How to conduct the retrieval process in the proposed approach?
How to obtain/construct the database used for retrieval?

I recommend the authors add a related work section to better discuss the difference between this work and previous works. Meanwhile, it can help the readers better understand the contributions of this work.

Although some newest methods, e.g., [2][3], perform worse than this paper, I still recommend the authors quote them in the Tables.

typos:

'To solve the problem, We consider' -> 'To solve the problem, we consider';

[1] Hybrid retrieval-generation reinforced agent for medical image report generation. In NeurIPS, 2018.
[2] Competence-based Multimodal Curriculum Learning for Medical Report Generation. In ACL, 2021.
[3] Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation. In ACL, 2021."
528	Trichomonas Vaginalis Segmentation in Microscope Images	"In Table 1, is there any specific policy to set the definition of the small object, i.e., ratio < 0.1

There is a related work (see below) also focusing on the Trichomonas vaginalis analysis using deep learning methods, please include the discussions and comparisons with it:

X. Wang, et al, ""Trichomonas vaginalis Detection Using Two Convolutional Neural Networks with Encoder-Decoder Architecture"", Applied Sciences, 11(6), 2738, 2021."	"An effort should be made with regards to reproducibility and evaluation. More specifically, the authors should provide a better description of the range of hyper-parameters considered for every method (including those of the state of the art), the number of training and evaluation runs, validation split and validation results, etc. In that sense, I recommend to follow the code of good practices proposed by Dodge et al. (""Show your work: Improved reporting of experimental results"", 2019).
In page 2 there is a typo: ""deep learning techniques has not yet been well-studied"" should read ""deep learning techniques have not yet been well-studied"""	"Major issues:
In the description of the segmentation method (Sec. 3), details about NCD are missing. There should be a high-level summary about how NCD [4] works. A comparison with the partial decoder component [31] should be made in the introduction, not here.
The overall presentation of the method should also be improved. For example, in Sec. 3.1 and Fig. 2 it is unclear what $P_1, ... P_6$ are, since they are only mentioned later in Sec. 3.3. In Fig. 2 the arrows for $P_3$ to $P_6$ are also confusing since it is unclear where they originate, and where the arrows for $P_1$ and $P_2$ are (or, whether $P_1$ and $P_2$ exist).
Minor issues:
Sec. 3.3: Why does the weak foreground region $F^2_{i+1}$ contain boundary information? Does ""weak"" mean that the foreground feature is not strong enough? If so, then the boundary information there might be inaccurate."
529	UASSR:Unsupervised Arbitrary Scale Super-resolution Reconstruction of Single Anisotropic 3D images via Disentangled Representation Learning?	"The paper is interesting and well presented. I will detail here the weaknesses to help authors understand what is not ok and why.
The dataset is not clear from the begining. At the end, ""507 MR images"" seem to be 507 complete exams of different knees, but in the beginning I though it was a single knee exam with 507 slices. The same for the spine.
The quantitative assessment should have at least a mean and sd so we understand that the data in table 1 is issued from a population of exams. Otherwise it looks like a single slice is being tested. If this is the case, the result is sample dependent and the contribution is very small.
Supposing that the experiment has a large population of exams and the data in table 1 is the mean, the differences in the comparison with SOTA methods are small. An analysis of variance would be necessary even to determine if the differences of the means are sgnificant.
The qualitative evaluation presented is limited to visualizing one sample image for each dataset and only in the authors' opinion. There should be an independent assessment by a population of experts.
Here, it would be also interesting to see how the method will perform on natively low resolution images or on increasing resolution of natively high resolution images.
Finally, even if just for curiosity, it would be interesting to see how the method performs on general photographs.
The fusion part was the most unclear for me. After reading the paper again, I could understand that the initial goal is to increase the resolution of the whole volume, but that should be made clearer form the beginning. While understand that new slices can be made from interpolation and an axial stack can be computed from a sagittal stack, I would not call that ""different views"". The term was misleading.
Moreover, interpolation and SR can be applied in different orders to obtain arguably different results. The pipeline in fig 2 indicates that the HR sagittal is built from the fusion of HR axial and HR coronal. To do so, LR axial and LR coronal are resampled from LR sagittal, then LR axial and LR coronal pass independently through the UASSR before being fused. In such a way, the original LR sagittal never passes through the UASSR. It seems that for lack of space the authors did not detail that part.
Finally, the paragraph ""ablation study"" does not make sense for me. I do not see ablation there, as fusion is not part of the method (at least I understood it as an extension). It is unclear in table 2 how the metrics are applied to ""with fusion"" and what ""arbitrary"" means in that context."	Please check 4 and 5 for details.	"The arbitrary scale is not clearly explained. If the proposed method can deal with three different scales in a single model, the resolution distribution would be Gaussian mixture model instead of a single Gaussian distribution. If the proposed method can deal one with one model, this is no difference with existing methods as existing methods can also retrain the model for different scales.
The investigation of resolution space is not enough. How this could interpret the resolution information?
Fig. 1 could be redraw to include more information about the training flow.
Resolution space is independent to the content. Can resolution space be transferred from MRI to CT?"
530	ULTRA: Uncertainty-aware Label Distribution Learning for Breast Tumor Cellularity Assessment	"This paper presents a novel and innovative method to learn tumor cellularity (TC) using distribution instead of deterministic label. The input undergoes multiple augmentations that each is fed to a separate network branch, from which the TC score distribution is predicted. The loss is evaluated by comparing the distance (Kullback-Leibler (KL) divergence) between distributions as well as the MSE between the distribution mean. 
The paper includes detailed description for the methodology. It also shows a comprehensive ablation studies and comparisons to other methods. 
I have some minor comments:

How does the standard deviation is reflected into the Target Distribution in Figure 2.
I mean you have a fully connected network to predict the mean of the TC distribution. Do you need to deploy another network to predict the standard deviation?
In Equation 5, what can be interpreted is that you have a fully connected network (MBFF) that predicts the pins of the distribution immediately. I feel that there should be some sort of conditional learning between pin in the MBFF? I get that you used KL divergence to constraints the network learning for this purpose, However I would like to check if you explored other methods at this stage, for instance adding vision attention layer or so perhaps?
In the ablation study (Table 1), I would recommend adding further augmentations more than 3 to address the performance of this system given this parameter."	"Authors can give more introduction about label distribution learning and emphasize the advantages of transferring a regression problem as a label distribution learning problem.
Authors should give more details on the generation of heatmaps shown in Fig. 3. If possible, please plot the ground truth heatmaps for comparison.
There should be more innovation in the method."	"The paper is interesting and well written. Yet the approach should be more justified to be completely convincing. The results are satisfactory but the rather weak statistical validation probably makes some comparison not statistically significant.
Uncertainty is considered by replacing labels by distributions but the sharpness of this distribution is set empirically (how?). More medical insight (e.g. prior estimation of inter-operator variability) would be very interesting. Using augmented version of input data to reproduce variability could work in principle but here the augmented transforms (horizontal, vertical flips and elastic transforms) are probably not the ones that would dramatically change an expert assessment. Perhaps more transforms (on contrast) could be considered or samples with high discordance TC analyzed.
Many hyper-parameters are set empirically as well as the procedure for predicting TC score from regression and label distribution branches.  These should have been set e.g. with a CV selection on the training set and then evaluated on the validations set."
531	Uncertainty Aware Sampling Framework of Weak-Label Learning for Histology Image Classification	Currenly. there are many publicly-available datasets with thousands on WSIs, such as TCGA-PRAD and PANDA. I would recommend the authors to increase the amount of WSIs used in other to evaluate their methodology in a most robust and realistic scenario.	"-Evaluate the proposed method with larger dataset and compared it with SOTA weakly supervised tile classification/WSI segmentation methods.
-The Resnet18 trained with noisy labels (WSI labels) for tile classification was used as the baseline. It can be regarded as the lower bound of the proposed method. Besides, the Resnet18 trained with tile-level labels, which can be regarded as the upper bound of the weakly supervised method, should also be evaluated and compared in Table 1. The experiments are not difficult to conduct, as for the slides seem to be fine annotated by pathologists."	"The ""1"" in the second paragraph, Section 1 seems to be a typo.
All figure axis labels are very small and can only be read with digital zoom. Try to increase the label size to at least 8 pt.

I assume that the prediction probability is referred to as $ max P(y*
x, D) $ but could not find this stated in the paper.

What do the values in Tab.1 represent? Mean +- std? Please state.
I would appreciate some sentences or a formula on the loss functions SCE and OR. This would make the paper more self-contained.
Beyond the scope of this paper: I wonder how well the uncertainties are calibrated, i.e., how well they correlate with the predictive error."
532	Uncertainty-aware Cascade Network for Ultrasound Image Segmentation with Ambiguous Boundary	"The authors apply the UAM structure before each encoder layer of the second U-Net. According to Eq. 2, the UAM requires uncertainty confidence maps, decoder features, and high-scale features. It is puzzling that the size of the uncertain confidence map is 1wh for each layer, which seems to be incorrect. How do you obtain the uncertain confidence map for non-top layers? Also, for the top layer, what are the high-scale features?

In Fig. 2, it is best to label the UAM in the U-Net and show more details of the RECM.

The operator symbols are difficult to understand. Does the ""x"" symbol in Eq. 2 reflect element-wise multiplication? Is the ""/"" symbol in Eq. 3 representing element-wise division? Does element-wise multiply M_conf and X in Eq. 3? And M_ca should be M_conf in Eq. 2.

The authors divide the network training into two phases. Can the whole network be trained jointly to achieve better performance?

The RECM and boundary loss actually do not contribute much to the improvement.

The first sample in Figure 3 on TN-SCUI shows that the segmentation result of the network differs significantly from GT, and even the borders appear curled.

For the results in Table 1&2&3, it is better to add the standard deviation.

The authors lack analysis of design ideas or experimental results."	"1) In Fig.1, how the uncertainty map obtained? Considering the proposed method rely highly on confidence map, it is better to list confidence map for better motivation introduction.
2) What is 'ultrasound feature' in Fig.2 and Sec.2.1?
3) Authors mentioned that the method improves segmentation by ""decreasing the weights of features causing the uncertain predictions"". Why ""decreasing""? Will ""increasing"" work? Considering ""increasing"" will make the model pay more attention to the hard regions. I'd like to hear about the discussion.
4) There are no qualitative results to show the effectiveness of proposed method in boundary issues, especially in recurrent edge correction. (Fig.3 failed in this issue.)
5) Considering AFM and UAM are time consuming components, the time efficiency analysis should be added.
6) Typos. e.g., ""duo"" in abstract."	"The writing is clear and well organized with clear motivation
The experiment is relatively comprehensive (3 datasets and compared with 3 relevant important MICCAI'21 works),  and does a good job on ablation study
The results show superior performance to well support the claims
The investigation of related work is sufficient
The code will be published also
There is no discussion of the failure cases, which may due to the length limit
There is no standard deviations for the results
The figure quality is not very good. Suggest to improve the image resolution.
The novelty of each module may be limited or incremental. However, I think it's ok to propose a framework with a good performance."
533	Uncertainty-Guided Lung Nodule Segmentation with Feature-Aware Attention	"Radical changes are not necessary. It is important to explicitly and throughly state why the method is better than a normal FCN (Eg. approaches used for comparison). Without going in too much detail about the dataset you have selected, you want to show that your approach yields better results. So, supposedly, you have a dataset with multiple annotations for each lung nodule. You propose experiments done on union, intersection and single GT annotations. When testing the performance on union you need to train the approaches used for comparison (which do not have the luxury of creating union masks as a byproduct of their execution) on unions. Same thing about intersections, etc. 
Revision of ablation studies in supplementary material is suggested."	Please refer to 5. for more information.	Please provide more details for the questions mentioned in limitations. The state-of-the-art methods are only conducted on a single annotation. Please also compare the state-of-the-art methods by adding the uncertainty - references [8-10] mentioned in this paper.
534	Undersampled MRI Reconstruction with Side Information-Guided Normalisation	As per weakness point.	"(1) For the OUCR backbone, the parameters of convolutional layers are shared in the recurrent design. Are the SIGN modules shared as well? 
(2) The author could mention a more general case of using SIGN. i.e., How to insert the SIGN modules in other backbone networks. Is it after the convolutional layer as well?"	"1: The image contrast (T1, T2 or the proton-density) is determined by the key imaging parameters of TR and TE. The question is if the scanning parameters are already encoded by the SIGN, the categorial variables related to the different contrasts are still necessary?
2: It seems the simpler network of D5C5 benefit much more from the SIGN module than the OUCR network. Does it mean that if the capacity of the reconstruction network is high enough, the network itself can model the heterogeneity in the dataset without explicitly inputting the side information?
3: For the baseline methods, even without the SIGN module, is the instance normalization kept? If not, the comparisons are not fair."
535	UNeXt: MLP-based Rapid Medical Image Segmentation Network	"Clarity

Writing and grammar are generally fine. Poor clarity is mainly a result of missing details or descriptions.
eq 4: is T actually T_W?
identify 'PE' as 'positional embedding' in table 2 caption
""We split the features to h different partitions and shift them by j locations according to the specified axis. This helps us create random windows introducing locality along an axis.""
What does this mean?
How do you determine j? is j different for each partition?
Don't you lose much of the info when shifting by a big j?
[looking at the code, it seems you roll the map instead of shifting it (and presumably zero-filling it then)]

""To tokenize, we first use a kernel size of 3 and change the number of channels to E, where E is the embedding dimension (number of tokens) which is a hyperparameter.""
What does this mean?
What is the full tokenization? is it just a single conv layer?
[looking at the code, it seems to be so - this must be clear in the paper]

What kind of skip connections do you use? This is not mentioned in the paper; concat requires many more parameters than sum [looks like it's sum in the code].

Positional encoding, shifting, and tokenization

[25] encodes position info with 3x3 conv from a 4x4 patch by relying on the zero-padding; there is no such padding nor any patches here so why would conv give a positional encoding?
How does shifting give a positional encoding?
Is shifting of feature maps just having a regularization effect?
It's not clear that shifting helps or how it helps. The performance improvement is marginal so shifting should be tested on more, larger public segmentation tasks. The benefit of regularization also decreases with training set size so a large training set would be useful.
The 'tokenization' method is never defined (the explanation is unclear and insufficient).
While words like 'transformers' and 'tokens' are in vogue, they seem ill-applied here where there are no image patches extracted; rather, performance improvements here seem to come from decoupling convolution from feature mixing: conv is depth-wise and feature channels are mixed by a fully connected layer.

Validation of results

How does performance vary across repetitions of model training?
How was statistical significance evaluated? This is not described.
There is no mention of or comparison to other fast segmentation methods: ENet, FSSNet, FastSCNN, Squeeze U-Net, C-Unet, etc, etc.
It's strange that all methods achieve almost the same performance on MoNuSeg and RITE - what about datasets where recent methods impr"	"Typo at end of section 2. ""...across the embedding dimension H... In our experiments, we set H to 768"". H is used for height. Earlier in the section it was stated that ""E"" is the embedding dimension."	"Typo in Page 5: ""more smoother"""
536	Uni4Eye: Unified 2D and 3D Self-supervised Pre-training via Masked Image Modeling Transformer for Ophthalmic Image Classification	"Short descriptions of the mmOphth-v1 ophthalmic dataset as well as the evaluation strategy (split into train, test, validation ..) should be provided in the main paper.
-Details concerning the backbone architectures of the encoder and decoder should be provided. As an example, the authors refers to Vit-large and ViT-base which is not clear for non expert readers.
-OCT and fundus images have very different texture patterns. It would be interesting to provide hypothesis on the ViT module adapts and benefits from such a different data types. 
-3D OCT images should be better described, eg OCT enface?? Illustrations are provided in the Appendix but no explanation regarding the difference between the different 2D and 3D image types. Would be nice to add some sentences on the differences and clinical practice regarding these different images."	"Major comments:

Table 1 of supplementary materials show some details on the collected and created dataset mmOpthth-v1. The table mostly show that mmOpthth-v1 is the concatenation of already existing public dataset (OCTA-500, GAMME, EyePACS, PRIME-FP20 and Synthesized FFA). Some further description / details on the contribution of authors to the collection of the data would be welcome in the main paper.
To further investigate the relevance of the self-pretraining and generated features, authors could show downstream task results while freezing the pretrained model and only training a classification head (e.g. linear layer, MLP) on top of it.
Authors decided not to describe (briefly) the ViT architecture, still two ViT sizes are used (-base and -large). A detail on the important architecture hyperparameters could be added for both sizes.
Authors propose to only feed the visible patches to the ViT encoder. Could this hamper the spatial information between the patches ? Is there any strategy to account for that ? Authors could explain / discuss further this point.
For 2D inputs, the proposed UPE module outputs 2D square patches, whereas for 3D inputs, UPE module outputs 3D cubic patches, how does the ViT handle both possibilities ?
As training batch sizes are different for 2D and 3D, authors could further explain their strategy to train over the whole 2D + 3D data sets. For example, maybe 1 training epoch = 1 epoch over the 2D data set + 1 epoch over the 3D data set ?
Reconstruction results are assessed only qualitatively on Fig. 4, it would be interesting to see quantitative results (e.g. MAE, RMSE, SSIM ...) on the whole dataset and independantly for the different modalities.

Minor comments:

Authors could explain more on the use of the gradient map with the Sobel filters for the self-supervision task. The gradient map with such filters appear as a rough segmentation of the image and as the collected public datasets seem to sometime also provide the segmentation maps, it could be used alternatively and authors could compare to their proposed approach.
authors state that the loss weights were set equals to ""make the network concentrate equally on global intensity information and local edge information"", further investigation could be performed on those hyperparameters."	"Fig.1 is a bit confusing. In the input, fundus, OCT en-face, and OCT, are there any relationships between them? or it's just showing the method can support one of the three inputs?
Fig. 2 and Sec 2.1 are also unclear to me. It seems that 2D and 3D branches are unrelated, thus it can basically considered as two models? Also, it seems that the blue and orange vectors are combined together to achieve f^d, but it seems to be contradict to the part that 2D and 3D are unrelated? More explanation is needed.
Since the method is based on masked auto-encoder (MAE), a direct comparison with MAE  seems to be a natural choice to show the contribution of the proposed components, but it's not compared. Any reason?"
537	Unified Embeddings of Structural and Functional Connectome via a Function-Constrained Structural Graph Variational Auto-Encoder	"This paper proposed a new idea in brain networks. However, in my opinion, there lacks of insightful thinking about the results and some minor comments need to be addressed.
-Figure 3 is not explained further in the article, what is the role of Figure 3?
-The classification methods ( SVM, MLP, and RFC)of AD need a detailed description. And why not compare with CNN for the AD classification tasks? In Table 1, what mean is the RFC?

Why not do an ablation experiment? How much does this node feature(fMRI) affect the downstream tasks?
How do the proposed methods and tuning strategies generalize to other datasets?"	"It would be necessary to discuss the necessity of including an AE model in the proposed framework.
Did author use separate training and testing dataset for classification task? Please add more details.
Lack the visualization of joint embeddings of structural and functional connectome, which makes it difficult to understand the specific correspondence between them."	"Major Concerns:
1). Methodological validation
The authors provide the validation based on two embedding techniques for AD classification in this work. In addition, the reviewers are interested in comparisons of other peer deep models. For instance, the authors can evaluate Deep Boltzmann Machine (DBM) to replace the AE in Fig. 1.
Here are some references for methodological validation:
Salakhutdinov, R., & Hinton, G. (2009, April). Deep Boltzmann machines. In Artificial intelligence and statistics (pp. 448-455). PMLR.
Furthermore, did the authors perform cross-validation for classification validation? Unfortunately, there have not been more details of classification validation included in this work.
Moreover, reviewers are curious about the comprehensive comparisons. In detail, can authors validate the proposed GVAE with other peer methods in terms of time-consuming and reconstruction accuracy? This validation would further benefit the clinical translational application in the future.
2). Sampling Technique Issues
In Fig.1, the authors described a vital computational pipeline. However, the authors do not discuss the sampling technique in detail. Which sampling technique do the authors utilize in this work? Can authors validate their sampling techniques with Gibb's sampling? Or some dimensionality reduction techniques can be thought of as an alternative way to replace the sampling techniques?
3). Why is orthogonality better?
In Section 2.5, the authors emphasized, 'Note that, unlike PCA, a single layer AE with no-nonlinearity does not impose orthogonality, allowing for better compression. Originally, orthogonality could maintain the lowest dimensionality of feature space. From the reviewer's perspective, there is probably extensive overlapping existing in feature spaces generated by AE. Alternatively, can the authors provide references to support their conclusion?"
538	Unsupervised Contrastive Learning of Image Representations from Ultrasound Videos with Hard Negative Mining	"Clarify how hyper-parameters are chosen and the impact on pre-training.
Add related work mentioned before"	"The work is based on an observation that in USG videos, there are no similarity between the temporally distant frames. However, I feel that the property also holds for many other medical images. Thus, I expect that the authors can provide an in-depth discussions regarding the generalisation of the method.
The investigation of the cross-video sampling is very limited. How is the 'n' determined? Is it better to combine the negatives by their ranking weights against just contrasting all the negatives with equal weights? How is the memory bank maintained, e.g., as a queue? How will the memory size affect the performance?"	"The authors could use some embedding visualization methods (e.g. PCA, tSNE) to see if malignant images are separated from normal images.
They could try their method on larger dataset.
They could specify some details like the size of the memory queue."
539	Unsupervised Cross-Disease Domain Adaptation by Lesion Scale Matching	"1)	Please give some explanation on E1. (2).
2)	Section 2.2 regarding how to determine the source and target size, this is totally based on prior knowledge for target domain?
3)	Can the authors include an algorithm to show the whole process of the method to help understand. 
4)	Can the author compare the proposed method with existing batch normalization-based domain adaptation method? For example:
*	Revisiting Batch Normalization For Practical Domain Adaptation by Y. Li et al. 
*	Domain-Specific Batch Normalization for Unsupervised Domain Adaptation by W.-G. Chang et al. CVPR'19"	Perhaps the choice of ResNet50 needs to be motivated	Due to the similarty of the standard data augmentaiton pipeline, the impact seems to be limited.
540	Unsupervised Cross-Domain Feature Extraction for Single Blood Cell Image Classification	"While the idea might be interesting, the authors need to carefully consider the way they present it. 
The writing requires some more clarity. I can only quote few examples. 
Page 2, Section 1, ""that is analyzed by an autoencoder"", what is analyzed? how is autoencoder an analyzer?
(Also, I am repeating comments from above section).
There is no comparison with state of the art methods. 
The authors does not report any insights on the computational requirements. 
The motivation of the pipeline is not clear. Why choose what they choose (for the different components of the model. 
Mask R CNN is a semantic segmentation model, not a detection model.
Page 5, Section 3.2, ""we decided to use 50 as the bottleneck size"", 50 what? 50 images? not very clear.
How does the different component effect the performance? What motivated the choice of these components?"	"Dear Authors,
I read your manuscript with great interest and I found it of good quality. Also the results are quite impressive and opens the field for further improvements.
However, I think that several clarifications are needed, as in some points I found it unclear. I list them as follows:
Introduction and motivations to the work: There is a great deal of nuance involved in haematological classification. The authors chose as a use case a very narrow and highly simplified problem of distinguishing white blood cell types on images containing individual white blood cells. Although the results obtained are of considerable interest and realistically applicable on further domains, ideally I believe the introduction lacks detail and characterisation of the problem at hand.
For example, what is the rationale for classification on images consisting of only white blood cells? To cite an example (please note that this is not a request for citation, if the authors do not consider it necessary): in some works, such as this one (https://www.mdpi.com/2076-3417/12/7/3269#) recently, it has been discussed how reliable CAD systems based on CNNs are for the analysis of globules on ""whole"" images, i.e. composed of a multitude of cells, and how it is basically impossible to have a reliable diagnosis on the basis of a direct classification carried out with CNNs.
Therefore, I would suggest to the authors to improve the introduction so that it provides a deep overview of the study.
In fact, I think it is unclear what unique challenges are associated with this task.
In Section 2: from the figure, it seems that the Mask R-CNN training was performed on the three dataset merged. This aspect is not stated in the text. Could the authors be more precise in this sense?
Section 2 again: why do you call it 'reconstruction'? It sounds more like image generation to me. If it were a reconstruction, I would expect to see the WBC clean and not something ""polished"".
Still Section 2: ""In our experiments GN was effective in image generalization."" How and why?
Section 2.1: What is the anchor dataset D0? Please explain.
Section 3.2: why was the constant b in equation 3 set to 5?
Section 3.2: the training procedure is not entirely clear to me. Did you train on the 80% of the original images? Or on the ""reconstructed"" ones?
In reference to Table 1, the authors state ""AE RF: random forest classification of features extracted by a similar autoencoder trained on all datasets with no domain adaptation."". However, I think it should be useful for reader to give, even if brief, a detail about the similar autoencoder, also considering it reached best results in same-dataset experiments.
With reference to the classification method used, Random Forest, I think the authors should better motivate his choice (Note: this is not a criticism of the method itself, just a request for clarification)"	As suggested in the main weaknesses section, the baselines are not very strong for comparison. The authors are suggested to use some domain adaptation methods as the baselines.
541	Unsupervised Deep Non-Rigid Alignment by Low-Rank Loss and Multi-Input Attention	"-I think the use of the low-rank loss for the registration of noisy images is good.
-However, I do not think the authors give a good description of the proposed methods. The whole method part is difficult to follow, I can only try to understand the detailed design by using my knowledge on RASL and robust PCA.
-The proposed network may require high computation resources and memory usage. The authors shall compare the parameter amount, floating point operations, and training time, etc. Moreover, too many hyper-parameters need to be tuned for different applications, which may reduce the generalization ability of the method.
-The standard deviation of the results should be provided.
-The RASL is a classical low-rank and sparse decomposition method, the authors could compare more methods focusing on fast and noisy robust decomposition, e.g., 
Wu, Yi, et al ""Online robust image alignment via iterative convex optimization."" 2012 IEEE Conference on Computer Vision and Pattern Recognition.
Zheng, Qingqing, et al ""Online robust image alignment via subspace learning from gradient orientations."" 2017 International Conference on Computer Vision.
-The registration accuracy seems not in a high level, with just 60% Dice. If there are other methods reported results on the same dataset, the authors shall mention them so let us know the current accuracy level for this application."	"For equation 1 loss noise, how is b chosen or calculated? If the noise can be defaulted to Gaussian noise with a constant mean and small variance, why not limit it, such as with KL loss, etc? and t is also used here, so the convergence is also affected by the transformation? In this case, I think the loss declared by the authors will be more subjective about the penalty for foreground signals.
When there is no noise or strong bias, the behavior of the denoising module or the corresponding loss?
For the deformation loss, what is the reason why introducing the third regularization for sparsing displacement field.
Sensitivity to the selection of the four hyperparameters for overall losses.
Regarding the comparison with other methods, it doesn't seem fair. Because the three modules are responsible for part of things (denoise, aligning, sparse or inpainting...), the number of parameters will be more compared to other methods.
The Ablation study is not sufficient. There is only a splicing and ablation of three sub-networks, and there is no way to prove the role of the innovation points discussed in each network. The contribution points in each network can not be demonstrated, such as loss noise, deformation..."	"Figure 2 has some confusing points.
Why outputs of noise decomposition network contains black rectangles?
While \tau_2 and \tau_i are different, I_1, I_2, and I_M are same.

How the images (S_*) from the sparse error complement network look like in the real data?
Since the noise will be removed in the noise decomposition network, inputs for sparse error complement network will be noise-free images.
Was there any meaningful sparse corruption in the real data?

How the Dice score is calculated for RASL in the synthetic dataset in detail? How many the images are given to the algorithm?
For robust alignment algorithms such as RASL, the number of images are important to acquire exact the low-rank space which drives the alignment. 
If the 8 number of images were passed to the RASL, it seems a small number of images compare to the experiments in RASL paper (See Fig. 5 of RASL paper).

Why the 6.4 pixels were used in the synthetic experiment?
It would be better to calculate the metrics for different level of noise level or non-rigid transformation, such as phase diagram in RPCA and Robust alignment papers.

In the Table 1, VMorph which is a non-rigid alignment method does not outperform even the RASL which is a rigid-alignment algorithm.
Did you inspected the aligned images from RASL and VMorph? Please give us a simple answer about this situation.

In the Table 1, what is the take home message for the time measurement? Is it just a result?
And time for the NN approaches calculated only for inferencing without training?
It is unfair to compare only evaluation time for NN to the optimization-based method such as RASL.

Authors' method and VoxelMorph fix one image and do registration to make other images well-overlapped to the fixed image.
RASL does not have this property and all images are registered together. In other words, first aligned image using RASL will different from the input first image.
If the ground truth mask to calculate dice score was drawn based on the first image, it is natural that RASL achieves lower dice score.
Then, it is not appropriate to calculate dice score to compare the performance.
One way to do a fair comparison using dice score is that we should first find the transformation matrix of first image estimated using RASL, and apply inverse transformation for all images to achieve a same property that first image is the 'reference' and not be aligned.

Followings are minor comments.

""Robust"" in the title is missing in the pdf submission, where it exists in the CMT submission.

Is the gaussian noise was applied in creating a synthetic data? Then did the data have passed such as ReLU to enforce non-negativity where the image should be positive?

In the Table 1, the word Dice Time seems mistakenly added in the third line.

Explanation of Figure 4 is bit confused. The rightmost one is the image averaging without alignment. Seems that the left and middle images are also the average projection after the alignment using propose method and RCN, respectively. If they are, please clarify three of them are all average intensity projected image from the aligned result and raw data."
542	Unsupervised Deformable Image Registration with Absent Correspondences in Pre-operative and Post-Recurrence Brain Tumor MRI Scans	MICCAI audiences are likely not interested in papers with minor methodology novelty and incremental performance improvements. The authors may consider a workshop submission.	"The last proposed method 'DIRAC-D' sounds promising, yielding slightly better results including additional MR modality as input. However, all other methods are only cmpared without this additional input
It is still a bit unclear how the parameters alpha and p for the forward-backward consistency constraint are determined
What are 'successfully registered landmarks'? If there is a threshold applied, how is it determined?
Typo on page 6: '...voxels are marked in m_{bf} and m_{bf}', the second 'm_{bf}' should be 'm_{fb}'"	"It is interesting to use the bidirectional deformation field to define the mask of the tumor. However, it is suspicious to extract accurate masks in the proposed unsupervised learning scheme. It is unclear how to extract the exact boundary of the masks, considering the smoothness regularization of registration fields.
Threshold \tau_{bf} is used to define the mask. It would be helpful to describe the threshold selection.
The last term in Eq. 7 is used to regularize the mask to be small. Since the mask is represented by a vector or matrix, the appropriate norm is required.
The proposed method was compared with those using cost function masking with the tumor core segmentation map. What did the cost function masking mean? Whether the prior mask of tumors was used in methods -CM. Table 1 showed that the proposed unsupervised approach outperformed the CM. It would be helpful to discuss the performance gain achieved compared with the CM.
The proposed methods used invertible constraints for valid correspondence. Since the diffeomorphic registration provided invertible registration fields, using the existing diffeomorphic registration network as the backbone would be interesting."
543	Unsupervised Domain Adaptation with Contrastive Learning for OCT Segmentation	See above.	"some formulas are not clear
tilda notation is typically applied to random variables. It's more suitable to use x \in D in the sum limit, or specify sum limits explicitly. Also in formula (1) for example, is the summation is performed over all samples of the dataset or over all pixels in y_i and F(x_i)? same applies to other formulas. What's the difference between d1 and d2? Both have L2 norm in denominator. Formula (5) uses \in notation instead of tilda. The formulas require revision.

validation compares only with contrastive learning methods.
The paper lack the comparison with the domain adaptation methods of another origin, CycleGANs for instance [1,2], or adversarial domain adaptation [3] or [4]. Classification methods can be adopted to segmentation.

In table 1, please also mention baseline score in absolute numbers, for positive numbers it might be more intuitive to use + to highlight that these numbers are relative.
[1] USING CYCLEGANS FOR EFFECTIVELY REDUCING IMAGE VARIABILITY ACROSS OCTDEVICES AND IMPROVING RETINAL FLUID SEGMENTATION https://arxiv.org/pdf/1901.08379.pdf
[2] Domain Adaptation via CycleGAN for RetinaSegmentation in Optical Coherence Tomography https://arxiv.org/pdf/2107.02345.pdf
[3] UNSUPERVISED DOMAIN ADAPTATION FOR CROSS-DEVICE OCT LESION DETECTIONVIA LEARNING ADAPTIVE FEATURES https://ieeexplore.ieee.org/document/9098380
[4] Unsupervised Domain Adaptation by Backpropagation https://arxiv.org/abs/1409.7495"	"Please address all points listed under ""weaknesses"". Please refer in the main paper to the results given in the supplementary, as they help for the understanding."
544	Unsupervised Domain Adaptive Fundus Image Segmentation with Category-level Regularization	"While the method can be applied to datasets with >2 subcategories, the dataset used in this work has only two categories including object and background (binary).
The authors presented only good results, and it would be good to provide the worst cases as well."	See weakness section	"What is the dimension of prototypes and how do determine which layer to extract features for calculating prototypes. You need to specify these details.
Edge discriminator is utilized in your work but without detailed illustration and ablation study.
are all the results based on the single run? Since the test data size is small, the performance variance may be large. You would better apply cross-validation in your experiments."
545	Unsupervised Lesion-Aware Transfer Learning for Diabetic Retinopathy Grading in Ultra-Wide-Field Fundus Photography	"The loss function for optimizing the multi-lesion generation task consists of the BCE loss between GT and predicted lesion mask and the adversarial loss to distinguish between source and target lesion. The supervision for UWF lesion detection is very weak (only the adversarial loss). The paper should include more analysis on the results of UWF lesion detection.
In Fig.2, please explain the code coding. What do red/yellow/purple/green pixel mean?
The design of the proposed LEAM is not fully justified. The attention map is obtained from features in lesion module. Why adding the attention in lesion features as oppose to features from grading module? How effective this module is compared with simple concatenation?
Please provide analysis and discussion of the limitation of the proposed method (failure case)."	"Here are some comments along with strong and weak points mentioned before:

It will be good if the authors can show some example images from different disease classes.
The author resized the images 512 X 512. The UWF images were 3900x3072 which significantly higher. Can the authors comment about the information loss while downsizing these images. Can these affect the final outcome?
The results in terms of numbers are not quite high. Is this a issue with UWF image in general? As the authors collected those from local hospital, is it possible to comment on the accuracy of diagnosis by clinicians with those images?
In table 1, the precision of M(Ultra) is significantly higher than CycleGAN where as F1 score is higher in CycleGAN. This is a little confusing. Is this because of the Recall?"	"The development of a framework for transferring knowledge from a more-common/better-annotated modality, to another (wider-coverage) modality, is desirable and likely has much application outside of ocular imaging. However, the manuscript in its current stage may be somewhat short on technical/implementation detail, and relevant comparisons.

In the Methodology section, it is stated that ""...we trained a U-Net to mask out such artifacts [on UWF images]"". How was ground truth obtained for the UWF images, since it is also stated that the UWF images were provided with any pixel-level annotations (Section 2.2)

The training of the models (as in Figure 1) might be clarified further. In particular, is the DR grading module trained jointly with the lesion generation module, or is the lesion generation module trained first and then frozen?

It might be clarified as to how the source and target image inputs (as shown in Figure 1) are selected, for each pair of inputs. This is additionally since the CFP and UWF images should not be of the same patient/eye, being from different datasets. Then, particularly if the model is trained in an unsupervised manner with respect to UWF, is there any assurance that the images are compatible?

Moreover, it is unclear as to whether the (arbitrary?) CFP image should be part of the input to the DR grading module (implied at Point C in Figure 1), if the objective is to obtain a DR class for the target UWF image. Does this imply that the DR class output for the UWF image might be different depending on which CFP image it is paired with? This might be clarified.

For the lesion segmentation for CFP in the lesion generation module, it might be clarified whether different types of lesions (MA/HM/SE/HE) are annotated and classified separately. Moreover, the term for the relevant loss might be standardized (apparently L_Seg in Figure 1, L_CE in the text and Equation 1)

The naming of the lesion generation module might be reconsidered, if its actual function is to provide pixel-level annotations of UWF images, and not to generate new lesions on the UWF images (as from Figure 2)

Moreover, it might be considered to directly evaluate the performance of pixel-level annotations from UWF images, by transfer from CFP images; ground truth on a (small) subset of the UWF images would be sufficient.

The most obvious baseline to compare against, would appear to be a supervised UWF model trained conventionally using the image-level DR labels, either with pretraining from ImageNet or CFP data. This does not appear to have been attempted, from the models/results in Table 1.

It is unclear why increasing the number of available CFP images (from 8,000 to 15,000) would result in reduced performance (Table 2). It is also not clear how F1 and Kappa metrics would diverge so greatly, and what the interpretation might be (2,000 Images having F1 values of 66.53% but Kappa of only 33.00%, compared to say 8,000 Images having barely-higher F1 of 67.57%, but Kappa of 51.01%). Repeating the individual experiments to estimate the variance of the results might be appropriate.

Implementation details (including training & hyperparameter search methodology) might be provided for the comparison methods.

It might be briefly checked as to how frequently the trained model gets the domain (i.e. source CFP vs. target UWF) incorrect, if activated during testing. This should provide some context as to the extent to which the grading module is actually domain-independent.

Some minor phrasing/spelling issues, e.g.
(Abstract) ""newly imaging technique"" -> ""new imaging technique""
(Abstract) ""is practically challenge"" -> ""is practically challenging""
(Section 2.2, Section 3.1) ""This moudle consists of two parts/grading moudle"" -> ""module""
etc."
546	Unsupervised Nuclei Segmentation using Spatial Organization Priors	"In general, this paper is well written and easy to follow.  It utilizes the consistency of nuclei distribution between HE and IHC images and imposes the spatial organization prior via generative adversarial learning. The motivation is clear and experiments on 3 datasets show its effectiveness.
Other questions:

How to build the HE mask database? Cells in different organs may have large appearance variance. Will it affect the performance a lot?

The main purpose of ICH images is to estimate the immunofluorescence correspondences. How can the method be generalized to multi-class nuclei segmentation?

The consistency loss only considers the colour augmentation. Have you tried other types of perturbations such as the VAT?

Will the processed Warwick HER2 dataset be published available?"	"The method proposed in this paper may be better termed as unsupervised nuclei detection. It may be hard to make a reliable segmentation method. Segmentation is usually used to extract the shape information of the cells or nuclei. If the segmentation is not accurate enough, it could extract misleading information for diagnosis.
What are the training settings for the supervised and unsupervised in Table 1? Actually, the proposed method also makes use of the labeled data. What data and how many samples are used to train the supervised methods? What and how many labeled and unlabeled samples are used to train the proposed method? Is the proposed method using less labeled data than the Unet and Nuclick in Table 1?
Precision and Recall may not be good metrics separately. Why not computing F1 score based on the precision and recall?
How is the accuracy balanced?
How is the object level segmentation achieved using Unet?
It can be helpful to show some failure cases for the method to help understanding the limit of the proposed method."	"The overall idea of the novel unsupervised method based on GANs is quite well described. I have but a few comments:

the figure reference on p.3 should bo to fig. 1 instead of fig.2
I suggest a redesign of figure 1 to be more consistent with the in-text description (generator-segmentator) and to possibly include the other discriminator
It would be also interesting to evaluate the influence of the included post-processing in the proposed metod. How much the evaluation metrics change without erosion and watershed. 
Maybe Unet+watershed would be even better but since only proposed method has postprocessing it is unknown. Please provide further explanation/results in either main paper or supplementary material.
Since this method is based on spatial organisation priors, it would be interesting to evaluate the consistency of the results in relation to tissue architecture compactness. (this is just a suggestion for future investigations)"
547	Unsupervised Representation Learning of Cingulate Cortical Folding Patterns	As an application, technical improvement to the existed approaches is acceptable. However, the authors may provide more solid conclusions on the findings, including the shape clusters and comparison across SimCLR and b-VAE. The authors may show the promise of this unsupervised clustering approach in some applications, such as abnormal shape detection.	The revealing of novel folding patterns is not so convincing, what will you find if you simply increase the cluster number? Also, what does these new folding patterns suggests? do they have any relationship with human cognition or disease? I feel it is not so complete by only exploring patterns.	"In introduction, please cite papers for some sentences, e.g., 'Contrary to macaque...making it a fingerprint of each individual'. Papers about cortical folding features used as fingerprints for individual identification should be cited.
The introduction section is not well-structured, its logic is not very clear, 8 paragraphs in this section are too much for MICCAI paper.
In methods section, the hyperparameter bin 1st equation should be mentioned and described.
In experiments and results section, please perform cross validation to validate that whether the models applied on different subsets of the dataset can obtain consistent cortical folding patterns, which helps to convince the readers that the result is reliable and reproducible.
Please describe the relationship of the folding patterns discovered by two models in both fig. 2 and fig. 3. In my opinion, though the models are different, most of the latent folding patterns should be similar or consistent. But they are not very similar here, please explain why.
Lack of a comprehensive comparison of cortical folding patterns discovered in this work and related papers."
548	Usable Region Estimate for Assessing Practical Usability of Medical Image Segmentation Models	In addition to the main weaknesses mentioned above, I have a couple of additional comments about the evaluation. First is that there are no error bounds or statistical tests showing significance of differences between the models. Second, it is unclear why different models were used in the sixth dataset evaluation. Third is that the bar graphs in figure 3 can be hard to read. I would suggest creating line graphs to better show the association between the values for different correctness requirements within each model.	"(1) In Intro the paper claim that CCRC is more practical, as a high CCRC indicates better alignment between correctness and confidence; I believe this also applied to ECE. If so, what is the advantage of CCRC over ECE?
(2) The computation of URE needs a pre-defined correctness constraint, this limits the flexibility of model evaluation; in fact, defining a proper correctness constraint may be hard in many applications. It would be good to see what the authors' thoughts are on this problem;
(3) In Figure 3, we can see that a small change of correctness requirement can lead to dramatic change of UR, e.g. Fig.3-4, a change of 0.01 dice can lead to 0.15 UR change for U-Net. Does this mean URE is very sensitive to the correctness requirement, or in other words, no stable? Because changing the requirement by very little may lead to a different model recommendation;
(4) How the conclusion (URE is stable on unseen samples) is drawn from Table 1? It's hard to judge whether a ~10% frequency violation is good or bad based on the current content."	The authors should justify why these two indeces should be used instead of the existing ones. They should add a comparison and show cases where the proposed  indeces are more representative of the model performance
549	USG-Net: Deep Learning-based Ultrasound Scanning-Guide for an Orthopedic Sonographer	"Abstract lacks quantitative information. E.g. results
2.1 pg 3: This section is quite confusing. What is the significance of the parallel lines? Assuming a cubic 3D volume, wouldn't any cross section contain parallel lines? And is there a specific plane/direction we look at the RCT (axial, coronal)? Or does it not matter. E.g. In fig 1 if we tilt the probe around the yellow line, we would get a pink parallel line which is a bit further down the bottom face. Would that still be an acceptable plane?
As for distance, are the authors measuring the distance between a plane and points on a 3D RCT volume? I think re-writing this section to highlight why this is done, along with a figure showing how this distance is measured, can clarify things.
When you talk about probe motion, would that be motion of the probe orthogonal to the surface only or could rotation/tilt of the probe (which could also eventually capture the RCT in the field of view) be a feasible solution?
In LU, L, LD, etc what are the up/down/left/right directions? What direction is S? (from later in the text it appears to mean ""stop"" but it is not clear here)
2.2. What do the authors mean by thick and thin volume?
2.2. If D is depth, I'm assuming W and H are width and height (again, relative to what? Please show in a figure). What is ""p""? This is somehow shown in figure 2 but not clear from this section.
In section 2.2. Do the authors mean to say that the direction of movement is calculated from the prediction of the location of the RCT, which is computed by predicting the 3D volume around a 2D slice?
Do results depend on how close the starting probe position is to the RCT?
Please provide information regarding the dataset (size, train/test split, how the data was collected, an relevant clinical information etc)
Section 4: what is the significance of this method for dataset construction (e.g. why randomly slicing 3D US images lead to better results?)
The authors state that since their  approach has never been studied in that the proposed model guides the probe to the target regions, they did not perform comparative studies. However, it is worth comparing the method to manual performance of experts. For example, how would using this method as a guide improve the performance of an expert compared to when they manually try to find the RCT. Would this improve the time needed? How would this help non-experts? The authors initially justify that this method would help non-experts. Would this improve the accuracy of them finding the RCT? By how much? Some clinical evaluation would strengthen the usefulness of this method. [The authors acknowledge this as a limitation, however ""some"" form of comparison is necessary in my opinion]
I wouldn't consider limitation 2 a limitation, but a strength."	"There is some points to improve:

More comparioson with other methods in Table I, as only the proposed method and its variants are compared.
More training epoch is recommended in the experiments.
More formal mathematical problem formulation in the method chapter.
An overall introduction of the experiments to provide general picture."	"As noted in the prior sections, the two major revisions are as follows:
1) Ensure that all variables used in the manuscript have been defined and provide hyperparameters where relevant.
2) Expand or modify the literature review provided in the Introduction to justify the need for the proposed method to overcome existing limitations.
Minor revisions:
Abstract:
-Unclear what is meant by ""structural complexity""
-The tense used for the methodology is confusing. Please ensure that methods are written in past tense.
Introduction:
-Paragraph 2: I would be careful with the definition of CAD as it is currently stated (""...the deep learning network diagnoses diseases in the acquired images"". The deep-learning algorithms currently aid in the diagnosis but are not making independent diagnoses.
-Paragraph 2: Good job acknowledging existing work in this area. I would like to see more justification of ""Despite the accurate guidance toward the standard scan plane by the proposed deep learning models, the unskilled sonographers have still difficulty in searching target disease regions"" to explain the need for the proposed approach
Methodology:
-The figures are very helpful for understand the approaches used.
-2.1 Dataset Construction: In the last sentence, the meaning of a movement in the ""S"" direction is not defined until later in the experimental results. Please add it here.
-2.2 Anatomical Representation Learning: Variables H, p, and W do not appear to be defined in the text.
Experiments and Results:
-Clearly described
Discussion and Future Work:

Limitations: For you second limitation, it sounds like this would be a straightforward transfer to another application but further work would need to be done for new applications to show the algorithm what to detect. Please clarify this in the text.

Conclusions:
-Unclear how the high evaluation performance demonstrates ""novelty"". Please rephrase."
550	Using Guided Self-Attention with Local Information for Polyp Segmentation	In local-to-global approach, do you think more multi-scale layers will improve the segmentation accuracy?	"(1) In terms of the PP-guided Self-attention: 
a. how to properly determine the parameter alpha? Is the model performance sensitive to the choice of alpha?
b. The vanilla attention matrix, i.e. M_{SA} in the paper, is normalized by softmax operation, but the proposed attention is not normalized; does this affect the model performance and training stability? 
(2) In Table 1, some numbers are not consistent with prior works, e.g. TransFuse [24] achieves 0.942 mDice on ClinicDB in their paper, but the number in Table 1 is only 0.908. Why is the gap so large? Also, to ensure fair comparison with TransFuse and PraNet, please compare the model size and inference speed;
(3) Where does the attention map from figure 1 comes from? The paper claim that the proposed method enhance model's perception of polyp boundary. It would be better if the paper can compute the similarity between the attention map and the groudtruth segmentation in the boundary region.
(4) Most medical segmentation application are on 3D images. It would be good to see how the proposed model perform on 3D data, e.g. MRI, CT., and compare the results with nnUnet3D (https://github.com/MIC-DKFZ/nnUNet)."	"I recommend adding measurements of dispersion in the quantitative results, and justify the choice of examples for qualitative analyses.
Also, please check the language. Some examples that need reviewing are:

""six metrics, i.e. mean Dice, mean IoU and etc.""
""an architecture consists""
""using prediction map""
""decoder has two stage""

In the implementation details I see the authors employ a ""multi-scale strategy"", but it is not clear what this means. I would also like to know why vertical flip was not used."
551	USPoint: Self-Supervised Interest Point Detection and Description for Ultrasound-Probe Motion Estimation during Fine-Adjustment Standard Fetal Plane Finding	"In equations (3) and (4), shouldn't there be no subscript ""j"" for the symbol ""M""? ""j"" is only used inside the summation.
Consider using the special symbol instead of the italic letter ""T"" for matrix transpose for all your equations.
Need more information on the motion estimation module. How do you use the Transform Net? Does it take all 2D point pairs and output a single 3x3 matrix ""T""? I find the process of predicting a 3x3 matrix to lift 2D pixels and then use a closed-form 3d point cloud registration solution based on SVD problematic. Can the authors explain more about why they choose this process for motion estimation?
End of page 7, ""arccos"" and ""GT"" should not be italic. These are texts instead of symbols.
It is important to know the performance of the local detector and descriptor coming out of the first two pre-training phases compared to the one out of the final phase of the proposed end-to-end pipeline. This is because the first two phases, mentioned in the supplementary material, come from other works and the authors need to show that their proposed end-to-end pipeline used in the final phase of the training process further boosts the feature matching performance.
The deep regression method authors compared seems to be published in 2018. Are there no more recent ones with better performance that authors can compare their method against?"	"Page 5, attentional graph matching section -> My sense is that 'intra' and 'inter' are standard terms as opposed to 'intra' and 'extra' ? Not a big issue though.
About the sinkhorn algorithm - how is it executed during training time? Is it a differentiable operation like the SVD? Or is just a callback or something? Could you explain how this works?
You write U, V \in SO(3). SO(3) isn't described - may be beneficial to some readers to define it.
The qauternion is also not defined.

Your method also seems to suffer from the standard drift issue (larger error for frames that are farther away from the reference frame) right? Do you have any comments/ideas on how to improve that?

There's some language issues here and there affecting clarity. Some obvious ones I caught:
page 1, motivation: ""locate the probe position"" -> However, locating the probe position.
page 2, ""the main limitation is naturally for lack of generalization ability"" -> the main limitation is naturally the lack of generalizability.
page 3: ""as Fig. 1 shown"" -> As Fig 1. shows"	"The title is very lengthy, please use a shorter title. Page 2, typo error method not methed.
I think there is miss-undrestanding for self-supervised training. 
The author should cite recent unsupervised motion estimation in US imaging including : 
 Tehrani AK, Sharifzadeh M, Boctor E, Rivaz H. Bi-Directional Semi-Supervised Training of Convolutional Neural Networks for Ultrasound Elastography Displacement Estimation. IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control. 2022 Jan 27.
Delaunay R, Hu Y, Vercauteren T. An unsupervised learning approach to ultrasound strain elastography with spatio-temporal consistency. Physics in Medicine & Biology. 2021 Sep 3;66(17):175031.
and semi-supervised methods:
KZ Tehrani A, Mirzaei M, Rivaz H. Semi-supervised training of optical flow convolutional neural networks in ultrasound elastography. InInternational Conference on Medical Image Computing and Computer-Assisted Intervention 2020 Oct 4 (pp. 504-513). Springer, Cham."
552	Vector Quantisation for Robust Segmentation	"It would strengthen the paper's claim of capturing anatomical structure if we can see some visualization of the codebook vectors and how it relates to the increased performance over baseline in single domain.

Results on ACDC Dataset will strengthen the paper.

Some ablation studies are missing. E.g., an ablation study on number of vectors in the codebook. Does increasing the number of vectors leads to ""more complete"" codebook and hence improve performance in the domain shift experiments?

Please refer the points in the ""weakness"" section."	"The author claims that the network could be robust by minimizing Ph(x+e)-Ph(x), this assumption could be true for the noise perturbations. However, for domain shift, the problem should become Ph(f(x))-Ph(x) instead of simply adding a small value of e. The function f() can range from a renormalization function to a non linear mapping, like in the paper shown in [1] [2]. Therefore, it is hard to believe that the VQ could solve the domain shift problem in general, as the domain shift could be brought by different reasons, like the setting of equipment, or the variance of imaging conditions. So I would suggest the author mainly validate the work on their second task to approve the robustness under noise or degradation.
There are many other works for solving the domain shift problem in medical images, such as [3][4]. While the author only compares VQ-Net against U-Net. So what is the advantage of VQ against domain adaptation methods? Are there any further experiments to approve this?
In the task of the domain shift experiment for lung segmentation, as shown in Figure 1, I didn't see there is an obvious difference between NIH and JRST. And in table 2, the segmentation performance improves from JRST to NIH encounting domain shift. Therefore this experiment is not convincing enough to prove the effectiveness of VQ for domain shift.
Minor Comments:
Maybe the name VQ-Unet is better than VQ-Net in this paper, since the network develops from U-Net. This follows the same naming rule from VAE to VQ-VAE.
For Table 2, it would be better if the author could reorganize the data. For example, they can represent the change of dice score and HD95 in seperate tables. Therefore, it would be easier for the reader to compare the difference brought by the domain shift.
Reference:
[1] Zakazov, Ivan, et al. ""Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021.
[2] Chen, Cheng, et al. ""Synergistic image and feature adaptation: Towards cross-modality domain adaptation for medical image segmentation."" Proceedings of the AAAI conference on artificial intelligence. Vol. 33. No. 01. 2019.
[3] Yan, Wenjun, et al. ""The domain shift problem of medical image segmentation and vendor-adaptation by Unet-GAN."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2019.
[4] Stacke, Karin, et al. ""A closer look at domain shift for deep learning in histopathology."" arXiv preprint arXiv:1909.11575(2019)."	"I think that the manuscript would benefit in terms of clarity if ""sg"" from Eq. 2 would have a more detailed explanation, as in [19] (right below their Eq. 3). Related to this, above Eq. 2 we can read that the gradients from the encoder need to be copied to the decoder. At first, I thought that it was to ignore the codebook from \Phi_q, but I think that this codebook gets updated by backpropagation as well right? (Eq. 2, third term).
I found Sections 2.3 and 3.1 a bit difficult to read/understand, in my opinion. First, in Section 2.3, ""r"" is defined (Eqs. 3-4) to later discover in 3.1 that ""r is obsolete"".
Typo above Assumption 3: ""and thereby enforce"" -> ""and thereby enforces"""
553	Video-based Surgical Skills Assessment using Long term Tool Tracking	"1- On using methods in refs 15 and 24 for longer videos: Would not the decomposition of longer videos into shorter ones make it possible to use these methods for long videos as well? This is instead of creating a brand new method to deal with this problem.

How sensitive is the proposed method to the authors' choice of the 3.5 threshold? (in the Dataset Description section)
It seems unfair to compare the proposed method with the GOALS assessment tool. The reason is that in GOALS one gets continuous value, representing the assessors' feedback, but the proposed method only considers two classes. In this context, from a trainee's prescriptive, GOALS gives more useful information compared with the proposed method.
In the discussion section, the authors wrote: ""This indicates the classification created by the model is comparable to human level performance"" I am not sure if the ultimate goal should be having models comparable to human level performance because humans in the surgical skill assessment task can vary a lot and we do not want to reproduce this variability at all."	"As said before, I will suggest to compare your proposal with other convolutional networks that were specifically developed for surgical tool tracking. There are some of them in the bibliography and at least in a qualitative way should be included in the discussion section.
The real contribution of the proposed algorithms could be fairly compared after this analysis because an important part of the state of the art is currently missed."	"Title matching content:
Yes, the manuscript describes a novel tool tracking method for skills assessment and thus matches with the title description.

Abstract summarizing content:
The abstract mentions the problem statement clearly along with the problems associated with prevalent methods for skills assessment.
The abstract mentions the use of the self-attention transformer network but did not justify the rationale behind their choice.
It would be nice to mention the percentage improvement obtained by the proposed model for skills assessment.

Knowledge advancement:
The method proposed a new cost function and uses an existing re-identification network with a transformer to perform tool tracking. However, the manuscript does not provide details on how it compares with other standard temporal models such as LSTM, TCN, etc., hence failing to provide a holistic picture of the tracking method.
The method is evaluated on a small subset of the dissection phase amongst the entirety of other phases present in Cholec80 for skills assessment. This limits the novelty only to the specific Calot Triangle Dissection phase.

Positioning with existing literature:
Bulk of reviewed works on object tracking are outside the scope of medical domains. However, there are lots of published works on tool tracking in surgical domains such as [1-3]. The manuscript needs to be properly positioned.
Related references are covered for the tool trajectory-metrics, transformer, and datasets are provided.
References
[1] Nwoye, Chinedu Innocent, et al. ""Weakly supervised convolutional LSTM approach for tool tracking in laparoscopic videos."" International journal of computer assisted radiology and surgery 14.6 (2019): 1059-1067.
[2] Robu, Maria, et al. ""Towards real-time multiple surgical tool tracking."" Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization 9.3 (2021): 279-285.
[3] Zhang, Lin, et al. ""Real-time surgical tool tracking and pose estimation using a hybrid cylindrical marker."" International journal of computer assisted radiology and surgery 12.6 (2017): 921-930.
etc.

Method description and rationales:
The method is divided into three modules with sufficient details and provides enough purpose for all the modules. The three modules correspond to Tracking algorithm, feature-based and learning-based skills assessment.
The method uses yolov5 for detecting tools in a scene for which yolov5 was evaluated on the ""last 5 videos of the dataset"". This part is confusing as it does not clearly mention which dataset is used for evaluation - is it the Cholec80 or the 15 videos annotated with a bounding box (section 2.1). It would be nice to clarify the dataset used for evaluation.
It would be nice to justify the Kalman filter method used for tool tracking and what formulation for the Kalman filter is used for the same. This needs to be explained better as tracked locations are further used for the construction of a cost matrix, a novelty the manuscript claimed. 
The description provided for the cost function under ""Cost Function Definition"" is not in proper order. The manuscript mentions the third term in equation 1 as the second term in the description (pg 5). Similarly, the second term in equation 1 is mentioned as the last term. These structural mistakes should be avoided to prevent confusion for readers.
Even though the transformer works well in time series data, there are other temporal models like LSTM, TCN, etc. which work well, but the manuscript did not provide enough justification for choosing the transformer model beyond the standard ""best performing model"" claim. This leaves the readers to contemplate the performance of other temporal models with respect to the transformer. The manuscript should clarify more on this as a simple ablation with other temporal models would have been sufficient.
The manuscript did not present a description of the transformer configuration or parameters. 
The traditional machine learning model used for feature-based skill assessment is the random forest model, however, the manuscript did not provide any rationale behind it. What happens if other models such as ""xgboost"" or tree-based models are used? The manuscript should provide ablations justifying the use of the random forest model.

Standalone figures and tables:
The problem statement in Fig 1 is clear and summarizes the goal of the paper.
The caption for Fig 1 should add short details of what the x,y,d, and w variable represents. Also, the intermediate cube mentioned with d and w/2 parameters should provide details on what it represents.
However, the acronyms provided in Table 2 such as IDs, MOTA, FP, and FN, and their full names are missing from the paper.

Data contribution/usage:
There is no dataset contribution, However, the method is implemented/evaluated on the dissection phase of the Cholec80 dataset. The manuscript did not mention train/val/test splits used for training and evaluation of the model.
The CholecT50 triplet dataset is annotated with bounding boxes (133k) for surgical instruments for detection during tracking.

Results presentation:
The results in the tables are clear but the best results must be highlighted in bold for Table 2.
The results are not specified with mean and std across different runs which would have made it easy to comprehend the stability of the experiments.

Discussion of results and method justification:
The skill efficiency is separated into two classes - low and high based on the 3.5 threshold value. Is there a justification for choosing 3.5 for the threshold? The author should provide more details on this. 
The manuscript points to the less number of Identity switches with the proposed method but it is important to know how the ID switch is computed; the detail which is missing in the paper.
To avoid the class imbalance problem, the manuscript mentions the use of random oversampling but did not provide details on the number of samples for the two skill classes - low and high.However, the readers would not have an idea about the number of samples used.

Comparative analysis with existing works:
The results were compared with the baseline ByteTrack and Random Forest models as there is no existing work dedicated to GOALS-based skill assessment on Calot Triangle Dissection.
Limitations are clearly specified and provided with reasoning.

Conclusion 
The manuscript presents insights for research continuation for extending skill assessment to other phases of the Cholec80.

Arguable claims
The manuscript claims the transformer to be the best performing model of tool tracking and yet did not provide ablations on other temporal models to position the importance of the transformer model. It raises serious questions on the nature of the experiments performed.

Manuscript writing and typographical corrections
Reference to Hungarian Algorithm paper is missing. (pg: 4 first row)
[Aglorithm] in Algorithm 1: [Algorithm]
[hungrian] in Algorithm 1: [Hungarian]
[inacive] in Algorithm 1: [inactive]
[cos t] in Track Recovery subsection: [cost]
Acuuracy in Table 3: [Accuracy]"
554	Vision-Language Contrastive Learning Approach to Robust Automatic Placenta Analysis Using Photographic Images	"Please use a table to improve the description clarity of the negative sample and positive sample in the dataset Section.
Please optimize the writing and figures."	"The notation is not defined. For example, in equation 1, what is t_{-1}? It is also not consistent - in equation 1, x_i is a vector without bold fonts, but later in equation 3, bold fonts are used for vectors and in equation 4, normal font v_i is a scaler. Please ensure consistency in notation.
There are many statements that are not clear to me. Please rephrase these:
a. ""NegLogCosh have less emphasis when vi and ui are very different thus reducing the effect of dominant feature from either the text side or the image side."" - why is that? log(cosh(x)) looks like |x| when |x| is large. Not sure what you mean here.
b. ""Because the similarity metric (5) compares two feature vectors element-wise, the
result of the comparison should not change much if some features are missing."" - why is that? Equation (5) ultimately takes the mean, which can is easily affected by outliers.
there are several places the equations/logic is incorrect/has typographical errors:
a. in equation 9, what is c? and how do you get (9) from (5)?
b. How does equation (10) imply (11)? It only holds if equation 10 is true for every possible realization of (i_1, i_2,...i_k) (all 2^k of them!)
Instead of heuristically assuming that 1 or 2 percentage point difference is significant, it may be better to perform statistical tests to verify significance.

Instead of a computationally expensive new loss function log(cosh(x)), what would happen with a simpler loss function
x
?"	"+In the right side of Figure 2: ""Stop gradient"" is not clear what it refers to, is it not just that the output for the non-relevant tasks is not used (equation 2?), or you used a multi-task objective and stopped the gradient each time for the tasks that were not the relevant at a given optimization step? This should be explained better in the text .

In what images the multimodal model was better than the vision only? Can you devise a method to assign word importance to image regions?"
555	Visual deep learning-based explanation for neuritic plaques segmentation in Alzheimer's Disease using weakly annotated whole slide histopathological images	"The quantitative estimation using the trivial software can be included to compare the results.
More dataset could be included. Deep learning methods on limited dataset could be over fitting.
Clinical value of the techniques is not clearly discussed."	"Emphasizing more on why this problem is important and why others have not done this would highlight this work.
Including results from UNet architecture (Section 3.1) for scanners and normalization in Table 1 would be helpful.
According to Table 2, attention UNet provides better results than UNet. In addition, Figures 3-5 show results of attention UNet. Then what is the message from UNet?
From Figure 5, the authors claim that attention map can improve human annotation. I think the changes in delineation is minor - what do the authors expect to see clinical improvements using this attention map for plaque segmentation?
There are some typos to be fixed: and -> an (page 4), tunning -> tuning (page 5), diffult -> difficult (page 8), assitive -> assistive (page 8)"	"I could not see any technical novelty in terms of the approaches and methods used even though the methods used are solid and well accepted within the community;
A bit more of explainability exploration could have helped to make the paper stronger, what exactly the model learned."
556	Visual explanations for the detection of diabetic retinopathy from retinal fundus images	"The justification why Equation 3 (ensembling) is more effective than Equation 2 (adversarial training) should be given more detailed discussion.

It is better to design experiments to justify why the values of tunning parameter beta (in Equation 2, adversarial train) could not help a robust model achieve high accuracy and good visual explanations simultaneously."	"The authors should provide more details regarding the generation of the saliency maps and oversampling.
It would be interesting to check the visual explanations generated by more recent saliency map techniques (e.g., LRP)."	"Major comments:

The paper spends a lot of time motivating VCE's and their derived saliency maps, although no evidence (and some weak counter-evidence) that they are better than simple, openly available frameworks. Ultimately, the paper would be improved by focusing solely on the first contribution, which takes up less than half of the methods (about 1 page of methods+results versus approximately 2 for the same sections for VCE components) but produces meaningful results. What would be warranted here is more ablation studies, studies using different ensembling techniques, or 'pure' ensembles of tranditional or adversarially-learnt approaches as well as mixed."
557	vMFNet: Compositionality Meets Domain-generalised Segmentation	"As mentioned in the paper, 'the vMF likelihoods contain only spatial information of the image', it would be interesting to further discuss the relationship between modeling via vMF distribution and the traditional disentanglement. Especially in Figure 2, the visualization of the most informative vMF channels is very similar to that of the content code feature maps in disentanglement.
In section 3.1, using 'D' as the channels is somewhat confusing. 'C' would be a better notation for channels.
Typo at section 4.5: 'subjectc'."	"I wish authors can explain in a more formal manner why the proposed model helps learn features that can generalize across domains. What is the difference/advantage of the method with respect to a simple clustering prior on the features?

Why use vMF instead of a GMM ?

The loss L_vMF seems incorrect. I believe it should be Sum_i min_i (1 - mu_j'*z_i) since the goal is to minimize the cosine distance, not cosine similarity.

The reconstruction loss in Eq (2) is somewhat arbitrary. Why would the log-likelihood be used as weight ? How can the image be accurately reconstructed from such low dimensional space ?

Shouldn't there be another weight in front of the reconstruction loss in Eq (3) ?

See the main weakness section for other comments."	"The feature extractor is almost a 'full' U-Net, but without the last upsampling layer. This means that the features Z would be half the input images in terms of spatial dimensionality. The reconstruction and segmentation networks are quite shallow with only 1 upsampling layer. It would be useful to clarify these aspects in figure 1 - for instance, you could add a partial decoder to the orange feature extractor, and reduce the number of upsampling in the blue and green blocks.
It is said that ""For data from different domains, the features of the same anatomical part of images from these domains will activate the same kernels."" This is a strong claim. Is this based on empirical results or can the authors support this claim theoretically? In the former case, can the authors provide an intuition why the learned kernels are robust?
In the evaluation, two settings have been mixed - those of semi-supervised learning and domain generalization. While the constraints of both these settings (less labelled data and domain shifts between training and test images) may occur concurrently in practice, the methods that the proposed method has been compared with have all been developed primarily for the domain generalization problem. A fairer comparison with respect to semi-supervised learning would have been to include methods from that setting as well. I admit that this would call for too many comparisons - a leaner way could be to focus on one of the two problems at a time.
Please provide details of which intensity and resolution augmentations are used and with what hyperparameters in the compared method SDNet + Aug.
I am not sure how much can be read into the compositionality visualization in figure 2. I suspect that if one visualizes the different channels of a layer before the last upsampling layer of a normal U-Net trained using a supervised loss, one might also see similar segregation of structures into different channels. Perhaps the presence of the reconstruction loss also preserves background structures, which would not happen for a u-net trained only for segmentation. On the other hand, the background structures (e.g. those on the top-right) are not necessarily separated into different channels.
How sensitive is the method to the choice of the number of kernels? Is it important to keep this number relatively low?"
558	Vol2Flow: Segment 3D Volumes using a Sequence of Registration Flows	More details about the user-provided single slice annotation should be added for clarity. Additionally, the influence on the segmentation performance from the slice annotation should be investigated.	"The paper presents an image segmentation method via slice-to-volume label propagation using image registration. It is overall an interesting approach and it has shown to outperform a few other methods in the literature (e.g. fully supervised, other registration methods, etc.). The actual applicability in real scenario worth an in-depth discussion. A few comments are provided as below.

From Fig.1 and the corresponding description, it seems the warped slice is used as the input to produce the next warped slice. Error will propagate rapidly.
The method requires a SVM training process to post-process the label, which seems quite inconvenient. In a multi-label scenario, do you need to train several SVM classifier? Please clarify.
Different organs may require to annotate different slices, if they're not appeared in a single slice simultaneously. It is desirable to investigate the effect of the location of the annotated slice to the segmentation performance. Is it better to annotate the central slice of the target organ? How to select which slice to annotate?
One drawback of registration based method is computational time. It is desirable to report the training time and inference time when comparing different methods.
It is an interesting approach to propagate the label from slice to volume. However, it is still an intensive workload to annotate one slice (or potentially a few slices) for each unseen volume accurately, especially in a multiple organ case. How does it compare with few shot learning/semi-supervised learning?
Could the author comment on how well the registration method work on slices with organ transitions or slices with larger thickness? When does the registration fail?
Worth considering applying the method to an interactive image segmentation scenario, where the user can interactively improve the segmentation result."	the method is consequentially novel and interesting and it deviates from a common algorithms for the task.  Nevertheless the approach seems not very practical, and the comparisons are limited.
559	Warm Start Active Learning with Proxy Labels & Selection via Semi-Supervised Fine-Tuning	I recommend the authors improve the paper presentation. Although they provided an algorithm to describe the proposed approach better, which is nice, they failed in providing a more robust experimental section.	"Explaining the different setting better and uniform the setting name between figures and tables would largely increase the paper's clarity.
In sec 2, it may be better to provide some arguments or related work for the claim that mean teacher and shape constraints are not ideal for AL.
The figures 2 & 3 are not easy to read, may be better to increase font size of the legend."	The method is three-staged at this point i.e. proxy ranking, supervised, unsupervised. One future direction could be making the method end-to-end
560	WavTrans: Synergizing Wavelet and Cross-Attention Transformer for Multi-Contrast MRI Super-resolution	"Detailed comments:

What's the model size of proposed method? And what's the inference time compared with other methods.
The architecture illustrated in Fig. 1 can be optimized, it's a little bit confusing as there are too many lines in the figure."	"proofread and fix any grammatical errors e.g. section 1 line 2 'is able to provides'; page 2 line 3 'To the end'; page 2 line 6 'MR images quality' -> 'MR image quality'; 3rd last line before section 2 'a in-house dataset';
fig 1 shows the WPD and the text in above sections indicate use of wavelet packet decomposition but do you need wavelet packet decomposition or is it just wavelet transform which is being used? why wavelet packet and not simple wavelet transform? if wavelet packet, how is it being used since the paper says we only use 1-level decomposition but up to 1-level there is no difference among the WT and WPD? can you clairfy this in the text
fig 2, not clear what each row represents; further, the columns need to be referenced with the relevant citation where the method is from another paper"	"Why authors choose Swin Transformer as backbone? Please compare it with other transformer model (e.g. DeiT, PVT) and state your reason.
Please show the parameter quantity and computational complexity in Table 1."
561	Weakly Supervised MR-TRUS Image Synthesis for Brachytherapy of Prostate Cancer	The authors did not conduct ablation experiments. This is not a big problem, but the author uses too many modules, I am not sure whether all these modules could work. For example, Contour, IDT loss and BCE loss are all used in Prostate Contour Segmentation, but I am not sure whether these settings are all necessary.	It is very helpful that the authors can provide additional explanation on the optimization objective for the generators G and H.	I have no suggestions here.
562	Weakly Supervised Online Action Detection for Infant General Movements	"It would be nice if Figure 2 shows the intermediate result for the STAM. Furthermore, it would be good to match and align the measurement points as well.
It will be helpful to explain why the proposed structure is appropriate for infant video data."	The ablation results do not sufficiently show the strength of the design of the proposed model. In particular, it seems that more diverse experimental designs and analyses are needed for action detection. In addition, a better evaluation would be possible if there was an in-depth analysis of the ablation results.	"The comments and suggestions from the reviewer are as follows.

The detailed information of the proposed method should be provided.
There are some typos. For instance, the authors used ""Table 1"" and ""table 1"" interchangeably in the manuscript. But the reviewer thinks they should be consistent with each other.
How many male babies and female babies were in the dataset?
The interpretation of the experimental results were too limited. The explained and analysis of the results should be enhanced.
In the manuscript, the resolutions of the whole dataset are not the same. How did the authors solve this problem was not mentioned.
The reason why the authors focus on F+ is not clear. The author may want to explain more."
563	Weakly Supervised Segmentation by Tensor Graph Learning for Whole Slide Images	"(Major + Minor) Points that should be addressed within the review:

The proposed methods does have a significant number of hyper-parameters. How were they parametrized? Using the optimal value overall experiments? Please comment (and if possible, include this information in the paper)
Please comment on the computational cost for the algorithm, especially as you haven't been able to use all training data. This is especially interesting for the additional cost by l1, which really adds only slightly to the overall performance.
Please provide some measures of uncertainty for the experiments. For example, by calculating a confidence interval using Bootstrapping.
In Table 1, the results of WSNTG are all bold, even if other results are higher. This should be changed, it is very misleading.
There are too many variables defined. A significant number of all the variables defined in the paper are only used once, during their definition. This makes it really hard to remember the important variables. Please reduce the number of variables.
The paper is generally well-written. However, there is still room for improving the language. For example, active voice is often used like ""WSNTG does ....."" which should be formulated in passive. Also, the use of ""the"" is often wrong.
On page 5 is a type. I assume FAAM should be GAAM

While this is most-likely too much for the revision, I would suggest addressing the following points before submitting this work again (to another conference or a journal):

Using two datasets is already quite good, however, I would suggest including more of the publicly available datasets in this field. This would make the evaluation even stronger.
Include more current state of the art methods, for example YAMU by Samanta et al."	The authors should improve the writing to explain the methods in more details or the reasoning of some choices.	"This paper proposed a weakly supervised network based on tensor graphs for segmentation of whole slide images. It efficiently segments WSIs by superpixel-wise classification and credible node reweighting using only sparse point annotations. The proposed network represents multiple hand-crafted features and hierarchical features yielded by a pretrained CNN to deal with the variability of WSIs. Experiments conducted on two benchmark datasets. This paper is easy to follow and to understand, the writing is well and feel like the idea is interesting, but it is not from my domain, so I could not provide some good insightful domain advice. I noticed there is no Related Work section which is very helpful for researchers to understand the context and background, some recent works even in a different domain such as, Liu, Xien, et al. ""Tensor graph convolutional networks for text classification."" Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 05. 2020., could be discussed like intra-and inter-graph propagation. Experiments could consider more publicly available benchmark datasets and to compare with more state-of-the-art methods, and more analysis suggest in the ablation study."
564	Weakly Supervised Volumetric Image Segmentation with Deformed Templates	"The idea is valuable as the use of an ASM is probably a good idea in this context. I would suggest to develop an algorithm that can take both positive (""this region should be part of the foreground"") and negative (""this regions should not be part of the foreground"") feedback from the user for example using left and right clicks."	Weakly supservised volumetric image segmentation is very useful in practice. Although the performance is not much better than the baseline, the novelty of the propose method makes it a good attempt to address this challenging problem.	The paper should be revised to make it clear exactly what the set up is and how it is evaluated.  In particular, is this a method applied to a single image, or is one training a model from a set of images.  I suspect this is considered so obvious to the authors that they forgot to mention it in the text (unless I've missed it).
565	Weakly-supervised Biomechanically-constrained CT/MRI Registration of the Spine	What is the likelihood this method will extend to other areas of the body, or general CT vs. MRI registration?	"1, The regidly panelties L_{rigid} includes four different variants: rigid dice loss, rigid field loss, properness condition and orthonormal condition. What's the basis of selection for your different experiments? 
2, The rigid transformations seemed to be calculated every iteration during the training which is inefficient. I prefer to get information about how you solve this.
3, You only showed the visualizations of the warped labels. However, the visualizations of the warped images and the DDFs are much more importent."	The paper will benefit from a more in-depth analysis of what loss term might be the most appropriate for the vertebrae registration. And although the title includes 'biomechanical', the actual implementation does not include much biomechanical component by only assuming the rigidity of the vertebrae structures. The work will be more exciting if some true biomechanical modeling can be introduced.
566	Weakly-supervised High-fidelity Ultrasound Video Synthesis with Feature Decoupling	"Quantitative results for the keypoint detector are essential to provide a better understanding  of their effects on the final results,
More clarification about VGG loss should be given; ""For texture part, due to unavailability of its ground truth, we adopt feature reconstruction VGG loss [5] to constrain the similarity of driving frame and the final prediction with texture information.""   It seems from eq 5 that this loss is directly applied to derived and source images with the hope that it can improve the texture output.
Contrastive loss can also be explored in the future work"	There is no explicit definition for the model variants: ours-P, ours-PT, ours-PTG.	"(1) Add more explanations to make the paper self-contained.
(2) Considering more ablation studies in the future work"
567	Weighted Concordance Index Loss-based Multimodal Survival Modeling for Radiation Encephalopathy Assessment in Nasopharyngeal Carcinoma Radiotherapy	"(1) It is recommended to supplement and compare with other current models;
(2) Regarding the processing of image data, I hope that the author will introduce more details;
(3) Experimental comparison in public datasets using the model proposed by the authors."	Is the CE in Table 1 weighted or not? What's performance if simply use weighted cross entropy for the data imbalance?	"This is a very well written and organized paper. Some suggestions to make it even better:

Replace ""synthesizing"" in section 3.2 with ""using all"" as you are not synthesizing data but combining actual data.
Possibly add more information about how the image data is pre-processed before it is input to the model.
Add methodology to test for significant differences between the measures for the different loss functions."
568	What can we learn about a generated image corrupting its latent representation?	"The caption of Figure 3 should be 'corresponding tumor segmentation map (and) the uncertainty heat map, respectively'
refer to the weakness"	"The experiment and results in sections 3.2 and 3.3 are very clear and straight forward, and the conclusion is obvious. But the experiment in section 3.4 which validates that the proposed confidence score correlates with the quality of downstream task, is not straight forward and convincing. In Table 4, the absolute value of a correlation between the confidence score and DICE coefficient for the proposed method is up to 0.54, which indicates moderate correlation. These results can not support the conclusion ""This suggests that our method can be used most efficiently in cases where the images are generated well enough for the downstream task network to also perform well."""	Need to explain more details about how to relate segmentation mask with uncertainty maps.
569	What Makes for Automatic Reconstruction of Pulmonary Segments	"Introducing the definition of anatomy of pulmonary segments and dice score is not considered as novelty. There are several MICCAI papers introduced the pulmonary structure in the last two years.
The experiments need to include more comparison with more recent methods.
The ImPulSe network uses less parameters comparing to UNet. but it might have a bigger feature space. I am a little concerned about the efficiency.
The boundary of the pulmonary segments should have a clear anatomical definition. A geometry reconstruction is needed before the segmentation output can be considered in the surgical treatment.
The labeling the pulmonary segments should be a important part of the task, since there is not a lot open resource. The labeling procedure and the quality of the labels are expect to be introduced in detail."	The idea of doing interpolation on feature space instead of in the segmentation space is interesting and has value. However, the experimentation performed are not a fair comparison between the two methods due to the nearest neighbour interpolation in output space and trilinear in the feature space.	In this study, the authors annotated pulmonary segments, bronchi, arteries and veins for 800 CT scans. Making the data open access or organizing a challenge will definitely enhance the impact of this study.
570	White Matter Tracts are Point Clouds: Neuropsychological Score Prediction and Critical Region Localization via Geometric Deep Learning	The Lossfunction and Table 1 should be fixed, or explained in more detail. All in all one should explain the choice of tuned hyperparamters more deeply and discuss the applied method and its limitations in more detail.	"How to keep the smooth information of a continue streaming when representation with a point cloud?
Unclear why only left AF be used in this work. It is necessary to use both AF across hemisphere, or have a enough reason.
Is there any way to automated select the best weight of difference loss w, and the number of input points?
In table 1, the difference in MAE across methods looks very small, but significantly improved in r. It is very interesting. Needs to include more discussion about that."	This work may be improved by presenting more quantitative results on the critical region localization or considering a weaker conclusion about it.
571	Whole Slide Cervical Cancer Screening Using Graph Attention Network and Supervised Contrastive Learning	"In section 2.1, the sentence ""applying them to the classification model"" should be ""applying the classification model to the selected patches"".
A sensitivity analysis on some key parameters should be provided, e.g., the number of the representative patches detected by RetinaNet and the number of patches used to construct graph.
Would it be better to choose the top K and bottom K patches from all patches in a WSI than from the top 200 patches detected by RetinaNet. In a positive WSI, all the top K and bottom K patches may contain lesion cells, so it seems inappropriate to force the graph representations of the top K and bottom K patches to be far from each other.
The authors should also directly compare their method with the whole slide screening methods in ref No. 2 and No. 20.
In section 2.1, the sentence ""applying them to the classification model"" should be ""applying the classification model to the selected patches"".
A sensitivity analysis on some key parameters should be provided, e.g., the number of the representative patches detected by RetinaNet and the number of patches used to construct graph.
Would it be better to choose the top K and bottom K patches from all patches in a WSI than from the top 200 patches detected by RetinaNet. In a positive WSI, all the top K and bottom K patches may contain lesion cells, so it seems inappropriate to force the graph representations of the top K and bottom K patches to be far from each other.
The authors should also directly compare their method with the whole slide screening methods in ref No. 2 and No. 20."	The manuscript lacks sufficient qualitative and quantitative evidence. Technical novelty is none or limited. The authors should validate their results by pathologists.	"(1) Add details of data preprocessing about WSI, big patches and cell patches 
(2) Final loss consists of cross-entropy loss and supervised contrastive learning. The experiment with only cross-entropy loss should be added for comparison."
572	Why patient data cannot be easily forgotten?	"Part of the problem of the presented application is the limited data, which makes almost every case an edge-case. It is unlikely that clusters emerge on high-dimensional imaging data when only a hundred samples are available. Neural networks will likely overfit to such data (they may still interpolate well in-between), but that means that the hypothesis of separating edge cases from cluster cases may not be valid. A more interesting application for this to be tested would have been image classification trained on 100k+ images (e.g., chest X-ray disease detection).
I am not an expert on privacy and newer regulations such as GDPR. However, the described use case of an individual whose data would need to be removed from a trained ML model seems unlikely to be a legal requirement. The so called 'right to be forgotten' does not seem to apply here (see https://gdpr.eu/right-to-be-forgotten/). I would think that a trained ML model falls under the exemption stating ""The data represents important information that serves the public interest, scientific research, historical research, or statistical purposes and where erasure of the data would likely to impair or halt progress towards the achievement that was the goal of the processing."" With this in mind, while the paper is thought provoking and stimulating, I am unsure about its practical relevance. In particular, ML models for production are typically developed on (fully) anonymised data where data privacy regulation such as GDPR does not apply."	"This is a well-written paper describing and addressing a very important aspect for AI in healthcare applications. The paper is easy to follow and the findings are relevant to the MICCAI community.
I would have liked to see a more in-depth discussion on the implications of the findings of the paper. Is forgetting patient data a realistic method to deal with withdrawn patient consents? What is more important: respect data protection or ensuring model performances? What is the connection to differential privacy and can we learn something from it?
If I interpret Table 1 correctly, a patient's data is forgotten if the accuracy on this data is 0.0. I would assume that the model does not have any knowledge about this data of the classification accuracy is 0.5 (random). An explanation/definition is missing here.
A minor comment is that the classification error is defined too late (only in the caption of Table 1). It should be introduced earlier in the text."	"This paper presents a framework for the patient forgetting problem that is an intuitive extension of what has been proposed in the literature, however the empirical validation is insufficient to illustrate it practical usefulness.
Detailed recommendations for improvement are provided in the ""paper weaknesses"" part of the review."
573	XMorpher: Full Transformer for Deformable Medical Image Registration via Cross Attention	"It is interesting to classify the existing registration methods as Fusion-first, Fusion-last, and cross-attention-based fusion. The authors claimed that the first two categories of methods failed to find the one-to-one correspondence between images. However, existing deep registration models, such as the diffeomorphic variant of the VM, were feasible to find the invertible registration fields.
The transformer has been applied in image registration and correspondence tasks in the last two years, such as [4, 20]. The images patches from both the fixed and moving images are fed to the transformer, so the existing methods [4, 20] did not just compute the relevance in one image as claimed by the authors.
It is unclear how to compute DVF \phi by the proposed XMorpher. As shown in Fig. 2, the Concat+Conv operations were required to compute DVF. Does it mean a CNN-based decoder was used to compute the registration field?
In Fig. 3, all compared methods achieved reasonable deformation fields with the organ contours consistent with the fixed image. We noticed that the proposed approach achieved smooth organ boundaries. It would be helpful to discuss the scheme in the proposed approach contributing to the smooth boundary.
It is unclear how to apply the XMorpher on the existing registration network of the VM or the PC-Reg. The VM utilized the U-net-based framework with the convolutional encoder for feature extraction and the decoder for the DVF. It would be helpful to discuss whether the VM-XMorpher used the CNN-based feature extraction and the field inference."	"1)	I like the way the authors represent in Figure 1, but the authors should provide more evidence / reference to support what they claimed.
2)	In Fig. 2, did you predict two deformation field? or just one? If you only predict one deformation field, you must fuse the moving-fixed features in up-decoder block, thus, your method should not be named X-shape, it actually is Y-shape. If you predict two deformation field, you cannot denote ""moving and fixed image"" as your inputs. Please give more details here.
3)	The proposed XMorpher seems like require a huge training dataset, thus, the generalization seems questionable. Can you give more quantitative number like minimum required training dataset, network parameters number, and computing efficiency(FLOPS)? Besides, I think the large deformation might be limited by window size if you remove the affine transformation?
4)	In Table1, why the model without cross block achieved the best performance on Jacobian metrics? Please discuss. Besides, your network is parallel which utilize cross block to fuse moving-fixed features. If you remove cross block, how does the model achieve registration task? Like comments 2), you fuse moving-fixed features in up-decoding block? If yes, your network is in Y-shape."	"1, The figures (especially Fig.2) in paper need to be more clearly, and I list some advices in follows:

The texts used in Fig. 2 is too small to read that I have to zoom in 300%. You should reorganize the layout your figure.
Both in Fig.1 and Fig.2, the moving images and the fixed images seems are two different modality which is a little bit confusing.
In section 2.2, you mentioned input 'b' and input 's', however, they are missing in Fig.2. Besides, the notations used in the whole text should be clearly defined and consistent, for example, typo error that in Section 2.3 'and thus S_ba has size of nxa  hxb w xg  d', where I think 'S_ba' might be 'S_se'.
Four figures in Fig.4(b) are lack of explanations. What are these pictures? Fixed image? Warp Image? or ...? And, what the arrows and the overlap map represent?

2, In your experiments, you applied the XMorpher as backbone in two CNN-based frameworks, VoxelMorph and PC-Reg. I don't really understand this because you said XMorpher is a full transformer structure, and I hope to get more detailed information about the implementations. Also, you did not introduce how you acquire the final dvf in your article which is important. 
3, You claimed that your method is more efficiency, please report your inference time.
4, There are many learning-based methods fuse features in multiple level and predict the dvf in multi-scale manner. The works will be much more solid if you can make some comparisons with these methods."
574	Y-Net: A Spatiospectral Dual-Encoder Network for Medical Image Segmentation	"-- Adding couple of lines on why x_g = 0 for the initial block will be helpful.
-- What is the number of learnable parameters in the proposed framework? The spectral domain part is proportional to the dimension of the input image?
-- Devil advocate: What could potentially downplaying the performance when skipped connection are added between the the two domains?"	"Comments:
I): The experiments are not sufficient. Only U-net is included in the experimental comparison, which makes me doubt whether it is an ablation study or methods comparison. 
II): The dice score has many advantages on training a model by materializing it as a loss. And it could be utilised as an excellent metric in segmentation. However, a network trained by dice loss should be evaluated by not only dice but also other metrics. For instance, M-IOU also is a popular segmentation metric which could be involved in the experiments.
III): The work lacks novelty. FFT blocks can not be considered as a newly implemented technique for segmentation, while there is no other claimed contribution for methods."	"The computation cost is not analyzed. It is better to provide the computation cost for each method in this paper.
-The author mentioned that spectral encoder can make the model pay more attention to high-frequency information. How to prove this and whether the corresponding features can be visualized?
-If Spectral Norm is replaced by a general convolution operation, what will be the impact on the results?"
