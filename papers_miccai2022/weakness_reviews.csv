id	title	review1	review2	review3
001	3D CVT-GAN: A 3D Convolutional Vision Transformer-GAN for PET Reconstruction	"The writing and organization need to be improved. Specifically, the methodology can be simplified while the visual comparison of ablation study can be appended.
Except for the number of parameters, the FLOPs can be shown for further evaluation of model complexity."	"There are so many transformer-based methods. And the method is not considered to be novel anymore.
The performance improvement is subtle.
The dataset is too small."	"Although the proposed method outperforms other methods in terms of PSNR, SSIM and NMSE, there is no too much difference in the visualization results as shown in Fig.2 for method of 3D-cGAN Transformer. It's better to provide some zoom-in regions to highlight the better result got by proposed method.
Statistic evaluations such as PSNR and SSIM may not be sufficient enough for proving the reconstruction effectiveness in clinical aspect, this paper can consider adding reader study for clinical diagnosis of NC and MCI using either GT SPECT or reconstructed SPECT respectively.
There is no explanation why the model size of proposed method is smaller than other methods."
002	3D Global Fourier Network for Alzheimer's Disease Diagnosis using Structural MRI	"Details about the competing methods are missing.
There is no time comparison.
The relevance of the shapely analysis is not clear because it is not done for other methods, for example CNNs."	The  idea of frequency filter is already proposed and used (ex., https://ieeexplore.ieee.org/document/7504251, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6485015/).	"(1) Motivation is not clear. Why the global information in the frequency domain is useful, What if just enhence the global information, as done by multihead attention?
(2) Not report results on MCI classificaiton."
003	4D-OR: Semantic Scene Graphs for OR Domain Modeling	"As for the clinical role experiment, I wonder why the authors did not embed the role into the scene graph where the relations will be like ""Assistant surgeon assist head surgeon"" instead of ""human1 assist human2"". What is the benefit of a role-agnostic scene graph?

The human poses and object boxes in the new dataset are automatically generated by the Kinetic SDK. What is the accuracy of these generated annotation? How does this source of error affect the evaluation of this paper?"	"The paper lacks details about how realistic the surgeries included in the dataset actually are. This is important for the reader to get an understanding of how challenging the clinical role prediction task is from the data. Also, since the surgeries are simulated, it is not clear if the most important parts of the knee replacement procedure are actually included in the data, namely everything that happens after the knee incision (placement of cutting guides, bone resections, knee joint assessment, trial implants, cement preparation). Furthermore, how different are the 10 recorded procedures? Knee replacement surgery can be performed through different clinical approaches depending on the desired alignment (mechanical or kinematic), the type of implant, etc. Are all these differences represented on the dataset? Or is it the same surgical approach repeated ten times? Are the actors in the videos different from one video to the other? Has the OR changed somehow from one video to the other?
Moreover, the size of the test set seems small for an evaluation of the performances of the proposed approach, especially since we do not know how different each of the surgeries are. Do you believe your approach would generalize well to variations in the surgical workflow?
The evaluation metrics and results are difficult to interpret and understand. More explanation should be provided. What is the purpose of the ablation studies that are presented?
The discussion section is short and as a reader you feel like more can be discussed about this work. Especially about how the approach would translate to a real clinical setting. No recommendations for this are provided or about how would this approach be implemented in a clinical setting."	"Limited technical contributions
It would be interesting to get the performance by perturbing pose estimation prediction.
It would be interesting to see on camera view layout
the paper does not include any information on the calibration. It would be interesting to provide some information on how the cameras are calibrated."
004	A Comprehensive Study of Modern Architectures and Regularization Approaches on CheXpert5000	No significant weaknesses to comment on.	"Experiments are reported only on one dataset (CheXPert) and only in the small data regime (5000 training images)
The paper is somewhat rushed and some parts need to be clarified"	Lack of novelty - there is no major novelty either technical or on data contribution.
005	A Deep-Discrete Learning Framework for Spherical Surface Registration	The paper presents promising results. Yet, some points are not clear; in particular, the motivation of using rotational equivariance seems weak. Please see my comments in the section below.	"It is not clear whether diffeomorphism of the spherical deformation is guaranteed, as in Spherical Demons. It may be so if the local displacements are constrained to be small enough and enough iterations of fitting are performed, but this is not stated, and in description of the coarse-to-fine architecture it seems like the coarse and fine level models are only executed once per pair of input spheres. So it seems like it would be possible for the deformation to cause folds on the surface. I wasn't sure whether this is captured by the strain variables - they all are plotted on the log scale so one has to assume there were no negative values.

The overall contribution is somewhat incremental."	No major weaknesses.
006	A Geometry-Constrainted Deformable Attention Network for Aortic Segmentation	"There are some weakness in this paper, however :

Who did the manual segmentation ?
Making an evaluation according to the part of the aorta is a good idea, but it could be interesting to have the same kinds of result according to the pathology
The Hausdorff distance is a metric to detect the outliers, and only considering the 95% of the Hausdorff distance is a non-sense, or something to hide bad results. Please provide the ""true"" HD
The standard deviation must be indicated in the Bland Altmann plots in the figure 5.
The diameters indicated in the discussion are not good. According to the Bland Altmann plots, the maximum error is higher than 0.98 mm
The units are not specify for the HD"	"The introduction is rather long for the format of the paper, but while it brings attention to some problems of cardiovascular radiology, it does not provide much information on the current state of the art, its limitations and why the reasons behind the architectural choice. 
Perhaps recent reviews on this topic can support the authors in editing the introductory part [1,2]. I would also recommend the authors to split the introduction from the related works.
In 2.1, ""h"" is not defined. Also, what do the authors mean with ""base fact""?
While comprehensive, I find the method description (section 2) to be too focused on the mathematical description of some aspects, while other aspects are left without any details (e.g. ""At the end, a lightweight decoder is used to fuse multi-level features to
decode."" -> it is not clear what the decoder actually is and how it exactly works).
Dataset: Was there an ethical approval for the access of the 204 CT scans? Please report statistics regarding volume size and spacing.
Train and test: after how many epochs was the learning rate reduced?
Diameter meausurements were not compared against state of the art but only against ground truth.

[1] Fleischmann, Dominik, et al. ""Imaging and Surveillance of Chronic Aortic Dissection: A Scientific Statement From the American Heart Association."" Circulation: Cardiovascular Imaging 15.3 (2022): e000075.
[2] Pepe, Antonio, et al. ""Detection, segmentation, simulation and visualization of aortic dissections: A review."" Medical image analysis 65 (2020): 101773."	The experimental section lacks analysis in the discussion of comparison method.
007	A Hybrid Propagation Network for Interactive Volumetric Image Segmentation	The validation study is designed to automatically simulate user interaction scribbles. It's not clear if the method would perform as well with a human scribbler.	It is unclear how many interactions are used for results in Table 1. Is same method used for generating user interaction for all methods?	"My main concern is that I am not sure whether I can see such a complex approach easily deployed in clinical use.
It is an interactive method, but it still (1) requires networks training and therefore specific to the anatomies it has been trained for, (2) requires some computational power on the user-end to run those networks.
In order to be used in practice, I feel like an interactive method should be swift and lean. Here, there is no mention of the computational time of each prediction, or the overall time it takes an actual user (not simulated scribble) to segment in a satisfactory way the organ of interest, or a study on generalization to new structures.
The complexity of the method makes it difficult to reproduce; for instance it uses both a 2D and a 3D network, the training has 4 different steps where various parts of the network are trained, etc.
The method could have been better explained. I don't understand why one part is in 3D and the other in 2D. Figure 1 is overall a bit confusing: the order of the different operations is not clear."
008	A Learnable Variational Model for Joint Multimodal MRI Reconstruction and Synthesis	"(1) The Introduction is not well organized. It seems more like stack the review of compressed sensing MRI and MRI synthesis without presenting their link. What is the value of study MR image synthesis under the setup of partially acquired MRI measurements of other modalities. In my opinion, this is the key issue to be addressed to show the value of this paper.
(2) Following the first concern, is this paper the first work to explore the joint MRI reconstruction and synthesis? If so, it should be stressed since it is a major contribution. If not, related works should be discussed.
(3) Another major issue is that the rationale or analysis should be provided on why the third term in Equation (1) is a reasonable design? Citing related papers, visualized analysis or explanations should be useful.
(4) The technical details should be better organized and concise to make the paper more readable and understandable to MICCAI community."	"One weakness is that the paper doesn't clearly describe how the k-space data was generated, since, as far as I know, the BRATS 2018 dataset is not for raw-kspace, please carefully describe the data processing steps. Also, please check carefully on this paper: Implicit data crimes: Machine learning bias arising from misuse of public data, which shows some potential data crime for MRI reconstruction.
The undersampling strategy for this paper is radial sampling, and 40% sampling rate is kind of considered as a ""low"" acceleration factor. Would be nice to elaborate more on how does the undersampling matters and what happened if the sampling is cartesian."	This paper proposes to use the idea of meta learning to optimize the hyperparameter, but it does not show that the parameter is the main limitation affecting the problem, and there are not enough experiments to verify the advantages of the optimized parameters over the general hyperparametric design.
009	A Medical Semantic-Assisted Transformer for Radiographic Report Generation	"1.The article only assessed language fluency, not Clinical Accuracy Performance, and we cannot judge whether it has clinical significance.
2.Does the CLIP module work when testing the model? If it works, where does the report that matches the image get it; if it doesn't work, how to extract the regional features of the image.
3.Part 2.2 and Part 2.3 Are the same as the content in Attention(), but the results are different.The loss function calculation formula in section 2.3 does not explain how pk is obtained.
4.The model proposed framework in the paper does not correspond very well to the formula proposed in the article. The inputs and outputs of the modules in the framework are not clear, and the corresponding modules are not labeled for what they do.
5.The MSA module in part 2.1 is very confusingly written, and it is difficult to understand what it means if you have not read the reference [13,19]."	"1.Fig 1 is not clear. I don't know the meaning of the part of module near the input image. Because the input and output(e.g. Q,K,V) are not clear.

Part of Memory-augmented Sparse Attention is similar to [1], but it is not mentioned in paper.
Medical Concepts Generation Network is common used in this task, such as [2], although the supervised words are not same.
Lack the experiments on IU X-Ray dataset, which is common used in this task.

[1] Meshed-Memory Transformer for Image Captioning. CVPR 2020
[2] Visual-Textual Attentive Semantic Consistency for Medical Report Generation. ICCV2021"	I found it very hard to follow notations used for defining various variables and weights of a network. It would be helpful to include a chart for it in the supplementary.
010	A Multi-task Network with Weight Decay Skip Connection Training for Anomaly Detection in Retinal Fundus Images	"The technical contributions are not sufficient for a good MICCAI paper. The intuitions brought by this paper are limited, and there are many aspects that the authors could study further. (1) In Eq. 1, weighted feature fusion has already been well studied in the vision domain (a similar study can be traced back to ""Identity Mappings in Deep Residual Networks"", He etal, ECCV2016) and many following U-Net based papers have tried such weighted skip connections since then. However, the only interesting part is that \alpha is gradually decreased in this paper instead of being learnt as others. In this way, the authors should spend more efforts to study the behavior of such decreasing schedule: linearly scheduled, cosine schedulerd piece-wise scheduled etc. (2) The HOG prediction in this work seems like a direct combination of [13] and [17]. Although there is indeed a new attempt (using different resolution HOG as GT), it is more like a training trick instead of a major technical contribution.

Experiment on one single dataset does not seem sufficient to fully evaluate the proposed method.

Since the primiary contribution claim is the network with skip connections, the authors should compare with more SOTA skip-connected networks (e.g. Attn U-Net, BiO-Net etc.) in Table 1. Also, these works should be cited in the related work section."	"a)  In ""Introduction"" Section, the description of the motivation for the proposed auxiliary HOG prediction task is unclear and insufficient. The authors only stated the effectiveness of HOG prediction for self-supervised representation learning, without explaining the correlation between this auxiliary task and the main anomaly detection task. 
b)  In the comparison experiments, except Sparse-GAN and Proxy Ano respectively published in 2020 and 2021, the other competing methods are somewhat outdated. Comparing with these methods can not fully demonstrate the advancement of the proposed method, though it achieves the best performance. In addition to methods designed for anomaly detection, some classic medical image reconstruction framework and method should also be reviewed and re-implemented as comparison methods."	"1.""The skip connections at higher levels tend to mislead the model to bypass the lower levels of features and essentially learn an identity mapping function.""How do you come to the conclusion that the model bypass the lower levels of features? Can you give any theoretical or experimental support it?
2.This paper proposes a weight decay skip connection training strategy. You've not give any references. What is the purpose of the strategy? What is the advantage of the weight factor a?"
011	A New Dataset and A Baseline Model for Breast Lesion Detection in Ultrasound Videos	The use of the method for dtecting a lesion when a lision is known to be present is not clear.	For a conference paper, I do not see a major weakness. For a journal paper, more explanation of the cost functions would be nice.	"Use of private dataset
Lack of intuitive reasons for the proposed structure design, especially the proposed inter- and intra-VFB modules"
012	A Novel Deep Learning System for Breast Lesion Risk Stratification in Ultrasound Images	"There are several parts in the paper that were unclear to me, such as the derivation of the soft labels, handling multiple images by the same patient, and handling inconsistencies between BI-RADS and pathology.  See detailed comments below.
It is unclear why different tests were done on the 3 datasets. The results were much stronger if the proposed architecture was showing superior results in all the three datasets - using the same tests. In other words, I would expect the comparison to state-of-the art and ablation study to be conducted in the three datasets that were used in this study."	"Only minor weaknesses:

there are 3 different techniques used in this paper, each of which is reasonable, but none of which are particularly novel or explored in great depth. they also seem to be generally applicable to many multitask learning problems (beyond medical vision), so it would be interesting to know how they have been used elsewhere
clinical utility is maybe unclear. Comparison to radiologist performance and analysis of failure cases would help on this front. It may be helpful to have a sense of how well radiologists can predict whether a lesion will be malignant, as well as inter-reader agreement. Is this framework also useful for mammograms?
I'm not familiar with multitask learning but I suspect there are stronger baselines than just ""train on both tasks simultaneously"""	"The equations of the proposed loss functions are not clearly specified, leading to the following confusions:
In equation-1, it is unclear if the SLB, SLP are calculated per lesion. It is confusing while reading as the equations depict that the soft-labels aren't a function of neither the lesion nor the image. This needs to be clearly stated.
In equation-3, the loss always sums up to a negative value? As the terms are all probability values in the interval [0, 1]. It is more adequate to call it a reward than a loss.
In eqn-1, its unclear why Ni and Nj are different, shouldn't they be equal? This suggest that images of the same lesion have different BI-RADS scores?
In equation-3, it needs to be clarified that the benign and malignant probability sum to one i.e. \(p_B + p_M = 1\) as the check for benign is \(p_B > 0.5\)."
013	A Novel Fusion Network for Morphological Analysis of Common Iliac Artery	"I feel that there is little methodology advance from a technical point of view.
Although I found the work interesting and potentially very useful, my major concern is on whether analysis morphological algorithm can ""obtain access diameter, the minimum inner diameter, and the maximum curvature of CIA with arbitrary shapes."" Otherwise, I think the experiment is far from validating this strong claim."	"The comparative experiment is not sufficient and lacks the comparison and analysis of the number of parameters and computation (e,g, FLOPS)
There were some minor mistakes."	The design of the method mainly focuses on the combination of CNN and Swin Transformer, which is weakly related to morphological analysis of the common iliac artery.
014	A Novel Knowledge Keeper Network for 7T-Free But 7T-Guided Brain Tissue Segmentation	"1) Novelty is limited. Knowledge transformation and synthesizing 7T from 3T images have been used a lot in the literature.
2) I expected to see comparison results with methods that ""synthesizing 7T-like image first and then conducting segmentation"" to justify the proposed method.
3) In Table 2, the authors only showed the effectiveness of plugging 7T-like features in the segmentation frameworks, e.g., the 3D UNet framework. However, their proposed method has used extra data to train the teacher network compared to 3D UNet and DSC improvement is not significant (around 1%)."	"Figure 1(c): what is the difference between Input 7T images, Reconstructed 7T images and Target 7T images? More information are required to understand how the teacher network works.
Section 2: ""Through two-stage KD methods, KKN learns to infer 7T-like representations from a 3T image using a paired 3T-7T dataset {X,Y}"", why paired 3T-7T dataset is needed during training (Figure 2(b)). This conflicts with authors' statement.
Figure 3: By comparing the result of the proposed method and ground truth, I do not think the result has 7T representations. Only the intensity distribution is similar with 7T images but it does not have tissue details that 7T images have."	The author claims that existing methods lack applicability and generalizability on datasets not including 7T images because they require pairs of a 3T image and its corresponding 7T image for training. But in this manuscript, the author still needs paired data to train the teacher model. How to explain that? Besides, I don't know what the practical application is. In my opinion, the task is to segment brain tissues, why don't you segment those directly by developing a better approach? Although the performance will be better if you translate a 3T image to a 7T image, you need 7T images to train the KNN, which is almost not available. So I think the practical application is limited.
015	A Penalty Approach for Normalizing Feature Distributions to Build Confounder-Free Models	"Related work on learning confounding/bias-free representations is very limited. The authors only refer to adversarial learning schemes (refs. 11,18), but a large amount of literature exists beyond that. Most importantly, the authors should refer to other works that are based on orthogonalisation too: (i) Tartaglione et al. ""EnD: Entangling and Disentangling Deep Representations for Bias Correction"". CVPR 2021. (ii) Neto. ""Causality-aware counterfactual confounding adjustment for feature representations learned by deep models"". (iii) Liu et al. ""Projection-wise Disentangling for Fair and Interpretable Representation Learning: Application to 3D Facial Shape Analysis"". MICCAI 2021.
The smallest batch size that has been studied is 80, however for many tasks, in particular involving multiple 3D volumes, a batch size of 80 is infeasible. Studying very small batch sizes (<20) would be helpful to understand whether the proposed approach would still be effective in this setting."	"Theoretical analysis of PMDN is not enough. In section 2, PMDN seems to be using a neural network to replace the linear regression and simply by adding a loss term during training. The contribution of this method is not enough unless further analysis, including convergence rate, theoretic bound of the error, etc., is developed.

Experiment baseline is weak. There are many other method (from Fairness deep learning) can be performed and compared with the proposed method. For example, Canonical Correlation Analysis (CCA), and Renyi Fair Inference. This aspect is not considered in the current paper, nor does the author discuss the pro and cons of this aspect."	"According to f - Mb, the metadata should have the same dimension as f. But the representation of different metadata has different dimensions: acquisition site  is one-hot encoded, the age is z-score and the sex is a binary number.  It is not clear how to feed the metadata into the model.

As beta is trained based on both the classification loss and the penalty loss, it is not clear if Mb contains all and only metadata related information.

Figure 3 only compare the results under a small batch size equals to 80 which is unfavorable for MDN. For fair comparison it is better to also compare the tSNE results under other batch size.

There are some typos, such as ""... the matadata of each group separately and report ...""

This paper has no keywords."
016	A Projection-Based K-space Transformer Network for Undersampled Radial MRI Reconstruction with Limited Training Subjects	"The description of the method is, at times, hard to follow.
The authors could have  used cross-validation to evaluate their  results."	The real and imaginary part was stacked together in a big array for network input. Should they be separated for two networks and combined? If this is the better case, a comparison is needed.	The model was compared against baselines like U-Net and squeeze and excitation network that were not designed for this problem. A better comparison would be to compare against a model like XD-Grasp. Without such comparison, it would not be possible to determine how well the proposed model performs.
017	A Robust Volumetric Transformer for Accurate 3D Tumor Segmentation	"1) In the introduction, the authors mention that the proposed method has fewer model parameters and lower FLOPs compared to existing method. While from Fig.1, there is no information about the FLOPs, the number of parameters and the FLOPs are not necessarily the same thing.
2) It seems strange that the authors did not mention any data augmentation in the experiment setting. If no data augmentation is used for all the comparison, this won't be ""fair"", given that data augmentation is a common technique that people use to trained DNN models nowadays. With data augmentation, the gap between different methods could be smaller because almost all the methods will become better.
3) Transformer usually needs a large dataset for training, when there is only limited data available, the proposed method may not be better than CNN, which could limit the usefulness of this method."	"1) Ablation study is missing. I believe experiments which show the increments in performance obtained because of the 3D shifted windows, K,V skip connections would be useful.
2) Not much novelty in terms of encoder.
3) I am still surprised by the drop in complexity. Structure wise I do not see anything much different that would actually reduce the complexity as reported in the paper. I read the supplementary where the authors discuss about the same. I think more clear discussion about the complexity reduction and the reason behind the same should be present in the main paper."	"The authors spent a lot of content introducing the existing work, although these structures are parts of the network. For example, the shift window,  relative positional bias, patch merging, and patch expanding are very famous existing work, but the authors focus on these contents as a subtitle.
The author declared that they propose a convex combination approach along with Fourier positional encoding, while there are few detailed introductions about the Fourier positional encoding in this paper."
018	A Self-Guided Framework for Radiology Report Generation	"(1) Many mature algorithms are used in the article, and the innovation is not outstanding enough.
(2) How are multiple parameters in the network determined? Some clarification should be given."	"*	The paper lacks motivation on why the proposed approach is good for medical report generation. I understand the approach don't require image-level disease labels, but the main building blocks of the models are not necessarily trained for medical domain. They are mostly pre-trained on natural images and just applied on medical data. 
*	The experiments don't evaluate performance on preserving negative mentions. A major differentiator between medical report generation and image-captioning on natural images is ""negative mention"" in medical reports. Two sentences can have high similarity score (calculated based on word overlap), while having an opposite polarity. For example, ""Lungs have pleural effusion"" vs ""Lungs have no pleural effusion"". 
*	The authors claimed to ""extract fine-grained visual features associated with text"" is supported by qualitative experiments in Figure 3. That is not enough to support the claim.
*	The heatmaps in Figure 3 not localized and covers most of the image. For ""no"" the heat map is highlighting upper lobe. It's not clear how this is a good result? The ""no"" in the sentence is in context with pleural effusion and pneumothorax. Both diseases are in lower lobe regions as highlighted in the last heat-map.
*	The method section of the paper reads more like putting different blocks together and lacks motivation on the need and the design of those blocks. For example, knowledge distiller is required for unsupervised clustering of the reports, but why it is designed to have a Bert based embeddings? Are these embeddings better than other methods for medical reports. Does fine-tuning Bert on medical reports would help in getting better domain-specific embeddings? Why was dimensionality reduction used? Why not cluster the reports using the report embedding? Is there some transformation that UMap can learn but not Bert? Why HDBSCAN clustering was used? How to define number of clusters? How to make sure the clusters are distinct enough?"	The self guided framework is a novel and useful approach to overcome some of the described challenges, however few questions remain.
019	A Sense of Direction in Biomedical Neural Networks	The authors failed to convey clearly and precisely their methodology. A lot of details are missing to fully understand the approach.	"on the tested datasets, there are only minor differences between MASC and the presented method which raises the question whether the addressed limitations of MASC are major
the new formulations to handle scale differences do not result in significant performance improvements, at least not on the used dataset"	"It is claimed that the original MASC used the response shaping function which authors found to be unstable in some cases, but this has not been verified or clarified in which scenarios. From the experiment results, it is hard to say that MASC is unstable (MASC even performs slightly better than the proposed G-MASC on CHASEDB1).

The difference between the original response sharing and the proposed calibrated response sharing is unclear to me. The original one calculates the response across different filters w, while the proposed one seems successively update M with learnt kernels W. What is the benefit brought by the new way?

In the ablation study, the performance degrades only when IMF and PoRE are both removed. With either one, the performances are close to the final performance. This raises a question - Do IMF and PoRE substantively or functionally overlap?

I think this paper requires a clearer comparison and analysis between MASC and G-MASC, both theoretically and experimentally. For example, why does G-MASC only show considerable improvement over MASC on MoNuSeg (the authors claim G-MASC work better when using fewer directional kernel sets and smaller kernels MoNuSeg, but this is an observation rather than an informative analysis)?"
020	A Spatiotemporal Model for Precise and Efficient Fully-automatic 3D Motion Correction in OCT	"There were no accuracy comparisons with competing methods, just time comparisons. Although estimations are provided according to the characteristics of the diverse methods, the lack of hard numbers decreases the confidence on the provided results, and it does not really facilitate obtaining an objective comparison by the reader. While this is justified and explained by the authors, one wonders if one or some of the competing methods could have been replicated, or the authors contacted to run experiments in the utilized dataset.
The dataset is not available and there is no word on if it will be made available in the future. While its characteristics are described in good detail, having a common dataset with ground truth metrics would facilitate that future methods can be compared with the presented one."	"What is the meaning of right part in the equation (1)?
In the Fig.3, how did authors take the top row two images almost at the same position, as the eye is moving? If the positions are different, how can you get the final OCTA.
Would you further explain the eye motion trajectories?
The runtime comparison is shown in Table 1. The authors reproduced the comparison algorithms and gave out the results based on the same GPU?
The references of Zang17, Makita21 and Chen21 in table 1 are missed.
How are the results from other diseased patients?"	"missing reference of Table 1
lack of enough comparison with more methods in terms of accuracy"
021	A Transformer-Based Iterative Reconstruction Model for Sparse-View CT Reconstruction	"1)	In section 1, the authors indicated that one of their contributions was replacing back-projection with FBP to improve the performance of network. However, many previous works, e.g., AirNet, have already demonstrated the efficiency of FBP operator.
2)	In fig. 4, the PSNR of FISTA-Net was higher than TGV, while the SSIM was lower. Could you please explain the contrary result?
3)	In section 4, the authors indicated that independent iterations made the network extract only shallow features. Since the proposed network was built by unfolding iterations, these iteration blocks were cascaded to form the whole network. Thus, the network was supposed to be capable of extracting both shallow and deep features.
4)  The manuscript does not give much discussions about why the method could improve the results, and how could the method be further improved."	"a. The filter of FBP is fixed with RL filter, a learned filter may be better.
b. The training is supervised learning and unsupervised training will be more clinically valuable.
c. More task-based evaluations will better demonstrate the performance."	"1 2d and small dimension images are reconstructed. Not sure if the method can handle the real and large 3d data volume. 
2 using FBP to replace the backprojection destroys the conjugate operation requirement on AAt. The authors are expected to perform careful evaluations.
3 As shown in Figs 4 and 6, subtle image quality improvement is observed. The real impact of the work is thus doubleful."
022	AANet: Artery-Aware Network for Pulmonary Embolism Detection in CTPA Images	"1) The network was pretrained on a large-scale dataset LUNA16. As the LUNA16 dataset is for lung nodule detection, the pre-training processing is not straightforward to understand. 
2) A challenge dataset CAD-PE was used. The authors made extra annotations (pulmoanry artery) on the public data, which was used as auxilary class for PE detection. The improvement is noticiable, but it might be unfair to compare with others participants of the challenge
3)  Based on the network prediction, post-process (morghology closing) is used after threshold. How much does post-processing improve the performance."	"At the end of the first paragraph of the introduction section, the author only stated that the high false positive PE diagnosis was partly caused by doctors, and did not discuss other reasons in detail, such as the impact of the gray, size and other characteristics on the CTPA images.
At the end of the third paragraph of the introduction section, the data set mentioned by the authors should be cited.
In the AANet of the Methods section, the authors did not explain the role of using different numbers (two or three) residual blocks, the difference between stdConv and Conv, and the role of fusion multiscale modules for PE segmentation.
There are too few method measures to confirm the accuracy of the method.
In the results section, the PE 3D or 2D segmentation results should be compared with the ground truth.This method should be compared with other methods.
The discussion and conclusions are insufficient, and more should be discussed about its advantages over other methods and its possible future applications."	"The method itself requires more annotation than traditional methods, and the annotation (as shown in Fig 2) requires lots of work. Although authors mention that this network and weights could be used as a pretrained model for other tasks, it is still questionable whether this method could be applied to other similar diseases.
The study/experiment design could be modified to make it clear to readers. To make a meaningful comparison with all methods, it would be better if authors could mention whether all methods were pretrained with LUNA16. And although mentioned in the paper that citation [13] missed some small cases, I would still want to see how AANnet performs on the 80+ cases subsets (as mentioned in [13])."
023	Accelerated pseudo 3D dynamic speech MR imaging at 3T using unsupervised deep variational manifold learning	"The challenge in assessing these reconstructions is a clear definition of the requirement of the image quality and target spatio-temporal resolution that is necessary (adequate). These are directly tied to the speech task at hand and defining these constraints might enable the algorithm to optimize better
The comparison to earlier CS methods while important to show improvement do not represent the state-of-the art methods for accelerated spatio-temporal acquisitions such as dictionary approaches (atom based), including simultaneous multi-slice methods, etc. Given the niche application, the exploration of these methods may be limited to describing them in the discussion section for the reader's comprehension.

In future experiments (or in discussion), it will be interesting to visualize and interpret the latency vector in line with the speech task to enable an understanding of the generator's outputs."	"The proposed method requires subjective iterative optimization, and the number of iterations should be carefully selected to avoid overfitting
Effectiveness only demonstrated in limited number of cases"	"(1) From the perspective of method originality, this paper offers little novel idea, which seems apply generative manifold learning in this problem. The ""Methods"" section is not explained and analyzed in detail, which is the major component of this paper. Novelty should be addressed here.
(2) No objective evaluation is applied. If the ground truth is not available, some subjective rating scores could be used."
024	Accurate and Explainable Image-based Prediction Using a Lightweight Generative Model	As for weaknesses I would highlight the difficulty in reproducing the method. The authors do not mention the code to be made available and the mathematical derivation might be quite heavy for unfamiliar audiences.	"The weakness in the evaluation is that the leading method, SFCN, was not reimplemented and tested using the current method's computational pipeline; this paper just re-prints previously published performance numbers.  Differences in the random partitioning of training and test sets, as well as differences in preprocessing pipelines, could have accounted for the reported performance differences.
Another weakness is that claims in the paper are not well supported.  Pointing to publications showing deep learners applied successfully to huge image sets, the assertion is made that deep learners must have such large training sets to perform well.  Poor performance of deep learners on small training sets has not necessarily been shown in these papers.
In the evaluation, we see the performance of the re-implemented methods, but not whether that performance is similar to that of prior publications with those methods.  In other words, it's not clear whether the author's reimplementations of these methods achieved the highest performance possible.
The argument that the proposed method is more readily interpretable than deep learners is not very convincing either.  It is possible to apply various schemes to ""trace"" the influence of each voxel through the machinery of the neural network, and thus get a quantitative sense of whether higher/lower intensity at that voxel tends to be associated with higher/lower age.
The argument that discriminative learners have more free parameters than generative ones is suspect.  Surely it is possible to assemble complex generative regressors and simple discriminative ones; people just choose not to.  The real point of this paper is not about generative vs discriminative, in my view- it is about complexity vs simplicity."	"The model proposes a causal forward model expressing the effects of variables of interest on brain morphology. However, the model is a linear model. Although a linear model is easy to interpret, the work doesn't motivate why a linear model is sufficient to capture all the nonlinear variation in structure and morphology given only variables like age and gender. The extensive research around deep learning (for both generative and discriminative models) has been to find the right ""features"" to generate/extract to faithfully capture the conditional probability distribution (of the brain image given other variables, in this case). Moreover, deep models also lend themselves to explainability and interpretability ([1, 2, 3]) and it is incorrect to say that discriminative models are harder to interpret. Moreover, the interpretability of the proposed ""causal model"" is not substantiated with experiments that show how the proposed model is different/better than other baselines in explaining different factors. For example, Figure 1 is really unnecessary and its not clear to me what exactly the figure is trying to convey. A comparison between baselines is required in the experiments to support the claim that the proposed method is a causal model that lends itself to better interpretation compared to other methods like VAE, SFCN or RVoxM). 
The paper has very little technical novelty. Linear generative models have been proposed before in the literature and the derivation of the posterior follows from the basic generalized linear models formulation. The major drawback of the linear model is also its lack of flexibility. Since the model only uses upto two covariates (age and gender) the model doesn't capture, for example, multi-modal behavior or non-linear deviations in the given generative model formulation.
Comparison is also slightly unfair with SFCN. For the same input (affine T1), SFCN has a consistently better than the proposed method for all training subject sizes. The tradeoff between accuracy and some other factor (training time, hyperparameter tuning) is not shown to justify the use of the proposed method rather than SFCN."
025	Accurate and Robust Lesion RECIST Diameter Prediction and Segmentation with Transformers	Some technical details are unclear (eg. step 1 outputs: do they use a segmentation objective, do they output only two heatmaps, do they use the same consistency losses and are these suboptimal for bounding box prediction, as opposed to keypoint prediction?)	"Overall, the impact of the method felt rather difficult to assess.
- While the idea of single-click RECIST diameter estimation via the use of a combination as proposed by the authors is rather novel, single-click lesion segmentation is not, and is part of the regular clinical workflow, e.g. when using clinical reading software (single-click segmentation for example is part of the MM Oncology Suite from Siemens, but similarly exists for other vendors).
- While I do not believe that these vendors currently use state of the art deep learning and transformer networks for this purpose, the question arises whether the conducted comparison is therefore adequate, as it seemed to only take into account deep learning-based methods, and with 5 out of 9 (namely [2] and [15-18]) having a very strong overlap of contributing authors, some optimized for the same dataset (e.g. 17,18). As a result, the representativity of the conducted comparisons felt somewhat vague to me.

Further, the choice of the data should have been somewhat better motivated. It did not become clear to me, how the 1,000 test samples have been chosen and how they were distributed across the various types of lesions in the DeepLesion dataset.

Finally, I was not completely convinced about both methodological aspects of the proposed solution (see below) as well as the significance of the achieved results, on which I would like to ask the authors for further explanation."	What's the internal structure of the transformer you are using? It's best to show it with figure. For example, whether the position encodings are all added to Q, K and V or only added to K and Q. What kind of structure is the encoder-decoder attention you mentioned in the decoder?
026	Accurate Corresponding Fiber Tract Segmentation via FiberGeoMap Learner	Experiments seem unfair, such as inconsistent training and testing data. The baseline methods are proposed in 2018, not the latest ones. A qualitative analysis should be also provided.	"some major concern about the paper.

The experimental evaluation seems not fair to either TractSeg or WMA. The author combined the two atlases together. The two methods have different definitions even for the same tract. This is a known issue due to the lack of concuss. Please refers to  https://doi.org/10.1016/j.neuroimage.2021.118502. So when comparing the tracts that are overlapped in these two atlases, there should be bias introduced. Also the two methods are performed with different tractography algorithms.

Second, the computation of the evolutions is not clearly described. Dice is for volumetric overlap, while prevision and recall are class prediction. Did the authors convert the streamlines to masks somehow?

The application on autism seems not necessary and redundant. Is ""the proportion of fibers"" a fair measure for ""abnormities""? Do we expect ASD individuals have such abnormities? As this is a technical paper, I would suggest adding experiments on additional datasets from multiple acquisitors as performed in TractSeg and WMA studies."	See some minor issues in detailed comments.
027	ACT: Semi-supervised Domain-adaptive Medical Image Segmentation with Asymmetric Co-Training	"The methodology lacks motivation from a machine learning perspective. It is unclear how the proposed approach can appropriately integrate the two components given the different assumptions in UDA and SSL.
The empirical evaluation is on one dataset. It would be more convincing to include more datasets in the evaluation to ensure the proposed approach does not overfit to the dataset.
Confusion between ACT and ACT-EMD. If ACT includes EMD, what is ACT-EMD denoting."	"This paper mentions that previous SSDA methods suffer from ""source domain supervision dominates the training"", is any reference or analysis to support this statement?
What is the detailed workflow of SSL and UDA? There are lots of implementations of these two tasks, but this paper does not provide more details. Therefore, it is not clear whether the improvement comes from the new framework or more advanced SSL or UDA implementations.
The meaning of SSDA:1 or SSDA:5 is not clear. Does it mean the number of labeled subjects (as claimed in the second paragraph of Section 3) or the number of samples (as claimed in the fifth sentence of the first paragraph on page 7) in the target domain? What does the ""subject"" mean?
Since the labeled target samples are quite limited in SSDA: 1 and SSDA: 5, is that enough to train a model? Even so, the generated pseudo labels would be quite noisy which may result in negative transfer problems.
Do other baseline methods share the same labeled target examples with ACT in the SSDA: 1 and SSDA: 5 settings?
This paper is not well written. There are a lot of long sentences which are hard to understand. For example, ""In order to prevent a segmentor, jointly trained by both domains, from being dominated by the source data only, we adopt a divide-and-conquer strategy to decouple the label supervisions for the two asymmetric segmentors, which share the same objective of carrying out a decent segmentation performance for the unlabeled data"""	"Unlike the statement ""no SSDA for medical image analysis (MedIA)""There are many works about SSDA for MedIA [1]. And decoupling labels to different parts, such as SSL and UDA, is not new to SSDA, e.g., [2] [3]. The authors should discuss more recent SSDA works in the introduction section.

Accordingly, the authors should compare the proposed method with more recent  UDA and SSDA works such as [2-4], besides these UDA and SSDA methods for natural images.

[1] Guan H, Liu M. Domain adaptation for medical image analysis: a survey[J]. IEEE Transactions on Biomedical Engineering, 2021.
[2] Zhao Z, Xu K, Li S, et al. MT-UDA: Towards Unsupervised Cross-modality Medical Image Segmentation with Limited Source Labels[C]//International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021: 293-303.
[3] Li K, Wang S, Yu L, et al. Dual-teacher++: Exploiting intra-domain and inter-domain knowledge with reliable transfer for cardiac segmentation[J]. IEEE Transactions on Medical Imaging, 2020, 40(10): 2771-2782.
[4] Chen C, Dou Q, Chen H, et al. Unsupervised bidirectional cross-modality adaptation via deeply synergistic image and feature alignment for medical image segmentation[J]. IEEE transactions on medical imaging, 2020, 39(7): 2494-2505."
028	Adaptation of Surgical Activity Recognition Models Across Operating Rooms	"The technical novelty is weak;
Code is not available while the reproducibility responses are checked;
Only one dataset is used."	The main weakness of the paper is the data description. There is no information about the type of surgeries and the annotation protocol	"limited technical contribution
limited information on the dataset, like procedure, video duration, clinical team, etc
not sure how balanced the dataset is and only reporting accuracy and mAP might not be enough. I would expect to see precision, recall and F1 score
experiment parameters are given in the supplementary material and not sure if it will be published with the paper"
029	Adapting the Mean Teacher for keypoint-based lung registration under geometric domain shifts	"Unclear and somewhat misleading motivations. As for the domain shift, it is somewhat incorrect to argue that ""The domain shift consists in exhale scans from the target domain exhibiting a cropped field of view such that upper and lower parts of the lungs are partially cut off"". This is not common sense for domain shift (CT-MRI, obviously different intensity distribution, etc). Particularly, the cut-off problem seems like because of pre-alignment preprocessing in the learn2reg challenge. I think it is somewhat misleading to regard this issue as a domain shift problem.

This work is built on top of the mean-teacher-based framework (which was initially proposed for semi-supervised learning). Adapting the mean-teacher method into registration is interesting, yet, the motivation is not strong and also ambiguous here. Here, it seems like the paper just follows a semi-supervised paradigm, and tries to use another dataset as unlabeled data. Therefore, I will just regard the method as a semi-supervised registration method. The claim for tackling the domain adaptation problem is not strong. However, in the image registration community, acquiring ground truths are always infeasible, especially for those not-landmark-based methods. That is why unsupervised registration becomes popular. Thus, this method may lack practical values.

Following the above concerns, the experiments are weak. Especially, since the proposed approach is inherently a semi-supervised registration method that enables learning with both labeled data and unlabeled data (w/o the landmark labels), it is unfair to just compare their methods with source-only, target-only. The method exploits more data (i.e., unlabeled data) during training, so it is not surprising to see the improvements. I mean, when you train with your limited labeled data, the method may struggle with overfitting, so it is not surprising to see a performance drop in your test set. Therefore, it is hard to evaluate the efficacy of this paper.

This is just a discussion point regarding the motivation of using the mean-teacher framework in registration since I noticed an interesting work [1] that utilizes mean-teacher design to achieve adaptive regularization weighting during training for unsupervised registration. From [1], another insight of using mean-teacher design in registration is that the solutions to this ill-posed problem may vary greatly among different training steps, therefore [1] enforces the consistency to exploit the ""temporal"" information related to the ill-posedness. Authors can further discuss similar works, although having different motivations. 
[1] Xu, Z, et al. ""Double-Uncertainty Guided Spatial and Temporal Consistency Regularization Weighting for Learning-based Abdominal Registration."" arXiv preprint arXiv:2107.02433 (2021)."	"The proposed pipeline requires ground truth deformations phi to use the supervised loss. The author did not explain how they obtain/calculate these ground truth deformations. With a classic registration algorithm or a deep learning based algorithm ? Can this method be expanded to unsupervised registration ?
The optimisation is performed without regularisation losses on the produced deformation. Is it a choice of the author ? Concerning the regularisation, the author do not discuss the performance of the regularisation in term of folding or standard deviation of the Jacobian. Do the proposed method produce smooth or noisy deformation ?"	The utility of reverse augmentation is not clearly demonstrated via an ablation study.
030	Adaptive 3D Localization of 2D Freehand Ultrasound Brain Images	"The clinical motivation is not clear
The statistical analysis requires some clarification."	"In my opinion the main weakness is the testing performed on free hand 2D ultrasound images, since the location of the image planes inside the volume is unknown. The authors propose a ""rate of change"" index to estimate accurate localization of planes, it seems to me that this index can result in optimum values for an smooth sequence of contiguos images, which are wrongly located in the US volume (e.g. an smooth sequence of planes parallel to the trancerebellar plane may have optimum values of ""rate of change"" for the bottom half of the cerebellum as well as for the top half of the cerebellum)"	This might be a little personal idea but it is a fact. In medical imaging, when we are reconstructing 3D image from voxels from 2D freescan Ultrasound frames, we need to be around 100% accurate in reconstruction. The reason is that we are looking at patient organ or lision. We cannot use estimation like the author's idea or train a model to generalize one image to other image. maybe in test data, we have a new problem in patient which was not in training dataset at all. Therefore, still using sensors and 3D ultrasound probes is the best way to solve this problem, and community should work on decreasing the cost of those techniques and increasing the performance and accuracy. It is not acceptable that every problem can be solve by training a deep learning model. For this reason, the proposed idea is not suitable for medical imaging. Estimating and predicting location of a 2D frame can be used for animation design or other engineering applications. Now this generalization issue can be see more clear when we are generalizing from Atlas to real human organs. This become even more worse when we are using unsupervised techniques for localization of frames in 3D slace.
031	AdaTriplet: Adaptive Gradient Triplet Loss with Automatic Margin Learning for Forensic Medical Image Matching	"As an alternative of the general loss function in field of metric learning, the proposed method should be evaluated on more mainstream architectures and datasets.
Some expressions are confusing and need to be modified."	"Minor:

The purpose of th AutoMargin is to choose the hyperparameter margin automatically. However, the AutoMargin method (equation 7 and 8) has new hyperparameter K_delta and K_an. How to select these two hyperparameters?
More explanation on this statement ""we want to increase the virtual thresh- olding angle between anchors and negative samples, which leads to the decrease of b(t)"" ? Is b(t) the threshold in AdaTriplet loss to ensure f_n is far from f_a?"	"While the FMIM problem is incredibly interesting, it is not entirely clear how AdaTriplet specifically tackles this problem, and it would be important to clarify/investigate this in more detail. Instead, it is proposed as a standalone novel Deep Metric Learning method. However, for this to work, much more extensive discussion of other existing ranking objectives, but in particular negative mining strategies, have to be discussed in conjunction, with AdaTriplet and Automargin sharing similarities to existing works such as margin loss (Wu et. al, ""Sampling Matters in Deep Embedding Learning"") or Smart Mining (Harwood et al., ""Smart Mining for Deep Metric Learning""). The issue of less informative/reductive triplets can be often already addressed with a tuple mining approach.

In general, there are many more recent approach in Deep Metric Learning to account for - given that classification-based approaches perform much worse than sample-based methods, it would be important to compare to stronger sample-based methods with tuple mining (c.f. e.g. Roth et al., ""Revisiting Training Strategies and Generalization Performance in Deep Metric Learning"" or Milbich et al, ""Characterizing Generalization Performance under Out-of-Distribution Shifts in Deep Metric Learning for a list of more Out-of-distribution capable extensions).
Particularly the automargin approach should have similar effects as adaptive mining methods such as Smart Mining (Harwood et al., ""Smart Mining for Deep Metric Learning"") or PADS (Roth et al., ""Policy-Adapted Sampling for Visual Similarity Learning), of which at least one should be compared to show whether AutoMargin performs competitively.

It would help the readability if some of the formulas and symbols are replaced or extended with full sentences/descriptions. Currently, the paper is very densely packed with a large collection of different variables and notations, which often requires multiple additional passes over various text passages.

Small note w.r.t 2.2: The reason to operate on the hypesphere (i.e. for normalized embeddings) is the much better scalability to higher-dimensional representation spaces (see e.g. Wang et al., ""Understanding Contrastive Representation Learning through Alignment and Uniformity"" or Roth et al., ""Revisiting Training Strategies and Generalization Performance in Deep Metric Learning""), and not just the fact that the margin is better behaved."
032	Addressing Class Imbalance in Semi-supervised Image Segmentation: A Study on Cardiac MRI	"The paper aims to improve the performance of the under-performing class. However, the results do not show the benefits for the under-performing class, as metrics such as DSC are used. There are no results/discussions of the effects on the under-performing/minority class.
The performance improvement of the proposed method for various levels of class imbalance (imbalance factors) needs to be demonstrated. How effective is each component when the classes are extremely imbalanced / fairly balanced?
The proposed method was not convincingly superior to SOTA on ACDC. Average DSC was lower for L=1.25% and 2.5%, and very similar to PCL and Global+Local CL at 10%. PCL results for MMWHS are missing.
The performance improvements due to each of entropy, variance, and confidence are not clearly demonstrated. In the supplementary, the evaluation appears to use DTS in the model (judging from CC results). What is the performance of each indicator without DTS?"	The main contribution of the paper is to address class imbalance during training. The application is, however, limited to the semi-supervised learning, and the evaluation is only performed on cardiac MRI datasets.	"(1) Too much formulas and notations which are hard to follow.
(2) It's unclear whether the improvement on under-performing clases will destroy the performance on other classes."
033	Adversarial Consistency for Single Domain Generalization in Medical Image Segmentation	"The mixup ratio is not clearly introduced. 
Are there any strategies for collecting patch pairs from the source and the synthesized images?
Experiments are only conducted for CT->MRI. How about the results of MRI->CT?"	"The authors make such a bold assumption that the real unseen domains are subsets of the collections of synthetic domains. However, the experiments are not sufficient to support such an assumption. Many key questions have not been fully addressed: 1) Are there any theoretical guarantees that support the above assumption? 2) Is there bias between the synthetic domains and the real unseen domains? For example, do the synthetic images show low noise level? 3) What is the segmentation performance on source domain where the segmentor was trained? How does the segmentation model perform after domain generalization compared with being trained on real target domains?
I don't think using an adversarial framework can avoid overfitting to a regular pattern, as claimed by the authors. It is almost practically impossible for a GAN model to converge to its global optima. In real application scenarios, it is very likely that the adversarial synthesizer only covers a subset of all possible ""styles"" in medical imaging.
The experiments section is not well written. Specifically, Secs 4.3 and 4.4 are very short and Fig. 3 is very confusing. It is unclear for the general audience which subfigure corresponds to which experimental settings (CT/MR or multi-scanner).
The paper lacks some insights about model design. For example, why the authors use two different $T$ networks but not one with different $z$ samples?
The authors should talk about the limitations of their method. I believe the domain generalizability of the proposed method is not unlimited."	"1.The reproducible findings are not clear from the current submission. The detailed information of how to use the random parameter z during training is not mentioned.
2.The motivation of using patch-level contrastive loss is a little bit weak, the contribution would be more clearly if the paper can compare several different mutual information estimators.

The assumption is too strong and needs more explanation between the assumption and proposed method."
034	Adversarially Robust Prototypical Few-shot Segmentation with Neural-ODEs	"The introduction starts with a number of bold statements that come with no references and, to me, without those references, it appears the entire premise of the paper is incorrect. The authors state that the lack of available data makes models vulnerable to adversarial attacks and proceed to cite three references in the paper (references 1,2,3 in the paper) that do not support this statement. In fact, availability of the data has nothing to do with adversarial robustness of a model. If more data would make a model more robust to adversarial examples, models trained on ImageNet (more than a million training samples) would be more robust to attacks as compared to CIFAR, which is not the case.
What confused me the most in the paper is the usage of the terminology ""framework"", as in, the authors claim that they propose a framework to make few-shot segmentation segmentation models more adversarially robust. However, in the experiments section, the proposed framework PNODE is detailed as having a CNN backbone following a Neural-ODE block, where this backbone is different than all other compared architectures. So, does it mean that the authors propose an architecture rather than a framework? Because if it is a framework that is applicable to any feature extractor, the expectation is to see performance (clean accuracy, robust accuracy) of (a) model, (b) model trained with SAT, and (c) model with PNODE, where the robust accuracy results obtained with (c) is hopefully better than both (a) and (b) while clean accuracy is comparable. Authors also claim that PNODE's clean accuracy is even higher than other architectures, but, is this improvement in the accuracy attributed to Neural-ODE or the superior architecture in the backbone? I do not understand why the authors did not provide the same results for the model employed in the backbone so that we can make a fair comparison between models/framework. As it is, experiments section leaves much more comparative results to be desired.
Authors also do a poor job of literature reviewing for robust few-shot model models. The paper could use a discussion on how PNODE differs (advantages vs disadvantages) compared to some other work in the field [1,2,3].
Finally, the references are not up to the standards of the MICCAI. Sometimes the first names are shortened (Paschali, M.) other times full name is writte (Cihang Xie). Also the venues of publications are not consistent (only acronym, full name, full name + acronym). Please refer to the publication guidelines for the correct and consistent reference format.
[1] Tan et al., Towards A Conceptually Simple Defensive Approach for Few-shot classifiers Against Adversarial Support Samples
[2] Goldblum et al., Adversarially Robust Few-Shot Learning: A Meta-Learning Approach
[3] Liu et al., Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks"	"No major weakness as far as I can see.
Authors may consider citing more related papers, for example,
Kang, X et al., Adversarial Attacks for Image Segmentation on Multiple Lightweight Models, IEEE Access Vol. 8, 2169-3536
Daza, L, et al, Towards Robust General Medical Image Segmentation. MICCAI-2021
Authors may consider testing your method using an ensemble of attacks such as AutoAttack."	"The authors claim that the adversarial robustness of proposed method come from the fact that the integral curves of Neural-ODEs are non-intersecting. The authors should expand on this and give more details to explain it.
There are a few pervious work that already propose to use Neural ODE for adversarial robustness. e.g. [1] [2], the authors may discuss them in related work.

[1] Shin, Yu-Hyun, and Seung Jun Baek. ""Hopfield-type neural ordinary differential equation for robust machine learning."" Pattern Recognition Letters 152 (2021): 180-187.
[2] Yan, Hanshu, et al. ""On robustness of neural ordinary differential equations."" arXiv preprint arXiv:1910.05513 (2019)."
035	Agent with Tangent-based Formulation and Anatomical Perception for Standard Plane Localization in 3D Ultrasound	"The agent is pre-trained by supervised learning with the ground truth data. The auxiliary task learns a representation of the SP close to the target SP by supervised learning. Although they are great tools, they raise doubt to what degree the success of the agent depends on the RL algorithm, but not due to the supervised learning segments. That also poses the question ""do we even need RL here?"" An ablation study might be useful to answer these questions. The results section compares the method with a regression based and registration based method, but it does not say anything about what would happen if we only use the imitation learning module along with the auxiliary task to refine the features.

How is a tangent-point based solution better than the directional cosine and distance from the origin based translation? Fig. 2 makes some effort at explaining it through some great visualizations, but the benefits are still not clear in Section 2.1. ""The coupling among the directions makes actions dependent""- It's not clear since x, y and z are orthogonal and thus have no impact on each other."	The anatomical structure reward is essentially a reward based on landmarks reward. For some designed module, there are lack of ablation study to demonstrate the significance of the tangent point formulation and imitation learning. It only show the impact of SCSP and SAR. Compared with model which without SCSP and SAR, the performance of proposed model has no significant improvements especially in SSIM, in ablation studies. .	"Proper justification is not provided for some claims e.g. ""rely
on initial registration to ensure data orientation consistency. They are easily
trapped when pre-registration fails""; evidence in the form of citations or preliminary experiments would be useful in order to justify claims like this
Some unclear wordings should be clarified e.g. ""ability to recognize subtle differences crossing Non-SPs and SPs in plane search""; what does 'crossing' mean here? how do you define subtle (maybe provide an example of what is subtle)? etc.
The components of the formulation e.g. imitation learning-based initialisation, anatomical feature-based reward and tangent-based formulation are mostly well justified but would have been good to see some more citations of previous works that utilise some of these techniques
Statistical tests to compare methods are not performed; these help to justify claims of superiority of one method over another, without them it is difficult to draw meaningful conclusions from the results; however, spread is reported which aids in drawing some conclusions"
036	Aggregative Self-Supervised Feature Learning from Limited Medical Images	The dataset split and backbone selection might be not appropriate.	One of the main weaknesses of this paper is the baseline setting. In particular, the main contribution of this work is the strategy to combine SSL tasks. The author also mentioned that this work is different from conventional methods in which all SSL tasks are combined and trained together. Therefore, one of the crucial experiments shows the difference between their approach and default settings. For instance, in Table 1, it would be useful if the authors could provide an experiment for training together all SSL tasks and compare them with the proposed method (SRC, SimCLR, 2D Rot). A similar result should be done for the 3D Brain hemorrhage dataset as well. Given this evidence, the contributions of this paper will be more convinced.	"The training cost of SSL aggregation is high. It needs iterative training of each SSL method.
The reported accuracy of existing SSL works is low and inconsistent with existing works."
037	An Accurate Unsupervised Liver Lesion Detection Method Using Pseudo-Lesions	"The training strategy and the exact jittering and other parameters used to simulate lesions are not detailed, which impedes reproducibility.
While the data augmentation is novel, the UNet-based discriminator and image-to-image translation for anomaly localization have uncited prior work. The discriminator has been proposed in multiple works, including ""A u-net based discriminator for generative adversarial networks"" by Schonfeld et al. Anomaly localization works that rely on image-to-image translation between healthy and diseased data include: (1) ""Towards annotation-efficient segmentation via image-to-image translation"" by Vorontsov et al.; (2) ""Visual feature attribution using wasserstein gans."" by Baumgartner et al.; (3) ""Pathology segmentation using distributional differences to images of healthy origin"" by Andermatt et al.
While the proposed method shows great performance, the metric is not defined. It is the AUC of a ROC curve but what is the measure over which you vary the operating point?"	"-The idea to use a Cycle-GAN model to translate normal to pathological data and vice-versa was proposed in Sun et al (JBHI 2020) for neuroimaging application to the segmentation of glioma based on the BraTS dataset. This reference should be added and discussed.
-Some important methodological details are lacking:
-1) The use of a UNET based discriminator should be clarified. It is not clear from Fig 1 what the output of this discriminator is (UNET-like architectures usually output images with dimension similar to the input image input...which is different from standard GAN discriminators outputting label (true or fakes). The authors mention ""This enables the discriminator to learn both global and local differences between real and fake images"" but it is not clear how. The wording of the whole section 2.2 should be reworded as it contains typos impacting the understanding.
2) The authors should provide the backbone architectures of the different networks (generator, discriminator) 
3) A clear definition of the loss terms, including the general GAN loss (different variants exist) and discriminator loss, should be provided. As stated above, introduction of the GMSD consistency loss term is interesting but requires clarification regarding its differentiability, for instance, which is not obvious.
4) GAN training is likely to be challenging, the authors should clarify stopping criterion and describe the validation dataset, if any."	It seems like the main weakness of the approach is the lack of detailed insight into when the method will succeed and will fail. It would be important to report statistics and examples of failure and success cases of lesions the method can handle.  Please see detailed feedback section.
038	An adaptive network with extragradient for diffusion MRI-based microstructure estimation	There is no main weakness. Please see my comments below for minor concerns.	"-The proposed method showed a slight improved fitting error benchmark against previous techniques. The manuscript shows incremental work with low novelty.

Results are limited to the prediction of gold standard NODDI maps in an in vivo setting. It would be of interest to measure the performances of the proposed method on other method parameter maps. Although, I understand the conference format limitation."	"The extragradient sparse encoding is originally proposed in Ref. [9], so the technical contribution of AEME is limited. The motivation of using extragradient in this study is unclear.
Some implementation details are missing. (e.g., batch size, the maximum epochs, etc.)
The experiment setting of Fig. 2 need further clarify. According to the method section, adaptive mechanism will determine the number of iteration block. How could authors achieve experiments in different fixed number of iteration blocks."
039	An Advanced Deep Learning Framework for Video-based Diagnosis of ASD	While the input to the model has a temporal dimension (i.e. it's a video), the architecture is not designed to take the variable length of videos into accont (e.g. using a recurrent model). Instead, they have chosen to subsample the video frames to a static size of N.	The paper lacks technical novelty as it utilizes existing methods or software (OpenFace) to extract the features followed by simple approach for reduction and classification using regular CNN. But the paper presented strong evaluation.	"It says the frames are deleted if the landmark detection confidence is less than 0.75. If that, how to keep the number of consecutive frames N in each snippet. Does it use the empty frame or all-zero landmark feature?
It seems there is no description on details of the dataset. How many videos are included in the dataset? Is it the same as the number of children? If not, are videos of the same child included in both training and test split?
It would be better to have an ablation study on the number of segments.
As the child with ASD may(not) have response to different moving directions, is it possible that max operation would have a better performance to aggregate the score from different segments?
I feel the technical novelty is marginal. Extracting face features for ASD is explored in previous works. HRC attention module is very similar to channel attention in SENet."
040	An End-to-End Combinatorial Optimization Method for R-band Chromosome Recognition with Grouping Guided Attention	The loss function used in the neural network is not given in full detail. It is a weighted combination of chromosome grouping loss and no other details are given. The authors should either provide with a reference or a clear formulation for this loss function. This is definitely needed for the reproducibility of the proposed method.	"*	The chromosome recognition problem is relatively new to the MICCAI community. It was not clearly specified in this manuscript.
*	It is unclear why the input data can be treated as a sequence. There is a lack of discussion on any spatial, temporal or other sequential information related to the chromosome data. I understand that chromosomes are numbers, but that is the output label the method aims to predict. Where is the sequential information for the input data?
*	In Table 1, the patient number is not equal to karyotype number. What is the relationship between them?
*	In Tables 2-3, the performance improvement is minor. The conventional Hungarian algorithm seems to be a good strategy. It is unclear whether the minor improvement would remain for other independent data sets.
*	There is a lack of ablation study. The individual contributions of GFIM and DAM were not quantified."	"The paper's main weakness is that it is telling a complex story in a very short format. The paper combines a couple of fields, (general problem of MICCAI submissions). Here we have a niche within a specific biological problem (karyotyping) and the technical solution (deep learning and graph matching). Quite a lot is assumed from the reader. The problem itself lacks a bit of motivation, but not much. It needs be clearly stated which diseases or disorders benefit from the R-band staining. It is stated that: ""R-band chromosome of bone-marrow cells can help identify the abnormalities occurring at the end of chromosomes"". This is not convincing me that the problem is of clinical relevance, name at least one particular case where this is important.
There is an attempt to explain why this problem seems to be understudied compared to work on other staining methods. More blurred bands are mentioned. This also relates to the clinical relevance. I cannot understand whether this problem is interesting because it is challenging or because other staining approaches are superior and thus more data is available for those. The motivation for why R-band is used is also lacking here.
The reported performance is convincing from the tables in the experiments sections. Performance on another dataset, comparing to reported metrics would certainly aid here as well. Some repetitions to provide error bars on the metrics would also aid the reader in assessing the differences between the approaches. The performance needs to be summarized a bit at the end of the introduction, the only mention of results is in the end of the abstract. It is better to have numbers than just stating ""state-of-the-art"".
The dataset itself looks like it is not open source. Maybe this is not very specific due to anonymity. The dataset lacks a description of the abnormal cases. Do these all have the same abnormality?
How is G=7 chosen in section 2.2 ?
Equation 4 looks more like programming assignment equal operator than an equation. Consider giving the normalised weights on the left hand side a different name.
An ablation study is required to convince the reader that the DAM is needed, this is done but not mentioned until section 3. This should be mentioned in the end of the introduction.
For the results, the dataset should ideally also be stratified by disease/disorder/abnormality to show whether the improvements are specific to a particular disorder. Maybe there are few cases in the abnormalities for doing this, but this should be addressed.
I cannot see that a specific test set is used for the normal karyotypes. The reported accuracy seems to be from a 5-fold cross-validation, this needs to be more clear, specifically in the table caption.
English language is generally good, one minor comment:
1) Sentence in first paragraph of section 2.1 does not sound right: ""For example, one can hardly to identify and distinguish these two chromosomes..."" Remove the ""to"", and it makes sense."
041	An Inclusive Task-Aware Framework for Radiology Report Generation	"Fig(1) is not clear.
Output after each step is not clear. Notation alone is not clear. 
Task distillation module is not explained properly.
Few other questions need to be addressed:
What information knowledge graph is providing? What is the type of the data that's generated after building the knowledge graph? How is it being used by this network in terms of dimension of the data?
Are the image embeddings and classification tokens processed parallely in the encoder.
What is the difference between classification embedding and token? It's not clear.
There is no mention of ground truth description for each structure.
There is no mention of ground truth labels between normal and abnormal for every structure.
How much performance variation has been observed with auto-balance loss function and weighted loss function can be added."	"Despite well formulated ideas and system designs, I have following questions:
1, how are the prior keywords determined? It seems like these keywords are predefined before the TD module. Any discussion in this regard or examples of keywords should be helpful.
2, the interaction between TD and the transformer encoders are not discussed very well. Even though I have the rough idea of the interaction as discussed in the strength section, I am still not very sure my understanding is correct or not.
3, how to evaluate the TD module at the first place? From my understanding and the ablation study (Table 2), this TD module help provide to training data (?) for the decoders in TRG. If TD fails over, presumably the whole model should degrades.
4, during inference, the abnormality types are also need for the report generation. How would performance change (presumably degrade) without knowing the abnormality?"	discussion about misclassifications and places where the reports are not correctly generated.
042	An Optimal Control Problem for Elastic Registration and Force Estimation in Augmented Surgery	"The role of the tetrahedral mesh in the proposed algorithm is unclear. It appears that the computation is done on the continuum/organ (liver in the example analysed in the study) discretiseed using a point cloud rather than the tetrahedral mesh.
Synthetic tests cases for liver: it is unclear how the deformation field resulting from the forces applied to the liver surface is computed. How it was ensured that the computed/predicted deformation field is plausible (i.e. physically correct)?
The reported computational speed/performance (update time of under 2 s using off-the-shelf PC with i7 processor) is impressive, but compatible only with real-time constrains of image-guided surgery. For haptic feedback (estimate foces for haptic is one of the motivations for the study, an update frequency of around 500 Hz would be required).
The registration errors and errors in estimating the surface forces are reported. However, no attempt is made to interpret these errors in the context of the accuracy and robustnessn that would be required for clinical applications.
The propose approach requires patient-specific information about the tissue material properties (Young's modulus). Elastography is mentioned as a possible way of determining such properties. However, in the context of determining patient-specific properties of soft tissues still remains a subject of active research. It's accuracy and robustness are hotly debated topic and are from being commonly accepted as amenable to clinical applications."	"your results are only compared to a rigid solution and are thus obviously better. Being ranked second on the dataset/challenge website is clearly positive, but without comparison with other non-rigid baseline methods it is not possible to really assess the added benefit of the method
you should at least compare your method with the previous solutions proposed in your group as in [17, 18]. (yes, references and the challenge dashboard are enough to easily break anonymity; fair enough)."	"1-No novelty
2- why the ICP is used , what about CPD approach (Coherent Point Drift)? 
3- The landmark selection of TRE selection is not described well , However im not sure if TRE is the best measuring tool for your data."
043	Analyzing and Improving Low Dose CT Denoising Network via HU Level Slicing	"The threshold selection and number of dynamic of the ranges seems ad-hoc

Slicing the problem adds complexity to the networks that may become more difficult to training and with more parameters and hyperparameters"	"Some important information is missing in the experiments:
The computational time is not presented.
A declaration of what software framework and version authors used is not presented.
A description of the computing infrastructure used (hardware and software) is not presented.
English could be improved."	The main weakness of the paper is that HU level slicing is not novel method. This is the traditional thresholding with heuristic parameters. The method needs ablation study for HU slice values and theoretical analysis why this HU level slicing is beneficial. Most importantly, HU values have a physical semantics in terms of attenuation so normalizing after HU slicing needs to be justified. In particular, the reconstruction loss is well balanced between different HU bins.
044	Analyzing Brain Structural Connectivity as Continuous Random Functions	"In the results (Fig 1), it's unclear to me if the higher rate of ""significant"" discoveries is simply due to CC's higher resolution, or if CC has more significant findings at the resolution of current methods."	The method and experiment sections needs a few more details, as I list in the detailed comments below.	"1). Topic Issue 
From the reviewer's perspective, this work concentrates on real analysis, functional analysis, and statistical Theory to discuss the continuous brain structural connectivity. This is a theoretical research work that probably does not match the topics of MICCAI. Furthermore, there are fewer medical, clinical translational, and imaging analytics provided in this work. Therefore, the reviewer would suggest submitting this work to NeurIPS, ICML, or COLT.
2). Inconsistent Format
This paper is organized inconsistently. At first, the reference citation is not consistent with the original template. The authors utilize (1) as a citation of reference #1, but this results in the confusion as an equation (1). Furthermore, the Bibliography should be replaced as a Reference. There are many inconsistencies in the format of this work.
3). Theoretical Issues/Questions
Although authors provide many theoretical analytics to brain structural connectivity, some theoretical issues lead to confusion among reviewers.
At first, the authors proposed the concept as Borel measurable regions, but the authors utilized these regions to denote a Lebesgue calculus. What is the definition of Boreal measurable regions? Are these regions could be covering via Boreal sets? Or what is the relation between Lebesgue and Boreal measure? Why cannot authors adopt the Lebesgue measure to denote these regions? Unfortunately, the authors do not provide more details and explanations.
Furthermore, in Section Reduced-Rank Embedding of a Sample of Continuous Connectivity, it seems that the authors try to conduct a novel space. This novel space is defined based on a standard Euclidean metric and finite dimensionality. Why can the authors directly definite all operators in a Banach space? Is the novel space complete? Can authors prove the completeness of the proposed new space? 
Finally, the authors denote the target function as Eq. (1). However, there is no detailed description to introduce the optimizer and the convexity of the target function. Can authors prove the proposed target function a convex or non-convex problem?
4). Technical Issue.
In section Algorithm, the authors employed SVD to perform the matrix decomposition. However, there are several shortcomings of SVD. For example, SVD cannot implement the decomposition of a sparse matrix; SVD is difficult to solve the over-complete problem; if the input matrix is not square, SVD could be time-consuming. Therefore, the reviewers provide the following research works for authors as references:
[1] Wen, Z., Yin, W., & Zhang, Y. (2012). Solving a low-rank factorization model for matrix completion by a nonlinear successive over-relaxation algorithm. Mathematical Programming Computation, 4:333-361.
[2] Shen, Y., Wen, Z., & Zhang, Y. (2014). Augmented Lagrangian alternating direction method for matrix separation based on low-rank factorization. Optimization Methods and Software, 29:239-263."
045	Anatomy-Guided Weakly-Supervised Abnormality Localization in Chest X-rays	"The novelty is limited. Utilizing reports to aid abnormality localization in chest x-rays has been popular in this area.
Baselines are not complete and state-of-the-art, especially some existing works that also utilize the medical reports are not considered for comparison.
Ablation experiments are not complete. For example, this model contains a few hyper-parameters, but many were not discussed in the experiments."	"I have a couple of concerns and suggestions as follows:

I found a directly related reference [R1] is missing, which tackles the same problem and also extracted the attributes of disease (including anatomical locations) from the report as a form of supervision.
The comparison to prior arts in weakly supervised localization is relatively weak, where only one previous work based on vanilla CAM is included. There are other methods, e.g., [R2] (another missing reference), which should be included and compared. Also, the localization results for other IOU values shall be included, at least in the supplementary. Especially results on IOU 0.5 are important.
The PU module for the uncertainty learning seems only to be effective in some scenarios, e.g., better results for pneumonia but worse for pneumothorax in MIMIC-CXR. It would be helpful if the authors could further discuss it.
There are public datasets with bounding boxes/segmentation masks for pneumonia and pneumothorax. It will be more convincing to adopt those datasets for the evaluation, considering the current GT set from MIMIC-CXR is a bit small (a couple of hundreds vs. thousands).

[R1] Bhalodia, R. et al. (2021). Improving Pneumonia Localization via Cross-Attention on Medical Images and Reports. In: MICCAI 2021
[R2] Li, Z., et al.: Thoracic disease identification and localization with limited supervision. In: CVPR 2018"	"In the experimental part, some state-of-the-art methods need to be compared in Table 1, 3 and 4.
Many important information is missing in the comparative experiment part.
The authors violated the guideline for supplementary submission."
046	Anomaly-aware multiple instance learning for rare anemia disorder classification	"For the introduction section, the contributions are not clearly summarized.
Why the proposed strategy can overcome the limitations of the attention mechanism?
For the Mask-RNN, is it pretrained on another large-scale dataset? or it is directly applied to the dataset used in the experiments.
For the Anomaly scoring, why using the Mahalanobis distance in Eq. (3).
The number of comparison methods is relatively few.
It is difficult to understand the difference between the Anomaly method and attention method in Fig. 3 and Fig. 4."	"It would be better if the authors can compare with some SOTA anomaly detection approaches for anomaly recognition.
Authors may consider training some of the MIL methods from computer vision on microscope data and comparing them.
A public dataset can be used to further justify the contribution of this work.
The novelty is a bit limited, given that [14] proposed to use attention and MIL classification for such problems as well. The loss function is the same."	"-The motivation of this work in its current form is not clear enough. Why the negative instance estimation is necessary when using MIL need more specific justification.
-Regarding the technical framework. The aggregation from instance representation to bag representation is required to be permutation invariant. Will the proposed anomaly score warrant this aspect? The author needs extensive effort to justify this issue in the rebuttal stage.
-This work lacks multiple strongly related deep MIL based works in the past few years. For example:
[1] A multiple-instance densely-connected ConvNet for aerial scene classification. IEEE Transaction on Image Processing, 2020
[2] Multi-instance multi-scale CNN for medical image classification. International Conference on Medical Image Computing and Computer Assisted Intervention, (2019)
[3] Local-global dual perception based deep multiple instance learning for retinal disease classification. International Conference on Medical Image Computing and Computer Assisted Intervention, (2021)
[4] Loss-based attention for deep multiple instance learning. AAAI 2020.
The authors are suggested to enrich the related work accordingly."
047	Assessing the Performance of Automated Prediction and Ranking of Patient Age from Chest X-rays Against Clinicians	"1.In Section 2.2, the paper says ""whether the ability of radiologists to order two images of the same patient by age is superior to their ability to estimate true patient age from a single image"". But the experiment to compare these two abilities was not introduced and What is the final conclusion.
2.In Figure 1(b), it is not clear how the pixel difference map by the model is implemented. 
3.The presentation of the purpose of separating aging features is insufficient and unclear.
4.The expectation of ranking success rate for humans is 59.7%+-2.8% in Section 3.2, What is the basis of the ranking success rate for humans?
5.In the model architectures comparison, the MAE of 3.33 from Efficient+LR just only 1% improvement compared with Efficient+CL and Efficient+OR in Table 1. A minor improvement is not enough to indicate that Efficient+LR network is optimal. Which method did you ultimately choose for Age Prediction?
Minor
1.Figure 3 (right) and its description in the Generalisation Performance on Public Datasets of the Section 3.1 is ambiguous. 2.98 years refer to your own dataset results or after fine-tuning on the Chest14 training data."	"The paper should be improved in clarity and has different grammatical errors/typos.
Some of the results are not described adequately and the results section must be reorganized/rewritten."	"My only concern is in the human AI comparison.
The authors used test cohort for this which may include images from the six hospitals protocols whic the AI network already know through training. This si not a fair comparison with human experts. The authors need to compare in a cohort where the AI tool never show before the hospital's protocol modality resolution and quality of the images. By this way the comparison is fair with the human experts."
048	Asymmetry Disentanglement Network for Interpretable Acute Ischemic Stroke Infarct Segmentation in Non-Contrast CT Scans	"The method is task (Stroke segm) and modality (non-contrast CT) specific. This limits the audience of interest. It is based on (at least) 2 assumptions: 1) All the asymmetries of interest are dark. This is mentioned briefly in method section and discussion (doesn't find bright blood). 2) The pathologies appear as ""additive"" intensity changes (X_comp=X+A-P). For example, it cannot deal with pathologies that create morphological changes (e.g. tumor). The work would benefit if a) task-specific assumptions were discussed very explicitly, b) if it would discuss how and to what tasks it could be potentially extended. E.g. multiple-sclerosis / leucoaraiosis in Flair MRI (though bright).

Evaluation is rather limited. Only 1 experiment per method. No runs with multiple seeds. So we aren't sure whether improvements are by a ""lucky seed"" or consistent. No train/val/test setup, only train/test. So it's not clear whether improvements are ""best run"" with optimally configured hyper-parameters on test set. This could be improved by averaging over multiple seeds per method, and by discussing how hyper-parameters were found for the method and the compared methods.

It is unclear whether some components of the method are adopted or proposed. In good scientific writing, this should be crystal clear. This needs fixing. Examples: In Sec.2, Transformation Network: Eq.1 is adopted from [18] but this is not explained. The text only mentions the difference that the model here is 3D whereas [12] is 2D. Please state explicitly that Eq.1 is adopted from [18]. Also, in rebuttal, please state explicitly if any other parts of the method are adopted (Eq.3? Parts of regularizer?), and same in text.

The regularizer has multiple parts. There is no ablation study whether each individual part is important. Knowing which part is important would help extend this work in the future. For example, one could see if the parts that are most important are applicable to another task and perhaps employ them for a different pathology/modality.

Parts of the regularizer could be straightforwardly applicable as a regularizer of the segmenter, and not just the asymmetry-predictor D. In fact, some of them may be the cause of segmentation improvement, instead of the asymmetry map generation. Example: For example, penalizing the pathology when it appears on CSF (1st part), or the regularization of the average size (2nd part). This has not been studied. There could be a comparison/ablation where those parts are tried on 3D ResUnet to see if they improve it, to identify whether the improvements come from the asymmetry-image generation or the regularization of the size/location of pathology. The authors & article could explicitly discuss this."	"Narrative application
Although the proposed method is interesting, aiming to disentangle two kinds of asymmetries and reduce the artefacts of input images, this approach might be narrative to stroke segmentation on noisy CT images, which rely on such a prior on asymmetry to improve the input images.

Potential issues: 
The authors propose to modify the input image X^ by X^ = X + Q = X + A  - P. This might be problematic as it might generate false positives due to the intensity range difference between X and Q (or X, A and P).  A clarification or discussion on this might be needed.

Results and Clarifications. 
x. The author mentioned that the transformation network T can be unsupervised. I am missing the details of such a training strategy. From my understanding, this is similar to regress the rotation angle of an object in an images. One could simulate an angle (transformation matrix in the authors' case). However, this would require some 'object-centered' images. Essentially, a question would be: how good the network is to regress the parameters? The transformation module is very important in the proposed framework as it generate A, which is base for next steps. 
x. For training D, is there any weighting strategy for four loss terms? 
x. How much does the segmentation maps (WM, GM, CSF) help as a regularization?
x. For self-mirrored version X', which plane/view is used?"	The idea I think is novel for the application, I think a weakness is potentially the lack of significance testing between the methods - which I think is nice to utilise in the evaluation, especially as you are comparing your method to current state of art methods.
049	Atlas-based Semantic Segmentation of Prostate Zones	"As the author mentioned, the assumption of having the available WG mask is very strong, and the impact of employing the WG mask as input is non-negligible, so this may harm the transferability of this method in practice.
Besides, the author uses an external dataset to evaluate the method, while the atlas is built upon the source dataset, if the data bias is huge, will this strategy still work out.
There are some atlas-based methods for prostate segmentation that are not included in the reference list, such as [Jia, Haozhe, et al. ""Atlas registration and ensemble deep convolutional neural network-based prostate segmentation using magnetic resonance imaging."" Neurocomputing 275 (2018): 1358-1369.], [Ma, Ling, et al. ""Automatic segmentation of the prostate on CT images using deep learning and multi-atlas fusion."" Medical Imaging 2017: Image Processing. Vol. 10133. International Society for Optics and Photonics, 2017.], [Padgett, Kyle R., et al. ""Towards a universal MRI atlas of the prostate and prostate zones."" Strahlentherapie und Onkologie 195.2 (2019): 121-130.], [Singh, Dharmesh, et al. ""Segmentation of prostate zones using the probabilistic atlas-based method with diffusion-weighted MR images."" Computer Methods and Programs in Biomedicine 196 (2020): 105572.]. Similar methods should be at least taken into discussion to better address the main contribution of this work."	"Minor problems with English grammar. Please edit.
Setting different weights of the atlas during testing means ""human in the loop"" and might make the algorithm operator dependent."	"Whole gland prostate segmentation is required as one of the inputs to the model, limiting its application to only cases in which this is already available.
The anatomical atlas was developed by registering images with an affine transform, but a deformable registration may achieve better results.
Figure 2 compares the Dice of the baseline method with and without mask, in addition to the proposed methods in which the hyperparameter that determines the influence of the semantic segmentation v. atlas is varied. Clearly, adding a mask improves the model performance, but in absence of a mask the model performs significantly worse than typical two-zone prostate segmentation methods in which no mask is used. It would be interesting to see how this U-Net model performs on two-zone prostate segmentation when trained without a mask on this dataset.
Only a couple comparisons of their results to other work are included, but multi-zonal prostate segmentation is a well-studied topic, and inclusion of comparison to more recent work would be beneficial."
050	Atlas-powered deep learning (ADL) - application to diffusion weighted MRI	implementations of other DL methods might not be correct	"there is no justification as to why learning from an atlas would be a better idea than learning from the individual images themselves, although results tend to show interest (see tables)
in methods, the atlas construction process is missing important details. Overall the clarity should be improved.
learning only scalar parameter values seems to be the lower end of what could be achieved, why not learning complete models ?"	"*	Limited proof that outperformance of the provided framework is due to the atlas
*	Limited analysis and discussion of the results, eg is there any bias in the results as per age or which is the limit point where atlas is no longer useful? (see my details comments below)
*	Questionable choice of registration method (also acknowledged by authors)"
051	Attention mechanisms for physiological signal deep learning: which attention should we take?	"The paper is not novel. No methods are presented that have not been presented before by the literature. However, it is my opinion that experiments presented are valuable and informative.
I'm not sure the inclusion of the multi-head self attention model makes sense in the experiment. Transformer architectures and multi-head self attention has recently become widespread in the literature. Their applications are widely varied. They have been used as sequential models, models that pool information spatially, enhanced with convolutional blocks, and applied in a wide variety of novel architectures. As such it is hard to draw conclusions about the overall efficacy of transformer models from the single architecture explored in the paper.
Similarly, each of the CNN architectures employs some method of aggregating over the final output of the CNN encoders (flattening or GAP). Architectures that replace this aggregation and fully connected classification layers with transformers have already appeared in the literature. Therefore it is possible to envision combining the CNNs presented in the paper with transformers and multi-head self attention. Given these two points it may be better to limit the scope of the experiments to just CNN based models.
It would be good to perform the experiment on a wider number of tasks. The authors choose two tasks with different that require information from separate phenomena in the ECG for the network to identify. Different self attention mechanisms are more effective in each task according to the phenomena of interest. The inclusion of more experiments in different ECG applications would help verify that the results presented in the classification and regression tasks hold in a variety of situations.
The paper relies on citations for explaining the self-attention mechanisms investigated. The paper does a decent job motivating these mechanisms in writing but as these mechanisms are central to the understanding of the paper, further detail should be presented. This detail could come in the form of equations or schematic diagrams."	There are no deep insights.	"Need more justifications about the novelty claims
Need to include more related work that are highly important
Need to check for grammatical errors and typos."
052	Attentional Generative Multimodal Network for Neonatal Postoperative Pain Estimation	The approach is novel and derives from similar work using CNN-LSTMs. The innovation is a small step as it also uses a combination of similar techniques.	"Lack of detailed information about the experiments: For example, authors do not describe how many data is used for the evaluation, how many pain/no pain cases there are. These information would help to understand the results exactly.
The results of the pain intensity estimation are seemed to be not good enough."	"1)	The performance of the proposed approach when dropping each modality was discussed in the paper, however, if some samples of one modality and other samples of another modality were missing at the same time, can the model reconstruct features?
2)	Did the authors consider different weights of three modalities during the training process? As shown in Table 3, when the sound modality was dropped, it seems that the performance decreased more."
053	Attention-enhanced Disentangled Representation Learning for Unsupervised Domain Adaptation in Cardiac Segmentation	* The attention bias module and the use of Hilbert-Schmidt independence criterion (HSIC) are not new (see ref [14] for the Hilbert-Schmidt independence criterion).	"1.The experimental results are not solid enough. Since the main innovation of this paper is the framework rather than the pre-training manner, all UDA methods involved in the comparison should be treated under the same configuration. According to the description in the #section2.5, the pre-training parameters, learning rate are not same. (The proposed ADRs are fine-tuned on the basis of SIFA, while others are initialized from scratch.)
2.The compared UDA methods are not SOTA now. It is recommended to add more comparison with SOTA methods."	"In section 2.2, the authors only used the GAN to achieve the image-to-image translation, which theoretically can not preserve the anatomical structure in the translation process. That is the reason why the cycle consistency loss is proposed in CyleGAN [1]. In this case, the subsequent cross-modality segmentation for anatomical structure does not seem to make sense.
Before the feature disentanglement, the authors firstly perform image translation, which really confuses me. The disentangled representation learning is based on the assumption that the image from different domains share the same domain-invariant features and have their own domain-specific features [2,3]. In this work, the domain-invariant feature and domain-specific features can be considered as the shared anatomical content and specific image style (CT or MR). Taking MR as the source domain, the style of translated image x^{s->t} should look like CT image, as shown in Fig.1. In this case, for two CT images (real CT and pseudo CT), how do disentangle their content and style as they share the same content and style? Therefore, it seems contradictory to perform image translation before feature disentanglement.
In section 2.5, the generator and discriminator used in image alignment are fine-tuned with a learning rate of 1e-10. My question is, with such a small learning rate, whether the optimizations of these two networks can be negligible? Whether the translated images are visually better or worse? The author should plot the loss function curves of these two modules and visualize the translated images.

[1] Zhu, Jun-Yan, et al. ""Unpaired image-to-image translation using cycle-consistent adversarial networks."" Proceedings of the IEEE international conference on computer vision. 2017.
[2] Lee, Hsin-Ying, et al. ""Drit++: Diverse image-to-image translation via disentangled representations."" International Journal of Computer Vision 128.10 (2020): 2402-2417.
[3] Huang, Xun, et al. ""Multimodal unsupervised image-to-image translation."" Proceedings of the European conference on computer vision (ECCV). 2018."
054	Attentive Symmetric Autoencoder for Brain MRI Segmentation	1) Novelty in terms of network architecture is not great. The authors use shifted windows in UNETR architecture.	"There are several concerns about the paper:

The paper makes a strong assumption that the input 3D brain MRI is left-right symmetric, and from this point, the author develops SPE to induce the anatomical symmetric prior in SW-ViT encoder. This is okay for the pre-training phase if all the brain volume is aligned to BIDS. However, I'm concerned about the brain MRI with disease/lesions that can significantly change the local region and appearance of a brain. For example, Alzheimer's Disease and brain tumor can both affect brain structure in a certain region in one side of the brain, and cerebral atrophy can even severely change one side of the brain structure. When the brain structure is asymmetrically affected by some neurodegenerative diseases, the anatomical symmetric assumption is not held anymore, does SPE still have any advantages over the vanilla PE? There's no discussion about this in the paper, however, this issue should be concerned, since the proposed ASA is a general brain segmentation model and should be able to take care of diseased brain.
One question about the downstream finetuning on BraTS 2021: this benchmark has 4 MRI modalities so the input volume will be 4 channels, however, the SW-ViT is pretrained using only T1 MRIs which is single channel. When fine-tuning the pretrained model on BraTS, how does the model handles 4-channel input? More specifically, how is the input patch embedding layer being initialized, given the gap of different channel number.
From Table 4 ablation study, comparing SPE&SSL with SSL-only setting, there's only marginal improvements in ET Dice and HD95 scores, and the Dice scores for WT and TC is even lower in SPE&SSL setting. Therefore, it looks like there's no obvious advantages using SPE under SSL pretraining scheme.
Although the paper emphasizes that SPE improves the symmetric details in segmentation, but there's no evaluation on how good the symmetric structures are preserved and segmented in the 3 downstream tasks. I suggest adding some segmentation results which contain symmetric brain structures and segmented better than that from other SOTA models/methods."	"As we know, ViT is a patch-based structure, and swin Transformer is a pixel-based structure. It is better to add more details about Shifted Linear Window-based Multihead Self-attention(SLW-MSA). 
It may be confusing for the reader whether it is computed on pixel-level or patch-level, so the author should add more details.
This paper is similar to ""Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis"" (https://arxiv.org/pdf/2111.14791v1.pdf)
The author should add more discussion to distinguish it from that paper."
055	Autofocusing+: Noise-Resilient Motion Correction in Magnetic Resonance Imaging	The main weakness is that the proposed method is seems a trivial step in addition to autofocusing methods. The newly introduced Unet loss mechanism is similar to a GAN setup as critic so the authors should compare their methods with a GAN based motion correction model.	"-It is not clear how the method works. In particular, it is not clear how eq (5) is related to the autofocus method proposed in [2], which is based on an entropy metric.
-The proposed method takes 7-9 minutes to generate motion corrected images. 
-Comparison with other state-of-the-art motion correction deep learning methods missing. For example, [13] was discarded because of the ""associated training routine"" and [29,30] because they do not take ""the physical nature of the MRI artifacts into account"", but these methods provide results within seconds, which is an advantage over the proposed method."	The proposed method was investigated for simulated motion which may not necessarily reflect the actual true underlying motion behaviour and impact on the image. Rigid motion correction for a single imaging sequence and imaging contrast was performed and hence generalizability of the autofocusing concept (task independent metrics/prior) is not known.
056	AutoGAN-Synthesizer: Neural Architecture Search for Cross-Modality MRI Synthesis	"1: The main ideas of the paper are known or incremental advances over past work.  Besides, the incorporation of the K-spcace has also been employed in some medical image processing works.
2: The number of images are too small to obtain meaningful training results. There are only 75 samples of BRATS2018 dataset and 25 samples of IXI dataset for training.
3: The experimental setup details are incomplete. For example, the value of  and  in Equation(3) is unclear. The authors should provide the value of the hyper-parameters for the reproduction.
4: The code and the data are not available to aid reproducibility."	"Table 1 is not suitable to place in Page 5, which makes readers hard to read this papers.
Authors just compare the model weights but not whole computational complexity. Most models contain less parameters but take longer inference time."	"The technical novelty is limited and incremental, as claimed in this paper, the two main contributions of this work are GAN-based perceptual searching loss and K-space learning while those two are off-the-shell techniques.
The authors use 2D axial image for training and testing, they didn't consider the consistency in the coronal view and sagittal view."
057	AutoLaparo: A New Dataset of Integrated Multi-tasks for Image-guided Surgical Automation in Laparoscopic Hysterectomy	In my view, the main weakness in this paper is that the authors made many assumptions regarding their data set that need to be properly justified.	"The major weakness is that the value of ""multi-task"" dataset is not fully presented. There have been plenty of surgical vision datasets. The key feature of AutoLap is multi-task. So, what is the relationship between the three tasks? How to leverage the correlation over tasks? How is the feasibility? Without deep analysis and discussion about ""multi-task"", the value and innovativeness of another new dataset are limited.

The AutoLap dataset should be positioned in the surgical research area. A table is suggested to compare the features of AutoLap and existing datasets. See https://arxiv.org/pdf/2011.02284.pdf.

The sample number for segmentation task is sufficient. But are the sample numbers for the other two tasks sufficient for training DNN?"	"""Towards surgical automation"" has been stated as the motivation of providing this dataset. However, the videos used for this dataset were collected from non-robotic laparoscopic surgeries. Given that robotics is an essential component for automation, it is not clear that how evaluating the ML approaches on this dataset can be directly transferred to robotic video datasets. As for robotic surgical tools would appear differently in the videos compared to conventional laparoscopic tools and the generalizability of ML approaches is still doubtful, the value of this dataset for surgical automation would still be questionable."
058	Automated Classification of General Movements in Infants Using Two-stream Spatiotemporal Fusion Network	"In general, the contribution and novelty is limited. The proposed method is a combination of the processing networks and the two-stream network. Is backgroud removal a necessary step? For one thing, it's very difficult to remove them all. Then, based on the most recent human motion and video content analysis research, it's unncessary to do that.
Lacks of some detailed of compare methods such as paper [26], [18] and some necessary analysis of why they(sota) work much worse here. In Table 1, the preprocessing networks were applied to both ""Baseline"" and ""Ours"" and all outperform [26] and [18] a lot. It seems that preprocessing is playing a major role. But the ablation study in Table 2 shows that it is not so important. It confuses me here the single stream network without preprocessing works so good. And it may be more convincing to test the proposed method on other public datasets.
In table one, the comparing methods are [7], [18], [25] and [26]. But they are not SOTA method in human motion analysis, and two methods are published in 2015. Latest video analysis and human motion classification methods from computer vision fields should be considered and compared."	"There is no clear novel technical contribution. It is at the boundary of a  ""Methodological studies"" and an ""Application study"".
The evaluation and comparisons of the method could be improved:
Preprocessing (3.1) + final evaluation and comparisons."	"It is not clear if the dataset will be publicly available. 
WM, FM and PR should be clarified what they are. 
The angle and scale preprocessing is similar to the idea of  ""In-Bed Pose Estimation: Deep Learning With Shallow Dataset"" , not cited."
059	Automatic Detection of Steatosis in Ultrasound Images with Comparative Visual Labeling	"There are some typographical errors, e.g. page 6 ""A drawbacks..."". Also some abbreviations are used without explicitly mentioning them, e.g., DL in the abstract. It may not be evident to all readers that DL is 'deep learning'."	"the technical novelty of the manuscript is limited, authors used well-known techniques for the ranking (RankNet implemented as a feed-forward network) and for the image classification (InceptionResNetV2 pre-trained on the ImageNet).

the proposed method is more time consuming compared to the conventional labeling. The usefulness of the proposed method would be probably limited for a larger dataset.

Authors performed multiply experiments to calculate various cut-offs, performance metrics and to determine hyper-parameters. It gives an impression that an independent test set was not used for the overall method evaluation.

experiments were performed on relatively small datasets, which makes it difficult to assess the usefulness of the methods (see the next two comments)

""Two classification CNNs were trained on Dataset 2 using SVL and CVL+RankNet labels, and tested on Dataset 1 ... ROC-AUCs were 0.89 (CVL+RankNet) and 0.86 (SVL), and the difference was not statistically significant (p = 0.34)""
It seems that the classification performance did not significantly increase thanks to the proposed method. The same issue is associated with the results presented in Table 2.

the proposed method did not improve the F1 scores for 2 out of 3 annotators, according to the McNemar's test and Table 1. It seems that the labels obtained with the conventional approach were of good quality. For a dataset of 50 cases the 10% drop in accuracy would correspond to 5 mis-classified examples, which is after all a small number. This suggests that the proposed method improved the labels for only several cases compared to the conventional approach."	"According to figure 2, what is fed to the RankNet are one-hot encodings of the image indices, not the data from the images themselves, and the network creates the scores based on these one-hot encodings. I believe the authors should elaborate more on this, to clarify whether this is really the case, and if it is how it leads to the generation of better labels.
Both datasets used by the authors are very small ones, and since the authors have not presented any information about the data stratification, I believe the networks are trained on very few samples, which in my belief, affects the credibility of the results. Authors should mention if they have augmented the dataset, or if they have trained and tested on the same dataset (For the results of Table 2, for example), which they shouldn't have!"
060	Automatic identification of segmentation errors for radiotherapy using geometric learning	X	"Although, the work is verry interesting methodically a few major weaknesses can be identified:
Terminologically the authors claim their method to be self-supervised (training with synthetic data) and unsupervised (pre-training of the feature extraction), however neither is the case. I would categorize the creation of the perturbed contours as a smart form of data augmentation and the pre-training to be supervised as the class of each patch to be on the contour or not is directedly extracted from the reference contour.  The usage of both terms self-supervision and unsupervised pre-training is confusing and does not follow their definitions. 
The authors use the public Dataset of Nikolov et al. as basis for their work. The dataset contains human bias freed reference contours (per-reviewed by expert radiologists and oncologists), and contours of a radiologist which are used in the clinical study of Nikolov et al. as a human reference. The authors simply claim to use one of the contours. Although the dataset is only used as a proof of concept for the work, the authors should make clear which contour they use and not train on human biased contours. 
The method is trained and validated on the same synthetic dataset, as the approach is methodically novel there is no reference method available, however, it would add a lot of value to validate the method on real erroneous contours i.e., the second contours of the used dataset created by an independent human annotator or the output of a model.
The selection of the hyperparameters for the creation of the synthetic dataset as well as the process of meshing the contour seems to be arbitrary. It could make sense to perturb the contours within the range of a meaningful observer variability or an expected error range of a current automated segmentation approach, both values that are studied in the work of Nikolov et al.. Although, the authors claim in the discussion that a future step is to generate training data based on real observer variations, could a meaningful parametrization of the perturbations already be integrated. In this sense, also, the resolution of the mesh could be identified rule based regarding the size of the organs, while the chosen values seem to work well for the tested organ, it is not clear how the mesh resolution would suit much larger or smaller organs. Finally, also the selection of five bins/classes of distances is missing an argumentation.
The ablation study shows that the pre-training of the feature extracting CNN does not lead to an increase in performance, dedicating less than a whole section for that process could be considered."	"Idea is clearly presented. However, it seems like only five level of errors are analyzed, from larger than 2.5mm to smaller than -2.5mm. Is there any reason for you to use the two threshold of 1mm and 2.5mm?
The dataset description is rare. At least we should know the resolution of the original CT scan, since you talk about errors less than 1mm. If the pixel spacing of CT is large than 1mm(especially in axial direction), the results become  meaningless.
The paper is more clinical driven instead of technique itself. As mentioned in radiotherapy, normally there is a margin between PTV and CTV. The margin is sometimes 5mm. If so, the importance of 1mm-2.5mm error check is not that important. So, talk about the motivation.
From Fig 4, subfigures a d are easy to understand that large errors are easier to be recognized. However, from b, it seems like the decrease of performance on small errors(confusion matrix value of -1mm-1mm)is not that obvious comparing with that of large errors. How do you comment that?"
061	Automatic Segmentation of Hip Osteophytes in DXA Scans using U-Nets	The contribution is rather fair. Use of the existing U-Net to segment osteophytes on DXAs.	"While the osteophyte detection performance is relatively high, the segmentation one is rather low for downstream clinical applications (Dice score of 0.6-0.7). One of the potential reasons is that the applied segmentation method is very basic (essentially, UNet without hyper-parameter optimization, as stated by the authors). Overall methodological novelty of the work is unclear.
Otherwise, no notable weaknesses."	"The proposed U-Nets require a priori landmark detection step using bone-finder software. The authors mentioned on page 6 that landmark positioning error in this step may adversely affect the U-Nets performance especially in the lateral acetabulum (corresponding landmark point 78). Given the power of deep networks, would it be possible to learn directly from the raw DXA scans and integrate the landmark placement algorithm into the U-Net architecture?

820 scans out of 41,160 subject were excluded due various reasons including image quality. How did you assess the image quality?

-The dice metric for segmentation accuracy is fair suggesting potential improvements would be possible by optimising the network further. You may report the dice index for manual annotation between the two radiographers and discuss the proposed framework utility in light of this results.
Minor comments:

In figure 4 (second row), it is better to overlay the ground truth mask on the actual DXA scan.
Page 5, 'with He normal initializer' -> 'with the normal initializer'?"
062	Automating Blastocyst Formation and Quality Prediction in Time-Lapse Imaging with Adaptive Key Frame Selection	"The main weakness is the limited analysis of the experimental results, and in particular it would be useful to expand on the frame selection results. (also refer to detailed comments). More qualitative results could be provided as supplemental material, including failure cases and interpretation of the corresponing selected frames.
Additionally, there are no comments on limitations of the proposed method, current limiting challenges, and future research. Conclusions could be improved by incorporating such comments rather than just summarising the paper."	Some statements and experimental settings should be clarified	It would be nicei if the authors could discuss about potential limitations about the proposed method.
063	Automation of clinical measurements on radiographs of children's hips	The Random Forest Regression Voting Constrained Local Models framework was previously applied to segment proximal femur from AP hip radiographs, by identifying anatomical landmarks around the femur contour. The contribution in this work is in extending the idea to also include acetabulum landmarks and proposes the application in a new patient population. As such, the work is making a contribution to the field, but does not have a unique novelty.	"This seems to be a direct application of known method (RFRV-CLM). The novelty is The authors do not describe any of the models' details, model training etc. The methods section focusses largely on experimental design.
As the authors self report there is a large spread in their ground truth annotations.
No comparison to other methods is provided."	"Lack of methodological novelty
The evaluation is not based on appropriate metrics and results are not fairly interpreted and discussed.
It is impossible to reproduce this work based on the methodological details provided in this paper."
064	BabyNet: Residual Transformer Module for Birth Weight Prediction on Fetal Ultrasound Video	"The network is not innovative enough. The proposed network only extends the temporal encoding based on the BoT.
Some details about the dataset and experiment is ignored (see detailed and constructive comments)."	Descriptions about the data processing mode of the BabyNet lack detail.	"The clinical significance of the paper is unclear, and further explanation of how it could be useful in clinical practice could be included. The authors state that ""Accurate prediction of FBW is critical in determining the best method of delivery (natural or Cesarean)."" however this statement needs appropriate citations and more detail, e.g., Is this decision solely based on fetal birth weight or other factors are also considered? Is this widely adopted in practice? When is the decision made (the paper evaluates videos only from 1 day before delivery)?

The paper does not discuss the interpretability aspect of the proposed machine learning model. Quantitatively the results look promising compared to existing architectures, however, it is not clear what makes this architecture unique and suitable for the given problem. Do the authors analyze how the model makes the decision qualitatively? Which visual features does this model learn to make the accurate estimation? Where were the highest and lowest attended parts of the video for the MHSA layers?

The method was evaluated on a dataset on 225 videos. In general, vision transformers require large-scale datasets for training and suffer from overfitting problems on smaller datasets. The paper does not mention this aspect and how was it overcome (e.g., regularization and augmentation).

Future directions of the work are missing in the paper."
065	Bayesian dense inverse searching algorithm for real-time stereo matching in minimally invasive surgery	-	Quantitative evaluation was not performed on either  in vivo or ex vivo data.	"In the result, one may question the interest of including the DNN approaches. As underlined by the authors, these methods have a sensitivity to the training.
From a quantitative point of view, the numbers demonstrate an improvement which is quite modest. The qualitative images are interesting, but it is difficult to assess if the improvement is significant for a clinical user."
066	Bayesian Pseudo Labels: Expectation Maximization for Robust and Efficient Semi-Supervised Segmentation	"Authors have not mentioned how many iteration of EM is performed in the experiment.
While SegPL significantly outperforms existing semi supervised approach, the SegPL-IV method gave only minor improvements over SegPL."	"The idea is novel and interesting. But the some descriptions are not precise enough. In addition, the experiments datasets are too small, the experimental results are different from many previous works (the reported basline and comparison results are too bad).
(1) To best our knowledge, there are too many pseudo label based semi-supervise segmentation methods, this work is not the first one!!!
(2) The two datasets are too small, why not use the whole BraTS dataset?And the results of BraTS is too bad, it's not convincing and reasonable. The CARVE dataset just has 10 volumes, why not use a big dataset? There are too many large-scale and open-access datasets.
(3) If possible, please provide the results of distance based metric, like HD or ASD. The statistical analysis results also should be reported.
(4) The methods missed too many recent works about the same topic[1,2,3,4,5, ......].
[1] Semi-supervised learning for network-based cardiac MR image segmentation, In MICCAI2017.
[2] Semi-Supervised Segmentation of Radiation-Induced Pulmonary Fibrosis from Lung CT Scans with Multi-Scale Guided Dense Attention, in TMI2021.
[3] Efficient Semi-supervised Gross Target Volume of Nasopharyngeal Carcinoma Segmentation via Uncertainty Rectified Pyramid Consistency, in MICCAI2021.
[4] Semi-supervised Left Atrium Segmentation with Mutual Consistency Training, in MICCAI2021.
[5] Semi-supervised Medical Image Segmentation through Dual-task Consistency, in AAAI2021."	No major weaknesses. I only have some minor comments (see below).
067	Benchmarking the Robustness of Deep Neural Networks to Common Corruptions in Digital Pathology	"Crucial detail of how the corruptions are implemented is missing and not available even in the supplement. No promise of code or dataset release is made. This has strong negative impact on reproducibility. This also make it hard for us to judge the true quality of the corruption. While the experimental results seem to suggest that the corruption is working as intended, there could be some unexpected deviation from reality (for e.g., not enough marking included, or deviation from non-random nature of pathologist marking). Releasing code and having more details about the method will improve the strength of this paper considerably.

I find each experiment to not be thorough enough. While high level conclusion drawn are interesting, there are many questions left unanswered, leaving that high-level conclusion rather weak. For example, I find correlation to be a weak metric to show that if a model is more robust to corruption, it is more generalizable. It could just be that the corruption creates more out-of-distribution samples, so it happens that the errors on two sets are correlated. I think more investigation is needed. One thing that comes to mind is to train with corrupted images and see if generalization actually improve. In my opinion, this would be a stronger evidence that the proposed corruption is working as intended.

Another example is the reverse relationship between severity and the model confidence. Isn't this expected result? As the corruption push the image outside the distribution, the model is likely to make higher confidence negative prediction in this case?

There are some discussion, that I think would be better left in supplementary to leave room for more important points (such as one discussed above). In particular, the exact formula for the metrics could be left largely in the supplement. The basic idea is relatively straighforward and do not need to be explained in depth in the main paper, in my opinion."	"Authors claim that the proposed corruptions are easy-to-use in practical settings since they are implemented by being plugged into the dataloader class.  Although Sec.3.1 describes how it implements in brief, it is still unclear how it implement and why it is easy-to-use.
The minor mistakes (e.g., the label of y-axis in Fig. 3.) should be corrected."	"I don't understand why a local dataset could serve as a benchmark, given that it was not published.
As known from domain shift investigations, model robustness towards a covariant shift in the input is highly dependant on training runs. Single training runs, as done in the reported experiments, are thus very likely to not really be conclusive for such investigations.
The authors write that the AlexNet scores the best in terms of their rCE metric. However, looking at Table 1 I would only infer that the CE metric is similar across all model architectures, so given the formula for the rCE metric it is natural that the worst performing model on clean data achieves the best value here. This, however, does not imply (as the authors motivate) that this metric is a good proxy for model robustness.
I also don't agree with the notion ,,although CNNs are constantly improved in the past decade, their performance
on corrupted images changes little while causing the incredibly worse robustness"": Some of these corruptions deteriorate the image severely, and a reduced metric might be highly correlated with a reduced performance by human experts as well. Thus, this is not a sign of weak robustness, but might be related to just information destroyed in the image. Robustness is only within the limits of information being still contained within the image. If the corruption scheme destroys diagnostic information within the image, a reduced performance is to be expected and cannot be attributed to a lack of model robustness.
The authors state that one of their findings is that overfitting harms the robustness of the models. But that's actually the very definition of overfitting.
The paper structure is also a bit unclear. Parts of the experimental results are already reported in the introduction. Further, the authors did not discuss the limitations of their approach in any way."
068	BERTHop: An Effective Vision-and-Language Model for Chest X-ray Disease Diagnosis	"While the results show the effectiveness of using PixelHop++, I was not able to clearly understand why the use of PixelHop++ is improving the learning outcome.
The description of PixelHop++ needs to be articulated - what do you mean by image at different frequencies? why does this improve the result?"	"How PCA is applied to PixelHop++ channels?
The pre-processing should be explained, despite only say that is the same as TieNet
Trying the method also on other datasets could confirm even more its effectiveness."	"Although the results are supporting the claims of the paper, the reviewer is concerned about the generalizability of the proposed architecture on other datasets. Also, the reviewer is interested to understand the change in results when the degree of unlabelled data is changed. Implying what will be the disease-wise AUC when 10%, 20%, 30%, and so on amount of the data is labeled. This is because a mixture of labeled and unlabelled is common in medical datasets.
Some areas of the paper were gramatically difficult to be followed."
069	Bi-directional Encoding for Explicit Centerline Segmentation by Fully-Convolutional Networks	"Limited Novelty, such heatmap based methods are widely used in landmark detection/pose estimation tasks [11]. The author applied it to such a specific task.

Inefficient evaluation. The author verified their methods on synthetic data. Why not evaluate it on a real-dataset?

overclaimed. The author argues that segmentation-based methods can generate some false positive pixels. However, the proposed method can also generate an incorrect centre line via the wrong keypoint locations."	"In page 3, the authors mentioned that 'p_i and p_{i+1} connect with an edge'. Does this denote a direct connection with a straight line? Or the edge can be a curve depends on other characteristics (if so, how?)? The 'edges' in Fig.1 prediction look like curves to me.

In page 3, the authors mentioned that non-end points are sampled from the centerline. I though those key points directly predicted by the HRNet? How is this sampling related to the predicted n key points?

In page 3, the authors mentioned that 'we fix n to be identical for all tubes regardless of their shape and length'. If so, the problem is essentially a signal sampling problem, where the authors use the HRNet trying to capture the most significant waves. Therefore, for smaller n, there will be an under sampling problem. For larger n, it will become oversampling. Although the authors provided an ablation study w.r.t n (Table 2), the evaluation was bound by the specific dataset and specific tube geometry and hence is not generic. This problem is actually an limitation of the proposed method, the authors should at least have some discussions on this.

Also, the width of the centerline can be another important factor for the performance. Apparantely, assumptions on all tube-shaped objects are in the same width is invalid. It would be better if the authors can explain their choice on width selection and even better if an ablation study can be provided."	"From my understanding, the paper's main contribution is the formulation of the bi-directional centerline encoding. The evaluation, however, does not focus on showing a benefit of this encoding. Specifically, different architectures are used for the segmentation tasks and the proposed method. Is is therefore not possible to judge whether improvements are mainly due to a better suited architecture ( or due to proposed encoding. This could have been easily avoided since most (all) of the reference methods could also work with the proposed encoding as targets.
It is not clear how / whether the hyper-parameters used for the different methods were tuned. I would expect that especially aspects like learning rate, #epochs, etc. may be different across architectures.
The focus and the presentation of the results is not ideal from my perspective. Important results (standard deviation, quantitative values) are only presented in the supplementary material, an analysis of failure cases (or non-optimal results) is missing."
070	BiometryNet: Landmark-based Fetal Biometry Estimation from Standard Ultrasound Planes	"(1) I supposed the third part of contribution should not be considered in your main contribution if the annotated dataset is non-public.
(2) The author did not discuss the influence of simple landmark annotations and fine annotation on accuracy. In addition, there are differences between different obstetricians in the labeling process, and the protocol among them should be given in the paper. 
(3) For different detection tasks (BPD and OFD), do you train one network separately for each task or train one network simultaneously for multi-task detection? If one network is trained for each biometry, the process of this work is complex.
(4) A comparison with the advanced methods on fetal biometry estimation is missing."	Previous works on automatic landmark detection in medical images, including ultrasound are not mentioned in the introduction.	The motivation of the landmark class reassignment should be detailed.
071	BMD-GAN: Bone mineral density estimation using x-ray image decomposition into projections of bone-segmented quantitative computed tomography using hierarchical learning	"The paper is a hard bit to follow, due to lack of details and lack of precision in the definition of the equations and variables used. 
Several steps of the proposed method are not described, e.g. cropping, registration, etc.
The original data used for the study are not described.
Experiments were achieved on a limited dataset and were not validated on a different data."	The dataset could be better described.  How much variability in vendor, time subjects?  What is the image quality required, particularly for the X-ray?  This is an important question and it is not well dealt with.	While it is an strength for the proposed framework to be trained on a small dataset with N=200, this is a weakness when it comes to validation. I would suggest testing your method on larger datasets without paired QCT to see how this may be generalised to other studies.
072	Boundary-Enhanced Self-Supervised Learning for Brain Structure Segmentation	"The major weakness lies in the experiment section.

I do not understand why the rest 5 baselines (3D-Rot, 3D-Jig, 3D-Cpc, PCRL, Genesis) perform worse (most of the time) than training from scratch in the fine-tuning stage, especially when the labeling proportion is 10%. Because self-supervised learning has been shown to be more effective when the amount of annotations is quite limited.

The authors should conduct more analyses about why RubikCube performs better than other baselines, because the effectiveness of Genesis and PCRL have been validated on other challenging tasks."	"I have three major concerns, elaborated below.

One of the main claims of contributions that the registration pretext task matches and enhances boundaries concerns me. According to Eqn. (3), the authors employ voxel-wise similarity loss for the registration, which does not emphasize boundaries by definition.
Several aspects affect the reproducibility of the paper. First, the hyperparameters used to generate the supervoxels are not given, nor is the impact of different hyperparameters on performance investigated. Second, it is unclear how the supervoxels of interest (and background) are identified. Third, there is inconsistency regarding the evaluation setting (train/val/test split versus five-fold cross-validation). Fourth, it is unclear how many epochs are trained for both pretraining and fine-tuning. Lastly, no code is submitted, nor do the authors promise to publish codes.
The improvements upon existing SOTA seem minor."	"The main weakness of the paper is the limited technical contribution and experiment performance. The authors claim they are the first to introduce registration as a proxy task for self-supervised learning, which is true to my knowledge. However, image registration has already been used in a self-supervision manner in [1], which limits the technical contribution of the paper. In experiments, the improvement compared to other self-supervision methods is very limited (less than 1% in most cases) and no information on the significance test is provided. This makes the method less convincing on boundary segmentation.
[1] Li, Hongming, and Yong Fan. ""Non-rigid image registration using self-supervised fully convolutional networks without training data."" 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018). IEEE, 2018."
073	BoxPolyp: Boost Generalized Polyp Segmentation using Extra Coarse Bounding Box Annotations	"Why are accurate pixel-level segmentations needed clinically? Even if a polyp is detected perfectly, the resection techniques are very coarse. In my opinion, it is important to find polys, but an exact boundary prediction is not necessary clinicaly.  Perhaps it would be good to put the clinical context in the paper to balance the technical contribution - e.g. could this be needed in an autonomous robot setting, etc?
The models were only trained and tested on polyp frames, so specificity has not been evaluated. A pretrained model on a segmentation dataset is needed to perform FFS."	The paper presented several strategies to improve the polyp segmentation performance through make full use of the pixel-wise annotations and extra bounding box annotations. However, the connection and relationships among the main components in the proposed method are not clear. Additionally, some important details about the experiments are missing.	"The paper lacks clarity and motivation of the paper is not clear. Please see constructive comments for details.
The paper uses previously proposed techniques in their model and propositions
Math symbols and equations needs to be checked. For e.g., images and labels needs to be a vector/matrix of dimension d and hence represented either in bold or capital
Question remains, is the method generalisable to other unseen datasets?"
074	Brain-Aware Replacements for Supervised Contrastive Learning in Detection of Alzheimer's Disease	"I did not see any major weaknesses in the paper. Only a couple of minor points:

The paper claims: ""Also, BAR implicitly forces the model to pay attention to the relationship between medically relevant brain regions, thus making it more clinically relevant."" This statement implies causality. Although the authors have presented some attention maps in the supplementary materials, causal feature learning requires deeper analysis.
Although the authors have targeted a very specific problem to solve i.e., AD detection in MR, but the general idea behind the method is not specific to AD. It would've been better if the authors included other clinical problems of significance."	"The applicability of the BAR augmentation is very limited and can not be generalized to other use cases since it sets hard constraints on the data: alignment and pixel precise information.
The evaluation is very limited since it does not compare to any of the state of the art methods. Since the method builds on  top of ViT, I am missing the comparison with methods using convolutional networks."	"They can compare the attention maps (grad-cam) from the different models for some more detailed analysis but not just some metric values. Since they focus on the AD classification, they can compare the performance of replacing brain regions related to with AD (e.g., hippocampus) or other unrelated regions.
They should compare with more SOTA pretrain methods. In this paper, they just compare with some simple baselines, such as naive contrastive learning, cutmix."
075	Breaking with Fixed Set Pathology Recognition through Report-Guided Contrastive Training	"The model has four different hyper parameters \lambda_1 - \lambda_4 to weigh the contributions of various loss terms. How do they arrive at the settings mentioned below Eq. 5? Which of these parameters is the performance most sensitive to?

Some of the differences reported in Table 1 are rather minor and do not report a measure of standard deviation. How consistent are these differences and would they hold up under a statistical comparison?"	In some cases, the improvement in the results is not very big. A statistical analysis would be helpful to better understand the significance of the obtained improvement	"The novelty of the framework is limited.

In my opinion, this work mainly adopts the existing contrastive language-image pertaining model, i.e., CLIP [1], from computer vision to a new domain or a new task. However, introducing large pre-trained models to solve a new downstream task cannot bring new insights to the community. It's very important to explain why the proposed approach can improve the performance and what problems can be solved by the approach?
There are some previous works, e.g., [2], that have attempted to adapt the contrastive language-image pertaining model into the medical image analysis field. The authors neither cite nor compare with it.

[1] Learning Transferable Visual Models From Natural Language Supervision. 2021.
[2] Contrastive Learning of Medical Visual Representations from Paired Images and Text. 2020."
076	Building Brains: Subvolume Recombination for Data Augmentation in Large Vessel Occlusion Detection	"Paper is hard to follow and clarity could be improved.
Motivation for this work can be improved
No actual images showing the recombination of vessels"	"*	The augmentation method might only be useful to limited applications, thus the impact is limited to a small field. 
*	No public datasets. No access to codes."	"The motivation for developing an automatic classification method is not clear. Section 1 reads ""Despite common anatomical patterns, the individual configuration and appearance of the vessel tree can differ substantially between patients, hence automated and accurate methods for LVO detection are desirable"". I don't see why differences between vessels trees across patients justifies a desire for automated methods. Can a radiologist perform this task quickly and accurately? If yes, why do we need an automated method?"
077	CACTUSS: Common Anatomical CT-US Space for US examinations	"No patient data with AAA diagnosis or borderline AAA. Therefore the generalizability of the proposed method for actually detecting an aorta >8mm diameter is questionable, as this case is not tested.
No mention of statistical testing on the results.
Focus on Dice score is not correct for this clinical application, as distance errors are more relevant to the AAA diagnosis."	"The real ultrasound images came from relatively young individuals.  These people are probably ""easy"" to scan.  People who need AAA screening are much older and they are probably more ""difficult"" to scan and could have worse image quality.  This could greatly impact the performance.
Commercial solution for AAA diagnosis is already available using 3D ultrasound alone (no RUS or camera).  There is no mentioning of this."	"A relatively small number of patients were used for evaluation and many of the same patients appear to have been used to train the comparator US U-net model and serve as test patients. The full heterogeneity of anatomy encountered in practice cannot be captured with so few patients and so failure modes appear unexplored.
The clinical need for automatic aortic segmentation on US is not entirely clear. US doppler studies are typically performed by a trained US technologist for whom it takes minimal effort to make aortic measurements. While CT has superior accuracy, discrepancies in US measurements of aortas in a screening context are rarely clinically meaningful.
The framework is complex and it can be challenging to follow which component is being discussed (US simulation vs CUT vs IR space aorta segmentation), especially with regard to data used for training/validation/evaluation."
078	Calibrating Label Distribution for Class-Imbalanced Barely-Supervised Knee Segmentation	"There is little consideration of clinical translation.  The methods developed do not seem to be designed specifically for knee cartilage segmentation this is rather a task to demonstrate the utility of the methods for improving semi-supervised learning.  However the authors do not comment on how these methods can be translated for other tasks or how they affect clinical translation
The ablation studied do not consider all combinations of the methods.  It is unclear how well PRC or DUS work in isolation
The authors do not present cross-fold validation results which would be helpful to understand the variability of the effect.  It seems that the methods would be highly dependent on the choice of labeled and unlabelled specimens particularly because of the increased dependency on the segmentation distribution.  Are the methods increasing performance at the expense of greater variability?
There are other methods based on similar ideas that could be commented on, focal loss or oversampling

R. Zhao et al., ""Rethinking Dice Loss for Medical Image Segmentation,"" 2020 IEEE International Conference on Data Mining (ICDM), 2020, pp. 851-860, doi: 10.1109/ICDM50108.2020.00094.
R. Mohammed, J. Rawashdeh and M. Abdullah, ""Machine Learning with Oversampling and Undersampling Techniques: Overview Study and Experimental Results,"" 2020 11th International Conference on Information and Communication Systems (ICICS), 2020, pp. 243-248, doi: 10.1109/ICICS49469.2020.239556."	"The basic semi-supervised framework used in the paper is based on CPS [3] (i.e., section 2.1), and its ethodological contribution is not notable. The major contribution could be in the section 2.2, to relieve the class imbalance problem in the segmentation of knee bones and cartilages using 3D MR data. In section 2.2, the ""Probability-aware random cropping"" may be effective, but this part could only be counted as an improvement for the preprocessing part, not for the core part of the semi-segmentation framework.
And the ""Class-aware weighted loss"" module is a minor improvement of class weighting, comparing with some traditional way, like the inverse rate of the volume of one class to the whole.
At last, other papers, e.g. [5], it also had some similar adaptive/dynamic weighting approaches. I do not find some substantial improvement in yours."	The conclusion is not at all written in the manner it should be and it repeats the contribution of the study as already mentioned in its introduction. This is the only weakness of this paper. Apart from this, all sections are clearly written.
079	Calibration of Medical Imaging Classification Systems with Weight Scaling	Adding some description about the limitations and problems of the proposed method would be desirable; as well as insights about future line of work	The concept of keeping the same samples' confidences high is the aspect that is not exactly clear to me when reading the paper - if its a weight scale but then fed back to the classifier, the values would change for predicted confidence, as you still pass through a soft-max layer?	"The notation used in the paper is not clear. Please use vector notation to make variables clear to the reader (for example, some variables are referred to as being vectors but they are written as constants, the same goes for subscripts). Also, a number of symbols are used for denoting different purposes which makes reading the manuscript confusing (x and y are referred to as inputs and outputs, respectively but later on, both variables are used to refer different patients [e.g., patient x and patient y]).
The main problem I see in the paper is the usage of ECE as a metric to showcase results. As noted by the authors, although ECE remains to be a top-contender as a metric of study, shortcomings of ECE as a metric of calibration is well documented[1]. In light of this information, having better results showcased (only) with ECE does not mean much. Authors also note that some of the shortcomings of ECE is alleviated with adaECE but refrain from providing experimental results on this metric, why? While reading those lines, I had the expectation to see results for both ECE and adaECE. Yet it is not there. This is especially confusing since the authors themselves acknowledge that ECE is not a good metric of evaluation. Then why would a new method that shows superior results showcased with ECE be useful?
A number of alternatives of ECE has been discussed in the works of [1] (also referred to in the paper), would it be possible to show results with metrics discussed in this paper (for example adaECE, TACE, SCE)? Does the proposed method still achieve better results when measured with these metrics? If the usage of those metrics is not possible, what is the justification?
[1] Nixon et al., Measuring Calibration in Deep Learning"
080	Camera Adaptation for Fundus-Image-Based CVD Risk Estimation	"The motivation of selection of CVD risk estimation after the camera adaption is not clear.
The organization and the writing needs improvement."	"The description of the method is not clear and accurate, for instance, ""o^r, o^r, l"" in chapter 3.2 paragraph 1 is a spelling mistake and the meaning of the superscript of variable ""y"" in chapter 2.1 paragraph 2 is not mentioned. Most references of this paper are review articles, lacking of references that related to ophthalmic images or similar research. The experiment data is not sufficient and the results are not convincing."	The loss function in Fig. 2 is not described in detail.
081	Capturing Shape Information with Multi-Scale Topological Loss Terms for 3D Reconstruction	"Improvement seems fairly marginal for the nuclei dataset, as compared to red blood cells, and there is not much explanation for why this may be the case.  This would be relevant, as it is discussed that computation can be a bottleneck (though speed-ups are proposed) in optimization with respect to this topological loss function, and so providing suggestions, based on dataset, on when this extra computation may not be ""worthwhile"" would be helpful for a practitioner."	"The construction of the cubical complex is not adequately explained. What do the nodes of the complex correspond to?
The description of topology and persistent homology may not be sufficient for the majority of readers who are likely to be unfamiliar with the approach."	"The main weaknesses are in the experiments. First, Sec. 4.2 mentioned that the regularization strength parameter \lambda for L_T is optimized in [10^-3, 1x10^2], which is a very wide range. \lambda = 0.1 is used for all experiments after the hyper-parameter search. This implies that performance degradation should happen at some point when \lambda > 0.1. The problems are:

There are no results on the robustness of L_T for different values.
This is no explanation for why it starts to decrease the performance at some point.
Does L_T directly work without BCE or Dice losses (L=L_T instead of L=L_G+\lambda L_T)? If yes, what's the performance of using only L_T? If not, why does L_T itself not achieve better or comparable performance than BCE or Dice?

Besides, although the loss is topology-aware, the 3D masks shown in the submission all have a Betti number of 0, which means a single connected component. It is not very clear if the L_T loss can improve the reconstruction of masks with a more complex topology."
082	Carbon Footprint of Selecting and Training Deep Learning Models for Medical Image Analysis	No major weaknesses. The empirical part is limited in scope, but sufficient to make an important contribution.	The methodology itself is not novel, as different existing tools to measure carbon emissions are used. However, I appreciate that the point of the paper is to make the community aware of this issue and recommend guidelines.	"1 The main contributions of this paper must be further summarized and clearly demonstrated. This reviewer cannot distinguish the new findings of this paper and the existing methods/approaches in the literature. This reviewer suggests the authors exactly mention what is new compared with existing approaches and why the proposed approach is needed to be used instead of the existing methods. 
2 The theoretical depth of this paper must to be strengthened. The principle of the proposed approach is not clearly explained, and there is no equation throughout the manuscript.
3 There is no comparison with other state-of-the-art methods in literatures. The effectiveness and superiority of the presented method in this paper should be verified through such comparisons.
4 This reviewer would like to suggest the authors add a flowchart of the presented method and the corresponding description to enable readers to have a better grasp of the approach as a whole.
5 The novelty and contribution of the present work need further justification. Authors need to add more results with more discussions to thoroughly support the main findings.
6 The practicality of the approach should be further discussed."
083	CaRTS: Causality-driven Robot Tool Segmentation from Vision and Kinematics Data	"I think fundamentally this paper is solving a more difficult problem than segmentation, which is 3D pose estimation. Although the idea is interesting, I am not convinced that this approach will yield better results in the long run. The challenges of 3D pose estimation, when the instruments are moved into complex articulations or when the tool movement is fast will cause this method to fail. Additionally, this method requires kinematics access (not guaranteed), 3D CAD model availability (again, not guaranteed) and a very powerful GPU to even get close to real time (which the authors admit is far away). Additionally it won't be able to handle laparoscopic instruments which are often used in robotic procedures.
I don't see the comparison to a vanilla UNet as convincing. For this approach to be comparable, it should be compared with a UNet (or ideally a newer architecture) that has at least been trained with augmentation for smoke and other artifacts so that the fall off can be compared. Looking at the failures of the UNet in figure 1, I don't see these as hugely challenging cases representing a fundamental limit of the state of the art. These are quite easy images that a well trained network should be able to handle."	"1) Despite the fact that the model does not directly link the image to the segmentation mask, it requires a pretrained segmentation network (a UNet) to extract semantically-rich features over which the difference between the rendered and observed image forms the utilized loss function (page 6). This network leverages the standard paradigm of mapping input images directly to segmentation masks. Conclusively, the overall method implicitly employs the ""contemporary"" causal model, in the form of a pretrained feature extractor. This merits some discussion in the paper."	"The authors state that the proposed model is very flexible, potentially capable of ""direct estimation of robot kinematic parameters, including joint angles, base frame and camera transformations, in an end-to-end fashion."". However, the authors cast the model experiments for surgical instrument segmentation. When focusing on the evaluation, the authors only compare the model against one segmentation model, U-Net, which is already 7 years old. I encourage to authors to compare against more recent segmentation models to compare against (e.g. EfficientDet, HRNet, Swin Transformers, ...).
In addition, the model seems to be designed to only work with binary segmentation (background and instrument) and in absence of occlusions. These are important limitations which makes the proposed framework interesting but its potential and real applicability is unclear"
084	CASHformer: Cognition Aware SHape Transformer for Longitudinal Analysis	The ablation study is not very sufficient. The first two rows of the bottom Table 1, proof that finetune LN layers with pretrain weights is better than train from scratch, but we can't know if only finetuning LN layers is better than finetuning all layers.	"I notice the frozen pre-trained transformer is also similar to the ideas of Adaptor [1], Prompt [2] in the natural language process.

[1] Houlsby, Neil, et al. ""Parameter-efficient transfer learning for NLP."" International Conference on Machine Learning. PMLR, 2019.
[2] Schick, Timo, and Hinrich Schutze. ""Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference."" Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021."	"Overlap with ref. [24]: TransforMesh [24] is the main state-of-the-art comparison method, and when reading the original paper, some considerable overlaps with the proposed submission are noticeable: in the structure (section 2.4, evaluation...), notations, figures (fig. 2 in [24] vs fig S2 here), and quite some amount of text (transformer architecture description, ""missing shapes"" paragraph, etc.). Still the proposed method is showed to outperform [24] and some clear methodological novelties are provided with the Cognitive Embeddings, Cognitive Decline Aware Loss and Frozen pre-trained Transformer training.
Authors could provide further discussion on the use of such modelization: predict diseases among neurodegenerative pathologies in clinical practice ? if yes, in what time horizons ? or maybe to better understand those complex diseases for researchers, neurologists ?"
085	Censor-aware Semi-supervised Learning for Survival Time Prediction from Medical Images	The methodology of the images used in the pathology case is not clear. The paper cites the work in https://www.pnas.org/doi/epdf/10.1073/pnas.1717139115 mentioning  1505 patches. However, in the cited work, there is no mention of these 1505 patches, and the only data available are the whole slide images.	"*	The authors use only two datasets for their evaluation. I would have expected more. 
*	The authors reported only the best run when comparing their approach to others. 
*	The authors used only image data, while the approach should be usable with other data types as well."	"For the CA-MSE loss, why the authors adopt MSE loss instead of MAE loss considering the evaluation metric is based on MAE?

It would be better to conduct more ablative studies on the impact of the amount of noisy labels on the proposed method.

How to overcome the influence brought by the difference of proportions of censored/uncensored cases in different datasets which causes different performances in this paper?"
086	CephalFormer: Incorporating Global Structure Constraint into Visual Features for General Cephalometric Landmark Detection	"It is confused about the landmark embedding. What is the definition? How is it calculated in this paper? Is it a coarse visual feature from the coarse predicted heatmap?

To strong statement about the global structure constraints. It hardly tells the technical novelty compared with it proposed in [p1p2].

Some method details are not clear in the Fine-Scale Coordinate Refinement section. For example, what is C of patches P? R^{g}? and label embedding? It is suggested to define a notation before using it.

[p1] Structure-aware long short-term memory network for 3d cephalometric landmark detection (TMI 2022).
[p2] Structured landmark detection via topology-adapting deep graph
learning (ECCV 2022)."	"*	The technique in the manuscript might not be sound. The two-stage method directly predicts the coordinates from the models, which might not be generalizable when applying the models to images with different (e.g., larger, or smaller) field-of-review, especially for 3D CT. And the second-stage model heavily depends on the predictions from the first-stage model. If the first model misses the landmark, it is very difficult for the second one to locate it."	No obvious weaknesses found for this paper.
087	Cerebral Microbleeds Detection Using a 3D Feature Fused Region Proposal Network with Hard Sample Prototype Learning	"1) The description of RPN is overly brief, and the mechanisms and utility of HSPL is not clear
2) All the image examples shown seem to be SWI scans. How the phase images are used are not illustrated."	"*	A direct comparison of the performance to methods of the state of the art is not possible because different data sets are used for every method including the proposed one, which was trained and evaluated on an in-house data set. The authors did not compare to a state-of-the-art method on their data set.
*	The authors state correctly that ratings of expert neurologists are error-prone and that there are substantial inter-rater differences, yet this potentially important issue is not addressed in the paper. The method was (supposedly) trained using labels provided by a single rater (of a group of experts) per subject.
*	No limitations or future work are discussed or mentioned.
*	more quantitative evaluations wrt. to the choice of a Unet backbone as compared to the original Faster R-CNN architecture are missing"	"It seems that the results are obtained with images of patients with CMBs. What would be the performance in healthy brains?
Generalisation analysis (i.e. testing in a different dataset)  is not shown.
Analysis of the loss contribution is not performed."
088	CFDA: Collaborative Feature Disentanglement and Augmentation for Pulmonary Airway Tree Modeling of COVID-19 CTs	Authors claim about transferring topological structure from healthy to noisy images seems optimistic, especially considering the absence of any registration between noisy and healthy patches and their random flipping/rotation.	The augmentation and training procedure is not described quite clearly.	"It's better to visualize the disentangled features to support the idea.
The feature disentanglement approach is widely used in domain adaptation segmentation, what's the difference compared with them.
For feature disentanglement, domain separation network (DSN) [1] designs shared encoder (similar to Ec in your work) and private encoder (similar to En in your work) to capture sharable features and bias features. A reconstruction network is proposed to ensure the feature completeness, which means that both share and private features are useful (avoiding trivial solutions). However, in your proposed CFDA network, there are no constrains to ensure feature completeness. How to prove the learned features are shared or private.

[1]  Bousmalis K, Trigeorgis G, Silberman N, et al. Domain separation networks[J]. Advances in neural information processing systems, 2016, 29."
089	Characterization of brain activity patterns across states of consciousness based on variational auto-encoders	No quantitative comparison to other methods	"Firstly, defining more equations are necessary, such as the loss function for dVAE, receptive field analysis. 
Secondly, the paper introduces too many contents but fails to explain them clearly. And the writing of the paper is poor. For example, the order for the subfigures of Fig.3 is a mess and Fig. 4 have few explanations in context. 
Thirdly, the sample size and the splits of training and testing dataset are not clear. 5-fold cross validation is used on training dataset. What is the test set?"	"Contributions of the paper are unclear. It seems the dVAE has already been applied to generative embeddings of brain collective dynamics in [21]. The authors should be more specific about their contributions.
Fig. 2: the performance on state 2 is bad compared to others. Please comment.
Experiments: ""..., we select the trained weights associated with fold 3."": given similar performance with fold 2 and fold 3, why the latter is selected?
Effect of the b regularization parameter: please provide more details about the grid (interval, log scale if employed, etc.). It is better to also present the search results.
More description and explanation are needed for Fig. 4(B).
The manuscript does not follow the MICCAI format, e.g., the first lines of paragraphs are not indented in Introduction."
090	CheXRelNet: An Anatomy-Aware Model for Tracking Longitudinal Relationships between Chest X-Rays	There is no ablation study for the classification module.	"Not significant improvement compared to the Global baseline: From Table 2, we can see that the proposed CheXRelNet slightly outperforms the Global baseline (0.68 vs 0.67 for All).
Lack of statistical analysis: Given such a small gain, the authors should give some statistical analysis, e.g., confidence interval, to verify the effectiveness of the proposed method.
Limited interpretability: A critical factor in using local information is that it can provide precise localization, which can largely help clinicians understand the outcome. The authors also claim that the CheXRelNet can output accurate localized comparisons (contribution 1). But no such demonstration is provided. The result is still largely performance-driven.
The way to build the graph: The authors use pathology co-occurrence to build the graph. However, this process is somewhat ""hard-coded,"" i.e., a manually defined threshold is used to determine the connectivity."	I could not see any major weakness in the study.
091	ChrSNet: Chromosome Straightening using Self-attention Guided Networks	The use of self-attention is not well-motivated. In particular, it is not clear that CNNs' inability to model long-range relationships matters in this use-case, and the division of spatial patches into a 1-D sequence (to enable application of attention methods) seems like a backwards step - throwing out highly salient spatial (neigbor) relationships.	"I would suggest to the authors to improve the introduction so that it provides a deep overview of the
study. In fact, I think it is unclear what unique challenges are associated with this task."	"One major issue is that for real-world validation, it is not clear if ground truth is available or not. I am a little confused. Meanwhile I have two minor comments:

Fig 2 is a little bit contradictary to the results in Table 1, or at least may not fully represent the evaluation in Table 1. For example, to my eyes, the Unet result on synthetic data seems to have better S score and L score than ChrSNet, at least for this example.

Why Unet performs so badly on real-world example, but does a very good job on synthetic data?"
092	CIRDataset: A large-scale Dataset for Clinically-Interpretable lung nodule Radiomics and malignancy prediction	"Extensions of the original Voxel2Mesh architecture appear to be minimal (limited novelty on that end)
Results are hard to assess as no baseline numbers or comparisons are provided
Description of the architecture and the training process could be improved"	Evaluation not convincing: some smaller issues (see below in comments), and malignancy prediction results look not competitive with state of the art (e.g. NoduleX)	There is no comparison with SoA best results on the datasets.The benefit of the deep features is not evident from the results, particularly on the external test set.
093	Class Impression for Data-free Incremental Learning	"A.	The incremental-learning method is assessed with tasks of classifying five views of echocardiography. Such an echocardiogram view classification task has a limited number of classification classes (<=5). The efficacy of the class impression to complicated tasks is concerned. 
B.	The clinical effectiveness of the echocardiogram view classification is limited.
C.	The exact value of quantitative assessment is not provided."	"There are parts where it is not clear if the authors are using something ""off the shelf,"" or if it is part of their contribution. For instance, what is the novelty of Class Impression compared to DeepInversion? It seems they have exactly the same optimization objective. Is the Class Impression specifically designed for medical images? What new challenges does the medical image introduce compared to the popular datasets widely used in computer vision, such as CIFAR, ImageNet, and so on? And how does the Class Impression address them?

The organization of the paper is incomplete. Since the paper contains various existing methods, the authors should provide more details in the literature review and background. Besides, the experiments section lacks the necessary discussion to analyze the results.

The evaluations are not solid. The four-task setup containing only five classes may be too simple to cause the catastrophic forgetting problem. The longer sequence of learning tasks with more classes should be used to evaluate the effectiveness of the method. From the current experiments, I still doubt if using pseudo images in task T, which are generated from the model in task T-1, for replaying can avoid catastrophic forgetting.

They also should double-check the bold statements and references. For instance, the authors claim that Smith et al. train a generative model to synthesize images without considering preserving class-specific information. As far as I am concerned, they not only did not train a generative model but also improved the DeepInversion which is used in this manuscript. Why are the images generated by Smith et al.'s method look so bad as shown in Fig. 1. (d)?

Ref: Smith, J., Hsu, Y.C., Balloch, J., Shen, Y., Jin, H., Kira, Z.: Always be dreaming: A new approach for data-free class-incremental learning. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9374-9384 (2021)"	This is a strong paper. There are no main weaknesses detected.
094	Classification-aided High-quality PET Image Synthesis via Bidirectional Contrastive GAN with Shared Information Maximization	"The evaluation needs improvement.
The dataset is too small.
The performance improvement is subtle."	"The dataset in this study is small, with only 16 subjects in total. The authors also have to do 2D image processing to obtain more 2D slice samples, which is a limitation.
The proposed method is likely not statistically different from the SOTA AR-GAN. It would be informative to discuss the pros and cons of the proposed method compared with the SOTA AR-GAN in different aspects, or how the proposed method can outperform AR-GAN in certain scenarios."	"This paper combines several existing technologies to form a new framework, thus the novelty is not significant. This is OK if the performance is extremely expressive, or the analysis is sufficient.
Although the quantitative and qualitative performances of the proposed method described in the paper have shown better than state-of-the-art methods, the performance gains seem marginal, as shown in Table 2 and Figures 2,3. The magnitude of the improvement of the proposed method remains unclear.
Although the overall architecture is novel, each individual components are largely inspired by previous works.
To overcome marginal improvements, the authors should compare more recent works. Furthermore, the experiments on more benchmarks should be presented to verify robustness.
Lack of enough ablation study analyzes the contributions of individual components to the final performance."
095	Clinical-realistic Annotation for Histopathology Images with Probabilistic Semi-supervision: A Worst-case Study	"The method could be compared to more classical semi-supervised approaches 
The tumour/tissue ratio is not so easy to estimate, a study on the influence of a ""bad"" estimation could be given"	"The assumption behind the designed annotation strategy (""easy"" and ""hard"" figure) is not very theoretically reliable.
2.The strategy is only evaluated on Camelyon 16 with small number of samples and limited tumor types, thus the generality is limited.
3.The method is only compared with limited self-supervised and weakly-supervised methods. It would be better to have comparison with latest semi-supervised methods, such as FlexMatch, SimMatch, etc."	"If the authors would address my concerns below, I would further adjust my score.

Major concern: estimation of ratios and multipliers. 
1) How do you obtain the 4-point or 10-point polygons for experiments? Is the proposed method sensitive to the quality of polygons? 
2) Estimating the ratio for small/medium focal tumors seems to be straightforward by computing poly_area/otsu_foreground_ared. But how do you estimate the multiplier for polygon area v.s the whole tumor area? An accurate estimation of the ratio, in my opinion, is the key to the success of the proposed method. Using the ground truth label to directly obtain this information can be seen as label leakage, but I'm okay with it if the leakage is not much. Authors should provide the information on the distribution tumor_area v.s. tissue_area, or the distribution of the number of small/medium and large tumors. This helps to inspect the degree of leakage.

Missing details about implementations. 
1) MC Dropout. Where do you put the dropout layer, and how many times of MC dropout are performed? How do you estimate the uncertainty U? Is it the standard deviation of multiple passes? How much time is increased for Probabilistic + feedback (use MC Dropout for feedback)? There are way more passes for MC dropout. And the gain seems to be marginal, (0.02 from table 2)
 2) Training details in each step. The paper has not presented the training details, e.g. how many epochs SimCLR / MixMatch are pre-trained and how many epochs those models are tuned, either using patch Camelyon or other annotation.
 3) Consider specifying what is stored in the sample queue. Are they images? Besides, is Y+ a vector full of ones (eq 3)?
4) Will the code be made available?

Minor issues:

The tumor region ratio indicates tumor_area/slide_area or tumor_area/tissue area. These two concepts both exist in the text, e.g., ""while large lesions can occupy more than 50% of the slide"" and ""the tumor region occupies 51%, 3%, and 0.01% of the foreground tissue"". Consider clarifying it more.

Comparison between MixMatch-variant [8]. In table 2,  the last line, is it equivalent to MixMatch+Polygon+Probabilistic+feedback? Is there any difference in stage-1?"
096	CLTS-GAN: Color-Lighting-Texture-Specular Reflection Augmentation for Colonoscopy	"training and implementation specifics are limited. what preconditioning was done?  What frameworks were used to impliment the methods
the algorithm training explanation could be clearer, particularly figure 1
the main advance is that texture and lighting are effectively transfered onto virtual colonoscopy frames, this is small advance on the styleGAN.  The novelty above that could be better explained"	"Overall the mathematical description supported with Fig. 1 intuitively is correct but I think there are few mistakes that make the reading very confusing. I will list some of this in the detail comments below.
It is unclear from the paper how the user will get satisfactory results when sampling randomly from a uniform distribution
While differences can clearly be appreciated for colour and lighting, attributes such as texture and specular reflexion are not noticeable in the examples shown in the paper. I think the paper lacks a quantitative approach to measure this."	"As the augmentations are performed on a frame-level, temporal consistency of the frames cannot be guaranteed. Therefore, the augmentation technique is only valid for frame-based downstream tasks. Especially for training applications, the temporal coherence of the generated video is crucial for realism. This is only rudimentally discussed in the paper.
The proposed framework has a lot of potential; however, the authors only evaluate one specific use case of the method. A more extensive evaluation would strengthen the work. Furthermore, a discussion of the different options how to use the framework for augmentation (last comment in the detailed feedback section) and also for training applications and colonoscopy simulation (using the VC data as reference) would clearly improve the presentation of the work.
Even though the method is very interesting and relevant, the presentation of the proposed method is lacking clarity (also because there are many different options how to use the framework and the presented approach is very complex). Instead of showing a lot of different applications of the framework without context, it might have been better for the limited space of a MICCAI submission to focus on one application and explain it in more detail."
097	Collaborative Quantization Embeddings for Intra-Subject Prostate MR Image Registration	The major problem of this paper is the lack of important experiments, which might undermine the main motivation.	I don't have major weaknesses to list.	"Format violations. The authors clearly modify the template in several places to gain extra writing space: 
1) In section 2, the vertical spacings below section title ""Method"" and the following subsections including ""2.2 Model overview"" etc are clearly modified and reduced, which are strictly prohibited.
2) Text is wrapped around Fig.1, Fig. 3, and Fig. 4., which are strictly prohibited as well.
3) The supplementary file also exceeds the limit by 3 pages.

The effectiveness of the proposed framework (collaborative + hierarchical) is not well-supported by the experiments. 
1) From Table 1, the combination of Dv, Dh, Dc yields lower dice conformance compared with Dv plus Dc, which seems unable to support the effectiveness of the proposed hierarchical quantization. 
2) Interestingly, the vanilla quantization (Dv) alone also yields better Dice and DC scores than the proposed combination (Dv, Dh, Dc), which shows that the combination of Dh and Dc's effectiveness under further exploration. 
3) The proposed method also does not show superior performance compared to Dv plus Dh in terms of DSC and CD in Table 1, leaving the effectiveness of pre-trained Dc questionable.

Potential misselection of baseline models. From Table 2, the NiftyReg method yields even worse Dice conformance compared to without registration in Table 1. This shows that NiftyReg doesn't work on the dataset or the authors didn't implement it correctly. This also disqualifies it from the ""state-of-the-art"" methods, making the argument of superiority less supported."
098	Combining mixed-format labels for AI-based pathology detection pipeline in a large-scale knee MRI study	"Too heuristic approach, and it is unclear how clinical relevance of the developed method can overcome this
The authors re-invent object detection, and should have tried single-shot object detection architectures as baselines
Statistical correctness of the claims has to be verified before this work is published."	"The architectural novelty of the proposed solution is rather limited. The study also does not review the state-of-the-art methodological solutions for the task. Subsequently, it is not clear, for example, why an end-to-end is not considered.
The paper is missing a discussion on the comparison of the proposed method against the prior studies, at least, in scope of ACL injuries - Namiri et al 2020 (https://doi.org/10.1148/ryai.2020190207), Astuto et al 2021 (https://doi.org/10.1148/ryai.2021200165)."	"The strategy and rationale in using peak finding algorithm in Stage I is unclear and needs to be explained better in the main manuscript. 
Also the methodology of obtaining categorical or global level labels is unclear."
099	Combining multiple atlases to estimate data-driven mappings between functional connectomes using optimal transport	"It is a bit weird that the predicted connectome has a better IQ prediction performance than the original one. Isn't the algorithm was designed to predict a connectome as close as the original one? That is, if the original one has a bad IQ prediction performance, the predicted one is expected to have a bad one, too. If the predicted one has a better performance, as shown in Fig. 4, this connectome is a 'hybrid' connectome, rather than a predicted one.
The authors may provide some in-depth discussion on the performance."	The only drawback is that it is somewhat unclear how practically useful this method will be. It still requires one to process some subset of the target dataset with the target atlas, and at that point, once you've set up all the scripts, doing it on the full dataset might just be a matter of computing power. But still, the method is elegant, so this is not a major weakness.	"1) The paper lacks insight into how many number of source atlases are optimal and what is the effect of increasing or decreasing these numbers
2) More details on choice of regions could help readers. What are ns and nt regions and how these are included?"
100	Computer-aided Tuberculosis Diagnosis with Attribute Reasoning Assistance	"5.1) The components in the feature representation module, such as group convolution[1] and multi-head attention[2], are directly transferred from general image recognition framework, which may be lack of novelty to some extent. 
[1] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[J]. Advances in neural information processing systems, 2012, 25.
[2] Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[J]. arXiv preprint arXiv:2010.11929, 2020.
(2) The insufficient experiments can not provide evidence to show the effectiveness of some operation in the module, such as relative position encoding."	"1.Fig.1 and Fig.2 can not explain the usage of features in each step.

The branches of detection and classification are commonly used in precious work and there are not novelty parts in the network.
The method is only evaluated on one dataset, that can not certify the effectiveness."	While the motivation is compelling, the paper seem to miss important details about the data collection protocol. For instance, what ages, genders, and other demographical information does the dataset represent? How was the dataset annotated? The paper mentions the TBX11K dataset, but it is not clear how the two datasets are combined.
101	Conditional Generative Data Augmentation for Clinical Audio Datasets	"No comparison with other methods (the authors claim there are none?)
Clinical application is not very clear. and so it is not clear what motivates the work."	"Limited method novelty with just applying the GAN method to do the augmentation
The experiments are not that sufficient, which only can be stated as some preliminary results for this new topic."	"The dataset analysed is very small, therefore the generalizability of the method is limited.
The authors do not well describe and justify the used of the the dataset being analyzed.  What is the purpose of analyzing the THA audio dataset?
The description of the results needs to be improved.  Only mean accuracy is presented, which is very limiting in terms of interpretation."
102	Conditional VAEs for confound removal and normative modelling of neurodegenerative diseases	"It is unclear what the VAE approach add beyond a standard outliers detection of the underlying data, the only comparison provided in figure 3 seems to be with a mean square error of the original data.
It is also unclear how the hyperparameters were chosen and influenced the results."	"There are major novelty concerns:

The confounder removal method (cVAE) is largely outdated. There exist many other options (e.g., domain adversarial networks, conditional normalizing flows, etc.)
The proposed deviation metrics are identical to the normative probability map (NPM) by Marquand et al. [16]. In fact, [16] suggests extreme value statistics to perform statistical tests on the NPM (which are essentially z-scores) for subject-wise statistics."	"why github code is in abstract?
Fig. 4 is kind of needs some work.
Table 1, red colors needed to be black. You can use different symbols to emphasize the model."
103	Consistency-based Semi-supervised Evidential Active Learning for Diagnostic Radiograph Classification	"The discussion of the results is not enough to help to understand the results. In particular, Fig. 2 is relatively easy to understand without explanation, but Fig. 3 definitely needs additional discussion.

The presentation and organization of this submission needs to be improved. It is recommended to move the figure close to the text where  the figure is referred or discussed (e.g., Fig. 1 is at page 5, but the corresponding text is at page 3).

The limitations of the proposed approach and the directions for future research are not discussed in this submission."	"Parts of the paper could be re-written for better clarity. The paper uses too many abbreviations which makes it hard to read.
Limited experiments: The method is only tested on a single X-ray dataset with a single model (Densenet-121 backbone). This makes it difficult to tell if the results generalize. Further, it is unclear if the same method (eNoT+AU) is superior on every dataset."	"While known semi-supervised algorithms are used to implement CDSEAL, they are not benchmarked themselves. For example, ePSU is benchmarked on the dataset. However, PSU (pseudo-label) itself is not benchmarked. Therefore, the question arises that the observed improvement over the supervised learning comes from the PSU or the CDSEAL loss.
The authors also used AUCROC for the evaluation of the methods. While AUCROC is a well-known metric, it is flawed and skewed for highly imbalanced datasets. It is suggested to use the area under the precision-recall curve instead."
104	Consistency-preserving Visual Question Answering in Medical Imaging	"*	Gathering special dataset with expert annotations either for marking presence or segmentation of hard-exudates or for marking fovea to compute macula region is expensive. 
*	To support clinical deployment, the authors should report results on a third dataset that have only DME labels.  The authors can report results in two scenarios,
o	Consider this dataset as just a deployment dataset for inference time only. 
o	Consider this dataset for fine-tuning the pre-trained VQA. 
Report how VQA helped in building trust by identifying DMW scale and specifying regions with hard exudates."	"Unclear what the difference between the proposed approach and the one in Selvaraju, Ramprasaath R., et al. ""Squinting at vqa models: Introspecting vqa models with sub-questions."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020."	"No technical novelty.
only one data is used. Generalizibility of method is unproven.
Recent VQA methods in medical domain (MedVQA) are not discussed.
Recent SOTA methods are not used to compare the performance of the proposed method.
Writing is a bit poor; there are a lot of typos and grammar issues."
105	Context-Aware Transformers For Spinal Cancer Detection and Radiological Grading	"The structure of attention mechanism (in Fig. 1b) was not described in the paper, therefore it is unknown how the attention is achieved.

More information of the baseline models has to be provided to evaluate the fairness of comparison. For example, do they operate on multiple sequences? In what aspects that they are good choice of baseline? In Table 3, instead of SpineNet (T2) and Baseline: SCT Encoder (T2), it would be fairer to use SpineNet (T1, T2) and Baseline: SCT Encoder (T1, T2) for comparison in Table 3."	The authors introduce a transformers-based algorithm, which is of raising interest in the community. However, there is few discussion of the role the transformers are playing in the performance increase, that might lead to uniformed conclusions.	"The proposed method is only compared with one existing method published in 2017. There should be more comparisons with existing methods, as well as more ablation studies to assess each component in the method. Besides, in Table 3, SCT only improved SpineNet by 0.7% in average accuracy with T2 images.
The content is somehow too much for this 8-page paper, so there are little space for more comparison and ablation studies. I think the introduction can be trimmed a bit."
106	Context-aware Voxel-wise Contrastive Learning for Label Efficient Multi-organ Segmentation	"- The methodology contribution seems to be relative minor. The main contribution is adding the contrastive learning loss to unlabeled voxels. However, the organization and description of this paper is not clear. It is hard to see how authors handle the partial label dataset using CL and why it works. There are also too many math symbols in the main text affecting the readability.
- There are several major concerns for the experimental evaluation. (1) The experimental setup description is not clear.  It seems that the baseline nnUnet's results in D1 to D4 is just a directly inference. If that is the case, then, reporting the retrained results of nnUnet on D1 to D4 should also be provided to see the upper limit of training directly on the target single organ dataset. (2) The improvement over the previous work is minor, e.g., there is a 1.7% DIce improvement over [12], this kind of improvement might be brought by the backone of nnUnet, instead of the developed CL method.  In other words, if [12] is also eqquipped with nnUnet, what are the results? (3) The ablation study on the network architecture is weird. When using original UNet as baseline to segment D0, it achieves better performance than nnUNet in both Dice and HD95. I think this cannot be true. There is also no detailed description on the UNet training details."	"1) To leverage the unlabeled information, the author proposed a new training approach: extract two overlapping patches and use contrastive loss to minimize the difference between features of the corresponding area. The motivation is that most of previously methods adopted patch/subvolume-based strategy. However, for organ segmentation in CT volume, it is always not necessary to adopt the subvolume-wise patches for training.
2) The work is more like an incremental work. Contrastive learning for enhancing the feature representation is not new in medical imaging area."	"The effectiveness of ""context-aware"", i.e., pulling feature of a voxel from different patches closer, needs to be justified in the ablation studies. What if designing the positive pairs by using same voxels from rotation-augmented patches instead of different cropping areas?
The author mentioned a recent work DoDNet (Ref. [13]), which is also designed for partially labeled deep learning segmentation. However, there is lack of comparison to this method."
107	ConTrans: Improving Transformer with Convolutional Attention for Medical Image Segmentation	"The authors mention the ""inductive biases"" of CNNs several times. However, they did not elaborate on what these biases are. According to Section 2.2, ""they may not perform well on medical datasets due to the lack of inductive biases inherent to CNNs."" and then propose ""Depthwise Attention Block (DAB)"". This DAB module provides depthwise convolution, channel attention, and spatial attention, which are not provided by vanilla-CNNs. The DAB is an enhancement of CNNs. It may be necessary to revise the expression here to show clearly what vanilla-CNNs bring to Transformer and what DAB brings to CNNs.
The evaluation metrics are insufficient to validate the performance. Specifically, the authors should include experiments of the critical metrics, i.e., E-measure, Fbw, and S-measure. References to these metrics can be found in the following papers:

[1] Liu et al., Visual Saliency Transformer. ICCV 2021 (E-measure)
[2] Wei et al., F3Net: Fusion, Feedback and Focus for Salient Object Detection. (E-measure)
[3] Su et al., Selectivity or Invariance Boundary-aware Salient Object Detection, ICCV 2019 (Fbw)
[4] Zhao et al., Pyramid Feature Attention Network for Saliency Detection, CVPR 2019 (Fbw)
[5] Fu et al., Deepside A General Deep Framework for Salient Object Detection, Neurocomputing 2019 (E-measure, Fbw, S-measure)"	"- The authors claim that ""The proposed Depthwise Attention Block (DAB)"". However, this module is commonly-used in the computer vision, which could not become the one of three major contributions of this manuscript.
- Thought promising results have been achieved, the whole comparison maybe not fair level. The authors better provide the efficiency comparison in the experimental section, for example, the inference speed and model parameters.
- The ablation study is insufficient. How about the performance of basis Swin Transformer backbone? Furthermore, I am interesting whether such improvement comes from the Swin Transformer."	"The proposed DAB module is basically the same with CBAM ([28] as cited in the paper]), with the only change of depth-wise conv; however, the reason why depth-wise conv would reduce the local redundancy compared to conv is not justified;
While the proposed method achieves SOTA on various dataset in terms of segmentation metrics, the model size and FLOPs are not shown. It is hard to know whether the improvement is due to larger model (as two parallel encoders are used here) or the proposed design;
The evaluation datasets are mostly 2D segmentation, it would be more interesting to know the performance on, e.g. 3D dataset such as MRI and CT;
As Swin Transformer has already introduced locality, the motivation of using two parallel encoders, one as CNN and another as Swin Transformer, is not well-justified;
The design motivation of SRCA module is not justified, e.g. why the transformer feature is used as query and CNN feature is used as key and value?"
108	ContraReg: Contrastive Learning of Multi-modality Unsupervised Deformable Image Registration	"Discussion/analysis regarding some key parts of the algorithm seems to be missing. Namely, the impact of the autoencoder performance on the alignment result, and the chosen parameterizations for the STN/registration network.
The dataset used for the evaluation of this work is disappointing. Outside of the rare cases of lost data, there are very limited situations where one needs to actually perform inter-subject registration between T1w and T2w MRI, since the two contrasts are almost always acquired together. The results would be more convincing if the method was tested on a real-world application."	"To the reviewer it is not clear if the proposed method is better than SynthMorph. Indeed, SynthMorph achieves a better Folds and sdLog (B1 Table1), and from the curve Figure 3 (row3), it seems that for the same level of % Folding Voxels, the dice score of the proposed method is much lower than SynthMorph.
The author did not discuss the impact of using a supplementary network in term of training time and memory used. If we have a fixed GPU memory, is it better to add two autoencoders for the constrative loss or increase the number of parameters of the registration network ?
The clarity of the paper should be improved, and specially the clarity of the methodology ."	"The paper contains a lot of information and the presentation is quite ""dense"" due to the page limitation.   Therefore, it is more accessible to a knowledgeable audience."
109	Contrast-free Liver Tumor Detection using Ternary Knowledge Transferred Teacher-student Deep Reinforcement Learning	"In testing phase, well trained student network drop the ternary knowledge from the teacher network. Why the student get rid of the knowledge from the teacher network? What method is used? Without the teacher network, the student will not have the previous knowledge plus with the new knowledge. Is this process necessary?
Experimental ablation is used in research on animals. Is this experiment necessary for the human liver datasets?"	More jobs should be done to better verify the work.	"It may not easy to reproduce the results, because some details are missing:

how to select hyper-parameter in the reward function, grid searching?
for the proposed P-strategy, since P-strategy only introduces one
item (4 items in total) from the student network's features that is most similar to the teacher at each step,  how to decide when shoud perform the 4 exchangement to make the training process stable?"
110	Contrastive Functional Connectivity Graph Learning for Population-based fMRI Classification	"There are several issues in the paper.

Notations are difficult to follow and I found several errors with mathematical formulations. 
For example, First paragraph, section 2.1:
Errors start with the notions
P is defined as number of number of patients. What is cal_P refers to? 
G_i^j is defined in line 1.
Line 7, what is G_i referred to? Is P missing? 
And so on ....

The effects of random partitioning time-series into two sets need to be studied.
Though authors generalize their claim on small datasets, methods are tested only one dataset.
The details of dynamic graph classification (DGC) is sketchy and from the table 1, I doubt it is ineffective.
References to some existing work on unsupervised graph embedding methods for connectome analysis is missing. For example:

Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning
Ming JinYizhen Zheng[...]Shirui Pan
(2021),10.24963/ijcai.2021/204

Though comparison results are provided, how the performance matrices were derived - either from your own fair simulations or from literature has to be mentioned."	The constrastive learning framework proposed is mainly a straightforward application.	The authors only perform experiment on one dataset, they could extend their experiments on more datasets.
111	Contrastive learning for echocardiographic view integration	The work seems to have failed to cite other LV volume estimation works, and they are a plenty. In turn, they have not compared their method with such methods. Yet, to be fair, they have compared their method with the ablated versions of their own model which highlgihts the impact of each component.	"The experimental results are not solid. Results are performed on only one dataset without comparison to the existing methods. I'm not familiar with the targeted task but I wonder if there's any existing work on this task for the purpose of comparison.
The technical implementation of the contrastive loss is trivial. Direct maximizing and minimizing the eculidian distance is not optimal and barely used. In this scenario, I think triplet loss or regular contrastive loss (in self-supervised learning) are better alternatives to try and compare with simple distance-based losses."	"The authors claim to be the first to apply volume contrastive networks to solve this problem but the evaluation lacks other methods in the literature that have been used to combine the views. The only comparison provided is the ablation study, which shows very convincing results.
It would also be helpful to show some clinical correlation rather than just the MSE. For example, how does this improvement in the volume estimation help diagnosis, is this improvement relevant to the final task for which the volume estimation is used?"
112	Contrastive Masked Transformers for Forecasting Renal Transplant Function	It's hard to reproduce.	"I see the authors make challenging task of serum creatinine prediction 2 years posttransplantation, however, I see only four different follow-up exams ends with (M12) after one year not two years as they discussed.
The authors need to discuss what is the limitations of the proposed framework and when the proposed system can fail.

I see the authors used ResNet18 to extract a latent representation
from the MRI volumes (size =, 512x512x[64-88] voxels), however, the resenet18 uses an input image of size 224 x 224; this mean a lot of information in the MRI images can be dicaseded during the training process. and this can effect on the training result.

testing sample size (20) is very limited in this study also.
I see the authors used the only 10-fold cross validations, are you tried different cross validations ? and what was the output ?"	"*	Results are incomplete
*	Missing Discussion"
113	Contrastive Re-localization and History Distillation in Federated CMR Segmentation	"The descriptions on CRL and MD lack clarity.
Results could be further refined.
The writing could be improved which could help the reproducibility as well."	"Limited statistical evaluation: FedCRLD seems to perform better than the prior methods, data sharing and within individual clients, but improvements in certain datasets seem quite marginal. It would be quite interesting to see the statistical significance of the improvement in performance. So authors should perform tests wherever necessary and report the significance to better appreciate the effect of the method (same goes for the ablation study). Also would be good to see the standard deviation in addition to the average.
Computational complexity: the method seems quite complex and attention networks are generally computationally heavy - what is the training time? Was any measure taken specifically to reduce computational load? Comparison of such settings could be something to try for the journal article or future work.
Repetition of aims and lack of details: The aims have been explained in the intro section and has been repeated in the method section in great amount of details, while that space could have been used for explaining cross-attention transformer better (eg. details such as number of layers and filter sizes are missing)."	The quality of writing and the way of presenting the methods are unsatisfactory (see 8 for details). The experiment descriptions are flawed.
114	Contrastive Transformer-based Multiple Instance Learning for Weakly Supervised Polyp Frame Detection	See comments.	"A clarification on Cls token is required; ""The Cls token is applied for a video classifier to predict if a video contains anomalies."" Please elaborate on this as all readers are not familiar with transforms.
it seems the paper is missing a comparison against another SOTA which is using similar snippet mining approach ""CoLA: Weakly-Supervised Temporal Action Localization with Snippet Contrastive Learning, Zhang et al, 2021""
More clarification about the dataset is required, as one of the claims is that this work has a better performance when small or flat lesions appear. Therefore, a table presenting the percentage of these sorts of lesions and the performance of the framework on them is required."	"(1) The authors did not cite previous work in the Contrastive Snippet Mining section.  Please also fix the words in the contribution section to reflect this.
Previous work is here:
Zhang, Can, et al. ""Cola: Weakly-supervised temporal action localization with snippet contrastive learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021.
It is called Snippet Contrast (SniCo) Loss in the above-mentioned paper. The Polyp Frame Detection is a special use case in the Temporal Action Localization problem category.
(2) The authors combined several public colonoscopy datasets and build a new large-scale diverse colonoscopy video dataset to benchmark their method against other methods. However, the authors only provided the training details for their proposed methods. As this is a new dataset, training parameters might be different from the original papers. Please also provide training details for all other methods that are used in the benchmark. It will be also more clear for the readers if the authors provide method differences (For example: Did MIL is used? What Loss is used in training?) in Table 1."
115	Coronary R-CNN: Vessel-wise Method for Coronary Artery Lesion Detection and Analysis in Coronary CT Angiography	"The paper presents a good application of the Faster R-CNN method for abnormality detection but in doing so lacks to present any new innovation/method for coronary CTA.
The paper also lacks details in centerline extraction and vessel segmentation methods
The paper does not provide any details on how straight line MPR views were extracted from cardiac CTA.
The authors should have omitted that the data was collected from 6 centers in China in order to not break anonymity."	The first stage of the method seems to be a simple application of Faster RCNN to the lesion detection problem without modifications for medical images or lesion detection target. The second stage of the multitasking network is an intuitive solution to accomplish lesion classification and stenosis regression at the same time. Therefore the method may not be innovative enough. Please see detailed comments below.	This paper is generally biased towards innovation in application, and the method and ideas are borrowed from Faster R-CNN.
116	CorticalFlow++: Boosting Cortical Surface Reconstruction Accuracy, Regularity, and Interoperability	"(1) Motivation of using fourth-order RK approximation is less discussed. The accuracy is theoretically expected to be better than Euler method, but isn't RK4 an iterative method, too? Is backward Euler or RK2 a reasonable option (esp backward Euler)? Comparing with these popular methods is welcomed.
(2) Concerns on the generalization. Is the proposed method sensitive to the inputs? For example,  if model is trained with high-resolution MRI scans, what's its performance on lower-resolution MRI scans (eg, train with ADNI2, test on ADNI1)? Such aspects are expected to be discussed.
(3) What about the training efficiency? Testing efficiency is well demonstrated, but the training efficiency is not given. Please provide more information about the training process, such as convergence information, data preprocessing, epochs used to train, etc. As far as I know, RK4 converges slower compared with forward Euler, you have to set a larger time step for RK4, so I'm interested in its actual training efficiency in this task.
(4) Not see in the submission if the code will be released to public."	"These improvements are incremental.
Although the weakness of convex-hull is explained, however, using the proposed smooth templates is dependent on the brain size. And the improvements contributed to this template is not discussed or validated."	See below.
117	CRISP - Reliable Uncertainty Estimation for Medical Image Segmentation	I would have liked to see more comparisons to generative approaches like using GANs or probabilstic U-Nets posterior distribution modeling to ground truth annotations. Given the complex latent space modeling of the paper more comparisons to simpler approaches for the latent space / similarity heuristics could be experimented.	"The authors did not clarify what kind of uncertainty this is. Given that they compared with Monte Carlo dropout, I assume it is epistemic. What guarantees that a perturbation in model weight will land on a similar latent vector in the hypersphere as found from the nearest neighborhood? It also raises questions about how compact is the latent space.
There is a major drawback to this formulation. The network produces perfect segmentation, which is identical to the ground truth. However, the nearest neighborhood ensemble will still produce uncertainty. Ideally, there should not be any uncertainty for a perfect prediction. How will one interpret the uncertainty in this scenario?
How is the method different from simply computing dice between y* and Y, then finding the k-nearest neighbor and computing the uncertainty?
Since shape is a major factor for medical image segmentation, how would the latent space disentangle shape and location information? How else has the location shift been taken care of in this study?
What are CRISP-MC and CRISP-LCE? These are not explained clearly."	"I think the only major weakness is the lack of summary of the relevant literature and comparison with them. There are methods that are developed to estimate uncertainty in image segmentation [1, 2, 3]. These methods are directly comparable with the proposed method and I believe the method should be compared to at lease 1-2 of them.

[1] Kohl et al. ""A Probabilistic U-Net for Segmentation of Ambiguous Images"", Neurips 2018.
[2] Baumgartner et al. ""PHiSeg: Capturing Uncertainty in Medical Image Segmentation"", MICCAI 2019.
[3] Monteiro et al. ""Stochastic Segmentation Networks: Modelling Spatially Correlated Aleatoric Uncertainty"", Neurips 2020.
Although not directly related, there is another work which was proposed to estimate the segmentation accuracy [4]. I think mentioning this work would be interesting because working principles are very similar. While the proposed method selects M most similar images in the latent space, [4] exploits similarity in the image space after registration. I think the proposed method is more practical and sophisticated, it would be interesting to discuss similarities/differences with [4] in the paper.
[4] Valindra et al. ""Reverse Classification Accuracy: Predicting Segmentation Performance in the Absence of Ground Truth"", TMI 2017."
118	CS2: A Controllable and Simultaneous Synthesizer of Images and Annotations with Minimal Human Intervention	I don't see any major weakness of this paper.	While I found the presented idea quite innovative, it would be very valuable to expand upon the quantitative experiments and discussion. Please also include a thorough description of limitations and failure cases of the approach.	"Weak systematic analysis of the proposed method:
It is not clear why the proposed method should be of attention to the readers in the field. Does the method really generalize? If so, why? Does it only apply to certain lung nodules in CT images that have distinctive HU values?

Experiments are not enough to justify the proposed method:
Lung segmentation is regarded as a relatively easy task that could be even done without any supervised ML method, not to mention the necessity of synthetic images.
GGO segmentation too, is limited in demonstrating the novelty of the proposed method.
The experiments do not fully show the strength of the method - there is only box plots comparing the proposed method and M2I for lung and GGO segmentation."
119	Curvature-enhanced Implicit Function Network for High-quality Tooth Model Generation from CBCT Images	"The method description is a little unclear because the descriptions of the training phase and that of how the method is used for unseen images have not been separated.
Many details on the network architecture are missing, such as the number of filters in each block, the filter size, the number of training epochs, the learning rate, the optimizer used, etc.
The evaluation uses a fixed test set of 20 images, which is not as convincing as a 5-fold cross-validation or multiple random splits."	"Technical details of the method especially the network parameters (filter sizes, optimization parameters, etc.) are missing. The figures are very useful for presenting the method however their details are not sufficiently provided in the text. Also, more details on the experiment for the baseline models should be added.
Another major weakness is dataset size. It is very hard to gather multi-model data, however, instead of using 20train, 10 validation, and 20 test data, cross-validation would be much fairer. Also, if are there any missing teeth, dental implants, braces, etc. they should be given.
There is also reproducibility concerns about the work given in the below sections."	The innovation is weakness
120	DA-Net: Dual Branch Transformer and Adaptive Strip Upsampling for Retinal Vessels Segmentation	"1) The use of the proposed upsampling block (ASUB) is not justified enough. It is not clear how well the stip upsampling block can capture the regional context in more complex vascular features, like bifurcations, where the vessel deviates from a line like shape. Another case where the use of the ASUB is not justified enough is in the smallest vessels case. The smallest vessels deviate significantly, in terms of shape, from a straight line like structures approximating curvilinear shapes. The pathology also can change their appearance making them more tortuous. 
2) The difference in the segmentation performance between the proposed method and the existing work is relatively close (see CHASE-DB1 dabase, Table 1). The standard deviation is not provided. Also, the authors do not provide statistical significance analysis of their results. 
3) The authors do not discuss the limitations of the methods on more complex vascular features, like in junctions, bifurcations, in cases of pathology, and they do not give examples of their segmentation of pathological images. For example, bifurcations consist of one parent and two daughter vessels and the strip like upsampling can not capture the organization of this structure. The future research steps in the discussion section are also missing."	"No statistical analysis is given for ablation studies to show if the contributions of the proposed modules on method performance is significant, which seems very small.
Although using a shared encoder for image and patch wise segmentation seems to helps network parameters to be kept small, I have some concerns about that it can reduce the information can be extracted from a fundus image (which is down-sized by a factor of N=4), which would loss small vessels due to downsampling, in contrast to previous work using full size of fundus images. So, this means the network does not use image-wise approach purely.  There is no ablation experiment provided to show if using dual branch improves performance instead of using only patch-level branch or image-level branch."	Though, the results seem to be excellent, the experment results are not sufficient. Only two datasets are included.
121	D'ARTAGNAN: Counterfactual Video Generation	"no comparison with similar approach applied on general computer vision tasks, e.g. https://github.com/autonomousvision/counterfactual_generative_networks
not sure if we need all the theorem in Section 2. The authors might have been able to explain in a simpler language."	"Novel technique is mentioned with SSIM score but no comparative analysis with any baseline is presented.
Reference to the EchoNet Github repository is missing. 
Too many and unnecessary abbreviations such as Ultrasound to US.
Related work either discuss simulators which are physical simulators and compute intensive or implementations of deep twin networks. No reference for cases where counterfactual queries are worked upon may be in domains other than medical imaging. 
Too much mathematical detail, for Definitions which are difficult to comprehend without much context. Overall paper is difficult to understand.
More explanation for the Abduction-Action-Prediction solution (discussed in Preliminaries) may be useful. 
Variable names should be described in Fig: 1 for a quick overview. Fig. 1 can be drawn wrt. the application on hand instead of a generic one.
Minimal or zero representation of models using figuratively, leads to a very lengthy and confusing description.
Poor sectioning and sub sectioning with casual paper writing. Also English needs to be significantly improved."	"The detailed descriptions of the model are not clear.
The authors do not sufficiently describe how it is related to existing work, and no quantitative comparison is given."
122	Data-Driven Deep Supervision for Skin Lesion Classification	"The paper reads like, authors utilized methods from the literature, on a novel application.
Presentation lacks of clarity at a few places.
The use of CAM from the last conv layer in order to determine the L_{target} in LERF may not be robust.
The isotropic nature of the LERF is not well justified.
Conclusions section is very short and lacks of discussions."	"While the study shows improvements, it is not clear if the improvements are indeed significant (no confidence intervals or statistical tests for example).
While the average performance seems to increase, it is not clear if it also introduces more variability in performance- for example, does the model perform significantly worse on cases where the object size is too small or too large (vs competing models)? There is no analysis presented which stratifies the performance by size of the object.
Due to 2 above, it is not clear if the method is generalizable to different datasets, particularly if the average size of the object varies across different datasets.
Additionally, because of the dependence of object size, it is not clear how the method will work with multiple classes with different sizes. Indeed, for ISIC 2018, the best result reported in the paper with deep supervision (0.701) falls far short of the best result (0.845)."	There lacks the comparisons to other activation mapping method such as Grad-CAM.
123	Data-driven Multi-Modal Partial Medical Image Preregistration by Template Space Patch Mapping	"Limited methodological novelty: combines several blocks from prior work
Unclear training routine: is the entire dataset used in training?
Time is not reported
Requires labels"	"*	There are some model training hyperparameters that could use additional evaluation and/or analysis, e.g. patch size."	"It would be interesting to see also execution time comparisons for the diverse registration methods
While the experiments section in general is quite good, there is not enough technical data about the dataset to recreate a similar one
There is no time performance indication.
The Conclusions section is too short and it doesn't really add much. It would be interesting to merge the Discussion subsection from the Experiments into the Conclusions section"
124	DDPNet: A novel dual-domain parallel network for low-dose CT reconstruction	"The logic and readability of the article are relatively poor.
a)	The introduction to loss functions is ambiguous. The respective effects of the L1, SSIM, and adversarial losses should be properly explained.
b)	On page 6, the sentence ""With the rich experience granted from the ..."" is lengthy and grammatically incorrect, making it difficult for the reader to understand.
c)	What does the ""coherent constraint"" refer to? How is it established in dual-domain cross-communication?
In Fig. 2, the arrowhead should point at the Unified Fusion Block, but not the image ysin~img.
There are many grammar and spelling mistakes.
a)	In the Abstract, ""The extensive experiment"" should be ""The extensive experiments"". In the same sentence, the period ""."" after ""13.54 HU"" should be dropped.
b)	In the Abstract, The sentence ""All of these findings reveal our method a great clinical potential in CT imaging"" can be replaced by ""All of these findings suggest that our method has great clinical potential in CT imaging"".
c)	The word ""readable"" is repeated in the sentence ""how to effectively make LDCT reconstruction in the more readable readable pattern..."" on page 2.
d)	""to comprehensively integrates"" should be ""to comprehensively integrate"" on page 4.
e)	""more realistic and details"" should be ""more realistic and detailed"" on page 4.
f)	""Fig. 3: IIF connect"" should be ""Fig. 3: IIF connects"" on page 5.
g)	What does the sentence ""IIF extracts the intrinsic noise distribution, and guide it image domain ..."" mean on page 5? Does it mean ""IIF extracts the intrinsic noise distribution and introduces it into the image-domain stream ...""?
h)	""the sinogram domain extracted information"" should be ""the extracted information from the sinogram domain"" on page 5.
Lack of details, especially in section 3.1"	"This work is quite similar to the following work that explored the interactive dual-domain parallel network. Although slightly different tasks, the motivation is the same. Without citing this related work and discussing the differences, this paper seems to lack of novelty. 
Wang, Tao, et al. ""IDOL-Net: An interactive dual-domain parallel network for CT metal artifact reduction."" arXiv preprint arXiv:2104.01405 (2021).
Fig. 5 showed that the proposed method has HU shift compared to NDCT images; the obtained images are darker. Such HU shift certainly limit its potential in clinical practice.
The authors use the downsampled image and sinogram in the dual-stream framework. But how to upsample the image is not mentioned in this article. The parameters of the FBP algorithm need to be scaled for the downsampled sinogram. If upsampling after reconstruction will bring errors. How can the authors eliminate this error?
A triple-cross attention block is designed in this article to combine the features extracted by the two-stream structure and LDCT images. However, the advantages of the block are not clearly stated. If it is feasible to directly use a 1*1 convolution kernel or a fully connected layer for feature fusion? How much will the performance drop?
In subsection 2.3, four operators are used to calculate the gradient image. Usually, only the first two operators are needed to calculate the gradient image. What are the functions of the last two operators, and how much does it improve the performance of the final model?
How to set the weight parameter ""alpha"" in the loss function is not mentioned in this article. How to combine the adversarial loss of the gradient image and the original image is also not mentioned.
The capitalization of symbols is not uniform. In ""Overall performance"" of subsection 3.2, ""db"" is lowercase. But in the following ""dB"" is capitalized."	"As authors claim DDPNet has a great potential in CT image, they should try the method on real clinical data.
The authors should perform ablation studies on different architectures of dual-domain frameworks to claim DDP can eliminate the accumulation error, like SD-ID, ID-SD."
125	Decoding Task Sub-type States with Group Deep Bidirectional Recurrent Neural Network	"The description in the methods section lacks some necessary explanations, e.g. The motivation for data preprocessing is unclear.
There is a lack of discussion on the importance of context information from both directions to decode brain network states."	"1.The structure of the manuscript needs improvement. The methods section does not correspond closely to each module in Figure 2, e.g. lack of detailed description about MITL. 
2.The article lacks a description of the motivation for the proposed method. The motivation of proposing random combinations of multi-sub-type tasking MRI sequences is unclear."	"The introduction of this manuscript is very difficult to follow and the motivation of the study was not well articulated.
The article has some grammatical errors, such as: ""Group-wise brain sub-type states convenient our method..."""
126	Decoupling Predictions in Distributed Learning for Multi-Center Left Atrial MRI Segmentation	"Partially problematic experimental procedure: the authors ""generated a test set of generic data, using 30 cases from Utah with no modification to the gold standard labels"". Since these images belong to the same distribution of those used for training one of the nodes, I don't think they constitute a good proxy for generic data. This aspect should be at least discussed in the Discussion section.
Lack of details about the network architectures: for instance, the authors state that ""The personalized module was built with five convolution layers and SoftPlus activation"", but no other information is available. This must be fixed by presenting the full architectural details at least in the supplementary material."	Difference from previous work are not clearly described. Result reliability may be affected by the dataset settings during experiments. See detailed comments.	
127	Deep filter bank regression for super-resolution of anisotropic MR brain images	There aren't any weaknesses from my mindset	"My main concern with this work is its applicability in a more realistic scenario - when slice-thickness is not an integer value. This is something that is not covered in the discussion and conclusions section. As this is a very common scenario in routine clinical data I would like an elaboration (or discussion) on this extension of the work in the paper? From looking at the model, my feeling is that this could be a non-trivial task?
Furthermore, as the model is fitted on an image-to-image basis, using two first-order optimization steps, I would like details on runtime. This is particularly important if large populations of retrospective hospital MR images are to be processed, in a feasible time.
Finally, it is a bit concerning that the results section shows a drop in reconstruction performance in the thick-slice direction, seeing this is the direction of interest. It may very well be that this is because data of this type is missing during training; however, the nature of the problem this paper is tackling is such that this type of data will not be available. Or if it was, could it be included in your model somehow? Please elaborate."	"(1) The method is not adequately described, for example, how does the data flow in the model? How does the data flow in the model? How do we manipulate the filter? How do we train generator?
(2) Experimental validation cannot support the benefits of the framework described in the Page2-3.  How does the method explicitly synthesize the missing high frequencies? The dynamic capacity for lower-resolution images? The robustness of the method?"
128	Deep Geometric Supervision Improves Spatial Generalization in Orthopedic Surgery Planning	"The manuscript is a bit difficult to read for me. I appreciate the effort by authors to explain the framework as generic as possible but this generalisation make the paper difficult to read. Please identify the cost functions in Eq. 1 as it is based T=3. What are MSE and BCE on page 4? what is spatial-to-numerical transform (DSNT) on page 5? Please define them mathematically in separate equations.

Figure 1. Please provide explanation in the caption explaining notations. Figures should be understandable on their own without searching in the text.

Figure 2, why the frequency of p>=2.5 and p<2.5 does not sum to 100%? Please also explain the vase shape in panel A."	"Lack of appropriate discussion of the results and experiment choices
Excessive use of math notation makes the paper convoluted and difficult to read in my opinion.
Some of the methodology could be better explained"	"I fully understand it is not easy to put everything into this paper due to the page limit, but the paper suffers from a lack of explanation. The organization could be improved to help readers better understand the problem to solve and developed methodology. 
All the papers cited to explain the limitation of the existing methods are from the author's group. Citing other studies would help better assess the outcome this study achieved."
129	Deep is a Luxury We Don't Have	This paper only compares with one existing approach, namely GMIC. Also, the proposed method can be seen as a modification to GMIC.	this work fails to clearly explain why the effective attention is suitable the high-resolution input. Meanwhile, the title fails to reflect the entire submission and misleads readers in the architecture design.	"Weaknesses:

Need more justifications about the novelty claims
Need to include more related work that are highly important
Need to check for grammatical errors and typos.
The evaluation needs to be enhanced in terms of baselines, datasets, settings, etc."
130	Deep Laparoscopic Stereo Matching with Transformers	Nothing as far as MICCAI conference is concerned. For minor suggestions, see detailed comments.	"Limited novelty in terms of method design. The authors simply try replacing the 2D and 3D CNN with the equivalent Transformer architecture that comes from prior works. No special adjustment has been attempted to better exploit the attention mechanism for the task of stereo matching.
Loss landscape visualization is not convincing. The authors try to use Figures 4 and 5 to show that the proposed model can achieve a lower local minimum with a flatter region around the optimal point. However, it is hard to appreciate the claim based on these figures. The scales of the figures are not controlled, and it is also difficult to see the value of the optimal point. Additionally, a lot of other factors besides the network architecture can affect the loss landscape upon the convergence. For example, the choice of the optimizer. There are works showing that Transformer can form a steeper loss landscape around the local minimum but with a better optimizer, it can achieve a flatter neighboring region [1].

The experiments are not thorough enough to validate the performance of the proposed method. The authors only use 10 frames in SCARED2019 to evaluate the performance. With such a small sample set, it is difficult to say if there is any statistical significance in the performance of various methods. The methods named ""Supervised"" are also not clearly cited, thus it is difficult to appreciate what prior works the authors have compared to. LEAStereo is also not included in Table 3. Authors claim that the proposed method works better than LEAStereo in the texture-less region. In Fig. 1 of supplementary material, I don't see a significant difference between the methods. More learning-based stereo matching methods need to be evaluated on a larger dataset to validate the claims made in the manuscript.

Chen, Xiangning, Cho-Jui Hsieh, and Boqing Gong. 2021. ""When Vision Transformers Outperform ResNets without Pre-Training or Strong Data Augmentations."" arXiv [cs.CV]. arXiv. http://arxiv.org/abs/2106.01548."	Maybe I missed it somewhere but I was wondering how fast the inference is, as it will be quite important for use in practice.
131	Deep Learning based Modality-Independent Intracranial Aneurysm Detection	"no true baseline method, comparison to other methods is based on literature values that were achieved using different datasets.
Dataset is biased towards larger aneurysms, which are easier to detect."	"The main weaknesses of the manuscript are:

missing related work on automatic intracranial aneurysm detection in both modalities, CTA and MRA by Hentschke et al.: https://ieeexplore.ieee.org/document/6235669
It remains unclear from the data description how well the used data sample reflects data from clinical routine. Has the data been acquired using the same MR and CT scanners by the same trained personell? Were there differences in the MR scanning protocol and sequence parameters? Additional information is necessary to assess whether the approach can be transferred to data from clinical routine. I also miss a brief discussion of data quality/imaging artifacts and their potential impact.
The key part making the method modality-agnostic is the vessel extraction. As such, it must be described in more detail. It remains unclear how good the segmentation is, whether it fails in which cases and if manual refinement is necessary. The post-processing of the initial surface, e.g., smoothing, usually requires the setting of parameters. Which parameter values were chosen? In summary, it remains unclear how good the vessel extraction performs and how much the aneurysm detection depends on the quality and the parameter adjustment.
Manual aneurysm identification was conducted by one rater. Since this is a difficult task and small aneurysms may remain unnoticed, I suggest for future work to include a second rater and also, determine inter-rater variability."	"The PointNet architecture used is somewhat outdated, and there have been significant advances for these types of architectures. Might be worth mentioning this in the discussion.
I also wonder if testing on CTA data that has at least 1 aneurysm per patient potentially underestimates the false positive rate, although the sampling performed somewhat addresses this concern."
132	Deep Learning-based Facial Appearance Simulation Driven by Surgically Planned Craniomaxillofacial Bony Movement	"The methods could be better explained with more detail.
o	Figure 1 needs more labelling and explanation in the caption
o	2.2 convulational layers are used but it is not clear how many filters are used
The context for the results is missing.  How large can errors be for this task?  What is the resolution of the imaging being used?"	One concern is that, as demonstrated in experiment part, no significant difference was found of the quantitative experimental results on six different regions. It probably because the different deformity may involve different regions, the author may further refine the task and the dataset in future work to further improve this point and apply the proposed method to real clinical application.	"The only main weakness that I find in the paper deals with related works. 
First of all, references to related works should be double-checked. For example, authors cite PointNet papers [9,10] to support the sentence ""Deep learning-based approaches have been recently proposed to automate and accelerate the surgical simulation"", which is not consistent.
Moreover, I would suggest authors to include some references to other works which had to cope with the problem of identifying correspondences between shapes/point clouds in surgery (such as https://link.springer.com/chapter/10.1007/978-3-030-87202-1_36 or https://link.springer.com/chapter/10.1007/978-3-030-59719-1_70). This would allow to strengthen the paper by stressing the potential impact of the proposed methodology outside the specific field considered."
133	Deep learning-based Head and Neck Radiotherapy Planning Dose Prediction via Beam-wise Dose Decomposition	"I do not understand the reasoning behind the value-based DVH loss. In order to calculate a dose volume histogram, voxels receiving a similar amount of radiation dose have to be binned together (see Nguyen et al., ""Incorporating human and learned domain knowledge into training deep neural networks: A differentiable dose-volume histogram and adversarial inspired framework for generating Pareto optimal dose distributions in radiation therapy"", Medical Physics 47.3 (2020): 837-849, reference 10). Without such thresholding or binning operations, the inner part of equation 3 simply becomes the average error - the sorting operation does not change this.

Radiotherapy dose prediction is one important step for radiotherapy treatment planning and has been arguably the most researched one in the recent deep learning era (Ge and Wu, ""Knowledge-based planning for intensity-modulated radiation therapy: a review of data-driven approaches."" Medical physics 46.6 (2019): 2760-2775). However, in order to generate an actual treatment plan, a set of radiation beam arrangements, shapes and intensities that delivers the desired dose distribution has to be found. While experiments demonstrating the utility of the proposed method as starting point for such an optimization may exceed the scope of this study, the authors should at least discuss how exactly they envision their method to fit into the clinical workflow and acknowledge that the proposed method offers no guarantee to generate a radiation dose distribution that is physically deliverable.

One aim of the work was to specifically improve the prediction accuracy increase along the radiation beam paths. However, dedicated experiments that quantify the obtained benefit have not been included.

Currently, several baseline methods are not included in the result figures 4 and 5.

In several passages of the paper, the authors use the terms ""statistic"" and ""significant"" without having performed any statistical analysis. The authors should either include a statistical test to determine whether the observed improvements are significant or refrain from using these terms altogether.

There are a few grammar and wording mistakes throughout the manuscript that should be corrected."	"No statistical analysis has been made for evaluation, and then it is unclear that the result can be confirmed. The authors should show the variance of the resultant scores. In addition, statistically significant difference should be discussed.
Improvement from the existing studies seems to be small.
Each radiotherapy beam might have a machine-specific dose distribution. However, it does not assume it."	"Some of the key technical components are introduced without sufficient details.

The evaluation and comparison are not sufficient.

A fair comparison description should be provided in the experiments.

More detailed comments are given in the following Sec. 8."
134	Deep Motion Network for Freehand 3D Ultrasound Reconstruction	Movement records from IMU looks really noisy and to solve that problem, better IMU sensor and possibly Kalman filter can be used. The scale of location and position is in meter which is not make sense in this study.	"Process of calibration mentioned but not explained. Similar it is unclear, how the ultrasound and the IMU sensors get synchronized at highest precision. 
Evaluation is performed with SOTA and proposed MoNet approach on two home in-house datasets. It would be nice to have other/external benchmarking datasets in the sense of an away game being in depth tested, too - if possible regarding input data requirements."	"I am a bit skeptical about the justification of some parts of the method.
The chosen notations make the paper a bit harder to understand (more details in Section 8)."
135	Deep Multimodal Guidance for Medical Image Classification	The improvement of the method is not significant, which is consistent with the intuitive expectation. In my opinion, the paper is a type of feature-level fusion work, which is common in the domain.	"No major weaknesses to report.
Comments:

You evaluated the performance on multiple splits. Please consider also mentioning the standard deviation between the rungs to allow estimating the robustness of the results (if the available space is too tight, possibly in the supplementary materials alongside the boxplots).
Consider highlighting the best performing methods in the tables for easier interpretation
Although widely know, consider introducing abbreviations like MRI and MSE at their first use"	The novelty is not 100% clear.  We are told that references 11, 16, 20, and 30 do a highly similar thing to this paper... but there is no explicit statement about whether the current paper consists 100% of applying a useful technique from another domain to this domain; or whether there are novel methodological aspects.
136	Deep Regression with Spatial-Frequency Feature Coupling and Image Synthesis for Robot-Assisted Endomicroscopy	X	"In the Methodology part, a general picture and mathematical problem formulation are recommended.
More details of fig.1 and 2 are preferred.
The viusalization of result is recommended."	"Equation 3, why use linear interpolation instead of other interpolation?
Wouldn't the interpolation create artificial image with artifacts or blurring that confuse the network?"
137	Deep Reinforcement Learning for Detection of Inner Ear Abnormal Anatomy in Computed Tomography	"The methods part is not clear and detailed.
	1.	How do you define D_image? How is it related to d_ji?
	2.	How do you ""merge"" the Q-values for each specific landmark? it seems you have a vector of Q-values per agent and a set of runs. How do you calculate the standard deviation if those are 10 runs of vectors? Please clarify
	3.	How does aggregation of variances across agents and Q-values validate the hypothesis of uniform distribution?
	4.	Describe Fig. 4 in the caption. 
	5.	The definition of the weighting factor looks somewhat arbitrary. How were D_training and U_training trained? 
	6.	The weighting factor defined with the median might dramatically change the variance of wU_image after weighting. Depending on their resulting variances, that may lead to an effective C_image equal to D_image or wU_image. "	"The strength indicated above of using normative data only could also be viewed as a weakness. Without knowing the representation of anomalies, the decision boundary for anomaly detection maybe ambiguous (see Xuan et al, 2022, GAN-based anomaly detection. https://doi.org/10.1016/j.neucom.2021.12.093). For example, this approach would be limited approach in differentiating novel anatomies vs real abnormalities. However, as a first attempt this can be considered a step in the right direction.
The core deep reinforcement learning presented is largely a straightforward application of Leroy et al (2020). https://doi.org/10.1007/978-3-030-66843-3_18 and Vlontzos et al (2019). https://doi.org/10.1007/978-3-030-32251-9_29. The innovation is therefore limited to the development of the abnormality measures.
There is no effort to explain the difference in the two developed abnormality measures' performance particularly for the second data set given that the two approaches are quite different."	Lack of comparisons to other methods. With enough amount of data, I was curious how a classification network would perform on this task.
138	Deep Reinforcement Learning for Small Bowel Path Tracking using Different Types of Annotations	there is little discussion on related literature - algo 1 could have been better analysed in text - It is not clear why RL is the solution here and not a wall aware graph method or topological method. Can the authors please expand on the motivation of their chosen methodology	"It is not obvious what the task of 'path tracking' means (without reading other cited work) and a complete definition needs to be added in order to make the paper self-contained
Some unclear sentences e.g. ""They were done during the portal venous phase"" (better to clarify what 'they' means); ""to include from the diaphragm through the pelvis"" (to 'include' what?)
Some informal wordings e.g. ""It would be nice if""
Some undefined terminology e.g. ""One more termination condition of zero movement"" (the condition needs to be explained further; is it when the agent converges to a steady answer?); ""wall detection as input"" (I appreciate the explanation in the following sentences but what is exactly is used as input? Is it a mask of the image?)
Some claims made without proper evidence e.g. ""Euclidean distance to the closest point on the GT path, which is used in [20, 9], would be inappropriate for our problem"" (I appreciate the explanation that follows but some preliminary experiments to demonstrate this would have been nice to see)
Statistical test results for comparisons help to justify claims such as ""increase the performance"" and provide information about comparisons, however, these are not performed"	The metrics to evaluate the methods are different from those in Ref. [16]. And the reason/motivation of using different metrics is not clear to me.
139	Deep treatment response assessment and prediction of colorectal cancer liver metastases	Since the dataset is relatively small ... 245 scans ... crossvalidation or montecarlo sampling would have been better to show robustness of the approach. It can currently be biased to the split.	"No clinical or demographic information of the cohort is provided.
PRODIGE20 dataset is for aged patients (~75 years). It can create a bias in the model. I am wondering how the model will work for the young generation
It seems authors have designed a classifier to predict 4 classes. In that case, it is not clear what the ROC curve is providing. Moreover, it will be interesting to see the results for each class.
It is not clear why the method is working so well. Which features were extracted using deep learning!!"	"There's limited innovation in the model design. The feature extraction model is based on ResNeXt architecture with slight change of adding skip connections.
The size of the dataset is limited. No cross validation was conducted.
The clinical value of treatment response assessment model is unclear."
140	DeepCRC: Colorectum and Colorectal Cancer Segmentation in CT Scans via Deep Colorectal Coordinate Transform	The relevance of some results in unclear: in table 2, the last row shows the results of nnUNet and Swin UNETR in the colon task of the Medical Segmentation Decathlon, but there are no results for DeepCRC. Hence, the reason to include results on that dataset is confusing. In addition, the analysis of those results is confusing because it is combined with the conclusion on the in-house dataset (last paragraph of Quantitative results).	"Would have been more valuable to also apply to public colorectal datasets with prepartion e.g. the medical image decathlon dataset so that true comparisons can be made to the state-of-the-art. This would demonstrate the generalisability of the method and a wider comparison to published results.
The authors conduct 5-fold validation on the 107 case dataset. While not overfitting during training, there is a risk that algorithm decisions and hyperparameters were tuned to this dataset. It would strengthen the paper to see an additional independent dataset or testing of generalisability on the medical image decathlon dataset
No code and models are planned to be released making this approach difficult to verify independently, especially given that a custom in-house dataset was used."	"To make the contribution of the paper clearer and make it to be valuable to a broader range of readers, more discussion about the existing literature on how topology information is exploited is important.
More state-of-the-art segmentation algorithms suitable for colon rectum and colorectal cancer Segmentation should be compared.
The expression of the paper is clear in most of the circumstance. However, there are still typos and grammar mistakes in the paper. It should be checked carefully. Here is an example:
""We validate our proposed method on a in-house dataset, including 107 CT scans with manual colorectum and CRC annotations."""
141	Deep-learning Based T1 and T2 Quantification from Undersampled Magnetic Resonance Fingerprinting Data to Track Tracer Kinetics in Small Laboratory Animals	As only T1 map is needed to detect Mn2+ tracer effect, a T1 mapping only method with much shorter acquisition time can be used instead of using MRF and complicate network and map both T1 and T2 information. In other words, simultaneous T1 and T2 mapping is not mandatory here.	"The proposed method is not hugely novel as it mostly builds on architectures that have been proposed for MRF in Humans before. (Method novelty is not necessarily a claim by the authors here, and they correctly cite previous works).
Most components have no discernable impact on the reconstruction score. The use of a U-Net over a template-matching approach, and the use of the moving window over a feature extractor (i.e., a single conv layer) are the only choices that seem to clearly improve things.
In general, an issue with single dataset studies like these is that it is unknown if the proposed method requires retraining on each new dataset, meaning time and compute, or if the learnt features are general enough to be applied to any new dataset (with the same sequence). The impact of the paper would be much stronger if generalizability was investigated. Another impactful question is: can you design/train the network in such a way that any number of spiral shots or any number of time frames can be used as input, so that the most is made of the available data."	"Data augmentation during training: an up-down flipping may barely augment usable data, other simple data augmentation techniques could be more useful.
Missing discussion on related works [3,4] and explanation of the lower performance of the cascaded network."
142	DeepMIF: Deep learning based cell profiling for multispectral immunofluorescence images with graphical user interface	"Cell detection is proposed on M-IF images, though the authors show promising classification performance based on the detection results. While cell segmentation is more desired from the biology point of view. Besides, there are existing deep learning based cell segmentation algorithm (deepcell).
With the detected cells, a fixed size (20x20x3 for nuclear, and 28x28x3 for non-nuclear) is used for classify marker positivity. While this size lead to the inaccuracy for the marker positivity evaluation since different cell can have different size."	While the paper is clearly written and the authors provided detailed results to demonstrate the effectiveness of the proposed method, some parts of the evaluation could be further improved. Moreover, as DeepMIF is proposed as an accessible tool, it would be great to see its application on different diseases. This is also mentioned by the authors in the limitations.	"My single criticism of the paper is that I could not find that the data is available. The code will be released through GitHub upon acceptance, and that is fine, but there was no mention (or I missed it) of where the data is and if it is available for comparison.
The other limitations (small sample size, etc.) are acknowledged by the authors and is understandable at this stage of their work."
143	DeepPyramid: Enabling Pyramid View and Deformable Pyramid Reception for Semantic Segmentation in Cataract Surgery Videos	"The authors use data from CaDIS dataset, but they do not compare its results to DeepLab v3+ and UPerNet, which are the two networks used in the original CaDIS article [1].
There is no discussion about the impact of the imbalanced class Instruments. Cornea (78:84), Instruments ( 3190:459),  Lens (141:48), and Pupil (141:48).
The authors provided an 8-page supplementary material in which they further discussed their proposal and presented additional experiments. However, according to MICCAI guidelines, only images, tables, and proof of equations are allowed and that these materials must not exceed two pages.
[1] Grammatikopoulou, M. et al. ""CaDIS: Cataract dataset for surgical RGB-image segmentation,""
Medical Image Analysis, Volume 71, 2021."	"The weaknesses of the paper are two-fold:
1) The proposed three modules are all combination of some exiting techniques. The novelties of this paper is very limited. 
2) The experiment analysis is not sufficient. The ablation study did not explain some details. For example, the PL loss is removed, which other loss is used?"	The paper needs more organization, a detailed description and more experiments.
144	DeepRecon: Joint 2D Cardiac Segmentation and 3D Volume Reconstruction via A Structure-Specific Generative Method	"The paper is good overall, however the following issues need clarification:
Dice on the segmentation produced by the generated images is a good proxy for the quality of the synthesized MRIs; however, I am surprised that no metrics on the image was computed in the experiments (e.g., SSIM, MSE, etc), as this could be a more direct measure of the synthesizing quality? Why was this not performed?
Some test for statistical significance (e.g., Wilcoxon) would provide improved interpretability of the results.
The loss function contains several balancing hyper-parameters, but the paper does not state how these were chosen (nor their values); please specify. Furthermore, is there also some weighting between the segmentation part of the loss, and the image part?"	"The novelty in the method and network design is not high.

Some technical components need to be clarified.

The evaluation and comparison are not sufficient.

More detailed comments are given in the following Sec. 8."	"The paper does not have great novelty  in the technical area, it makes an ensemble of already existent architectures, to improve a known problem with a different solution. 
The solution could be too elaborated, although it has very good results. 
It is missing more information about parameters and the type of infrastructure needed. 
Some steps are not so clear, though I understand that 8 to 10 pages is not enough space to describe it completely."
145	Deformer: Towards Displacement Field Learning for Unsupervised Medical Image Registration	"The idea of using multi-head attention to capture the long-range dependencies in image registration is not particularly novel. Recent Transformer-based methods [1,2] share a similar design.
Model complexity. It is worth noting that the multi-head attention is notoriously computationally intensive, while the runtime and model complexity are ignored in this work. The results will be more convincing if the model complexity, i.e., number of learning parameters and FLOPS, and runtime of each method are reported.
The authors argue that the attention mechanism in the DIR task may lead to inferior performance as corresponding voxels should only be found in a limited local range. However, there is no particular solution to this issue in this paper. And the proposed Deformer module also leverages global multi-head attention to model the long-range dependencies of the features, which contradicts their statement.
The sample size of the test set is not sufficient. Only 1 atlas and results of 28 (8 + 20) scans are reported. The paper will benefit from including more atlas for evaluation to reduce the statistical bias.
References
[1] Chen, Junyu, et al. ""ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image Registration."" MIDL2021.
[2] Zhang, Yungeng, Yuru Pei, and Hongbin Zha. ""Learning dual transformer network for diffeomorphic registration."" MICCAI2021."	"Lack of novelty. Transformer has been widely used into medical image registration, authors should compare with the existed methods.
Zhang, Y., Pei, Y. and Zha, H., 2021, September. Learning dual transformer network for diffeomorphic registration. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 129-138). Springer, Cham.
Reviewer concerns about the validation of the experiment. In terms of LPBA40 with 56 annotated regions, although it is widely used in registration evaluation, the validation seems not to be well qualified. Like revealed in Wei et al., the annotation of LPBA40 is of low quality. If there is no refinement, it is better to avoid directly using it to evaluate the registration methods.
Wei, D., Zhang, L., Wu, Z., Cao, X., Li, G., Shen, D. and Wang, Q., 2020. Deep morphological simplification network (MS-Net) for guided registration of brain magnetic resonance images. Pattern Recognition, 100, p.107171.
In terms of the Fig. 2, it is still hard to find the improvement region."	It is still an encoder-decoder architecture which adds several Deformer Module. It would have more novelty with more model design.
146	Degradation-invariant Enhancement of Fundus Images via Pyramid Constraint Network	"the main issue if the paper is in the results section and comparison to other methods. Since the proposed method has a augmentation module (SeqLC), it is not fair the competitor methods did not have any data augmentation step. Or similarly, we need to have the performance of the proposed method without the SeqLC. Therefore the contribution of the paper is not well examined.
make table 1 easier to read
it seems that figure 1 missed the cnn layers. Perhaps the orange arrows need to be denoted as conv/downsampling. same for gray arrows."	"The novelty is limited, for example, the feature pyramid structure is a common strategy [1] and the constraint is contrastive learning conducted on features of each level. Hence, the FPC may not be qualified a contribution to this work.
The authors claim that the straight constant on feature maps is too inflexible. However, they do not demonstrate the convergence of FPC.
[1] Lin, Tsung-Yi, et al. ""Feature pyramid networks for object detection."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017."	The experimental results focus on the vessel segmentation after enhancement. How about the results on classification tasks after enhancement.
147	Delving into Local Features for Open-Set Domain Adaptation in Fundus Image Analysis	"computationally expensive, could be infeasible for large data
model details need clarification"	"1.Some technique details are not clear:
   a.The paper does not mention whether the training process of the CNN model is end-to-end or not since it contains multi-branch.
   b. In the Informative Region Selection part, it depends on the CAM to select patch features, but not mention the size of the patch.
   c. The experiments does not mention the input size. People may want to know it because fundus images are probably in higher resolution than natural images.
2.Datasets are not enough: I notice that the path cluster contains hard exudates, which are sign of diabetic retinopathy(DR). Try some DR datasets such as IDRID may make the paper more promising."	"The proposed method seem to be relying on relatively strong assumptions: a) lesions (discriminative information) are localized; b) the source model performs reasonably well on the target data (small domain gap) so that initial clusters of the same class between the source and target can stay correctly close; c) there is little class-imbalance issue (otherwise majority classes may overshadow minority ones in the clustering); d) the number of private classes ($\beta$) is kind of known beforehand. The authors are encouraged to comment on whether these assumptions are too strong to be applied in practice?

Given that there is a k-means clustering in the method, what is the computational overhead of that, in comparisons with other steps during the training process?

There is little information regarding the source model: a) does it use a linear classifier in the end? If so, why are clusters assigned based on l2 distance, while the detection of private classifier are based on thresholding the softmax output of a linear classifier? b) will the source model to be updated during adaptation as well? If so, would it hurt the performance on source-domain data?"
148	Denoising for Relaxing: Unsupervised Domain Adaptive Fundus Image Segmentation without Source Data	The paper builds on the previous UDA methods and the innovation is incremental. The accuracy boosts are also incremental. How clinically significant is the accuracy boost?	"W1: This paper lacks related work review in self-training based SFDA methods. Authors only review one self-training based SFDA method [3], and hence the comparison makes the proposed approaches seem to be novel. However, there are many self-training based SFDA methods, such as [1-3]. For example, [*1] also propose an uncertainty-aware self-training method to generate reliable pseudo-labels. It is suggested to compare the proposed method with more self-training based SFDA method, so as to elaborate the contributions of proposed method.
[1] Ye M, Zhang J, Ouyang J, et al. Source Data-free Unsupervised Domain Adaptation for Semantic Segmentation[C]//Proceedings of the 29th ACM International Conference on Multimedia. 2021: 2233-2242.
[2] Prabhu V, Khare S, Kartik D, et al. S4T: Source-free domain adaptation for semantic segmentation via self-supervised selective self-training[J]. arXiv preprint arXiv:2107.10140, 2021.
[*3] Kim Y, Cho D, Han K, et al. Domain adaptation without source data[J]. arXiv preprint arXiv:2007.01524, 2020.
W2: As for contribute C1, the novelty of the proposed adaptive class-dependent threshold strategy is limited. It is similar to [*4], and this idea has been widely utilized in self-training based unsupervised domain adaptation methods.
[*4] Zou Y, Yu Z, Kumar B V K, et al. Unsupervised domain adaptation for semantic segmentation via class-balanced self-training[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 289-305.
W3: As for contribute C2, the definitions of symbols are not clear, making details of the whole process hard to understand. For example, it is not clear what 'the out-of-sample predicted probabilities P_{t}^{hat}'  means, what 'out-of-sample' is and what the shape of P_{t}^{hat} is."	"-In section 2.1, not all parameters are described before they are used. For example in equation 6, what does v stand for?

Details about the  complexity of the model (e.g., number of model parameters, or training time) are missing.
-The split into training and test data on the source domain is unclear to me. Are there 50 images in the training and 51 images in the test set? 
-I guess table 1 shows the mean and the standard deviation over all subjects of the test set? If so, this needs to be stated in the caption. The standard deviations are quite high, is this due to the small number of images in the test set? How big is the test set?
-No hyperparameters were mentioned. If they are taken from other implementations, please indicate that in the paper.
-In Section 2.2 in the last sentence, the mislabeling is only the case for i not equal to j, which is not stated in this section.
-For the ablation study, it is not clear exactly what parts are ablated in the cases a) to e). I suggest to refer to the subsections of Section 2 to make clear what part of the pseudo labeling is left out."
149	Denoising of 3D MR images using a voxel-wise hybrid residual MLP-CNN model to improve small lesion diagnostic confidence	"Information about computational time is not presented.
Limited comparison to state-of-the-art:  Only PSNR and SSIM mesures are studied. But, it's enough for a conference paper.  A computational time comparison is missing."	"The authoer only evaluate the proposed method on data with simulated noise.
Albation study of the proposed MLP-CNN architecture is missing.
Relatively week baseline. Recent proposed Transformer-based denoisers are not compared, e.g., SwinIR."	The only suggestion is to consider conducting ablation study to show the contribution of residual MLPs and residual convolutional subnetwork parts for final denoising results.
150	DentalPointNet: Landmark Localization on High-Resolution 3D Digital Dental Models	"The novelty of the paper is limited.  Although there is an RPN stage and a new loss function, the contribution of improvements of the DLLNet is not clear.
In the paper, DLLNet is criticized for not performing well on meshes with missing teeth and being sensitive to the segmentation errors. More experiments are needed for this claim. Is the RPN  only needed to eliminate edentolous patients or has it more advantages.
There is also reproducibility concerns about the work.
The method is claimed to work well on patients with partial edentulous. However, the experiments are limited because there are only 15 patients with partial edentulous."	"Major weakness:

Limited novelty. In section 3.2, the authors indicate the main differences are balanced focal loss and the curvature constraints to prior work. 
However, the hyper-parameter in focal loss is not given, i.e., alpha and gamma for both the proposed method and PointRCNN. The proper tuning of these hyper-parameters for the use case of this paper may improve the reported baselines, although I believe the proposed method could still outperform the improved baselines. Besides,  I also suggest giving a complete form of focal loss in the paper.
Second, it is unclear how the curvatures are computed and how the threshold (0.65) is defined. Without curvatures (variant-1), the proposed method degenerates to a similar performance as DLLNet in table 1 but is better for edentulous patients. Please consider clarifying this point.

Missing results on only coarse stage. The proposed method is further refined by training using DLLNet after coarse stage training. What is the performance of only the coarse stage? Is it similar to PointRCNN? (To me, the coarse stage is PointRCNN + curvature. Correct me if I am wrong.)

The training time seems to be doubled or more (PointRCNN training can be time-consuming) than only training DLLNet. Will DLLNet get improved by longer training?
Minor issues:

It should be ""Adam"" optimizer, right? ""ADMA optimizer"" appears twice.

At the beginning of page 5, the shapes of feature matrices should be modified to be consistent. For example, F^4_s should be R^{1x256}, instead of 256x1. It confused me at the first glance."	"The proposed method uses parts (same parameter values and sentences) from the DLLNet method published in the MICCAI-2021.
The choice of some parameter values used in the paper should be detailed.
Tables 1 and 2: Apparently the FP and FP rates are computed for all five assessed regions - it would be more appropriate to present these numbers for each region."
151	DeSD: Self-Supervised Learning with Deep Self-Distillation for 3D Medical Image Segmentation	"There are several weaknesses:

Since self-supervised pre-training has been widely adopted in 3D medical images [1-3],  it is somewhat strange that the authors directly omitted the comparison with these methods in the experiment section. Meanwhile, the authors provided no literature review on the development of self-supervised learning in medical images. From my perspective, it seems that the authors just tried to borrow existing self-supervised methods, which are originally designed for natural images.

The authors fail to clarify whether they have used a 3D backbone and how to build a 3D segmentation network in details, which matter a lot in medical image segmentation. In fact, it is confusing that the authors used a 2D ResNet-50 (the authors cited kaiming's paper) and applied 1x1x1 convolution on top of it. So, do you employ a 2D or 3D backbone in practice?

Lots of implementation details of baselines are missing. Moreover, they provided no visualization results of most baselines in the supplementary material. In my opinion, I'm not sure about whether the experimental comparisons are fair.

[1] Taleb et al. 3D Self-Supervised Methods for Medical Imaging. NeurIPS 2020.
[2] Zhou et al. Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts. ICCV 2021.
[3] Zhou et al. Models Genesis. Medical Image Analysis 2020."	"Limited novelty. The major issue with this paper is its novelty. The deep supervision idea is incremental to DINO, and the other parts are almost the same as the original DINO paper. Meanwhile, deep supervision itself is not a novel design. It has long been introduced to train deep neural networks [1].

Unclear practical use of the proposed method. Although this paper demonstrates the effectiveness of introducing deep supervision to SSL, it is unclear how this idea may be used to address medical image segmentation in practice. In particular, the current mainstream segmentation approaches are U-Net or transformer-based, while the segmentation model used in this paper is simply a ResNet followed by several decoder layers. The paper does not show 1) how the proposed SSL method may help the mainstream approaches and/or 2) how the segmentation model in this paper may compete with the mainstream approaches.

[1] Lee, C. Y., Xie, S., Gallagher, P., Zhang, Z., & Tu, Z. (2015, February). Deeply-supervised nets. In Artificial intelligence and statistics (pp. 562-570). PMLR."	"The novelty of the proposed method is limited. The main framework is based on DINO [1]. And the idea of deep self-distillation is identical to Zhang et al. [2].

[1] Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P. and Joulin, A., 2021. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 9650-9660).
[2] Zhang, L., Song, J., Gao, A., Chen, J., Bao, C. and Ma, K., 2019. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 3713-3722)."
152	DeStripe: A Self2Self Spatio-Spectral Graph Neural Network with Unfolded Hessian for Stripe Artifact Removal in Light-sheet Microscopy	"Some of the exposition was insufficiently clear (examples given in Detailed Comments).
I was unclear why a GNN was desireable, and why a more straight-forward method might not work (see Detailed Comments)."	The model is very complex, has several layers, and it is unclear how many parameters, the method has, what the computation resources need to run the method, and the method has only been compared against the state-of-the-art on synthetic images. Since SSIM is linked to human perception, which seems a less obvious choice in this case, and it is unclear how the PSNR between two images has been calculated, and thus the results in Table 1, can be difficult to interpret in light of the noise removal task.	I don't see many weaknesses. If I have to name one, I would say the paper could benefit from a few more explanations to lead the reader to better understand the technical details of the methods.
153	Detecting Aortic Valve Pathology from the 3-Chamber Cine Cardiac MRI View	I do not see any major weaknesses.	"Many details are not actually clear from the manuscript and the presentation is rather confusing at times:
1) What does curve tracking mean and how is this implemented?
2) What quantitative criteria do you use to obtain ""probability of being a pathological curve""
3) How are features (angles, length, distance to image patch center, probability) computed from the heat maps?
4) ""coronary cusp leaflet"" is a confusing formulation. Does this model only one aortic leaflet? Does it model contours or a landmark (located on the free edge of the leaflet)?
5) The number of frames to be used also depends on cardiac phases, as aortic stenosis is only apparent during systole, and regurgitation only during diastole, which are typically of different duration. Also patients may rarely suffer from both stenosis and regurgitation, more frequently from only one of these conditions. How is this handled?"	"Lack of methodological/implementation details: certain modules/aspects of the proposed methodology lack sufficient details to be fully understood and potentially reproduced. These include details regarding heatmap creation, curve tracking algorithm, proposed network, feature extraction, etc. (see specific comments below).
Limited results supporting algorithmic decisions: does replacing the conv block with dense blocks effectively improved heatmap regression? Does a three-stage heatmap regression (Fig. 3) outperforms a single- or dual-stage network?"
154	DEUE: Delta Ensemble Uncertainty Estimation for a More Robust Estimation of Ejection Fraction	"no novel DL model per se, but method to be used on top of trained models
mainly a benchmarking paper where it is not clear what the alternative uncertainty estimation adds and how would be helpful for in diagnostics, ie a further downstream task is missing"	"My main criticism is that it is unrealistic to expect the computation of variance over 5 samples to be a good estimate of the actual variance of the network parameters even when the expected variance of the parameters is low (reference [4] in the paper). Nevertheless, the authors are able to show that the estimates are in line with other approaches to estimating the variance. These other methods also have issues with sampling but it is encouraging that the results across methods seem consistent. Also, the few examples of data producing high and low uncertainty estimates are very reasonable. But I would like to see more trained networks be used for sampling the parameter variance despite the computational challenge.
The authors assume that the network model weights are independent. But I would be curious to see how large some of the non-diagonal terms of the covariance matrix are. Is the independence assumption reasonable for their network? This is a minor issue for the publications of this conference paper but this is something I would expect to see in a journal paper."	"The definition of epistemic uncertainty in Eq. 2 and 7 needs further clarify.
Experiments of this paper is inadequate. 
See detailed comments below."
155	DGMIL: Distribution Guided Multiple Instance Learning for Whole Slide Image Classification	This paper only uses AUC for evaluation on one dataset (CAMELYON16) and does not visualize slides with high positive scores.	"Lack of explanation for some settings: e.g., the reason for using Mahalanobis distance instead of others like cosine similarity, the reason for using feature after projection head instead of backbone, and the definition of refinement convergence, etc.
The reported results in table.1(a) are significantly lower than that in DSMIL, e.g., the Slide AUC of Ab-MIL is 0.6612 in this paper but is 0.8653 in DSMIL. I notice that the settings of patch size (512 vs. 224) and magnifications (5x vs. 20x) are different. It would be more convincing to run the proposed method in the same settings as DSMIL instead of reproducing other works to avoid unfair comparison."	"-About the proposed method. Rigid MIL formulation requires the permutation invariance of the MIL aggregation function (e.g., from the instance space to the bag space). However, the authors do not discuss this, and whether or not the cluster-conditioned feature distribution modeling method and a pseudo label-based iterative feature space refinement strategy can grantee the permutational invariance is unclear in the current form of the manuscript.
-The authors missed multiple latest works of deep MIL and its application in the medical image. For example,
Deep MIL:
[1] A multiple-instance densely-connected ConvNet for aerial scene classification. IEEE Transaction on Image Processing 29, 4911-4926 (2020)
Deep MIL & its application in medical image:
[2] Multi-instance multi-scale CNN for medical image classification. International Conference on Medical Image Computing and Computer Assisted Intervention, (2019)
[3] Local-global dual perception based deep multiple instance learning for retinal disease classification. International Conference on Medical Image Computing and Computer Assisted Intervention, (2021)
The authors are suggested to enrich the related work, and if necessary make more comparison.
-More validation on additional dataset is highly preferred to fully demonstrate the effectiveness of the proposed method."
156	Did You Get What You Paid For? Rethinking Annotation Cost of Deep Learning Based Computer Aided Detection in Chest Radiographs	"The way to leverage bounding boxes in this paper can be improved. The boxes are directly converted to noisy pixel annotations to train the segmentation head, which is not optimal. There are many weakly-supervised segmentation algorithms that can better utilize the boxes. Thus, the comparision of image-wise labels and box labels in the segmentation task may be debatable. 
c.f. Tajbakhsh, N., Jeyaseelan, L., Li, Q., Chiang, J., Wu, Z., & Ding, X. (2020). Embracing Imperfect Datasets: A Review of Deep Learning Solutions for Medical Image Segmentation. Medical Image Analysis. http://arxiv.org/abs/1908.10454
Tang, Y., Cai, J., Yan, K., Huang, L., Xie, G., Xiao, J., Lu, J., Lin, G., & Lu, L. (2021). Weakly-Supervised Universal Lesion Segmentation with Regional Level Set Loss. MICCAI. http://arxiv.org/abs/2105.01218"	"When comparing the segmentation models, is the ground truth all contours? If that is the case, and if the authors also using the same dice + bce loss during training for all models, then using Bbox can also be regarded as using noisy labels for the segmentation task, as the authors have also mentioned in the paper. I think the comparison in Table 4 is mainly on the impact of quality, not granularity.
Some details and discussion could be made to improve the paper."	"The main weakness is the noise model: the authors generates error randomly at a given F1 score, whereas it is plausible that the errors in NLP labelers, such as that used in CheXpert, are not randomly distributed across classes. For instance in CheXPert the percentage of uncertain labels is not evenly distributed. The chosen F1 score is also relatively low (0.8), which appears to be low compared to the F1 scores declared in the CheXpert paper
Statistical analysis is not conducted to verify whether differences are statistically significant
Some parts of the methodology are not clearly described - in particular the annotation process and some aspects of the training methods.
Related works are not discussed."
157	Diffusion Deformable Model for 4D Temporal Medical Image Generation	This paper proposed a diffusion deformable model (DDM), which can generate images of continuous trajectories with latent code. The result is comparable with state-of-the-art image registration tool voxelmorph in terms of PSNR and DICE score. However, no down streaming tasks are carried out to evaluate the effectiveness of the proposed images, so we cannot really draw conclusions about the quality of the proposed images.	"The reference style should be consistent.
Typos like ""deiffusion"" in ""Then, for the reverse deiffusion, DDPM learns the following parameterized Gaussian transformations""
It is very hard to visually check the difference between the proposed method and VM. From the Fig.4. and Table 1, their difference is still very tiny. The statistic significance from paired t-test is not enough, I would like to have a test of Wilcoxon signed rank test.
The difference of the performance on training data and testing data is not given in the experiments.
The PyTorch code provided is quite difficult to follow and verify some details, like x0 in Fig. 2 is not described in the code and I could not find theire implementation."	"The accuracy of the interpolation method is difficult to validate.
The authors have shown the superiority of the proposed method with respect to VoxelMorph, however, this is probably not the closest competing method methodologically speaking and/or competitive enough for this application."
158	Diffusion Models for Medical Anomaly Detection	"Denoising Diffusion Implicit Models seems not novel enough given that the authors did not provide many modifications to the previous method [25].
The experiment lacks quantitative comparisons.
It would be better if the author can compare with some of the SOTA unsupervised anomaly detection methods and semi/weakly-supervised classification/AD methods. Right now, one cannot justify whether the proposed method is promising or not.
The motivation of why denoising diffusion implicit models works better than the traditional Generative methods is not clearly discussed."	The paper seems to be mainly an application of an existing technique from computer vision to medical imaging.	"The performance of the method is not entirely convincing (in some hyperparamter settings it seems to surpass the presented baseline).
The VAE baseline appears to have been trained very poorly.
Hardly any quantitative comparison.
Not sure using BRATS2020 as dataset for anomaly detection is the best choice (prevalence of tumors already gives a different data distribution between normal/healthy and abnormal/diseased slices (even when discarding the lowest and uppermost slices), furthermore it is hard to tell if a tumor has deformed some unlabeled part of the brain )
A discussion on using the ""reconstruction"" error / residual as anomaly localization score would be appropriate since it has some received some critics in the recent years (see Meissen paper)"
159	Digestive Organ Recognition in Video Capsule Endoscopy based on Temporal Segmentation Network	The weakness of this paper is combinations of the existing methods. MS-TCN++,  timeSformer and  I3D. However, novel combination of existing methods can be evaluated.	"The authors take two off the shelf feature extractors and use them in an off the shelf temporal organ classification model. There is virtually no technical novelty in the proposed approach. Although the application use case is an understudied problem with the previous state of the art being a simple 2D CNN.

There is nothing in the proposed method which enforces temporal consistency. In Figure 3 we can see several areas where the proposed approach bounces around from stomach to small bowel, back to stomach, to colon, back to small bowel, and back to colon. While the final proposed approach doesn't show these in the illustrated example (only in the ablated parts) there is nothing to enforce temporal consistency to identify these transition points and ensure they're localized to a single location (one case of this is shown in the supplemental materials of the 40 cases).

There is no comparison with any prior works (e.g. [11]) or even some reasonable baselines. But there are no true technical contributions to this work, so any baseline would just be an off the shelf method, which their work already is.

How the data was split into training, validation, and testing, and any attempts to ensure no bias was introduced in this splitting decision (e.g. purposely putting easy examples in test) was also not discussed."	"Video temporal segmentation is a widely studied topic in MICCAI, with a large number of recent methods published (TeCNO, TransSVNet, TMRNet, etc). Without any direct comparisons, it is hard to understand if the proposed method has any advantage over these (see detailed comments).
When compared to these other methods, the one being proposed is very complex, with two spatiotemporal feature extractors and an MS-TCN with a very large number of stages. I would suspect this method is computationally more expensive by a significant margin, even though such details are not currently provided in the paper (happy to be proven wrong)."
160	Discrepancy and Gradient-guided Multi-Modal Knowledge Distillation for Pathological Glioma Grading	"The improvement to the accuracy seems incremental.
Please clearly state the relationship between Eq. 1,2 and Eq. 3.
How is g_{ens} in Sec. 2.2?"	"Nor, p_m (the labels ground truth) or the fusion F of genomic and CNN features are properly defined.

There are no qualitative results showing meaningful genomic expression + morphology in image regions.

There is no statistical significance test for the difference of the methods results."	"The description of stage I training is insufficient.
The explanation of DC-Distill loss is insufficient.
The description of data preprocessing is too brief."
161	Discrepancy-based Active Learning for Weakly Supervised Bleeding Segmentation in Wireless Capsule Endoscopy Images	The presented method design is mostly empirical, lacking theoretical analysis. For example, no discussion provided to illustrate convergency of the proposed training procedure.	"The proposed method is based on the assumption that large probability values represent high reliability. The assumption is natural in general, but how valid is the assumption? It should be better to add more discussion or experiments about this point.
The methods replaces CAM labels with generated labels when the samples have small model divergence and large CAM divergence. And the rank is represented by the multiple of model divergence and CAM divergence. Do the model divergence and CAM divergence have the same weight influence in the final result?  Is there any better way to measure the influence?
It will be nice to evaluate on more than one dataset in experimental part.
It will be better to introduce more details about the selected baseline methods. Why choose these methods?
What's the SOTA performance on the CAD-CAP WCE dataset? Only one fully supervised result is report, no SOTA result is mentioned and compared."	"The design of multiple propensities (coarse, standard, fine) lack details and justification;
The training procedure, especially the design of loss function, lack intuition;
The improvement is mostly on 10% data regime, while in 20% and 30% data regime the improvement is marginal; also, the evaluation is only done on one dataset;"
162	Disentangle then Calibrate: Selective Treasure Sharing for Generalized Rare Disease Diagnosis	details of network architecture are not clear	"The MICCAI writing template was clearly violated (removing spaces between text and table, wrapping text around table).
No statistical test to compare proposed methods in table 1.  This is particularly important as the method is compared against multiple baselines.

Questions:

Was the WideResNet backbone also used for the baseline experiments?"	"The authors could have also thought of framing the problem into a continual few-shot-learning problem with a single incremental epoch. A comparison with such a method like AIM (Lee et al. Few-Shot and Continual Learning with Attentive Independent Mechanisms ICCV 2021) would have made the paper stronger

The authors argue that learning new rare classes leads to a decrease in performance for the major classes. Although this is an intuitive behavior, several methods from the incremental learning literature have been adopted in the few-shot learning research and help alleviating the catastrophic forgetting issue.(Minor)

This paper might present a serious fairness issue since it compares different methods that prerequisite different and partly contradictory setups in order to work well. For instance, the FCICL method which relies on contrastive learning expects larger batch sizes than what the authors report in sec3.Implementation. From the text, it is not clear whether the hyperparameters have been optimized from each method separately (or at least taken from the respective papers). If not, then we are in a setup that favors the proposed method over the other SOTA methods.

The description of the DTC deserves better formulation to make easier to understand and reimplement. (minor)"
163	DisQ: Disentangling Quantitative MRI Mapping of the Heart	"Major concerns:
The double-encoders approach is a common practice for disentanglement, however, it often requires additional contrast loss or adversarial loss for different modules to prevent them from learning the same representations. (One recent paper I remember is [1].) 
In this paper's case, for example, the contrast module need to be forced not to learn anatomy representation and vice versa, such that they are disentangled. I am a little supervised that the cross-reconstruction alone can achieve disentangled contrast and anatomy. For bootstrapping disentanglement, this work proposed a similarity constraint for anatomy (Eqn (4)) and an information bottleneck constraint for contrast (Eqn(5)). However, it is not clear to me how this can prevent one module from learning representation from another? Maybe the way of combining anatomy and contrast as in Eqn(6)? It would be better that this work can discuss more on how the disentanglement is promoted and achieved. Some metrics of dismantlement will also be helpful besides figure (3).
Minor concerns:
It would be better to be more specific on the drawbacks of ""Groupwise"" baseline for readers to have a sense on the tradeoff between performance and computational cost. Instead of saying Groupwise ""demanded lengthy optimization"", maybe consider pointing out how roughly the computational time or resources increased?
[1] Harada, Shota, et al. ""Order-Guided Disentangled Representation Learning for Ulcerative Colitis Classification with Limited Labels."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021."	"Myocardial motion correction in T1 mapping has already been addressed [[1, 2]], but the disentanglement representation in this field is novel.
The employed models did not present execution time.
Besides showing a higher precision made by ""Groupwise"", no further limitation has been discussed.
Although the paper properly validated the proof of concept, the amount of data is highly limited, not well described, and conclusions may be limited by the generalisability constraints."	"Not appropriately justify the medical motivation of the work
Unclear presentation of the results"
164	Distilling Knowledge from Topological Representations for Pathological Complete Response Prediction	"There is no statistical analyses of the results in Table 1.
The paper is difficult to follow and lacks clarity on several points."	"The motivation of using Betti curves as topological features is a bit weak. As mentioned in the paper, it is more efficient in terms of computation. However, the topological features in the distillation method are only extracted once, and are involved only in training. It does not seem very necessary to use the faster yet less expressive feature (Betti curves) instead of persistent homology. To strengthen the motivation, empirical evidence should be provided (e.g., computational speed of Betti curves over persistent homology). Ideally, the comparison should be in the same resolution (right now the processing reduces the input image resolution).
The empirical comparison needs improvement. The authors cited the baseline results of TopoTxR from [Wang et al. 2021]. However, the backbone in the original paper is using a much weaker backbone than DenseNet. Without using the same backbone, it is not clear whether the performance boost is due to the proposed contributions or the more advanced backbone.
The authors used baselines DenseNet-CONCATE, DenseNet-MSE and DenseNet-KD to show the distillation is necessary. The main issue is that these baselines are not well designed. The authors used the logit output of a DenseNet to concatenate with / map to / compare distribution with topological features. But the logits (only 2 dimensional if I understood correctly) are already losing too much information. These baselines should be done using the last layer representation (e.g., the embedding before the last FC/soft-max), not the logits.
In general, I think the real benefit of the proposed method is the efficiency not performance gain. Even if the baselines, including TopoTxR with DenseNet backbone, DenseNet-CONCATE, and DenseNet-KD are slightly better than the distillation method, it is OK. The key is the time saved during the inference time.
42 pCR and 116 non-pCR do not add up to 162, the reported total number."	"1 - While the approach in this paper is novel there are some ambiguities in this paper. Authors should clearly justify the term ""quicker or less-time consuming"" in Betti-curve compotation as opposed to the previous persistent homology-based approaches. 
2 - The cubical complex filtration example and its definition are not very clear to the reader. While other types of simplicial complex filtration are based on the appearance and disappearance of homology groups, like graphs, which are intuitive, their cubical complex counterpart is not. Also, the term ""structure"" in appearance and disappearance is not a good choice for homology groups.
3 - There are some state-of-the-art approaches to deep learning-based persistent homology presented in top machine learning venues like ""PLLay: efficient topological layer based on persistent landscapes"" and ""PersLay: A simple and versatile neural network layer for persistence diagrams"" and ""Deep learning with topological signatures"". All these approaches circumvent the non-Hilbert Space nature of persistent diagrams by appropriate kernelization while in this study it is not clear how Betti curves are used as mere features for concatenating with CNN features.
4 - Although the authors claim the previous approaches do need feature engineering, in this study, the normalization and adaptation of two very different sets of features can also be the same engineering (clearly Betti curves need feature engineering to be used as features). In terms of pre-processing, however, the authors correctly address the minimum effort to do so."
165	Domain Adaptive Mitochondria Segmentation via Enforcing Inter-Section Consistency	"1) It seems that the performance of the Oracle and NoAdapt for adaptation from MitoEM-R to MitoEM-H in Table 2  is  low. 
2) It seem less informative to conduct ablation results onadaptation from VNC III to Lucchi (Subset1)."	"The authors developed a 2D model for a 3D task. 3D models have been used for organelle (synapse, mitos, ...) segmentation and detection since at least 2017 and vastly outperform 2D models. The method presented by the authors relies on comparing predictions on 2D slices; however, it is not clear whether this would work for the more appropriate 3D task. Further, one could argue that by having access to a ""larger field of view"" in the third dimension, their comparison to existing methods is not precise. It is not clear, whether the access to adjacent slices alone would have created the observed performance increase
The presented datasets are not representative for current EM datasets. Two of the datasets are < 20um^3 in volume."	"UDA network design needs more justification and clarification. Please see comment section Point 2 for details.
Descriptions about UDA in the introduction and method section are a bit confusing and inaccurate."
166	Domain Adaptive Nuclei Instance Segmentation and Classification via Category-aware Feature Alignment and Pseudo-labelling	"1). The details of the method part are not clear enough. The losses are not explained either in the main paper or the supplementary materials.
2). It could be better to provide other methods' results in the Dpath -> GlaS task as those in the Dpath -> CRAG task"	"Some details of the proposed methodology is missing. For the class-aware discriminator, it is unclear how the classes are assigned for the target domain due to the lack of labeled data. How to handle prediction errors that could interfere with the discriminator.
The paper doesn't adequately discuss the limitations of the proposed approach. Extensive use of pseudo-labels (in both the class-aware discriminator and in self-supervised learning) could result in model deterioration and poor calibration due to enforcing the model to optimize towards what it has already know. What are the motivations and practical considerations for using pseudo-labels? Is there any issue encountered/resolved due to mistakes in the pseudo-labels?"	"What's the main difference between the proposed method and Ref.12, so what's the highlight of your paper.
There are a lot of papers about Zero-shot or one-shot image segmentation or classification, why do not you directly use these techniques and select the UDA, which more complexity than others?
How about the failures?
There are some grammatical and sentence expression errors in this manuscript, pls revise them carefully."
167	Domain Specific Convolution and High Frequency Reconstruction based Unsupervised Domain Adaptation for Medical Image Segmentation	"How did the authors select the beta, the window size for filtering out the low-frequency component? The sensitivity analysis of model performance against different choices of beta is missing.
Any other data augmentation did the author used in this paper, except for the Fourier style augmentation based on frequency spectrum replacement? I would like to know if training images have been augmented with high-frequency noises or itself contain some high-frequency artefacts, will the HFR bias the model to keep these non-robust features in f_img? In that case, can the segmentation branch (which takes f_image as input) still produce reasonable segmentation? Have the trained segmentation model been tested on noisy test images?"	"What would happen if we use the one-hot encoding instead of providing the output of resnet?
A better commentary on Table 2 in discussion would improve the paper further. For instance DoCR is better than Intra-Domain, a simple explanation on these aspects."	"The description of DSC module doesn't seem very clear. For example: 
(1) The light-weighted DSC head is missing in Fig.1. 
(2) What the domain code is designed for? 
(3) How x^a is generated? What augmentation method is used?
(4) Why the DSC module can extract domain-insensitive features?
In Table 2, the results produced by w/o DA are better than the results of some methods with domain adaptation. It confuses me while necessary analysis seems to be missing."
168	Domain-Adaptive 3D Medical Image Synthesis: An Efficient Unsupervised Approach	"The evaluation could be performed in a cross-validation manner.
Only mean values were reported in the quantitative results. It's hard to tell the improvement is statistically significant. Please add standard deviations or test statistically to show the significance of the proposed method."	"Performance: The qualitative results in Fig. 3 are underwhelming. It's difficult to gauge without an example of what a ""good"" synthesis (in the absence of domain shift) would look like.
Writing: The paper lacks clarity and is difficult to follow at times.
Evaluation: Given that the adaptation strategy is quite generic, the paper could have more impact if the strategy was evaluated on multiple tasks. For example, it seems that the exact same architecture could be used on a segmentation task, where the VAE would be trained on one-hot labels."	"The clinical applicability of study is limited. Among publicly available datasets, I could not find the one that contain some of the modalities only partially. And the authors do not explicitly describe the application scenarios. [More details are given in Sec. ""Detailed comments"".]

The metrics SSIM and PSNR do not explicitly measure the quality of some algorithm. Furthermore, one of the frequent metrics in image synthesis is the quantitative assessment of the impact of generated images on the downstream task. More specifically, the authors could add Dice score of the glioma segmentation task, using the original modalities, as an upper-bound, and present Dice score of the same model using the generated modality instead of the original one (or, alternatively, trained from scratch using generated modality). This difference in Dice score quantitatively assesses the image synthesis algorithm also with the clear motivation in terms of further applicability."
169	Domain-Prior-Induced Structural MRI Adaptation for Clinical Progression Prediction of Subjective Cognitive Decline	"The technical contribution is moderate because the proposed mechanisms are existing and not nobel.
The proposed method is not persuasive.
The experiments should be more rigorous and thorough by comparing with the state-of-the-art methods."	"(1) The proposed DSMA model is similar to the Inductive transfer learning (ITL) model except for the feature adaptation module, so the innovation of this paper is limited.
(2) The paper employ the MMD based feature adaptation module to alleviate the inter-domain discrepancy, but still ignores the intra-domain data distribution gaps that may be caused by different imaging scanners and scanning protocols.
(3) The references are not adequate and up to date."	-
170	DOMINO: Domain-aware Model Calibration in Medical Image Segmentation	N/A	"My main concern about this work is that the loss function now depends more on the training data. I wonder whether the performance on another dataset would then be compromised. The authors need to include experiments on other datasets acquired with distinct imaging considerations to show whether this is the case or not.
Performance of network wrt to headreco: Fig 5 shows headreco can segment most regions nicely (except air). I wonder then what would a model trained on headreco with the domain-aware loss would have to offer?"	
171	Double-Uncertainty Guided Spatial and Temporal Consistency Regularization Weighting for Learning-based Abdominal Registration	"the ablation study does not seem to have a clear conclusion
the statistical significance of obtained ""VM (AS+ATC)"" results should be verified wrt. methods in comparison in order to make enable clear conclusions
it should be investigated how the initial regularization weight impact the obtained final result (i.e. is hyperparameter tuning still required with the proposed approach?)"	The discussion on the mean field or images is required to justify the uncertainty computation. There lack some descriptions of the threshold selection and the loss function.	Description of the implementation was somewhat hard to follow, especially as it appears that it requires manual specification of numerous parameters (e.g., thresholds).  How might these things change for a different dataset?  It also seems like uncertainty results for registration are mainly based on 6 forward passes, which would appear fairly small.
172	DRGen: Domain Generalization in Diabetic Retinopathy Classification	"No major weakness, however the results are a bit underwhelming. Performance on 3 datasets is clearly improved, whereas the performance on the 4th one, is markedly decrease. As a results the overall performance is only slightly above baseline.
This is noted in the discussion but no explanation of why this is happening is provided. It would be worth investigating a few hypothesis."	The motivation behind the proposal is unclear.	"I see 2 points as main weaknesses:

it is not clear to me the contribution of this work with respect to the method. It is clear that the existing method was not yet applied to the chosen dataset, but what is exactly the difference vs. Fishr? is it the usage of stochastic weighted averaging? if it is, then this is not a new method, rather a variation of Fishr.
the standard deviation measure is misleading. In this case the standard deviation quantifies the difference in performance for different testing datasets, however here it is not clear to me why a successful method should have less variability, as the different datasets vary in distribution and size. A standard deviation measure should be given to any result by re-running the experiments with different pseudo-random numbers, in order to quantify the improvement vs. variability; not quantifying the variability of one experiment across different settings."
173	DS3-Net: Difficulty-perceived Common-to-T1ce Semi-Supervised Multimodal MRI Synthesis Network	The technical contribution of this work is not very clear. Some key parameter settings seem adhoc. The experimental results are not well explained.	The authors only tested the proposed method on the BraTS2020 dataset. More tests are required. Otherwise, the quantitive evaluations for the proposed method on Table 1 are not over all for other methods. The authors should apply the CUTGAN, pGAN, MedGAN and Pix2pix methods on the different Paired percentages. But, the proposed method are interesting for the clinical applications.	"The main concern of this paper is their ablation studies is not enough, which is summarized as follows.
1) Regarding the attention map defined in Eq. (1), a small constant is set to 0.2 at x_i^T=0 lack of sufficient clear explanations e.g., some empirical validation results.
2) It seems that the authors do not show the performance of the plain DS^3-net by only preserving the GAN loss and the distillation loss in Eq.(4) and Eq.(7), i.e., LAGAN with distillation.
3) The hyperparameter is very intuitive in Eq.(4) and Eq. (7) where the authors do not shows how they motivated to be set.
4) It should be useful to show the gap of the model over previous baselines by also conducting their experiments with only 5% paired data in Table 1."
174	DSP-Net: Deeply-Supervised Pseudo-Siamese Network for Dynamic Angiographic Image Matching	This study mainly focused on searching the matched image of the X-ray fluoroscopic image from the angiographic image gallery, and lacks the evaluation of time efficiency.	"Incorrectly written order of references;
Excessively long sentence structure;
Lack of theoretical support for some of the descriptions;
The experimental test data accounts for less;"	"More comparison with other SOTA Siamense structure is recommended.
Registration methods literature review."
175	DSR: Direct Simultaneous Registration for Multiple 3D Images	"There seems to be very little practical use for this method.
For one, the improvement over other methods is marginal. Then, only the most basic portion of a product-grade registration algorithm is addressed, namely a straight-forward SSD cost function, without any overlap normalization, artifact considerations (portions of ultrasound volume will be occluded that are visible in others, which violates the SSD assumption of the intensity relationship); and most important some real-time motion handling, which provides for some of the most interesting research problems. Hence one could argue that the authors are solving a non-problem, despite the elegant mathematical formulation.
No implementation details, runtime, and other computational resource information whatsoever is provided. So I will have to suspect that in light of the highly parallelized GPU implementations that are state of the art today, this method will fare unfavorably, by require dealing with huge sparse matrices and throwing a BLAS library at it (which might be hard to parallelize)."	"Paper needs additional clinical motivation w.r.t 3D Transesophageal echocardiography registration
Few simulated cases and in-vivo cases"	Lack of illustrations for the method
176	Dual-Branch Squeeze-Fusion-Excitation Module for Cross-Modality Registration of Cardiac SPECT and CT	"1) Novelty is limited
2) Lack of evaluation of real data."	"The proposed approach simulates mis-registrations by manually introducing translations and rotations. It is unclear as to what mis-registrations were inherent to the dataset. How bad was the alignment between two images before adding translations and rotations? This leads to two cases:
*	Case - I: If there were mis-registrations before adding rotations or translations, why did the authors introduce them? 
*	Case - II: On the contrary, if the images were in reasonable spatial agreement, why did the authors simulate mis-registrations?  If the image were in good agreement, then mis-registering the images could possibly lead to a simpler task of learning rotations and translations? Moreover, if the analysis cohort does not exhibit naturally occurring mis-registrations, would it be useful to evaluate the model over a cohort that has some natural mis-registrations?"	"The detail illustrations of figures and tables are severely insufficient. i.e., the captions of figures and tables are too simple and need to be enriched.
The difference in the visualization results are not clearly to see. Please update the figures with zoom-in view highlight."
177	Dual-Distribution Discrepancy for Anomaly Detection in Chest X-Rays	"The word 'label' is not defined, maybe refer to the label of normal and abnormal? It can be confused because the label can also be defined as the lesion annotation label. Similarly, in the introduction, ""To the best of our knowledge, it is the first time that unlabeled images are utilized to improve the performance of anomaly detection"". As all the unsupervised methods use the unlabeled data, 'unlabeled images' may confuse.
In training, if model A trained with a pure normal image, will the input of the abnormal image be failed with both normal images generated? If model A trained with all abnormal images, will the normal image be failed to be detected as abnormal images with high inter-discrepancy as one abnormal and the normal image generated?
To compare with the SOTA methods, why the AS intra outperforms the AS inter with such large margin, but opposite scores observed for RSNA dataset?
By training both the normal and abnormal cases, will the robustness be affected by the limited abnormal data the network has seen?

Minor:

The fonts in Fig. 3 are too small to read."	"The full writing and abbreviations in the thesis should be consistent with each other. There are abbreviations in the front and try to use abbreviations in the back. The full writing and abbreviations should not be repeated.
How K was chosen, please explain.
There are some spelling mistakes, please note, such as fisrt time.
Regarding the selection of training data sets, when comparing with other optimal methods, use data sets that are common to other methods for comparison. Otherwise, the conclusion drawn is very likely to be overfitting. Please find the datasets used by the other best methods listed in the article, and do experiments to compare the results.
The code used in this paper needs to be open-sourced to demonstrate the reproducibility of the method, or to provide relevant proofs."	"The proposed setup is similar to noisy-label learning, it would be better if authors can explore some of the baselines from noisy label in this problem.
In practice, collecting a large number of normal images can still be time-consuming, it would be better if the authors can show the performance with fewer normal training images (e.g., hundreds instead of thousands)
In figure 2, the comparison is a bit unfair given that the proposed model contains much more learnable parameters (K AEs for module A and K AEs for module B) than the baseline autoencoder.
It would be better if the authors can compare with some recent SOTA anomaly detectors from 2021.
Some of the references are missing [1,2,3,4]

Reference:
Tian, Yu, et al. ""Constrained contrastive distribution learning for unsupervised anomaly detection and localisation in medical images."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021.
Dey, Raunak, and Yi Hong. ""Asc-net: Adversarial-based selective network for unsupervised anomaly segmentation."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021.
Marimont, Sergio Naval, and Giacomo Tarroni. ""Implicit field learning for unsupervised anomaly detection in medical images.""  International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021.
Chen, Yuanhong, et al. ""Deep one-class classification via interpolated gaussian descriptor."" AAAI 2022."
178	Dual-graph Learning Convolutional Networks for Interpretable Alzheimer's Disease Diagnosis	Fig.2 is small. The version of tensorflow and GPU should be provided.	"Correlation is the metric being used for construction of subject graph and the
feature graph.  Is it optimal metric as inherent normality assumption is always there in this metric? Moreover, how to justify this statement statistically ""correct correlations among the data are captured""?"	The corresponding critics is minor in nature. Given that the authors include a sparse learning approach in their comparison, they should consider including the approach by Adeli et al. The description of the data set is too terse - it should at least include sex. Given that the topic is interpretability, the authors should at least discuss how confounders, such as sex, could influence their findings.
179	Dual-HINet: Dual Hierarchical Integration Network of Multigraphs for Connectional Brain Template Learning	"The proposed method was only evaluated on brain images of single modality (structural), instead of evaluation on different types of brain connnectivity from multi-modality data, such as fMRI and sMRI.
The generated CBT was only evaluated in terms of representativeness and topological soundness. It is also important to evaluate if the generated CBT can better capture inter-subject differences and the associations between brain connectivity and phenotypes."	The organization of the paper needs improving, as there are many repetitive parts in the paper.	"It is unclear what is the 'hierarchical structure of neural interactions'. It is recommended to give a specific definition to avoid confusion.
The 'hierarchical structure' is obtained through the block C in Fig. 1. It is recommended to give more clinical/medical intuitive how and why this learned clustering-level hierarchical structure helps building a better CBT.
It is unclear how GCN works in the proposed method. Typically, GCN requires a fixed adjacent matrix to perform the graph convolution. However, the proposed method inputs multiple graphs and there are multiple GCNs modules in the proposed method. It is recommended to give more technical details about how each GCN works to make the proposed method more convincing.
There is a 'predefined number of clusters in the 'C) Hierarchical multigraph clustering'. It is recommended to give a detailed discussion about how this predefined hyperparameter impacts the model performance.
The proposed method is very complex, with multiple GCN and clustering modules. However, the training set is relatively small compared with the proposed method's complexity. It is recommended to give a more detailed discussion about how and why the proposed method can be trained on such a small dataset without overfitting."
180	DuDoCAF: Dual-Domain Cross-Attention Fusion with Recurrent Transformer for Fast Multi-contrast MR Imaging	"Experiments are conducted on real-valued single-coil MRI dataset (i.e. simulated dataset), which it is far from real scenarios where reconstruction algorithms deal with complexed-valued multi-coil MRI. Also, it is not clear how this proposed method can be extended to multi-coil MRI. This should be clarified.
The proposed method contains a so-called Swin Transformer Layers, but from my point of view, the key ideas of swin transformer (patch merging and W-MSA/SW-MSA) are not mentioned in the main text. If I misunderstand, patch merging and W-MSA/SW-MSA"	"1)The authors need to analyze the computational complexity of the proposed method in comparison with baseline methods.
2)The authors need to discuss the impact of the differences between MR images from healthy subjects and those from patients on the performance of the proposed method.
3)The authors need to provide in-depth discussion on the consistence of multi-contrast image synthesis obtained by the proposed method."	"In k-space CAF, feature maps are cropped into smaller patches and then embedded and input into RGT and TGR. Mathematically, each data point in k-space will contribute to every pixel in the image domain after Fourier transform. Please example the reason for cropping k-space.
It is very interesting to know the improvement of each recurrent block. Please consider replacing Fig, 3 to show results from each recurrent block.
In the ablation study, please provide more details on the network structure design for each experiment show in Table 2 as the statement of w/o CAF, w/o DD in the paper is confusing."
181	Dynamic Bank Learning for Semi-supervised Federated Image Diagnosis with Class Imbalance	"Some figures and equations are not well explained,like:

In Fig.1, it is not clear what is the \pi_1\cdots \pi_k. Additionally, it is better to simply  explain the overflow in the caption.
In Eq. 2, it is better to explain the function 1(\cdot)."	"The technical improvement is limited to the local training at the client side.
In the imbalanced case, accuracy is not suitable as a primary analytic metric.
The experimental results in Fig. 2 (d) are somewhat strange."	The labeled data on the server is a subset of the client-collected data, which can raise privacy concerns.
182	EchoCoTr: Estimation of the Left Ventricular Ejection Fraction from Spatiotemporal Echocardiography	"Strong reserve on the applicative impact of this work, given the current performance of LV segmentation including for echocardiography.
Limited methodological originality, although the authors perform extensive evaluation."	"Although it has a first use of the transformers to calculate the ejection fraction, is  looks the method still  does not possess such convincing results when compared to the other methods.
And the theoretical part is not so strong to demonstrate the reasons of the different experiments."	"The novelty of the work is limited. The authors took the existing UniFormer [1] architecture and adapted it to their task. It is unclear how efficient this approach is compared to the other state of the art.
[1] Li, K., Wang, Y., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatiotemporal representation learning (2022)"
183	EchoGNN: Explainable Ejection Fraction Estimation with Graph Neural Networks	"Some of the limitations of the method could be explicitly summarised at the end. There is half a page available.
Some may argue that the improvement overall is incremental, but I agree with the authors that the lower complexity of the problem described is important for clinical translation.
Clinical aspects could be further analysed. The authors have a wealth of data available, many more experiments are possible. Please note that echo is not the only modality where EF is assessed. This is also routinely done with CMRI, and other imaging techniques. Might add a comment.
What are the clinical implications of having an unbalanced training set? Could you tease this out in the Discussion? How could you 'correct' this in future?"	"While the model does offer some explainability, its not clear how impactful it can be to clinicians. It is unlikely that clinicians will take the time to look at adjacency matrices, frame weight curves and make assessment. It may be a good idea to use these more complex information to synthesize a single number, or metric to be used a proxy for quality and/or confidence. Because in terms of clinical workflow, its much more convenient to report a number or two and keep track of those numbers, as opposed to having qualitative description of quality, etc.

While the possibility of using multiple echo videos is interesting, it may not be a good idea clinically. Reason being, sometimes videos are acquired at slightly different angles, imaging parameters, etc. Your model may have to learn, not only to be responsive to these variations, but also the interplay between different variations when multiple videos are used together. The frame sampling may also not work too well when there are too many frames from too many videos."	"Generally, there are no major limitations. 
The paper would, however, benefit from a more critical discussion on results that were presented in the supplementary material. 
A few comments are detailed in (8) but these can be addressed during the rebuttal period."
184	Edge-oriented Point-cloud Transformer for 3D Intracranial Aneurysm Segmentation	"Overall of presentation of the proposed method is great. I have a minor question about the paper.
The authors provide three edge-oriented processes to incorporate boundary information of the target. Although the overall results and some ablation studies are provided in the experiments. The details of effectiveness of each component is not given in the paper. It would be better to give more analysis about the presented components."	"The clinical significance of this work is doubtful. If aneurysm segmentation is critical for clinical decision making, it is better to do the segmentation carefully by the doctors and it is not difficult to segment an aneurysm manually.
The proposed model is rather complex while the dataset used for evaluation is rather small, in which there are only 116 aneurysm.
The proposed method is based on point cloud, while generating the point clouds from original images may introduce some errors, which further increase the uncertainty of this method in clinical practice.
Ablation study is not complete, since the results of applying any two of the three components are desirable."	"Qualitative comparisons with SOTAs on the segmentation near edge boundaries are necessary but missing in the paper. The paper claims that they proposed a three-stage paradigm to focus on distinguishing the edge boundary segmentation (which thus improves the overall segmentation performance), however, this claim is unjustified in the manuscript. It would be insightful if we could see these improvements near edge boundaries qualitatively.

The ablation discussion looks lack of insight. However, considering the page limitation, I may find it personally acceptable if the aforementioned additional qualitative comparisons can be added to reveal the insights.

The authors achieved better results, however, it's probably due to their usage of edge labels, which are unavailable for other SOTAs. Given a standard setting (where edge labels are unavailable to model training), it's unsure whether the proposed method can still outperform others by a considerable margin. It's likely their overall proposed pipeline might not even work when these edge labels are unavailable, as the proposed modules look rely heavily on the edge annotations."
185	Effective Opportunistic Esophageal Cancer Screening using Noncontrast CT Imaging	The authors seem to not describe clearly the main methods including how to train the segmentation model and what is the loss function,  the operation details of the position-sensitive full-attention layer which would be the main distribution in this study, such as, whose position is the position o=(i,j,k), possible position p et al, as well the classification steps. These will influence the readability and reproducibility of the proposed model.	"The statistics are lacking in this paper: a) Whenever a value for a performance metric is reported (such as AUC, sensitivity, specificity) it should be accompanied by an error estimate (preferably a 95% confidence interval), b) No statistical tests were performed to support claims of superior performance. Just because one number appears to be higher than another, does not mean that this improvement is statistically significant.
In spite of the large dataset (and relatively large hold-out test set which is presumably completely independent from training/validation and only used once as a test set), there are only 80 normals within the test set. The authors note that the prevalences are different from screening, which is the intended applicationI (it is normal practice to do this to increase statistical power), but the authors don't focus much on the performance for these normal cases. Of course you want to detect all cancers but the cost of false-positives is important, especially when extrapolating to a screening setting. Without statistical proof or error estimate, the baseline nnUNet (Figure 3) seems to have no false-positives for normal scans while the proposed method finds one more cancer at the cost of 2 false-positives. Is this benefit worth the cost (assuming statistical proof can be made)?
The 'reader study' is not really a reader study but a comparison (without supporting statistics) of performance of the deep learning to that of physicians. What is of more interest is how the physicians perform with and without aid.
The model appears to be an incremental change from the baseline model (ref 8)
Because of the class imbalance of the test dataset 80:20:80, the authors should consider adjunct weighted versions of 'accuracy' as performance metric since 'accuracy' is influenced by class prevalence."	"The main weaknesses:
(1) The overall novelty and innovation of the work were limited. The self-attention module was not new tech, and nnUNet was also the classic segmentation method.
(2) The experimental demonstration and discussion of the research work were sufficient and complete, but the research methods and ideas were relatively simple."
186	Efficient Bayesian Uncertainty Estimation for nnU-Net	"""Efficient"" in the title is not well supported by the results.
Only 3D data is evaluated. 2D data segmentation should be included."	I do not see any particular weakness in this paper.	"Too limited novelty. This method proposed in this paper is essentially similar to ""Snapshot ensembles"" (ICLR 2017), cited as [13] in the paper. However, this paper does not discuss it in related work and only mentions [13] in the cyclical learning rate setting (section 2.3). To me, the single-modal posterior sampling is similar to NoCycle Snapshot Ensemble, and the multi-modal posterior sampling is similar to Snapshot Ensemble. 
In my reviewing process, I was waiting for the EXPLICIT discussion and comparison of the difference between the proposed method and Snapshot ensembles but got rather disappointed. The differences are small, e.g., the use case  (medical images), and the cyclical schedule (from cosine lr to a proposed one). Besides, there is no ablation experiment on the proposed cyclical schedule.

If the authors would clarify the difference between the proposed method and Snapshot ensembles, I would consider adjusting my rating.
Minor issue:
The name ""multi-modal"" can be confusing in the medical domain.
Snapshot ensembles: Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J.E., Weinberger, K.Q.: Snapshot ensembles: Train 1, get m for free. ICLR 2017."
187	Efficient Biomedical Instance Segmentation via Knowledge Distillation	Contribution is somewhat limited. Applying knowledge distillation on the affinity maps is not very novel. Please refer to 'Adaptive Affinity Fields for Semantic Segmentation' .	nothing significant	"The complexity of the distillation method was not discussed, especially, instance graph distillation. Since constructing graph can be costly and time consuming, it would be better to include such discussion in the paper.
Some details are missing from the paper, for example, L_{PAD} in Eq. 4 is not defined anywhere in the paper and other reproducibility mentioned in the section 7."
188	Efficient population based hyperparameter scheduling for medical image segmentation	"authors have not convincingly explained why results are worse than the default setting, when the number of workers were small (W-9)
the authors are making an assumption that a good set of hyperparameters exists for a given model/problem which may not be the case. If this default set of hyperparameters used is not good enough, I am not sur eif this method would work well.  If authors can show that this is not the case, that will make the method more convincing. For example, given a task and a model, if the authors were to use a random set of hyperparameters to initially train the model, and apply their hyper parameter optimization technique, will the method still yield a good enough solution compared to when a good set of initial hyper parameters were used to train the model the first time  ?
As such the method seems to be useful to find incremental improvements to a model, rather than help find a set of optimal hyperparameters as it appears to be a more localized search than a global search in the hyperparameter space."	"the main novelty of this method is to start PBT from default hyperparameters, which already achieve good performance. This restricts the use of the method to settings where  'good' hyperparameters are already known and can be used as a starting point.
The authors acknowledge that the performance improvement using their method can be quite small, it might be mainly useful to tune hyperparameters to compete in challenges
a performance comparison of the proposed method, original PBT and default training is missing."	The main weakness of the paper is the limited validation and novelty. Although it is a good idea to incorporate PBT into medical imaging using prior knowledge and can obtain consistent improvements, the main value of the method is quite practical and lacks methodological contribution. That makes the current version of the paper less interesting. However, this can be a good paper if more solid validation is provided, e.g. improves nnUnet in more medical imaging challenges.
189	Electron Microscope Image Registration using Laplacian Sharpening Transformer U-Net	"synthetic deformations are suboptimal, as these only test the robustness and precision, but not the accuracy
unclear why (simulated by motion) adjacent slices in EM require registration - what is the application/question?
EM is suffering from intrinsic motion artefacts (line shifts) which are not dealt with here
unclear why this methods is ""specifically designed for ME registration"" - it appears to me to be quite generic in set-up for other 2D registration tasks?"	"The novelty of the propose method is questionable: The authors choose a CNN architecture already published in the literature to show it's applicability in a different microscopy image processing task. Additionally, it is not clear from the paper whether their method works in 3D or in 2D. In the original Swin U-Net, they used 2D slices and propose to work on 3D as a future work. An important contribution would be for example, upgrading this method to 3D, which is not clear whether the authors did it already.
The authors speak about catching the global information of TEM images to improve the registration. Indeed they claim that this work is specifically designed for serial-section. Although they are using transformers, I think that if they are not analysing the global 3D information, such statement can be confusing. The main reason is because in connectomics, when speaking about serial-sections or global information, it is related to the 3D information contained along all the EM slices, and not only the one that is observed in pairs of slices.
If the authors are indeed analysing 3D information, please, make a clear statement in the text and indicate the number of slices entering the training batch."	"The paper proposes a methodologically simple idea of combining related work.

No standard deviations nor significance tests are reported for the results.

Use of both correlation coefficient and structural similarity is not motivated well. No ablation study with regards to this choice of loss is included."
190	Embedding Gradient-based Optimization in Image Registration Networks	"The reviewer does not understand exactly the way the registration problem is solved and why the lower optimisation problem is called ""the forward pass"" by the authors. A pseudo algorithm of the proposed methodology would have clarify it.

According to the reviewer, it is necessary to compare the proposed formulation with other network than Voxe"	"-The description of the method is not sufficiently clear. Is this a deep-learning registration, which is used to generate an initial solution for a conventional registration? (Meaning DL-Reg + Instance Optimisation like e.g. Mok et al. did for task 2 of the learn2Reg challenge)
It seems to me that this is because the network parameters are not changed in the second optimisation (1b)
This would mean that the optimisation from 1b does not have to be carried out during the training of the network, as these have no influence on the network parameters. 
However, the title as well as other wording in the text suggests that this is an interrelated optimisation/network. Why else should the image dissimilarity in the section ""Image Dissimilarity Gradient"" be differentiable twice?

For the evaluation, automatic segmentations are used on both data sets. It is not clear how good these are and accordingly how great the influence of errors in the automatic segmentation is on the evaluation of the registration.
For the 3D dataset, GraDIRN is with a Dice of 0.799 significant better then RC-VM with a Dice of 0.794. Is that really significant better taking into account the segmentation error? Even without this, the difference seems very small to me for significantly better results.

It is not clear to me, why the authors have chosen to evaluate their method on this datasets. They clearly propose a new method and therefore, it would be appropriate to evaluate the method on a publicly accessible data set with manual annotations. (e.g. the Learn2Reg datasets)

the discussion and contribution section does not contain any discussion. Please discuss your results!"	The formulation of the approach in a bi-level optimization view is not conclusive and probably rather misleading, since the optimization problem formulated in this way is not solved. The role and the design of the regularizer in the lower-level step is not clear (\nabla R_{theta_t}). Which form of regularization is used here should be made clearer. The motivation why the structure of the procedure described in Figure 1 fits the formulation of the optimization problem is not obvious. In the experiments part, the newly presented method is compared with other already published methods. Unfortunately, it remains unclear why the methods used in the paper were chosen. Here, a short justification of this choice would be nice. In the experiments it is motivated by Tab. 2 that the explicit gradient is necessary for the success of the method, here a more detailed consideration would be useful.
191	Embedding Human Brain Function via Transformer	The authors stated that the embeding  vectors represent regularity and variability of human brain function, maybe they should show the variability part for different  brains and time points.	"One major drawback of the submission is the experimental design. If I remember correctly, HCP contains 7 tasks, but only the results of two are given in the paper. Regarding to measure the prediction performance, more statistical metrics are expected to use, but I only see accuracy and pcc. Besides VAE-based model, I see no other comparison methods. Such an experimental design is not convincing to me.
From the perspective of methodology, the proposed method did not well address the issue they mentioned in Introduction. How does the proposed method encode the regularity and variability of different brains is not clearly answered."	The evaluation is limited to a single dataset/task, which may however be sufficient for a conference paper.
192	End-to-End cell recognition by point annotation	"1 the novelty of this paper is quite limited. For pyramidal feature aggregation, please refer to ""Feature Pyramid Networks for Object Detection"" and  ""Richer Convolutional Features for Edge Detection"" .
2 Inadequate experimentation. Experiments were performed on only one dataset. The generalizability of the method cannot be well assessed. The division of the test set will have a certain impact on the experimental results.The paper does not conduct multiple experiments for statistical description.
3 The formulas are confusing."	Some notation is confusing.	"The motivation for the proposed loss functions needs to be highlighted.
The selection of hyper-parameters  is unclear , such as lambda.
Some typos make this paper hard to follow and less convincing."
193	End-to-End Evidential-Efficient Net for Radiomics Analysis of Brain MRI to Predict Oncogene Expression and Overall Survival	Few weaknesses are noted: (1) only a single dataset was used to train and test the model, making the generalizability of the model uncertain, and (2) it is not clear why the authors chose to discretize the overall survival prediction into three bins rather than perform a survival analysis (e.g., using a Kaplan Meier curve).	"(1) The author claims that the motivation for introducing evidential deep learning is to improve the prediction accuracy, which is untenable; Evidential deep learning is usually employed to address the ""know unknown"" flaws, rather than to improve the prediction performance of traditional models;
(2) The description logic of the paper is confusing. MGMT methyl status prediction is a classification problem, while OS prediction is a regression problem, which should be solved by using evidence classification and evidence regression, respectively. However, the authors incorrectly used evidence regression to address the MGMT methyl status prediction problem;
(3) Figure 2 illustrates the architecture of the proposed model, which obviously confuses the Dirichlet distribution and the evidence regression."	More details and descriptions are necessary to avoid the confusion by the readers regarding the data preparation, preprocessing and training. More importantly, the main contribution of the model is to perform the classification without needing for the segmentation labels. It is very important to verify if the model really learn from the target tumoral region to conduct the final classification.
194	End-to-end Learning for Image-based Detection of Molecular Alterations in Digital Pathology	What if the task is related to only a very small area of the original image? Would this approach still work compared to e.g. a manual approach that ROIs the area that is relevant? E.g. can k be learned based on the complexity of the task?	"No comparisons with MIL and other end-to-end pipelines (cited in the intro). Even though these are technically complex, comparisons with these methods are needed to justify the proposed pipeline. Does these methods give comparable accuracy or better? Recent papers such as ""Benchmarking artificial intelligence methods
for end-to-end computational pathology"" (https://www.biorxiv.org/content/10.1101/2021.08.09.455633v1.full.pdf) provides deep insights into the problem with extensive comparisons against end-to-end deep learning models (including MIL and Vision Transformers along with the classic [12] paper that was compared in this manuscript) for tumor subtyping as well as predicting molecular alterations on publicly available datasets (which have been preprocessed in a consistent way). This paper comes with a well put together github that makes it easy to run these end-to-end pipelines: https://github.com/KatherLab/HIA."	Even though the authors show that making a decision for k tiles by a k-Siamese CNN is better than combining k decisions by a regular CNN, however, they failed to show that an end-to-end approach (where segmentation is not done) is better than the two-stage approach (where segmentation is done before the final classification). Because the Seg-Siam approach is a two-stage approach and the AUC of the Seg-Siam approach is slightly higher than the k-Siam approach, an end-to-end approach. It is possible that using a k-Siamese CNN over a regular CNN is the reason for getting higher AUC for the k-Siam approach than for the Two-Stage approach.
195	End-to-end Multi-Slice-to-Volume Concurrent Registration and Multimodal Generation	This is more of a system paper, describing a well working overall system; as expected, the inherent mathematical novelty in the method is limited - however many powerful techniques are combined in a smart way and thoroughly evaluated.	"Unclear usefulness in the real multi-slice to volume registration
References not related to the problem being solved
Results are not reproducible"	"Lack of discussion of the limitations of the proposed method.
The influence of the slice ratio on the performance of the method was not evaluated.
The influence of the coverage of the field of view by the 3D CT and 2D SCT slices is not discussed.
The main innovation is the combination of state-of-the-art building blocks in one method."
196	End-to-End Segmentation of Medical Images via Patch-wise Polygons Prediction	"*	The paper needs fundamental improvements in terms of structure and writing (see detailed and constructive comments). The current form makes it very difficult to understand the rationale on why the proposed approach should provide better segmentation performance when compared to classical approaches.  In particular, the authors states that a second type of output representation is provided, which seems not correct the final output is based on a rastered image, the latter being similar to classical approaches. Therefore, it is remains unclear why a classical approach could not learn an equivalent mapping.
The related work section contains a enumeration of segmentation approaches and lacks conclusions on what is lacking in the literature and how the proposed approach could solve current issues.
Fig. 1 uses many concepts that are introduced much later in the paper, making it difficult to understand.
*	The performance reported in Section 4 is impressive, but it is not clear how internal parameters (in particular s and k) were optimized for the various datasets. Only one parameter sensitivity analysis is reported in Fig. 6 of the appendix, suggesting high performance variation (well above inter-algorithm variations in Table 4) and overfit. The mIoU reported in Table 4 correspond to cherry picking the top performance among all parameters tested in Fig 6 of the appendix (i.e. 90.92 for k=5 and patch size=2^3)."	"1.According to the training script in the code,the proposed method is a single-class segmentation method. There is no such claim in the paper, which will make readers confused. 
2.There is no figure to present the CNN model structure. Although it is not a hard requirement in the paper, it would largely help the reader understand the design of the techniques.
3.The references are limited."	The description of the network architecture is a bit hard to follow. It is better to have an image illustrating the overall structure and flow of the network.
197	Enforcing connectivity of 3D linear structures using their 2D projections	"I missed information on the datasets, especially the number of images in each set and train/test splits.
Parameters were selected based on their performance on one of the test sets, whereas for previous approaches Perc and PHomo parameters as suggested in the papers were used. While there was no extensive tuning for the proposed method and the same parameters were successfully used in multiple datasets, this makes the improvements with respect to the previous topology-aware approaches less convincing."	reproducibility (see below for details)	"The authors mentioned that occlusion is an example of this formulation not guaranteeing connectivity preservation. However, the dataset used in this example has occlusion very often. This is a huge contradiction. How would the authors still argue in favor of their approach?
The technical contribution of this paper is heavily sacrificed by the plug-and-play application of an existing loss [23] in 3 canonical projections of 3D space.
The literature review is incomplete. The authors failed to cite the following papers on topology-aware loss function (Byrne et al. STACOM 2020, Shit et al. CVPR 2021), including methods that work directly on 3D data.
While APLS and TLTS were proposed for 2D scenarios, e.g., road networks, I am not convinced that it is still a good topology metric from 2D projection because of the abundance of occlusion. On the other hand, Betti error would be a better alternative to topological metrics."
198	Enhancing model generalization for substantia nigra segmentation using a test-time normalization-based method	"The contribution is merely incremental. 3D U-Net, sequential networks, and time-test normalization are well known techniques.
There is a lack of description of the in-house dataset. How many experts labeled the in-house dataset?
Experimental section must be expanded. Average Dice Index is not enough to assess the performance of the proposal. Specially when the difference is < 2% (Table 1)."	N/A	"Some of the areas of the paper are unclear and need to be clarified. For eg. post-processing re-threshold to maximize H^, How is N=10 for support set chosen? Instead can this be set based on some pre-defined Dice threshold? In the Qualitative evaluation, authors claim TTN helps the model to identify SN regions better without showing the GT. To this end, labeled sample could have chosen. Further, it is unclear from Fig 3 that ""the estimated uncertainty maps indicated larger oscillations in the boundaries of SN"". I see these oscillations throughout the segmented regions. Also, a color bar could be useful here."
199	Ensembled Prediction of Rheumatic Heart Disease from Ungated Doppler Echocardiography Acquired in Low-Resource Settings	"Some details of the training/validation procedure were unclear.
Details of statistical testing were not presented clearly."	The deep learning strategy is an integration of existing methods.	"Organization:
In Section 3.1, the authors use too much space on implementation details of preprocessing networks, which causes not enough room for the experiment section to include comparisons with other methods.
Lack of technical novelty
a.	All models in this paper are from existing methods. Both view classification and frame selection use ResNet, and ROI localization use VGG based LinkNet [Ref. 26]. The first prediction models are a 3DCNN model and a DenseNet+transformer model, which are widely used in the community. The straightforward maximum voting is used to aggregate two RHD scores by prediction models.
b.	The whole framework is complicated. The pipeline is long, resulting in minor errors at the beginning of the pipeline and would become large at the end with propagation.
Unsound evaluation
a.	The authors only show their experimental results with their proposed method. There is no comparison with other methods, for example, other echocardiograms classification methods, ultrasound classification methods, or even video action recognition methods for natural videos. 
b.	The authors only show their method with the data after processing. Showing the experiments on the data with less preprocessing (view selection, view classification+frame selection, and view selection+ROI localization) would be helpful for readers to understand how challenging the original task is. These can be a part of the ablation studies."
200	Estimating Model Performance under Domain Shifts with Class-Specific Confidence Scores	"The method described is not innovative. Probability calibration is a well-studied field. There are many well established methods that needs to be compared to (read On Calibration of Modern Neural Networks by Guo et al.). In addition, class-specific calibration has been discovered and widely adopted to address issues caused by training data imbalance (read Improving class probability estimates for imbalanced data by Wallace et al.).
This is more than a domain adaptation problem but a general ML problem. To tie it to DA is restricting.
Result. The Authors should also compare to unseen test set performance within the same domain to evaluate how calibration works without domain shift.
Result is unreliable. Fig 3 shows that the predicted accuracy values does not distribute well from 0 to 1, making the linear fitting unreliable.
The manuscript also does not explain well how probability calibration is an important topic for medical imaging problems."	"The main weakness of this paper is that it states class-wise calibration, especially with temperature scaling, as own contribution without acknowledging prior work. E.g., Guo et al. (2017) already suggested vector scaling as class-specific extension to temperature scaling. Kull et al. (2019) and Nixon et al. (2919) further discusses these topics in the context of calibration error metrics. The actual contribution of this paper seems to be the use of class-wise calibration in the context of performance estimation. This should clearly be stated as such.

Given my first point, the actual novelty of this paper seems quite low. Calibrated performance estimation has already been intensively investigated by Guillory et al. (2021) and class-wise calibration is also not novel. Luckily, the authors clearly show the benefit of the combination of the two approaches (see strengths).

I expect Tab. 1 to at least include MAE +/- std dev - or even better, show box plots instead. Instead of using bold font to highlight the best MAE, proper statistical tests to show statistically significant improvements are much appreciated.

Kull, M., Perello Nieto, M., Kangsepp, M., Silva Filho, T., Song, H., & Flach, P. (2019). Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration. Advances in neural information processing systems, 32.
Nixon, J., Dusenberry, M. W., Zhang, L., Jerfel, G., & Tran, D. (2019, June). Measuring Calibration in Deep Learning. In CVPR Workshops (Vol. 2, No. 7)."	"The authors state that their method differs from class-distribution-aware TS for long-tail problems and list a few reasons. Yet I have trouble grasping how significant these differences are. It would have been interesting to get a comparison on long-tailed problems of the authors' method with the other existing methods.

I find it unusual how the background class is handled for segmentation task. Similar methods such as [29] also require a separate handling for the background compared to other classes. It might be interesting to verify how relevant the Dice for the background is for the problem at had, especially considering that the background is most often the majority class in segmentation problems and that this Dice is rarely reported."
201	Evidence fusion with contextual discounting for multi-modality medical image segmentation	the method lacks substantial algorithmic novelty, and has weak evaluation	"The limitation of the study is not discussed.
Discussion on qualitative results on the scans with less dice score will be helpful.
The results shown in Table 1 does not show significant improvement.
Memory footprint is not discussed, the complexity of the network is increased in this method while not giving significant improvement (around 1%)."	The descriptions about evidence discounting mechanism is very vague. The writing is confused for readers. A large number of errors exist.
202	Evolutionary Multi-objective Architecture Search Framework: Application to COVID-19 3D CT Classification	"Details in preliminary is controversial. The author mentions the advantages of mini batch but chooses size 1 as the best size. These two opinions are opposite.
In section 3.1, the author mentions that E is a vector recording the epochs, then it should be a list of numbers of epoch count. How can the transpose of such a vector multiply itself in formula 3? What is the meaning of (E^TE)^(-1) and E^T F? The number of specific item influences the results if formula 3 is right.
Results of experiments should be put more clearly. For example, figure2 is very hard for comprehend a conclusion."	"The evaluation of this approach is quite narrowly focused on COVID chest CT classification. The architecture search algorithm itself seems more general and it would have been interesting to see the performance on several other classification tasks, including for example 2D chest x-ray classification.
Furthermore, the baseline comparisons are based on methods proposed for the same sets of data (also both by the same group of authors). It could have been interesting to see how the proposed method compares other popular neural architecture search techniques, e.g. DARTS."	"Search 3D neural architecture is not new[1], and it seems not hard to transfer such method to 3D CT data. The authors could highlight the difference caused by the domain knowledge. More important, the Regression Methods[2], like ordinary least squares (OLS), random forests (RF), Bayesian linear regression (BLR) are commonly used in NAS for performance evaluation and NAS acceleration. From this perspective, it seems the novelty of this paper is limited. The authors should compare the proposed potential objective to such methods to prove its advantages.
[1] Video Action Recognition Via Neural Architecture Searching, 2019.
[2] Accelerating Neural Architecture Search using Performance Prediction, 2018."
203	Explainable Contrastive Multiview Graph Representation of Brain, Mind, and Behavior	"Too much details are missing for the whole model. e.g.  rare hyper parameters were provided; how edge of graph was defined? no time-window used for dynamic FC? why distillation is necessary? 
And for sex classification result, it's better to compare to other published methods, since a lot methods have been proposed for this problem."	However, I still have a few major concerns before possible publishment. (1) In the introduction, the authors mention several reasons why the previous method is not applicable, and in the article authors should emphasize how the method proposed in this paper solves these problems and why it has advantages over the previous methods. (2) Authors should add the parameters of the methods. It would be better to add some necessary arguments for Equations to make them easier to understand. The overall schematic illustration needs to be clear and easy to understand and highlight the innovative points of the model. (3) The first experiment used the causal explanation model to obtain the regions that play an important role in the classification, and the authors should further show and analyze them. (4) In the second experiment, three different FC data were used to assess whether the analytical approach of the role of SC is the innovative approach of this paper. Please add some details of this experimental approach or relevant literature.	there are not enough validation tasks for the proposed framework, e.g. to use external dataset or different combinations of imaging modality.
204	Explaining Chest X-ray Pathologies in Natural Language	"There is a strong focus on explanations for positive findings, however radiographic exams are often used to rule out hypothesis.  Natural language explanations for negative findings would be relevant as well.
Some aspects of the work are simply asserted, without proof or reference.
E.g. ""we observe that [...]  a small selection of phrases, ... are very accurate identifiers"" How accurate?  How did the authors avoid confirmation bias here?
E.g. The evidence graph was constructed using prior radiologist knowledge and by empirically validating the coocurrences.  How many radiologists?  Authors or non-authors, and could the empirical validation be provided?
The example discussed of how ""a model that generates generic NLEs that make reference to Lung Opacity"" will yield a good score"" may indicate a more serious problem than the authors suggest.
The baseline results provided are not entirely convincing."	"- Contribution is limited. Presented a new data collected from already public dataset.
- I could not find any explaining factor in the paper. Traditionally, explaining means that authors will explain how their results/method they used in not a black box but an explainable. (Title of a paper is misleading)
- Previous methods are used to set the baselines results. Authors claimed they proposed a new method. 
- all baselines are from previous studies including DPT is inspired from previous SOTA method. DPT leverages a DenseNet and GPT-2.
- This paper lacks a discussion on the experimental results and motivation analysis.
- - The results lacked visualization or statistical analysis."	"It is difficult to evaluate NLEs with automatic NLG metrics.

The GT NLEs obtain an absolute rating score of 3.2/5 is quite low.

It's not clear to me that an absolute rating score of 3.2/5 can be explained by a generic inter-annotator disagreement between the clinician and the author of the reports or it comes from another source. The author can verify this assumption via an additional evaluation.

The clinical evaluation should be performed by a consensus of clinicians."
205	Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation	"The main concern is that the proposed two methods are both existing semi-supervised solutions. The authors adopt adversarial noises from VAT [13] with a small modification of discrepancy measurement and the inter-class separation strategy is the same as [1]. Most of the equations are very similar to the original papers. The authors should clearly explain the novelty of their method compared to [13] and [1].
The performance gains are limited."	"Since the paper is highly related to two directions, i.e., strong perturbation and class prototype learning, related work should be more comprehensive, with more relevant methods regarding to their insights. For strong perturbations, authors can discuss more relevant papers about it besides the VAT they borrowed. For example,
French, Geoff, et al. ""Semi-supervised semantic segmentation needs strong, varied perturbations."" BMVC (2020).
Also, for the class prototype-based methods to push feature separation and compactness, authors can also discuss relevant papers, including but not limited to:
Xu, Z., et al, All-Around Real Label Supervision: Cyclic Prototype Consistency Learning for Semi-supervised Medical Image Segmentation. IEEE Journal of Biomedical and Health Informatics (2022).
The method introduces some other parameters compared with previous methods, like in their adversarial perturbation and the tradeoff. It will be good to see some sensitivity analysis to help readers better understand the method and tune for their applications.
The supervised results are very low with your lowest portion of labeled data. It is sometimes due to the batchnorm module will have negative effect trained with very limited data. Like, if you train MT model but turning off the L(unlabeled), it can also serve as the supervised method, and the results will be normal. I suggest using InstanceNorm here to avoid this problem."	"The contributions are a little bit over-claimed.
The technical novelty of this work is limited.
Detailed comments please see below."
206	Extended Electrophysiological Source Imaging with Spatial Graph Filters	There is too much supplementary material, better write a journal paper to be more comfortable, this would allow you to give more details in the method, or limit your paper to synthetic data analysis. As it is, it is frustrating to have only partial information.	The main weakness of the paper is the lack of information about the tuning of the parameters and their optimal selection. In methods were multiple regularizers are employed a critical point is the difficulty of the selection of the regularizers parameters (a,b,g etc). Especially when comparison among such methods is made it needs to be sure that all the parameters are optimized (for fairness).	"Here, there are some comments of this reviewer:
1 In the introduction section, the literature review must be strengthened. Avoid lumping references as in [2,21,24,32], [30, 22, 5, 4, 1] and all others. Instead summarize the main contribution of each referenced paper in a separate sentence
2 In this work, how to guarantee of the convergence of the ADMM used in the final algorithm?
3 The proposed method might be sensitive to the values of its main controlling parameters. How did you determine these parameters? Please elaborate on that.
4 In practical applications, noises may be non-Gaussian noises. Have you considered such non-Gaussian noises? Please discuss how this would impact the results and conclusions of this study."
207	FairPrune: Achieving Fairness Through Pruning for Dermatological Disease Diagnosis	The proposal is confined to binary senstive attributes. Future work should extend to categorical (race) and continuous variables (age).	"The attribute examined in the second dataset as sensitive data is the gender. First, it should be mentioned whether there is a clinical difference among genders regarding skin conditions. Moreover, the difference between the two groups even for the vanilla method was marginal, thus it is critical to justify the clinical need for fairness in this case.
The groups are named 'privileged' and 'unprivileged'. I would replace these terms since they are not descriptive of the particular situation. 'Privileged' is a group that achieves higher performance on the vanilla model, however, that collides with the common use of the words privileged or unprivileged that are used for majority and minority groups that enjoy different rights and face different societal issues.
It is crucial to mention how many samples were included in each group of 'light' vs 'dark' skin and 'male' vs 'female' and whether there was class imbalance. Why is one group achieving higher performance over the other? Could it be attributed to more training samples originating from that group?
It would be interesting to show that the proposed pruning approach works for more complex architectures like ResNet-50 and is not only performing well on simpler architectures like VGG-11.
The experiments were not repeated or cross-validated, thus no standard deviation was reported."	The literature review on fairness in medical imaging applications is slightly limited
208	Fast Automatic Liver Tumor Radiofrequency Ablation Planning via Learned Physics Model	"Ground truth taken from simulation rather than real data.
And to that point, it is unclear how the threshold set for generating the ground truth was selected.
Validation dataset is small on only 10 patients
Simulation parameters taken in general rather than from a distribution from possible patient-specific values"	"(1) The method part was not clear (some important information was not provided) and it is not easy for readers to follow this work.
(2) The results of the proposed method mainly depend on the reference solutions. However, there were two problems on the reference solutions: (1) formula 1 was not consistent with the formula in ref [2] and seems not right. (2) the simualtion resolution is 4mm, which seems too big for accurate simulaiton. The authors should clarify the two issues."	The authors need to further clarify which portions of their work is their main contribution. For example in the introduction they state that the work was inspired by DeepONet. How much of this work is novel compared to prior work? This work was compared against one reference ([15]). But it is not clear from only that comparison where the current work stands in the literature.
209	Fast FF-to-FFPE Whole Slide Image Translation via Laplacian Pyramid and Contrastive Learning	"*The paper focuses on increase in computational performance, and it does not devote any time to analyze the network. It would be interesting to see what the intermediate results of the Generator look like , for example the masks, etc
*It was not clear in the MSI task prediction if the gain comes from the combination of FF->FFPE + data augmentation or only from data augmentation."	"Using the unpaired dataset for training is totally making sense. However, for evaluation, since the motivation of this work is to transfer FF to FFPE, I strongly recommend the data be paired or at least ask a domain expert to grade the synthesis result, i.e., if the synthesized image is useful in clinical.
The result shown in Table 1 does not convince me to use the fastFF2FFPE  (vs. AI-FFPE) even though the training time, memory usage, and inference throughput are better than the other two baselines because of the FID performance. Authors might need a sensitivity analysis to show results with different hyper-parameters."	"Whilst the paper has many merits, there are some weaknesses.

since the methodology is a naturally off-line process in which the FF is post-processed, the speed of processing is less important than the quality. After all, it is possible to process the tissue with FFPE and get good images anyway. Thus the focus should have been the quality.
The quality obtained does not seem to be that good!  Best case on the GBM data was 46.85 but that is not comparable with other methodologies. At the same resolution the results of the proposed method are 49.67 against 46.89. For the LUSC set, the best of the proposed is 43.64 whilst the alternative was 34.81. 
*The paper is fairly well written but at times is confusing, more details to follow."
210	Fast Spherical Mapping of Cortical Surface Meshes using Deep Unsupervised Learning	See the comments below.	Lack of experiment with ground truth.	The only weakness of this work is that Spherical U-Net has been previously proposed. This paper utilized it for spherical mapping.
211	Fast Unsupervised Brain Anomaly Detection and Segmentation with Diffusion Models	"My main concern is the effectiveness of the proposed method. Basically, this architecture is based on latent DDPM [16, 19], and the unsupervised strategy is based on an observation. At least there should be some theoretical supports or visualizations for this observation.
The performance of the unsupervised segmentation is worse than previous methods."	"Lacks statistical analysis to demonstrate whether the improvements are significant. Also, it would be better if the authors can report the standard deviation of the Dice scores.
To better understand the upper bound, it would be valuable if the authors could report the segmentation accuracy of supervised methods (maybe U-Net) in each experiment."	"While the paper is in general well-written (especially the detailed introduction), the writing starts to fall apart when it comes to the methodology. The abuse of notation only obfuscates the point the authors are trying to make. For example: why say that training samples follow the distribution of the training dataset with notation x0~q(x0)? This fact never comes again and seems superfluous. The same goes for most of the formulation for VQ-VAE and DDPM. There is a contrast between the way each concept and definition is presented mathematically in the original papers and the rough summary presented here for the same ideas. In fact, most equations are mostly taken as is with slight modifications (and some typos). A short summary of the most relevant ideas and a link to the original papers for the interested reader would have made the methodology clearer and easier to read.
Why is there a need to combine two different synthesis methods for anomaly detection. Why is VQ-VAE not enough? Why are diffusion models applied on the latent vectors from VQ-VAE necessary? While I can see from the results that the combination is better than VQ-VAE alone I would have preferred a better intuition for the need of combining both. What happens if the diffusion model is applied on the original data and not the latent representation? Why is the upsampled mask from the latent space so important for the final result? What happens if the mask is used as is? Hypothesis or discussions on these questions would strengthen the paper.
The results on the synthetic dataset are misleading. First of all, a dataset of 64x64 images seems like a less than ideal benchmark for segmentation where image detail is one of the most important things. Furthermore, the way the experiment is setup is unrealistic when compared to lesions in pathological brains (by randomly masking pixels). As a consequence, the impressive results obtained in Table 1 are far lower than those presented on real imaging datasets (Tables 2 and 3). In fact, the ensemble model obtains the highest results in Table 2. While this is compensated by the time comparison in Table 3, for segmentation purposes the results seem fairly low for all the methods (especially for small lesions).
Finally, the metrics are either not defined or their definition is relegated to the captions. What does ""theoretically best possible Dice score mean""? How is that upper bound calculated? Why is AUPRC never defined (I assume it means area under the probabilistic ROC curve)? Furthermore, some other concepts are also poorly defined. I understand that the original image has HxW dimensions, while the latent representation has hxw, which implies a smaller size. However, this is never clearly stated. While it is implied by the fact that masks obtained on the latent space are upsampled at the end, clearly stating the relationship between the dimensions would help."
212	Feature Re-calibration based Multiple Instance Learning for Whole Slide Image Classification	The evaluation on only two datasets, one of them is a bit outdated and the other one is in house.	"The technical novelty is limited. The whole architecture is built on top of PEM and PMSA modules.
The main contribution is exploring feature recalibration for increasing separation between +/- samples which has very limited application as it only works on the binary classification problem.
I think a simple contrastive loss (or loss with weighted samples) probably makes this improvement while it is not limited to binary classification.
The experiments are not also enough. Note the CAMELYON16 is a fairly old dataset. Further, there is no ablation study on gi hyperparameters."	The authors didn't include qualitative results. Hence, it is hard to review the classification performance. Multiple instance learning is not a new approach. Hence, technical novelty is limited. The authors didn't share their source codes.
213	Feature robustness and sex differences in medical imaging: a case study in MRI-based Alzheimer's disease detection	The paper could provides a more comprehensive analysis of possible pit-fall for deep-learning methods.	"The volume of brain substructures can vary subject to subject, it is not considered a good feature.
During CNN training, the dimensions are normalized, which can compromise the shape of the brain.
They do not mention anything about age, it is important in the study of dementia
CNN and logistic regression are already widely used algorithms."	"The work focuses on ADNI, a database that has been extensively used and includes peculiarities, such as the fact that there are more men than women while the prevalence of AD is generally higher for women than men. Extending the proposed work to other data sets would strengthen its impact.
The discussion/conclusion could remind that the training task is limited to AD vs CN and that the conclusion reached might not hold when the training task is pMCI vs sMCI (for example because women tend to progress faster than men)."
214	Federated Medical Image Analysis with Virtual Sample Synthesis	"The main weaknesses are summarized as follows:

Using such MNIST-like datasets for evaluation is not quite convincing.
Relatively limited performance improvements.
Experiment details are missing especially on simulating domain shift (i.e. non-i.i.d.)."	There is a concern in the initial training stage. The global virtual sample synthesis assumes that the global model has roughly learned the gloabl distribution, which is not true in the begining of training. Could it be better to add the global loss in eqn. (2) after a certain iterations of training?	"The clarity of the sample synthesis process can be improved. It is not clear how the direction r is obtained.
The choice comparing 10 and 20 communication rounds are confusing, it seems the model has not converged.
No visualization of virtual samples to demonstrate the results by using the VSS.
Besides ACC and F1, better use more evaluation metrics (e.g., sensitivity, AUC) to make the comparison strong."
215	Federated Stain Normalization for Computational Pathology	"Despite the claims there have been papers that have shown decent results for FL in non IID settings (e.g. arXiv:2009.01871v3) . There are different types of non IID that could have different effects on FL training and this is not really explored in the paper.
Little detail is provided on the actual training task. Labels and annotations are mentioned but it is not clear what role they play in the training.
""...architectures like U-Net probably process a pixel differently depending on its position within a crop"" - U-Net often ingesting larger tiles than they predict to get around this.
The fact that the styles seem to be imposed artificially on the client images undermines the credibility of the value of this - especially with small numbers of WSIs at each client site.
Very little detail on the actual training (e.g. local epochs, batch sizes, tools used, loss functions, optimizers etc.) 
Some comparison with FixMatch and FedAvG but there are lot of factors at play in this setup and it would have been much stronger to have done more rigorous ablation studies. 
Not sure about the need for the central server to have its own public dataset. What would happen if each of the client's simply used this too? 
Having both a local and a central training cycle could leave all the client hardware underutilised for long periods."	"1: Since the proposed BottleGAN network can explicitly transfer staining style, what is the advantage of integration of BottleGAN into WA-based FL? 
2: In this paper, it said that BottleGAN network to learn staining style transfer with linear growth, it is suggested to add a training time for comparison with other GAN network. 
3: In 3.2, author said the proposed architecture is entirely independent of the size of the input image, how to make it, it needs to make clarification?

In the experiments, this paper is to solve stanning style normalization, why choose IOU evaluation criteria for performance evaluation?There are no statements in the manuscript."	The main strength of proposed FL BottleGAN concept seems to be also its main weakness (in a relative sense). The concept requires a shareable public dataset of reference stained or destained whole slide images owned by a server. It is not certain whether this assumption can fulfilled in some real-world privacy concerned scenario.
216	FedHarmony: Unlearning Scanner Bias with Distributed Data	"While there is some innovation here in terms of the image analysis/ machine learning methodology that is used, overall it is relatively minor as the approach (including the individual  loss function components and the VGG-based architecture) are mostly taken from the already-published literature. There is some original insight in terms of how the experiments were performed and the observations made about scanner classification accuracy, however.
The age-prediction accuracy achieved by FedHarmony does not appear to be particularly improved over other techniques as noted above, although this may be reasonable given the potential scanner identification improvements (reducing it to close to chance).
The testing done appears to be performed on T1 MR structural ABIDE data alone, whereas the  ABIDE dataset is much richer, and includes resting state functional MRI data. Indeed others have looked Federated Learning using these rs-fMRI data (e.g. see X. Li, et al,  in Medical Image Analysis , 2021). Perhaps this should be mentioned/ referenced."	"Only demonstrated in one scenario. Although longer term it will be important to reinforce the results with more examples, I believe the single example shown is sufficient for this first publication of the idea.
Wording in the abstract ""We show that to remove the scanner-
specific effects, we only need to share the mean and standard deviation
of the learned features"" is too strong. The single experiment shows that in one specific scenario, this minimal amount of information can still produce decent results. The statement is not true in general. While I can believe that in most practical scenarios the proposed strategy will perform well, one can certainly construct scenarios where it won't. Bounding the conditions under which the strategy performs well would be a good focus for further work on this topic and some preliminary discussion of that in this paper would be useful.
Further to the above, the simulation experiments are valuable but limited. It would be nice (not necessarily needed for this submission but for future work) to explore a much wider range of scenarios to identify conditions under which the strategy might fail, e.g. large imbalance of data from different sites, only one or two cases at some sites, large numbers of outliers at some sites, etc."	"The application for evaluation is not very representative--the authors explored age prediction as an application scenario of their method. However, age prediction is not a representative task for neuroimage analyses. Experimenting on other tasks like segmentation, disease prediction, or image-to-image translation could provide more insights of the proposed method.
Figure 2 shows that the age of all four sites is similarly distributed. In other words, the variables of interest (in this case is age related features) should not be site-dependent. This requirement could limit the application of the method: aging subjects usually have anatomy related changes (e.g., brain atrophy). If one site have generally more aging subjects, this might compound the harmonization."
217	Few-shot Generation of Personalized Neural Surrogates for Cardiac Simulation via Bayesian Meta-Learning	"The compared methods are not explained clear or cited.
The conclusion does not reflect the contribution clearly."	This paper is not self-contained and extremely hard to follow. For example, on page 3, what are the dimensions of s, theta, and x? And how are they corresponding to notations in Fig. 1? On page 4, what is the meaning of context observations D_c? In Table 1, 1.1 is less than 4.4, but why the latter is marked bold?	"1) Some naming is a little confusing. The common terminology for few shot learning and meta-leaning are ""meta-train"", ""meta-validate"", ""meta-testing"", query and support set, etc. It is better to review the methodology and make it more clear. 
2) It is not clear why using Bayesian meta-learning here. There are other popular meta-learning frameworks, like model-agnostic meta-learning, which is used in few shot learning tasks like classification, segmentation, and also cardiac modeling(motion estimation).
reference:
Finn, Chelsea, Pieter Abbeel, and Sergey Levine. ""Model-agnostic meta-learning for fast adaptation of deep networks."" International conference on machine learning. PMLR, 2017.
3) Table results are not very clear. The meaning of numbers in Table 1 is not clear to me. Seems PNS has a lower MSE in target set compared with meta-PNS"
218	Few-shot Medical Image Segmentation Regularized with Self-reference and Contrastive Learning	"Existing comparison methods are primitive, and they need to be supplemented. After they were published, several advanced models have been proposed. Please check below references [1, 2, 3]. 
Novelty of this paper is limited because their model seems like a simple combination of existing methods. For example, idea of self-reference is already introduced in few-shot segmentation of natural image and cross-reference models are also proposed [4, 5]. The authors should compare their model with them and clarify contributions. 
In addition, contrastive learning is applied in several few-shot segmentation papers [6, 7]. Considering improvements in few-shot segmentation models for natural images, improvement of their proposed method is not that surprising.
[1] Tang, H., Liu, X., Sun, S., Yan, X., & Xie, X. (2021). Recurrent mask refinement for few-shot medical image segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 3918-3928).
[2] Sun, L., Li, C., Ding, X., Huang, Y., Wang, G., & Yu, Y. (2020). Few-shot Medical Image Segmentation using a Global Correlation Network with Discriminative Embedding. arXiv preprint arXiv:2012.05440.
[3] Kim, S., An, S., Chikontwe, P., & Park, S. H. (2021, May). Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 3, pp. 1808-1816).
[4] Zhang, B., Xiao, J., & Qin, T. (2021). Self-guided and cross-guided learning for few-shot segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8312-8321).
[5] Liu, W., Zhang, C., Lin, G., & Liu, F. (2020). Crnet: Cross-reference networks for few-shot segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4165-4173).
[6] Liu, W., Wu, Z., Ding, H., Liu, F., Lin, J., & Lin, G. (2021). Few-shot segmentation with global and local contrastive learning. arXiv preprint arXiv:2108.05293.
[7] Liu, C., Fu, Y., Xu, C., Yang, S., Li, J., Wang, C., & Zhang, L. (2021, May). Learning a few-shot embedding model with contrastive learning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 10, pp. 8635-8643)."	"One could argue about the novelty of the self-reference used, since it is similar to the one proposed by the PANet model1. However, the PANet models use the query features to compute prototypes while the authors use the support set features, and this suffices to claim novelty.

Additionally, the manuscript would be greatly improved if authors included proper statistical significance testing (hypothesis tests or confidence intervals) in Tables 1 and 2. It would also be very informative to include 5- and 10-shot experiments on the same datasets, as well as experiments on other areas of the body that have abundant public CT/MRI datasets, such as head/neck, thorax and pelvis.

1 Wang, K., Liew, J.H., Zou, Y., Zhou, D., Feng, J.: Panet: Few-shot image semantic segmentation with prototype alignment. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. (2019) 9197-9206"	Potentially limited method novelty as there are multiple works (Liu et al. 2021, Learning a Few-shot Embedding Model with Contrastive Learning; Liu et al. 2021, Few-Shot Segmentation with Global and Local Contrastive Learning) that have applied contrastive learning to image semantic segmentation tasks.
219	FFCNet: Fourier Transform-Based Frequency Learning and Complex Convolutional Network for Colon Disease Classification	Using complex CNN is not novel enough, in addition, the main claim of the paper is the ability of this network to deal with brightness imbalance challenges within colonoscopy images, yet no visual results are presented to determine that the network could learn to ignore those areas while doing the classification.	"There has been many works treating complex data as double channel input to network, so as to mine information from both real and imaginary parts, for example, in fast MRI reconstruction task[1]. As these works are not compared nor referred, it's unclear the difference or improvement brought by the complex network design in this paper.
[1] Image reconstruction by domain-transform manifold learning

The random shuffling operation in PSM is somehow confusing. If the patches are randomly arranged, i.e. channel index of input is irrelevant to position, how would the network model the relationship between different patches of input? From my perspective,the kernel for each channel would be eventually equivalent in this way. And if the experiment results are correct, then it means this task requires only local feature within a patch."	"The baselines are a bit weak and no related method for colonoscopy classification are provided.

There are a number of approaches which perform pixel and patch shuffles (e.g. ""PatchShuffle Regularization"") and none of them are cited. This claimed novelty is a stretch at best and these other works should be cited. I would really like to see why the proposed approach is different or better than these similar shuffling methods.

The authors claim in the conclusion that they introduce complex convolutions. These have been known in the literature for years (e.g. ""On Complex Valued Convolutional Neural Networks"" and many works since then). Be careful not to overstate your novelty. You are not introducing complex networks for the first time. But you are one of the first to apply complex CNNs in the way your are to this type of problem are it is interesting."
220	Fine-grained Correlation Loss for Regression	"1.The novelty is limited since two correlation losses have been presented before.
2.The experimental setting is insufficient and more comparison experiments and ablation studies should be designed to demonstrate the proposed method.

Although the presentation is accept, it still should be improved further."	I do not see any major weaknesses as a conference paper; however, I do have some questions that I asked in the comments section.	"Improper wording:
a.	In Section 2.2, both the coarse and fine losses are not related to the SRC, so it is not proper to name it ""SRC loss"". The coarse loss is an L2 loss on similarity; the fine loss is a margin-based L1 loss on the difference of similarities and the difference of ratios. 
b.	The fine loss is similar to the margin-based loss, but there are no positive or negative examples. So it is improper to name them 'L_pos' and 'L_neg'.  They stand for ascent and descent constraints in the ordered tuple. 
c.	In the equation P(xi, xj) = [R(xi)/ R(xj)], the notation R(*) probably confuses readers. In eq. (2), the y_i is used to denote the target. They should be consistent.
Unsound experiments:
a.	There are no experiments on public datasets and other image modalities. If the proposed method works, it should work on any dataset. The authors only evaluate their method on the private dataset and ultrasound images, which seems that the authors are not confident in their methods.
b.	The ablations for Lpos and Lneg are unnecessary. If R(xi) > R(xj) > R(xk), then S(xi, xj) > S(xi, xk) and S(xj , xk) > S(xi, xk) should both be satisfied. Using Lpos or Lneg separately cannot guarantee xi, xj, xk in ranking order. So it is unnecessary to run the ablation for the loss.
c.	There are no experiments for the hyperparameter a in Eq. (3). Readers may be curious about how this value influences performance.
d.	There are no ablations for adaptive margin in the fine loss.  Similarly, readers probably would like to know how the adaptive margin works compared with different fixed margins."
221	Flat-aware Cross-stage Distilled Framework for Imbalanced Medical Image Classification	The illustration of Fig. 1 could be improved.	There is still no guarantee that local optimum of the second stage can be found in the flatten local minimum of the first stage.	"The technique novelty is limited, and most components exist in the literature and some have been widely used in many existing works.

This paper claimed that most of existing methods improve the predictive performance for the minority class but at the cost of decreasing the performance for the majority. However, there is no related metrics or discussion (only in Figure 1) for this. For example, for the long-tailed dataset, you can use three groups ""many"", ""medium"", ""few"" to give a complete illustration on how your proposed methods can well improve the performance both on majority classes and minority classes, especially for a long-tailed distribution with more than 20 classes. Lacking related results may make the paper seem to be over-claimed.

For the fundus dataset, DR grading is a common challenge, but I also expected to see how different methods affect the performance. Moreover, to the best of my knowledge, the data imbalance issue does not do damage to the performance as we imagined, i.e., the performance of minority classes may exceed that of major classes. For instance, NPDRIII always shows good performance even if there is a few samples available since it show obvious pathological features. I believe DR grading is not a good task. I recommend the following references for some related information.
[1] Relational Subsets Knowledge Distillation for Long-tailed Retinal Diseases Recognition.
[2] Automatic detection of rare pathologies in fundus photographs using few-shot learning.

This paper studied and tested their methods on imbalanced dataset and its specicial condition - long-tailed. However, I suggest to evaluate it on more long-tailed benchmarks due to the reasons I mentioned above.

Some other comments:
Compared with RS, the two-stage methods (which contains RS component) such as cRT and DiVE did not obtain good results. To the best of my knowledge, two-stage methods show good robustness in many long-tailed benchmarks. Can you explain that?"
222	Flexible Sampling for Long-tailed Skin Lesion Classification	"The anchor sample selection model assumes that the embedding representation show a unimodal gaussian distribution. However, this assumption was not supported theoretical or experimental data.
A few minor details of the model are missing."	Authors include too much techniques in one paper. The relation between the modules and the effectiveness of each module are not well-explored.	"Although the author's method does show improved results, they are relatively close to some methods in the comparative study. e.g., RW and ELF. The latter also lies on the curriculum learning-based category, making it significantly close to the proposed method.
The paper is in general well written. However, some parts of the manuscript regarding the description of the method (section 2) are not entirely clear. For instance some symbols and mathematical variables are used in equation without introduction."
223	fMRI Neurofeedback Learning Patterns are Predictive of Personal and Clinical Traits	"Neurofeedback is a major innovation of this paper, however, neurofeedback data is not easy to obtain, the authors need to further clarify how these neurofeedback data (e.g. fMRI data, Clinical data) for the experiments were obtained.
The design of the method section needs to be further clarified, e.g., why K-means clustering operations are performed.
It's not clear that why the authors use the error of prediction as the signature, what is the motivation?"	"Selection of the datasets is unclear, while it is part of a bigger dataset.
The fMRI processing method has not been described. Was there any quality control (movement or artifacts in the data)?
How was the fMRI acquisition performed?"	Not sure how to properly interpret the MSE results. They seem to be arbitrarily scaled.
224	Free Lunch for Surgical Video Understanding by Distilling Self-Supervisions	"Major weakness: Potential overstatement and incomplete state-of-the-art
Note that I am not expert on this particular topic.
P3 ""To best of our knowledge, we are the first to investigate the use of self-supervised training on surgical videos.""
This claim may be removed considering the following publication: ""Learning from a Tiny Dataset of Manual Annotations: a Teacher/Student Approach For Surgical Phase Recognition"", Tong Yu et al., IPCAI 2019
After looking at the literature, I found that this paper only reports the existing works on self-supervised learning for images, while there are some works on videos. Note that I could not find any paper that shares important similarities with this work and then harms the novelty of the submitted work. They consider other problem configurations and do not have the same assumptions.
Here are two papers I found:
""Learning from a Tiny Dataset of Manual Annotations: a Teacher/Student Approach For Surgical Phase Recognition"", Tong Yu et al., IPCAI 2019
""Teaching Yourself: A Self-Knowledge Distillation Approach to Action Recognition"", Vu et al., in IEEE Access, vol. 9, pp. 105711-105723, 2021, doi: 10.1109/ACCESS.2021.3099856.
- Why is this group of works not mentioned in literature review?
Minor weakness: lack of qualitative results and impact of MS-TCN
One of the main difference with MoCo v2, from which this work is inspired, is that the data used in the submitted work has a temporal component. This work handles this using a multi-stage temporal convolution (MS-TCN). Even if this choice is supported by three references, there is no display of surgical phase sequences, which makes very hard the appreciation of this aspect. Considering that there is no left space in the current version of the paper, at least an appendix would have been great.
Minor weakness: Lack of clarity
Fig 3 is not clear about the meaning of the x-axis: does a label fraction of X% mean that X% of the data is labeled? Also, it is hard to relate the results of Fig 3 and Table 1."	"In recent years, computer vision community has shifted to 3D conv-net style models for video action recognition. Models like I3D, SlowFast, TimSformer and Swin has been consistently beating ResNet+LSTM/GRU/TCN style models. The proposed methods apply to the older approaches and not the newer models more applicable to this problem. Would be good to apply the same methods to Kinetics for pre-training a clip-based model and then apply to a 3D conv-net method for testing.
Would be good to add ablation studies on the effect of parameters Tau and Lambda.
mAP is metric often used in computer vision community for evaluation of action recognition models on long videos. Might be good to add this metric to the paper for completeness."	The results show some counter-intitutive analysis where the results obtained with 20% annotated labels are better than the 100% labels.
225	Frequency-Aware Inverse-Consistent Deep Learning for OCT-Angiogram Super-Resolution	"More experiments need to add in section 3.
(1) The OCTA image enhancement algorithms [4,5] should be compared.
(2) The ablation experiments of total loss should be added based on formula (7).
(2) Some public OCTA datasets are available and can be used in section 3, such as:
[1] Ma Y ,  Hao H ,  Fu H , et al. ROSE: A Retinal OCT-Angiography Vessel Segmentation Dataset and New Model[J].  2020.
[2]Mingchao Li, Yerui Chen, Zexuan Ji, Keren Xie, Songtao Yuan, Qiang Chen, Shuo Li. Image projection network: 3D to 2D image segmentation in OCTA images. IEEE Transactions on Medical Imaging, 39(11): 3343-3354, 2020

The authors should give more details of the related works, especially the work [4,5]."	"The image evaluation methods are not reasonable. SSIM and PSNR are used only for paired data, and there are obvious structural and domain differences in the low/high-resolution images presented in the experiments.
As can be seen from the experimental results, the reconstructed image quality is not significantly improved compared with the original image.
In the Introduction, the authors do not  present the clinical significance and value of 6x6-mm OCTA enhancement.
The authors used unpaired generative networks to enhance 6x6-mm OCTA images, which may lead to missing structural features or generating pseudo-vessels. The method lacks effective supervision of the vascular coherence loss function."	The experiments seem a little inadequate and have not verified the clinical value of resolution enhancement on optical coherence tomography angiography.
226	From Images to Probabilistic Anatomical Shapes: A Deep Variational Bottleneck Approach	No major weakness was found.	"The paper mentions the latent representation is learned, however, it still needs several samples to get the expectations. This part might be over claimed.
Some details of the method is not fully exposed."	"(1) There seem to be two main differences for VIB-DeepSSM vs other methods - (a) loss and (b) MLP decoder. It's unclear which is contributing more significantly to the improved shape prediction and uncertainty estimation.
(2) Unclear whether the dataset is public. If not, may be worthwhile to include more info about acquired images.
(3) No mention of Figure 4 in the main text - the x-axis seems to be Training size, but PPCA-Offset-DeepSSM model doesn't change values at all throughout different training sizes. Why is this the case?"
227	FSE Compensated Motion Correction for MRI Using Data Driven Methods	"It wasn't clear why FSE images were generated by multi-contrast brain MRI data. My biggest concern is both training and testing data were only based on simulated motion/FSE images. Testing the model onto true FSE images with actual motion corruption will provide a better understanding of how the data-driven approach would improve the motion compensation.
Motion artifacts were simulated by k-space ordering and signal decay, which will be highly dependent on echo train length and TR, but it seemed like one set of ETL and TR was considered.
How many testing datasets were used?"	"Additional training details are not listed in the supplementary material
Reason for training a cGAN as opposed to other available models/architectures are not stated"	The experiments are lacking. Experiments were only carried out on simulated dataset. The method was not evaluated on actually acquired MRI data with real motion.
228	Fusing Modalities by Multiplexed Graph Neural Networks for Outcome Prediction in Tuberculosis	The literature review is very light, and only mentions the classic fusion schemes. Some modalities of data were only available for subsets of the patients, and it is unclear how this impacted the models' performance. The procedure for forming edges between feature nodes is somewhat heuristic (e.g., saliency threshold), but this is fine for a proof-of-concept.	"Demonstration of results on only 1 dataset (though page limit wouldn't allow to explore more datasets, see 8.)
The method does not seem to have a built-in mechanism to deal with missing modalities (as compared to e.g. XgBoost), which is a common problem in clinical multi-omics. Instead imputation needs to be performed as pre-processing (in this case simple mean-imputation from the training set). Could be an aspect for further study.
The paper is very complex and hard to understand, especially for readers not familiar with Multiplex-GNN theory. Difficult to improve, given the page limit, but still somewhat of a weakness of the paper.
Several unclarities remain: 1) Are the d-AEs and c-AEs pre-trained and then frozen during multiplexed-graph training, or are they learned end-to-end with multi-target learning? 2) If there are 5 modalities, why is the ""multiplexed graph [only used for fusion of 3 modalities, i.e.] for multimodal fusion of imaging, genomic and clinical data for outcome prediction in TB"" (cf. bottom of page 2)? 3) In Fig 1, c-AE yields a 1D latent vector, so why are 3 latents illustrated, and why does this lead to 3 multiplex planes? And does this mean that K=3 in the figure, but the experiment actually uses K=32 concepts? I think it would be helpful to tie notation to figure contents, e.g. by indicating the values of k, K, P, i etc in the figure wherever possible."	-There is no explicit discussion on possible ways to incorporate interpretability/explainability of the model predictions
229	FUSSNet: Fusing Two Sources of Uncertainty for Semi-Supervised Medical Image Segmentation	"Lack of clear statement of contributions in context of most closely related work
Details of hyperparameter optimisation are unclear
There is no statistical testing for significance of results"	"The technical contributions are a bit weak. This paper combines the existing idea of an uncertainty-based mask [14] with an extra aleatoric loss function [11] which is a trivial extension.
Enforcing the consistency loss for the EU regions of the image between teacher and student prediction seems empirical and not well supported. One can argue that teacher and student networks can have the same EU, and thus consistency loss will not be helpful in such a scenario.
Inclusion of aleatoric uncertainty with epistemic uncertainty did not improve much over epistemic one, and there is no statistical test reported on the contribution coming from aleatoric uncertainty. This raises the question: how much weight does one need to provide between them in the final loss function? Is this something dataset-specific?"	Evaluation is not fully convincing, especially given the sometimes small margins for various metrics between competing methods. The main reason is that the paper uses small datasets only (~100 volumes per task, of which only 12 or 16 labeled ones were used) and at the same time reports only the results of a single training run per task/method. To minimize the risk of random lucky results, the authors should re-run the same experiments multiple times with different random network weights initialization, and more importantly, different random splits of training datasets (at least shuffle the labeled and unlabeled training datasets randomly). Then the distribution of the resulting metrics should be reported and compared to state of the art (e.g. using mean and standard deviation, if applicable). Is FUSSNet still coming out on top for both tasks? Furthermore, were other tasks tested and are the authors aware of any limitations of their approach?
230	GaitForeMer: Self-Supervised Pre-Training of Transformers via Human Motion Forecasting for Few-Shot Gait Impairment Severity Estimation	"Some important details are missing. For example, in Sec 2, despite citing previous work, it is not clear enough for the model architect, and in Fig. 1, some details are not shown, such as positional embeddings, self- and encoder-decoder attention in decoder, add operation in residual connection and so on. And the classification result after linear should be ""Activity when pretrain and MDS-UPDRS Score when fineturn"". All these confuse me when I read the paper."	"1) The reproducibility of this paper is low. 
2) The dataset used is not disclosed, but an accurate explanation is still lacking.
3) Since only simple explanations are listed for the experimental results, it is difficult to grasp the advantages and disadvantages of the proposed method based on the results."	Discussion on possible misclassification missing. Also novel contributions in the work are unclear. The use of pretraining using public datasets is fine but this is general idea that pretraining improves performance which is already established.
231	GazeRadar: A Gaze and Radiomics-guided Disease Localization Framework	"Using only a few radiomic features
The metrics used for comparison are not typically used for localization/detection tasks."	The performance boost seems rather negligible. It would be ideal if the authors can demonstrate that the proposed method performance increase is significant.	There is a need to work on the clarity of the text. Many abbreviations and incomprehensible references within the text make confusion and make it difficult to understand.
232	Geometric Constraints for Self-supervised Monocular Depth Estimation on Laparoscopic Images with Dual-task Consistency	"The paper is missing some explanations and insights of methodology in the technical sections.
There are also some mistakes in Fig 1 and notations which have made it harder for the readers to capture the gist of the proposed architecture."	"The prediction of the scene coordinates S and the prediction of the depth D are very similar tasks (almost the same except for the representation?). I miss a small discussion of how they are different and why they would complement each other. Would it be possible that training multiple depth-prediction networks (and use them for the dual-consistency check) would have similar results?
Section 2.2 is difficult to grasp without reading it multiple times (see details below)
Were the runs for the ablation study performed multiple times? Some of the values are so close to each other I feel it may be a coincidence that the ""full"" model performed best. Maybe report the average over N runs?
Maybe I missed it, but: Monocular depth estimation is underdefined, i.e. there's no way to know the actual distance (unless objects of known size are in the scene). How do you handle this unknown scale factor? Does this mean that you would have to train on a per-patient basis?"	"The intuition behind using siamese poses is unclear. The author mentioned the complex rotation in laparoscopic application makes the problem harder than the autonomous driving cases, but averaging forward and backward poses has no direct relation to dealing with this issue. If the purpose of this operation was to better optimize the pose, additional a loss function on it could be an alternative, such as optimizing the T^t'_t and (T^t_t')^-1 to be the same. Better explanation and more analysis of alternative solutions would be helpful.
Regarding camera pose evaluation, the authors only reported the rotation results but missed the translation results. Since both translation and rotation determine the camera pose quality, camera translation evaluation is necessary; at least a demonstration that the proposed methods would not sabotage this aspect is needed."
233	Gigapixel Whole-Slide Images Classification using Locally Supervised Learning	"Experiments are not comprehensive at all. 
1- As you were able to feed bigger images in K=4, why other experiments like 10x and 20x are not reported. This is really interesting if we can see even low numbers with higher magnification in some datasets. 
2- Selecting batch size one does not make sense to me. One image and class is one, then the next image class two. Even if you pad images or crop images and feed at least 4 was much more meaningful. 
3- Why just compare with MIL methods? To me, this is not even a MIL strategy. MIL networks use a part of patch information, and this is not a fair comparison to compare with just MIL methods.
4- I believe SOTA is ignored. You may report the original paper number  (SOS for LSK) at least and then discuss the advantage of your method. Also, readers want to know the comparison with many other states of the art WSI classifications. It may be good to compare results with results of already published papers with the same experimental setup (at least for lung, I know 4-5 papers with more than 95% ACC)
4- big question mark here. ""only. Our method was able to fine-tune the ImageNet pre-trained weights to adapt to the medical image domain, while other methods directly used the ImageNet pre-trained features."" What does that mean? Other models were not able to fine-tune!"	"The technical novelty is limited. The main difference between this work and [24] is the random sampling step added to the reconstruction unit.
Some results have not been reported. For the LKS dataset, Please report the result of the original paper, which is the SOS method [18].
There is no explanation why  10 locations are used for the RFR model.
There is no hyperparameter to control preference of classification and reconstruction losses. An ablation study is needed.
The batch size is 1. There is no discussion around this. Doesn't this affect the stability of training?
The equations have not been referenced.
This is not clear what do TCGA-NSCLC  and  TCGA-RCC  stands for"	"In 2.1, what is necessity to use feature reconstruction unit?
In RFR, the detailed parameters are suggested to give.
At the same position, how does random ensure that most positions are ergodic? The corresponding proof was not found in the following text. And, the reason choosing cropped size is memory?
In LKS dataset, why the magnification was 4x, which different from other two dataset. Meanwhile, suggest to explain the reasons for choosing 4-5 magnification.
How are the number and location of instances determined?
The difference between different number of Module block is minor, however, no specific analysis and verification were given.
For results, it is suggested to add statistical analysis of significance, or cross-validated."
234	Global Multi-modal 2D/3D Registration via Local Descriptors Learning	"processing successive images in a sweep is a plus, but the ""Multiple frames"" section is really brief
the generation of your ground truth pose is unclear, which could impact both the training and results.
You use uniformly distributed keypoints to limit the avoid the need for annotated landmarks, but are these points reflecting a matching aven with your ""softer loss"" (page 3)?
The estimation of the ground truth pose by manual registration is also questionnable. Was it done and validated by clinicians?
the maximum errors (Fig.2) remain very large, which is a strong limit for any clinical use. The failure cases should at least be discussed (image properties, far initially pose, ... ?)
it would be interesting to evaluate your method on a dataset with annotations available, to better characterize your results on TRE. You should find several registration challenges with appropriate data (e.g. Learn2Reg or CuRIOUS).
while your study is a very good first step, rigid registration is always limited to estimate non-rigid deformation... First evaluate on a more adequate task?"	"The method used is not entirely novel since it's largely based on LoFTF algorithm. On the other hand, it has been well adapted to solving the problem for medical images.
The results are encouraging but still not reaches a very high accuracy which is needed for most image-guided intervention."	"Validation with only 16 patients
Incremental improvement of existing work (it is an improvement of LoFTR algorithm)"
235	GradMix for nuclei segmentation and classification in imbalanced pathology image datasets	"The proposed nuclei augumention is not very impressing in improving the nuceli segmentation and classification performance. On the public CoNSeP dataset, the improvement to segmentation is quite limited, and to classification, the improvement on the Miscellaneous nuclei is with the performance sacrifice on the inflammatory nuclei, which actually is also no the major-class.
The experimental results of CutMix is really disappointing, which is nearly worse on all evaluated metrics even compared to without using any data augmentation. Though CutMix is the compared method, but the authors shall compare to a more competitive method."	The authors use their own deep learning network to evaluate the performance impact of the synthetic data generation method. This makes it hard to evaluate if performance gains shown in the experimental evaluation are due to the specifics of the deep learning network and whether the synthetic data generation method will lead to similar performance gains with other networks. For example, the nucleus segmentation performance numbers for the CoNSeP dataset (Table 2) with the proposed method are lower than or equal to those achieved by Hover-Net [2] using real data only (Table III in [2]; e.g., 0.853 DICE with Hover-Net vs 0.846 DICE with the proposed method).	This work would be more appreciated if the authors compare between CutMix and GradMix in terms of the methodology. CutMix was not described in the introduction, so without understanding CutMix, it was difficult to follow the result when comparing CutMix and GradMix.
236	Graph convolutional network with probabilistic spatial regression: application to craniofacial landmark detection from 3D photogrammetry	What is the inter- and intra-rater repeatability for the manual annotation?  How does your method compare to that?	"A weakness to the study, in this reviewer's opinion, is the use of a single expert's landmark placement as ground truth. Typically in morphometry studies, more than one expert's landmarks are used to remove subjective bias and this maybe more important when the landmarks are to be used for training an algorithm.
It seems to this reviewer that isotropic vertex placement of the raw image data would reduce the need for the proposed multi resolution approach? Isotropic vertex placement regularises the distance between neighbouring nodes regardless of spatial location thus possibly enabling single order polynomial use. Given that the authors are already doing some preprocessing of the raw image data anyway (section 2.2), could this note gave been done within that step? Please note this is not a major limitation, rather an honest question.
As indicated above, technically, the proposed framework seems to be an extension of Chebnet, albeit with some important contributions to handle the peculiarities of automated 3D landmark location."	"1) The literature review is limited. There are a number of point-cloud deep learning methods [1], which are originally designed for the object detection from 3D point cloud, should can be directly applied for the task of this paper. For instance, VoxelNet [2] and PointRCNN [3] should be mentioned in the Introduction and compared in the Experiments.
2) The detection accuracy of this method is relatively low. The landmark localization error can be more than 10mm as reported in the Experiments section, such large errors should be not clinically satisfied. 
[1] Guo Y, et al. Deep learning for 3D point clouds: A survey. IEEE TPAMI. 2020, 43(12):4338-64.
[2] Zhou Y, et al. VoxelNet: End-to-end learning for point cloud based 3D object detection. CVPR, 2018, pp. 4490-4499.
[3] Shi S, et al. PointRCNN: 3D object proposal generation and detection from point cloud. CVPR, 2019, pp. 770-779."
237	Graph Emotion Decoding from Visually Evoked Neural Responses	"Although this paper showed interesting results and a novel method, the number of fMRI subjects is small (n=5), which may cause reproducibility issues when applied to other datasets. Also, since this paper didn't use the emotion scores from the individuals that performed fMRI sessions, which means that the emotion decoding they did was the emotions that people mostly feel from videos, not the emotions of the individual itself
This paper doesn't have a conclusion section. It would be worth showing the conclusions by summarizing the results and the interpretation to emphasize the main point of this study.
Lastly, it would be better for this paper to show the relevance of this method by interpreting more of the embeddings from emotions and the brain regions, such as interpreting which of the ROIs showed meaningful results to interpret the corresponding emotions."	The experimental design is not clearly stated in article. Also authors should compare their results to some of the newest models.	I don't get a clear understanding of how the evaluation is done. If there are five subjects, are we doing leave one subject out validation? What are 10 folds here? How this 10 fold is formed? Or one question did BrainGNN apply a similar evaluation setup? or it's run by the authors with this idea.
238	Graph-based Compression of Incomplete 3D Photoacoustic Data	"(1) The benefit and significance of compressing PA data is not elaborated in the paper, and I think it is a major issue to support the value of this paper.
(2) Experimental results need more explanation. For example, I noticed the authors claimed a larger compression ratio K brings more advantages of the proposed method. But the results shown in Supplementary Material seem not very convincing.
(3) The clarity and accuracy of the writing language needs improvement."	"Some technical details are unclear to me, such as how the image data is turned into the graph structure is not entirely clear. I believe it would not be straight forward to reproduce the results. Nevertheless, the authors have indicated that code will be released, so this would help.
The premise of compression of undersampled data in the reconstruction is not clear. The undersampling would appear in the PA measurements and the image domain is usually fully sampled. This needs more detail or motivation
I would have hope also for more computational details to be included: needed memory in absolute (MB) and not relative. Computation times etc."	"The proposed reliability-aware rate-distortion optimization (RDO) need try all three coding modes and then calculate the coding cost, which significantly increase the overhead of compression.
Comparison is limited to Graph-based methods. While authors point out several disadvantages of popular image/video compression standards, there are not experiments to support it.
The detail of decoding is missing in method part."
239	Greedy Optimization of Electrode Arrangement for Epiretinal Prostheses	"The application does not involve the use of medical images like most MICCAI submissions, so it may have less broad interest across the conference. But I think the work still fits well under the computer aided intervention umbrella and is a nice example of model-guided design.
As written, the validation study lacks data showing the optimized utility function will translate to optimized image quality."	This paper is way to specialized for  being presented at MICCAI conference. The mathematical and algorithmic approaches are known but the modeling of the problem at hand and its implementation in this filed is new, however this paper will not be appreciated in MICCAI community.	"While the approach per se is novel, it is not clear why the authors reproduced (?) a figure of Ref. 2, or why the did never mention that Fig. 2 was calculated with the Python library in use for this study. It seems that the author only refer to one single other work. If so, this should deserve mentioning. If not, the section on related work is incomplete.
While this work is exploring a rather exotic field of neurostimulation it is quite surprising that the introductory part is extremely short. It would be highly recommendable to introduce the readers to the field with its challenges and problems.
Concerning the Phosphene model, some more information on this would be readily welcome. In the section H and W are used but never defined. This has to be fixed, as later on these two values pop up again; comprehension of eq. 3 suffersb dramatically. In addition, F as defined to be [0,HW] is not clear.
Furhtermore, eq. 3 is not motivated or referenced clearly.
In view of these shortcomings it is hard to judge the real novelty of the approach.
Concerning Fig 2 an improved visualization of the values for the precentual coverage is suggested."
240	Hand Hygiene Quality Assessment using Image-to-Image Translation	The proposed framework comprises of two main step: Segmentation and shape transformation. Although the segmentation step was extensively tested, evaluation of the shape transformation step was only performed on simulated data	"I think the paper lacks convincing motivations and explanations why the chosen methods are appropriate (or necessary). The authors have to make sufficiently clear why a deep learning-based pipeline is needed at all, if the reference annotations were actually created with more traditional (non-learning) image processing methods (Maybe I've missed something but that's what I took away from it).
I do not understand, what the benefit would be to translate the results in a standardized template space using a U-net (or any other learning-based method). Wouldn't a spatial registration to a template (that could be created on-the-fly from the available data) be more robust and standardized then an image-to-image translation model? I think this has the be clearly explained and it would also nice to see a comparison with a registration-based method.
I am also not convinced with the validation of the methods, in particular for the hand translation model. It is not obvious to me that the synthetic data are actually a good proxy for the real skin-coverage patterns. Also: when would you consider a result good or robust enough for medical documentation? Is variability of the estimates (and their quality) an issue? This part would have to be clarified."	The approach is analyzed using a single model (U-Net), and it would be important to compare to more recent segmentation model (e.g., U-Net++, DeepLabV3, Transformer). Also, the details and limitations of the approach with respect to generalization to: the type of fluorescent used, the demographics of the dataset, and any other confounding factors should be clearly discussed.
241	Harnessing Deep Bladder Tumor Segmentation with Logical Clinical Knowledge	I think the main weakness of this paper is, lack of literature review and comparative experiments on clinical logic rules fusing method.	"The authors should provide more information about some parameters. In particular they should explain how the parameter beta was obtained. Was the value obtained empirically? (The narrow range 0.9-1 should be justified. The value 0.01 in the Lsegment formula is not justified. 
Were the experiments (groundtruth) validated with clinicians? There is a lack of information regarding to that issue."	"-please mention the results in the abstract.

the author should write precisely about the bladder tumor. What challenges did you face when segmenting the bladder tumor in the introduction?

The introduction sections contain a limited number of the related work for bladder segmentation, only three. However, there are many publications for bladder segmentation in the top journals. Besides, it would help to write at least a paragraph about related work for merging the DCNN with clinical logic rules, not necessarily with bladder segmentation but with any other applications.

You wrote in your manuscript, ""To the best of our knowledge, the related works of involving logic rules into DCNNs for medical image segmentation are very limited."" You did indicate any current model in your manuscript. What is the difference between your model and the current one?

How did you estimate the value of the variable b?

-construction logic rules for the bladder tumor are not clear enough for me. Would you please rewrite or reconstruct the figure for it, Fig. 2?

many aspects of the method without references, which means you created everything equations, ideas!!

-The author compared the segmentation results with only the general techniques, not current state-of-the-art approaches for bladder tumor segmentation. They compared their results with only one reference with number 14 related to bladder segmentation. However, many recent works are published in the top tier."
242	Hierarchical Brain Networks Decomposition via Prior Knowledge Guided Deep Belief Network	The writing needs improvement and a few details are not clear. For instance, how the overlap rate is calculated is not clear. For Table I, in addition to the average overlap rate between DBN inferred RSNs and their corresponding RSN templates across subjects, the standard deviation should also be reported.	The aim of the proposed framework is not very clear. In my understanding the previous DBN approaches are used as unsupervised method due to the lack of labelling. However, in this proposed framework, as we are aware of the task labels already, why can't we switch entirely to the supervised method?	The writing needs improvement and more details should be clear.
243	Histogram-based unsupervised domain adaptation for medical image classification	Table 1 is very confusing. That needs major work. For example, the training error is shown in the first few lines for each experiment, but that isn't explicitly explained. Also, what exactly is the baseline method for each experiment? Finally, please add p-values for each row. Re-do Table 1 and add a better caption that explains what exactly is being classified with the AUC.	Experiments are insufficient and the motivation is unclear.	No obvious weakness.
244	How Much to Aggregate: Learning Adaptive Node-wise Scales on Graphs for Brain Networks	"Writing can be improved

The proposed method is interesting though it seems just an incremental improvement on the GrapHeat (Xu et al.  2019)"	"(1) In the paper, the authors emphasize the proposed model can achieve the aggregate the information adaptively as a contribution. How does it reflect the adaptively of the proposed method in the paper?  What does the s in Eq. (6) mean?  It seems that in the paper the only contribution is to make the scalars become trainable. My concern is that this operation can be whether achieve the  Node-wise Adaptive Scales.
(2) I do not understand the reason why you give the derivative of the loss? As far as I am concerned, the loss in the paper can achieve by back propagation. The Eq.10 to Eq.15 are the formulation for the back propagation. Do exist some constraints that make it difficult to perform the backpropagation, such as the discrete domain optimization? I do not find the constraints.  Compared with the [1], the only difference for the paper is to make the scale s in Eq6. become learnable.
[1]. Xu, B., Shen, H., Cao, Q., Cen, K., Cheng, X.: Graph convolutional networks using heat kernel for semi-supervised learning. In: International Joint Conference on Artificial Intelligence. Macao, China (2019)"	"No need to provide detailed formulas of graph convolution or GNNs in the preliminaries.
This paper considers only one dataset. More extensive datasets that involve different modalities should be added into consideration."
245	Hybrid Graph Transformer for Tissue Microstructure Estimation with Undersampled Diffusion MRI Data	None.	The threshold of theta appears a bit arbitrary, and it is not clear how sensitive the proposed method is to the threshold.	The experiments focus exclusively on NODDI parameters, which is perhaps something to note earlier.  It is not clear how this might generalize to more complex models, e.g. axon diameter or soma density mapping.
246	Hybrid Spatio-Temporal Transformer Network for Predicting Ischemic Stroke Lesion Outcomes from 4D CT Perfusion Imaging	"The authors propose to have a CNN-based encoder to process each of the input time steps of the CTP. This comes with a computational load increase that is linear with the number of steps. Therefore, it poses limitations in terms of application, but also on the capacity of the CNN encoder.
A major weakness is the in-house dataset used to evaluate the proposed method. While the authors will release the code, it is hard for other methods to compare against the proposed work.
Evaluation raises some concerns. 1) while using 10-fold cross-validation can be good, it leads to the assumption that results are averaged from the fold used for validation, which may be overly optimistic. In other words, a separate test set is missing. 2) the authors crop the images such that only the brain hemisphere with the stroke is maintained, which may lead to overly optimistic results (artificially reducing False Positives), and being impractical in a real setting, where the annotations are not available to determine the hemisphere."	I found no major weaknesses.	"1, Method in the novelty seems to be limited. The structure of the encoder and decoder are based existing works, and the authors also directly used the transformer for the outputs  of the encoder. This makes me feel that the method is a simple combination of these existing methods. 
2, The experiments are not sufficient. The authors only compared their method with TCN and U-Net. 
3, Some important details of the method are not provided. The authors claimed that ""full details of the proposed architecture can be found in the supplementary material"". However, I only see comparisons of model sizes and runtime in the supplementary material."
247	Ideal Midsagittal Plane Detection using Deep Hough Plane Network for Brain Surgical Planning	"Reproducibility is unfortunately rather moderate (if at all). Details on runtime and memory consumption are missing although the method is being claimed to be beneficial from that perspective.
For the comparison against other approaches, it remains unclear if those are based on available implementation or re-implementation. Of course, in the latter it is uncertain if alternative methods are implemented correctly and optimally tweaked.
In general, more justification should be provided why proposed method significantly outperforms SOA."	"There is no reasonable explanation for the selection of some parameters.
The rationality of the method description is slightly inadequate.
The number of comparative tests is insufficient, and the comparison method selected is not new enough.
The types of experiments are not rich enough, lacking some ablation experiments.
Insufficient references.
The summary part is less."	"The work is interesting. I have no doubt on the novelty of the algorithm, but I am concerned about the application of the work.
Detection of MSP looks good, the strength of the paper would increase if the midline shift could be calculated. In the Figure 1, the axial brain CT slices, the ideal midline or ideal MSP is detected, it would be better if the deformed or the midline offset is also detected. After detection of ideal and offset midline, the midline shift could be calculated, which is one of the important parameter in predicting the severity of the brain.  It would be better if midline shift is calculated, and a better validation would be the error in midline shift estimation."
248	Identification of vascular cognitive impairment in adult moyamoya disease via integrated graph convolutional network	"The proposed method is quite a general graph-based tool, which is not specifically designed for VCI diagnosis, and to resolve the potential issues in this research topic. 
The implementation of the node-based constraint item is confusing and should be further explained. 
Experiments were not conducted in cross-validation, so the validity of the results is questionable, and training number is also limited to construct the proposed model in a proper way."	"The process of the graph construction is not clear in this paper.
The author should use the cross-validation method and repeat the experiment some times to finally obtain the generalization ability of the proposed method."	The dataset is not sufficiently described.
249	Identify Consistent Imaging Genomic Biomarkers for Characterizing the Survival-associated Interactions between Tumor-infiltrating Lymphocytes and Tumors	Why is clinical or demographic info not integrated into the framework? Please check the rest of the minor comments in section 8.	The paper could be much better written. See comments below.	Nothing
250	Identifying and Combating Bias in Segmentation Networks by leveraging multiple resolutions	For further analysis, two additional approaches (drop Group H, and downscale Group H) could be helpful to examine the systemic bias due to image resolution.	"Unclear value of the down-stream task classification

Statistical significance of the results

2.5D approach"	The methods presented are not novel and have been used before. The models used are all presented in other papers cited by the authors. The use of a volumetric bias in adolescent and hippocampal brain segmentation evaluation has been performed before by Herten et al. in their 2018 paper accuracy and bias of automatic hippocampal segmentation in children and adolescents.
251	Identifying Phenotypic Concepts Discriminating Molecular Breast Cancer Sub-Types	The related work section has limited reference to other literature in latent space clustering and TCAV and DTCAV implementations in medical imaging, e.g. Gamble et al; Clough et al.2019; Janik et al. 2021. It is not clear how the proposed methods differ from existing work.	Weakness of this paper is really minor and some typos can be corrected during the proofreading process.	"There are a few limitations:

I fear the novelty in the paper is only incremental, with little innovation proposed to further advance the work done by Ghorbani et al.
I think the related work is not sufficiently up to date. The authors missed important works that are relevant and related to their analyses and mentioning these works would have made the paper more complete.
More attention and more insights should have been given about the concept formulation, particularly about the contribution proposed in Eq. 1. This is the most important contribution of this work and I think that it was not sufficiently discussed. How do the concept footprints compare across patients? Are there some concepts that are shared among patients?"
252	Implicit Neural Representations for Generative Modeling of Living Cell Shapes	"The references are cited randomly in a disturbed order.
The best results in Table 1 are not annotated.
Section 2 only describe the proposed method."	No non-trivial weaknesses evident to me.	There are no comparisons with other methods, therefore making it difficult to assess performance. The integration of time by normalisation of all coordinates onto [-1,1] is practical, but seems a little unsatisfactory as it does not fundamentally address the different scales in space and time (eg whether the processes are fast or slow). The same applies to the spatial dimensions. Some of the design choices are not well explained, eg section 2.2 why are the code vectors inserted into laters 1,5,8. I did not find the synthetic textures in figure 4 to be very convincing because they treat the segmentation mask as a hard boundary and the behaviour near to the edge does not appear to be very realistic.
253	Implicit Neural Representations for Medical Imaging Segmentation	"The contribution of the paper is not clear since the literature review lacks the comparison with other similar algorithms like the mentioned reference [2, 14]. Are there any other algorithms?

What is the difference between the classic discrete segmentation algorithms with a sampling mechanism and the proposed algorithm?

More related algorithms should be compared to illustrate the effectiveness of the proposed algorithm.

4, What is the shortcoming of the proposed algorithm?"	For segmentation task, the method is not novel. UNet has similar characteristic.	"An analysis of the inference efficiency of the proposed method is required but missed in the paper. According to the introduction of the proposed method, its inference efficiency is highly correlated with the resolution of the output image. The trained model may need to perform a large number of forwarding passes through the decoder to get the segmentation labels for each output pixel, which could be a time-consuming procedure. However, the author did not provide any analysis on this issue.
The size of the testing set is too small (10 cases) to make the experimental result convincing. The author used a large training set (261 cases) to train the proposed network but evaluated it using 10 cases, which makes the experimental results less convincing. The author is suggested to rebalance the size of the training set and the testing set."
254	Improved Domain Generalization for Cell Detection in Histopathology Images via Test-Time Stain Augmentation	"The technical contribution is minor. Half of the methodological section is explaining the existing work (mix-staining) [4]. The difference from the existing work is using stain mix-up in the test time and fusing the results. However, the reason is unclear, why this mix-up augmentation is good for test images without using training (the reviewer understands the effectiveness of stain mix-up in training, the reason is the same with the mix-up augmentation for semi-supervised general object classification). In test time, it is not always right that the test image is properly transferred to the color distribution of source images. Please clarify it.

In the experimental results in table 2, in most cases, \alpha=0 is the best (this is related to the above comments). The reviewer considers that the performance improvement mainly comes from an ensemble of several detection results. It is unclear whether the mix-up augmentation is suitable for the ensemble of cell detection. Even if we used other types of experts (augmentation, network, and data sampling), the performance may be improved. To show the effectiveness of the combination of the stain mix-up and the test-time ensemble approach, it is better to compare another ensemble method (there are many methods for test-time augmentation).

*In addition, the existing test-time ensemble method for object detection [3] is used for fusing the results.
*There is no discussion about the existing test-time ensemble methods as related works."	"Limited Novelty: The work is an extension of [4], whose modified version is used during inference instead of training. For modifications, the authors take a representative of the stain color matrix from the source instead of using a random matrix. However, an approach for combining the multiple augmented test image predictions for cell detection is also proposed.
Limited Application: The method is specifically designed for cell detection and hence is limited from this perspective.
Limited Analysis: The rationale behind some steps is not clear. The applicability of equations (4) and (5) is not clear. The performance is not reported directly using a random image from the source in (3) similar to [4]. The reason for using Mahalanobis distance is also not justified. It can be replaced with other distance metrics as well."	The main weakness of the proposed method is that the whole concept relies on a conjecture that model's predictions based on various source-target domain mixtures are likely to be suppressed if the mixture ratio is not right. That is based on an assumption that improper mixtures will have poor confidence scores. But that number (set on 3 in the problem considered in the paper) is selected on ad-hoc basis. It is unclear whether the same setting would work for different scenario (dataset). The authors were supposed to perform at least sensitivity analysis with respect to this parameter on the same dataset.
255	Improving Trustworthiness of AI Disease Severity Rating in Medical Imaging with Ordinal Conformal Prediction Sets	"The performance of the proposed method seems largely on par with LAC in the evaluation presented. It is unclear what the benefit of the current method is. Does it suffice to simply apply LAC to the current problem to get an uncertainty estimates?
While the authors have demonstrated that high uncertainty cases tend to contain problems, is this true on the other end of spectrum as well? That is, do low uncertainty examples also contain less problems?
The proof of theorem and proposition is left out completely from the main text. Some high-level idea should at least be included for the reader to follow along. In the supplement itself, the proof is also largely left to the cited article. It can be helpful to reproduce those proof so that it is easier to understand for readers who may not be as familiar with this literature.

Some follow up questions/comments:

why do higher severity prediction have higher uncertainty scores? Are the problem becomes harder to recognize as the severity increase?
There are some runaway text between Fig 4 and 5."	"The provided evaluation of uncertainty estimation is biased. The radiologist checked only the cases with high uncertainty for potential issues, but the cases with lower uncertainty might have issues as well.
Clarity issues in the algorithm description and few other occasions

See details in the comments section."	"The motivation for the paper is that deep learning methods suffer from certain shortcomings and that radiologists have hard time trusting these models when these models fail in unexpected ways. The authors even cite a survey which shows that radiologists have indeed hard time trusting such models. Given that the aforementioned fact is the main motivation of the paper, I expected to see a language that is more digestible by radiologists whereas the language employed in the paper is not straightforward to consume by a person who is not familiar with the underlying math.
Herein, lies a contradiction: 
a) if the target audience of the paper is computer scientists, the experimental results on a single dataset with a single model fail at showing convincing results. The IID assumption can easily be challenged in the context of medical imaging, which opens long discussions and puts the correctness of Theorem~1 at stake.
b) if the target audience is radiologists, the language employed is not appropriate.
Given the evidence in the introduction, I believe authors envisioned the paper to target radiologists, rather than computer scientists. With this assumption, I suggest the authors to be more explicit with the definitions, theorems and propositions to allow digestion by a larger audience. This paper must be understood by clinicians who are familiar with the concept of deep learning but not so math-savvy. I understand that number of pages is a limiting factor, my suggestion is to
1) Move Algorithm~1 to supplementary and use that space to enhance the text (algorithm pseudo code also has a limited value since the authors will share the code anyway). 
2) Put images in Fig~4 next to each other instead of two rows.
Also, experimental details need enhancing, what model has been used to obtain scores? How was it trained? How was the data split? Many details are missing.
Other than that, there are number of typos in the paper:
1) (Page 2) set Y is defined to be within {0,...,K-1} but later used in \arg\max as y \in {1, ..., K}, is this the same K? If so, pick one range, if not, use another letter.
2) (Page 2) D_test = {-}^{n} , is it n or n_{test}? since train is defined as n_{train}
3) (Page 4) ""sequence of nested sets that includes Y Using ..."" , missing full stop? 
4) (Page 4) at the end of Proposition 2, ""... satisfies 3"". 3 what? (3)? Equation (3)? 
5) (Page 6) ""majority class (---). observed in Figure 3"" full stop by mistake?
6) (Page 7) Please put Fig~4 and Fig~5 right after each other with no text in between. Reading a single line between those figures makes it awkward.
7) (Page 8) Full stop missing after \textbf{Uncertainty Quantification for Critical Applications}.
The language employed makes or breaks the paper for me, since the current drag for widespread AI deployment is mainly due to the resistance from radiologists. As such, papers like this one which show convincing results on these points of stress need to be digestible by clinicians."
256	Incorporating intratumoral heterogeneity into weakly-supervised deep learning models via variance pooling	None	"The MIL architectures in the experiments are not well introduced. ""Deep Sets"" architecture is never mentioned before the experiment section.
Lack of implementation details. The process of incorporating variance pooling components to ""Deep Sets"" (without attention mechanism) and ""Attn MeanPool"" (with attention mean pool modules) should be different, but no details are provided.
The selection of some experimental settings are not explained. For example, variance pooling projections K=10 and nonlinearity=log()."	"The authors describe two interpretability approaches. However, it is unclear how this interpretability approach would address some of the routine diagnostic workflows. Could the authors describe some of the structural alterations as the spectrum progresses from negative and positive SAsqr values?
Did any discussions with the pathologists happen prior to planning this study for providing model interpretations? Pathologist guided studies will help in developing models that could help in clinical adoption."
257	INSightR-Net: Interpretable Neural Network for Regression using Similarity-based Comparisons to Prototypical Examples	The choice of the clinical problem and dataset is dubious, as it is originally an ordinal classification problem and not a regression problem. As the authors did not consider the problem as an ordinal one, they also do not use any ordinal metric.	"Inadequate comparative experiments for the the main claim.
Detailed ablation experiments were done in this work, however, the authors only compared the methods with the ResNet-based baselines and did not compare the other interpretable work.
Unreasonable choice of dataset, which makes the main claim less convincing.
Considering that the method proposed by the authors is for the regression problem, experiments on the dataset of the regression task will make the method more convincing compared to transforming the discrete labels into a continuous distribution."	The contribution does not bring much novelty wrt to the original paper (Chen 2019), apart from the medical application. Also, the contribution lacks comparison wrt baseline methods for regression.
258	InsMix: Towards Realistic Generative Data Augmentation for Nuclei Instance Segmentation	not critical: the discussion is missing an outlook to other applications of this strategy. Can it be used for other histology segmentation tasks?	"some details needs further explanation
very little improvement in quantitative evaluation
background perturbation in not proven enough to have positive impact on the results"	"The actual values of some hyper-parameters such as , r, d and g which occurred in SSD constraints are not mentioned in the experiments section. The text said ""determined by cross-validation."", maybe it's better to give the actual values of these hyper-parameters in experiments.
In experiments, why all the model only trained for 300 epochs? Does all the model convergent?
In Table 4, the performance of Cowout drops evidently, what's the possible causes? Although this method is not proposed by the paper, I encourage the authors to analysis and discuss it."
259	Instrument-tissue Interaction Quintuple Detection in Surgery Videos	"1) The proposed neural network architecture is not re-producible since many important implementation details are missing. These details include:
a) The dimensions of input and output feature maps of different sub-networks and layers (STAL, FC, and GQPL).
b) The operations' details (size and number of the kernels in all convolutional layers after the ROI align operations, the location and type of the adopted activation functions).
c) The number of trainable parameters in the proposed architecture compared to the rival approaches, and the number of trainable parameters in each evaluated network in the ablation study.
2) Regarding  STAL (the spatio-temporal attention layer), it seems that feature aggregation is mistakenly formulated using the term ``concatenation"". The authors should note that concatenating N feature maps of size $N\times H \times W$ results in a new feature map with dimensions $(N*C)\times H \times W$. Besides, the authors mention that the visual feature of the current frame are augmented by addition. Hence, I would expect that the features which are added together should have the same dimensionality.
All mentioned feature maps and vectors (including queries, keys, and value vectors) should be formulated using the details of the layers they pass through (e.g., linear layers, convolutional layers, or activation functions).
3) It seems that the proposed multi-task learning approach is inspired by the previous work related to instrument-tissue interaction [16]. However, the authors have not provided any comparative results to show the superiority of the proposed network compared to this important reference. This competitor approach does not localize the instruments and tissues notwithstanding, it is expected that the proposed quintuple detection network outperforms this network in instruments, tissues, and action classification.
4) The evaluation metrics adopted in this paper cannot reveal the network's performance in quintuple detection. The two metrics used in this paper only consider the average precision for the instruments, tissues, or joint instruments-tissues. Indeed, while ``action"" appears to be the main component of the quintuple, no metric is used to demonstrate the network's ability to classify the actions. I would expect the authors to provide comparative results of mAP for joint instrument-tissue-action (as in triplet recognition performance measurement in [16], but for quintuple).
5) Regarding the dataset, the authors have mentioned that the frames which do not include any instrument-tissue interaction are removed. However, the prepared network exploits a number of consecutive frames before each keyframe for spatio-temporal feature extraction and refinement. Which strategy is adopted when the
reference frames are removed? In case the frames with no instrument-tissue interaction should be removed before evaluations, this method has a major weakness of relying on manual annotations for the test sets."	"They propose a graph-based quintuple prediction layer (GQPL) to infer the relationship between instruments and tissues and this is emphasized as a novel contribution, however there are examples of similar works in surgical domain. (Such as the somewhat recent work (2021) by Islam et al. titled STAN: Spatio-Temporal Attention Network for Next Location Recommendation.)
A literature review on both the generic object-object interaction graph representations, and particularly tool-tissue interaction graph representations is missing."	"Minor improvement
Limited data"
260	Interaction-Oriented Feature Decomposition for Medical Image Lesion Detection	"Despite Novel design and promising results, I do have the following concerns:
1, how is the model trained on the OCT dataset? This dataset is very small and only has 32 cases. Given the complexity of the proposed model, I am assuming each slice in each case is fed into the model for training and testing. If so, the positive sample should be highly sparse, how is this issue solved? If not, the dataset is too small to be significant.
2, Since the brain tumor dataset is public and exists for more than 5 years, I am wondering how the proposed solution compared to another published work specific to the same task? Even though many other solutions are listed in Table 2, they all are general solutions for natural image modalities, which is less persuasive."	"The writing of the paper can be improved. There are some word mistakes and some unclear parts, see detailed comments below.
The Global Context Auxiliary module seems to only support one lesion type per image. What if there are two lesions of different types in one image?
The OCT dataset is very small, only 23 cases.
""the recall is equal to the precision, which means the number of detections is equal to the
number of ground truths. Therefore, GCE can effectively avoid missed detections."" I cannot understand the logic of this sentence. If GCE can avoid missed detections, it should have high recall."	"What is the IoU threshold for reporting precision and recall? What are the APs for IoU=0.50 and 0.75 like?
The proposed method only modifies the features for classification branch. Why not use the enhanced features for box regression branch?"
261	Interpretable differential diagnosis for Alzheimer's disease and Frontotemporal dementia	Not being as detailed about other models used in the pipeline, such as; everything used in the preprocessing pipeline, details of AssemblyNet used in image segmentation, and details about the SVM used.	"They did not include subjects with early cognitive impairment. The images are available in the databases used.
It is not clear how they partitioned the data in validation and final testing.
They do not give details of the implementation of the SVM, kernel or optimization.
There are no details of the equipment, or acquisition characteristics. There may be a bias because they are images of different characteristics."	"The proposed method seems to be a combination of previously developed diagnostic tools.  The innovation may need further improvement.
The authors claimed that the ""early"" and accurate classification of dementia sub-types is desired.  However, there isn't sufficient clue of ""early"" detection demonstrated in their experimental setup and results.
The number of subjects in the Frontotemporal dementia might be too small (45 for training and 29 for testing) to reach a concrete classification result.  Furthermore, as shown in Fig. 2, the abnormality map of Frontotemporal dementia appeared asymmetric between left and right hemisphere, why is that?"
262	Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis	"For our brain, there's a common hypothesis that every brain is unique even if belongs to the same group. But this paper aims to find the group-wise biomarkers though good results were obtained, there's still a question that why the unique explanation
for each subject is not good? Or why don't the authors combine the subject-wise and group-wise features together?"	"analysis fails to account for confounder (such as motion, sex, ... ) so that interpretation of findings is difficult
fails to test for significant differences to baseline"	"Main issue: the method's main shortcoming is that it cannot provide subject-specific interpretation and hence cannot be used in a more realistic clinical practice.
Minor issues:
1, definition of node feature x_i is not clear (under eq. 2).
2, ""C"" is not defined in eq. 5
3, In section ""Prediction Performance"", ""IBGNN+ can further increase the backbone by about 9.7%..."". However, this observation is not supported by Table 1. 
4, define ""HC"" before using it"
263	Interpretable Modeling and Reduction of Unknown Errors in Mechanistic Operators	"While the paper does propose a method for interpreting the refinements and sources of error in the original modeling of the forward operator, it is unclear why this interpretability is clinically useful. It seems interesting from a methodological standpoint to understand the sources of error in the forward operator, however the paper does not make it clear if there is any clinical use for this information. If interpretation of the correction of the forward operator has a direct usefulness it should be made clear. The motivation for the interpretability of the method should be much more clearly motivated as this is emphasized in the title and paper itself.
The implementation details are not fully described and sometimes unclear. For example, I am uncertain if the SOM mapping plays a central role in the generative model or is performed after the fact to add interpretability to the latent variables z. Similarly a lot of training details and architectural information are missing.
The results are difficult to interpret and do not make the overall effectiveness of the method obvious. Figures like figure 2, which shows the forward operators and is not human interpretable, could be replaced with more easily interpretable figures such as figures of the simulated electrocardiographic sources displayed on the heart and their reconstructions. This would help the audience verify that the inverse solver is working as correctly.
The authors train on a variety of error sources. However, it is unclear if these errors correspond to those seen in actual practice. Motivation of these choices would be helpful to validate the experiments. Similarly, it would be interesting to see the quality of the reconstructions under different error conditions. For example, translations or rotations could be gradually increased and the error in the reconstruction could be plotted to show if the model is more capable of adjusting to different ranges of errors."	The results are not adequate to fully support the claims in the manuscript. Solid results are required to justify the advantages of the proposed model.	The authors could  have provided more results  (on both synthetic and real data) using the remaining space.
264	Interpretable signature of consciousness in resting-state functional network brain activity	Authors used the Modular Hierarchical Analysis (MHA) linear latent variable model, which was already published by Monti et al. (2020), to uncover disjoint networks and associated activities for all subjects. For training, the MHA model is fitted on the 4 monkey dataset, and the log-likelihood is computed for 1 monkey validation dataset. Due to the limited number of subjects (only 5 subjects), the MHA model could be biased to the small number of subjects, and the uncover networks could be changed according to the sample size. Authors could describe how to interpret the difference of the disjoint networks according to the optimal K, and according to the different atlases (CoCMac, DictLearn, and CIVMR). The variation of functional patterns and associated activities across runs within same subject also could be examined for clarity.	"The paper only adopted an existing method without any improvement and the references listed in the paper are not recent literatures, which weakens the novelty of this paper.
Varol, A., Salzmann, M., Fua, P., & Urtasun, R. (2012, June). A constrained latent variable model. In 2012 IEEE conference on computer vision and pattern recognition (pp. 2248-2255).
The adoption of three atlases in this work is not well explained and ambiguous.
While the method can reveal the differences in network activities among different consciouness states, the inferred spatial patterns and dfifferent network activaties are group-wise, which can not be used, at least in current work, for subject-level recognition and thus limits the clinical application of proposed strategy."	N/A
265	Intervention & Interaction Federated Abnormality Detection with Noisy Clients	Not much	The paper leverages two strategies to handle the applications of FL in abnormality detection.  The motivation is good, I appriciate this idea.  However, I consider the authors should provide more details of how to design and incorporite the strategies into FL model. Specifically,  model training is crucial to employ the proposed model,  more details and discussions are neccessary. For intance, what is the  added computional cost compared to the common FL model.	"The following causality assumptions are a bit counter-intuitive and will benefit from clarification. Does the link between C (client) and M (image features) indicate that each client has its own direct impact on the 'locally' learned model (and therefore, features). That is, by changing the client, we induce a change in the model and therefore, M. Or, is M the 'global' feature set, affected by each client? 
Shouldn't there also be a direct link between client C (e.g. hospital labeling the images) and the noisy label Y_tilde as changing C will have a direct effect on the noisy label assigned by C's diagnosis. This causal effect is independent of the fact that changing C, changes M, and therefore Y_tilde during prediction. If so, how will that affect the modularity of the SCM and the proposed do-calculus? It is not clear why equation 1 doesn't include P(Y_tilde|X). P(c|M) is also counter-intuitive to the direction of the causality between C and M.
As per Sec 2.2, the stratification assumes that the global model is an approximation of the confounder set. So, the shuffling and mixing of features between global and local makes sense - using globally learned information to suppress local noise (equation 4). However, in this context, what is the specific advantage of extending the mixing across different images from the same local batch (M_c_hat_1 and M_c_hat_2)?
Sect 2.4 says that all clients are used for the initial training. If so, the learned global model may still be biased by local clients' feature distributions. How will this impact the ability to generalize to unseen test clients with a distribution-shift in the feature distribution?"
266	Intra-class Contrastive Learning Improves Computer Aided Diagnosis of Breast Cancer in Mammography	"Some details are missing.
Question over some of the choices in the approach."	The authors mentioned the cancer/benign/normal ratio in the in-house dataset, but in validation and test sets, the ratio is not the same (almost 1:1:1 instead). Why to design the validation and test sets like this way? This might introduce a distribution gap between the model and the actual distribution in the real screening scenario.	The evaluation on independent test set in a prospective manner is important for testing the clinical feasibility. However, this can be future work.
267	Invertible Sharpening Network for MRI Reconstruction Enhancement	"1) The proposed model lacks novelty. It seems that the proposed model was built by simply using pre-existed invertible networks. To highlight the novelty and demonstrate the effectiveness of the proposed model, more explicit explanations and rigorous experiments would be needed.
2) Although results of radiologists' evaluation showed better performance than the baseline (Table 1), it is difficult to see the performance increment in the presented figures (Fig. 2, 3). Also, quantitative evaluation metrics showed degraded performance compared to other models."	"(1) There is already some work using invertible network for image denoise, for example[1]
[1] Invertible Denoising Network: A Light Solution for Real Noise Removal, CVPR 2021."	"-Quantitative results with the proposed methods is inferior to the comparison method, as shown in Table 2. 
-Fig.1(a) is not clear, and the font is too small."
268	Is a PET all you need? A multi-modal study for Alzheimer's disease using 3D CNNs	The theoretical novelty is very limited and the experiment is not innovative enough.	No novelty, beside the fact that the validation strategy is novel and validate previous findings.	"(1) The authors design three fusion strategies to show the comparison results, and the results show that  FDG PET alone is sufficient for AD diagnosis. However, whether do the results rely on the current classification models? Therefore, more SOTA classification models should be investigated with only using FDG PET. 
(2) The current comparison experiment is comducted on one dataset. To effectively the effectiveness of  FDG PET, it is recommended to emply on more datasets."
269	iSegFormer: Interactive Segmentation via Transformers with Application to 3D Knee MR Images	"The literature review is not written well enough to clarify how the proposed ISegFormer model differs from SegFormer[10]. This casts doubts on the method's novelty.
The method overclaims interactive 3D segmentation but only offers 2D segmentation that is compared with the state of the art. The 3D segmentation, achieved through segmentation propagation, obtains only so-so results that are not compared with any state of the art."	"Throughout the paper, a couple of methods are left under-explained, which hurts the reproducibility of the work (see comments regarding reproducibility below). However, this may have more to do with the authors' efforts to fit their work into the word/page limit. A few methods/details are explained in the supplementary material, but a couple still seem to be under-explained even with the supplementary material.
In addition, in many of the tested tasks (across domains) the improvements in performance seem very incremental (non significant) and many of the tasks are more compute vision and not medical datasets so might not be as interesting to the MICCAI community (though that does not take away from the contribution of the work here)."	"There are no statistics supporting any claims of superior performance. Just because one number appears higher (or lower) than another, does not mean that this perceived difference is statistically significant.
The manuscript lacks error estimates. Whenever a performance metric is reported, whether this is the number of clicks, Dice, etc etc, this should be done in the form of a mean value and an error estimate such as a 95% confidence interval or, at a minimum, a standard deviation.
I am unsure of the clinical utility of having a method that requires, say, 20 clicks to obtain a satisfactory segmentation (85% IoU is high though, so if less stringent then this number of clicks would be less). In my area of research which involves 2D/3D lesion segmentation, the number of clicks required by a clinician is limited as much as possible to 1 (approximate center), 2 (2D bounding box), or 3 (3D bounding box). Please comment."
270	Joint Class-Affinity Loss Correction for Robust Medical Image Segmentation with Noisy Labels	"My major concern to the paper is regarding the affinity in Sec. 2. It seems to me that P' is computed for each pair of pixels in an image, thus, it can measure both the intra-class and inter-class affinities. However, why is it claimed to only ""reveal the intra-class affinity relations"" in Sec. 2.1? It is also not clear how the reverse version  P_re' measures the inter-class affinity.
The ablation study is not comprehensive. The necessities of intra-class and inter-class relation learning (Eqn.1) is not well investigated. The class-level loss correction and affinity-level loss correction are not respectively studied also."	"(1) The writing of this paper needs to be improved. Some descriptions are unclear and somehow misleading. For example, the paper mentioned that the affinity map measures the similarity between two pixels and reveals the intra-class affinity relations. But it is unclear why this map captures the intra-class affinity relations. In other words, where do these relations appear? Does this map also capture inter-class relations?
(2) Although it is demonstrated that the proposed CALC is effective in ablation study, it is unclear if the Class-Affinity Consistency Regularization is indeed effective. It is possible that only the class-level loss correction and affinity-level loss correct play roles in CALC. Additional ablation studies are necessary.
(3) The class-level loss correction and affinity-level loss correction are not novel. They are from existing works.
(4) The computation of pairwise affinity maps is quadratic, and thus could be expensive. In the experiment it is unclear if the pairwise affinity map would need a lot of computation and memory. Some statistical numbers would be helpful in illustrating this."	"For the overall loss function, the effectiveness of using different weighting factors has not been discussed.

Some references to the figures are incorrect. For example, in the last paragraph of Section 3, the Jac curves are shown in Fig. 5, instead of Fig. 4."
271	Joint Graph Convolution for Analyzing Brain Structural and Functional Connectome	"The proposed method seems to be related to weighted matching. Related methods might be considered in the baselines or at least be discussed.
There are large scale dataser for age predictions. 662 samples are considerably limited in size.
The statistical significance between the proposed method and the baselines is not clearly identified.
The ablation study seems be missing, e.g., fixing the matching using one-on-one matrix?"	The coupling strength is changing according to prediction tasks. It is a results-oriented weighting rather than the intrinsic properties of brain connections. Lots of studies have shown the coupling between SC and FC follows a typical spatial pattern from primary cortex to association cortex. The authors only found a little similarity between their findings and the classical brain function gradients. This is hard to be interpreted.	"1) lt looks like only the edges between the corresponding ROIs are included as the inter-network edges. The latent hypothesis that the functional and structural connection only limited to the corresponding ROIs is used without theorical support and may affect the performance of the model.
2) The experimental results were obtained from only one time of 5-fold cross-validation. The standard deviation of the results is not reported.
3) The correlation of the predicted and real age is kind of low (~0.38) comparing with the results in other cohorts (Infant: ~0.85, Adult:~0.89)."
272	Joint Modeling of Image and Label Statistics for Enhancing Model Generalizability of Medical Image Segmentation	"While the language is fine, I find the paper hard to follow. I would have preferred more motivation and intuition in explaining the model details. This is understandable given the short MICCAI format, but the paper does repeat information in a number of places, so a more conscisely written manuscript could have had room for this.

Only cardiac substructures and two problems - generalization to a different site and sequence is attempted. I would have preferred more experiments to see if this type of modelling is also useful for different modalities and structures."	A complex but strong pipeline - I don't see any significant weaknesses.	"Even though the paper is relatively well written, I found some detail about the network training is missing, which could be helpful for the reader to understand the proposed method. The author used 3 CNNs to infer the posterior distributions. However, it is not clear whether the three networks were trained together in an end-to-end fashion (which seems to be what the text implies) or trained separately or in an alternating order with individual ground truth computed from ground-truth segmentation(which is easier for me to understand). I believe the latter is more plausible because, in an end-to-end setup, there is a lack of regulation to force the two ResNets to focus on either high-pass or low-pass components.
Another missing detail is how to generate the final segmentation labels. As the U-Net outputs distributions, a post-processing step is needed to convert the distribution to segmentation labels.
Finally, as the contour and basis decomposition remind me of the high-pass and low-pass filters, it might be good to include a baseline U-Net trained on high-pass filtered images to demonstrate the strength of the proposed method."
273	Joint Prediction of Meningioma Grade and Brain Invasion via Task-Aware Contrastive Learning	"The weaknesses of this paper include:

Why was the input limited to T1c, FLAIR-c, and ADC? The addition of T1 may add more information as it would help the network learn where contrast was up-taken.
Given the span of time (5 years) and number of scanner models, it is unlikely that all 800 studies were performed under the ""same scanning parameters"". Should add acquisition protocol ranges (TE/TR, gradients for DWI, etc).
Was there any check to make sure randomly drawn training and testing sets had similar distributions of low/high, invasion yes/no?
How did all three runs have the same distribution if selected randomly.
Only reported means of three training/testing sets, should also report ranges or SD.
Only used cropped ROIs for input scaled to be the same size - what were the original sizes (rows x cols x slices)? [Maybe this is in the reference.]
Tumor type must be known a priori before using the network to predict the grade and presence of invasion, something not usually verified until after biopsy or resection.
Not sure mean AUC is a valid measure.
Also, could do some sort of statistical testing between the quantitative metrics to test for significant differences (e.g., software from http://metz-roc.uchicago.edu could be used to compare the ROC curves from the proposed method to the other methods)."	"It is not clear how the common task features are aligned to the task specific features as described on page 5. The justification and method for this alignment should be made clearer. Why are non-task-specific features aligned to a specific task? How is doing so preferable to an initial disentanglement into only task-specific representations without a common task representation?
The authors do not have a validation data split for their experiments. This suggests the possibility that hyperparameters were chosen in a manner that would overfit on the testing data split.
It is difficult to interpret the true improvement in performance the model provides. The improvements for each ablation experiment are incremental but small, and the model outperforms baselines only by the AUC metric in the invasion prediction task.
The difference between other comparative methods and the authors' method should be clarified. Why is the authors' method better than others? What is the rationale behind the authors' method should be clear.
The authors mention that they adapt MMoE. The authors should clearly explain the difference between their method and the MMoE method? Which part is different and why do they make the change?
The technical novelty should be better described
'Moreover, the accuracy of brain invasion
determination heavily depends on the clinician's experience'. -> reference supporting this claim?
how are the three conv and avg pooling operations different? - not clear
Paragraph before conclusion -> not clear if this improvement is statistically significant. In fact, the meningioma prediction performance decreases. Premature to draw such a conclusion based on these experiments alone. I have the same concern with subsequent addition of modules in the ablation experiments. Please perform a statistical significant test and report if these are indeed significant changes. This might just be a function of the dataset split"	Rather weak statistical analysis does not really show that the proposed method if more adequate than simpler ones (Table 3) or other approaches (Table 2).
274	Joint Region-Attention and Multi-Scale Transformer for Microsatellite Instability Detection from Whole Slide Images in Gastrointestinal Cancer	"The method proposed is quite complicated and details of a critical step is missing: FWUS. I cannot find description of how the scores of FWUS is generated.
Since transformer does not make any assumption on the correlation between instance, one major problem of adopting transformer is the demand of large amount and rich training data. However, for this task, the amount of training data seems to be limited to a few hundreds or thousands. Thus I am a little bit skeptical on the result."	"Since my theoretical knowledege of transformers is not excellent, I find insufficient the details given in the paper regarding their use. For example, it is unknown to me what are the positional embeddings and what's the role of the class token. Either a brief explanation or some references would be appreciated.
It is unclear the use of the thumbnail image in the whole slide level architecture. The magnification level of that image is not given."	Nothing
275	Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification	"1). In 2.1, I'm confused as to why Foreground needs to be used in both the Tiled WSI and the Feature matrix. That means, in my opinion,  if you have divided the tissue region into patches based on the mask, there are only patches of the tissue region. So you need not extract the patches again.
2). In Fig.2.,  you probably could explain what the T operation is.
3). It is unclear whether your results have statistically significant.
4). This paper is interesting. But why not perform experiments on another public datasets (e.g., Camelyon16) to prove its universal effectiveness?"	"One main issue is the lack of clear details to present motivations. The motivation of using kernel attention in transformer is not very clear.
KAT cannot outperform baselines in large margins. The claim about the effectiveness and efficiency of the KAT is challenged. Some representative WSI models are not compared."	"Very few. If I had to find one, perhaps it would be the arguably incremental nature of the innovation.
With some of the metrics showing quite marginal gains, it would be useful to see confidence intervals for these figures."
276	Key-frame Guided Network for Thyroid Nodule Recognition using Ultrasound Videos	"The manuscript has given a brief review thyriod nodule classification method in the Introduction Section. There have been publications on ultrasound video-guided CAD method on detecting other organ/tissue. It is therefore suggested to also discuss the relative video-guided CAD works such as [U-LanD]:
[U-LanD] M. H. Jafari et al., ""U-LanD: Uncertainty-Driven Video Landmark Detection,"" in IEEE Transactions on Medical Imaging, vol. 41, no. 4, pp. 793-804, April 2022, doi: 10.1109/TMI.2021.3123547."	"No information regarding the computation time is provided.
There is no discussion on the feasibility of implementing the proposed approach for real-time clinical application."	"The Motion Attention module only has incremental innovation as compared to the similar temporal attention module in [2], using motion speed to replace video brightness as the radiologists' attention indicator.
Because the baseline methods may preform worse on the self-collected dataset, additional experiments should be conducted to compare the baseline methods with the proposed method on other datasets, e.g., the used datasets reported in the baseline method papers.
The authors should provide a complete description of the data collection process, such as descriptions of the experimental setup, device(s) used, image acquisition parameters, subjects/objects involved, instructions to annotators, and methods for quality control."
277	Knowledge Distillation to Ensemble Global and Interpretable Prototype-based Mammogram Classification Models	The accuracy improvement is insignificant.	Maybe limited by the paper length, this paper lacks of important ablation study regarding the hyper-parameters, such as the hyper-parameters in Eq. (1)-(3) and the number of prototypes.	"One issue unclear to me is in section 2.1: ""The prototype layer has M learnable class-representative prototypes....with M/2 prototypes for each class"". In this paper, the label space is only binary classification: with cancer or without cancer. Could the authors clarify on this for their specific application?
Since there are multiple loss functions used to update the parameters, it should be essential to show the loss curve and evaluate how they change by the training epochs.
In figure 4, there seems to be more than one activation on the ProtoPNet++ KD, which is less concentrated and accurate than the mere ProtoPNet. Is there any reasoning on this phenomenon?
For the experimental sections, the authors did not report any statistical tests regarding the performance, thus we may not know whether there is a significant improvement."
278	Landmark-free Statistical Shape Modeling via Neural Flow Deformations	the comparative experiment is lack.	"The novelty introduced in this paper is somewhat limited to the i) addition of the local NFD compared to the shapeflow approach and ii) the use of PCA modes in the latent space instead of the whole latent space.
The authors do not compare their approach with that of shapeflow from which it is based on. It is therefore difficult to check whether the additional shape space refinement are important or not.
The proposed approach for NFD is fairly complicated as it corresponds to the PCA modes of the control points of an RBF function describing a latent space which is itself parameterizing a velocity flow.
The authors do not  provide a lot of implementation details. For instance, they do not describe the optimization technique to perform inference of the shape  representation.  Why choose an encoder free approach ?"	The mathematical method is described very superficially. It is difficult to check correctness and novelty. The idea of solving a differential equation by parametrization, using a neural network is not novel, but the application to shape modeling is interesting.
279	Layer Ensembles: A Single-Pass Uncertainty Estimation in Deep Learning for Segmentation	"The computational performance analysis is desired to show its computational advantages over Deep Ensembles.
Unfair to only apply Simultaneous Truth And Performance Level Estimation (STAPLE) on Layer Ensemble but not on Deep Ensemble in the quantitative evaluation."	"the new metric is definitely on point to analyse the task at hand and yields good metric to compare different methods; however, it is not the first image-level metric of this kind for uncertainty estimation. This claim needs to be refined. See the area under sparsification error curve introduced in ""Uncertainty estimates and multi-hypotheses networks for optical flow"".
epistemic uncertainty is not predictive it is rather empirically computed, this needs to be corrected throughout the manuscript.
advantages of the method over multi-headed networks is not clear given that multi-headed networks are also light weight since they add small overhead at the late most layers. See the mentioned paper up.
Authors highly correlate easiness of a samle to high confidence. I wonder if this is a general mistake in the community of uncertainty estimation. I believe not all easy samples are supposed to yield low uncertainty as in easy samples that might come in the test scenarios but totally new to the network since it has not seen such an image before (sample novelty). Correlating it to error in my opinion is a better termed way.
I believe the comparison of the methods should have equal number of ensemble members for a fairer comparison.
The idea in the paper is very novel and worth to be tested in different scenarios including big natural image benchmarks where the standard ensembles are still dominating. For me it would be interesting to see a comparison in this regard to see how the method generalizes to different domains.
comparison to multi-headed network is missing in tables. Especially in runtimes since i believe that building ensembles on parts of the networks still might take some time than one forward pass of multi-headed network."	"-The rationale of using multi-layer output needs further justification. Different from deep ensembling that only uses estimation from the last layer of each network, the proposed method considered the estimation from internal layers. Notice that internal layers are classifier determined by low-level (or low scale) features. This method considered low-level feature the same weight as high-level feature in determining a segmentation confidence or uncertainty. It is still questionable how much the low-level feature can be used to determine uncertainty. As shown in the experiment with more difficulty segmentation, more layers are needed. This can be a potential limitation for extremely challenging segmentation task.

Significance of image level uncertainty. As the uncertainty evaluation is mostly used to highlight challenging region, it is not clear what is the specific application of image-level uncertainty."
280	Learn to Ignore: Domain Adaptation for Multi-Site MRI Analysis	the method is restricted to applications where the target domain is labeled and source data is available, meaning a trained model can't easily be finetuned on a new domain when source data is not available anymore.	"The proposed method is complex. I wonder how you can generalize it to multi-way classification?
There are many hyper-parameters introduced in the proposed method, such as d, r, lambda_cls, lambda_latent, and lambda_cen. The authors need to include hyper-parameter search results in the paper;
There is a flaw in the experimental design. Why do you train on both source and target domains? Then, why do you call your method a domain adaptation method? Here you have nothing to adapt to. And why do you bother to compare with those domain adaptation methods? They are not designed to train on both source and target domains;"	"[1]	As ones of the key parameters, Lambda_cls in Eq (1) is not necessary. The other two parameters are enough to control the contributions of all the three losses.
[2]	Some discussions about the influence of the key parameters are missing. Since there are three losses during training, which one plays a more important role in the final classification result?
[3]	Some lightweight adaptation/harmonization methods, such as Combat, TCA can be used for comparison."
281	Learning Incrementally to Segment Multiple Organs in a CT Image	"The clarity of the paper is in my opinion insufficient. Certain sentences are unclear:

""...Then the probability of classes not marked in ground truth or pseudo label will not be broken during training."" what does ""broken"" mean?
""These conflicts between prediction and ground truth break the whole training process."" I believe it would be much clearer to state that - for example - ""These conflicts between prediction and ground truth are the reason the network ""forgets"" the knowledge learned in previous steps""

There are more examples of such unclear sentences in the paper. (Eg. ""all datasets in the meantime"" which I suspect means ""all datasets at the same time"")
Fixing small mistakes is not as important as fixing the presentation of the paper. The bigger issue is the fact that I honestly could not fully understand the method itself!
For example equations (1) and (2) as well as Fig. 2 contain notations and terms that have not been discussed in the text of the paper. The purpose of the two terms q-hat and q-tilde is not clear to me. I suspect there might be also some mistake where t-1 should have been used instead of t.
For what concerns Table 3 I am puzzled because it seems that huge variations in terms of HD metric are not associated with any variation of dice. If the contours aren't matching by THAT much, how can it be that dice stays more-or-less stable. In MargExcIL (Ours) I see a HD of 2.30 compared to HD 8.10 of the MarcExcIL (woMem) but exactly the same dice. The culprit might be extremely small (1- or 2-voxel-sized) mis-classifications by the model, especially when not using the memory module in intermediate steps. That could be solved by simple euristics and morphological operations."	"I would have some concerns listed as follows:
1) The proposed method seems to learn one organ at a time incrementally. This training process could be tedious and time-consuming if one center contains multiple organ labels.
2) How to tackle the annotation style difference across different datasets? E.g., Center A would label the parotid's anterior tip, while Center B would not. The authors might want to discuss this potential labeling issue."	"The global loss as well as sub-loss weights should be explicitly defined
Both marginal and exclusion losses from Shi et al. could be better described"
282	Learning iterative optimisation for deformable image registration of lung CT with recurrent convolutional networks	"The authors do not give a theoretical basis for why their network mimics the Adam optimizer, i.e., a loss function gradient descent. The reason why this is important is that the results presented in Figure 3 shows that the Adam optimizer continues to improve with iteration while the proposed method appears to level out and possibly get worse as the number of iterations increase.
The authors do not show the before and after registration results. It would be good to difference images and Jacobian images for good, average and failed registration cases.
The method was trained and evaluated using a small number of data sets. The paper states 28 pairs of image volumes were selected from the EMPIRE10 (selected cases), DIR-Lab COPD and DIR-Lab 4DCT data for 5-fold cross validation. There is no indication of how many image registration cases failed (if any) there were in this study."	"(1) Section 2.1 introduces a lot about Adam optimization. However, it will be better to focus on the techniques/methods of this paper itself (Learning iterative optimisation for deformable image registration), rather than the related work. The title of section 2.1 is also misleading and confusing.
(2) Section 2.3 and Section 2.4 are difficult to follow. I suggest the authors use an Algorithm block to clearly state the entire optimization algorithm.
(3) As shown in Table 1., Adam, as the main baseline method, outperforms the proposed L2O both with and without Pre-Reg. Though this work reduces the required number of iterations, the contribution is relatively limited. It will be better for the authors to add comparisons of running time to show its advantages in terms of efficiency."	The main weakness of the paper is that the motivation to develop an instance optimization emulator is not that clear. The results indicate that the Adam optimizer works better than the L2O for both the datasets used.
283	Learning Robust Representation for Joint Grading of Ophthalmic Diseases via Adaptive Curriculum and Feature Disentanglement	"I have no major concerns for this manuscript.
The only suggestion I have is to improve the introduction in order to better state the unique challenges are associated with this task and to provide a deeper overview of the study."	details of the network needs to be clearer	The dataset is not well introduced, and the data split is not introduced.
284	Learning self-calibrated optic disc and cup segmentation from multi-rater annotations	No major concern can be raised for this paper.	"A concern is how applicable/realistic it is to apply such a model in clinical settings: how time-consuming and how much computational power is needed?  Can the hyperparameters like the number of recurrence steps for self-calibration fixed during inference?
Grammar issues throughout the manuscript. It is highly recommended that the authors proofread and modify."	"The general idea of using a combination of meta segmentation loss and annotator specific loss is also suggested in [1].

[1] Liao Z, Hu S, Xie Y, Xia Y. Modeling Human Preference and Stochastic Error for Medical Image Segmentation with Multiple Annotators. arXiv preprint arXiv:2111.13410. 2021 Nov 26."
285	Learning shape distributions from large databases of healthy organs: applications to zero-shot and few-shot abnormal pancreas detection	The comparison methods in this paper have only one baseline, which is somewhat less.	Lack of compared method in experiment part.  In the experiment part, the authors only discuss the parameters or ablation study of their method. If possible, the author should add more discussion about related works.	"The technical novelty is limited
The technical descriptions are limited."
286	Learning towards Synchronous Network Memorizability and Generalizability for Continual Segmentation across Multiple Sites	"The paper is very limited in scope to the domain of continual learning, but fails to appreciate that there are other approaches to train on data from multiple sites, such as federated learning. While continual learning is necessary in multiple-task setups and when transferring knowledge between tasks without forgetting the previous task, it is not necessary when the task is ultimately the same across sites/datasets.
Comparison to a simple FL approach is necessary.
The paper would have benefited from statistical comparison between methods (e.g. statistical tests) or at least providing some confidence bounds.
The ablation study is incomplete and limited to the features implemented by completing methods. We don't actually know the performance improvement caused by each contribution."	"The paper seems to assume that all DG methods consist of pseud-train + pseudo-test splits, which is not true. The gradient alignment is also one of many DG methods. The authors need to clarify that the proposed method has chosen a particular DG method to solve the DG problem. Also, this makes the ""relationship with CL and DG methods"" argument a little weak. It would be ""relationship with CL and a meta-learning DG method"".
There seems to be a little connection between the DG solution and the CL solution. The CL solution may be combined with other DG methods.
In L_SGA, are the losses for the first and second meta-objectives of equal ratios?"	The manuscript has no important weakness.
287	Learning Tumor-Induced Deformations to Improve Tumor-Bearing Brain MR Segmentation	"A big caveat of this paper is that the tumor must already be segmented.
Validation is missing baselines, which could include simple ones like basic tumor synthesis, atlas registration with tumor masking, etc. The main comparison is against SAMSEG. If I'm understanding correctly, the second baseline (called Our(tau=0)) inserts the segmented tumor into the atlas but never actually deforms the atlas to the new image. In my opinion, this is an irrelevant baseline since no one would reasonably do this."	"The novelty of the work is very limited.
Lack of quantitative validation of the model using real data is the most important weakness of the method. Without evaluation using a real dataset, it is not possible to correctly evaluate the performance of the method.
The method relies on an initial automatic tumor segmentation, and it is not clear how sensitive the method is to the quality of the initial segmentation.
Training a network using only synthetic data especially
There is no comparison with the state-of-the-art methods."	"Evaluation:

is there any explanation on why the SVF-based approach performs worse than the plain deformation-based approach
I agree that the method shows clear improvement if compared with the traditional atlas based method. I am wondering how would it compare with a pure segmentation method trained on the same type of synthetic data the evaluation is done on
also, the method could have been compared with a traditional model-based approach [ex. ref 1,11, 29] where the tumour growth is simulated using a reaction-diffusion equation

Synthetic data used in testing - how are the tumour deformations simulated ? are the author using the same TumorSim method ? I would be a bit concerned that both training and testing data are simulated using the same method.
In terms of references, there are related works on tumour growth simulation using deep learning. The authors could mention the connection.
In conclusion, the method is sound, but I have some concerns with the evaluation."
288	Learning Underrepresented Classes from Decentralized Partially Labeled Medical Images	The presentations of the method and experiment are not clear, I have difficulty to fully understand the method and the experiment.	"SSL step seems effective for common classes but not useful for the underrepresented classes, which might be the main focus of the paper?
Doesn't passing meta data from local servers to the parameter sever violates the privacy policy?"	"Practical issues. Whether the real-world application needs this method or not is not clear. The experiment is about simulation and the number of examples is so small that whether we need such a complex algorithm is a question.

Related Work. The paper needs to cite more related works and compare them to general vision tasks and medical image tasks. Whether there are similar datasets in general computer vision and how the proposed FedFew works on general image datasets are interesting problems to explore.

Format. Fig 2 and Fig 3 are not quite clear."
289	Learning with Context Encoding for Single-Stage Cranial Bone Labeling and Landmark Localization	"The methodology section could be written a bit better with some sections being a bit too consice for an easy understanding of the method- I assume this is the product of heavy editing to make the paper fit in the prescribed page limit. There are some questions though that naturally arise from the description of the method.
* why is the context module only taking into account features from the bottleneck stage of the U-Net? It is well known that the most crucial part of a U-net is the skip connection - I would assume more interesting performance and information could be extracted if the skip connection information was also incorporated.

Why were the landmarks regressed as heat maps and not as coordinates ?

Moreover, there is comparison with only 1 other piece of literature - It would be helpful if the authors compared and discussed the benefits and limitations of their method as they relate to other papers."	It is unclear what are challenges of automatic skull bone segmentation and landmark detection from cranial CT images. Compared with related works (i.e., Ref. 14, 18, 19, 20), the task of this paper is less than difficult, as the skull structure is generally easy to extract from CT images and the number of localized landmarks reported in this paper is very limited. Besides, only a few of relevant methods were tested for this relatively simple task.	There are no major weaknesses present in the paper. However, the paper lacks visualization of the outputs generated along with the comparisons with other networks.
290	Learning-based and unrolled motion-compensated reconstruction for cardiac MR CINE imaging	"No major weakneses.
The authors disregard methods that could iteratively register and reconstruct, although those  are computationally expensive.
On this regard, the authors could test whether an iterative  approach of registration-reconstruction using  GRAFT and CG-SENSE (trained separately) performs similarly.
The results using elastix are worse than expected. It may be possible to improve them with more work on that end."	A clear description of the mathematical setting is needed.	"Primary weaknesses of the paper have been discussed well in the text.
Kindly mention if actual k-space data was used for training and testing or synthetic k-space data was used."
291	Learning-based US-MR Liver Image Registration with Spatial Priors	The technical novelty of this paper is limited. Almost all the components used in this system is established methods, such as dense-vnet based segmentation, coherent point drift based point cloud registration, and the linear correlation of linear combination (LC2) image similarity based non-rigid registration.	"There is no comparison to an alternative technique, other than to quote the results from Ref 23 which are in the same range although image resolution, etc, are not given.  An explanation of why the proposed algorithm is an improvement over Ref 23 is needed.
How robust is the initialization technique?  Given the small number of cases, it is not clear that the rib and intercostal space modeling will always work.  How good does the initialization have to be to avoid failure?
There is also no ablation study to give an idea of what aspects of this approach are important.
MVTK is mentioned but not cited or explained."	-
292	Lesion Guided Explainable Few Weak-shot Medical Report Generation	"(1)	Poor readability. Although it is not a technically difficult paper to read, the content of the paper is incoherent and often needs to be searched up and down. For example, in Eq, 2, the explanation of ""y"" need be traced back to the section ""Problem Setting"". It causes great trouble in understanding important formulas.
(2)	Some mistakes appear in formulas. In Eq. 2, it sees ""i"" has many possible values. Also, I'm not sure if this soft label ""L^s"" is one-hot coding, which need all elements sum to one. The explanation of the soft label is confusing.
(3)	What is the theoretical rationality of ""using KL-divergence can can force the network to learn the relationship between seen and novel diseases"".
(4)	I understand that the soft label establishes the relationship between seen and novel diseases, but why not just using the cross-entropy loss since you can straightly using the softmax to get \hat_{y}^s.
(5)	In the section ""Few Weak-Shot Report Generation for Novel Diseases"", it seems that the model has complex operations in the inference stage, which are not shown in Fig. 2(overview of the proposed approach).
(6)	In comparison experiments, since the method is not the best in all metrics, author should give reasonable explanations. Also, the conclusion ""Our method outperformed other state-of-art approaches to medical report
(7)	generation in six experimental settings"" is incorrect.
(8)	In Fig. 3, why not showing the results of the compared method Grounded.
(9)	Authors do not discuss the generality of the proposed method, e.g., data conditions necessary to use this method, generality of proposed modules. 
(10)	Since in the ""Problem Setting"", we see ""Each case in the dataset contains images of a patient at different periods"". Is this a necessary condition? The authors did not provide discussions about parameters ""N"".
(11)	Also, is there a limitation for the type of provided weak annotations?"	"I have the following questions:
1, does the few shot setting also help the lesion detection task? I understand the main focus here is the text generation part. But I am still interested to see how detection performance is affected compared with the baseline Faster RCNN.
2, during inference, how are new classes classified? It is unclear to me that when the Q set images are fed in, how the predictions from the seen classes and novel classes will be combined. It seems like new classes are separated from the seen classes. Could you specify what the final prediction will be like from the detection perspective? Or simply, we do not care about the class label?"	While the model allows use of common features between seen and unseen classes, it is unclear how big does the overlap of these features is necessary.
293	Lesion-Aware Contrastive Representation Learning for Histopathology Whole Slide Images Analysis	"The paper state that a lesion-aware contrastive learning method that is specially designed for pathological images is proposed. This seems to be over-sale. This method can be performed on any other type of image with a corresponding label. The authors need to clarify this in the introduction.
Why a dynamic queue is needed (The lesion queue). As the label of each WSI is integrated into this framework, what if selecting negative samples from WSIs with different labels directly. More descriptions and experiments are needed.
The datasets seem to be private. If they will not be released, more details about them need to be listed, like the patients/WSIs of each class, and the magnifications of WSIs.
Please clarify the metrics reported in Table1-3 are in which level. Patient-level? WSI-level? Or patch-level."	"The improvements are expected since more supervisions ( i.e., WSI labels) are used, which makes it an unfair comparison with other SSL methods.
LACL is motivated by relieving the class collision problem in SSL. However, WSI classification is a typically multi-instance learning problem, where the tile labels are usually different from WSI labels. Though the WSI classification performance is improved, to what degree the class collision problem is mitigated, is not discussed and analyzed.
Some settings seem unreasonable, e.g., the expected distribution is queue refinement strategy, where the similarity is 1 and 0 for same and different pseudo-labels, respectively. However, the ideal similarity between instances with different labels should be -1 under cosine similarity. More explanation should be provided, as well as the setting of threshold in eq.5 and the same length of each queue, etc.
Experiment analysis in section 4.3  is not convincing."	"The method comparison is insufficient/unfair. The proposed LACL is a semi-supervised learning method, it utilizes the labels of WSI during contrastive representation. However, authors compare their method only with self-supervised/unsupervised learning methods.
Authors abuse the mathematic notation for i and y in Eq. (3) and Eq. (4), which may confuse readers."
294	Lesion-aware Dynamic Kernel for Polyp Segmentation	"The authors should try to position the clinical value of the work a little better. At times, it is mentioned that the new method would improve concealed lesion detection/segmentation, however this is neither shown in experiments nor made clear as to logically how.

Results are reported on a new internal dataset. These, however, use a slightly different splits to the other experiments and also it is entirely unknown if the data will be seen by anyone else. Hence somewhat limited value."	"(optional) the authors could add more efficiency comparison in the benchmark table.
There are several recent works [1,2] may helpful to the section of related works.

[1] Progressively Normalized Self-Attention Network for Video Polyp Segmentation
[2] Video Polyp Segmentation: A Deep Learning Perspective"	"Unclear what is meant by 'conceal' polyps. A quick literature search did not reveal a definition. Authors should define any terms they are introducing in the paper.
While the paper shows improvement, it is hard to assess the significance of the improvements. Authors mention that their ablation studies revealed significant improvement upon adding dynamic kernels, for instance, however how the significance is calculated is not explained. How significant is an improvement of ~1% over baseline results in the 90% range? Is this improvement a result of the additional parameters introduced by this method and how many more parameters are needed to generate a ~1% improvement?"
295	Less is More: Adaptive Curriculum Learning for Thyroid Nodule Diagnosis	"The author should supplement more information about the contributed dataset, such as the source of data, device(s) used, image acquisition parameters, instructions to annotators, and methods for quality control.
The paper writing can be further improved."	Some unclarities in the methodology	"The motivation of the paper is to solve the inconsistency between labels of dataset and the ground truth of biopsy. However, I cannot see any biopsy informations during description of method, e.g., in Eq.(2), there are predictions and the label information, respectively. Please clarify this confusion.
What is the setting of Baseline method in experiments?
How the model perform on the training setting without discarding hard samples? This will help exhibit the superiority of the proposed method."
296	Leveraging Labeling Representations in Uncertainty-based Semi-supervised Segmentation	Lack of comparison with existing methods on other datasets. Encoding priors of the segmentation masks with the DAE provides more guidance for uncertainty estimation. However, it might have drawbacks considering the generalization ability. While all the experiments were conducted on the Atrial Segmentation Challenge dataset, how does it perform on other datasets, for example, ISBI LiTS 2017 Challenge and NPC MRI dataset?	Quite simple: It is mainly improvement of mean teacher, why you design it, what is the main differences from mean teacher.  Can you give the motivation much clearer.	"The motivation is not strong. Several uncertainty estimation methods which require only single inference have already been proposed. Besides, requiring an additional task to constraints shape prior is not an obvious limitation if it does not require any additional images or annotation for training.
The comparison with previous methods is not sufficient. There is a lack of comparison results with several SOTA methods, thus the conclusion that the method of this paper achieves the best result is not strong."
297	LIDP: A Lung Image Dataset with Pathological Information for Lung Cancer Screening	"Clarity of presentations. Even after two reads, it is unclear to me the number of nodules in the database. What is the precise meaning of 'samples'? Do the author refer to CT scans, nodules or pathology reports? Please clarify.
Please confirm that all visible nodules on the CT scans have pathology-based reference standard. 
The section on the visualization of the datasets is not very informative, since there is a lot of processing involved. What are the features extracted? Are the features from the nodules or from the whole CT?
A more interesting analysis would be the comparison on the image characteristics of LIDP and LIDC-IDRI with respect to reconstruction kernels, slice thicknesses, etc... Simply that information could explain the differences in visualization of figure 1."	"The paper attempts to cover a wide area of topics and ends up missing important details that make it difficult to assess the quality of the data curated. It would be better to eliminate extraneous material in the manuscript and focus on details around the dataset and its preparation to better emphasize why this dataset is of sufficient quality or to highlight its limitations for future research.  Proper adjudication appears to be missing, making the dataset less appealing for research without re-labeling.
Most striking is the following:
ADJUDICATION - section 3.1 - The contour ""was checked twice"" seems like a very unscientific way to establish a contour."	"The experimental part should be extended, e.g. in Table 2 it can be confirmed that LIDC-IDRI does not generalize well for other datasets like LIDP, but it is not possible to confirm that LIDP generalises correctly.
I also agree that LIDP is very complementary to LIDC-IDRI, so it would be interesting to improve the evaluation by using both datasets to train models.
The dataset is very unbalanced, containing many more malignant than benign cases, which can lead to an increase in false positives (malignant) when training the models. This is evidenced in your table 3 by high recall values but low specificity values. 
No information about the size (gigabyte) and the resolution of the dataset."
298	LifeLonger: A Benchmark for Continual Disease Classification	"One of the main novelties of the benchmark the cross-domain incremental learning is poorly motivated. It is not clear how the knowledge learned on e.g. BloodMNIST could be beneficial for learning on data of e.g. PathMNIST.
The evaluation for cross-domain incremental learning is limited with only three baseline methods compared. Why are the other approaches not used for cross-domain incremental learning?
For all evaluations an upper bound by running joint training on all tasks/classes at once is missing. Such an upper bound is relevant to judge the performance of the continual learning methods."	commonly in domain adaptation, different domains are given by e.g. data from different hospitals, but the task stays the same. Here, domains are defined as different datasets with also different tasks, e.g. CT scans for organ classification vs kidney microscopy images for cell classification.	"Though I understand the advantages of a computationally inexpensive benchmark dataset, MedMNIST includes very unrealistic images with a resolution of 28x28. It is unclear how results in this benchmark would translate to more realistic medical image applications.

Considering that the main contribution lies in sharing benchmark results, a major weakness lies in the choice and definition of the metrics. The forgetting metric, which observes the difference ""between the highest and lowest accuracy for each task"" does not follow conventions in continual learning literature that quantify forgetting as the difference in performance directly after training the model with a certain task and after continuing training with future tasks (Diaz-Rodriguez at al.). The authors also do not measure forward transfer or include any metric that quantifies loss in model plasticity.

For the ""cross-domain incremental learning"" setting, the ""domains"" are too different. It is unclear how much this would mimic the situation that the authors describe, namely that of ""datasets originating from different institutions.""
In addition, the authors name the introduction of this scenario as a key contribution, yet previous research has looked at similar settings within medical imaging (Perkonigg at al., Srivastava at al., Memmel at al.), even research that the authors cite (Srivastava at al., Memmel at al.).

It would have been preferable to evaluate a pseudo-rehearsal method alongside iCarl, which can be considered an upper bound in many scenarios.

References:
(Diaz-Rodriguez at al.) Diaz-Rodriguez N, Lomonaco V, Filliat D, Maltoni D. Don't forget, there is more than forgetting: new metrics for Continual Learning. In Workshop on Continual Learning, NeurIPS 2018 (Neural Information Processing Systems 2018 Dec 7.
(Perkonigg at al.) Perkonigg M, Hofmanninger J, Herold CJ, Brink JA, Pianykh O, Prosch H, Langs G. Dynamic memory to alleviate catastrophic forgetting in continual learning with medical imaging. Nature Communications. 2021 Sep 28;12(1):1-2.
(Srivastava at al.) Srivastava S, Yaqub M, Nandakumar K, Ge Z, Mahapatra D. Continual domain incremental learning for chest x-ray classification in low-resource clinical settings. InDomain Adaptation and Representation Transfer, and Affordable Healthcare and AI for Resource Diverse Global Health 2021 Oct 1 (pp. 226-238). Springer, Cham.
(Memmel at al.) Memmel M, Gonzalez C, Mukhopadhyay A. Adversarial continual learning for multi-domain hippocampal segmentation. InDomain Adaptation and Representation Transfer, and Affordable Healthcare and AI for Resource Diverse Global Health 2021 Oct 1 (pp. 35-45). Springer, Cham."
299	LiftReg: Limited Angle 2D/3D Deformable Registration	The main weakness of the paper is that the number of and variation of x-ray imaging angles is only briefly described in the introduction and figure 1, but not enough detail is given.	Although this work is presented as a 2D-3D registration paradigm, to the best of my comprehension, no evaluation is performed based on real 2D data. The evaluation dataset was in fact created following the same DRR generation framework used during the training process. This point undermines the clinical applicability of the developed algorithm. Furthermore, there appears to be a disconnect between the mathematical expressions and the illustration (e.g., Fig 1). Based on the provided evaluation study, it appears that the improvement made in the Dice coefficient metric compared to the existing networks (e.g., regnet) is very minimal and the authors' justification for this is not sound. As shown in the ablation study, the back-projection (aka., lifting) process has minimal impact on the accuracy, this is not well justified in the manuscript.	"The use of a population-based PCA model lacks support and validation. Different patients may have very different anatomy distributions and mechanics, which result in very different deformation patterns.
The Lift3D module mainly serves to back-project. Why not use a standard back-projection layer that is differentiable? The stacking of 3D volumes from all angles lacks justification."
300	Light-weight spatio-temporal graphs for segmentation and ejection fraction prediction in cardiac ultrasound	"The method has a failed to achieve improved results compared to existing methods (both in table 1 and table 2).

The papers lacks the ablation study for the multi-frame GCN model. The GCN branch is parallel to the EF regressor; the results of EF regressor with and without GCN is not presented."	"The keypoint based approach may not be easy to emulate in a different setting with a different set of training data. Do the keypoints have to be in anatomical correspondence? I guess so, given their name? If so, it'd be quite hard to build a large dataset with keypoints that are registered to one another. Or is it that only apex/base keypoints are labeled? Perhaps this part is just unclear to me.

While more interesting, and also explainable, the keypoint based approach doesn't seem to give better EF  results.

About spiral net - I think the reason its preferred in the original literature is that it bakes in a stronger inductive bias about the graph. Prior to this, message passing was done by aggregating messages from neighbors in a permutation invariant way. I thought that with spiral net, you don't need this requirement because now you have well defined neighborhood encoding (just like in CNN)? It'd be better to state that explicitly in the paper as opposed to just mentioning the efficiency.

Because there's the single frame method and the multiple frame method with keypoints, ed/es classification, direct regression, it's a bit hard to follow right away."	I do not see any major weaknesses in the paper, but explanations on the methodology could have been improved.
301	Local Attention Graph-based Transformer for Multi-target Genetic Alteration Prediction	"There are no explicit major concerns,  the following relevant work should be discussed,
Shao, Zhuchen, et al. ""Transmil: Transformer based correlated multiple instance learning for whole slide image classification."" Advances in Neural Information Processing Systems 34 (2021).
Please check my minor comments at 7."	While the paper is well-written, the experiments part of the paper could be further strengthened to better demonstrate the effectiveness of the proposed method.	Nothing
302	Local Graph Fusion of Multi-View MR Images for Knee Osteoarthritis Diagnosis	"Several sections are not well illustrated. Please see as below,

the selection of slices from the three views for intersecting is not well illustrated. It is the basis for the construction of graph.
the number of vertices N is not displayed.

What is the meaning of the sentence ""
Ni
=9 can include the i-th vertex itself"" ? Doesn't the i-th vertex is the nearest vertex of vi?

It would be better if the meanings of S, C, and A in Table 1 can be displayed. Additionally, there is another S at the last sentence of the second paragraph in Page 4.
Section 3.3 is kinda of redundant with Section 3.2. I suggest to merge the two sections. Moreover,  the specific pre-training strategy is not well illustrated. Do you pre-train the LGF module using an existing dataset or just load weights from an existing model?"	"The whole proposed framework and its application of this article is highly similar to this work: Knee Cartilage Defect Assessment by Graph Representation and Surface Convolution. The authors should give the discussion and comparison with the above work.

I also find a previous paper called: Graph Transformer Networks, it also proposed a ""Graph Transformer layer"". But the authors did not cite this work, and there has no discussion or comparison with this work."	None
303	Localizing the Recurrent Laryngeal Nerve via Ultrasound with a Bayesian Shape Framework	More details about the experiments and analysis related to the results should be included.	"It would be better if the dataset could include negative cases, so that the accuracy is more meaningful.
For ultrasound images, it is better if authors could discuss how to select qualified frames from all images, because each sonographer could have a different acquisition habit. And this is a protocol that could be used for a multi-center study."	"The writing is basically good, however, many errors are not checked in the manuscript, e.g., the result of heatmap-based method UNET is put in the coord-based section in Table. 1, missing the first name of the first author in reference [1], etc.
Some details of the dataset are missing, the total scan frame number should be given, the original resolution of the frames is missing, and how the disagreement of different ground truths is handled as three clinical experts annotated the data.
The Tabel.2 shows that the distance/hit rate are 7.52 pix/89.0% for left RLN, which is already significantly better than the baseline methods. Why the Coord-based/heatmap-based methods' performance is so terrible?  Could the authors interpret the poor results of the baseline models? Were these models well trained?
Many hyperparameters are set without explanation, such as the crop size of local/global patches, the learning rate and batch size of model training, and the threshold distance for hit rate evaluation. If you follow the setting of previous works, please cite them."
304	Local-Region and Cross-Dataset Contrastive Learning for Retinal Vessel Segmentation	"I have one question about the proposed method.
The global contrastive learning can be treated as a mini-batch with a large batch size. Therefore the claimed global contrastive learning is also in local contrastive learning manner."	"The novelty of the paper is limited. A very similar method has been introduced in [1], which, however, is not included in the reference list. It seems to me that the paper just adapt the algorithm from natural image segmentation to a medical image segmentation task. The quality-aware anchor sampler is very similar with the segmentation-aware anchor sampling in [1].
[1] Exploring Cross-Image Pixel Contrast for Semantic Segmentation. ICCV 21.
Another major issue is that the performance improvements of the paper are minor on the two datasets."	"The technical novelty of this paper is moderate, considering these two contrastive strategies have been proposed and exploited early in [4]. The main methodological difference between this paper and [4] is that this paper uses a quality-aware anchor sampler to select a collection of challenging pixels for contrastive learning. However, this selection has also been introduced and verified in the paper [1], which is missed in reference.

[1] Wang, Wenguan, et al. ""Exploring cross-image pixel contrast for semantic segmentation."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.
[4] Hu, Hanzhe, Jinshi Cui, and Liwei Wang. ""Region-Aware Contrastive Learning for Semantic Segmentation."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021. (cited by the paper)

Correlating to the point above, some descriptions can be more objective. Instead of claiming ""we have proposed a local intra-region contrastive learning strategy and a global cross-dataset contrastive learning strategy..."", I would suggest changing it to ""we have applied ... into retinal vessel segmentation"". I believe a paper with a novel application can also show lots of strengths.

The image sizes of DRIVE and CHASEDB1 are smaller than (1000, 1000). Some publicly available datasets with large image sizes, like HRF, can better evaluate the capability of segmenting thin and elusive vessels. Also, the performance is comparable to some miccai works last year [2, 3].

[2] Zhou, Yuqian, Hanchao Yu, and Humphrey Shi. ""Study group learning: Improving retinal vessel segmentation trained with noisy labels."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021.
[3] Kamran, Sharif Amit, et al. ""RV-GAN: segmenting retinal vascular structure in fundus photographs using a novel multi-scale generative adversarial network."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021."
305	Longitudinal Infant Functional Connectivity Prediction via Conditional Intensive Triplet Network	"A few concerns arise when inspecting the present paper.
MAJOR COMMENTS:
The so-called identity conditional module is critical for the good performance of the model, yet the description in section 2.3 is quite brief. The main idea behind its design is visible, but the inner details are not easily accessible. Furthermore, figure 2 does not provide significant help when reading the section. 
Figure 4 is not really a must and perhaps should be displayed in the supplemental sections. I don't think that the result reported there should be taken into consideration when assessing the performance and validity of the model. Visual proof that the model correctly disentangles and differentiates attributes from the data is somewhat assumed whenever a model performs well. 
When assessing the accuracy of the predictions, only two measures are reported. Although enough to see the good performance, perhaps other network measures would proof useful for transparency and acceptance across the clinical community.
MINOR COMMENTS:
As always, careful review of misspelling is advised, despite only finding small errors in the last lines of pages 2 and 7.
Perhaps the reviewer misses a more extensive literature review. Even if there are not many previous works on this area, maybe larger comments on the ones already made would provide useful for anyone interested in the topic.
Lastly, both pages 3 and 7 have two ""alone"" lines on top of the figures which might be a little bit misguiding and inconvenient for the reader. Even if space is a scarce resource, moving them to different parts of the manuscript would improve accessibility and clarity."	The influence of brain node partition should be taken into account.	The major problem of this paper is that method parts is not clear. More detail should be give, otherwise it's hard to follow.  With such complicated designed network, ablation study is necessary, e.g. is age inf extractor really helped?
306	Long-tailed Multi-label Retinal Diseases Recognition via Relational Learning and Knowledge Distillation	"Given the main idea is similar to [16], the methodology contribution is limited.
The performance improvement of the proposed method is marginal."	"The class activation maps are not useful to highlight the obtained results.
For example, In Figure 3 (a), (c) and (d) the highlighted areas are not specific to the diseases.
The results are good but some issues are missing in the paper.

Why CCT-Net method was not explored using ResNet-50 backbone?
Which results are obtained using healthy images as input?
What about the CAM in healthy images?"	"The framework in this paper is very similar with existing methods, such as the ""Relational subsets knowledge distillation for long-tailed retinal diseases recognition (MICCAI 2021)"".
In section 2.4, the authors extracted image features with a pretrained ResNet50 and employed k-means algorithm to cluster the images into subsets. I wonder if the samples are directly clustered into three subsets (i.e., {D, AMD, H, M}, {G, C}, {N, O}) or first clustered into 8 classes (i.e., D, AMD, H, M, G, C, N, O) then divided into three subsets based on the pre-defined rule? Besides, according to the section 3.1, the images are labeled as the 8 classes. Why not use the class labels to divide all samples into three subsets?
In section 2.3, the authors preposed a novel region-based attention, which slightly differs from CBAM by combining a trainable convolutional layer. However, the authors did not provide the experimental comparison between the proposed attention and CBAM. I think this is a crucial comparison and suggest the authors to provide the comparison results.
In section 2.6, the training and testing samples are randomly splitted. The randomness of data partition might affect the results. I would suggest 5-fold cross-validation to produce more solid results.
In table 1, the performance of the proposed method is slightly worse than the existing method, i.e., CCT-Net. The performance is not satisfactory since a novel method is supposed to outperform existing methods.
In fig.3, only original images and heatmaps are provided. I suggest the authors to present the pixel-level ground-truth labels to indicate the produced heatmaps are able to locate the lesion regions. Without pixel-level ground-truth labels, the readers do not know the location of lesion regions."
307	Low-Dose CT Reconstruction via Dual-Domain Learning and Controllable Modulation	"As for the dual-domain network, the results of PSNR/SSIM outperform other methods in most datasets, but the inference time and model complexity are not considered. They are all important indicators to evaluate the performance of the model. Besides, as mentioned in this paper, ""Current methods leveraging both domains by connecting them through some simple domain transform operators or matrices"". This work proposed a complex network but lacked an explanation about why and how these designs work.

As for the controllable design, the details of CB are not clear. As mentioned in this paper, the controller parameter f_{\alpha} is learned from fully connected layers, which is dependent on \alpha and learned parameters of fully connected layers. This means that doctors can not ""manually"" select features based on \alpha because the value of \alpha is different from the value of f_{\alpha}. Besides, the training is conducted in two steps,  \alpha=0 for CB and \alpha=1  for MB, which represent two patterns of feature combinations. While the other combination patterns are not trained, it is supposed to show the overall evaluating results under other values of \alpha.

The dual-domain reconstruction methods show an overwhelming advantage over image domain reconstruction methods, but the introduction of sinogram data is not the innovation of this paper. So the authors need to consider comparing more dual-domain reconstruction methods to demonstrate the superiority of their method. 
The following papers can be considered for comparison:
[1] Zhang, Zhicheng, et al. ""TransCT: dual-path transformer for low dose computed tomography."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021.
[2] Adler, Jonas, and Ozan Oktem. ""Learned primal-dual reconstruction."" IEEE transactions on medical imaging 37.6 (2018): 1322-1332."	"The performance improvement with respect to state-of-the-art is not very significant

Neither the dual domain nor the controlable modulation are novel ideas"	"1 small dimension evaluation.
2 unclear description of the dual-domain module"
308	Low-Resource Adversarial Domain Adaptation for Cross-Modality Nucleus Detection	The proposed method has limited novelty. It adds a stochastic data augmentation module to a GAN network. In numerous papers, data augmentation techniques have already been shown to be an effective solution for training deep neural networks on small datasets. Also, the paper studies a particular case (limited target training data case) of unsupervised domain adaptation. If one has enough target training data, the proposed data augmentation module will not be necessary. Thus, the proposed method does not increase the general capabilities of GANs for unsupervised domain adaptation. Hence, this paper only repeats known contributions of data augmentation by proposing a new data augmentation module and integrating it to a GAN network. Compared to existing works, this paper proposes a slightly modified GAN architecture and a slightly improved data augmentation module.	"While the authors presented an original work, I found it difficult to understand what's the real motivation or the gaining that drive this wok. First, from the title it seems that the work is specially designed to allow nuclei detection when there's lack of annotated to train a specific model for it. However, the issue about lacking data is to train the domain adaptation (DA). DA is an unsupervised task for which you would only need the input images. For example, the following sentence ""In this paper, we propose a novel GAN-based UDA method (see Fig. 1) for cross-modality cell/nucleus detection in a low resource setting, where unlabeled target training data is scarce, a more realistic case but rarely explored."" is confusing because the low resource setting to be explored here is about the unsupervised method (non-labelled data). Also, the content of this sentence may relative: scarce labelled data in bioimage analysis is the common situation and there are plenty of works targeting it, so the authors may want to be more specific in this statement. Same for the conclusions ""With limited unlabeled target training data (e.g., 1 image),..."", you do not need to have the labeled data (i.e., nuclei detection) for the target to train the method, but only to evaluate it's performance.
Second, the authors motivated this by the fact that acquiring data is expensive, but could it be possible to use publicly available datasets of the same modality? (even if they are acquired with different devices). Finally, they are not considering any other biological meaning of the generated image despite the nuclei detection, so it should be possible using an heterogeneous dataset of a single modality and biological organism.
Related to the previous comment, along the text, the authors say that the domain adaptation is supported by the nuclei detection task. However this work was proposed to get nuclei detection supported by the fact that we may need domain adaptation.
It is important to specify in the text that this approach cannot be applied to all type bioimages as it potentially requires, by definition, all nuceli in the image to be visible."	* Visualization of the qualitative results needs to be improved.
309	LSSANet: A Long Short Slice-Aware Network for Pulmonary Nodule Detection	"The description of the formula in the paper is not clear enough.
Full writing and abbreviations in the paper are confusing and should be consistent. If the abbreviation has been abbreviated in the front, please keep the abbreviation in the back, instead of using the full letter and the abbreviation back and forth."	Some typos: Section 2.2 line 6: '5 our proposed LSSG blocks are integrated into the second and third stages of the ResNet50 encoder'. Table 1, LSSANet does not achieve the best performance, but authors still bold 89.87, which may mislead readers. Please examine typos carefully.	
310	MAL: Multi-modal attention learning for tumor diagnosis based on bipartite graph and multiple branches	"Writing in my opinion is not very good and could be better than presented in the manuscript.
Many long sentences and in some places unclear meaning is given.
Lack of given results in terms of figures is one weekness, I would like to include the figures of the suplmentary material in the main manuscript."	"1)	The proposed method was only evaluated on three spine tumor datasets while the author emphasized its general applicability on tumor diagnosis in the title, Abstract and Conclusion.
2)	It is mentioned in the paper that PMSLoss can encourage the model to extract similar high-level semantic features from different modalities, however, features of different modalities may be complementary to improve the classification performance. It is also demonstrated in Fig. 1 of the supplementary materials. Is it contradictory? Did the PMSLoss surpress complementary features during the training process?
3)	The experiments are not convincing enough. In Table 1, when comparing MRI-Axi&Sag results of the proposed MAL method (the last row) with the method without AB and TS modules (the fifth row), the AUC is similar while the ACC and SP of the latter is higher than the former. 
4)	The author mentioned that the convolutional neural network branch and the attention auxiliary branch extract local and global features individually, is there any visual results or other experiments to further prove this?
5)	The writing of this paper is extremely poor with plentiful grammar mistakes."	1) In the experiment results, it would be good to compare with the existing SOTA method. It is unclear from Table 1, which method is the current SOTA method.
311	MaNi: Maximizing Mutual Information for Nuclei Cross-Domain Unsupervised Segmentation	"The paper motivates the choice of using JSD as a lower bound of MI by discussing its advantages over InfoNCE and by saying that JSD maintains good performance without using a large number of negative samples.  However, this evidence is not supported experimentally. If one carefully checks their cited papers, 4 said that InfoNCE provided better results compared with JSD in classification tasks, while 5 indicated in its supplementary material that reducing the batch size (thus the negative examples) degraded the performance.  Hence, it is not clear to me whether this statement holds.

In addition, I am not entirely convinced about their provided experimental results. The performance values in the comparison methods were taken from reference papers, but the authors should at least report results using their own runs. Even while using the same codebase, many parameters such as the library version, random seeds, etc. can impact the results.

Authors could better motivate the applicability of the JSD lower bound on MI to UDA. Specifically, I wonder how to model and sample the joint distribution in Eq (1) since z_s and z_t come from independent images. Isn't the joint same as product of marginals in this case?"	"1) The ablation study on removing the MI loss is missing. Since utilizing the MI maximization learning to facilitate the domain adaptation is a major novelty in this paper, it is important to report the model's performance without this loss function, to further indicate its effectiveness.
2) The motivation for enlarging the mutual information for the cross-domain features to facilitate the domain adaptation has been proposed in [16] and [a].
[a] MI2GAN: Generative Adversarial Network for Medical Image Domain Adaptation using Mutual Information Constrain, in MICCAI 2020
Although the MI maximization loss function in the proposed MaNi has a different implementation, detailed discussions and comparisons with [16] and [a] are missing.
3) Visualization results are missing. In addition to the quantitative comparisons, qualitative analysis of the instance segmentation prediction is also an important metric to evaluate the model's cross-domain segmentation performance."	"It would be interesting to compare the empirical results of JS with other types of MI estimators, e.g., infoNCE, Donsker-Varadhan representation 1.
The selection of negative pairs in the MI estimator lacks motivation. What are the different ways to select negative pairs and why prefer this particular approach?

1 Belghazi, Mohamed Ishmael, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R. Devon Hjelm. ""Mine: mutual information neural estimation."" arXiv preprint arXiv:1801.04062 (2018)."
312	Mapping in Cycles: Dual-Domain PET-CT Synthesis Framework with Cycle-Consistent Constraints	"a) The motivation for this paper is not clear. In ""Abstract"" section, the authors mentioned that ""Considering radiation dose of CT image as well as increasing spatial resolution of PET image, there is growing demand to synthesize CT image from PET image (without scanning CT) to reduce risk of radiation exposure."" To the best of our knowledge, CT scanning is cheaper with sufficient training samples in clinical practice, while PET scanning is relatively more expensive with limited available data. In addition, PET scanning also suffers from radiation exposure risk. Why to use PET to synthesize CT?
b) Although the authors claimed that the proposed method ""is the first time to exploit dual-domain information in cross-modality image synthesis tasks, especially for PET-CT synthesis"", I still doubt that the innovation of this paper is not sufficient. As shown in [7], the dual-domain network for improving CT image quality was proposed as early as 2019. And the bidirectional mapping idea that comes from CycleGAN [4] could also date back to 2019.
c) As displayed in Table 1 and Table 2 of the ""Experiments"" section, the SSIM metric obtained by the Base variant has no significant improvement over the RU-Net, while the authors claim the secondary CT-to-PET task could contribute to the PET-to-CT synthesis task in structural consistency. Does the CT-to-PET task really work? What's more, in Table 2, the SSIM result of the model incorporating both image domain cycle-consistent loss and cross-domain cycle-consistent loss is even worse than that of the model only employing cross-domain cycle-consistent loss.
d) In ""Experiments"" section, the training details, such as the number of training epochs and whether the cross-validation strategy is adopted, are not stated.
e) The proposed method has not been compared with the existing cross-modality image synthesis methods. Furthermore, the comparisons with M-GAN [2], P2PGAN[6], and U-Net[12] are not appropriate, since [2] and [12] are designed for PET image synthesis, and [6] is proposed for natural image translation. It is unfair to compare the proposed framework with these methods that do not aim at the CT synthesis task."	"The two-stage training strategy is complex and not elegant.
Ablation study should compare models with and without domain-domain losses to claim the effectiveness of dual-domain learning."	This method is based on 2D image processing. However, volumetric information in 3D data is essential for the majority of medical tasks.
313	Mask Rearranging Data Augmentation for 3D Mitochondria Segmentation	"The method is only tested on a single public dataset, while others (Kasthuri++, VNC, etc.) are publicly available as well. Moreover, the selected dataset (Lucchi) is isotropic, while many EM datasets present a lower resolution in the z direction. This problem should be taken into account for a more generalist solution.
In the comparison with the state of the art, the number of execution trials and hyperparameter exploration is unclear.
There is no information about execution times, which would be very interesting for the final user."	"The authors fail to review previous studies on  learning the mapping from instance masks to real EM images, ahlthough previous methods only synthesized 2D images.
The authors fail to investigate the effect of the postprocessing. 
When comparing with other methods, it is better to declare which method uses the postrpocessing strategy in this study. For example, did the 3D U-Net and 3D U-Net (w/ ours) in Table 1 use the same postprocessing?
From the results in Table 2, it seems the model using the proposed method and 1/16 data outperforms the model without using the proposed method but using 1/8 data. Note that the second method use the same amount of data ( due to your special training setting of each batch) as the first method but with real data. It is insteresting to interperate this result."	"The proposed method is a straightforward extension from 2D version for generating synthetic 3D mitochondria images, which directly utilizes pix2pix setup. The authors designed mitochondria mask rearrangement technique, but this is a general engineering tweak with incremental contribution: take the pix2pix GAN for example, in the test stage, one can use any hand-drawing as mask input to the generator, where the objects are arranged in any location the users want; such a general and already existing technique is not specific to mitochondria image generation and thus the first contribution the authors stated seems a bit weak. In addition, there are other concerns about the mask rearrangement technique; please see the comments section.

The experiments are limited to one mitochondria dataset with two correlated volumes, which makes the second contribution (experimental validation) not particularly strong as well. Although the authors demonstrated improvements over previous works, it is hard to be fully convinced about the superiority of the proposed approach with results from one dataset. It will be highly valuable for the authors to validate the approach in other public mitochondria EM segmentation datasets, such as the rat and human datasets from mitoEM (Wei et al., MICCAI, 2020) and potentially also VNC III (Gerhard et al, Figshare, 2013)."
314	MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation	The Reviewer does not find many major weaknesses except one: during result comparison, it would be good to include a brief description of what those compared methods are. In particular, the Authors should describe what the baseline is.	There is one issue in Table1. As the images generated by style augmentor is feeded to train the segmentor, why the performance drops comparing with baseline on IID? In my opinion, the model has learned the variants and it should achieve better results than the baseline.	Although Fig. 1 shows visualizations of Mixstyle and Mixstyle-DA, it would be helpful to add few more visualizations from other baseline methods.
315	MCP-Net: Inter-frame Motion Correction with Patlak Regularization for Whole-body Dynamic PET	"The difference between the proposed MCP-NET model and the previous  B-convLSTM model [10] is not clear. The full text of [10] is not available.

Is MCP-NET= B-convLSTM + Penalty(Patlak)? If so, why was the hyperparameter lambda set to different values (0.1 for MCP-NET and 1 for B-convLSTM) in the comparison?

The proposed model aims for inter-frame motion correction in whole-body PET imaging. The frame length for motion correction in this work is 5 min. However for whole-body imaging lots of the movements such as cardiac motion, respiratory motion, sliding motion etc, are continuous. Thus the motion correction is, to some extent, limited in time resolution.

Motion-introduced mismatch in attenuation/scatter correction is not mentioned. Based on the protocol given in the Supplementary, a single CT was taken before the PET scan, and the motion correction was done post reconstruction. I understand this is a problem for all post reconstruction motion correction methods, but hopefully the authors can be aware of this.

The original image resolution is not given. 4X downsampling was applied before feeding the image data to the neural network, which could mean 5-10mm voxel size? The effects on the resolution of motion estimation is not clear. Fig2 does not have units for motion fields either.

Why use LNCC as a similarity measure for dynamic frames?"	"It seems like the amount of data used to train the model is smaller than usual. I might be wrong, but it seems that there are only 19 frames per volume for 27 patients in total. Is this correct? If so, how many volumes were used for training/validation/testing? This point could be clarified further as I am not sure if the model is overfitting given a limited data quantity.
57 hyper metabolic regions were selected for additional evaluation by a nuclear medical physician. Are these the same regions that are shown in Figs. 3, 4, and 5?
While it must be challenging to acquire data from additional scanners (other than the Siemens Biograph mCT), how well does this model translate to data acquired from different scanners within the same/different institute?"	The lack of ground truth makes the results questionable to a certain extent.  The authors tried to address this by motion simulation, but it is not known to me how realistic the simulation was, especially with the low number of subjects involved.
316	Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction	"This paper restricts the under-sample matrix M to be a diagonal matrix with element to be either 1 or 0. It limits the impact of the proposed method. It is worthy to study how to apply the proposed method to other under-sample patterns, such as random or spiral.
The assumption that the noise in equation (5) is set as zero needs supporting evidence/justification.
Comparison with DDPM on the image domain is expected to justify the superiority to do DDPM in the measurement domain."	In the discussion part, the weakness of the proposed method is somewhat less.	"-The paper is not clear. Apart from the typos, the paper introduces CT, rather than MRI in section 2.2. I understand the authors want to introduce a general framework, but it makes me feel confused. 
-The authors only use magnitude image for DDPM, however, MRI is complex-valued in nature, and the phase information has clinical significance. How to apply to complex-valued MRI? 
-The comparison with U-Net is not sufficient. More comparison with state-of-the-art models, like unrolled networks, is needed."
317	Mesh-based 3D Motion Tracking in Cardiac MRI using Deep Learning	"(1) This paper utilizes no regularization loss for the registration step. Though the proposed method imposes Laplacian smoothing loss on predicted mesh, it would be better to provide discussions or analyses of the reason for the absence of the registration regularization loss.
(2) Self-supervised differentiable segmentation losses have been widely used in related motion tracking works [1][2]. It will be better for authors to discuss these works in this paper.
[1] Self-supervised Learning of Motion Capture.
[2] Self-Supervised Learning for Cardiac MR Image Segmentation by Anatomical Position Prediction."	"*	Unclear how well the method performs quantitatively across the full cardiac cycle (results appear to show motion correction between end diastole and end systole). 
*	(minor) Comparison registration methods may also benefit from multi-view inputs, but these are never tested."	"1) Cardiac motion tracking on a geometric mesh model is not uncommon.
2) Simultaneous motion tracking and segmentation is not a completely novel achievement.
3) Comparison with only a few previous methods before claiming ""the proposed method quantitatively and qualitatively outperforms both conventional and learning-based cardiac motion tracking methods"" is not convincing."
318	Meta-hallucinator: Towards few-shot cross-modality cardiac image segmentation	"The main weakness is that the methodology lacks clarity. The method consists of multiple components and needs to be trained in separate steps with different types of data. Without clearly knowing how each component works and how the data split, it is difficult to evaluate the method.

It is not known how the labeled source data, unlabeled source data, and unlabeled test data are sampled and split respectively in the meta-training and meta-testing stages. How to simulate the structural variances and distribution shifts? What types of data are inputted into the teacher model, student model, and the hallucinator respectively?

The effect of the hallucinator is confusing. In the method section, it is presented that the hallucinator is to produce ""more meaningful target-like samples"" x^{s\to t}. However, implementation details present that ""Since limited labels are provided in the source domain, we transform target images to source-like images for training and testing"". The two descriptions are conflicting with each other."	"Since the main contribution of this work is unsupervised domain adaptation, it is expected to see experiment results on more datasets of different types and modalities. Currently, there is only results on heart images provided.
The ablation study discusses only meta-hal and meta-seg. More detailed analysis could help to understand the proposed method better, e.g., how the choice of support images will affect the performance, what the hallucinator learned through meta-learning."	"The author should consider adding another experiment of few-shot learning (FSL) method, e.g. SSL-ALPNet[1], in target domain to show the upper bound of FSL;
Some minor errors:
The 'leverage' in the second line of page 3 should be 'leverages'.

The proposed method adpots CycleGAN to achieve unpaired image translation for image adaptation, which is compution intensive.
[1] Ouyang, Cheng, et al. ""Self-supervision with superpixels: Training few-shot medical image segmentation without annotation."" European Conference on Computer Vision. Springer, Cham, 2020."
319	MIRST-DM: Multi-Instance RST with Drop-Max Layer for Robust Classification of Breast Cancer	I'm not convinced about the clinical motivation of adversarial defenses in medical images. It is possible that this problem might gain traction and significance in future, however, I don't see the significant interest in this problem for wider audience.	"In computer vision tasks, especially on adversarial machine learning, it exists on adversarial examples on small datasets. The authors may be one of the first attempts on this task, but the overall idea is still somewhat weak.
Adversarial training or co-training is one useful trick for improving the robustness of one medical image classification task, while the overall learning scheme is hard to avoid the severe overfitting problem.
Could the proposed method be adaptive to target attacks? It seems most of these efforts are devoted to un-target attacks."	"The dropmax layer is an interesting and intuitive idea. However, choosing the second largest value does not guarantee that the perturbations are ignored. To give a simple example of an alternative approach; one could ignore or 'drop' the upper quantile of values instead of ignoring just the top-1. An interesting experiment would be showing how the performance changes as more of the max-values are ignored.
The standard deviation across the five folds could have been reported."
320	Mixed Reality and Deep Learning for External Ventricular Drainage Placement: a Fast and Automatic Workflow for Emergency Treatments	"*The paper is mostly an integration of existing work and presents very little novelty.
*Comparison between guided and blind navigation may be partly unfair"	"The main weaknesses are:
1) the display of the MR visualization results is not smooth, with some display delay.
2) this study doesn't consider the brain shift during the EVD placement."	"The novelty of the techniques involved is limited
Only two anatomical models were used for evaluation, and can be limited in terms of anatomical variability"
321	mmFormer: Multimodal Medical Transformer for Incomplete Multimodal Learning of Brain Tumor Segmentation	"The results are shown on BraTS 2018 dataset, where as BraTS 2021 is also available now. The experiments can also be extended.
There should be a subsection to discuss complexity of the proposed mmFormer architecture in reference to other methods.
The last line in conclusion section seems to be is contradictory. Need a rewriting. (""Our method gain more improvements when more modalities are missing...""). However, from Table 1, results are not best when there is only one modality is present and three are missing."	No model size / number of parameters is provided and comparison with the Adversarial Co-training Network in ref [21] is not provided. Though ablation studies were performed to understand the contribution of various components of the model, it is hard to discern how modeling the long range interactions with transformer modules is improving the segmentation (does it help with better segmentation of larger lesions and/or does it have any effect on smaller lesions?)	"The motivation is clear and the significance is strong both in clinical and pure research. However, it is difficult to know why transformer is chosen. As a popular work, transformer is well used in several medical image processing tasks. From the sentence that ""the dedicated Transformer for multimodal modeling of brain tumor segmentation has not been carefully tapped yet, letting alone the incomplete multimodal segmentation."", it seems like that the work is proposed in order to apply a new method on a specific task instead of that the task needs the method to get improvement. Of course, it is easy to understand we try many methods and find one is good. But why it is good and why you choose it should be clearly presented in the submitted paper.
The related work should be carefully abstracted. Compared with brain tumor segmentation, incomplete modality of brain tumor segmentation is a specific field, in which there is not that large number of papers. You may categorize and comment methods following their main ideas and talk about the pros and cons class by class.
Why do the U-HeMIS and U-HVED are chosen as your benchmark? Is it because they are all latent space based model?"
322	Modality-adaptive Feature Interaction for Brain Tumor Segmentation with Missing Modalities	"It seems that the comparsion in Table 1 is not under the same cross-validation slpit, which may result in unfair comparision. The results of some methods are directly extracted from their papers.
Significance test is missing. Since the proposed method has small improvement in comparision with RFNet, significance test is recomended.
Disccusion about the limitations of this method is missing."	"1) As the authors mentioned in Graph representation G=(V, E) in which E denotes the adjacency edge matrix representing the relations between nodes (modalities), the r_{ij} is finally computed using the formula (2) which is the output of Leaky Rectified Linear Unit by inputting the concatenation of voxels of v_i and v_j, why the authors defined the edge in such form? What's the main purpose to define the edge in such a form? What are the advantages? All of these are not described in the paper and the relative experiments are not conducted.
2) The experimental results showed that the highest accuracy is obtained by using all of these four modalities images including F, T1, T1c, and T2, which suggest that the proposed MFI module seems to haven't much better effect.
3) In Fig. 1, the GN hasn't a description."	Limited experiments: The proposed method was only applied to the BraTS 2018 dataset which is not the latest BraTS dataset.
323	ModDrop++: A Dynamic Filter Network with Intra-subject Co-training for Multiple Sclerosis Lesion Segmentation with Missing Modalities	"The design of the dynamic head is trivial and lacks proof, the filter scaling matrix is just served like a normal linear layer without much more modifications. Thus it is hard to justify the usefulness of such a matrix to scale the features extracted with missing modalities during training and inference phase.
The loss used for features extracted after dynamic head are treated equally, however, the features extracted by more available modalities should have more confidence. The use of SSIM loss seems not appropriate for features in latent space, why not using the perceptual loss? Also, the reason to use fixed layers after feature extraction was not justified.
The experimental result further proves my opinion where the performance of the proposed method dropped significantly when FLAIR is absent from the training and inference. While the performance was comparable to full modality setting when only FLAIR is available. This observation only proved that FLAIR is crucial for MS lesion segmentation tasks but not the effectiveness of the proposed method. It is actually true that FLAIR is important for lesion identification tasks in real clinical settings and the proposed method was not able to gain performance improvement. Furthermore, even Contrast-Enhanced (CE) sequence was provided in the dataset, I don't think this sequence can be used in this setting since CE is used to detect the active lesions. For the general lesion segmentation task, the information provided by CE on a MS lesion can be different (bright for active lesion and dark for MS lesion), thus including this sequence may lead to a negative performance gain (as shown in Table 1).
The results of ISBI dataset showed no significant difference in performance between full and semi modality. The discussions on this observation and performance differences between two datasets were not found."	"-The evaluation of the proposed method is somehow weak. Two publicly available datasets are considered, but the ISBI one includes only 5 patients. Throughout the manuscript there is no mention of the validation dataset. The core objective of this work is to increase the generalizability of MS lesion segmentation approaches. Thus, the two datasets considered could have been pooled together to examine the effects of the proposed framework. 
-The lesion deliniation heavily depends on the sequences analyzed by the experts while performing the manual lesion annotation. Very often the only sequence used for MS white matter lesions is the FLAIR, and thus results of the automated approaches are considerably worse when FLAIR is a missing modality. This should at least be discussed in the manuscript as it is quite evident from Table 1."	I didn't notice any major weakness in the paper. The paper is well written and references have been made to relevant prior work.
324	Modelling Cycles in Brain Networks with the Hodge Laplacian	"** Although we appreciate the algebraic methodology proposed by the authors, the motivation of the proposed method is not clear. Computationally, simply using the non-MST edge and the MST will give you a cycle. This way of computing a cycle basis with a MST is very efficiently. It is not clear why we need the Hodge Laplacian and its eigen decomposition. Plus recomputing the Laplacians and their eigen vectors for each non-MST edge will be extremely expensive.
** There are already methods like persistent homology to differentiate networks with different topology. What is the benefit of the proposed method over persistent homology? Some empirical comparison with persistent homology features might need to be provided to prove the proposed method is better and necessary.
** More details in the validation experiment need to be provided. In equation (6), the authors define the difference between two groups with the same nodes. However, they do not mention how to calculate the difference between two different networks, like the difference between group 1 and group 3 in Fig.2. In such case, how to match the nodes between two groups is not trivial, even when the two groups have the same number of nodes.
** There are no baselines in the application experiment. Although we can see the proposed method can differentiate the male and female brain network successfully, the authors did not provide any baseline to compare with. Such baselines can include the other metrics used in validation and also methods like persistent homology. For persistent homology, here is a very relevant work that can be considered to be a baseline,
T. Songdechakraiwut, L. Shen, and M.K. Chung. Topological learning and its application to multimodal brain network integration. Medical Image Computing and Computer Assisted Intervention (MICCAI), 12902:166-176, 2021."	Unfortunately, one potential major error was found (#1).	The authors could have better demonstrated the motivation for cycles in the brain network by providing more evidence in the neuroscience literature. Although this feature can be discriminative, the intuition behind finding some cycles with carriable length may not be very clear from a medical analysis standpoint. Also, there could have been a comparison to 0-cycles features based on the same approach to see the significance of 1-cyles are more important and meaningful.
325	Momentum Contrastive Voxel-wise Representation Learning for Semi-supervised Volumetric Medical Image Segmentation	"My primary issue with this paper is the structure and writing, which make it very hard to exactly parse how each of the proposed objectives operate, with confusing or in parts missing notations and definition.
In particular,

No additional information on the supervised loss is given, only that it comprises a cross-entropy and dice loss. Are they weighted? Is it a smoothed dice loss variants? Are any additional hyperparameters introduced here?
Throughout the work, the EMA network is treated as a separate entity and effectively independent teacher network (see e.g. Fig. 1, and separate student/teacher notations throughout the text).
In general, it is incredibly hard to understand where and how each objective is applied in the overall quite expansive pipeline setup - the notation of high- and low-level contrastive loss is introduced in the beginning of section 1.1 and 1.2, but with hardly any motivation. If I understand correctly, low- and high-level refer to applied in the latent spaces of the first encoder as well as the last encoder, which however is described as being of ""similar architecture"". What then makes the contrastive losses high- and low-level? And since parts of Fig. 1 and Fig. 2 are never specifically referenced in the paper it becomes quite hard to understand where what is applied. Similarly, equations 1-3 would benefit from more indices to much more precisely highlight which components are contrasted against which.
As it is not made clear throughout the paper - are stop-gradient operations applied anywhere? Or does backpropagation also happen into the EMA teacher network?

In addition, it is not entirely clear what the main novel contribution is - MT[21] introduces output matching to an EMA target, component (de-)correlation along the batch axis has been introduced e.g. in Barlow Twins (although with different formulation) and the components in the supervised objective are commonly used in literature. And while the authors claim their regularization to be ""anatomy""-informed, it is not entirely clear how this is reflect in the objective? Is it the fact that the volume cube is broken down into voxels?
There is also no experimental support for the claims that CVRL makes the proposed setup less prone to dimensional collapse - just looking at the baseline Dice/Jaccard performances, while it is overall worse, there is no indication that each pipeline component does what the authors claim it should be doing.
Some smaller issues:

Fig. 3 is incredibly cherrypicked and makes CVRL stand out disproportionally.
Table 2 techincally misses L^low + L^con and L^high + L^con references. Why where they not included?"	"On page 8, it is unclear what is meant by ""efficacy of both inter-instance and intra-instance constraints"". Both terms were mentioned before, but what do they mean? Please describe this.
In Figure 3, what do the red and blue lines stand for.
In Figure 2, the color coding (yellow, red, green and blue) is not clear.
Describe what is meant by the hardness-aware property. While it is stated that the hardness-aware property is inherited, it is unclear what advantages this provides and why.
-In table A2 in the appendix, I guess these are the results for tha LA dataset. This must be written in the caption.
-In the appendix, I would make two seperate sections: Appendix A for additional results, and Appendix B for the proof of the hardness-aware property."	"The main weakness is having similar idea that existed for pixel-level contrastive learning with memory bank [1]. Figure 1. specifies the low-level contrastive loss and high-level contrastive loss. However, it is hard to follow the loss defined. Another weaknesses is lacking of contrastive learning baselines and citations for segmentation task.
[1] Alonso, Inigo, et al. ""Semi-supervised semantic segmentation with pixel-level contrastive learning from a class-wise memory bank."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021."
326	Morphology-Aware Interactive Keypoint Estimation	"Part of the explanation and discussion in the experiments section is confusing. The definition of manual revision in Fig. 4 is not clear, the user interaction is also a type of manual revision.

The performance of the proposed approach is close to the general method RITM [16]. As shown in Fig. 4, the MRE for increasing the number of user interactions, the curve of RITM is similar to the one of the proposed method. It is the same in the right figure as well in Fig. 4. Considering the proposed method is tailored for X-ray vertebra images, large improvement is expected but not observed.

It is shown that the proposed method can correct most of the wrong keypoints based on the input where a user corrects only one point. However, which keypoint should be selected and corrected by the user to achieve such an efficient user correction is unclear.

The limitations of the proposed approach and the directions for future research are not discussed in this submission."	In general, the amount of detail presented for this algorithm is not consistent throughout the paper. While some components of the network are explained in high detail, crucial information about the network architecture and the training process are completely missing. For instance, previous prediction of the network appears to be a separate input channel to the network while it's not clear how this is imbedded inside the training workflow. Furthermore, aspects related to the gating network and the morphology are loss are not well communicated therefore one may find it difficult to fully understand the underlying methodology behind those network components. Additionally, the writing tone and clarity can be improved to help with the general comprehension of the paper. For instance, the authors' statement about the NoC_5@3 and FR_5@3 is extremely hard to grasp due to subpar writing quality.	"There are some issues in the mathematical & formal description of the method, which I would encourage the authors to revise (see below).
The concept of ""low-variance""/""high-variance"" landmarks is only described superficially (e.g., was there a specific threshold selected? how?).
The authors do not evaluate nor discuss ""noisy"" interactive inputs (or repetitive corrections of the same point) by real users, but only work with ""simulated"" interactions (based on the ground truth).
Only mean results are reported, without variance and / or effect of repeated training. Failure cases are not analysed further."
327	Moving from 2D to 3D: volumetric medical image classification for rectal cancer staging	"The performance of the proposed method is only marginally better than the baseline 2D model. Specifically, a variety of network structures are tested but only the f-rMC5 model outperforms the baseline f-R2D model.
A few details are missing. See costructive comments for justification."	"-The introduction should contain more details about the open research problems. The introduction should rewrite with a general overview of the study, such as what are the rectal cancer stages? What challenges face the physician to distinguish between T2- and T3-stage rectal cancer? How did you solve these challenges? 
-why did the authors distinguish between T2- and T3-stage rectal cancer, not other stages?

Is the manner of rectal cancer treatment depend on the cancer stages? Which one?
you mentioned in the manuscript, ""Our goal is to solve a clinically challenging problem: to distinguish T2-stage rectal cancer from T3-stage rectal cancer with MRI."" Then please compare your automated accuracy and the clinical accuracy for this problem.
-Please add a graph to show the value of the loss function during training. This will reflect the behavior of the new loss function.
Is the segmentation for rectal cancer good preprocessing step for this classification?

You didn't define the parameter p in equation 1.

what are these appreciations of Acc(T2) and Acc(T3) mean? Why is there accuracy for T2 and T3 separately?

would you please add the ROC cures?
I think the conclusion should be more simple."	"1, except the machine parameters, the MRI database of rectal cancer  need more details about tumors, patients, and data, such as tumor size, tumor volume, patient ages, patient gender, slice thickness, voxel size and so on.
2, Tome notations of x+,x-,f+ and f- are required to specified in for Eq(3)
3, The output of Layer4 should be some 2D data. How to conduct 3D convolution over 2D data on Layer5 is an import issue which needs more details in section 2.2.
4, The experiment section mentioned the loss many times. But all loss values of different models are missing  in Table 1 and Table 2.
5, I disagree the assumption of video and VMI's similarity since video is considered as 2.5 dimensional data while volumetric data is one kind of 3D data.
6, 3D volume contains more information than 2D slices. The results over 2D are better than 3D data in Table 1 which needs some analysis.
7, This comparisons in Table 3 occur among different datasets.
8, Table 1, Table 2 and Table 3 do not give their parameters adopted in experiments."
328	MRI Reconstruction by Completing Under-sampled K-space Data with Learnable Fourier Interpolation	"A relatively small training 2D image dataset (300 slices for training and 21 slices for testing) has been utilized to investigate the performance of the proposed method. Its recommended to validate the claims on a larger dataset as concluding that the proposed method outperforms all the other benchmarking methods will not be a valid statement, otherwise.
The article needs fixing some grammar mistakes found in a few places. Proof reading is recommended."	"- The technical novelty is not well-explained and several notations are missing or need clarification. Therefore, it is hard to appreciate the level of novelty.
-  Authors highlight in several places new optimisation schemes such as deep unrolling and PnP methods. However, it seems like the authors somehow are confused in the terminology.
- The experimental comparison seems limited and a major drawback on the paper is the lack of discussion behind the fundings."	The interpolation technique which is the strength of the paper (and method) should be more detailed. Additionally, the method evaluation is not enough comprehensive to ensure the generalization and reproducibility of the findings of the paper (only one dataset was considered, the dataset size is not composed of 3D volume MRIs but 2D MRI slices, the testing data set is small, no cross-validation was performed, etc).
329	mulEEG: A Multi-View Representation Learning on EEG Signals	The proposed idea is not very reasonable and has not been fully justified in the manuscript.	"Did the authors try other techniques for learning except for RESNET?
Which data augmention bring better result? I don't see any discussion on this. Please clarity more on this."	More ablation studies about data augmentation are recommended.
330	Multidimensional Hypergraph on Delineated Retinal Features for Pathological Myopia Task.	"*	This paper has limited technical novelty, because the only contribution is using the hypergraph learning on the features extracted from other machine learning and deep learning techniques.
*	The authors use ResNet34 encoder to compare with the performance of hypergraph learning. What would be the performance using other state-of-the-art CNN, for example ResNeXt-50? ResNeXt-50 provides better performance in comparison with ResNet"	"The itroduction section needs to be more elaborated by discussing better the limitation of different related works. I see that it does not cover well papers related to the problem to solve.
The authors mentioned that the use of CNN embeddings to generate statistical features is limited to the loss of interpretability and they did not explain that. I expected more discussion as well as mentioning related works about this point.
The choice of the loss term R_emp(M) is not explained. What is the reason of suming Frobenius norm and L1 norm, why not one simply using one of them?
The edges weights were defined as the similarity between retinal characteristiques. How such similarity was computed? The definition of the hypergraph should be better explained in the method section. If an euclidean distance was used, what is the dimension of its input variables? How do the authors define mathematically the retinal carachteristiques?
To evaluate their method, the authors used a 80/20 split strategy for training and testing on the dataset. I expected a cross-validation evaluation strategy. How are they sure their model is not overfitting?"	"Main weaknesses of the paper:

Overall, the performance improvement of the proposed approach is minor.  One potential advantage of the proposed algorithm is improved interpretability, which the authors discuss; however, they don't investigate interpretability of the method on this dataset or compare interpretability against the standard CNN-based approach.

Figure 1 caption: there is no explanation of what the symbols (circle, x, square) correspond to in the data.  From the text, I assume it is the level of pathologic myopia, but this should be clearly labeled in a legend.

p-values should be reported as, for example, p-value < 10^-4.  They should not be reported as p-value = 0.000

Minor weaknesses:

Why is AUC score missing for RGB CNN in Table 1?

page 6 type: ""modals"" should be ""models"""
331	Multi-head Attention-based Masked Sequence Model for Mapping Functional Brain Networks	"Limited discussion of designed models
o	Because the proposed method was based on a deep learning model, the hyperparameter of the model is a crucial part when training the model. So, it would be more helpful to understand the effect of each component in MAMSM such as the contribution of multi-head attention, percentage of masking, number of hidden layers in the encoder, and decoder of feature selection layer. 
o	Also, it would be good to explain the training procedure and parameters in detail. It was confused about how the input dimension was changed during training, and what is the exact number of features in the feature selection layer.
Limited evaluation
o	Since there was no gold standard method to extract functional brain networks from task fMRI, the author compared the results with GLM. But, as they mentioned in the introduction part, GLM also has some limitations when detecting functional brain networks. 
o	It also would be helpful to describe simply how functional brain networks were extracted from GLM. 
o	Additionally, using only 22 subjects in HCP data could be viewed as one weakness of this paper. HCP provided about 800 subjects' task fMRI data, however, the authors used a limited number of samples. So, using a larger number of samples would make this paper more robust and impressive."	"a) Ablation studies of proposed loss function are needed. In Table 2, the MAMSM has a higher accuracy probably because of the Loss_cos which makes the first six features of the encoder output close to task designs. For SDL, the method in this paper should be included for comparison :
Zhao, S., Han, J., Lv, J., Jiang, X., Hu, X., Zhao, Y., ... & Liu, T. (2015). Supervised dictionary learning for inferring concurrent brain networks. IEEE transactions on medical imaging, 34(10), 2036-2045.
b) The writing qualilty should be improved with necessary details. For example, why mask approximately 10% tokens? Why add continuous masks besides random discrete masks? What is the ratio between random mask and continuous mask?
c) The recent HCP S1200 release have more than 1000 subjects. Even the HCP Q1 release have 68 subjects. Why only 22 subjects are included in this study. How the training/validation/testing datasets are split among these subjects?"	"1). Validations
It would be biased to validate a supervised method with unsupervised methods, such as SDL and ICA;
2). Discovery Limits
Obviously, the authors employ an autoencoder ( a deep neural network) with embedding techniques to reveal the brain networks. The reviewers are very curious about the hierarchical structures identified by the proposed method.

Some mistakes in mathematic formula

In Eq (6), given all variables are matrices, it should be the Frobenius norm.

The arbitrary comparison

In Fig 4, the background of brain images is not consistent. For instance, the results of GLM mapping to the T1 weighted images but other background images are different.

Some artifacts are reported in Fig 5(b)."
332	Multi-institutional Investigation of Model Generalizability for Virtual Contrast-enhanced MRI Synthesis	"Authors should provide data description in detail. Are all T1w, T2w, and CE-MRI images from the same institute obtained using the same MRI scanner with the same pulse sequence? Do all institutes use MRI scanners from the same manufacturer? These descriptions will help readers better understand the difference in MR images from different institutes.
Another suggestion is to consider adding a reader study to evaluate the quality of synthesized images."	"The strength of the paper is mostly based on the multi-centric cohort the authors gathered. For improving the generalization power of the model, no technical developments were shown in this study. The clinical application of the study is important but seems not novel.
The proposed method was not compared to other state-of-the-art methods.
The method evaluation lacks precise statistical analysis."	"1)There is no statistical analysis about the significance of the testing cohorts so the authors can strengthen the final conclusion about the training in different cohorts and generalization.
2) More details about the institutes and the difference of the cohorts' modalities, resolution quality needed.
3) minor typos (conclusion needs capital C, reference of L1 loss or equation is needed)"
333	Multimodal Brain Tumor Segmentation Using Contrastive Learning based Feature Comparison with Monomodal Normal Brain Images	"The only comparison technique is nnUNet. The Brats2019 data labels were not compared for each, so it is difficult to compare it with other techniques of the leaderboard.

The results show that the proposed method performs well on binary segmentation task, but how does it perform for multi-class segmentation? This is what I am more concerned about because it seems difficult for normal images to help distinguish the types of tumors. Please provide some more details about it.

Interestingly, the baseline 1 has already outperformed nnUNet even though it consists of a general Unet structure. It is necessary to explain how such a result can be obtained.

Compared to the baseline methods, the improvement of the proposed method is not large."	"The proposed method has a lot of components such as 1) segmentation network, 2) normal tissue network, 3) IntroVae, and 4) feature alignment module. This makes it harder to implement the method, especially because IntroVAE needs training just for itself.
The authors claim to use Contrastive Learning to align the features, but the employed method is SimSiam, which is a non-contrastive learning method [1] (i.e., it only uses positive pairs). This requires adapting the text and title.
The proposed method only deals with whole tumor segmentation task as a binary task. But, in BraTS, it is well-known that a brain tumor can be divided into 3 major regions: enhancing tumor (ET), Tumor Core (TC), and Whole Tumor (WT). The proposed method would be stronger if it was shown to address these classes.
The results are obtained in a five-fold cross-validation way. Therefore, we can assume that they are computed from the held-out fold in each run. This can be susceptible to overfitting and overly optimistic results. In other words, a separate test set would be important."	The performance of the whole pipeline depends on the first step of using IntroVAE model. Quantifying the effect of this step separately will further improve the strength of the paper.
334	Multimodal Contrastive Learning for Prospective Personalized Estimation of CT Organ Dose	"Some details on the TCM map generation are not clear:
What does DCT stands for in this case?
Authors describe that the doses are modeled as the weighted sum of single-view doses with weights coming from the tube current profile.
How are the weights calculated from the tube?
How is the contribution of each organ calculated for each view?
Was the training and test set of scans split randomly?
The augmentation was performed only in scale of the same TCM maps. Differences in the scouts would be recommendable.
How are the inconsistencies between scouts and CT scans palliated?"	"Many ideas are combined (TCM map generation, contrastive learning in the profile domain,""multimodality""), which makes the reader wonder what is the core message of the paper. 
It is not clear if the gain between pure image-based prediction (Scout-Net) and ""multimodal-based prediction"" (Scout-MCL) is actually due to the multimodality, as there is not an ablation study with respect to other image inputs of the ScE branch that are used by Scout MCL (i.e. the TCM map).
More importantly, I wonder if the DwE is really useful as  contrastive learning could also be done directly in the scout image domain
I Struggle to understand to what extent this information is multimodal, given that the Dw profiles are extracted from the scout images. Unless I am mistaken (which could be the case given my non proficiency in CT technicalities), this is more here a combination of a projected representations the scout views rather than indeed modalities that are combined.
the whole method describing the generation of synthetic TCM maps from scout images is unclear. It is said that ""we generated synthetic 2D TCM maps by fitting profiles from DCT basis images to TCM profiles as is shown in Fig. 2"", which shows only a result.
The paper is very technically oriented towards CT and thus requires expertise in CT that I unfortunately do not possess completely."	"An ablation study is performed only for the CL and some augmentation, not for other design choices such as the inclusion of body size and TCM in the pipeline.
The dataset is rather small and it is not clear whether it originates from a single or multiple centers."
335	Multi-Modal Hypergraph Diffusion Network with Dual Prior for Alzheimer Classification	"1.The result lacks annotation. The discussion of the differences between the work and HF [23] HGSCCA [24] HGNN [10] DHGNN [15] is insufficient and unclear.
2.The result of HF in this work is ambiguous, as different parts in the HF [23] describe it differently.
3.In section 2,the terminology and wording chosen are in parts particularly intricate, which further reduces the readability that is already affected by somewhat deviant grammar. Considering the presented approach is mathematically elaborate , the work's reproducibility would be greatly enhanced if if it was written such that the international community could follow more easily, and the reception of a paper could likely be improved if it was easier to read.
Minor revisions: 
1.Problem with abbreviations 'late mild cognitive impairment (EMCI)',Please change it to'(LMCI)'.
2.In 'Fig. 3: Performance comparison for the four classes case',the vertical axis'HGGN'does not match what is mentioned in the text."	"Firstly, the methodology section of the paper is not presented in enough detail to be easily understood. For example, in 2.1, the authors propose to introduce T transformations without giving detailed reasons why they should do so and what the disadvantages would be if they did not.
And for non-imaging data, the authors hold the view that creating a subject-phenotypic relation can mitigate the neglect of perturbing directly the data. This needs proof to justify. In Fig. 2, the authors do not go into detail about what each of the different colored nodes in the diagram represents and what the connecting lines between them indicate.
Secondly, the authors did not cite the most recent references for hypergraphs, e.g., literature [30].
Thirdly, the experimental part of the paper is unconvincing. The authors do not list the SEN and PPV metrics for the binary classification task in the Supplementary. Moreover, the authors' experimental results surprisingly show that their proposed method is far ahead of other algorithms for both binary classification and multiclassification tasks (e.g., 81.69% accuracy on the AD vs NC vs EMCI vs LMCI classification task), and the authors' explanation of this result is unconvincing.
What's more, the author's writing is hardly satisfactory, with frequent grammatical errors. For example, Page 1: ""has show have shown"" should be ""has shown"". Page 2: ""we introduce a a more"" should be ""we introduce a more"". ""pLaplacian setting"" should be ""Laplacian setting"". Page 6: ""EMCI"" should be ""LMCI"". ""5e-2"" should be "" "". Page 7: Table1 ""SEN PPV ACC"" should be ""ACC SEN PPV""."	"1.For the introduction, improving predictive uncertainty is not clear.
2.The proposed method is reasonable, yet hypergraph diffusion module is not clear, more in-depth analysis will be better.
3.Some typographical and grammatical improvements should be made, such as the performance of ""ours"" in Table 2."
336	Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training	No major weakness detected.	"Some key details are not explained well. 
In section 2.2, representation selection for reconstruction paragraph, if you use the image features obtained from the k-th layer to reconstruct the input image, how will the last (N_m - k) layers be trained? How will you train the feedforward sub-layer of the N_m-th layer?

There are some typos. 
(1) In section 2.1, vision encoder paragraph, the dimension of $p_n$ should be $p_n\in^\mathbb{R}^{P^2\times C}$.
(2) In Fig. 1, for text, it should be ""Text Embeb"" rather than ""Image Embeb""."	"For different masking ratios, there is no experiment to vertify your perspective and lack of experience to explain what ratio is the optimal choice.
For the ablation study of 'Effectiveness of different layers to perform MIM', The reason for ""layer3 is the best, but the accuracy of layer4 to layer6 declines"" in the experimental results is not clearly and fully explained and a bit far-fetched.
There is a lack of experiments to vertify your designs to make this simple approach work for decoder designs.
Initialize the vision encoder with CLIP-Vit-B, which is equivalent to using the CLIP dataset. In experiments, other relevant methods seem not to be used in this way, leading to unfairness."
337	Multi-modal Retinal Image Registration Using a Keypoint-Based Vessel Structure Aligning Network	"The proposed method only tested on narrow-field retinal images. More challenging cases needed to be tested.
While the paper validates the proposed methods on three datasets, the baseline methods are limited. See other suggested baslines in ""comments for authors"""	"The method is trained only using synthetic data. Although this is quite understandable, given the difficulty of obtaining a large dataset of correctly manually annotated data utilizing multiple modalities.
Synthetic data is created using homographies. This works mostly for images with a narrow field of view. This is shown in works like [C. Hernandez-Matas, X. Zabulis and A. A. Argyros, ""REMPE: Registration of Retinal Images Through Eye Modelling and Pose Estimation,"" in IEEE Journal of Biomedical and Health Informatics, vol. 24, no. 12, pp. 3362-3373, Dec. 2020, doi: 10.1109/JBHI.2020.2984483]. where it was shown than utilizing an eye model is more accurate than simple homography for images with a FOV over 40o
While the method is geared to multi-modal registration, it would be interesting to see performance in single mode registration, as there exists at least one publicly available for evaluation registration performance on fundus images, the FIRE dataset that was also utilized in the paper mentioned above
Color retinal dataset utilized contain only low resolution images (576 x 720), when it's almost been a decade where high resolution images (at least 2500x2500) are widely available, so such low resolution images should not be considered enough anymore."	The interpretation of symbols in the formula needs to be described in more detail.
338	Multi-Modal Unsupervised Pre-Training for Surgical Operating Room Workflow Analysis	"Certain details/terms not explained in the paper
No real discussion of results
Some grammatical errors, the paper would definitely benefit from another read-through"	"-Online V.S. Offline: This work opts for the online manner to do the pretraining. However, as far as I know, the SwAV is not comparable to the other offline methods, such as MoCO. It is not clear why the online method is so necessary.
-Fair comparison: This paper is about multi-modal unsupervised pretraining, however, the compared methods, i.e., pace prediction, clip order prediction are based on the uni-modal video frames, as far as I am concerned. This is not a fair comparison, since I will be confused whether the multi-modal data or the pretraining process proposed helps to improve the performance."	"the proposed model relies on an architecture similar to [4]. It seems that the technical contribution is limited.
[4] relied on single modality and used different augmentation to achieve the same goal, I believe the author should have used compared their results with [4] as another baseline.
while the proposed model used both intensity and depth images, the baseline model only took intensity images as input. I think it would be interesting to establish another baseline that relies on the same data as the proposed model as depth data was used during pretraining.
[16, 30] have studied the effect of using bot RGB and depth images on improving performance. The author could have used the both modalities when the models are trained for the task.
there were multiview data. Were the cameras calibrated? As multiview data was available, it would be interesting to look at this aspect as well."
339	Multimodal-GuideNet: Gaze-Probe Bidirectional Guidance in Obstetric Ultrasound Scanning	It is not completely clear to me if the paragraphs in the Methods section explain existing ideas that have been incorporated into the proposed system, or if they are also contributions. It would be nice if this distinction were made more clear.	In figure 4,  Multimodal-GuideNet* reports the error of the best generated gaze pointthat is closest to ground truth for the Multimodal-GuideNet procedure. It would be helpful to see this parameter reported for the Gaze-GuideNet procedure as well.	Not much weakness from my perspective, though the paper is a bit compact with a lot of details not well elaborated, but I guess mostly due to the page limit of the paper.
340	Multiple Instance Learning with Mixed Supervision in Gleason Grading	"Limited pixle-level labels are required for the setting. While the pixel-level labels might not be available.
What would be time cost compared with just using the slide-level supervision."	"By using mixed supervision transformer, the authors achieved the best performance on the SICAPv2 dataset but there was only a marginal improvement. 
The improvement they obtained seems to be due to masking method, not due to the new design of their method, questioning on the novelty of the method. Hence, the effect of mixed supervision is not so obvious as considering the effect of masking."	"Except for the instance label generation (Section 2.2), the novelty of random masking in Section 2.3 is very limited, which is quite similar to MAE [4].
The effectiveness of mixed supervision is not solid in the experiment.
The experiments are performed on a small dataset, which is not fair for slide-level baselines. The algorithm should also be evaluated on a larger dataset comprehensively."
341	Multi-scale Super-resolution Magnetic Resonance Spectroscopic Imaging with Adjustable Sharpness	"Fig. 1 (b) is not clear for me. The function c(n) that outputs scaling factors(C_in x C_out) from n (input resolution) and E(m) should be deeply discussed. For instance, what is the dim of inputs of fc? Why outputs of fc could be acted as the upscale factors for filters (seems there missed a element-wise product.). Then, MCM conducted the next fc to represent \lambda into 2 x C_out in CIN. However, the \lambda is numerical scalar. The part of MCM is hard to follow and are not sufficiently motivation.These should be included in the corresponding ablation. What dose upscale factors for filters do? I don't find upsampling or deconvolution operations. What does ""as ground truth images contain missing pixels due to quality-filtering, we remove those pixels in network outputs ..."" mean? What the differences between quality-filtering ahead of network inputs and quality-filtering after outputs?
It is strange for embedding 7 metabolites (text data) as the auxiliary variables in MCM. In Sec 2.2, E(m) is a trainable embedding layer that converts words to numerical vectors. The analysis on E(m) is excessively brief, not providing the complete description on specific operations, effects, sufficient discussions. This affects the quality of writing and the clarity of the presentation, also, scores.
3.""The adversarial loss uses a discriminator (4-layer CNN)"" , Whether Wasserstein GAN will be trained alternatively together with proposed networks. If not trained, there should lead to a mistake. In this time, how Wasserstein GAN train a 4-layer CNN without a fc layer.
""statistically insignificant p-value "" don't have illustration and reflect on the specific results. I can't follow its meanings."	"1: The idea of the paper that super-resolve the image in multi-scale manner is not sufficiently new. There even have been some works arbitrary scale super-resolution, such as ""Learning A Single Network for Scale-Arbitrary Super-Resolution"", ""Arbitrary Scale Super-Resolution for Brain MRI Images"".
2: The experiments are performed only on H-MRSI dataset, but the number of images are too small to obtain meaningful training results. There are only 15 3D samples. The authors are encourage to estimate the proposed model on another dataset.
3: From Table 1, the proposed method doesn't improve significantly than the unconditioned method but consumes more time and takes more memory for the parameters."	"The numerical results are not so significant compared to the existing multi-scale methods.
The visual comparison to existing blind SR methods should be also included."
342	Multiscale Unsupervised Retinal Edema Area Segmentation in OCT Images	"weights in Eq 6 unclear. 
extra examples in supplementary material would have been nice. 
'failures not discussed (some 'no's in the reproducibility checklist)
so open questions remain, see 8."	"The design of the proposed architecture is heavily reliant on the existing DCCS approach. Section 2.1 consists of excerpts from the original DCCS study and hence, not a contribution of this manuscript. The main methodological contributions of the manuscript are actually presented in Sec 2.2 and 2.3.
There are too many grammatical mistakes throughout the manuscript, which is affecting the readability."	"Formulation for the main novelty, which is scale-invaraint regularization for clustering, is not properly explained.
More experiments should be performed on lesion types of significant scale differences to validate scale-invariant capability of the network.
Failure analysis missing."
343	Multi-site Normative Modeling of Diffusion Tensor Imaging Metrics Using Hierarchical Bayesian Regression	"Normative modeling (https://dx.doi.org/10.1016%2Fj.biopsych.2015.12.023) and hierarchical bayesian regression framework are not particularly novel.
Some aspects regarding the clarity and organization needs to be improved significantly. A general introduction about the Hierarchical Bayesian Regression framework needs to be provided first, followed by mathematical description in Methods. Texts introducing about 16pDel still appeared in the Methods section which should be moved to Intro. All the ROIs from the JHU atlas were provided in abbreviations without being fully spelled out in their first occurrence. Lastly, at the end of Results, it was stated that ""For additional data, please see the Supplements,"" but no Supplementary materials were found."	"The motivation and evaluation of the study are limited in most clinical applications.
The explanation from the results about genetic-driven neuroscience seems a bit far-fetched."	"There are some weaknesses worthy to point out:
1) The language is generally good, but thee re some typos, e.g. ""Sire"" instead of ""Site"" in the first row of table 1. Missing space in Site4 in row 4 of table 2. There are a few more issues like this that need to be fixed. Please use a spell checker tool or something like grammarly.com to fix this.
2) In the 3rd paragraph of section 2.2, the authors claim: ""Consequently, the algorithm does not yield a set of ""corrected"" data, i.e., with the batch variability removed, as in ComBat. It instead preserves the sources of biological variability that correlate with the batch effects."" Firstly, I would not call this an algorithm, but a model. If the biological variability correlates strongly with batch effects, then I doubt that you will be able to preserve them.
3) Links to software should be in footnotes, e.g. PCNtoolkit.
4) The paper suffers a bit from lack of novelty. This is mostly applying known and published methods to a new dataset, trying to solve a problem that many have attempted before.
5) Captions of images are lacking. There are so many acronyms, these should be spelled out in the caption, e.g. fig 1.
6) The y-axis numbers in Fig 1 should be numerically sorted, not alphabetically, the order is not 1,10,2,3,4...., 10 should appear at the end. It is not explained in the caption what the colors in the image mean.
7) Fig 2 also needs acronyms spelled out."
344	Multi-Task Lung Nodule Detection in Chest Radiographs with a Dual Head Network	In multi-task training, there is an assumption that the training data contains images that may or may not have nodules. For those without nodules, the second task on nodule detection will not contribute to the model training. If most images do not contain nodules, what influence will it has on the overall model performance?	The method selected in this paper is resnet-18. The author should prove why resnet with 18 layers is selected in the experimental part. In addition, there are few methods compared in the experimental part.	"The reason for properties of pulmonary nodules need adopting deformable convolutions is unclear
How to use the two results is unclear.
The experiments comparison with state-of-the-art on pulmonary detection performance is not shown."
345	Multi-task video enhancement for dental interventions	The evaluation results are not impressive enough in a couple of aspects.  Firstly, the ablation studies show that the performance degradation after disabling one module is often not significant enough, which makes me wonder if the architecture has created enough synergy among these tasks. Secondly, based on the results, it looks to me that simply having several state-of-the-art models run in parallel with the original image input for processing could result in higher FPS and similar or better performance compared with the proposed method. The authors also mention that MHN on the original video frame works better than the proposed method and will become significantly better if the high-definition image is used instead. All the above may suggest that there is still room for further improvement in the proposed method.	"It seems the restoration here is with an emphasis on video deblurring, while the author is also encourage to have some brief discussion on video stabilization, as they are closely correlated and discussed together. Actually some blurs are introduced by the camera jittering. The stabilization is related to homo-estimation and could be viewed as a smoothing task using estimated transformation. 
reference: Yu, Jiyang, and Ravi Ramamoorthi. ""Learning video stabilization using optical flow."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020."	"Since the authors indicate that the method runs in near real-time, any indication about the computational time and space with respect to the current state-of-the-art would be useful
Model is very task-specific; any indication if this can be used for other similar application would be useful."
346	Multi-TransSP: Multimodal Transformer for Survival Prediction of Nasopharyngeal Carcinoma Patients	"The paper needs more discussion on the results (e.g. good/bad examples by case studies, interpretability of model's decision, how the model can be further improved) to help interpret the impact of the work and interpret what 0.02498 MSE and 0.6941 CI actually mean in terms of quality.
The novelty of the work is limited to applying Transformer and multimodality on a new task, which is ok if more clinical insights can be provided to interpret the model performance as mentioned above."	"Demographics and details about the non-imaging features used should be added
The writing would profit greatly from a thorough grammar checking."	"The details of the use of the transformer model are inadequate. For instance, from the description of the input data in section 2.2, it is unclear what the exact ""sequence"" is for the input to the transformer. Is it the different feature channels for the same voxel, or the different z-slices of the image? How are the text embeddings included in the input? Where does the expanded text feature join the imaging features? What is N in the ""N sequences""?
What is a space embedding? Why is it needed? How is it constructed? How does it capture the space? Is this a trainable embedding, or a pre-defined embedding?
Why do the authors make use of mean-squared error loss for predicting survival instead of the more commonly used Cox-based losses such as those used in DeepSurv? The MSE loss does not take into account the censoring. Is the y_i used in the MSE loss corresponding to the time of death/last-observed?"
347	Multi-view Local Co-occurrence and Global Consistency Learning Improve Mammogram Classification Generalisation	"Lack of repeated n-fold cross validation.
Lack of discussion on the performance."	"1) In table 2, the first two columns are named ""endtoend"" where there is no citation and I am not sure what the authors are referring to. 
2) In equation 4, the authors didn't specify The query, Key and value vectors and left them in general format. I suggest that the authors mention specify the Q, K, V vectors.
3) There are a lot of grammatical errors such as 
generalisation-> generalization
right hand size of the image -> right hand side
optimiser -> optimizer
area under the precision-recall curve (AUC-PR) -> the area
....
4)  Provide additional details regarding the implementation of the model so the model can be reproduced."	"There are minor issues as follows:

What is FN in Fig. 1?

It would be better if the detailed structure of each component is provided, including that for global consistency module, multi-view local co-occurrence module, and MLP."
348	MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep Models for X-ray Images of Multiple Body Parts	"Generally, I have to say that just confirming very high accuracies is not the reason of accepting a top conference paper. The authors made a lot of experiments to show the superiority of the proposed method, while the contributions of this paper are a simple combination of existing well-known learning schemes. The authors only made some simple modification on two resi-net networks.
The authors are hard to figure out why is it useful when combining the self-supervised learning and continual learning for multi-task tasks.
For multi-task problem, using different datasets or collecting parts from one body is hard to validate its effectiveness. If possible, the authors need to compare your work to multi-view or multi-model learning from theoretic and experimental perspectives."	However, the main contribution of the paper lies in training pipeline innovation rather than training algorithm innovation. The experiment section just lists the results. More analysis is need to discuss about the inherent reasons for the results improvements.	"-Minor extentions and modifications to existing SSL and CL methods  are proposed in this work and applied to xray images.
-7/9 are chest xray images. Only 2 datasets are for other body parts. The ""multi-dataset"" claim is much narrower than it seems, because there is less diversity in the input data.
-The performance is evaluated only on 4 datasets. Why is the evaluation not done on all the 9 datasets that are used in the first stage SSL? 
-Imagenet and Scratch are not strong baselines to compare against. ImageNet has RGB natural images, while this paper is dealing with grayscale xray images."
349	NAF: Neural Attenuation Fields for Sparse-View CBCT Reconstruction	"The clinical need for low-dose CT using sparse-view acquisition is not very pressing. Currently the motivation for this technology is driven by advances in reconstruction methods, becoming better in dealing with this setup. However, manufacturers currently prefer to use lower intensity measurements because the resulting denoising problem can be handled well with current algorithms. In addition, while radiation dose should be kept as low as possible as a general principle, current clinical applications are rarely hindered by dose considerations.
The simulation of CBCT datasets in the Chest and Jaw images seems to suffer from the so-called truncation artifact which is caused by projections not covering the entire acquisition volume which causes additional artifacts which are especially noticable in the FDK reconstruction. Also the FDK reconstruction seems to suffer from a bad handling of the sparse-view setup leading to a multiplicative constant distorting the attenuation values. This leads to a much worse image impression compared to e.g. SART which may simply be eliminated by scaling.
While the method is interesting and the resulting images show better numerical results a visual examination suggests that the reduction in conventional artifacts is paid for by introduction of a new class of artifact which is characterized by noisy boundaries and a novel noise appearance exclusive to this method. In addition no more distinctive features of the dataset become visible than in the classical methods. This is also in line with theoretical expectations since the method does not introduce an explicit prior over the data distribution by incorporating information from multiple CT scans. It rather resembles a novel variant of a reconstruction method which should be theoretically as limited as any other iterative reconstruction without regularization."	"The loss in eq. (3) is a standard reconstruction loss. The claim of self-supervision is incorrect in this paper.
The proposed method is claimed to be useful for sparse-view CBCT reconstruction. However, only one setting of # of views is provided in the experiments. It is necessary to study the performance against different # of views to understand the sparsity of views."	"Comparison results did not significantly demonstrate the superior performance of the proposed method.
Network structure and training process are not clearly stated."
350	NerveFormer: A Cross-Sample Aggregation Network for Corneal Nerve Segmentation	"The biggest weakness of the paper is a relatively poor presentation. In places the paper is unclear, which makes it less convincing. The introduction is very messy. First it introduces a number of studies on fibres - that's fine, but many of those studies (also quite recent) are not put in relation to the proposed method, and are also not present in comparison. Later it mentions [11], which seems to be improvement of [10]. Then it goes on mentioning the weaknesses of [10]. For other methods mentioned in introduction is unclear why are there relevant - some are general segmentation methods (not especially developed for nerves), while some are specialized pipelines. Moreover, the figure 1 uses results of [10] and [2] for motivation, but the text says almost nothing about why those methods are relevant (apart from having weaknesses). In the figure 1, the yellow text should be placed on the left of each row - as it is now it seems that nerve discontinuity is only relevant for CS-Net. The similar comment applies for figure 2. In places, language is informal and unclear: ""...Inspired by [21] we motivate our model too focus on..."", ""...method presents better immunization against artifacts..."". The key contributions: DEAM with TDA and TEA should be motivated and explained more clearly. E.g. the first sentence explaining TDA is not explaining much - and it just keeps on. Figure 3 is a bit confusing. For nerve continuation the arrow points to a place where the proposed network does well, but in other places (top middle) the nerve is broken by the proposed method, while other methods do better. Also, if other methods are performing so poorly on Langerhans cells, are those method relevant for comparison. Acording to the table 1, the methods shown in figure 3 are not competing to the best place. Also, table 1 seem to contains only one method developed for CCM (another one for fiberous structures). Why it that?"	The theoretical justification of the proposed method is weak. A lot of implementation detail is missing.	"The generalizability. I am not sure about the generalizability of the proposed method, as no description of the test setting is provided. Based on Ref.8, external attention relies on two learned memory units which are the representation of the training dataset. The proposed method seems to preserve one such unit.

However, such a dataset-level representation could be less representative if the pre-trained model is tested on a different dataset. Thus, it is important to know how the memory unit is used at the test time and how well the model can be generalized to different datasets.

Insufficient baselines. As the proposed method is a fusion of deformable DETR (Ref. 21) and external attention (Ref. 8), it may be a good idea to include these two methods in the baselines to verify the benefit of this fusion design.

The novelty of TDA is unclear. I cannot see any significant difference between TDA and the deformable attention module in Ref. 21.  If this is a major contribution, I would recommend to highlight the difference and demonstrate the motivation and novelty."
351	NestedFormer: Nested Modality-Aware Transformer for Brain Tumor Segmentation	"There should be section or subsection to discuss key highlights of proposed architecture which make it better than other SOTA.
There should be thought presented on extension of the architecture to any other medical dataset where multiple modalities are available."	If I do no understand wrong, the design of NMaFA is similar to channel and spatial attention network, which is widely adopted in the literature, but the authors do not mention that. Maybe the authors can further justify the differences.	"There are some parts where explanations are insufficient.
There is insufficient discussion of the experimental results."
352	Neural Annotation Refinement: Development of a New 3D Dataset for Adrenal Gland Analysis	"The proposed method is not very clear. What's the symbol a specific represent for?
The novelty of the paper low. It only changes the deep implicit surface by adding an a, and it aggregates features from multiple scale.Why is a needed? And multi-scale feature fusion is a widely used operation in deep learning.
The experiments are not convincing. First, it doesn't compare with other label refinement methods. Second, for the segmentation comparison experiments (Section 4.1), the inference label uses golden annotation, while training label uses distorted annotation. Thus it is not a fair comparison with the baseline methods, since they don't have any label correction process."	"There are some moderate weaknesses:

Results. 
x. In Table 1, the authors compare their method with Seg-FCN and Seg-UNet. The two models are trained on distorted annotations. However, it would be great if the author could show the upper bound, e.g., models trained on good annotations. As a repairment, one could consider an ensemble of multiple models to reduce segmentation noise and uncertainty. Other heuristic methods to remove noise, such as hole filling/connected component analysis might be other baselines methods. 
x. In Table 2, the method without using the appreance as the input works worse. It would be great if the authors could discuss potential reasons behind, e.g. does it mean the basic refinement. method does not improve the annotation?

Clarifications.
x. in Table 2, which view is used in the 2D method?"	"My biggest concern are on the motivation and comparisons of the proposed method:

For training on the distorted data set, maybe 20% distortion won't affect the downstream task performance and human experts only make inside of 20% distortion, it would be nice to find the distortion threshold on gold standard dataset to see how much distortion actually brings down the performance of the downstream task. For example, 30% slices are distorted or more severe level distortion. Such an analysis is expected.

This method is data centric, but what if authors train a bigger classification network with more diverse and complicated data augmentation focused on the appreance and better regularisation? Wouldn't that be a simpler way to tackle this problem? The authors need to add comparison with model with more intense data augmentation and larger model, I noticed authors are using resnet-18, larger models can learn better generalisation and the classification performance gap between the proposed method and baseline is not as big as it seemed. I strongly suggest the authors to consider this issue and I am willing to raise the score after the rebuttal if the authors address this issue.

Minor weakness:

Only used on one data set. It would be nice to see how the method applies on another data set, that would make this paper really appealing.

What happens when the proposed NeAR model is used on very sparse annotations? Do you think the models will have limits on discontinuous objects and extremely small objects?

No stds on table 1"
353	Neural Rendering for Stereo 3D Reconstruction of Deformable Tissues in Robotic Surgery	"Clarity. This paper is quite difficult to follow, particularly with respect to the details of the method. Some acronyms are not defined, and some methodological descriptions are vague. Work flow and block diagrams describing their methodology could be very helpful. Moreover, photometric error measurements are not clearly defined.
It would help to describe the un-met clinical need more concisely, particularly with respect to the need for 3D reconstruction during a typical  procedure for a radical laparoscopic robotic prostatectomy (RALP) for example. How would RALP for example be improved if the proposed technique (or any other approach for that matter) if reliable surface reconstruction of the surgical scene were available.   I do agree however that ""robust scene reconstruction is important for augmented reality, surgical environment simulation, immersive education, and robotic surgery automation"".
What is the clinical impact of being able to remove the surgical tools or reconstruct the 3D scene during tissue deformation in real time? It would be helpful if the paper indicated explicitly what makes the proposed method superior for dealing with tissue deformation.
Why is ""single viewpoint"" important, since in a RALP procedure , the tools are in constant motion, and the camera is moved frequently to track them. In the reconstruction of tissue deformation during an actual procedure, I would have thought that eliminating the tools from the image would be counter-intuitive.  Is the purpose to superimpose the stereo representation of the real tools onto the reconstructed deformed scene?
Validation: If I understand the  validation method correctly, the photometric error measurements are not definitive, especially for tissues under the surgical tool. While it is acknowledged that ground truth is difficult if not impossible in clinical cases, could not a phantom study have been employed to obtain ground truth on some examples?
Some phrases are very obscure. For example, what does ""To capture high frequency
details, we use positional encoding g(*) to map the input coordinates and time into Fourier features before fed to the networks"" mean?.
How long does the algorithm take to reconstruct each frame? Is it feasible for real time application?
Is E-DSSR the only available competing algorithm?"	"The proposed method requires per-scene optimization as the authors state in the ""Implementation Details"", it means that the proposed method seems cannot achieve the real-time reconstruction of tool-occluded areas. That is, surgical tools in the intraoperative video cannot be removed in the reconstructed scenes. Therefore, what is the significance of the proposed method for robotic surgery, in which the tool occlusion often occurs? The motivation of this article should be more clear."	In Section 2.5, the equation (3) and (4) are not well explained. Equation (3) is too dense, maybe consider dividing them and explaining each clearly and also make sure the meaning/definition of each parameters are described in the text. For example, the T in (3) and I in (4) were not defined in the text.
354	Neuro-RDM: An Explainable Neural Network Landscape of Reaction-Diffusion Model for Cognitive Task Recognition	"While the authors weave a nice story to motivate their work, I find the claims about neurobiology to be overstated. For example, the statement ""the ensemble of evolving neuronal synapses forms a dynamic system of functional connectivity (abstract)"" is a simplification of the complex neural and hemodynamic attributes leading to the BOLD response. Likewise, the statement ""uncovering hidden brain states...becomes the gateway to understanding flexible and adaptive human cognition (page 2)"" is an exaggeration at best. Also, the statement ""we develop the machine intelligence of system-level explainability into a deep learning model...to yield new underpinnings of biological processes (page 2)"" is unsupported by the experimental results, which consist of a few predictive analyses.

The is little acknowledgement of existing methods to analyze dynamic functional interactions or extract brain states. There is also no acknowledgement of existing work on interpretable AI or methods that blend model-based and data-driven techniques. This is a large oversight in an already popular field.

Several details of the proposed method are unclear. First, how is the graph w_ij generated? Is it subject-specific or fixed across the cohort? Second, the diffusion process does not require an attention model, so why is it implemented? Third, how are the ""ground truth"" cognitive states defined for training and evaluation? Fourth, why is the BOLD time series truncated? I would expect a dynamical model to accommodate different acquisition lengths. Fifth, what does the statement ""the training data is mixed with the test/retest fMRI data"" in the evaluation section mean? If the data is truly mixed, then the results are optimistic due to data leakage.

There is no ablation study to quantify the impact of the different modeling components (e.g., choice of W, attention vs. no attention, neural network sizes, etc.)."	It is not clear what is gained with the new network compared to the original model of differential equations	"Architectural Description: There is very little information provided in terms of implementation details, eg. number of GNN layers, width of the embeddings, type of graph convolutions used, non-linearities etc. They also do not clearly mention the paradigm for training the models (epochs, learning rate, optimizers etc). This would make it very difficult to adopt their framework for any future applications.

Evaluation is performed on a single dataset split into train/test/validation as opposed to a cross validated, which may not provide a reasonable estimate of the robustness of the improvements. Additionally, since a single dataset split is used, it is unclear how the distribution of the accuracies in Fig. 3 and 4 are generated .

Attentional selection:
a) A contribution of the work is in introducing graph attentions into the Neuro-RDM framework for discovery of new links. However, there is no experimental comparison that establishes whether having this additional component (increased parameterization) is necessary, for example via an ablation study
b) The authors average patterns learned by the attentional framework as a proxy for studying replicability of the patterns. Since evaluation was performed only on a single data split, this does not provide a good indication for whether these patterns would be consistently replicated for a different subset of the population"
355	Noise transfer for unsupervised domain adaptation of retinal OCT images	"SVDNA evaluation in this study applies only to known domains
Key contribution appears to actually be the final histogram matching, and not the SVD-based noise transfer, which might not have been emphasized
Comparison against prior methods not very clearly presented"	"By design, and as mentioned by the authors, the proposed technique has limitations. Specially, when the source domain is more noisy than the target domain, the proposed technique will potentially not perform well. The performance is uncertain in this scenario. This important limitation reduces the applicability of the technique to more scenarios.
The Algorithm 1 is not clear enough. Variables within the algorithm are never defined so that it cannot be properly understood. Authors should properly define all the variables employed and add comments on the algorithm when needed. The description of the algorithm in the body of the paper helps understanding the technique applied.
There exists mismatches between Algorithm 1 and text. Clipping and histogram matching are not within the Algorithm but they described in the text.
There are some variables defined in the text (e.g. probability 'p') which are never applied anywhere. If a variable is defined, it should be used somewhere (e.g. in an equation)"	Some minor mistakes exist in the paper (see detailed and constructive comments).
356	Noise2SR: Learning to Denoise from Super-Resolved Single Noisy Fluorescence Image	"Execution time is not presented.
Limited comparison to state-of-the-art:  Only PSNR and SSIM mesures are studied (but, it's enough for a conference paper). A computational time comparison is missing."	Lack of extensive evaluation: The experiments are done only on fluorescence images. It's unclear how applicable the method is for general biomedical images. E.g. N2N is evaluated on MRI.	"The paper lacks enough rationale behind self-supervision and subsampling technique.
The paper requires ablation to provide evidence on each stage improving results. Currently the overall gain is marginal and its hard to say where this is coming from
N2N and propose N2SR have competitive results so why the proposed is better or effective? This is Not well established."
357	Non-iterative Coarse-to-fine Registration based on Single-pass Deep Cumulative Learning	1) The evaluation is performed on brain MR datasets, in which the displacements are relatively small.	"1) The innovations of the proposed method are not enough for MICCAI: the main innovation of the proposed registration method is the different architecture of the registration framework. Using different scales of features to obtain different resolutions of the displacement fields to make a coarse-to-fine registration. 
2) Only one UNet-like architecture may hasn't the ability to capture such a large amount of features produced in different resolutions, this is the main reason that lots of previous work adopt multiple networks to conduct the coarse to fine registration. 
3) In Table 1 the highest registration accuracies are obtained by using \lambda=0, it seems that the Jacobian determinant regularization didn't work well in the loss function, which suggests that the proposed method may lead to image folding. 
4) In Table 1 the registration accuracies are not higher than those of ULAE-net by using \lambda=10^{-4}, it demonstrated that the performance of the proposed method is not better than the ULAE-net.
5) Authors mentioned that ""while the \lambada is set as 10^{-4} to ensure that .... is less than 0.05%"", how to get this conclusion? The \lambada is a weight that is to balance the loss items. How about the other values of \lambda, comparison results using different values of \lambda should be shown in the manuscript.
6) In the ablation study, what parameters such as the value of \lambda were used?"	"1- Limited discussion of the qualitative results and comparing with the state-of-the-art.
2- Lack of training results: The authors utilized a total of four public datasets of 3D brain MRI for training their proposed network; however, the training results are missing."
358	Nonlinear Conditional Time-varying Granger Causality of Task fMRI via Deep Stacking Networks and Adaptive Convolutional Kernels	"The proposed method only test on real world data fMRI sequence of a known task. It lacks of a more practical application.

Topology network to test the causality is unclear in the synthetic data (it seems just couple-triple)

The case in exam for the real data is more suitable for other tools as DCM.

There is no discussion or analysis for indirect connections

The use of convolutional networks to estimate source-target relationship is new though already proposed for other causal physiological systems (Antonacci et al. PeeerJ 2021)."	"1)	In addition to what the author has claimed in the Introduction section, ""One prior method limited the time lag to exactly one time point to reduce computational complexity [9], while the other methods required the user to specify the time lag a priori."", there have also been works where different time lag setting (e.g., lag of 0, 1, and 2) will be tested independently to order to account for the possible different time lags when estimating GC. In the proposed model, the ACK filtering is conceptually similar to the mentioned approach.
2)	The possible typo in the sentence in page 3, ""First, CNN-ACKs 1 and 2 are trained to transform previous time points of Y_t into Y_t, and Z_t into Y_t..."" makes the reviewer difficult to understand all the latter descriptions.
3)	In the experiment results of ""Real-World Task fMRI Dataset"", are the six connectivities listed in Supplemental Fig. 1 all of the non-zero causalities, including time lag k Granger causality?"	"Some papers are not included in the discussion, please cite and compare (if applicable).
https://www.nature.com/articles/s41598-021-87316-6
https://arxiv.org/abs/1802.05842
I am still a little confused about the optimization. Is the optimization performed purely based on the minimization of the MSE between the predicted Y_t and the actual Y_t? In that case isn't the problem underdetermined? Is there only one possible kernel that can match the data? How is this problem solved in the proposed framework?
What is Z_t in the synthetic dataset?
How does the model perform in cases where there are bi-directional recurrent connections?
How are the p-values evaluated?
What is the performance of the model under non-Gaussian (or more generally non-iid) noise?
What is the performance with increasing dimension of the data and covariates?
How does this compare to other approaches for nonlinear connectivity estimation, such as convergent cross mapping?"
359	Nonlinear Regression of Remaining Surgical Duration via Bayesian LSTM-based Deep Negative Correlation Learning	"The network is validated on a single dataset and surgery. The cataract surgery is showing to be generally short (5min-20min) where the performance on longer surgery such as cholecystectomy might need further justification in comparison with other methods that are designed for those surgery.
The surgeon's experience estimation is described as a function of the network but with limited information provided in definition and setups for them.
Having a future work paragraph showing a developing direction of this model in conclusion section may be helpful."	"All the uncertainty-related results (Fig. 2(A) + suppl. Fig. 2) are restricted to selected surgeries - making it difficult to understand the average-case performance.
The qualitative results (Fig. 2(A)) only seem to show easy examples with similar duration.
It is not clear how the variance-maximization objective of DNCL is compatible with uncertainty estimation.
It would be very helpful to include an ablation without both DNCL and Bayesian LSTMs (i.e. only phases/expertise).
It is not clear from the paper if 6-fold cross validation was used like in CataNet. If not, then the SOTA scores in Table 1 are not completely comparable.

The weaknesses and possible solutions are elaborated in more detail in section 8."	The authors state that they follow the Bayesian-CNN, DNCL, and uncertainty estimation methods in [13-19]. When describing the adapted methods for RSD prediction task in this artical, the authors should give more and clearer explanations, instead of forcing the readers to read the references [13-19] by themselves.
360	NVUM: Non-Volatile Unbiased Memory for Robust Medical Image Classification	"Some points are not clear, like:
Between Eq. (2) and Eq. (3), what is the definition of \pi_c?
In Eq. (3), why utilizes z_k^i-log(\pi)? It is better to show its motivation.

Since this paper aims to solve the noisy multi-label problem with class imbalance, it is better to individually provide the experiments results on noisy multiple labels, class imbalance, and noisy multi-label with class imbalance.

Some related works are missed, like:
[1] Laine, Samuli, and Timo Aila. ""Temporal ensembling for semi-supervised learning."" ICLR (2017).
[2] Shi, Xiaoshuang, et al. ""Graph temporal ensembling based semi-supervised convolutional neural network with noisy labels for histopathology image analysis."" Medical image analysis 60 (2020): 101624."	"The explanation of the effect of proposed regularization loss is insufficient. The four cases listed in Sec. 2.1 are all based on the strong assumption that networks have high confidence in the classification results. The author should rethink the gradient analysis of the noisy situation. In my opinion, author may consider the distribution of the multiplier of Jacobian matrix for clean samples and noisy samples.
There is a lack of discussion of the class-level AUC results. In the experiment part, it's insufficient to reflect how the method handles noisy and imbalance labels."	"One concern of NVUM is about the space size of the memory module, which is S*C and will linearly increase with the size of training set. Since the paper focuses on real world large scale datasets, when the training set gets extremely large, the size of memory module might be a severe issue.
There's no ablation study for different noise level in training set. Most paper focuses on noisy label problems will introduce different level of noisy labels to the clean training dataset and evaluate their methods under multiple noise levels. This paper does not contain such ablation study, so people would have no idea about the noise tolerance of the proposed NVUM method.

Another potential problem is that, NVUM takes class prior distribution into account, however, this prior distribution is directly estimated from noisy training set, thus the prior distribution might be corrupted under severe noise level. This issue is mentioned in Future Works part.

One minor issue: Figure 1 is never referred to in the paper, and I think it's demonstrating the training and updating pipeline of NVUM. Please add reference to Fig.1 in the paper."
361	On Surgical Planning of Percutaneous Nephrolithotomy with Patient-Specific CTRs	"Authors should further elaborate on the implication of their results and relate back to the original objectives. The conclusion appears abruptly after the results but more discussion of the cases outputs is needed.
Error plot shows a deviation for all 7 cases. The authors should discuss what those deviations mean and what the implication of a 1 mm error means.
Figures need to be larger and clearer to make it easier to see the scale (esp. Fig 1)."	"Minimal evaluation on whether the path is clinical useful (Unclear whether reaching the calyx is sufficient for successful PCNL and whether the whole path of the CTR is feasible without damaging surrounding tissue)
No comparisons to other methods
No discussion on the path to clinical deployment after calculating these parameters"	"1-The Novelty is not highlighted, the author should describe more about the Significant improvements of their work. 
2- lack of comparison with the state-of-art 
3-Some English error but its possible to understand"
362	On the Dataset Quality Control for Image Registration Evaluation	"The paper has two significant weaknesses that I believe could be addressed quite easily.

The paper does not adequately evaluate the authors' key hypothesis that variograms can be used to rapidly identify suspect landmark points. The authors classify points into three groups (definitely suspect, maybe suspect, and OK). Samples of these points are then given to 2 independent radiographers. To test the hypothesis the paper needs to show that there is a statistically (and practically) significant difference in the number of suspect points confirmed by the radiographers in these samples. At present the paper does not show this.

The first part of the point classification algorithm (construction of the variograms) is well described and I am confident it could be reproduced. The second part of the algorithm, (classification of fiducial points from the variograms) is less well described and I am not confident it could be reproduced. More details need to be given on the skills and training of the people doing the classification. More quantitative data could be given on this process."	"In the methods section, the pairs of image data sets are to be registered. Right? If so, the process of registration will affect the methodology here, as the quality of the registration depends on the choice of the landmarks. A vicious circle.
If the pairs are not registered, how are the landmarks compared??
The work of Bardosi et al on FLE_image, IJCARS, presents very relevant information for this type of research and should have been addressed.
This method is more or less a qualitative approach to testing the ""quality"" of selected fiducials that is based on the assumption (end of section 2.1) that wider separated landmarks have larger differences in their displacement. First, this is not backed by any reference and second, it is counter-intuitive as it would imply a ""bias"" proportional to the distance."	"the main weakness is that the ""sensitivity"" of the method is unknown. Especially, how many landmarks of poor quality are not identified through the variograms? (missed cases)
It is difficult to assess this, since all landmarks would have to be thoroughly checked by third-parties (or maybe by looking at the most challenges cases in challenges like [30]?). Nevertheless, this limit should at least be discussed in the paper.
they might be other patterns in variograms that were not identified.
it would have been interesting to list (some) other datasets that could be directly checked with your method."
363	On the Uncertain Single-View Depths in Colonoscopies	"Only an internal comparison are performed, against variants of the proposed methods methods. Continuously, this makes the fact that the teacher student model outperforms the other methods less significant/impactful. Furthermore, no mention on other works that combine AU and EU quantification.

While two data sets are used for evaluation, only one of them consists of real data and focusses on a single application. In my opinion, this is a big loss, as I have the feeling that the proposed method could also show interesting results for other applications. Demonstrating this would really improve the impact. Of course there is limited space in a conference paper, but I would love to see some additional data/tasks for evaluation.

The authors fail to clearly mention the limitations of the proposed approach and makes often claims that are a bit too bold.

The uncertain teacher performs only incrementally better over the teacher-student (Table 2), and in all honesty, to me the increment is negligible, even though it seems to be consistent.  This combined with the fact that the training is only done over one split/initialization begs the question if the results are statistically significant or not, this is not mentioned in the manuscript.

Paper lacks implementation details on the model besides information on the loss functions and the metrics are not explained. More in general, the paper feels fragmented at times. There could be more effort in to making the paper more coherent."	The authors may want to revise the literature review to better summarize the related works.	"Details in implementation are missing. For instance, how many epochs were models trained for, what hyperparameters were used and how were they chosen, etc. Without these details, results from this method may be difficult to reproduce.
Clarity in the results section could be improved (see detailed comments)."
364	One-Shot Segmentation of Novel White Matter Tracts via Extensive Data Augmentation	"The data augmentation method is quite straight-forward, and it is not convincing that implementing cutout as the augmentation is the most suitable solution to this topic.
Lack of literature on data augmentation for segmentation, and also no comparison between the proposed method with the SOTA augmentation method.
Descriptions in Section 3 are not clear and should be organized in a more comfortable way."	"The discussion is extremely brief. Limitations and possible ways for further improvements are not discussed.
The authors use existing methods and combine them. No methodlogical novelty."	One minor comment that I have the private dataset used for testing has very similar quality with the HCP data. Wonder if the authors have any comments about applying to it to low quality, clinical style dataset.
365	Online Easy Example Mining for Weakly-supervised Gland Segmentation from Histology Images	"*The related works are not sufficient.
The most related work was not cited and there is no discussion about the contribution from the following paper. This paper also focuses on the problem that pseudo labels generated using CAM are noisy labels, and introduces weights to the loss function, where the weight is obtained on the basis of uncertainty. The method is not completely the same but the idea and approach are very similar to the proposed method (the reviewer feels that the technical contribution is minor from this related work). Please check it.
In addition, the adaptive training strategy is a common approach for learning with noisy labels. These are not cited and discussed. There are cited in the related works in the following paper.
Li et al., Uncertainty Estimation via Response Scaling for Pseudo-mask Noise Mitigation in Weakly-supervised Semantic Segmentation, AAAI2021.
*Evaluation was not sufficient.
In the evaluation, this paper listed many supervised approaches, but only one weakly-supervised method. In particular, the reviewer recommends the authors to compare with a typical pseudo labeling that usually takes pixel sampling using the confidence or uncertainty and trains a network with the masked loss, which ignores the non-selected pixels. This sampling strategy also has the ability to avoid confusing pixels. To show the effectiveness of the weighting by OEEM, this ablation study is required. Then, add the discussion of why weighting is a better strategy than the thresholding-based pixel selection method.
*It is unclear whether the training and test data were separated by patients (i.e., these data do not contain the same patient or WSI). Because the image features have similar features in a WSI, the network may be overfitted in training if the test and training data contain the images captured from the same patient or WSI."	"(1) The method description is not clear enough. For example, how patch-level labels are composed and how they differ from image-level labels.
(2) Why equation 6 performs best requires careful analysis.
(3) As a weakly supervised method, the paper only compares with a weakly supervised algorithm SEAM, and the experiment is inadequate."	"*	The difference in performance between SEAM and PSPNET+ResNet38 (w/o OEEM) is questionable. This suggests the improvements are from the architectural choices. 
*	For fair comparison, OEEM should have included to prior/compared works to validate that performance gains are not based on the network choice. E.g., UNet + OEEM, MedT + OEEM.
*	It is unclear if results on the fully-supervised methods on Glas Dataset are re-implemented or taken directly from the prior papers. 
*	It is unclear how loss map $L$ was obtained from $\hat{X}$. Descriptions regarding this point were limited."
366	Online Reflective Learning for Robust Medical Image Segmentation	"Meta-learning DG approaches are not mentioned in related work. At least the ones in previous MICCAI [1, 2]. I also suggest the authors to cite the TTA papers in previous MICCAI. There are also some relevant papers in last DART workshop.
It is unclear how the segmentor for differentiable learning works. It is unclear what is the label intensity value. The multiplication and addition are not clear. I suggest the authors to update and clarify.
If use the normalised bad heatmap (with failures) as the attention matrix, will it not ""emphasise"" the wrongly classified area (such as the RV area in Fig.2)? I mean the purpose is to penalise the wrongly classified area. But with the attention, it does not work as expected. I think ablation should be conducted and discussion about this needs to be included.
The results of the proposed method are relatively very similar to some strong baselines. I think statistical significance analysis needs to be conducted, where the authors clicked the check box about this in the reproducibility response.
In fact, I don't think the inference time is a selling point of TTA method. Typically, TTA method is model-agnostic. Although, nnUNet based models are slower in terms of inference time. TTA + nnUNet models will definitely achieve better results, which has not been touched by the authors yet.
For TTA, one problem is overfitting when the number of iterations goes higher. It is unclear if this happens for this method. The number of iterations in this paper is predefined as 10. It will be good to see some more experiments on this hyperparameter.
The overall objective is missing.
The approximated MI is not clear. Self-contained description needs to be added for the readers to understand that loss.

References:
[1] Liu, X., Thermos, S., O'Neil, A. and Tsaftaris, S.A., 2021, September. Semi-supervised meta-learning with disentanglement for domain-generalised medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 307-317). Springer, Cham.
[2] Liu, Q., Dou, Q. and Heng, P.A., 2020, October. Shape-aware meta-learning for generalizing prostate MRI segmentation to unseen domains. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 475-485). Springer, Cham."	Out of 3 datasets used for evaluation, 2 are in house datasets (which makes it difficult to validation or reproduce the results).  On the public dataset M&M the comparisons against nnUNet is not conducted, but included on the private data (why not include on everything).  Several steps in the method are not well explained, which makes it difficult to reproduce.	"Missing reference. The idea of using synthesized image for failure detection has been explored in [1], which should be cited and discussed in the related work.
Methodology design. For the test time adaptation, why the synthesizer needs to be finetuned in together with segmentor?  I am concerned with the instability of the optimization with such a dynamic image synthesis network at test time. Have the authors tried to optimize the segmentation network only?
Methodology design. Any stopping criterion for the test time adaptation? From Fig 4, it seems that the performance can drop after a number of iterations.
Unstable performance observed on test cases. The Dice curves over the test set in Fig 4 shows that there are still quite a few cases where RefSeg fails to refine the segmentation (especially on the M&Ms dataset), yielding even lower Dice scores during test time adaptation. Are these failure cases all from the unseen test domain due to the domain shift? A discussion on those failure cases should be added.
The definition H_att is not that clear to me. Better to give a mathematical definition on it. It seems that H_att is the network predictions after softmax multiplied by label values. Why not simple feed the multi-channel output to guide the image synthesis? Please clarify.

[1] Li, K., Yu, L. and Heng, P.-A. (2022) 'Towards reliable cardiac image segmentation: Assessing image-level and pixel-level segmentation quality via self-reflective references', Medical image analysis, 78, p. 102426. doi:10.1016/j.media.2022.102426."
367	OnlyCaps-Net, a capsule only based neural network for 2D and 3D semantic segmentation	"-Only using one metric for comparison. Authors could use Hausdorff, accuracy, sensitivity and specificity metrics beside DICE metric because using just one metric such as DICE is not very reliable for showing segmentation results quantitatively.
-Authors can update one reference for capsule paper with a newer one because it fits more to medical field. ""Capsules for biomedical image segmentation"" https://www.sciencedirect.com/science/article/abs/pii/S136184152030253X"	I do not see real weaknesses in this paper, except that the given formulas are not easy to read for someone which is not expert in capsule networks and should be completed with parentheses for operators priorities (see formula (1)).	"Introduction of optimisation strategies known in the literature to a state-of-the-art architecture.
The contribution in terms of performance compared to state of the art is limited.
Unclear savings of memory footprint of the current method compared to state of the art.
No visual examples for 2D segmentation.
Lack of statistical tests to strengthen the discussion of results"
368	Only-Train-Once MR Fingerprinting for Magnetization Transfer Contrast Quantification	It's hard to reproduce, because they did not share their codes.	"The study needs to incorporate explainable AI components to demonstrate the tasks that the bi-LSTM is actually learning.
The lack of a gold standard MT experiment and only comparing the MRF reconstruction methods is weak given that the MRF MT maps themselves need to be validated for clinical use. This needs to be at least included in discussion.
Random sampling of schedule components enables the generalization to different schedules but the use of TL to specific schedule contradicts this feature. So, the implementation can be a container model reference which would be optimal once a TL is employed. It is best to state this explicitly and underscore the benefit of a generalized model as a baseline rather than claim that the model can deal with any schedule. The reviewer recommends toning down this claim which might be confusing."	The state-of-the-art baseline method of dictionary matching is not included.
369	Opinions Vary? Diagnosis First!	Results show that the proposal is somewhat a bit convoluted for the improvement that, in some cases, for instance DICE in Cup Disc Segmentation is not present in a consistent way. These differences, some times in favour of previous SOTA, are not properly addressed and diminish the real contribution value of the proposal as is	"in general the network architectures never discussed.
Authors need to provide a pseudo-code for training and testing phases and explain the details of training steps
In equation 2 and 4, the size of ExpG and m are not consistent. (how n is missed)
why only dice score is considered? we need more metrics, including false rates
I can not find/understand the details of ExpG
As far as I know, the masks in refuge-2 was based on 7 raters, but I can not find the 7 masks. it is also not mentioned in the cited reference. Authors may had access to the data in other way. please make sure the claim is correct.
what was the architecture of attentive diagnostic network in the figure 1.
sharing the codes would be helpful"	"The experiments are not sufficient.
(1) In section 2.3, the statistical results of the various high-frequency elimination methods should be supplemented.
(2) Except for MV, other multi-rater fusion methods in Table (a) should be compared in Table 1 (b).
The related works should introduce in the introduction.
Some writing details of the paper are confusing and need to improve
(1) The symbol ~ represents equal in section 2.1, while it means a range in formula (4).
(2) h and w denote the size in formula (2), while H and W are used in formula (4).
(3) The DiagFirstGT is defined as the result of formula (1) in the second paragraph of section 2.2, while it is redefined as the optimal expertness map in the third paragraph of section 2.2"
370	Opportunistic Incidence Prediction of Multiple Chronic Diseases from Abdominal CT Imaging Using Multi-Task Learning	The main weak point is the doubts that the results pose. If we look at table 2, there is almost the same result for ischemic heart disease (IHD) when using the axial slice, that does not contain information about the heart, as when using the coronal or the sagittal, that have at least the lower part of the heart on them. Similarly, the coronal slice does not have information on the vertebrae, but osteoporosis is almost as well predicted as with the sagittal, that has most of the column or the coronal. Multi-slice representation has almost as good performance as the others. These data suggests that the network is focusing on information that is not of relevance for the task at hand.	"The segmentation performance of aorta is slightly unsatisfactory. Whether the segmentation results would affect the prediction accuracy? If that's the case, other SOTA segmentation methods can be consider to perform segmentation tasks.
The reason of concatenating three slices laterally did not explain clearly, dose concatenating three slices in channel dimension can also work?
The training process using sparse labels did not explain clearly, more details should be supplement in the article."	"I don't see the clincial aspect/translation - does the source and target combinations/disease affinities make sense from a clinical perspective? (apologies if missed)
I feel the dice score for the segmentation is quite low? Any comments about this?"
371	Optimal MRI Undersampling Patterns for Pathology Localization	Have you ever tried to use the mixture of the two or three datasets for the training?  It would be interesting to see the results.	"I would suggest the authors to provide additional quantitative analysis for the fastMRI dataset.
My additional comments are presented below (Q#8)."	The main weakness is the lack of explanation about optimization like IGS and LOUPE. The gradient computation on these optimizations are not clear. The convergence analysis should be included to check the stability of the method.
372	Optimal Transport based Ordinal Pattern Tree Kernel for Brain Disease Diagnosis	The notation could probably be simplified to make the topic more approachable.	"It is unclear why a tree structure is a good way for brain network analysis. It may reflect the hierarchical structure but it also loses important brain connections and its connection strength information. In other words, the proposed algorithm just analyzes partial information. Brain network is different from general graph analysis since brain network node structures are limited and fixed. The resulting tree structures are not necessarily consistent from person to person.

It may need some further study to analyze whether the OPT or the OT makes significant contributions to the improved performance. For example, the OT can also be applied to other structures like Tree++. Such experiments will help justify their discoveries.

The description of OT algorithm is not clear. It hurts its reproducibility."	"(1) Motivations of using OT distance are expected to be discussed. Tree edit distance is commonly used to measure the similarity of two tree-structured data and it is easier to compute. Some comparisons are preferred.
(2) Ablation studies are desired, such as directly applying OT to the brain network and build the kernel, different OPT starting from different regions, etc
(3) More statistical metrics are expected. Considering the possibility of imbalanced data, some other metrics are expected in classification experiments"
373	ORF-Net: Deep Omni-supervised Rib Fracture Detection from Chest CT Scans	"Though the paper is well illustrated in general, I do have the following questions:
1, Fig 1, is a bit confusing. From my understanding, the model should have a single backbone, multi-heads structure. Namely a single image is passed through the network and different branches will work together to provide supervision based on annotations for the image. However, from Fig 1, it seems like an image triplet is sent to the network. Could you confirm which one is correct?
2, What will the score be like during inference? Since we have three branches, how do we aggregate the confidence?
3, Does the ""CA"" version of experiments in Table 2 also apply to the box regression? From the paper, it seems to claim only the classification loss adopts the confidence multiplier. If so, any explanation why?
4,Given the 3D shape of CT scans, how are the 2D slices selected to train and test the model? Only use the key slice? Moreover, since 310 box annotated images are used for testing, does it mean all testing cases have fractures?"	"I have several concerns and unclarity of some parts:

It is not clear how many patients are involved in the private dataset. And, is the data split patient-wise?
Have the authors considered or experimented with how the amount of data with different types of annotation varies?
SGM and IGM are not that different. And have the authors considered other maps, e.g., a uniformed W with P_bP_dP_u?"	"Some details of the proposed method need further clarification.
What is the rationale of combining output from two other branches to guide the training of one branch?
What does ""three different annotation types of data are equally sampled"" mean?
Are the regression results combined using non-maximum suppression?
Is the i in W_i, p_i, b_i the index of different pixels in a feature map?

The threshold t is set to 0.5 for the dynamic label assignment and confidence-aware classification loss. How does the threshold value affect the model performance?"
374	Orientation-guided Graph Convolutional Network for Bone Surface Segmentation	"The paper does not report inter or intra rate variability. Bone segmentation from ultrasound imaging is quite challenging and often results in a low inter-rater agreement. The impact of the uncertainty of the ground truth segmentation on the final quantitative evaluation is not clear.
The study does not seem to utilize any data augmentation techniques. Other studies have shown that the performance of UNet and other machine learning algorithms could be improved through data augmentation."	The authors can strengthen the justification by describing the consequences of surface discontinuity. Also, some terminology needs to be better explained to the unfamiliar reader earlier in the text, such as orientation and connectivity.	"Clinical Motivation: The unmet clinical need addressed by this work is not well-described or clear to the reader.
Discussion: The Discussion section does not provide much insight to the reader, beyond what has already been described elsewhere in the paper. The limitations of the proposed approach are not addressed."
375	Orientation-Shared Convolution Representation for CT Metal Artifact Learning	The experimental materials involve limited anatomy. I am very interested to see various other anatomies, including total hip arthroplasty.	"Compared with NMAR and DuDoNet, large streaks between metals are not reduced.
The backbone of the method is based on the existing DICDNet method."	"The prior knowledge exploited does not hold true for all image artifacts caused by metal. E.g. in Fig. 5 the black area inside the mouth cavity can clearly be seen still contain the black band between the metal implants in the presented method, while other methods can remove it. In general the visual impression of the results does not match very well with the quantitative evaluation. CNNMAR leaves a very convincing visual impression compared to the proposed method.
Only simulated data for evaluation.
The method section is extremely hard to follow for me.
No ablation studies. This work introduces steerable convolutional filters and a network architecture inspired by an unrolled optimization scheme. Without an ablation study it is unclear what parts of this method are essential."
376	Out-of-Distribution Detection for Long-tailed and Fine-grained Skin Lesion Images	"It is not clear if mixing up the lesion images in the input pixel domain makes sense. The mixed up images does not look realistic and it is not clear why the model should learn a better representation from them. For example, authors mentioned that they also included some unusual samples, such as blured images as the OOD class. Depending on the mixup weights, a mixup lesion can easily look like a blurred image, so it is not clear if training in this way helps during such situations.
It is not clear how susceptible the model is to the selection of the 3 groups (head middle tail). How did the authors pick the cutoffs between the groups? Did the authors try to pick different cutoffs to group classes in a slightly different manner and conduct experimetns? Authors did not touch any of these issues.
It is not clear how did the authors train the prototypical network."	"The prototype learning part needs more explaination.
Confidence score section is interesting but needs more explaination. How is it computed?
No statistical significane of the results is reported"	Some parts of the paper may be worth revision for more clear and straightforward presentation. That is, the variety of the mixup strategy might be better presented to facilitate the comprehension. Moreover, the results are worth further discussion as the reported Mixup results appear to outperform the proposed method.
377	Overlooked Trustworthiness of Saliency Maps	"Some of the details are missing.
A few questions on the basic premise."	"Although there is novelty in the work presented, the authors fail to recognize previous works that analyse the reliability of saliency maps (e.g., Adebayo et al. [https://proceedings.neurips.cc/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf]).
Even though the authors present seven different saliency map methods, some of them are very similar (e.g., VG and VG * Image) and they do not present the results with some other different and well-know methods (e.g. LRP, DeepTaylor). It would be interesting to check if the lack of relevance and resistance still happens in these ""relevance""-based methods."	"The novelty of this work is limited. The fundamental equations (2) and (4) are similar to the equation (4) in the paper [1]. There are many existing works studied on the trustwoorthy of interpretations, e.g., [2-6] and many others.
Although this work is solid with experimental evaluations, there is no mathematical analysis for the proposed criteria and no discussion of how to improve the trustworthiness of saliency maps.

[1] Improving Deep Learning Interpretability by Saliency Guided Training. Advances in Neural Information Processing Systems, 34. 
[2] Ghorbani, Amirata, Abubakar Abid, and James Zou. ""Interpretation of neural networks is fragile."" In Proceedings of the AAAI conference on artificial intelligence, vol. 33, no. 01, pp. 3681-3688. 2019;
[3 Dombrowski, Ann-Kathrin, Christopher J. Anders, Klaus-Robert Muller, and Pan Kessel. ""Towards robust explanations for deep neural networks."" Pattern Recognition 121 (2022): 108194; 
[4] ""Proper network interpretability helps adversarial robustness in classification."" International Conference on Machine Learning. PMLR, 2020; and many others. 
[5] Kindermans, Pieter-Jan, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Schutt, Sven Dahne, Dumitru Erhan, and Been Kim. ""The (un) reliability of saliency methods."" In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 267-280. Springer, Cham, 2019.
[6] Kindermans, Pieter-Jan, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Schutt, Sven Dahne, Dumitru Erhan, and Been Kim. ""The (un) reliability of saliency methods."" In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 267-280. Springer, Cham, 2019."
378	Parameter-free latent space transformer for zero-shot bidirectional cross-modality liver segmentation	"The writing quality renders much of the text very difficult to parse.
Lack of baseline zero-shot approaches
numerically varied / heterogenous zero-shot performance
unclear degree of domain specific information embodied in the transfer function
lack of robust baselines for core intramodality model"	The evaluation is not adequate. More experiments should be done to compare the proposed method with other related works.	"The comparative experiment is not sufficient: The performance of the current backbone in the case of single-modality, cross-modality and cross-site images is listed in Table 1, and it can be seen that the backbone performs moderately well in the case of single-modality and poorly in the case of cross-modality and cross-site images. However, this experiment can only show that it is the defect of current backbone, and cannot prove that most of the existing methods cannot do cross-modality and cross-site images segmentation.
The comparative experiment can not fully explain the problem: In Table 1, for the experiment of CT - > CT of backbone setting1, after replacing the test data, dice decreased from 97.08 to 85.07, which seems acceptable; However, in the MR - > MR experiment, after replacing the test data, dice decreased from 87.03 to 41.09, which can not indicate whether it is a problem with the model or the data itself.
LST bidirectional problem: It is mentioned that LST is a bidirectional latent feature space, and its feature mapping is the common feature closest to different modalities. However, in the third experiment in Table 2(IG kernel setting3), there is a large gap between the experimental results of CT -> MR and MR -> CT under the same dataset, which seems to fail to reflect the bidirectional nature of LST, and there should be no large difference in results in the same latent space."
379	Patcher: Patch Transformers with Mixture of Experts for Precise Medical Image Segmentation	"It is unclear why the authors include the mixture-of-experts (MoE) into the decoder for boosting the segmentation method. What about the performance of removing the MoE from the decoder?
According to Section 2.4, the authors just claimed that the upsampled MLP features as the expert features and then utilized an attention block to weight the so-called expert features for predicting the segmentation result. Hence, the novelty of MoE-based decoder is limited.
The technical novelty of the Patcher block is unclear. It seems that the Patcher is based on vison transformer block. What is the main difference? And an ablation study is required to evaluate the effectiveness of the main difference."	Can you explain or show the limitations of the proposed method?	The experiments do not accurately depict the performance of the model. All the presented results are provided by the authors, which does not guarantee that each model was set up to perform in optimal conditions.
380	Patch-wise Deep Metric Learning for Unsupervised Low-Dose CT Denoising	The sample size in this study was limited and the one-time split could not verify the generalization ability of the network well.	"The purpose of the proposed metric is not well explained. Why can setting the positive pair and the negative pair be used to maintain the structural information and suppress the noise level? Why is the metric method used in the mid-layers of the generator? Why is the metric loss L2 normalization with a temperature parameter? All these setting is quite similar to contrastive learning instead of metric learning.
In the ablation study results (supplementary material table 1), how the proposed method addresses CT numbers shift is not demonstrated.
The motivation of maintaining previous CT number statistics is questionable. The LDCT has worse CT number statistics when compared to NDCT. The proposed method should learn the CT shift and make the output as much close as the NDCT.
The objective function in metric learning is to pull the patches from the same location together while pushing the patches from neighbors away. The trivial solution is to make network output the same input; that is, doing nothing. This is contrast to denoising.
The description of the datasets and the experiment implementation is insufficient. Especially for the AAPM challenge, even the quantity and the division of the dataset are not given. How the unsupervised data is constructed is not stated."	"This paper proposes learning in a deep feature space as novel. Actually, using deep feature extractors, particularly the VGG extractor, is a very common method in LDCT denoising problems. Reference 22 in this paper has introduced this method.
While several GAN based approaches are discussed here, I am interested to see comparisons with other directly supervised approaches. Perhaps the GAN loss is not sufficient.
The choices for the windows used in Figures 3 and 4 are very unusual.
The authors have used the 2016 AAPM dataset. Actually, this dataset is out of date, and has been superseded by the 2020 Cancer Imaging Archive dataset, which is significantly larger."
381	PD-DWI: Predicting response to neoadjuvant chemotherapy in invasive breast cancer with Physiologically-Decomposed Diffusion-Weighted MRI machine-learning model	"1) From a technical point of view, the novelty is a bit incremental. The technique for extracting approximate pseudo-diffusion fraction and pseudo-diffusion maps is based on ref [11]. The radiomics method using XGBoost is fairly standard. The novelty is in the combination of techniques and the application to this particular task.
2) From Table 2, it seems there is a substantial improvement compared to state of the art methods. From Table 1, we see that the largest part of that improvement is already achieved by the ADC_0-800-only method, i.e., the one based on a simple ADC estimate. The advanced PD-DWI approach only adds another percent. This makes me wonder what was the key innovation in the overall framework that explains the improvement compared to state-of-the-art. Is it the use of XGBoost? The manuscript would have been more insightful if this question was answered.
3) Confidence intervals and/or results of statistical testing on the AUC measures are missing, so it is not clear whether the reported improvements are statistically significant."	"It was not clear how clinical features were combined and used. AUC with and without including clinical information will be required.
The feature selection process needs to be clarified with some justification (the number of features, selection criteria, and consistency and stability)."	"The main weaknesses:
(1) The overall innovation may not be enough. The main innovation was in DWI processing, and the parts of feature engineering and modeling were normal. And the DWI signal processing methods were not original innovative, the formulations were others in medical physics [1-2].
(2) The model architecture was simple (Fig.3).
(3) The work found some interesting findings and demonstrated that PD-DWI could improve the machine learning model performance. But the work was not further finding new imaging patterns or biomarkers using the ADC 0-100 and F maps, which was the most important for clinical application.
(4) In Table 1, all the ADC-based machine learning models overperformed the BMMR2 challenge Top-3 performances. I think the reliability of the results may be questioned by readers.
[1] HST.583 Functional Magnetic Resonance Imaging: Data Acquisition and Analysis. https://dspace.mit.edu/bitstream/handle/1721.1/51692/HST-583Fall-2006/NR/rdonlyres/Health-Sciences-and-Technology/HST-583Fall-2006/Assignments/ps3.pdf.
[2] Le Bihan D, Breton E, Lallemand D, Aubin ML, Vignaud J, Laval-Jeantet M. Separation of diffusion and perfusion in intravoxel incoherent motion MR imaging. Radiology. 1988 Aug;168(2):497-505. doi: 10.1148/radiology."
382	Personalized Diagnostic Tool for Thyroid Cancer Classification using Multi-view Ultrasound	"It is not clear how many patients are included in the study.  Hence, it is difficult to evaluate the effect of personalized aspect of the proposed approach.
There is no discussion on how feasible is to have this tool for the clinical application."	"The conducted experiments are quite limited.
The reproduced AdaMML has poor performance, but no explanation is given.
The description of the data collection process is not completed, such as descriptions of the experimental setup, device(s) used, image acquisition parameters, subjects/objects involved, instructions to annotators, and methods for quality control. Were the 4529 sets of multiview US images collected from 4529 patients or not?"	"Personalized weighting generation module is functionally similar to the attention mechanism, which is used more frequently.
For experimental implementation,  the l is set to 0.01, the authors should provide the explanation for it.
It will be better that if the authors provide the actual time cost."
383	Personalized dMRI Harmonization on Cortical Surface	"1) The proposed local correspondence matching method only considers the geometric information and cortical thickness which does not require diffusion MRI data. It is not clear why matched geometry indicates matched diffusion MRI measures. 
2) The ""grayordinate"" format as included in the cifiti data in HCP has become standard for surface-based analysis. It is not clear why that is not an option for surface-based harmonization."	"No statistical test was done to prove the significance of the numerical results.
There is little to no discussion about the limitation of the work."	Certain colloquial language throughout the submission can be substituted
384	PET denoising and uncertainty estimation based on NVAE model using quantile regression loss	"The novelty compared with a previously published approach also based on the NVAE and enabling uncertainty estimation seems very limited. The gain in synthesis accuracy is very small and the improved computational efficiency is not quantified.
The usefulness of estimating uncertainty is not really demonstrated.
The data set used is small (20 subjects for training, 3 for validation, and 3 for testing), meaning that no strong conclusion can be drawn."	"Considering that the low-quality PET images are generated by down-sampling the list mode data, it is also important to consider other noise factors related to PET, such as noise distribution, motion, imperfect attenuation correction, etc that differentially contribute to the noise in the PET images. To bring this algorithm to a real-world application, have the authors investigated combinations of other noises to improve using the deep learning approach? Do the authors then expect that the NVAE-QR would still perform better than the other methods?
In addition the methodology while useful and applicable to an important problem in PET imaging is not that novel."	"There is no reference to prior work where the quantile loss has been used.
The dataset is very small, only 3 subjetcs were used for validation and testing.
English is not very good."
385	PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation	"Missing reference to closely related work.
Datasets are small."	This paper cites but do not show numerical results of UNETR, which is very important and necessary SOTA method.	"This submission has good quality of finishing hence I am in general satisfied with acceptance, if have to list weaknesses the following are minor rather than major:

The authors argued Volume-to-Sequence (V2S) and Sequence-to-Volume (S2V) operations as one of their key contributions, however, it's not clear to me, unless I have missed, what are those operations? Are they learnable or simple transformations?"
386	Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography	Biomechanics constraints in displacement tracking is not completely novel. Also, limited testing is performed and only one previous method is used for comparison. The work does advance the field of ultrasound displacement tracking but is not revolutionary in its approach and still needs to be combined with other displacement tracking innovations for reliable accurate tracking.	"The regularisation during the training is of course only present at train time. As such here is no guarantee that the predictions will always be physically plausible. Therefore, I recommend the authors to investigate whether known operators [1] could be applies in this scenario. This might lead to another paper on MICCAI 2023...
[1] Maier, Andreas K., et al. ""Learning with known operators reduces maximum error bounds."" Nature machine intelligence 1.8 (2019): 373-380."	"The amount of in vivo data is unknown.
The PICTURE appears to be limited to uniform axial strain (see detailed and constructive comments)."
387	Physiological Model based Deep Learning Framework for Cardiac TMP Recovery	"The main limitation of the paper is validation. 
The authors provide little details about  it. Despite the claim that they tested this method on 600 subjects, in reality ECGsim provides a  maximum of  3 geometries, since the authors did not specify, the logical assumption is that they only used 1 and simulated multiple cases on that geometry. Failure to test on multiple geometries leads to over-optimistic results,  specially for data-driven approaches.
The authors did not  specify  whether they are adding noise to the measurements or not.  In its absence, all the results will be over-optimistic.
Separating  experiments by  location and  ischemia is  logical and acceptable, however, the reader is left wondering whether the network could not generalize to all cases.
No information about which activation times or localization method is provided. Without it, it is not clear how generalizable are the results on the corresponding metrics."	The results are produced on simulated data without discussing the size of simulated data (is it good enough size) and what would be the challenges faced with application of this method on real data. The paper also doesn't have a thorough discussion, including any limitations of this work, in particular with respect to its application on real data or comparing to other existing recent methods.	"Authors argue the lack of literature in considering the temporal dynamics in this problem. However a recent work, Jiang et al 2021 titled ""Label-Free Physics-Informed Image Sequence Reconstruction with Disentangled Spatial-Temporal Modeling"", consider a very similar approach to solve the ECGI problem. They similarly consider the state space model approach and learn the transition model and the temporal dynamics in the latent space of their model and use a decoder network as emission to the observation space. Authors need to discuss how their work is compared to this work.
Authors need to revise the text as there were a few grammatical and punctuation errors in the paper."
388	Physiology-based simulation of the retinal vasculature enables annotation-free segmentation of OCT angiographs	Lack of specification of the training details, and network architectures used.	he paper used one synthetic dataset but as it did not have ground truth annotation, the quantitative performance measure was not reported. This is a crucial information as the motivation of training only with synthetic examples can only be validated if the trained model performs similarly well in real experimental data.	"The citation and explanation of some methods are not intuitive, which bring concerns to the reproducibility of the method.
The description of the manipulation of 3D-level deformation of vessels is too general and needs more details.
In the experimental part, the description of the evaluation dataset is vague."
389	Point Beyond Class: A Benchmark for Weakly Semi-Supervised Abnormality Localization in Chest X-Rays	"The novelty is relatively limited. The work seems an application of the Point DETR framework into the field of Chest X-Rays. Although the two regularization terms seems novel and interesting, the work is an incremental improvement of the Point DETR framework since there may be numerous such kind of improvement based on the framework.
Some details may need to be made clearer:
(1) In the description of Step 3, it is said that ""After the above two steps, we get a well-trained Point DETR (Fd(*, *)), which is regarded as the teacher model to generate pseudo box labels for point-level weakly-annotated data."" However, I think the training stage of Point DETR is still needed to train the model to predict box labels from point labels. Please make clear.
(2) It is said that for the student detector, two models (FCOS and Faster R-CNN) are adopted. So I wonder which model is used for the systems in Table 1, and why the results in Table 1 do not match with the results in Table 2."	"I am not fully convinced by the effectiveness of the proposed model on a single dataset. If possible, enhancing the current results by another dataset will be extremely helpful.
In Figure 2, it will be better if you could point out the key differences from the existing DETR baseline."	"My main critique is that while the overall paper is quite clear regarding the technical aspects, some of the authors claim need to be further refined or additional information needs to be stated:

The starting point of the paper is Bearman et al.'s analysis, which was done on the PASCAL VOC and not CXR images. This should be stated.
In the abstract, an improvement of ~5% mAP is claimed. This improvement should be nuanced, esp. with regard to which detector arch. / %age of labeled data was used, as the improvement can vary greatly.
The authors' claim to have produced a ""publicly available benchmark"", while there is no mention that the code used will be made available. Only the training data is currently available.
 "
390	Poisson2Sparse: Self-Supervised Poisson Denoising From a Single Image	The solution  proposed in this work, is being studied in the literature.  The only novelty comes here is work out the solution for poisson noise.	"This paper operates under the primary assumption that noise in medical images is Poisson distributed. This is incorrect, noise in the collected medical image projections is Poisson distributed, however, the reconstructed volumes no longer follow the Poisson distributed noise. The noise in reconstructed volumes is in fact very non-stationary and follows no distribution at all.
The FMD dataset has no ground truth. The authors write that the ground truth is obtained by averaging. While qualitative results may be relevant, I don't think such a dataset should be considered for quantitative analysis.
The paper is fairly light on experiments and is heavily theoretical.
Figure 2 states that the ribs have been recovered. To my untrained eye, the ribs have not been recovered, they are little better than random noise.
Table 1 shows the highest SSIM in the DIP for the FMD dataset. In Fig. 2 and Supplementary Fig. 3, The DIP image is visually the most blurry image, which usually implies a very low SSIM. In my opinion, the results have not been reported correctly."	"1)	The authors proposed to approximate traditional iterative optimization algorithms for image denoising with a recurrent neural network. However, the application of the recurrent neural network is not reflected well in the paper.
2)	Some mathematical expressions are not standardized. For example, the coefficient of loss L_N in Eqn. (12), \mu_N, is inconsistent with that in training details.
3)	The description of Fig. 1 is inconsistent with Eqn. (10), please check carefully.
4)	The method overview in Fig. 1 is not described in sufficient detail, and does not correspond to the text well.
5)	The visual results shown in Fig. 2 and Fig. 3 should be accompanied with quantitative evaluations such as PSNR and SSIM to compare different approaches better."
391	Pose-based Tremor Classification for Parkinson's Disease Diagnosis from Video	"1) The framework relies heavily on OpenPose performance.
2) It is hard to see why the system is an interpretable automatic PD diagnosis system.
3) The paper insists that using seven keypoints can be more beneficial than using all keypoints, but there is insufficient proof.
4) ST-GCN [1] was introduced in 2018, and it can be considered outdated."	"The main limitation of the paper is related to the understanding of the clinical context of Parkinson's disease, and the related motor and no motor symptoms. From the title, the authors claim that the approach allows for early support diagnosis but tremors mainly appear in advanced stages. But also, there is not any validation that shows that the proposed approach deal with tiny tremors that appear at early or promodal stages. In fact, there exist resting and postural tremors, in the recorded dataset only use postural tremors that magnify tremors but introduce external tremors as a consequence of muscle and movement of particular activities. How does the proposed approach filter out Parkinsonian tremor??. Moreover, the authors omit scale index and protocols to stratify Parkinson, such as UPRDS y H&Y. In such cases, how the approach may assume that carried out early diagnosis?. In fact in the state-of-the-art, the authors also omit some works that amplify tremors from optical strategies, which also discover natural discrimination among different types of tremors.
Regarding the methodological pipeline, the main drawback is the recovery of joint points from OpenPose which result highly noise with high frequencies among frames. How do filter such movements from real Parkinson's tremor?"	"It's hard to assess if the system really has clinical significance and if tremor classification in video recordings is enough to make a diagnosis - maybe it would be better to describe the system to be an indicator for further patient examination.
The system heavily relies on the 2D joints computed by the OpenPose framework. A more thorough discussion (and comparison of different pose estimation frameworks and framework-specific advantages/disadvantages) would greatly improve the presented work."
392	Position-prior Clustering-based Self-attention Module for Knee Cartilage Segmentation	"The continuity of segmented results may negatively affect the diagnosis of knee cartilage diseases. For example, the cartilage defects in [2]. A higher continuity in the segmented outputs may fetch up or conceal the defect situations in cartilages, which may cause diagnostic errors. Thus, I don't think the key contribution (i.e., higher continuity) that the proposed method claimed is completely matched to the medical scenario on knee cartilages.

The proposed module has a high similarity to the ""Class Center"" step in [10], which seriously reduces the novelty of this paper. And the author does not use lots of contents to do method-level comparisons with this highly relevant method.

Although the coarse-to-fine approaches ([3, 4]) and the ROI-fusion approach ([5]) did the knee cartilage segmentation, their goal is to increase the resolution of processed data while overcoming the limited GPU memory resources. Yet your goal is to propose a class-center based self-attention module and add it to a baseline model to increase some performance (e.g., the continuity). Adding your module will increase the memory cost of any baseline. Thus, comparing with these coarse-to-fine/ROI-fusion approaches may not directly demonstrate the superiority of your method.

3D visual comparisons could clearly show what your method improved in detail, yet I only saw some quantitative results."	N/A	The authors included Ref. 9 in the paper for dilated convolutions. Adding an approach that uses dilated convolutions to the comparative study will improve the quality of the paper.
393	Predicting molecular traits from tissue morphology through self-interactive multi-instance learning	"One weakness of the paper is that the proposed method mainly focuses on attention pooling/integration from tiles, and thus restricts itself to binary classification tasks. 
Binary classification is a relatively easy task compared with instance segmentation. While instance, e.g. tumor, detection and segmentation is a crucial step towards WSI classification, this method may not be used to improve instance segmentation for WSI.
Again, in Table 1, only the AUC score is reported for the classification task. What is the accuracy of each method on each dataset?
Fig. 2 is confusing. What does ""subtype and non-subtype"" mean in this figure? Should it be e.g. tumor and non-tumor?"	"Lack of ablation experiments demonstrating the effectiveness of three kinds of instance selected.
Lack of experiments for hyper-parameter sensitivity, e.g., the number of attention tiles, supplementary tiles and low-attention tiles."	There are some weaknesses in the experimental configuration that need clarification (refer to details comments).
394	Predicting Spatio-Temporal Human Brain Response Using fMRI	"Some technical details are missing. It is not clear how the fMRI graph and MEG graph in Figure 1 were generated; It is also not clear what is the role of the connectivity matrix in model training. As consequences, the theoretical soundness and the reproducibility of the study could be contaminated, and it is a bit confusing when matching Figure 1 with its descriptions in the main text.
It seems the model was trained at ROI-level as the time series within a brain atlas ROI were averaged before input to the model. Considering the functional heterogeneity of the cortex, this average operation could largely degenerate the specificity of brain activities. I have a sense that this average operation is related to high dimensionality of the fMRI and MEG data, but how would this operation affect the prediction performance of the proposed model? The authors are encouraged to justify this issue explicitly if my understanding is correct.
The authors provided qualitative and quantitative evaluations of how well high-resolution fMRI signals can be predicted. Besides Table 1, the authors are encouraged to provided more details about the performance difference among different brain ROIs. If possible, they are also encouraged to provide fMRI time series prediction for a single voxel rather than averaging over ROI."	This paper compares the proposed method with only two baselines, LSTM and GRU-D, with few comparisons.	"a) It's better to include more experiments or discussions about how behavioral representations evolve with the proposed model. I think this is the major point of the paper, however, the experiments only compared the spatial and temporal patterns from fMRI and MEG, respectively.
b) Only 3 brain networks are shown ion Section 3.1. It's better to show more networks in Fig.2. Meanwhile, the quantitative measurements should be included for a comprehensive evaluation.
c) In Section 3.2, MSE may not truly reflect the similarity between the prediction and ground truth. It's better to include more measurements such as PCC. In addition, it is not clear the similarity is averaged over all subjects? Or the time series is firstly averaged and then similarity is computed."
395	Privacy Preserving Image Registration	"It is not straightforward to come up with a real-world use case of this, outside of research/academia (but then after all, this is an academic conference), this could be motivated better in the manuscript.
Tailoring the image registration algorithms to suit the encryption is done by classical steps one would do on a very slow or old computer, i.e. sub-sample with smart schemes as much as possible without compromising quality - there is not much novelty there."	"The contribution is not clear. Like, is the cryptographic tools are adapted for the registration task?
Not sure why the evaluation metric would concern about the image similarity. Is there any difference for the results among CLEAR and other methods? Or the encrypting way will make the decoded image different with the original one?
What is other baseline methods? In reviewer's view, such proposed framework is use the cryptographic tools to perform registration task. And why not apply the framework to other tasks, i.e., segmentation?"	The main weakness of this paper lies in the evaluation and time effectiveness. The authors motivation to develop a PPIR algorithm is weak.
396	ProCo: Prototype-aware Contrastive Learning for Long-tailed Medical Image Classification	"The main contributions of this work are built on the contrastive learning and prototype learning. For the widely-studied image classification task, they are well testified on different vision-based learning schemes. By contrast, such a learning model is not extensively studied in the medical field. These are the advantages and also the disadvantages.
For the concept of category prototype, this has been widely used in few-shot learning or detection or segmentation, or others. This is not a new concept.
For recalibration, the reviewer may doubt the effect is very little to the whole learning scheme.
The motivation of this work is also less attractive. Technically, the proposed method can be adaptive to any stations, rather than the specific long-tailed situation.
The experiments are less sufficient in the present form."	"I think the paper is not in a complete form. There are many missing details to fully understand the contributions.

The problem of interest is long-tailed classification, while there is no discussion on the inference phase in Sec. 3 at all. For a technical presentation, I think it is good to present a complete pipeline.
The idea of sythesizing hard examples for contrastive learning has been proposed before. I think more dicussion and analysis for Sec. 3.1 are needed. A good example could be found in [1].
What's the intuition behind Eq. 4? I think Sec. 3.2 could also be expanded.
Similarly, Sec. 3.3, as a major contribution, should be expended. I am a bit confused: how do you train the classifier? Based my understanding, F and G are only projectors, right?

[1] Hard Negative Mixing for Contrastive Learning, NIPS 2020"	"The test accuracy of each category is not presented. Because this work is proposed to address the long-tailed problem, a confusion matrix or a table containing the test accuracy of each category is a better way to reflect the effectiveness of the proposed method. Improvements in tailed categories are expected.
The number of instances to compose adversarial proto-instances and the random interpolation coefficient are important hyperparameters in ProCo. However, they are not specified in the paper, and no sensitivity analysis or selection criteria are provided."
397	Prognostic Imaging Biomarker Discovery in Survival Analysis for Idiopathic Pulmonary Fibrosis	None	The main weakness of this work is the comparison with rather old methods (3D ResNet-18 and 3D ResNet-34).	"The proposed approach is only compared against two 3D ResNet models. Other baselines would be helpful. A regular ViT trained end-to-end for survival prediction would help to judge the benefit of the proposed clustering scheme. A shallow model (e.g. Random Survival Forest) based on texture features (e.g. radiomics) or features extracted by ResNet from step 1 would help to justify the two-stage approach.
Many hyper-parameters (patch size, number of patches, number of clusters, size of latent representation, ...), but it is unclear how the sensitive the proposed framework is their choice. In particular, the number of clusters K seems to be critical as it seems offer trade-off between expressiveness and complexity of the ViT."
398	Progression models for imaging data with Longitudinal Variational Auto Encoders	The authors proposed a novel way to synthesize images of AD patients, and provided numerical estimations of disease progression for each individual. I like how it combines linear mixed effect model and a generative model, but additional analysis of the performance of the model can be provided to support the model. For example, the authors can generate different average images for different disease stages (NC, MCI, and AD), and try to find the difference between modalities. They can also analysis different onset time and acceleration factors of disease stages, and apply statistical tests to see whether the difference is significant. Sometimes it's hard to evaluate the results with generated images for a random subject, so quantitative evaluation may be helpful in this scenario.	In my opinion, the method has not apparent weaknesses. To say something, I would like to read something about the use of interpretability with the parameters estimated by the method in a clinical application.	"The main weakness of this paper is its negligence of existing tranditional methods on image regression that works on the same problem and misses the comparsoin to traditional methods and the existing deep learning based methods.
For example, this sentence in the abract is misleading, ""few progression models for entire medical images have been proposed that allow missing data imputation and prediction at any timepoint."" In the past decade, a bunch of researchers work on image regression for longitudinal/cross-sectional medical images and many papers (just google regression on image time series or image regression) were published and achieved good performance in interpreting and prediting the entire images, images with even much higher resoultions compared to the ones used in this manuscript.
The downsampled images used in the submission show missing details in many brain structures, which also indicates the high computational cost of the proposed method. This questions the motivation of this work. Since both the regression quality and computation cost are not improved by comparing to the traditional methods, what is the motivation for using this method? Just because it is a deep learning based method?
Also, the mixed-effects model in the latent space is linear, how about nonlinear changes? Should it be limited to an age range that the evolution is almost linear? Otherwise, how to handle the nonlinear case, which is very often in brain degeneration or disease evaluation?"
399	Progressive Deep Segmentation of Coronary Artery via Hierarchical Topology Learning	-Adding evaluation results using public dataset is better.	"1) For the SAD module proposed in the method, if the coarse mask is severely fractured or has a large number of over segmentation, is the distance map accurate enough to be used for subsequent training?  It is not mentioned in the discussion of the paper. Meanwhile, if the image is cropped, the chamber structure will be destroyed, can the distance map be used correctly?
2) HTL proposed in this paper is functionally more like an integrated learning or multi-task model, and there is no detailed description of how topological constraints are provided between tasks.
3) Inadequate experimental evaluation. This paper claimed to achieve the accurate segmentation, but the results did not show the segmentation performance for the regions with the stenosis. Meanwhile, this paper proposed the use of the topology, but the paper did not use OV, OF and other indicators to evaluate the continuity of main artery segmentation."	"There are some minor reference problems.
Section 2.2 - Cube-Connectivity Prediction: <1> Qin, Y. , et al. ""AirwayNet: A Voxel-Connectivity Aware Approach for Accurate Airway Segmentation Using Convolutional Neural Networks."" 2019."
400	Progressive Subsampling for Oversampled Data - Application to Quantitative MRI	"The merits of this paper are difficult to analyze/understand between changing network capacity, interacting (because joint) mask and hyperparameter optimization, writing style, and evaluation metrics.
Despite the additional Ablation Table in the Suppl. Mat.s (which is unclear), it is difficult to attribute the performance gains.
Misleading performance numbers and unclear ""reason"" for performance.
The difference in MSE between the SARDUNet paper [15] and here are unclear ([15] reports 5-8x better MSE in Table 2, making a better description of the evaluation mandatory). ""altering the second network's input across different batches, producing instability"" indicates a problem in the setup of SARDUNet (randomly changing network inputs are incompatible with a MLP).
Across all tested architectures, the authors employ MLP architectures. MLPs scale extremely well for Diffusion SuperResolution, so increasing both the number of hidden layers and/or features (also units) are always beneficial (even beyond the depth of 4 used here). While obfuscated and ignored in the paper, the performance increase by more layers/features (=more parameters= network capacity) is - in my eyes - trivial. To clarify this, authors should 1. report the network size (parameters, maybe even FLOPS), and 2. Normalize different architectures to one network size (In contrast, finding the ""sweet spot"" for the ratio of hidden layers to features/units as an ablation/hyperparameter would be useful). As is, I expect the ""largest"" architecture (which coincidentally is larger than the baselines) to achieve the best performance (This seems to explain improvements for M <=50).
Performance of SARDUNet-NAS.
Since the SARDUNet hyperparameters are in the superset of the SARDUNet-NAS hyperparameters, the fact that SARDUNet-NAS does not consistently achieve the performance of SARDUNet implies the used hyperparameter search is not stable.
Quantitative downstream performance metrics.
Signal MSE can be misleading in light of the signal-inherent noise, so quantitative downstream analysis is required to verify the validity of predictions, e.g. FA, NODDI, fODF, etc.
The results (table 1) seem to be an MSE across N=1344, which implies all 1344 DW signals are predicted despite M = {500, ..., 10} being in the input. For these M signals the optimal network behaviour is to return the ""noisy"" input value. This implies 1. performance numbers might be biased (the difference between M=500 and M=100 might be an overfit to those M noisy input signals) and 2. it is optimal to use high-noise DWIs as input to reduce the impact of the measurement noise (e.g. use high b-value measurements which typically are low SNR). As a consequence, I believe the direct comparison in Table 1 is invalid (this effect becomes more dominant with increasing M, maybe explains performance in M>50). [see also details]
Is this paper employing NAS? This is incorrect terminology in my opinion. [see details]"	"I am unsure how relevant this task is for clinical applications. I do understand, that being able to reconstruct all those measurements from a subset allows for a fast acquisition. But what is the value of having access to all those parameter variations? From my point of view, if one is able to accurately recover those additional measurements the information content of those additional measurements must be negligible. So why would I recover those? For applications calculating properties of the diffusion tensor this should hold true as well.
The introduced improvements over previous algorithms are well motivated but the neural architecture search leaves the impression of being orthogonal to the work on this method. The architecture search would be expected to improve any method based on neural networks independent of the task. So I am unconvinced, whether it adds substantially to the state-of-the-art. If this is taken into account, this work leaves the impression of an incremental improvement over previous methods.
Evaluation only considers variations of a method which previously won the MUDI2019 challenge based on neural networks. It is hard to judge the merits of these methods in absence of other methods."	"It is important to discuss how this subsampling scheme might be used during real-world application (prospective deployment) towards an accelerated acquisition.
Difference images in figure 2 will enable comprehensive comparison as well as SSIM values
A visual interpretation during the iterative elimination of the features will be useful to enable explainable AI"
401	Prostate Cancer Histology Synthesis using StyleGAN Latent Space Annotation	"The major weakness of this paper is the clinical application of this approach. Generating new pathological images using GAN is not new. Though this paper demonstrate that the learned latent space by StyleGAN2 is able to disentangle different prostate cancer features, but how we should use this technique and whether it is beneficial to the clinics is not clear to me. Some potential application might be:
(1) use the generated images for some downstream tasks such as PCs classification or segmentation; however some previous studies show that this approach is not very effective;
(2) treat this technique as an approach to anonymize patient's data; however the accuracy of the generated images labeled by the pathologists v.s the latent cluster is not good enough; especially when we see the big discrepancy of G3 and G4FG."	"The dataset is tiny especially the evaluated one (160 256x256 patches, right?);
The paper would contribute from public repo with the code otherwise it'd be hard to reproduce;
Why not validate the results on public data (e.g., PANDA Kaggle competition data)?"	"The paper as it stands doesn't give any direct insights (e.g. via visualization) into the latent space structure or its geometry, which would be very interesting to see
Not all technical details or general ideas behind the methods are explained well enough."
402	PRO-TIP: Phantom for RObust automatic ultrasound calibration by TIP detection	This paper designs a phantom consisting of nine cones with different heights for ultrasound calibration. However, the heights' choice of nine cones should be further discussed and analyzed. Why it is the best design in this study.	"Augmentation method is not explained well
Testing is performed on devices from three manufacturers only.
The weaknesses of the method are not described well."	"Need: Although the limitations of current phantoms are described in the Introduction, it is not made clear how the proposed phantom would overcome some of these limitations (ex. manufacturing tolerances will still affect the present design) and therefore it is unclear why a new phantom is needed rather than adding an image-based refinement step to an existing phantom.
Limitations: No discussion of the limitations of the proposed method is included in the paper.
Details of CNN: The authors describe using a CNN for segmentation of the images for image-based refinement; however, many details about the implementation are missing, most critically a description of the dataset splits used for training and testing. More information on the generation of the ground truth labels is also necessary."
403	Prototype Learning of Inter-network Connectivity for ASD Diagnosis and Personalized Analysis	Lack of ablation study: Are all of the suggested components (inter-network encoder, prototype-base classifier, transformer, etc.) contributing to the high performance of the model? It is not clear how much each component is contributing, and whether, e.g., replacing transformer with a non-attention module would degrade the performance.	"The authors use 5 fold cross validation, but do not explicitly mention a validation set for hyper-parameter tuning. It is unclear how key hyper parameters for the methods and baselines have been selected and whether this was done in an unbiased way. I think it is an important point for clarification, since there seem to be at least 4 hyper parameters (\lambda_1, \lambda_2 \lambda_3 for losses and m for the prototype margin) which seem to dictate generalization.

I found some of the claims of the paper a bit strong and not as well explained:

a. Table 1, the authors claim that prototype learning provides improvements in performance when comparing BrainNetCNN and BrainNetCNN+P (with prototype learning). The AUCs/Accuracy for both are very close (in fact BrainNetCNN+P has larger error bars). There seems to be some tradeoff between sensitivity and specificity. I am not sure whether the 'improvements' would be statistically significant.
b.  ""To be specific, if a summary feature vector of a TD subject is replaced with the ASD prototype (pc=pASD), we can predict the functional degradation of FC as if the subject were suffering from ASD.""
""It should be noted that our proposed method can generate counterfactual FC patterns for a subject, which can be of great benefit and used to obtain deeper insights into the functional characteristics of a brain in regard to ASD.""
It is not immediately clear why replacing the prototype of an typical individual with that of the ASD class would necessarily correspond to generating a valid counterfactual. Could the authors please provide further explanation/references on why this is a reasonable assumption to make?"	"While I like the idea of embedding interpretability directly into the classification model, I have concerns about learning just 1 prototype for each of the normal and ASD classes. ASD is known to be extremely heterogeneous as it describes a spectrum of disorders - thus, I feel that having only 1 prototype to represent the whole class may not be the right model. There is also a large range of ""normal"". Can the authors comment on/justify this modeling choice?

There are missing details/explanations in the methods/experiments that need to be added/clarified in order to improve the understanding of the paper and results. Detailed comments are given below in question 8.

There is also some missing analysis I feel in the experimental results section that should be included to strengthen the arguments that the proposed method outperforms other approaches. Detailed comments are given in question 8."
404	Pseudo Bias-Balanced Learning for Debiased Chest X-ray Classification	not sure how realistic is the training sets, what happens when the data has no bias, or multiple forms of bias.	Questionable experiments given the problem statement and dataset definition	The major concern of mine is the paper lacks proper visualization of the biased/unbiased model. For example, model A could be biased towards gender information while model B is trained using the proposed method and is bias-balanced. It is important to show the class activation maps of the models before/after the generalized learning.
405	Radiological Reports Improve Pre-Training for Localized Imaging Tasks on Chest X-Rays	The methodology is not novel in general but the authors provide comprehensive evaluation framework for 18 localized tasks.	I think this paper is more like a technical report but not a scientific paper. The authors spend lots of time conducting tons of experiments to show the effectiveness of existing self- and text-supervised methods. However, I do not see any new methods in this paper. I think it is also okay to analyze the effectiveness of existing methods in different settings. But the authors should give a detailed analysis that why the text-supervised method is better and how the radiological reports help to improve the performance. The conclusion in this paper does not focus on the medical images, and the authors should analyze the differences in the performances of these self- and text-supervised methods in natural and medical images.	While this paper provides detailed comparison between different self-supervision methods, no principled analysis is provided for choosing the best pre-training methods for a given task. For example, given pre-training dataset A and downstream task B, how to choose the self-supervision methods based on the characteristics of A and B?
406	RandStainNA: Learning Stain-Agnostic Features from Histology Slides by Bridging Stain Augmentation and Normalization	"The combination of SA and SN is not novel, and has been already presented in the past in combination with more recent techniques (domain adversarial learning) and more pathology-informed color space extraction (stain absorption matrices), see for example[1,2,3].

The method is evaluated at the patch level, and thus computing average and standard deviation is fast, but I don't see this method scaling well at the whole-slide image level.

The method was not compared with state-of-the-art methods such as CycleGANs (that keep morphological information) or domain adversarial learning, not even with other combinations of SA & SN.

The main weakness of the paper is that there is not a statistical analysis of several runs of the methods. Given the stochastic nature of the generation of the templates and augmentations, the average (and std) for several runs for the method and baselines should have been reported to have a more robust estimation of the real performance.

[1]: Van Eycke, Yves-Remi, et al. ""Image processing in digital pathology: an opportunity to solve inter-batch variability of immunohistochemical staining."" Scientific reports 7.1 (2017): 1-15.
[2]: Marini, Niccolo, et al. ""H&E-adversarial network: a convolutional neural network to learn stain-invariant features through Hematoxylin & Eosin regression."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.
[3]:Otalora, Sebastian, et al. ""Staining invariant features for improving generalization of deep convolutional neural networks in computational pathology."" Frontiers in bioengineering and biotechnology (2019): 198."	"a lot of description is unclear
it is difficult to understand even for experienced reader
lack of cross-validation of the results"	"In my own opinion, the idea of combining SN and SA is straightforward and is trivial to deal with.  For example, a simple solution is a naive serial combination of the two components. The authors should state clearer on the advantages of your work against some simple combinations of the two components.
From methodology perspective, the overall pipeline of the proposed framework is lack of depth. For example, the random color space selection is just doing random choice among three color spaces with equal probability.
Some experimental settings are intricate. Since the proposed method belongs to data pre-process and augmentation, did other data augmentation methods (geometric transforms, noise, rotation, contrast, cutout, mixup and so on) are performed when running baseline models? In other words, although the proposed method has large performance improvements against baseline, I am concerning whether other augmentation methods can also achieve such improvements and whether the proposed method can consistency gain performance improvements besides these augmentations. Also, why all models just running for 50 epoches? Did all the models convergence?"
407	Real-Time 3D Reconstruction of Human Vocal Folds via High-Speed Laser-Endoscopy	"The literature review is very weak. In particular, it fails to explain how the method differs from the other laser endoscopy based methods [13,16,20,21], especially [21].
The algorithm description is not clear, for example it is not clear where the first two steps MS (mask sweeping), GA (global alignment) were described, since they were not named so in Fig 1 or in Sections 2.1 and 2.2.
The dataset is quite small, containing only 10 videos. The 21 Phantom videos should also be included in the dataset.
The quantitative evaluation is lacking in many respects. First, it is not clear what kind of labeling is evaluated. Second, a quantitaive comparison with other state of the art methods such as [21] is missing."	"In qualitative evaluation, L1 error and standard deviation of grid offsets are used (Table 1). The following criticism can be raised here:
(a) Justification for using these particular error measures is not provided;
(b) It is unclear how relevant are the error measures used given the context of study (diagnosis of laryngeal and voice related disorders);
(c) Are the reported accuracy and robustness of the proposed framework sufficient (in quantitative sense) for application in diagnosis of laryngeal and voice related disorders (which is the motivation for the study)."	The reconstruction pipeline is very similar to what's presented in the reference [20]. The authors do not clearly distinguish their contributions in contrast to reference [20].
408	Recurrent Implicit Neural Graph for Deformable Tracking in Endoscopic Videos	"None as such. Im a little concerned that the standard error of pixel tracking errors seems almost too small to be true? Might be worth checking.
There seems to be very little explanation of what the method is for, or ultimately what the author hopes it will be suitable for."	"My main concern is that I found the Methods section quite hard to understand. This is in part because the method is complex and requires many different concepts like Graph Networks, Recurrent Networks, positional encoding, Attention... and the MICCAI format is very limited in space.
I assume reference [1] explaines some of these in more detail, but it was anonymized for the review.
However, even so, I think the Method section could be made much more readable by slightly changing some sentences (and often the order of sentences). At multiple points, it was not clear to me whether a new concept was being introduces or the previous concept was refined further (some examples in the details below). Similarly, some concepts are explained at one point in the text and the corresponding equation comes multple sentences later, making it hard to follow. Overall, I think the Methods section should be reworked to ensure a clear flow through the paper, introducing one step after the other."	As mentioned in the paper, it extends a previous method. There's nothing wrong about this but then a lot of space is used for explaining the previous methods.
409	Reducing Positional Variance in Cross-sectional Abdominal CT Slices with Deep Conditional Generative Models	"The main purpose of reducing position variance is a little bit vague. In the abstract and introduction, the paper just emphase tacking the 2D slice positional variance problem. But what are the direct clinical application is not clearly written. It is not friendly for readers who are not experts in cross-sectional abdominal CT slices.
I do not know why it is necessary to synthesize an image at a pre-defined vertebral level. I would like to see an explanation why not conducting registration to find a pre-defined vertebral level slice. Or use organ/vertebral information as reference to find the pre-defined vertebral level slice (see the last column in Fig. 3, it is not difficult to localize the red line according to this view).
The synthesized CT slice seems very different from the target (see Fig. 3). Is it really helpful for clinical applications? The author should discuss more on this.
Why do we need to reduce positional variance for single slice longitudinal analysis instead of 3D patches? Since this paper proposes a new application, it is important to illustrate more.
Muscle area and visceral fat area may change during time. Does the results in Fig. 4 verify the effectiveness of the proposed method in reducing positional variance?"	"I'm concerned about the clinical use of this work as this cross-sectional 2D scans are generally done to assess body composition. I doubt that a generative approach would be the best way to solve this problem, given the fact that it doesn't take into account shape, boundary information and heterogenous soft tissues.

The backbone of the proposed approach is VAEGAN, so it can't be considered a novel approach from technical perspective."	"Although i appreciate authors' effort, some words used in the current manuscript are not clear. For example, harmonizing ""
By computing body composition metrics on synthesized slices, we are able to harmonize the longitudinal muscle and visceral fat area fluctuations brought by the slices positional variation."" or 
""we demonstrate that the proposed method can consistently harmonize the body composition metrics for longitudinal analysis.""
I expect that  authors would show the difference between VAE and their proposed method because they claim that they extend VAE method in their paper. It is not clear how authors extend current method in the literature.
I would also expect to see CycleGAN and Pix2pix for fair comparison."
410	RefineNet: An Automated Framework to Generate Task and Subject-Specific Brain Parcellations for Resting-State fMRI Analysis	The architecture of RefineNet was not cleary described.  And the authors stated that they obtained the optimized functional brain parcellations with higher temporal coherence, in order to validate it, they plotted parcellation cohesion in  Fig.3, but why is the cohesion of RefineNet than that of Combined?	none	"1) The clarity of the paper, especial the method section, could be improved.
2) The method to select the best hyper-parameter configuration are not clearly described.
3) It is not clear the improved accuracy is from the integration of the prediction task with the process of individualized parcellation learning. It seems like the RefineNet-only model cannot consistently improve the performance comparing with group-level parcellation that most of the individualized parcellations claim to outperform. Then, it is necessary to obtain the brain parcellation with the state-of-the-art individualized parcellation method and adopt the corresponding deep learning framework used in the three tasks. Then compare the results with the ""Combined"" proposed in this paper."
411	Region Proposal Rectification Towards Robust Instance Segmentation of Biological Images	I am both fine with the methodology part and the empirical results part. This paper has some novelty and provides new perspectives to redefine the procedure of biological instance mask generation. It's relative good and solid application paper, and I don't see much weakness.	"No evaluation experiment on the hyperparameter K. I guess for different datasets, the best K would be different.
How many parameters and memory consumption are introduced for the attentive FFN? Since K is relatively small, I guess the memory consumption would be ignorable. However, these details are still expected to be included.
If possible, add a section about related works."	The rationale for RPR module was to improve volume and shape quantification. However from the results in Table 1, the ASD improvement for segmentation is lower than that of bbox regression. Only average precision is used as the evaluation metric. Dice coefficient and average surface distance for segmentation task might provide additional information on the value of RPR module.
412	Region-guided CycleGANs for Stain Transfer in Whole Slide Images	"The generation of the bounding boxes for region-guided discriminator needs manual operations. That is, a series of carefully-designed image processing techniques is required. This weakens its feasibility in practice.
The experimentation setting is problematic. The introduced method focuses on an ROI-based discriminator that can take patches in various sizes as input. However, in the experimental setting, the authors fix the bounding box size as 48x48. The motivation to introduce an ROI-based discriminator and the experimental setting is conflict. The original patch discriminator also works if the bounding box size is pre-fixed."	"Annotation efforts: The method requires additional annotations to train the region-guided discriminator. The authors have used an image processing-based approach for generating the library for training the discriminator. This implies that the quality of the annotations will be a key factor in the performance of the discriminator.
Limited application: The usage of the proposed discriminator is provided for a specific application of stain transfer. Can there be other potential applications of this approach?"	"The clinical effectiveness of this approach is not clear. What is the actual clinical application of this approach? In other words, who will be the eventual consumer of the synthesized IHC images? If it is the medical professionals, then there are missing evaluations of how this method may facilitate the identification of metastatic cells by medical professionals. In particular, how many chances will they find the transferred stains make it easier to spot metastatic cells, and how many chances it may compromise the reading of images? If the synthesized IHC images are going to be used to train models to automatically detect metastatic cells, then there is missing comparison of methods using H&E stains directly.

There is missing ablation study on the effectiveness of library generation. The validity of the proposed method largely relies on the correctness of the cell boxes. From Section 3.2, it seems that the generation process is quite heuristic, and thus the generated boxes may not be accurate. What is the actual accuracy of the library generation, and how poor and good cell boxes may affect the performance of stain transfer?

There are missing comparisons with baseline ideas. While the proposed method is reasonable, it is unclear how this method is positioned when compared with the straightforward alternatives (if not better, why apply this idea then?). Two possible ideas: 1) applying loss attention masks to the patchGAN map. The loss attention mask is generated from cell boxes, i.e., we have high attention values in box regions and low attention values otherwise. 2) Cropping the input image to the cell box regions (we will also need negative samples, of course) and using the cropped samples to train CycleGAN models. During testing, we could simply use input with larger sizes thanks to the translation invariance of CNN."
413	Regression Metric Loss: Learning a Semantic Representation Space for Medical Images	Minor aspects of the paper require better clarification (see details in section 8. below)	The description of the methodology as well as its practical justification however lacks of clarity. This makes the paper difficult to read and its methodological impact not clear enough for a conference like MICCAI. I would not accept this manuscript in its current form, but I would also recommend the authors to give more maturity and clarity to their work, as I believe it could have a good potential. More specific comments are given below.	There is no significant test to valid the proposed loss. And it is not clear that if the results are stable during different runs.
414	Reinforcement Learning Driven Intra-modal and Inter-modal Representation Learning for 3D Medical Image Classification	"1) Experiments are shown only on one dataset making its generalizability a bit limited. 
2) The size of the dataset used is too small.
3) No qualitative results are shown on how sematic inter/ intra modality networks are performing."	"Major concerns:

I totally agree with the challenges of multi-modality learning (paragraph 1 of Introduction). Could the authors show that the proposed method actually addressed the problems, for example, with feature visualization?

Is it possible that the accuracy boost is due to the increasing of model sizes? The authors compared with several other models with different model architectures, hence different model complexities. Is it possible that a larger/smaller baseline model can outperform the proposed method?

The organization of the paper is very poor. Without reading about the experiment, I don't have any idea what's the task and why it needs to be solved by reinforcement learning.

What are the actions of the agents, a continuous number? According to Fig1, the actions of intra-agent is used to modulate an intermediate layer of the modality-specific network. But how?

Questions regarding the experiments:

The authors preprocessed the input images and only used the lesion area for classification. This is concerning, as the whole process can be regarded as semi-automatic method.
The training procedure seems very strange to me. Why three optimizers? Did the authors only used fixed learning rates? If so, is it possible that a different learning rate will greatly boots the prediction accuracy?
The authors mentioned that they used 10-fold cross validation. However, they mentioned again that the best model is used to evaluate the testing set. It seems contradictory to me."	"(1) For the proposed model, how to obtain the final classification results? 
(2) Compared M2Net with the proposed model, they all adopt the modality-specific network and shared network. There, what are the main differences and its advantages? More discussions should be included. 
(3) Multi-modality fusion is a wide research topic, thus the related fusion methods and multi-modality learning algorithms should be discussed.
(4) The dataset looks small, with only 165 subjects."
415	Reinforcement learning for active modality selection during diagnosis	Not that extensive comparison with related literature in theoretical and practical terms. Major assumption that each new modality will give information that is beneficial for the diagnosis and that there is no overlap in these potential new bits of information. A causal analysis of the necessity and sufficiency of the inclusion of the modalities	"The scope of the study: The authors simplify the usable modalities to be represented as valued vectors. However, it is non-trivial to get these representation vectors for some common modalities in real practice, such as images.
The number of selected modalities is small, which may be solved by heuristics-based methods. Meanwhile, the cost for the modality specified by the authors is arbitrary. Hence, it is trivial to make a comparison.
The decision-making process is quite unclear as the method can only get some value numbers for each modality and does not get any meaningful interpretation for a specific patient. The discussion by the authors in 4.3 is result-driven.
Given the simplified assumptions, this work is more of a proof-of-concept. The work needs to be carefully designed for more complex real-world problems."	SOTA comparison: Despite having adequality shown the superiority of their strategy with respect to a simpler population-wise feature selection (both in terms of prediction error and stability, which I acknowledge them for), comparison with other SOTA approaches were not included (namely those mentioned in Section 1 based on patient-specific feature selection, etc.).
416	Reliability of quantification estimates in MR Spectroscopy: CNNs vs. traditional model fitting	"The investigation was somewhat limited in scope - however, I do not perceive this as a major limitation, as the authors have clearly identified this limitation in the paper, and have provided a roadmap for further investigations.
The quantification network seems to be based on generic (non-spectroscopic) references [25,26], but it is not clear how these methods compare to deep learning methods designed specifically for spectroscopy like those in [10,11,12]. It would have been nice to see this same evaluation on additional methods, since not all deep learning methods are expected to behave in the same way."	"The application area is quite niche - model-based spectroscopic MRI - but nevertheless of interest to some MICCAI attendees and the problems and conclusions highlighted are more broadly important (they have been shown before in diffusion MRI - ref [9] in the paper - but the confirmation in a different application is valuable).
The study is simulation only.  A demonstration of how the issue might manifest using real data sets would have added a lot to the paper."	"The main weakness of this paper is the lack of an example on actual MRS data (whether in-vivo or a phantom). I am aware that this is a synthetic study, but just a small real-world example of the DL failing in a pathological case would have greatly strengthened the paper.
Furthermore, one of the papers cited as an example of deep learning quantification [10] also conducts an uncertainty analysis, and concludes favorably on the deep learning method. I think it is necessary to address this/discuss the potential contradiction."
417	Reliability-aware Contrastive Self-ensembling for Semi-supervised Medical Image Classification	For me, it is a nice miccai paper submission without obvious weakness	Some details about the experiments are missing, which should be added in the revision.	While this papers describes the proposed learnable weight function from the perspective of reliability, no evidence or evaluation of how this weight function improves the reliability is demonstrated. Instead, only the accuracy metrics are used.
418	ReMix: A General and Efficient Framework for Multiple Instance Learning based Whole Slide Image Classification	"While the authors ablate each of the proposed augmentations, it is unclear which augmentation is more useful in a general sense i.e., the considered baselines (ABMIL,DSMIL) show gains with different augmentations.
'Mix-the-bag' is a key contribution of this work. However, results only show isolated evaluations on different components of the strategy. It would be beneficial to have included 'Mix' i.e., all combinations of the augmentations ( append + replace + inter. + covary ) in the evaluation to better assess the generality of the idea.
It is unclear how such augmentations would work with recent works that have spatial WSI reasoning such as TransMIL [1]. This work should have been included in the evaluation to better hightlight potential failure cases.

[1] Shao et al. Transmil: Transformer based correlated multiple instance learning for whole slide image classification. NeurIPS (2021)"	"The novelty is limited, either Reduce or Mix. They are somewhat straight-forward processing strategies which have been used in WSI-related application before. See my detailed comments below.
No comprehensive comparisons with other MIL baselines.
Not easy to be used in the practice."	"The training budgets comparison in Table 2 may be over-claimed, ignoring the cost of necessary pretraining.
The reduce step may lose the spatial information, which is important for specific WSI tasks."
419	RemixFormer: A Transformer Model for Precision Skin Tumor Differential Diagnosis via Multi-modal Imaging and Non-imaging Data	"However, there are some places that are not clearly expressed.
In Section 2.2, ""When DWP is turned on (based on p > Tp, p  [0,1])"", what does p mean here? How is the p obtained?
What are the global features and local features?
In fig. 2, what does patch token mean? It's never shown in the main text.
Section 2.3 is very confusing. 
For example, the authors mentioned gc or gd are generated by GAP layer, gm is generated by LN layer. But in the formulas below, it's zx and gx' that are generated by LN and GAP. I don't know if the gx' here represents the same gc, gd, gm mentioned before, or it means something else. 
Also, I suggest to add the notations lc, ld, and lm(if there is one) to the figure 2."	"1) Although the chosen problem is exciting, technical novelty is still missing. Authors use state of the art Swin Transformer for a given task.
2) The proposed architecture seems to have multiple branches for each modality, however the computation complexity, number of parameters are not discussed. 
3) In table 1, the second best performing network would be Inception-comb. With respect to this the performance exceeds by 5.5% in terms of average accuracy and not 12 %.
4) Augmentation might not be clinically sound. 
5) Experiments sound a bit weak.
6) May be please add significance test to check the significance of the results."	"It is hard to reproduce the method as lots of details are missing in Figure 2. How patch are embeded? What is RS? What is M?
parameter settings of detailed architecture not disclosed."
420	Removal of Confounders via Invariant Risk Minimization for Medical Diagnosis	The numerical improvements of the proposed ReConfirm are sometimes marginal with respect to the traditional Empirical Risk minimization principle, so further testing could improve the result of the experiments, especially in view of a future submission to a journal.	"The comparative studies seems limited.

More datasets and backbones like transformer can be added to verify the generalization ability of ReConfirm.
More existing methods could be included for comparisons."	"The evaluation of the method is weak, in my opinion. In particular, comparison with adversarial learning based invariant representation learning would have been useful. I suspect that with the used environment definition strategy, such invariant representation learning methods would also work quite well.

The effect of the class conditional penalties is unclear. Although I understand the intuitive motivation of allowing learning of class conditional invariant representations, I do not understand why one would apply such a penalty to only one of the classes in question. Further, I could not find a satisfactory explanation as to why the setting with the penalty on only the control class (cReConfirm y=0) leads to the best performance most of the time."
421	RepsNet: Combining Vision with Language for Automated Medical Reports	"Novelty in terms of network design is limited but the proposed methodology is new.
Comparative in terms of space and time is missing.
Network training needs more elaboration."	The transfer of the developed methodology to clinical practice remains uncertain. Limitations of the work are not illustrated.	"Most parts of the proposed method are a combination of existing approaches: bilinear attention network (Kim 2018), contrastive vision and language learning (Chen 2020), and prior context knowledge (Johnson 2017).
Experimental results:
  1) No BLEU evaluation results on Med-rad 2019. The results only reported accuracy evaluation in Tabel I on Med-rad. 
  2) The experimental setting of Med-rad is not clearly stated performed on the validation set or the test set. 
  3). The author claims their RepsNet for medical reports generation task. However, the authors only performed generation task evaluation on the IU-Xray dataset."
422	Residual Wavelon Convolutional Networks for Characterization of Disease Response on MRI	"There is already a body of literature on wavelon networks, and the addition of short-cuts for residual learn is not very novel, but is worth investigating.
Overall, the results produced by the best wavelon residual network (RWCN-ResNet-15) are only slightly better than the best conventional CNN (ResNet-50). Also, the numbers in Table 2 and Table S-1 are generally quite high, which indicates the chosen clinical tasks may not be challenging enough for the RWCNs to demonstrate their theoretical advantages.
The three data sets used for the experiments are small and the results presented cannot be assumed to be representative of the distributions of rectal cancer and Crohn's disease patients."	"(1) Some description in section 2 is not correct.
(2) The contributions of this paper are not clearly described, and lack of innovation.
(3) The experimental part is not compared with the improved method, nor with the advanced method in recent three years;
(4) Too little validation data."	No obvious weakness. Just I suggest a source code of the deveolped layers and networks.
423	Rethinking Breast Lesion Segmentation in Ultrasound: A New Video Dataset and A Baseline Network	"It would have been valuable to describe some details of the proposed video ultrasound segmentation dataset. e.g Types of lesions, the number of the subject would be helpful.
Description of the loss function is insufficient. e.g. How the binary cross entropy loss, and dice loss are weighted."	"A.	The result does not present the computational complexity required for the proposed dynamic selection algorithm. The sorting and cosine similarity calculation is carried out for every frame, which might have adverse impact on the inference time.
B.	Descriptions and qualitative assessments are insufficient for a dynamic memory selection algorithm."	The methodology contributions are not significant. Most models are well studied, especially in natural video segmentation community. The model is like to combine existing techniques together.
424	Rethinking Surgical Captioning: End-to-End Window-Based MLP Transformer Using Patches	"Although the authors specified their difference from ViT and Swin Transformer, I may still find it a lack of their own uniqueness and technical contribution. This judgement might somehow reduce its overall impression, as Swin Transformer might be the main reason for the reduction in computation burden.

The authors claimed to have their design with less-expensive computation cost, however, I did not find it well-justified in the result section. A runtime comparison with other counterparts and efficiency analysis (GPU, #FLOPS) is suggested, so that their efforts paid in efficiency improvements can be quantitatively demonstrated. Besides, the authors mentioned about ""real-time robotic surgery"", could this be justified by numerical results as well?

Qualitative comparisons with SOTAs on captioning demonstration are preferred but missing in the manuscript. The quantitative results in Tbl.1 does not look convincing enough to demonstrate the superiority of the proposed method."	"""Nonetheless, the feature extractor still exists as an intermediate module which unavoidably leads to inefficient training and long inference delay at the prediction stage""
""De- spite the impressive performance, most of the approaches are required heavy computational resources for the surgical captioning task which limits the real- time deployment""
""take the im- age patches directly, to eliminate the object detector and feature extractor for real-time application""
""These lead to inference delay and limit the captioning model to deploy in real-time robotic surgery.""
""reduces the training parameters, and improves the inference speed.""

In the work the authors highlight the advantage of their approach in terms of efficiency and speed (point 1 to 5). No comparison regarding flops or fps between detection and detection free models. The model speed and efficiency is not necessarily restricted if its not end-to-end. In summary I cannot follow the argument of the reduced complexity and efficiency.
In Fact ResNet has significantly less parameters and flops compared to the used Swin-L model.
Parameters resnet-18: 11mio
Parameters Swin-L: 197mio
Flops (224x224) resnet-18: 2 G
Flops (224x224) Swin-L: 34.5G
------ 
The metrics for SwinMLP are significantly improved compared to Swin. However, the performance of Swin and SwinMLP should be comparable and Swin should also be comparable to Transformer[5]. It would be good to see if there is any rational for this difference as this makes the results less trustworthy. 
------ 
Evaluation missing against:
Zhang, J., Nie, Y., Chang, J., Zhang, J.J.: Surgical instruction generation with
transformers. In: International Conference on Medical Image Computing and
Computer-Assisted Intervention. pp. 290-299. Springer (2021)
------ 
Ablation missing between 2D, 3D and 2D/3D. Also, this should be compared to a 3D baseline e.g. 3D ResNet.
------"	"-Limited novelty: the paper's main improvement is from the visual backbone, which is swintransformer. However, there is little discussion about why a unified structure is needed and is there any other more powerful convent is not suitable for the caption task. 
-Experiment: The paper modified replaces the multi-head self-attention with multi-head MLP and provides the time complexity comparison. However, the third row in table 1 does not show the FPS and the table 3 does not show any parameter improvement compared to the swin transformer. 
-Baseline: the baseline methods in the experiment are not the state-of-the-art method. I think there are many captioning methods using the fully transformer."
425	Rethinking Surgical Instrument Segmentation: A Background Image Can Be All You Need	"dataset size for Syn-A, B, and C are different
*the segmentation performance is not very good for purely synthethic case. The authors don't emphasize the Table 2 results properly. That is the most interesting part of the paper."	"It is not clear what is the technical novelty of the paper.
The approach is very similar to the AugMix and there is no comparison with other existing approaches used to create synthetic datasets (i.e. GAN based).
The results in the experimental section are not very convincing. It seems that the approach produces good results on the simulated images. However, when the approach is tested on the unseen target domain (EndoVis-2017) the improvements drop significantly.
Fig 3b is not discussed in the paper."	"The main weakness in the paper is that the results are not separated/collected in a way that would clearly show the magnitude of improvement caused by the method. The authors have done some work to provide contextualization, but much more could be done to really make the method shine. See the Constructive Comments for particular suggestions.
The paper is also missing some brief discussion of where the method may be conceptually lacking (i.e. instrument shadows / interaction with anatomy, keeping instruments from crossing/obstruction in multi-tool scenes, etc...) which could help contextualize where it fits in with more data-hungry methods such as GANs which could theoretically handle these things."
426	Retrieval of surgical phase transitions using reinforcement learning	"(1) The experimental validation is not convincing. In Table 2, the last two rows only list the performance of the proposed method under not full coverage which is not convincing. It is suggested to show the performance of the proposed method under full coverage. It is also suggested to compare with more previous work.
(2) Reference [1] 's information is not given. It is suggested to give this information.
(3) Since it is well known that RL methods take more computational resources for training. It would be good to compare the computational efficiency of the proposed method to previous work besides the performance comparison."	"The novelty of the method is limited, and lacking in details.
Paper title does not match the content and work presented.
The results are below the baseline on one dataset which makes its hard to justify the efficacy and generalization capability of the proposed method."	"The authors propose an offline method but only compare to online methods
There are several open questions regarding the method design which indicate limitations of the proposed task formulation. These open questions are mostly related to how the model behaves in edge cases (phase does not occur, phase is predicted inside another, average frame index is out of range, constraint ""f_nb < f_ne"").
The way windows are traversed by the agent might make the task unnecessarily difficult.
Why was DQN chosen since there are many newer RL methods?
It is not clear how the modification of the baselines affects performance.

The weaknesses and possible solutions are discussed in more detail in section 8."
427	Revealing Continuous Brain Dynamical Organization with Multimodal Graph Transformer	"Some critical technical details are missing. For example, how was the T1-to-T2 heterogeneity map incorporated in the model? How do the fMRI and Meg graphs relate to functional correlation matrices in Figure 1? Where does the functional gradients in section 3.2 come from? Considering the space limit, part of 2.2 can be removed to Supplementary materials.
What do the results tell us about the dependency of functional connectivity on anatomical structure, or structural connectivity?
Model training details are missing, which may degenerate the reproducibility of the study.
The authors showed the connectivity patterns for one hemisphere. Why not both hemispheres as there exist both strong structural and functional connections between some symmetric brain regions?"	Some explanation about the concepts are not detailed provided in the paper, such as heterogeneity, structure-function coupling index.	"The major weakness of the paper is the writing quality, which significantly confuses audience. Here are some examples:
a) G_i = (V, E_i) is used to represent the heterogeneous graph representation. According to the illustration, i=1,...,N represents the brain ROIs, which means for each ROI, there is a graph G_i. However, I think G_i represents the graph for different modality in the first paragraph of Methods section.
b) In the same paragraph, what are the multivariate values X_A and X_B? Are they features of nodes in graph? Similarly, what are the Y_A and Y_B?
c) In Section 2.1, N was used to represent the number of neurons. What is the meaning of neurons here? Is it corresponding to the brain ROIs mentioned above?
d) In Section 2.1, it is mentioned that fG, L^j_G, L^y_G are graph encoder networks. What is graph encoder network? How is it implemented? Z^+_t is introduced to represent the value after discrete operation. What is the discrete operation? I think Z^t+_t is not used in Eq.2 and afterwards.
e) How can we reach Eq.4 from Eq.3? The graph encoder shows up here again. Is it same as the graph encoder network in Section 2.1? 
f) ...
Overall, it is hard to figure out the input of each module, data flow, implementation details of the whole framework."
428	Rib Suppression in Digital Chest Tomosynthesis	"The network design aims to predict the I_delta. What if the network predicts the I_rs and then reconstructs and then refined with another 3D network? That would also be a rational design.
Table 1, is there any statistical difference in the improvements?
One of the major concerns the reviewer has is the real data study. The authors only collected 5 real data in total for the reader study, which should be expanded.
Following the above comment, it is clearly seen in Table 2 that the proposed method does not really outperform the baselines on average for the clinical real data set. This means the method cannot generalize well to the real-world dataset, unlike the 3rd and 4th columns where it performs reasonably well on the simulation dataset since there is no domain shift. To alleviate the domain shift and to use this method, it is required to construct/acquire real data with and without rib which is impossible. So how can this method be deployed in real-world scenarios still remains unclear."	"The main concern is the actual evaluation on the clinical dataset : small datasize.

Details on the networks and training configurations are missing."	"Data preparation is unclear: ""in-paint rib mask with surrounding tissues"" lowers the confidence of 2D projection. There is no tissue inside the 3D rib regions, and if someones want to eliminate rib artifacts, they need to set the maximum transparency of those regions. Hence, final pixels compositing will not count the presence of ribs. Filling the 3D rib regions with interpolation from surrounding tissues will cause inaccurate tissue appearance, leading to unreliable analysis.
The role of the merging module (aggregation-net) needs further discussion. One counter-example is that the difference of two inputs to the \mathcal{F} should be minimal, which results in V^M_{\alpha, \Delta} being the average of these two inputs. It leads to having the \mathcal{F} model will be over-parameterized."
429	Robust Segmentation of Brain MRI in the Wild with Hierarchical CNNs and no Retraining	I don't see any mention about when the proposed method fails either in not extracting the desired structures or producing outliers. Also, it would be beneficial to know what MR conditions affect the model's outcome.	"The authors only provide comparison in terms of Dice parameter. Other metrics should also be provided. 
The authors could provide a fair comparison with other state-of-the-art techniques (in recent challenges) using public databases so the benefits of SynthSeg+ could be better displayed.
A deeper discussion on related works should be provided, as the authors focus mainly in SynthSeg as a precedent."	"1, Technically, the novelty of this paper is relatively weak because its major idea is a modified SynthSeg network where both U-Net and Denoiser network used in the paper have been proposed in other references.  This paper just put them into a new project to carry out the segmentation for brain images.
2, The article mentions that the training dataset in S1 and S2 is augmented with four linear transformations. However, the denoising model is trained with some distorted images which will bring some negative effect on the training of S2. Therefore, the experiment still needs more reasonable schemes.
3, The composition of rotation, scaling, shearing, and transformations should be a liner transformation as well. It is not a nonlinear transformation. The author does not tell us how to choose the parameters of four transformations and how many mappings are used in the article."
430	RPLHR-CT Dataset and Transformer Baseline for Volumetric Super-Resolution from CT Scans	The paper aims at two objectives at the same time (introduce dataset and propose network), which results in non of them are explained in a sufficient manner.	"1)	The importance of the ""medium-sized"" dataset is hard to comprehend. Authors mentioned the work of Li et al. [1] that collected 880 real pairs also. What is the main difference with that dataset?
2)	It is challenging to understand the difference between ResVox, SAINT, and TVSRN in terms of SSIM in Figure 3. The statistical significance tests need more detail to really show the benefit of the proposed model. A table with confidence interval with these results can be placed in the supplementary material to help the reader.
3)	Table 1 shows the importance of the picked architecture, suggesting a performance boost compared to the ViT. However, the rest of modifications differ in the 3rf digits of SSIM. So, the improvement is at best marginal. Perhaps, other image quality metrics are needed to support authors claims."	"1) The patch examples of the RPLHR-CT dataset may be supplied in the supplementary. Meanwhile, the visual comparison between RPLHR-CT and the other dataset is recommended to supply. 
2) TVSRN should be tested on other datasets to further reveal the robustness.
3) For the visual comparison in Fig. 4, it is better to give quantitative results like PSNR or SSIM. Because the visual differences of the compared methods are not easy to observe, it is more intuitively displayed through quantitative indexes."
431	RT-DNAS: Real-time Constrained Differentiable Neural Architecture Search for 3D Cardiac Cine MRI Segmentation	"*	MS-NAS [1] is an existing method. The NAS part of the proposed method is also similar to one of the comparison methods HW-Aware NAS [2] and [3] with cell-level and network-level search, the only technic contribution would be the combination with a genetic algorithm which is used for network path selection optimization 
*	Details of data processing are missing, which may add difficulties to the reproducibility of the paper
*	The experiments setting is not convincing. the comparison methods may not be the state-of-the-art, there is one paper from Medical Image Analysis that worked on the 3D cardiac cine MRI segmentation with manually designed network architecture and obtained better results [4]. Also, there are other latest NAS methods for medical image segmentation, such as [5].
*	The results gap between ICA-UNet and RT-DNAS is quite trivial based one the second row and last second row in table 2. It seems the difference between these two methods will decrease when the hyperparameter n of ICA-UNet and the N of RT-DNAS increases
[1] Yan, X., Jiang, W., Shi, Y., Zhuo, C.: Ms-nas: Multi-scale neural architecture search for medical image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 388-397. Springer (2020)
[2] Zeng, D., Jiang, W., Wang, T., Xu, X., Yuan, H., Huang, M., Zhuang, J., Hu, J., Shi, Y.: Towards cardiac intervention assistance: hardware-aware neural architecture exploration for real-time 3d cardiac cine mri segmentation. In: Proceedings of the 39th International Conference on Computer-Aided Design. pp. 1-8 (2020)
[3] Bosma, M., Dushatskiy, A., Grewal, M., Alderliesten, T. and Bosman, P.A., 2022. Mixed-Block Neural Architecture Search for Medical Image Segmentation. arXiv preprint arXiv:2202.11401.
[4] Dong, S., Pan, Z., Fu, Y., Yang, Q., Gao, Y., Yu, T., Shi, Y. and Zhuo, C., 2022. DeU-Net 2.0: Enhanced deformable U-Net for 3D cardiac cine MRI segmentation. Medical Image Analysis, 78, p.102389.
[5] He, Y., Yang, D., Roth, H., Zhao, C. and Xu, D., 2021. Dints: Differentiable neural network topology search for 3d medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 5841-5850)."	Some information/comments about the computation/time required for neural architecture search would have been nice.	few bits of information is missing.
432	RTN: Reinforced Transformer Network for Coronary CT Angiography Vessel-level Image Quality Assessment	"One of the major weaknesses of this paper is the relatively small amount of data (CCTA scans from only 40 patients) that is used for evaluation. It isn't publicly available, either.
From my point of view, the number of small issues pile up to a significant overall weakness of the submission."	The proposed method was evaluated on one hospital which did not include CCTA images with different resolution. Only one doctor did the visual assessment of the quality for the CCTA images. So, the presented work might need more evaluation on its accuracy, effectiveness and robustness	"Despite the strengths, I have the following concerns and questions:
1, the dataset is relatively small making the significance of the finding suffer. Also given the fact of a private dataset is used here, more details of the labeling process should be disclosed.
2, since the centerline tracking algorithm comes first place before the proposed algorithm, I am wondering how the performance will be if the failure of this algorithm happens.
3, how is the crop size is determined? Would it affect the final performance?"
433	S3R: Self-supervised Spectral Regression for Hyperspectral Histopathology Image Classification	"The author's introduction and visualization of the data structure of HSI images is not very clear. Compared with regular RGB pathological images, is it just a few more channels? What are the specific challenges for pathologists to analyze HSI images in human vision?
For S3R-CR, the necessity of band dropping and is debatable, since the regression targets is the coefficients of the bands that were not dropped.
Similarly, for S3R-CR, the necessity of spatial masking is debatable, since the regression target is the missing band.
Overall, the masking operation is far fetched. The relationship between CR and BR is not direct.
Moreover, the ablation study is insufficient. Image masking and band dropping are not the premise of CR. Image masking is not the premise of BR. So what would the performance of the model be without image masking?
Missing standard deviation of ACC."	"The authors highlight that S3R method can help explore the morphological characteristics of tissue images. However, no illustration or intuitive explanation is provided to address this contribution.
The results from all the experiments having missing error bars to understand the stability of proposed algorithm."	"This paper lacks the literature reviews on unsupervised / self-supervised methods for HSIs[1].

The baseline models are insufficient to demonstrate the improvement. This paper did not evaluate the performance from classical unsupervised-learning methods since the linear coefficients among different bands from HSI can be closed-form solutions, and the features could be mathematically formed. For example: [2]. Meanwhile, this paper did not compare the SOTA deep learning based models for spectral-spatial feature learning of hyperspectral images. For example: [3,4,5].

This paper did not provide the 3-band results of the proposed methods, which is not fair to the baseline contrastive learning models since the selection of bands might highly influence the performance.

[1] Ortega, Samuel, Martin Halicek, Himar Fabelo, Gustavo M. Callico, and Baowei Fei. ""Hyperspectral and multispectral imaging in digital and computational pathology: a systematic review."" Biomedical Optics Express 11, no. 6 (2020): 3195-3233.
[2] Li, Kun, Yao Qin, Qiang Ling, Yingqian Wang, Zaiping Lin, and Wei An. ""Self-supervised deep subspace clustering for hyperspectral images with adaptive self-expressive coefficient matrix initialization."" IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 14 (2021): 3215-3227.
[3] Mou, Lichao, Pedram Ghamisi, and Xiao Xiang Zhu. ""Unsupervised spectral-spatial feature learning via deep residual Conv-Deconv network for hyperspectral image classification."" IEEE Transactions on Geoscience and Remote Sensing 56, no. 1 (2017): 391-406.
[4] Yue, Jun, Leyuan Fang, Hossein Rahmani, and Pedram Ghamisi. ""Self-supervised learning with adaptive distillation for hyperspectral image classification."" IEEE Transactions on Geoscience and Remote Sensing 60 (2021): 1-13.
[5] Hong, Danfeng, Lianru Gao, Jing Yao, Naoto Yokoya, Jocelyn Chanussot, Uta Heiden, and Bing Zhang. ""Endmember-guided unmixing network (EGU-Net): A general deep learning framework for self-supervised hyperspectral unmixing."" IEEE Transactions on Neural Networks and Learning Systems (2021)."
434	S5CL: Unifying Fully-Supervised, Self-Supervised, and Semi-Supervised Learning Through Hierarchical Contrastive Learning	"Novelty
Authors should more clearly differentiate the proposed method from most relevant methods, such as BYOL, SupCon.

Experiments and results
(1) In experiments, authors choose three methods (i.e., CE; CE+SupCon; semi-sup MPL). However, pure self-supervised baselines are not includes. I strongly suggest authors include SOTA self-supervised methods as well.
(2) The results on NCT-CRC-HE-100K dataset are very high while the results of Munich AML Morphology dataset are very low. Is it because the second task more challenging? Why? How is the performance influenced by the difficulty of dataset? 
(3) On Munich AML Morphology dataset, the results of MPL are strange (i.e., the F1 score is not increasing with more labeled data). 
(4) Authors say that the learned features of self-supervised methods and supervised methods are different. But in Fig. 3, we can see that different clusters are clearly separated. Although ""weakly augmented images and similar images are embedded the closest to their origins, then comes strong augmentations as well as different looking images from the same class"". How do it contribute to final classification performance? Why is the embedding meaningful? 
Authors also claim that ""it also makes the feature embedding space more compact and explicable"". I don't see why it is more explicable. For example, given an unknown image (suppose we do not know the label in advance), the trained model may categorize it into a certain cluster, but how do we know the relationship between this image and all other points in the feature space? And what can we learn from it?"	"(1) This work is related to fully-supervised, self-supervised, and semi-supervised learning. But in the paper, the relationships and differences among the three are not clearly stated, which may lead to confusion for readers. It is suggested to improve this problem in the Introduction.
(2) In the experiment, all models use the same encoder ResNet18. But there are many other advanced CNNs, and how about other backbone networks as encoders?"	"The proposed work is quite similar to some of the referenced studies (i.e., [25])
There are some details that are missing and can further enhance the quality of the paper (refer to details comments)."
435	Sample hardness based gradient loss for long-tailed cervical cell detection	"The rule of choosing hyper-parameters of the loss is vague.
Lack of axis labels in Fig.4.
The scale of the dataset is missing, hence it's doubtful that the quantity of rare samples is large enough to make the experiment results convincing.
More detail should be provided about how the variable g is gained in Eq.3. Where do the output logits come from?
Lack of insightful analysis. The idea is somewhat similar to Focal Loss, how's the nature different from that of Focal Loss?"	"(1) There is a lack of systematic methods to determine the parameters \alpha+ and \alpha-. As discussed by the author, \alpha+ and \alpha- are important parameters to tune in order to achieve satisfactory effect, it would be useful to see how these parameters can be determined in a systematic way.
(2) Minor: isn't equation (3) a monotonically increasing function depending on g?"	"1.The performance with different value of modulating factor  alpha has obvious fluctuations. The value selection of modulating factor  alpha requires more detailed discussions and verification;
2.The method seems almost irrelevant to medicine. This method doesn't utilize any medical prior or clinical knowledge;
3.Although this method provides an amazing performance boost and shows comprehensive benchmark results, this method is only benchmark on private dataset. The experiment results may be a little bit incredible;"
436	SAPJNet: Sequence-Adaptive Prototype-Joint Network for Small Sample Multi-Sequence MRI Diagnosis	"this paper proposed to use Transformer and additive-angular-margin loss for small sample multi-sequence MR image classification. However, this paper has several major flaws and fails to illustrate their point:

The first component, Transformer model, is claimed to filter intra-sequence features and aggregate inter-sequence features, based on attention mechanism. But I didn't see any details about how to achieve these 2 goals in section 2.1. The overall writing quality is poor and difficult to follow.
Similarly, neither the section 2.2, prototype optimization strategy, illustrates how to approximate the intra-class prototype and alienate the inter-class prototype. For example, the query sample q^a and support sample s^b corresponding to which modalities, the loss_2 should be explicitly expressed like loss_1, and why you choose the additive-angular-margin loss instead of the ordinary cross-entropy loss for classification.
Fig 2 is very ambiguous and lacks sufficient explanation. E.g., what are the vectors besides the local significance block, how to aggregate into global correlation (through concatenation, pooling, or others), where does the support prototype come from, where are the 2 stages in Prototype optimization strategy, and how to approximate intra-class prototypes and alienate inter-class prototypes.
The experiment section lacks the explanation of 3 modalities of data (SAX, LAX, and LGE).
Some minor issues:
a.	P2, 1st paragraph, the hyperparameter p's explanation is ambiguous.
b.	Section 2.2 mentions robust classification. So what kind of attack methods you are dealing with?
c.	Section 2.2 2nd paragraph says ""two outputs of the SAT ..."" what does the two outputs referring to?
d.	Still in Section 2.2 2nd paragraph, ""The former is reserved for ..."". After this, where is the latter one? What's the purposes of two stages here?
e.	Table 3 didn't explain what's VOT."	"1) Lack of details in Method
2) The output of the networks is unclear
3) Lack of justification of the steps chosen in image preprocessing, and in some parameters chosen in Method
4) A few grammar issues"	"1) Why did the authors choose cosine similarity as a correlation method? Also it would be great to have some ablation in terms of losses employed in alienating features. Please compare against other naive similarity losses - like modified Barlow twins, SimSiam etc
2) The authors came up with a transformer-based learning paradigm. Can you please explain 2.1 in more details - especially how the self-attention is modeled to filter intra-sequence features. A few mathematical equations would be great. The local significance section is vague.
3) Since the authors called it a Sequence adaptive transformer, what will happen if one or two sequences are missing for few patients but available for others? Like motion is available for one and not for another. It's a common and practical scenario for MRI input.
4) I get how the authors 'approximate intra-class prototypes' and 'alienate inter-class prototypes' through the loss function. But it would be good if they can give a more clear explanation regarding it. Also, only their mention without any context in Fig 2 makes it a bit hard to understand."
437	SATr: Slice Attention with Transformer for Universal Lesion Detection	The equations in the method section is messy. List the whole workflow as equations is unnecessary and not essentially a mathematical contribution. A better way is just show an algorithm flow or pseudo codes, which would be much better and clearer. My recommendation is only keep the most important innovation as mathematical equations.	"The final design of SATr block, while has strong motivation, still needs more justification; for example, why we should use adjacent slices as query and key? can we simply use key-slice feature as query and all-slice feature as value?
The analysis of model size and flops are not provided; as the proposed block will introduce additional computation overhead, it is important to see the trade-off between accuracy improvements and computational requirements;"	"Weakness:
1.For the ablation study in Table 3, the authors ablate the few components. How about different hyper-parameters in the SATr? Would it be sensitive to the hyper-parameters.
2.I think the CAM figure is interesting. Could the authors kindly offer some comments or constructive explanation based on their understanding please?"
438	Scale-Equivariant Unrolled Neural Networks for Data-Efficient Accelerated MRI Reconstruction	"1) The proposed model lacks novelty. It seems that the proposed model was built by simply changing the existing CNNs with pre-existed scale-equivariant CNNs. To highlight the novelty and demonstrate the effectiveness of the proposed model, more explicit explanations and rigorous experiments would be needed.
2) Although quantitative results showed better performance than the baseline (Table 1, 2), it is difficult to see the performance increment in the presented figures (Fig. 1)."	"(1) Some important references are missing, for example, unrolled neural networks [1][2], equivariant network for inverse problem [3].
[1] Deep ADMM-Net for compressive sensing MRI, 2016
[2] A deep cascade of convolutional neural networks for MR image reconstruction, 2017
[3] Equivariant Imaging: Learning Beyond the Range Space, 2021"	"It is said that the proposed changes provide data efficient training options. Unfortunately, the only comparison that is done in the study is based on data augmentation vs none. I would have hoped for a stronger support of that claim by reducing the training data size.
Reconstruction results are all in supplementary, some results would have been also good in the main body.
A limitation to only 90 degree rotations does not take slight rotations of patients into account."
439	Screening of Dementia on OCTA Images via Multi-projection Consistency and Complementarity	"My main concern is about the experiments.
First, authors conducted the dementia prediction experiments on a private dataset. It may cause the over-estimation of the proposed method, since the hyper-parameters, training settings and data preprocessing can be specifically desinged for the dataset. 
Second, there is no ablation study in the experiments. I think a detailed ablation study is needed for this paper. For example, the effectiveness of consistency attention, complementarity attention, CVF, three different supervisions should be verified. 
In addition, authors should also show the effeciency of the model. For example, FLOPS/model size/memory-usage information are recommended to be reported."	"There are many hyperparameters in the loss function. The value of the hyperparameters is directly given in the paper, and there is no experiment to illustrate the influence of different hyperparameters. And the settings of the hyperparameters are not explained in the second set of experiments.
The attention mechanism designed in the paper is not novel enough. The consistent and complementary attention module is similar to the previous positive and negative attention mechanism, and the cross-view fusion mechanism is similar to the co-attention mechanism."	"(1) Novelty may be limited. Residual self-attention in Cross-view fusion module is similar to [1][2][3]. 
[1]. Exploring Self-attention for Image Recognition
[2]. An Explainable 3D Residual Self-Attention Deep Neural Network For Joint Atrophy Localization and Alzheimer's Disease Diagnosis using Structural MRI
[3]. Studying the Effects of Self-Attention for Medical Image Analysis
(2) The author said that CsCp was gradually added after each feature extraction stage, and it was found that the classification accuracy became better.
But why is the result of MUCO-stage3 much lower than other stages? Why is the effect of MUCO-stage1 better than other stages?
(3) In Table 2, the result improvement is not very large. Besides, are the model size and test speed inferior to the SOTA approachs?"
440	Scribble2D5: Weakly-Supervised Volumetric Image Segmentation via Scribble Annotations	"Much of the work is extending [1] to 3D with a few modifications, including:

3D backbone network (2.5 attention UNet)
different edge detector
use of attention blocks
additional static pseudo mask

It appears that the previous SOTA scribble methods were trained and designed for 2D images instead of 3D. 
The method INExtremeIS, which was designed for volumetric segmentation shows similar performance to the proposed methods using extreme points, which is arguably a less informative signal.
It would be better to compare with methods designed for 3D such as [2].
Scribbles are generated for VS and CHAOS datasets through ""erosion"". This process is not cited or further explained.  It should be clearly stated to ensure the these generated scribbles are representative of real scribbles. Additionally, scribbles could be generated on the ACDC dataset and compared to scribbles provided by experts.
Why did the authors introduce an extra hyperparameter lambda_2 for active boundary loss
that is not present in the 2D formulation (Chen et al., 2019)?
[1] J. Zhang, X. Yu, A. Li, P. Song, B. Liu, and Y. Dai, ""Weakly-Supervised Salient Object Detection via Scribble Annotations,"" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020.
[2] H. Kervadec, J. Dolz, M. Tang, E. Granger, Y. Boykov, kai I. Ben Ayed, 'Constrained-CNN losses for weakly supervised segmentation', Medical Image Analysis, t. 54, ss. 88-99, 2019.
[3] X. Chen, B. M. Williams, S. R. Vallabhaneni, G. Czanner, R. Williams, and Y. Zheng, ""Learning Active Contour Models for Medical Image Segmentation,"" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019."	The scribble generation protocol of VS and CHAOS datasets may need elaborating. If it is a well-known pipeline, then the paper should cite it. Otherwise, the paper should at least state the parameters that affected the scribble quality, such as length, thickness, distance to boundary, etc.	"It misses some related references in the proposed topic.

The evaluation and comparison are not sufficient.

Some scribble generations are not clinically realistic.

More detailed comments are given in the following Sec. 8."
441	Scribble-Supervised Medical Image Segmentation via Dual-Branch Network and Dynamically Mixed Pseudo Labels Supervision	"Single-dataset experimental set-up: the authors limited their experiments to a single dataset (ACDC). While this is a very well-known dataset, experiments with additional datasets are required to better assess the real performance of the described technique. This being said, I don't think additional experiments are strongly necessary for a MICCAI submission, also considering the ample comparisons with different types of baselines that are available in the paper.
Lack of discussion: the authors reported their results against many baselines in Table 1. However, there is no critical discussion of the achieved results. Specifically, do the authors have any idea why their method outperforms RLoss in DSC rather substantially, but do much worse in HD? Also, how can a method that produces rather poor DSC scores like RW provide instead such remarkable HD results? These are important aspects that need to be clarified."	To increase persuasiveness of the proposed method, the experimental settings should be illustrated more detailed, include comparison methods. The authors did not mention how much supervision information used of each comparison WSL methods. The proposed method adopts a dual-branch network, include main decoder, and an auxiliary decoder, the authors should give a comparative experiment between w/o auxiliary decoder to illustrate the effectiveness of auxiliary decoder. The network architecture is critical, it could influence the final results significantly. However,  main decoder+auxiliary decoder has been used in previous work(UDC-Net: Dual-Consistency Semi-Supervised Learning with Uncertainty Quantification for COVID-19 Lesion Segmentation from CT Images, MICCAI'21), which is not a new idea, and the authors should make some analysis between UDC-Net and the proposed method in the experiment.	
442	SD-LayerNet: Semi-supervised retinal layer segmentation in OCT using disentangled representation with anatomical priors	"Major Concerns:
1). Manually tuning parameters
The reconstruction loss proposed in this work involves more parameters tuning. The reviewers are afraid that the parameters tuning would influence the performance of proposed method.
2). Further validations
In addition, it is more interesting to show the validation of time-consuming of proposed SD-LayerNet with other peer methods."	"Lack of ablation study. If change some of the settings in e.g. anatomical priors, what will happen. The author can do more ablation studies to show the effect of each module of their network.

More analysis. Need to show more analysis about the experiment results. The author can discuss more in the results part why this design is better than baseline models.

Typos. E.g. Fig 3 caption includes a colored bracket"	"The role of the textual factor branch in the anatomy encoder is unclear.
Too many hyperparameters are involved in the algorithm, including eight loss balance weights and several empirical parameters related to the anatomical priors."
443	SeATrans: Learning Segmentation-Assisted diagnosis model via Transformer	"Could you explain more (or give an example) on the non-regional relationship between the segmentation and diagnosis task?
Visualization of the relationship between segmentation and diagnsosis tasks may help to understand the reason behind the proposed method for performance improvement.
Missing comparison with some papers which designed for the down-stream task learning, like MoCo, SimCLR, etc."	"The state-of-the-art methods compared in Table 2 lack the reference citation. 
The author failed to analyze the efficiency of proposed methods with recent works."	"Not well-written - The authors repeat their exact words many times. An example is the last two paragraphs of the introduction that are written very much similarly. 
Or the methodology starts with what the authors propose. This is stated in the introduction and now they should start explaining their methods."
444	Segmentation of Whole-brain Tractography: A Deep Learning Algorithm Based on 3D Raw Curve Points	"I would doubt the name of the ""segmentation model"" in the framework. It is actually a classification model which categorizes a set of point curves of brain white fibers into 10 major bundles. The proposed framework requires identified fiber curves as network inputs, whose pathways are already decided and aligned in the process of whole-brain tractography. Hence, the novelty of this work is downgraded due to this setting.

Back to the classification task in the experiments, the comparison with baseline approaches seems unfair to me. Because the testing data are not aligned, e.g. different sizes of subjects, various quality, and even different input data formats (the most critical factor), the classification difficulty of baseline approaches are not at the same level and shall not be compared.

In the ablation study, I suggest the author report the averaged performance under cross-validation. We are unable to draw statistical confidence about the reliability and effectiveness of the proposed modules based on the given numbers."	"It may be not the weakness, but something needs to be clarified.

what is the input of the model. The model took 256x3. I suppose that is a curve with 256 points and each point with 3d coordinates. The major fiber bundle can be selected 256 points randomly.  However, did it also work on the non-major bundles, which may have less than 256 points?
In the curve sampling section, it is not clear what is the mid points and how they were selected. Is that possible to use interpolation instead of adding points randomly to achieve the same data size?
Ablation studies may be required to confirm the performance of the proposed model.
Were data from different subjects mixed? Was there any difference among subjects?"	"One of the challenges of fiber segmentation is the anatomical individual variability. An effectively method should have the capability to handle the individual variances and provide robust results. In this paper, the method was evaluated using 10 subjects. However, the results part focuses on the accuracy value but the individual variability is overlooked.

As the accuracy values in table1 is the reported values in each work obtained using different datasets, therefore this results cannot effectively compare different methods.

As a major part, the motivation of Spatial attention module is not clear enough. From table2, we can see the performance of Baseline Network + Spatial attention module is worse than single Baseline Network.

Some sentences are not correct, such as ""...processes using FSL [17] was used"", and ""...which parts are the informative in ..."""
445	Self-Ensembling Vision Transformer (SEViT) for Robust Medical Image Classification	"Weakness:
1.Can the authors specify what is the motivation or application of adversarial learning in medical area? I can not think of an example where the perturbation happens and the adversarial learning would help with.
2.The conducted experiments in Table 1 is unfair. During the training of ViT, different level of adversarial attacks should be added during the training iteration to improve the ViT's robustness to perturbation, like what is did in the PGD serial paper.
3.Other adversarial learning techniques should be compared, for example the YOPO (https://proceedings.neurips.cc/paper/2019/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html), FOSC (https://arxiv.org/abs/2112.08304) and so on."	The motivations are not clear. The proposed method seems to lack novelty. Experiment needs further comparisons with state-of-the-arts.	"The introduction describes the good background of adversarial attacks and the robustness of the transformer, however, the adversarial attack is more introduced in the natural images processing. The motivation for conducting the same analysis in the medical image analysis is not well discussed.
The paper proposes a simple but effective method of handling adversarial attacks. The self-ensemble approach is clean and easy to follow. Is the method adaptive to natural images such as benchmarking on ImageNet? It would be better to discuss this as the proposed method is designed for generic transformer models not for medical image analysis.
According to Table 1, the proposed method is effective when an adversarial attack exists. However, when comes to robustness as the paper claimed, the performance decreases a lot. Will the community accept the conclusion that a large decrease is observed as adversarial attacks are not commonly considered in the deployment?"
446	Self-learning and One-shot Learning based Single-slice Annotation for 3D Medical Image Segmentation	The paper could be a little bit more clear at times, where terms such as one-shot are used in a possibly unusual way without exactly defining what is meant here.	"Novelty - in my opinion, most of the proposed method is certainly a sensible solution to the problem, but not particularly novel or thought provoking.

Binary segmentation only - this paper appears largely limited to binary segmentations, as the single segmented slice must contain all labels in order for them to be propagated and this is unlikely in many multiclass segmentation problems. In particular, this significant limitation is never discussed.

Depends on the interaction between the slice orientations and the anatomy's shape - the proposed method propagates segmentations from slice to slice. In particular, it learns pixelwise features that are used for each pixel in the unsegmented slice to find the most similar pixels in the previously segmented slice. Such methods can fail if the shape of the object's cross-section changes rapidly from slice to slice. In addition, the authors also restrict the matching to pixels within a 13x13 window. For objects that rapidly shrink or expand from slice to slice, it's possible that there aren't any pixel with the correct label in this small window size.

Evaluation on liver/spleen only - Given the constraints above, I would need more convincing that these segmentation tasks are sufficiently non-trivial with respect to both image appearance and object shape to truly demonstrate the utility of the proposed method."	"The values of the weights is not discussed
Relying on one slide with ground truth may indeed influence the results. The scheduled sampling should solve this, but some examples for this would have been nice (supplementary material is missing)"
447	SelfMix: A Self-adaptive Data Augmentation Method for Lesion Segmentation	"1) The authors did not explain why this method is distortion free. This method computed voxelwise weights with distance map and generate new images by combining two images with the weights.  As organs in different images may have different distribution, distortion may still happen.
2)  Symbols were not used in a good way in equations.  In Eq.1 and 3, the same symbol appeared in the both sides of the equations"	"Clarification on the selection of non-tumor regions. 
For liver lesion, I guess the non-tumor regions should only be liver tissue. Otherwise, the synthetic image would not realistic. However, I didn't see a clarification on this.

Demonstration of sample synthetic images. 
Unfortunately, there is no more synthetic images to give the reader more ideas how good the proposed SelfMix is compared to cutMix and CarveMix. Fig 1 is not clear enough. Maybe find another case for demonstration.

Results 
It would improve the quality of this work if the SelfMix is done in 3D and training the segmentation models in 3D. 
How cutMix is implemented in Table 2? Are the tumor allocated on random positions?

Presentaions. 
Fig. 1 is nice but it can be improved. I could not see the difference between CutMix and CarveMix. 
Too many typos."	"The abstract pays several sentences to describe their motivation, which is of course vital for a paper. However, the talk about method is short, lack of details. And they present their advances similar with the sentences in contribution part(end of introduction). Maybe description part should be reconsidered to be brief and bullet point about method itself should be given.
Some descriptions are not true. i.e. ""Compared with normal organizes, the lesion has a very small amount and meanwhile only occupies a small region in the whole image, which leads to less information can be proved to CNNs"" Actually, some head-neck organs are very small while lesions like glioma maybe very large. I understand why the authors talk like that, but accurate description is important in a research paper.
From Fig 2, it seems that when fusing tumor information with non-tumor region, there is a mix both inside or outside tumor lesion. However, the mixed tumor is still labeled with the fused tumor contour. How to keep the label accurate?
Section 2.2 ""Relationship with Mixup, CutMix and CarveMix"" is not method part. Normally it should be talked about this in discussion.
Some typos. i.e., in Fig 3, TAD should be TDA, is that right?
There is a constrain in augmentation of medical images, how to make sure the augmentation is clinical available. i.e. how do you make sure to locate a simulated tumor in probable position? Do you manually choose non tumor region?
The performance of Vnet without data augmentation should be presented."
448	Self-Rating Curriculum Learning for Localization and Segmentation of Tuberculosis on Chest Radiograph	The proposed ranking function has some novel aspect, however, the idea of SRCL is not new. The self-ranking curriculum learning or self paced curriculum learning or automated curriculum learning has been proposed and reported in papers in the past.	"1) The problem being solved (TB segmentation/classification) was not very challenging given very high and almost identical AUCs using the proposed method and the teacher network (see Table 2);
2) There was not a lot of novelty in the deep learning network. Mask-RCNN with ResNet was used."	"The paper could have been written in a better way with better formulations, more organized way of parameter listing, and comparison outcomes.
I think the method is not compared to ""without SRLC approach"". What would be the results of the Mask-RCNN+Resnet50 backbone architecture trained with the same training CXR dataset without SRLC approach, and tested on the same test set. Then, we would have a better understanding how much SRLC have contributed to the learning process. Is these results somewhere in the text (paper needs a better organization - especially experimental section - while providing the outcome of the test results)."
449	Self-supervised 3D anatomy segmentation using self-distilled masked image transformer (SMIT)	"The relationship between [CLS] token and global image embedding is unclear.
The conclusion of 1-layer vs. multi-layer decoder needs to be clarified."	"Missing one ablation study:
What is the performance of using MIM only? 
This would help us to understand what role MIM and self-ditillation play respectively in the pre-training process."	"As a matter of fact, I am more curious about the performance of applying the CT pre-trained model on MR segmentation tasks. Please also provide the segmentation performance of all methods trained from scratch, and the results of SMIT using the proposed SSL method (basically the MR version of Table 1).

When finetuning SWIT for target downstream tasks, what are the computational time and memory consumption used to achieve a satisfying segmentation performance? How is the situation compared to other SSL methods?

When finetuning SWIT on the MR segmentation task, will it require more samples and more training time to obtain good results in comparison with finetuning it on the CT segmentation task?"
450	Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion	"1) The 3D mesh estimation description is difficult to understand, the reproducibility is poor, and there is no particular novelty in that part.
2) The credibility of the results is limited because the cross-validation is not performed on the data with a small number of patients."	A fair comparison with the current state-of-the-art is missing	"*	I think the experiments part in this paper could be stronger. The entire section is not as well written and structured as the remainder of the paper. Some choices and methods are not entirely clear from the description (see detailed points). 
*	The clinical significance and impact of the achieved results is not discussed. It remains unclear whether the method satisfies clinical demands, and if not, which limitations remain.
*	Although the number of required labelled training data can be significantly reduced by the approach, still, a considerable amount of labelled training pairs from clinical, in-bed pose images are needed to train the network. Aside from a public dataset, the authors use a large collection of proprietary data. For most researchers aiming on using or building upon the presented approach, such a dataset will be difficult to obtain."
451	Self-Supervised Depth Estimation in Laparoscopic Image using 3D Geometric Consistency	"There was a very similar paper published in MedIA early this year: Bardozzo et al.  ""A stacked and siamese disparity estimation network for depth reconstruction in modern 3D laparoscopy"" , which is also a self-supervised network minimizing a 2D reconstruction loss using SSIM as the metric. Can the authors refer to this paper and describe how the submitted work differs from it?
It is not clear from the text what the primary motivation for this work is. The authors mention the acknowledged problem of providing ""ground truth"" for laparoscopic 3D reconstruction, against which various reconstruction algorithms can be evaluated, and the way the paper is presented, the reader could understand that this was its objective.  However, further reading reveals a structured light based generation of ground truth used to validate their new approach.  Perhaps the intro could be revised to make the objectives clearer - perhaps along with clinically-oriented statement of the ""un-met need'  that is being addressed.
The training/testing scheme in this paper is somewhat problematic. ""Hence, only key-frame ground truth depth maps were used from this test dataset while the remainder of the RGB data formed the training set."" If I understood this correctly, the author used n-1 frames for training and 1 frame for testing for each sub-dataset. Would  this not cause bias for overfitting, since all images in the sub-dataset are very similar and have similar depth. Would it not be more appropriate to use several subsets for training and 1 or 2 for testing as was the case for the challenge?"	"The main weaknesses of this paper are

the utility of this method given a) many laparoscopes are stereo and b) 3D information is not really obtainable from mono depth methods
the motivations are not clear
whether this is the right community for this paper
I elaborate more on this in the constructive comments."	"Major concerns:

Novelty. The two claimed methodologic contributions of this paper, 3D geometric consistency loss from ICP and blinding mask, have already been well-studied by previous depth estimation work. The ICP loss for monocular depth estimation was proposed in [13], and the idea of using geometric constraints in endoscopy image depth estimation is not new [a]. The blinding mask is a widely-applied technique/trick; for example, it was applied in [13] and previous medical implementations such as [b]. Although this work re-implemented the approaches in a new application scenario, laparoscopic stereo images, the approaches are not novel.
Necessity of 3GC. The proposed framework has three differences from Mono1, i.e., the additional ICP loss (3GC), blinding mask (BM), and decoder structure (FD). The selected results in the ablation study Table 3 only show combinations that are in favor of this paper, but the full ablation in supplementary raises concerns about the efficiency of the main contribution of 3GC. As shown in Supp. Table 1, modifying the decoder (Mono1 w/ FD) can already bring significant improvement comparing baseline Mono1, but further adding ICP loss (Mono1 w/ 3GC, FD) decreases the performance greatly (the second-worst result in the table). This is an important finding, showing the ICP is not as useful as claimed for a decent baseline. The authors should have pointed it out.

Minors:

The three baselines compared in Table 1&2 are all photometric-based methods. It would be more convincing if some geometric-based baselines were compared, such as general depth estimation baseline [c,d] and medical baseline [a].
An error in Eq.2

[a] Liu, Xingtong, et al. ""Dense depth estimation in monocular endoscopy with self-supervised learning methods."" IEEE transactions on medical imaging 39.5 (2019): 1438-1447.
[b] Ma, Ruibin, et al. ""RNNSLAM: Reconstructing the 3D colon to visualize missing regions during a colonoscopy."" Medical image analysis 72 (2021): 102100.
[c] Yang, Zhenheng, et al. ""Lego: Learning edge with geometry all at once by watching videos."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018.
[d] Bian, Jiawang, et al. ""Unsupervised scale-consistent depth and ego-motion learning from monocular video."" Advances in neural information processing systems 32 (2019)."
452	Self-Supervised Learning of Morphological Representation for 3D EM Segments with Cluster-Instance Correlations	"Weaknesses:

Only one (although very big) dataset is used. Will the learned representations transfer to another dataset without retraining?
The classification problem of soma/neurite/glia is not that difficult. It would interesting to test the method on the more common axon/dendrite/soma classification problem in mammalian brain data."	"Lack of novelty: The proposed method is a direct combination of BYOL and SwAV with minor modification. It'll be good if the paper can directly re-use the terminology from the previous work and put things in context. Otherwise, the paper reads as if it proposes all these new modules.

Lack of reference and comparison with state-of-the-art point cloud self-supervision methods [A]. BYOL and SwAV were designed for image input, while [A] is more proper for comparison.

Lack of comparison with prior work. The proposed method can be more convincing if compared with prior work [15]. (1) Although the input field-of-view of [15] is much smaller, it can still be trained on the proposed dataset. (2) Also, it'll be great to test out the proposed method on [15] dataset.

Unclear how hard the proposed benchmark is. It'll be good to use handcrafted 3D point cloud features as a comparison. For example, the scale of the soma is vastly different from the shown neurites and glias, which makes it a simple task with even handcrafted features.

[A] Xie et al. PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding. ECCV 2020"	"Missing the quantitative ablation study for the cluster/instance level contrastive learning. 
Missing the description of the choice for the hyper-parameters."
453	Self-Supervised Pre-Training for Nuclei Segmentation	"In places, the presentation is unclear. Especially section 3.1, which is a pitty, as this is the main contribution of the paper. For example, you define a matrix Hard, elements of which indicate how hard it is to predict a non-boundary patch. This seems to be only used element-wise to define sets of feature vectors (2). Why not define (2) directly from prediction difficulty. Also, the argumentation for calling this sets foreground and background patches is unclear - this is only an assumption, or...? To me it seems that we only can say that sets contain feature vectors of patches that are difficult/easy to predict. 
Mathematical notation is difficult to follow due to many variables, and the use of italics (math) font for multi-letter variables. For example $An_{i,j}$ would normally be $A$ multiplied with $n_{i,j}$. The name Hard is also not a good name for a matrix, but if you insist, type it in roman. (On a similar note, for subscripts which are not variables, it should be $L_\mathrm{region}$. And functions like max and softmax should also be typeset in roman.) 
In Figure 2, the text is very small. In Figure 3, the images are very small (and in the printed version, blue and yellow arrows provide poor contrast to black and white images). 
Conclusion is blant. It's much better to make conclusions which are refutable. For example something that: Our results show that VT may be pre-trained to ... 
Tiny thing: in 1 you say k=32, but later it seems you divide into 16x16 patches. What is the explanation?"	"The comparison seems to lack the setting with TransUNet (segmentation baseline) pre-trained with MoNuSegWSI.

The evaluation of the methods did not include an ablation study. Although the performance comparison table does prove that the pre-training on MoNuSeg and the fine-tuning with supervision enhanced the performance. The paper didn't provide a compelling analysis of how much the triplet loss contributes to the improvements, or whether the pre-training on MoNuSegWSI is responsible for the performance boost.

The idea could be useful, however, the layout and writing can still be further improved, and some more revisions might be helpful and necessary. Overall, the submitted version looks an incomplete version completed in a hurry, especially the experiments section."	"The triplet loss and scale loss are not new. They are from previous works. But I think this is not a big issue.
In the paper, it is unclear why the proposed self-supervised training method does not fall into trivial solutions, e.g., the network simply outputs a feature map with constant values. For me, I think this could be due to the scale loss. Some explanations would be necessary in the paper.
In the experiment, no ablation studies are provided to show the impact of scale loss and triplet loss. It is unclear if the triplet loss only contributes marginally to the overall performance. In other words, it is possible that the good performance of TransNuSS is mainly due to the scale loss.
The proposed method utilizes Vision Transformer, which can be more effective than ResNet in many cases. In the experiment, it is demonstrated that the proposed method outperforms InstSSL, which employs ResNet. Therefore, such a superior performance could be due to the Vision Transformer. To make a fair comparison, it is suggested to compare with InstSSL (and other methods) which also employs Vision Transformer.
This paper claims that applying the proposed pre-training technique to a Vision Transformer is a contribution. However, I think this contribution is trivial because Vision Transformer can also be replaced with convolutional networks here.
In the experiment, the paper does not compare with other general self-supervised methods which can also be applied to medical image processing, such as MoCo, BYOL, SimCLR, etc. It is suggested to also compare some of these methods to show the advantages of the proposed method.

References:
[1] Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning. NeurIPS 2020.
[2] A Simple Framework for Contrastive Learning of Visual Representations. ICML 2020.
[3] Momentum Contrast for Unsupervised Visual Representation Learning. CVPR 2020."
454	Semi-supervised histological image segmentation via hierarchical consistency enforcement	"The paper novelty is to support the idea that applying perturbations in the feature space is preferred than doing them in the input or in the output space. Nevertheless, the authors do not compare or explain properly how it improves against such SSL methods, which weakens the paper.
Details and discussion on pixel-segmentation results are lacking.
No statistical analysis of the results, or average performance over several repetitions are reported, which makes difficult to evaluate the robustness of the methods.
The qualitative results show that the method struggles to separate the boundary of touching structures, would have been interesting to discuss deeper and propose further enhancements for the method to cope with this. State-of-the-art methods such as [1] make this the central innovation in their methods, I think HCE could be further improved if you take this type of domain knowledge into account.
The qualitative results show that the method struggles to separate the boundary of touching structures, would have been interesting to discuss deeper and propose further enhancements for the method to cope with this.
[1]: He, Hongliang, et al. ""CDNet: Centripetal Direction Network for Nuclear Instance Segmentation."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021."	"*	The paper is based on the idea that decoders at different layers of the network can reconstruct the exact same output as the last decoder. However, there is no clear proof that this consistency enforcement does not result on identity functions for the last layers of the encoder leading to a network that under-fits the already small training data.
*	The current reported results employ 3 hierarchical layers in the student model. It is unclear if the improvement stems solely from the use of multiple layers. Including ablations/evaluations for the number of layers may further support the need for hierarchical learning."	"1) Even though I agree with the authors that the proposed HCE is simple, the novelty is limited. It's more of a marginal improvement over the current perturbation-based Mean-Teacher method such as [8]. More importantly, I think it is necessary to introduce more details about the perturbations, including both types and degrees, as I believe the essence of the method is to learn perturbation-invariant features for a more efficient learning process. It would be interesting to see more discussions on how would different feature-level perturbations affect the HCE performances. Could authors also comment on why transformations as in [6] or prediction-level perturbations as in [8] are not used in the paper? If possible, an ablation study on perturbations would be constructive.
2) Authros could consider including more recent related works, in the discussion and/or comparisons, such as Extreme Consistency: Overcoming Annotation Scarcity and Domain Shifts by Fotedar et al. and SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation by You et al.
3) In Fig. 2, it would be helpful to better locate the visual differences if authors use the same or similar colormaps for different methods."
455	Semi-supervised Learning for Nerve Segmentation in Corneal Confocal Microscope Photography	Many papers in literature describe methods for corneal nerve segmentation from confocal images. The proposed method achieves a limited improvement in the segmentation performance, and the clinical relevance of this small improvement is not demonstrated.	The whole framework is solid but widely used in self-training, unsupervised training computer vision papers, so the novelty is not strong for a methodology paper for example, Kaiming He etc, Masked Autoencoders Are Scalable Vision Learners, used similar approach to do self-learning problems.	"(1) Some statements are too strong to be demonstrated.
(2) Computational efficiency is not analyzed although it's mentioned in the last sentence of Section 1 that ""our method is data efficient and more friendly to computing resource."""
456	Semi-supervised learning with data harmonisation for biomarker discovery from resting state fMRI	"The technical improvement is marginal by exploiting the existing mechanisms.
It needs to survey recent work on multi-site brain disease diagnosis thoroughly.
The reported performance is not persuasive by showing big difference from the existing work."	Effect of augmenting datasets through others not fully investigated	"While the motivation for the proposed semi-supervised learning approach was that the there may be label inconsistency issues across sites. However, I'm not sure that this is a real concern in the presented ASD classification case - the diagnosis of ASD is largely objective and follows clinically well defined parameters. Still, there should certainly be other applications where label uncertainty could be an issue.

The proposed data harmonization model does not consider the target label (ASD/typical control). I am wondering if there is a concern then that the two neurologically different groups are being harmonized to one set of parameters? For example, perhaps activation in social motivation areas is greatly reduced in ASD subjects compared to controls, but the same alpha_v and beta_v are learned for both groups.

The experimental methods do not include any comparisons to other data harmonization approaches. At a minimum, I would have expected to see a comparison to a method that first runs ComBat on all the data and then uses the harmonized output to perform supervised learning for each site individually. Other potential approaches for the general domain shift problem is something like a generative adversarial network that tries to learn a representation invariant to site and other factors (e.g., [1-3]).

[1] Bashyam et al., Deep Generative Medical Image Harmonization for Improving Cross-Site Generalization in Deep Learning Predictors, 2021
[2] Gao et al., A universal intensity standardization method based on a many-to-one weak-paired cycle generative adversarial network for magnetic resonance images, 2019
[3] Liu et al, Style Transfer Using Generative Adversarial Networks for Multi-site MRI Harmonization, 2021"
457	Semi-Supervised Medical Image Classification with Temporal Knowledge-Aware Regularization	"The proposed IPH exploits cluster-aware information that conducts an unsupervised clustering method (e.g., K-Means) within a mini-batch, so the batch size is vital to IPH. I guess the experimental results are sensitive to batch size, which, however, is not discussed in the paper.
For ISIC dataset, the authors seem to copy the comparison results of GLM [36] and NM [22] from NM, which is an unfair comparison because the proposed method adopts a strong augmentation for training data."	"the IPH compute clustered feature centroids within each mini-batch which makes it a little bit unconvincing. Because the batch class mean is likely to be a poor approximation of the real mean. Also, the batch size may affect the clustering result. when not all classes are present in a batch, the clustered features cannot reflect the correct class distribution and thus may bring new problems.

the author did not show how the metric which measures the consistency of clustered feature centroids changes over the training iterations. For instance, a figure show the distance of centroids across iteration before and after using IPH might help illustrating the mechanism of IPH."	"*Some of the explanations are unclear.

in Eq.5, \sigma is the normalization function. However, $u$ is a single sample (LE(u;t) is a scalar). How to normalize it? It indicates using all the unlabeled data? The current description is a bit ambiguous.
In the comparison and ablation study, what value of 'b' in Eq.5 was used?

*The comparison is O.K. but, to show the effectiveness of the proposed method, it is better to compare with the semi-supervised methods that use training iteration information, such as mean-teacher algorithm.
*Once carefully read the process of the method, the reason why the method is effective is understandable. However, it is not easy to understand it by reading the introduction. The reviewer recommends the authors to more clarify it, i.e., show the simple examples what case the relaxation is large or small and why."
458	Semi-Supervised Medical Image Segmentation Using Cross-Model Pseudo-Supervision with Shape Awareness and Local Context Constraints	"The novelty of the paper is limited. Although with a handful of adaptions, the overall architecture of using two Unet networks and feeding the prediction to the other is not new.
The gains of using the proposed architecture are marginal. Overall, the proposed method underperforms the baseline method with full supervision."	"The author does not seem to be clear on what exactly are novel on top of their citations [4] and [9]
The ablation studies can be improved."	"It uses two different networks rather than one. Why not one?
In the unsupervised loss training function (page 5) for unlabeled data the authors claim that the labels provided by the shape network are more accurate because of the shape information. This is not explained and substantiated.
In general the cost terms and their terminology are explained after the whole cost is introduced.
They should state any pre-processing necessary for the shapes whether the prior shapes must be aligned and the robustness of the method after rotations."
459	Semi-Supervised PR Virtual Staining for Breast Histopathological Images	There is no clear if the dataset is free and public	"more qualitative than quantitative results
the metod works on 5x magnification (big FOV) with such magnification clinical usefulness of such processing is questionable"	"The image processing part about labelling the PR+ patches based on color analysis is tricky as brown is a difficult color to model.
The post analysis with pathologists that could be interesting to explain why there is discordance is not done."
460	Semi-Supervised Spatial Temporal Attention Network for Video Polyp Segmentation	"There is nothing particularly novel about this work in terms of technical innovations. Vision transformers have been used both spatially and temporally in the literature for natural images. However, there is good enough application novelty in the reviewer's opinion to justify publication in this venue. There is a significant need for a semi-supervised video-based polyp segmentation method, and further for the data annotations.

There is no discussion about how the training, validation, and testing images were split, no cross validation, no error bars, and no discussion of potential bias introduced. This can be mildly excused due to the cost of video data annotation and only having a limited number of videos fully annotated, but a discussion of how videos were chosen for the splits should at the very least be included."	There is no information regarding the computation time. It is difficult to evalute the paper based on the feasibility of implementing the proposed approach for real-time clinical application.	Two main modules improved the higher accuracy of segmentation performance but the modules are not originally proposed in this paper.  Segmentation task is important but I feel automatic classification task of benign or malignant of polyp is more important these days. Some product has been sold these days.
461	Sensor Geometry Generalization to Untrained Conditions in Quantitative Ultrasound Imaging	"The main weakness of the paper lies in the description of the conducted experiments and the obtained results. While the presented evaluations look reasonable, further details should be provided to better understand how the method assessment has been performed, why conducted experiments are relevant, and why the obtained results represent a proof of framework accuracy (more details below).
Moreover, description of the methods is sometimes difficult to follow due to the many details, parameters and acronyms. Used parameters and notation are not always properly defined."	The biomechanical properties used for simulation are not described.	"Some of the parts should be refined so that the readers can have a better undrestanding of the approach (exp: Meta learning, DSA )
In simulation results, there is no experiment for CNN trained on signle imaging setting (I think Baseline without those modules still trained on multiple imaging settings?, if not, please clarify)
There is no comparision with non-deep learning approches"
462	SETMIL: Spatial Encoding Transformer-based Multiple Instance Learning for Pathological Image Analysis	"Trans-MIL has already adapted the transformer to solve the MIL problem in pathological image analysis. Essentially, This paper seems similar to Tran-MIL[17]. The differences with Trans-MIL [17] should be clearly descripted.
2.In fact, The SET module is the relative encoding module, it's not clearly claimed in the paper. 
3.This paper displays that the local information of neighbouring instances is vital for the pathological image analysis. This view is not mentioned in the paper.
The pyramid multi scale fusion module and the SET both enhance the local representation of the neighbouring instances, the purpose of this design and the difference of these modules should be described clearly."	"I am concerned about the practical application of the current framework as its memory usage is converging to a setting where we feed WSIs directly to the model. This is because that current approach is employing all patches from WSI for representation learning which leads to the famous memory bottleneck that exists for this task.

The fact that all the patches are being used makes the experiments against many MIL methods unfair as they only employ a small number of patches for training the model.
Comparing against a framework like: 
""Neural Image Compression for Gigapixel Histopathology Image Analysis"" 
David Tellez*, Geert Litjens, Jeroen van der Laak, Francesco Ciompi

That uses all patches seem to be a fair comparison.

The training here is not end-to-end, which may lead to sub-optimal solutions. Recently there have been efforts to develop end-end MIL methods for WSI representation learning. This is necessary to discuss your approach against methods that use a small proportion of patches but trained in an end-to-end manner. Which approach is preferred and Why?

""CNN and Deep Sets for End-to-End Whole Slide Image Representation Learning""
Sobhan Hemati, Shivam Kalra, Cameron Meaney, Morteza Babaie, Ali Ghodsi, Hamid Tizhoosh"	"1- The main consern is the fairness between experiments. Most MIL methods are using small amount of WSI whiledeveloped method is using all patches in one WSI. So it might not be fair to compare light algorithm with bruteforce method. 
2- The other weakness is connected with the first one. As feeding whole WSI to the model (even with L'' and W'') still model is large. Such that just 4 of them fitted to V100. As a result, the work might not be repruducable with regular GPUs.
3- The last weakness is feeding the blank space to the model. Even if model remove them after training still they will  be fed to the model. 
4- I wish more pathology related point of view like multi-magnification (5x, 10x, 20x) was used rather multi-level"
463	SGT: Scene Graph-Guided Transformer for Surgical Report Generation	"Some clarification should be made, see the detailed comments
The evaluated dataset is quite small, therefore, cross validation is better to be conducted"	"In general, the clarity and organization of the paper could be improved. The method section could describe the model from input to output. These modifications will make it clearer for the audience. Additionally, the mathematical notation should always support the text.
The ablation study does not allow an assessment of the model generalization capacity. Optimizing the architecture over the test sets might result in overfitting in the benchmark dataset. The ablation study should be performed in the validation set."	"Some components are not explained clearly. For example, in equation (1) what is the matrix Z? How to obtain Z?
There are some typos.
(1) On page 3, section 2.2, line 3, either ""the"" or ""a"" should be used.
(2) On page 4, in line 5, what does p mean in \mathcal{E}_{he}^{p}?
There are some missing references. This paper has mentioned several recent chest x-ray report generation papers as related works. However, please also consider citing the following two easiest works on chest x-ray report generation.

[1] Jing, Baoyu, Pengtao Xie, and Eric Xing. ""On the Automatic Generation of Medical Imaging Reports."" In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2577-2586. 2018.
[2] Wang, Xiaosong, Yifan Peng, Le Lu, Zhiyong Lu, and Ronald M. Summers. ""Tienet: Text-image embedding network for common thorax disease classification and reporting in chest x-rays."" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9049-9058. 2018."
464	Shape-Aware Weakly/Semi-Supervised Optic Disc and Cup Segmentation with Regional/Marginal Consistency	"Adding a clear example of the weak labels would help the reader to understand exactly the supervision setting.
Though the handling of the multi-classes is not clear and hamper the comprehension of the paper"	"The motivation and the contribution of the proposed mSDF is not clear. In the previous work, e.g., [15], SDF is adopt to the represent the target mask. In this paper, it proposes a modified mSDF. What's the difference between the proposed mSDF with the previous work? And what's the advantage of the proposed mSDF compared to the previous work?
The experimental setting is not clear. Did it train a single model using both the labeled SEG dataset and unlabeled UKBB dataset or train separate models?
The way to utilize the labeled images and unlabeled images is not clear. It mentioned that in one batch, it has both labeled images and unlabeled images. Does it have the same training loss for both the labeled and unlabeled images?"	I don't think the manuscript has any major weaknesses.
465	Shape-based features of white matter fiber-tracts associated with outcome in Major Depression Disorder	"The paper has some weaknesses that could be addressed.
1) The paper assumes a lot of technical knowledge from the reader, and could sometimes benefit from being a bit more clear in explaining what exactly is meant by specific words. E.g. in the abstract the authors state ""Shapes are characterised via the deformation of their center line from a centroid shape."" It is not clear at this point what is meant by ""a shape"". A shape could be a path in 3d space, a surface, a volume, this needs to be clarified. If it is a path, then it can be a closed loop, non-closed, or something that branches, this needs to be a bit more clear, since the word shape is very generic.
2) The novelty seems to be in using these shape features for the following analysis. It is not exactly clear what has been done prior. I would also have wanted to see a comparison to some baseline, where other kind of information is extracted from the bundles.
3) The paper could explain the dataset better. E.g. depression is more common in women, is this reflected in the dataset? What was the depression severity of the patients at the start of the experiment? What is the age distribution? This should all be in a supplementary table, along with all the other covariates used for the linear models. Each covariate should also be described.
4) A figure could aid sections 3.1 and 3.2. This could also be used to clearly define what is meant by a shape. It seems to be a path defined by a smooth mapping from the unit interval [0,1] to R^3. This figure should ideally clearly explain the local and global features.
5) The analysis could be improved (see detailed comments).
6) I think it is very strong wording to call the findings a biomarker.
7) The size of the dataset (number of individuals) should be mentioned in the abstract and the introduction."	"The voxelsize is anisotropic; 2 x 1 x 1, which causes issues on the tensor model fitting.
A specific result was found on the right hemisphere. There was no correction for handedness in the study. In order to be correct, this needs to be added to the model."	The approach seems novel, to the knowledge of the review not many serious concerns. Mostly text improvements and capitalization in the bibliography.
466	ShapePU: A New PU Learning Framework Regularized by Global Consistency for Scribble Supervised Cardiac Segmentation	I'm missing the quantification of the time savings compared to fully supervised methods, which is a major motivation for this work. Is the tradeoff annotation time/accuracy arguable? In the end doctors want the most accurate model, even if this means tedious annotation work.	It is lacking a clinical test, to show real robustness in any case.	"Errors in the equations: There seem to be errors in the condition used for getting the main equation (1) in the paper: sum_{j=1}^m pl(cj
x) = 1 should be replaced by  sum_	{j=1}^m pu(cj
x) = 1 for the equation (1) to be valid. Also check equations 3 and 5 in the supplementary material (numerator and denominator seems exchanged and the pu is replaced by pl). Also, given that we do not know what classes the unlabelled pixels belong to, the authors should explain how the assumption for eqn (1) can hold good.

Also, in the loss function authors determine the positive and unlabelled pixels as \omega and \omega_bar, but they haven't used unlabelled voxels in the loss equation? They seem to have calculated the marginal probability only for the positive pixels? Can they explain why?
Lack of details regarding differences between datasets: While it is good that authors evaluated on multiple datasets, more information should be provided on how they different and what measures were taken to reduce domain shift? is ACDC also post-gad? how is it handled (any normalisation done)? differences due to pathological conditions (eg. myocardiopathy)?"
467	Show, Attend and Detect: Towards Fine-grained Assessment of Abdominal Aortic Calcification on Vertebral Fracture Assessment Scans	Nothing to be specific as weaknesses, however, some strategies to achieve better classification outcomes with some more enhanced results for the evaluation metrics (accuracy, sensitivity, and specificity) could have been suggested. It says that the AAC-24 scores are highly correlated with expert assessments with an accuracy above 80%. How can this be improved, too?	"One significant weakness is that the results presented here are significantly lower than the previous publication of the state-of-the-art model they are training against, despite using the same model. The authors suggest that the previous publication may not have performed an appropriate evaluation, but the explanation here is lacking. It would be helpful to show a replication of the previous results and clearly demonstrate why those results are not a realistic representation of the model's performance.
The authors also weaken the justification of their sequential approach (the main innovation in their model) when they state that the individual scores are rigid and random. It seems that the authors were just trying to contrast their problem with language since, in later descriptions, it sounds like their is structure in the sequences, so it does seem like there is value in this approach. It would just be good to be descriptive of how the structure of these scores can be modeled using a sequential approach and to stay consistent.
Finally, there are several mistakes and inconsistencies in the results that make things a bit hard to follow or appreciate."	"The paper needs to be proofread again, as there are a lot of typos and mistakes
Results validation could be improved and the reported results are not very convincing"
468	Siamese Encoder-based Spatial-Temporal Mixer for Growth Trend Prediction of Lung Nodules on CT Scans	"It is better to add some examples of images where the algorithm fails / succeed to classify correctly the output, a lake of images make the quality of the discussion section poor.
The mathematical formulation of the loss functions and model in general is poor. It would improve the quality of the paper to improve the mathematical formulation.
The discussion section is poor, and authors did not specify why their algorithm classify correctly or misclassify samples.
The proposed method is not innovative, and components of the framework have been used before for similar tasks. This paper would have been more suitable for miccai if a more innovative approach was used."	The characteristics of pulmonary nodules considered in this paper are only diameter and texture, and do not utilize other sign information and clinical information of each patient. Some important formulas are not listed in the Methods section. There are few methods compared in the experimental part, and the results of the comparison experiments are not visualized. A description of the limitations of the method and future work is missing.	"Limited comparison to state-of-the-art: 1) In the subsection of NLSTt dataset acquisition, it would be better if authors could explain clearly how they pair nodules whose locations change a lot between two Ti and T0, and what is the accuracy of pairing correctly nodules. 2)Authors compared between the CNN and ViT encoder, as well as between different mixers, whereas the proposed method is not compared with existing methods, such as [17,19] and [R1,R2]. It would be more interesting and convinced to compare between such methods.
[R1]. Rafael-Palou, X., Aubanell, A., Bonavita, I., Ceresa, M., Piella, G., Ribas, V.,Ballester, M. A. G.: Re-identification and growth detection of pulmonary nodules without image registration using 3d siamese neural networks. Medical Image Analysis, 67, 101823 (2021)
[R2] Tao, Guangyu et al. ""Prediction of future imagery of lung nodule as growth modeling with follow-up computed tomography scans using deep learning: a retrospective cohort study."" Translational lung cancer research vol. 11,2 (2022): 250-262. doi:10.21037/tlcr-22-59"
469	Simultaneous Bone and Shadow Segmentation Network using Task Correspondence Consistency	Unfortunately, with my experience I cannot evaluate the novelty of this work in terms of programming. The CTFT looks for me similar to a GAN approach but a bit different. The rest of the proposed method is based on well-known previous studies.	"Small number of subjects
Ground truths segmentations for curvilinear probes appear problematic"	The method is argued to be inteded for CAOS, but not specific condition, and strategy for clinical implementation is given. The subjects are all healthy subjects, not particulary represeting the image features of US images taken in CAOS procedures.
470	Skin Lesion Recognition with Class-Hierarchy Regularized Hyperbolic Embeddings	"The weaknesses are reduced:
1) I consider questionable the data augmentation that they employ. In an application where color is essential for the diagnosis, it does not seem adequate to incorporate color transformations in the augmentation.
2)  An explanation of the dynamic range and justification of the values of the different metrics in Table 1 would be desiderable. For example, they do not justify that 50% accuracy is reasonable for 65 classes or the maximum possible value of the mistake severity  or the maximum possible value of the mistake severity or if 1.X is a good value for HD-k"	"The logic of the hyperbolic hierarchy structure of the skin class labels is debatable. What specific values does such formulation brings? In hyperbolic space, the distance becomes longer close to the boundary areas. However, the final classes, i.e., the leaf nodes, are all there. The reviewer does not fully understand the full motivation. Why are the distances between two nodes that share the same parent nodes are formulated with the same distance between two nodes that do not share the same parent nodes?

It is exciting to see a dataset with ~230k images from 65 disease subtypes. A natural question is whether the image numbers in each class are balanced. What are the maximum image numbers and minimum images numbers for a disease type? Any performance evaluation methods to address them?

The performance improvement was relatively incremental.

The training details are not provided in Appendix."	Despite the interesting formulation of the method described in the paper, the results (mainly table 1 ) look relatively closer to the baselines. The performance improvements look marginal compared to the rest of included methods. In particular, it does not look like the hyperbolic approach contributes much to the performance as it does the hierarchy aware variants.
471	SLAM-TKA: Real-time Intra-operative Measurement of Tibial Resection Plane in Conventional Total Knee Arthroplasty	I'm doubtful about the clinical applicability of the proposed method. Although the authors tested the clinical feasibility of the method in a pilot clinical study, in my opinion, the advantage of using such system in clinical practice was not sufficiently discussed.	There is no clinical evaluation in the current study.	Proof-reading is required.
472	SMESwin Unet: Merging CNN and Transformer for Medical Image Segmentation	"Although the proposed work shows good results and proposes a new design of a Transformer-based segmentation method, the work is built based upon several pre-existing methods. Hence, there is limited technical novelty. Simple adoption and combination of swin unet and CCT as well as addition of a CNN layer.
The authors evaluated the proposed work on three datasets. For MoNuSeg, the results were inferior to UNet++ for both metrics."	"1: The technical contribution is the main concern. I feel that it is not certainly enough for a MICCAI paper.
2: I think the usage of the superpixel may hurts the feasibility of the method. If this procedure could be removed, and the performance just degrades slightly, then I will vote for publication.
3: It lacks of the evaluation in the 3D datasets; for a general network, such an evaluation may be required."	"As the system was based on CCT, it is kind of a must to compare swin-transformer with CCT in the final performance to prove the CNN branch added is useful. If CNN branch can be proven truly useful, this can have a bigger impact to the research field for better deep learning design.
The system is a hybrid of swin-transformer, CCT, and ET. So relatively the work lacks novelty in technique."
473	Sparse Interpretation of Graph Convolutional Networks for Multi-Modal Diagnosis of Alzheimer's Disease	"Sparse representation in GCN is not a novel contribution. The SGCN method has been used widely on other computer vision applications, which makes the novelty of the approach moderate. Few examples are: Renjie Wu et al. 2018 ""k3-Sparse Graph Convolutional Networks for Face Recognition"" and Liushuai Shi et al. 2021 ""SGCN:Sparse Graph Convolution Network for Pedestrian Trajectory Prediction"". See also the survey in David Ahmedt-Aristizabal et al. 2021. ""Graph-Based Deep Learning for Medical""
Diagnosis and Analysis: Past, Present and Future
Some items of the loss function and mathematical notions are not clear.
There is no explanation of the architecture and implementations."	Comparison of proposed method need to be done with existing state of the art methods for multimodal disease diagnosis	Correlation analysis between multimodal data is slightly lacking.
474	Spatial-hierarchical Graph Neural Network with Dynamic Structure Learning for Histological Image Classification	"Explaining the multi-level structure of the proposed structure, it would be informative to mention the number of levels used in the experiments and to explain the effect of that as a hyper-parameter.
The literature review could have been enriched on the use of vision transformers in previous work."	"Despite the detailed description of the graph-based metrics, the authors do not explain how the training strategy was designed. They mentioned that they used a 5 repeated 3-fold cross-validations but they did not explain if it is an end-to-end learning fashion or no.
There are several spatial graph convolution methods existing in the literature. Why choosing GraphSAGE in particular?
The rationality of choosing transformer is not clear. Why not using GAT network which is based on attention mechanism?
The limitations of this framework needs to be discussed."	results are not very convincing, above all on BRACS dataset, further results should be added
475	Spatiotemporal Attention for Early Prediction of Hepatocellular Carcinoma based on Longitudinal Ultrasound Images	"The proposed method lacks novelty in some technical aspects. The proposed method is almost a combination of existing technologies. The core idea of the ROI attention block is the same as the method originally proposed in [1]. In addition, the transformer encoder used in the proposed method has the same architecture as the original transformer [2]. The age-based position embedding in the proposed method has some originality (and experimental results show that it can improve performance), but it is not novel from a technical point of view.

[1] Wang et al., ""Non-local neural networks"", CVPR2018
[2] Vaswan et al., ""Attention Is All You Need"", NIPS2017"	No glaring weaknesses, although comparison to radiologist performance and analysis of failure cases would help demonstrate clinical utility. I am also curious how much radiologists actually rely on previous scans to perform a diagnosis, and how much this model does. Is it sufficient to just show the latest + current scan to get good accuracy? What about the current scan alone?	"The experimental evaluation should be clearer.
The writing quality of this paper is not satisfactory.
There are sentences which require some references."
476	Spatio-temporal motion correction and iterative reconstruction of in-utero fetal fMRI	"The computational effort of this algorithm needs to be detailed - specifically memory and time per iteration
The choice of the Lagrange parameter (Alphas in the current implementation) needs to be demonstrated based on L-curves or other methods - Tikhonov for example.
It will be useful to see the original SSIM values instead of just the difference"	"Comparison is not fair enough from my point of view. 
In Section 3.3, baseline methods are simple interpolation of independently re-aligned 3D images of different time frames. 
Firstly, comparing the proposed method with linear, cubic, and sinc interpolation is redundant. There is not a significant gap between different interpolation methods, thus it is not very necessary to compare all this interpolation method. Also, better baseline methods, rather than simple interpolation, would make the proposed method more convincing.
Secondly, for fMRI analysis, is it necessary and important to interpolate between different time frames? It seems interpolation is not used in experiment in Section 3.4 (Fig. 4).
In Section 3.4, it seems that the proposed method is compare with observed image without any re-alignment, which is not fair enough. It is better to perform basic 3D registration before fMRI analysis. If I misunderstand something in Section 3.4, this should be clarified."	"The iterative spatial-temporal reconstruction may be computationally expensive; the computation time should be clarified
The motion is estimated prior to the iterative reconstruction. It is unclear whether the motion parameters can also be iteratively updated during the reconstruction."
477	Stabilize, Decompose, and Denoise: Self-Supervised Fluoroscopy Denoising	"Literature review of the paper is very poor. Although it has discussed different method for self supervised denoising, RPCA but no previous study regarding fluoroscopy denoising is described.
Ambiguity in clinical study. It is not clear what the author mean by ""In addition, for each group, the five images are permuted randomly"". If this permitted images are used as input to denoising network, then for some case the already denoised image may have been used as input to the network. What is necessity of this step is not clear, and how the author ensure the above did not happen mistakenly.
No comparison with existing methods for fluoroscopy denoising. The paper only considered baseline denoising method without first two stage and complete method. However did not compare their method with exsinting fluoroscopy denoising methods.
a. Matviychuk, Yevgen, et al. ""Learning a multiscale patch-based representation for image denoising in X-ray fluoroscopy."" 2016 IEEE International Conference on Image Processing (ICIP). IEEE, 2016.
b. Amiot, Carole, et al. ""Spatio-temporal multiscale denoising of fluoroscopic sequence."" IEEE transactions on medical imaging 35.6 (2016): 1565-1574.

Also other SOTA self supervised video denoising method should be used as baseline."	Please see the detailed comments below.	Please see below
478	Stay focused - Enhancing model interpretability through guided feature training	"Overall, it was difficult to know the purpose of the proposed study. The proposed guided feature training looks like a variant of the general feature fusion-based training method. The fused training with additional labeling information can improve recognition performance and enhance feature visualization such as GradSmooth in the model.
In conclusion, I am not sure if guided feature training can be viewed as a branch of XAI research."	"The authors evaluate the proposed approach in three settings. The third one as stated by the authors is ""investigating the effects of GFT in its realistic application"". Table 1 shows the results of these three settings. The one in realistic applications, in which segmentation is automatically inferred, shows that a conventional training ('none') obtains an average F1 of 76.7%, using blurred backgrounds ('blurred') obtains 74%, and using non-blurred and blurred images ('combo') obtains a 77.3%. This suggests that using only blurred images damages the performance (though improving the interpretability). The experiment with 'combo' seems to slightly improve the performance (+0.6%). However, it seems that there is no experiment that shows wether this small improvement comes from using non-blurred + blurred images or simply by training on double amount of (repeated) data. Authors should evaluate the model in a fourth setting 'naive combo' where they use double amount of (repeated) training data without any blurring to obtain a fair evaluation.
It is difficult to assess a paper for which the data is not public at the time of submission."	"Interesting results, just thinking that if it performed well on a small dataset, perhaps we don't have a clear indication of performance but a start of a particular trend. My concern is probabably the different combinations of original/modified datasets - what/how is an optimal vlaue chosen?  This is not clearly defined/explained (page 3).

On page 3, perhas a more technical word/wording for 'decent approach', as it is a formal paper.
I would like to see more statstical testing performed to compare model performances."
479	Stepwise Feature Fusion: Local Guides Global	"The main claim that the LE module works becuase it can reduce ""attention dispersion"" (I guess it's more commonly known as ""feature oversmoothing""?) But the intuition why this can reduce ""attention dispersion"" is not clearly explained. The authors only support this claim with visualizations (Fig. 2).
Important baselines are missing in the experiments, esp. ViT based models, such as Segtran (IJCAI 2021), TransUNet (arXiv:2102.04306), Swin-UNet (arxiv:2105.05537) and SETR. In addition, PraNet should be compared in Table 1.
Citations are missing for Segtran, TransUNet and Swin-UNet.
Various writing problems (see Section 8)."	"To verify the effectiveness of the LE module, it would be better to have an ablation study in which conv layers are removed. In Fig 1(b), does it mean that the sizes of output are always H(W)/4?
The framework exploits the transformer encoder to have enhanced capability. The whole pipeline is similar to Unet. Overall, the technical novelty is marginal. For results, compared to SegFormer, the improvement is marginal."	"The paper's novel PLD contains two parts: LE and SFA. Although the results show that this PLD decoder improves the segmentation mDice score on multiple datasets, there's no ablation study to explore and demonstrate the effect of LE or SFA separately, i.e. how is the performance gain with only LE combined with traditional parallel feature fusion? and how is the performance gain with sole SFA paired with simple upsampling without precedent two conv layers? Without these ablation study, it's hard to verify whether the performance gain of the whole model benefits from LE or SFA or both.
The scores of other horizontal methods in Table 2 and 3 (generalization tests) are directly refer to [9, 14, 16, 25], instead of reproduce those papers' methods using the same experimental settings, especially data augmentation in this paper. This is because the data augmentation itself is able to improve the generalizability/robustness of the model on unseen dataset, which has been proved in both robust learning[A1] and domain generalization[A2]. Therefore, if the scores are refer to the original SOTA papers, people cannot know whether the improvements of generalizability is from the proposed model or the different data augmentation.
The paper uses mDice and mIOU as evaluation metrics, but these two metrics are actually evaluating the same aspect of segmentation by calculating overlapping area, so only keep mDice is enough, which is widely accepted in medical segmentation. In addition, in order to better demonstrate the segmentation performance, Hausdorf-Distance, another commonly used segmentation metric, should be used to evaluate the predicted mask shape accuracy.
The novelty of the method is slightly limited: the pyramid transformer encoder design is from [20,22], and for the two novelties in PLD, compared to Segformer[22], the Local Emphasis simply adds two convolution layers before upsampling feature maps of each scale, and the Stepwise Feature Aggregation, instead of parallel fusion, inserts a linear layer between each aggregation level and progressively fuses multi-scale features from deep to shallow, where this progressive feature fusing scheme is also used in many other segmentation decoders, even widely-used Unet is using a complicated version of this scheme. Thus, the method novelty of this paper is more like some small increments of modules from previous works, which somewhat limits the method novelty of the paper.

Minor problems:

The convolution kernel size of LE module is missed.
The size of C in PLD is missed.
The description of Fig.2 has no explanation of (c).
The description of Table.4 has no definition of CvT.
Eq1 doesn't contain the upsampling operation in LE.

Ref: 
[A1] Hendrycks, Dan, et al. ""Augmix: A simple data processing method to improve robustness and uncertainty."" ICLR 2020.
[A2] Volpi, Riccardo, et al. ""Continual adaptation of visual representations via domain randomization and meta-learning."" CVPR 2021."
480	Stereo Depth Estimation via Self-Supervised Contrastive Representation Learning	I do not have major concerns for this paper.	Despite the novel application of CRL on stereo depth estimation, the proposed method seems to be a combination/modification of multiple prior work, which limits the technical novelty. For example, the proposed momentum supervised contrastive loss is a combination of the MOCO loss [8] and supervised contrastive loss [9]. And the decoder is modified from DispNet and PWCNet [14]	"It is unclear what is the major difference between the proposed method and existing methods. It seems the two stage framework with the first stage using the contrastive representation learning is the main difference. If so, it is necessary to study other approaches to learn the representation at the first stage as the first stage only requires class labels rather than the ground truth disparity maps. The class labels are easy to get. There are other options to learn/initialize the encoder such as finetuning a pretrained model.
Both the memory dictionary and the momentum based key encoder appear in the literature of contrastive representation learning. It is hard to identify any new method/contribution in the proposed stereo contrastive representation learning.
It is unclear what is the difference of the proposed decoder and the decoder in DispNet as the details of DispNet is missing.
It is unclear what image data, what feature representation, what model etc are used to obtain the t-sne visualisations in Fig.1. Thus, it is impossible to draw any conclusion based on the figure.
It is unclear how to train the Encoder when using only the Lpe. Does the Encoder use weights pretrained on another dataset? Is the Encoder initialized randomly and jointly trained with the decoder?"
481	Stroke lesion segmentation from low-quality and few-shot MRIs via similarity-weighted self-ensembling framework	"Lack of intuition on the process. Brain tumor is irregular with a complex structure (oedema, core and enhanced regions) represented by four labels, while the ischemic stroke don't present the same complexity in structure, being represented by a single region (one label). This mismatch in complexity is not considered in the paper and the authors don't provide a rationale for the soundness of the approach.

Lack of clarity on the architecture. The architecture in Fig 2 is explained briefly, omitting the definition of some modules, such as, background and foreground attentive inception, channel and spatial attention, number of feature maps, places where upsampling are performed and the use of some symbols, etc. So it is difficult to understand how the architecture were engineered.

Lack of detail on the training. How the authors deal with the fact that both problems use a different number of input sequences and the output stage are different? Also, the trained process is not adequately explained. We do not know when the SDU interacts with the network. Every batch? Every epoch? And the loss function? How is it composed?

Lack of clarity on the tests. The authors don't explain how the three methods (MLDG, PROTO and Reptile) were trained. Also, it is not explained why those were chosen and if they could be applied in this context. For instance in MLDG [15], the common factor was the bone, while the variation was the place, acquisition protocol, orientation, field of view, or surgical implant. So, the rationale of including methods whose assumptions are quite different has to be properly explained.

Size of the test set. Although the metrics improve over the methods used for comparison: 1) the test set is small, so the metrics could be due to the random choice of cases and the inadequacy of the compared methods due to the mismatch on their assumptions. 2) Also, we don't know the architecture of the baseline. Since, the authors only provides the boxplot for the baseline, we can't compare the baseline with the proposed method and methods used for comparison at the same time."	"-The authors describe in details the shortage of radiologists and the low quality MRI available in developing countries, and this can be of interest to the reader. In this work, however, high-quality datasets are degraded to simulate low-quality data. It would have been extremely interesting to evaluate the proposed method also on an acquired low-quality MRI dataset.
-Statistical tests to support the results obtained are missing."	"Meta-learning baselines are not really fair. The meta-learning algorithms are typically applied when you have multiple training tasks (more than 1!) that are drawn from the same task distribution. It seems a little different from choosing a related task to pretrain/co-train on. It also would be helpful to have an attention-based baseline to compare IDN against. Maybe a segmentation transformer like UNETR.

Ablations are not sufficient to understand what parts of the innovations matter, or why IDN/SDU synergize. For IDN, multi-layered supervision should be its own ablation vs. the attention layers. ""SDU only"" should have its own results. Pretraining-then-finetuning is a well-proven approach for a lot of other applications, help us understand why SDU makes sense / performs better in this application."
482	Structure-consistent Restoration Network for Cataract Fundus Image Enhancement	"(1) Compared with other studies, the model proposed by the author has a certain improvement in evaluation metrics.  But the samples of public data sets used for evaluation and comparison are few, public data sets are also few, so the model's validity is limited.
(2) IoU was not validated in the Kaggle dataset.
(3) The authors said intuitively SCS generated by cataract simulation models with Eq. 2 share the same identical structures. They should use metrics for quantitative and qualitative evaluation.
(4) In abstract part, the authors said the SCS were generated from cataract fundus images.  However, in other parts, such as in Fig.1, SCS were generated from a clear image.  They should be unified.
(5) Were the clear images from post-operation eyes or normal eyes?If the clear images are from post-operation eyes, would they also suffer from the short of data.
(6) The authors should not augment their conclusions. E.g., they demonstrate that their approach was effectiveness in the follow-up clinical applications and the existing algorithms ignored the performance improvement of clinical applications from the enhancement. But there was no corresponding evidence. 
(7) In conclusion part, the authors said ""Thanks to its independence from annotations and test data, the proposed algorithm is convenient to deploy in clinics"". Please explain ""independence"" and the convenience on clinical applications."	"The value settings of specific parameters in most formulas are not clear, the authors need to show the actual parameter values in paper and explain the reasons for choosing these values. Moreover, some revelant experimental data in parameter settings need to be introduced, which means it would be better if following ablation studies of parameters are shown in the experiment section.
In introduction section, the article summarizes the challenges and shortcomings of cataract restoration algorithms, however, ""To address these problems"" in page 2 is not suitable. The point four, ""ignoring the performance improvement of clinical applications from the enhancement"", seems not clearly illustrated how it was solved. Besides, several experiments to evaluate clinical applications need to be done.
Though the authors have done a solid work, the novelty of the work is limited. The designment of the whole restoration process is too complicated, it's hard to say whether the final results are benefit from the structure consistency idea or just many additional detailed local designments."	"1.The discussion of three loss function in Eq.7 is not sufficent, conducting ablation studies on them would make the proposed method more convincing.
2.It would be better to present more qualitative results of cataract restoration results on appendix, considering Fig.3 only contains one test sample."
483	Super-Focus: Domain Adaptation for Embryo Imaging via Self-Supervised Focal Plane Regression	* Some parts of this manuscript can benefit from more detailed explanation. For example, it is unclear how the decision on about which focal planes are missing (and need to be generated) is made.	"Is Frobenius norm the best for L_per? Does a different loss such as cosine similarity make any difference?
It was not studied if there is any domain shift between the datasets from different centers due to different settings of microscope. The paper only talks about the number of focal planes assuming different domain and centers produce identical images."	"There's not much to criticize from my side but I had a few questions that may be worth answering in the paper:

You only use D_A for training of your model. In Table 2 it seems that the model consistently performs worse on data sets C-E. Why not including a few images of these clinics as well to increase the variability seen during training? As it's a self-supervised approach it should be straightforward to include other training images as well.
In Table 2, it also would have been interesting how human raters assess the unaltered images without any additional slices. Do complete images also obtain a score of 5 as would be expected?
You mention several changes you made to the original U-Net architecture but don't explain why those changes were made. The same applies to the training: you state you train for 30 epochs but don't mention any criterion for stopping the training. Please comment.
I did not fully understand how you know when to apply which of the generators. Are slices systematically missing, such that you could use the same generator for all data of a particular clinic? Or is there any other sophisticated way of identifying the missing slices?
I do understand that predicting a slice between two existing ones might be reasonable and doable (similar to interpolating between the slices). However, I have some doubts that the predictions at the border regions (above / below the acquired stack) are guaranteed to reflect the reality. Is there any way how to assess the validity of the slices in these border regions?"
484	SUPER-IVIM-DC: Intra-voxel incoherent motion based Fetal lung maturity assessment from limited DWI data using supervised learning coupled with data-consistency	"The authors provide quantitative results to show the superior performance of their approach but they do not perform a statistical analysis of any kind to prove the significance of this performance gain. As it stands, the presented results only show that their approach might be better but this can only be confirmed with a thorough statistical analysis.
The correlation between f and the gestational age is pretty tenuous. I doubt that this relation is a useful indicator of clinical impact of the method. Maybe there are better measures relating the lung development to the signal/IVIM model parameters?"	"clinical scenario: the authors removed cases with motion, however, it is sure that the retained cases had some residual motion. It is also not clear what motion correction strategy they used. The low SNR of fetal MRI, particularly for motion-sensitive sequences like EPI, is often a result of spin dephasing. Therefore it is important to characterize residual motion and perhaps describe the reliability of the method as a function of motion or other artifacts

the authors tested their study on one participant only. It is not clear how the network would behave on new data

Fig 2. Is a bit misleading since the Y axis has been scaled and shifted in a way to exaggerate the magnitude of improvement: the NMSE of SUPER-IVIM-DC ranges from 0.26 to 0.3 while for IVIMnet is 0.25, however, in the plot, it looks like it is orders of magnitude better. The improvement for D is actually quite marginal."	DNN-based methods have formalized the IVIM model parameters estimation problem as a prediction problem, such as [3][4][11]. The problem formulation is not novel. The clarity of the paper is not very clear. There are many mistakes in expression.
485	Supervised Contrastive Learning to Classify Paranasal Anomalies in the Maxillary Sinus	"Contribution is not major, more application of an existing method to a medical dataset.
The presentation and structure can be improved and the Figures 2&3 are terrible.

Minor:

For [7] (citation/introduction of contrastive learning) please use the first original paper from  van den Oord and not a survey paper.
For t-SNE please always report the parameters, otherwise the results are hard to trust (basically t-SNE is not a good way to show separability, since no separability for one t-SNE setting does not mean there is no separability, it's nice 'anecdotally' but does not show anything).
Pg 5. ""F())"" there is a typo.
I can just assume (knowing contrastive learning) the there also is a Proj_1 which is used for the contrastive task (Proj_1 is only briefly mentioned in the architecture but not what its used for)."	"The novelty of the paper is not enough, which basically combines the supervised contrastive loss and binary cross-entropy loss.
The experimental comparison lacks the current SOTA classification models. Thus, it is hard to justify the paper's contribution.
It would be better if the authors can provide some results on public medical classification benchmarks"	"Experiments could have been better - it is possible that other methods could be comparable or even better than the proposed method. Showing scenarios where the proposed method really is more beneficial than the existing methods would be good.
Fig. 3. could be better - more explanations on what are shown - are the hyper-parameters fixed as much as possible? What about the hyper-parameters for t-SNE? What are the number of samples for the visualization and how are they chosen?"
486	Supervised Deep Learning for Head Motion Correction in PET	"Language problems need to be checked, many long sentences are not clearly expressed and are confusing.
The experiments section needs more clarifications:
The subject group contains patients with normal cognition, cocaine dependence, and cognitive diseases. I would like to know if any strategy was used in splitting the training and test sets? For example, keeping a diversity of patient types in each subset.
In the ablation study (Table 1), the test MSE value for ""more data, FWT, Deep Encoder and Normal sampling"", which is located in the second row, has a much higher value than the others. Can the authors explain this?
The results In fig. 2(a) are for subject 1. Also, there is another subject 1 in Table S1. I assume they are different subjects since one is for single-subject experiments and one is for multi-subject experiments. But referring to them the same name still causes confusion.
In section 3.2, the authors claimed that subject 2 has a mean MSE of 0.02. But Table S1 shows the mean MSE of subject 2 is 1.114. They are contradictory. I assume they are also different objects as I mentioned above. The confusions need to be addressed.
""The results for Subject 2 (mean MSE 0.02) show that the network is capable of accurately predicting motion from training subjects even though the motion relative to the reference frame was never used for training."" Does this mean that in the experiment in figure 2, the moving images of subject 2 have been resampled and different from the images that are used during training? The detail of the experiment settings needs to be clarified."	"The definition of the ""3D cloud representations of the PET data"" is not very clear. From the paper it looks like a back projection image, but why is it called 3D cloud data? Does it have anything to do with point clouds?

Also not clear if the 3D cloud images are attenuation corrected? For head motion, the non attenuation corrected data can be beneficial because there are quite strong signals near the skull.

It is not clear to me why the outputs of the encoders and FWT were multiplied, why use this way to combine the two sets of features?

The results don't seem very convincing for demonstrating the performance of motion estimation using the proposed method, as shown in Fig2. It is indeed a very challenging task, and it will be very helpful to know what the DL-HMC network is doing. The ablation analysis included is appreciated."	The clearest drawback is that the experimental tests are not so extensive, but considering the novelty of the approach, I would say that is acceptable. There are some minor inconsistencies, e.g. wrt acquisition time, in the methods which I will go into in detail below.
487	Suppressing Poisoning Attacks on Federated Learning for Medical Imaging	"The limitations of the proposed framework are not sufficiently discussed. Why is it necessary that the proportion of clients experiencing byzantine-failures
is less than 50%? There is not sufficient discussion, or proof, to justify this.
The discussion about limitations should also include distribution of data/classes into the different nodes. ie. in the non-iid data distribution case, what are the limitations depending on how the data and classes are distributed?
E and C groups (referenced in some parts of the paper, like Algorithm 1) should be more clearly defined through the text."	"Some key references and baselines are missing, e.g, RFA (Robust Aggregation for Federated Learning) and SparseFed (SparseFed: Mitigating Model Poisoning Attacks in Federated Learning with Sparsification).

Two differences between the proposed method and existing methods are: 1) using COPOD to score the clients, and 2) performing scoring on a distance space instead of the parameter space. The implementation details are well-described but lack theoretical analysis.

Despite the variety, the experiment settings are not comprehensive and the evaluations are less insightful."	"The authors use -1 as temperature parameter in softmax computation to reduce client weights with higher outlier scores and increase client weights with lower outlier scores. Though -1 is a feasible approach, it changed the original relative distribution of the learned outlier scores from the exp function. It may be better to keep the relativity for easier learning of the network.

The fonts in figures should be increased. Now it is so hard to see clearly.

Why the authors assume 40% of the clients are malicious instead of, say, 50%, 60%, 100%?"
489	Surgical Scene Segmentation Using Semantic Image Synthesis with a Virtual Surgery Environment	I don't have any complaints.	"The paper is technical and experimental. The scientific insights and methodological innovation are kind of lacking.

The paper is wordy and a little disorganized. Better figures and clear layout are expected. Section 3 could be extended for more informations."	"It's not clear where the authors see the novelty of the presented work. Is it the datasets, the synthetic generation method, or both?
The authors use out-of-the-box methods for semantic image synthesis and segmentation. The novelty of the work is limited to the dataset and the synthetic pipeline which could be valuable for further research and other clinical applications. There, the authors should publish the synthetic data generation method which could be transferred to other applications.
For models that perform already very well, the generated synthetic data does not improve the results."
490	Surgical Skill Assessment via Video Semantic Aggregation	The evaluation metrics are not acceptable, despite citing previous works that used correlation as a metric. Correlation is perhaps one of the less relevant measures when evaluating model predictions. Mean absolute error is ok, but not sufficient. I don't mean to undermine prior work on the JIGSAWS dataset that is listed in Table 1, but the sub-par choice of evaluation metrics by the community was likely because of insufficient input from collaborators with statistical expertise. There are no measures of variance, which makes the claims made in the paper unclear to me. Many of the claimed improvements may be within what is expected from sampling variance.	"(1) The writing needs improvement. For example, Fig. 3 in Sec. 3.3 mentions that ""SGM facilitates the concentration on the task-related regions such as tools and discarding the unrelated background regions."" However, it is not clear how the conclusion was obtained. Does it mean that the authos conduct an comparision experiment that Grad-CAM combined with and w/o SGM?
(2) As for temproal context modeling, have the authors considered other models besides LSTM and BiLSTM?
(3) As for the number K, besides the ablation study on K=2,3,4, is there any more analysis and consideration for the choice of K? For example, does it depends on the scene complexity of the surgical videos? or sugery types?"	"The inverse kinematics are used as supervision in the Semantic Grouping
Module (SGM), however, inverse kinematics are not sensitive enough to guide this process, and it can in fact be observed that the tool positions are not accurate, nor precise in the Fig. 4. What I understand is that the authors use inverse kinematics as to provide somewhat close to accurate supervision, and it seems to improve performance.
No supervision is required to discover semantic features which are expected to belong to different surgical elements such as tools, the tissue, and the background. While this is interesting, it is achieved by clustering, and the number of the clusters is chosen somewhat arbitrarily with the belief that they will corralte to the surgical elements. In Fig 4, for example, we see that there is only a loose correlation."
491	Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer	"The novelty of the proposed VisualBert ResMLP is limited considering it as a combination of VisualBert and ResMLP modules.

VisualBert ResMLP seems to achieve very close performance to VisualBert alone in Table 1 and Table 2. This can also be seen from Figure 4 (c). This makes me wonder the effectiveness of combining the ResMLP module.

Table 1, the Acc performance of MedFuse is different from the results in the original MedFuse paper. Can the authors explain more on this?

How long does it take and what are the GPU requirements to conduct this experiment?

The font size should be increased for better visibility in Figure 2, 3, 4."	"The principal weaknesses of the paper are listed below:

This work does not present technical novelty. All the individual components used in this work already exist and are already implemented. This paper presents just a compilation without almost any modification. Specifically, the main architecture was taken from [15], and it uses ResMLP from [24].
The improvements by incorporating the ResMLP into the main architecture are marginal. The reported results do not demonstrate the required empirical contribution of the paper.
The experimental setup does not allow an assessment of the model generalization capacity. Optimizing the architecture over the test sets might result in overfitting in the three benchmark datasets. Without results in an independent set of data, it is not possible to discard that possibility.
The data used is not public. According to the reproducibility checklist, the data will be released upon acceptance. However, within the text, there is no clear intention to make it publicly available or as a relevant contribution to the paper. If the data were not released, it would limit the reproducibility of the results and the progress in this task. Considering that the paper provides ablation experiments, these should be performed on the validation set to guarantee its generalization."	"-For the ResMLP, the figure illustration and the equation (1) (2) are so general that the input shape, output shape and the processing is not well explained.
-Computational resource, the transformer structure is a high space-complexity method, it has the fundamental disadvantage to handle long sequences. It should be discussed the computational resources used to train & test the model. Also, length of input sequence should be clarified.
-ResMLP: The effect of ResMLP seems to be minor according to the tables 1."
492	Survival Prediction of Brain Cancer with Incomplete Radiology, Pathology, Genomic, and Demographic Data	"The authors use a two-layer decoder to reconstruct the embedding learned by unimodal feature extractors. Performing that in two separate stages makes sense; however, during training in an end-to-end fashion, making the decoder learn projection of a previous layer is a non-trivial choice. In such scenarios, the model could learn random projection that does not contain the input's formation. Usually, decoders are used to reconstruct back the whole data. It would be helpful if the authors could provide further clarification on this. This could be the reason for the poor performance of the end-to-end approach.
The authors fix the hyperparameter \lambda to 1. They argued that it is empirically selected to balance the cox loss and the reconstruction loss. Does this mean L_cox and L_recon lie in the same range? If not, this choice is not natural and should be fixed using cross-validation."	"More details are needed for the extraction of radiology and histology image features.
More discussions could be conducted on the relationship between information provided by different modalities.
The introduction of embedding reconstruction seems to bring marginal value to the prediction."	The paper claims to completely solve 4 important questions in multi-modality fusion with incomplete data. This is a very large claim that needs to be stated with caution, with assumptions and limitations. This paper only works on a limited glioma dataset. Thus, any claims should be restricted to this setting. The optimal setting of the multi-modality fusion network discovered by the authors is for a particular set of setups which the authors consider - i.e. where mean vector fusion is used, along with autoencoders for helping with generating informative embeddings.
493	SVoRT: Iterative Transformer for Slice-to-Volume Registration in Fetal Brain MRI	"Quantitative validation is limited to 12 subjects only, though x4 are generated from those
Overclaims on the impact of the proposed method to final 3D reconstruction methods in practice (authors limit up to 3 stacks only in this paper)
Lack of details on SOTA parameters or how SVoRT parameters are set (lambda)"	"Singh A, Salehi S S M, Gholipour A. Deep predictive motion tracking in magnetic resonance imaging: application to fetal imaging[J]. IEEE Transactions on Medical Imaging, 2020, 39(11): 3523-3534.
The related work is missed. This paper also predicted motion parameters from sequences of slices based on RNN. It also utilize spatio-temporal information."	"No baseline comparison to fetal SVR non DL methods
Some choices are not backed up by explanations or experiments (please refer to point 8)
It would have been great to see more real world examples such as in supplementary materials
Minor comments (point 8)"
494	Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI	"Supplementary material is not provided though mentioned in the main paper
The values of model parameters are missing, e.g. L in RSDTB/RDTB blocks, r in deformable attention, the number of feature channels, and the window sizes for Swin attention computation
In the ablation study, it is unclear how DDDDDD-O models perform, i.e. models using dense deformable attention only. I understand it might be computationally costly, but will it bring significant performance gain?"	"The novelty is limited, given the fact that the authors combined the UNet with Transformers (SWIN) and deformable convolutions. As an example, U-Net on Transformers already exists: 
1) Petit, O., Thome, N., Rambour, C., & Soler, L. (2021). U-Net Transformer: Self and Cross Attention for Medical Image Segmentation. ArXiv, abs/2103.06104.
2) Unet based on SWIN: Cao, Hu et al. ""Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation."" ArXiv abs/2105.05537 (2021)
3) Gao, Yunhe et al. ""UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation."" ArXiv abs/2107.00781 (2021):
4) Yeonghyeon Gu, Zhegao P., Seong J. Y. ""STHarDNet: Swin Transformer with HarDNet for MRI Segmentation"", Applied Sciences 12 (1)468, 2022. DOI:10.3390/app12010468
Here is a nice collection of models for MRI acceleration using transformers:
https://github.com/junyuchen245/Transformer_for_medical_image_analysis
There is no comparison with any of these methods in the experiments. In general, the authors compare only to the three basic approaches and SwinMR, with respect to which there is almost no improvement, except the reduction of the number of parameters. It would be important to compare with performance to, e.g., SwinUnet (second reference on the list) - also because of its efficient configuration.
Also, the manuscript presents only 1 dataset, on the topic of fast MRI where the the top benchmarks of Fast MRI challenge (https://fastmri.org/) are not considered."	"While the proposed SDAUT outperforms the competing algorithms in the paper, it lacks the comparison with state-of-the-art algorithms.
The authors claimed the proposed SDAUT provide explainability. It is not clearly written how the proposed methods/which mechanisms provide explainability. While the discussion & conclusion section shows an example of such claim, it lacks of explanation of 'explainable fastMRI' . The authors could define and clearly mention how does SDAUT is explainable.
The supplementary material (which has been mentioned multiple times in the manuscript) is missing.
Data consistency (DC) module plays an important role in undersampled MRI reconstruction. The proposed module does not contain DC layer, potentially leading to lower fidelity reconstructed images. Nor did the authors compare with the models that contain DC blocks, such as D5C5, variational network, MoDL etc.
While the writing is easy to follow, the motivation is not well-explained. e.g. How the proposed methods reduce computational cost? Why/How does it provide explainablity? Although the results section confirmed this, the introduction/methods part did not mention it, which makes the manuscript less convincing.
The data used in the manuscript is 12-channel, how does the authors combine the multi-coil images? Did you use sensitivity map?"
495	Swin-VoxelMorph: A Symmetric Unsupervised Learning Model for Deformable Medical Image Registration Using Swin Transformer	-  only minor weakness (see below)	The main weakness of the paper is with the figures. Figure 3 is hard to follow. Are the warp field and Jacobian determinant corresponding to the top images or the bottom images? The colorbar is also very hard to see.	"The The structure is similar to reference [6]. 
The main difference is that swin transformer replaces fully revolutionary networks.
The technical novelty of the method is limited and the performance increment is incremental."
496	Tagged-MRI Sequence to Audio Synthesis via Self Residual Attention Guided Heterogeneous Translator	"Details or justifications were missing for operations used in pair-wise disentangle training.
The sample size for the experiments was small. There were only two words investigated in this study, and the number of the data sample was imbalanced.
The performance gain of adding GAN is marginal compared to attention and pair-wise disentangle, taking the computational cost of GAN into account, the involvement of GAN is questionable for such a model. Loss with/without GAN was missing."	It's hard to reproduce, because they did not share the source codes.	"The proposed model is trained with a total of 63 tagged-MRI sequences which is a very limited sample size. It would be interesting to have a discussion how this method can scale up.
The differences to the competing architecture, Lip2AudSpect, could be explained better. It could also be explained better how Lip2AudSpect was adapted to MRI data.
The authors did not mention the research field of speech generation from real-time ultrasound which is very related."
497	Task-oriented Self-supervised Learning for Anomaly Detection in Electroencephalography	"The simulated abnormal EEGs are generally different from real abnormal EEGs, e.g., more irregular and include combinations of various anomalies. 
It is expected that more realistic amplitude-abnormal and frequency-abnormal EEG data can be generated for anomaly detection. 
Hence, it is in doubt whether or not the proposed method is capable of detecting more complex noisy abnormalities in EEG signals."	"I worry that how deep the knowledge human experts need to have to create valuable but not detrimental SSL rules. For instance, the proposed method works in these three datasets due to the simulated anomalies make some sense under the context. However, do this approach generalize when we inject incompatible simulation rules?
The baseline are not SOTA. For instance, OCSVM, KDE, and AE are very basic methods. There are a large number of SOTA sequence-based anomaly detection methods in https://github.com/datamllab/tods [1].

[1] Lai, K.H., Zha, D., Wang, G., Xu, J., Zhao, Y., Kumar, D., Chen, Y., Zumkhawaka, P., Wan, M., Martinez, D. and Hu, X., 2021, May. TODS: An Automated Time Series Outlier Detection System. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 18, pp. 16060-16062)."	""" Impirical evaluations show that the above-mentioned simple transformations are sufficient to help train an anomaly detector for EEGs. ""
The comparsion with fully-supervised method is required to support this claim. This claim may mislead the readers !   Intuitively, more complex anomaly data can improve the performance.
Also, why not choose more complex anomaly similation combining frequency and amplitude abnormal ?"
498	Task-relevant Feature Replenishment for Cross-centre Polyp Segmentation	"It is not clear how attention is used in the DIFD module. In section 2.1: the paper states that h() and g() are the attention functions, but no details are provided about them.
An overview of the DIFD module is provided in Fig.2 but there is not a clear explanation of the design of the module in the paper.
Sec 2.3 How the w_bg is calculated? What the ""significance of predicted background region alignment"" represents?
What is the real contribution of adversarial and TCLoss? The lamda parameters used are very lower. Why? Have these two losses different orders of magnitude w.r.t. the segmentation loss?"	"Lack of qualitative analysis of each module: while each component (e.g. DIFD, TRFR) has clear objective, whether in the end they realize the goal or not is hard to tell from the current presentation;
Some design decisions are lack of careful justification (TCLoss, PAAL and TRFR)."	"Figure 2 needs to be redrawn again. In the current form, It is difficult to follow. For instance, From the reader's point of view, it is hard to understand where X(s) is being used. The authors also need to mention which is their final output (prediction masks)? From the Figure, it is unclear.
The author uses SE-block. CBAM has already shown improvement over squeeze and excitation block. Is there any specific region behind choosing the block? An ablation on this would be helpful.
From the reader's point of view, the authors confuse domain adaptation and domain generalization. Please make it clear.
The authors only test on still images, consisting of tiny images samples. Currently there are many public video polyp segmentation datasets available with large number of samples and are public. Please validate your results on larger datasets such as ASU-Mayo, CVC-videoClinicDB, and SUN datasets).
The authors could site some relevant important domain generalization and recent multi-centre work in the literature."
499	TBraTS: Trusted Brain Tumor Segmentation	I did not detect any real weakness in this pape except the lack of references in matter of U-Net variants.	"The experiment section is not well organized and the results are not well explained. For example, Fig2 a) is complex and hard to understand. 
In paragraph""Differences from similar methods."", the authors should compare with other similar Dempster-Shafer-based methods. See detailed comments below."	"Lack of novelty. The proposed method is a direct application of the existing evidential deep learning method [22] to the brain tumor segmentation task. Although [22] is 2D image classification-oriented, and this paper focuses on 3D medical image segmentation, the overall difference is small. Generalizing techniques from image-level classification to pixel-level classification (segmentation) is straightforward. The authors are suggested to elaborate more on their contributions.

Is there any other paper that also relies on evidential deep learning [22] in medical images? If yes, the authors should discuss them as well.

The writing should be improved. The authors are suggested to proofread carefully and enhance the writing.

Minor issues:
Scale Fig. 2 to a proper range. It is hard for readers who prefer to print the paper and read."
500	Test Time Transform Prediction for Open Set Histopathological Image Recognition	There are some points that needs clarification.	This work lacks of the performance investigation of appearance transformation stage.	While the paper is meant to provide output of detailed classification of known regions, this specific performance is not well analyzed. In addition, the performance improvements with regards to the other evaluated methods seem minimal. Therefore, the added value of the approach (in technical terms) is unclear.
501	Test-time Adaptation with Calibration of Medical Image Classification Nets for Label Distribution Shift	"The clarity associated with the derivations, especially on Eqs 1-3, should be significantly improved. 
The underlying assumption on multiple binary classifcations with sigmoid function is not discussed in detail."	"W1: Although the authors have elaborated the significance of the tackled problem: label distribution shift, I cannot grasp the significance of solving the label distribution shift issue if there is no long-tail problem in training dataset or there is no domain gap between training and test datasets. Since we usually conduct inference for test image one by one, the final classification performance is not sensitive to the label distribution of test data.
W2: As illustrated in the experimental section, the training and test data are from different centers with domain gap. What is the difference between the tackled label distribution shift problem and widely investigated domain adaptation tasks, such as domain adaptation and test-time adaptation?"	"The difference with [34]. Please add information on how much and in what this method differs from the cited works as the provided explanation (1 line with no clear info) is no enough in order to compare them
2.3 ""...with the implicit knowledge of label distribution on the test set"" can give more insights about this information? I see from the test time adaptation loss that label information is not present (and I suppose that this information cannot be used) but this sentence leaves me with doubts
The advantages of test time adaptation are not clear to me in the medical setting. If the authors can clarify this aspect in the paper in the rebuttal would be beneficial. Why do I need to adapt at test time? How your model works when trained on a balanced train and test sets and compared to a classical training procedure? I think that one of the major barriers to applying Ai in medicine is that models are not still effectively explainable. I don't know how a model that adapts itself at inference time could be helpful in the medical community."
502	Test-Time Adaptation with Shape Moments for Image Segmentation	"-Minor spelling mistake - 3.1 Adaption instead of Adaptation.
-The Shannon entropy expression in page 4: Did the authors mean to put the class weights inside the summation over 'k'?
-Compared to ref [2] and [20], the novelty is limited to inference on the test subject and use of soft constraints on centroid and distance-to-centroid shape descriptors.
-How does the approach do when the DNN is adapted using the proposed loss function on the entire target domain data? 
-There is a large gap between the oracle and the proposed method."	There are two major concerns regarding the work. First, the presentation of the mathematical aspects of the approach is inconsistent and accordingly at parts very hard to follow. There are no major issues, however multiple inconsistencies sum up and make it hard to build an intuition of the general idea of the work (see section with detailed constructive comments). Second, it seems that the chosen datasets are well suited for the incorporation of the defined shape priors, especially the centroid. It might be a good idea to optimize the center of mass of a class within a slice of the image towards the estimated center of mass defined by the whole 3D image for roundish classes (as they appear in the two datasets), however, this is not necessarily true if there are i.e., classes with elongated shapes diagonally covering the whole image domain. Intuitively, the matching of the distance to the centroid from one slice to the whole stack of slices is probably more robust in that regard, the authors could probably comment on the choice of Moments in their discussion, why and where they are expected to work well?	"It is hard to understand the main idea of this work for the first time.
Compositions of Figures 1 and 2 need to be improved."
503	Test-time image-to-image translation ensembling improves out-of-distribution generalization in histopathology	"Although there is a good improvement, however, the idea's novelty is still somewhat lacking. A similar idea can be found in https://proceedings.neurips.cc/paper/2021/file/a8f12d9486cbcc2fe0cfc5352011ad35-Paper.pdf.  They achieved 94.8% on camelyon17.
When comparing with other TTA methods, can the authors provide a forward count/test time comparison? The performance increases while the time required by the authors' method may increase many times, which is fatal for WSI images"	"The manuscript could be improved further and needs clarity and details on various points (see point 8).
The proposed approach may not scale well with an increase in a number of domains. Getting predictions against a large number of invariants in order to get a final prediction for an input image would be time consuming.
It is not clear if separate discriminator and classifier are trained for each domain style. If so, it could be a bottleneck when there are many domains."	"1)	Efficiency is also important for medical image especially for the inference stage. Yet, execution time/computational cost are not compared.
2)	No comparisons with any stain normalization approaches, which are mostly used in histology to overcome the stain variations.
3)	To make the comparison fair for evaluating the performance of StarGAN-V2 (which is a stain augmentation approach), comparing with (i) TTA + Stain Normalization; (ii) TTA + Stain Augmentation [ref. 21], which are much more efficient  histology-specific TTA approach, are missing. These approaches needs no extra computational cost to train a StarGAN but shows very good empirical results from my experiments (which is a very effective and efficient techniques for competition)."
504	TGANet: Text-guided attention for improved polyp segmentation	"TGANet uses many channel and spatial attention modules in the baseline, such as FEM. From Table 3, it seems the FEM is more important which improves the performance by 2.4%. However, simply adding such channel and spatial attention in FEM is a well-known trick for DNNs and lacks of the novelty.
To better evaluate the effectiveness of label attention, the ablation study lacks of the comparison that only using label attention (without FEM and MSFA)"	"The most important innovation, text guided attention mechanism, is not fully explained and investigated. The feature enhancement module and multi-scale feature aggregation are just new re-designs but not novel ideas. The authors should emphasize more on text guided attention mechanism. The so-called FEM and MSFA might be useful but not fresh at all.

The Label attention module should be introduced in detail, with more sentences. For example, byte-pair encoding (BPE) is proposed in NLP area. The authors should describe why and how BPE is used. Or the readers would be confused.

More experiments about text guided attention should be conducted and presented. For example, the classification performance, the visualization&discussion on the generated attentions."	"As for me, I think this paper does not bring many new insights. They add some attention-based modules (""Feature enhancement module"") and multi-scale feature learning modules to improve the performance. However, I think these similar components have been proved many times to be effective for network training in recent works, e.g. CBAM, Non-local, U-Net... Also, the text-based embedding method is more like a multi-task framework. I think the multi-task framework also has been used in many recent works."
505	The (de)biasing effect of GAN-based augmentation methods on skin lesion images	Novelity of the paper need to be mentioned clearly in an introduction section. GANs are being used widely for data augmentation. How the (de) biasing will change the overall workflow?	"The authors in the introduction clearly state that they are interested in understanding algorithmic bias, setting aside questions of racial, gender, etc... bias. This is very odd considering that the domain they are investigating, dermatological image processing, is definitely very affected by ethnicity and racial considerations.
There is a clarity issue for Section 3.3, notably the difference between the aug. GANs and the synth. GANs. One assumes it appears that artifacts were inserted into the evaluation dataset (in order to calculate the number of ""switches"") and also into the training set for the aug. GANs (and not the real data nor the synth. GANs) but this should be made explicit."	The quality of the annotation is not evaluated; for instance no information are given on the expertise of the annotator (practitioner, naive, ...). This have a direct impact on the final evaluation results.
506	The Dice loss in the context of missing or empty labels: introducing Ph and 	The introduction section is not clear and not helpful for understanding the context of the paper. The topics of missing and empty labels are barely mentioned there.	"(1) I would like to point out that this paper is not easy to follow. Some descriptions in the paper are confusing and unclear. For example, for the BRATS dataset, what are partially and fully labeled data? In addition, the heuristic in Section 2.3 is not easy to understand. For example, what does it mean for the sentence ""A very simple strategy would be to let ...""?
(2) In the experiment, it is mentioned that the marginal Dice loss and the leaf Dice loss are compared. However, I did not see the results for these two losses in the experiment. Do these two losses correspond to some certain B values in Table 1? This is confusing.
(3) To demonstrate the heuristic regarding the choice of $\epsilon$, this paper sets $\epsilon$ to the expected volume of the groundtruth, which turns out to work well. To better illustrate this heuristic, it is necessary to conduct experiments where other $\epsilon$ values are chosen. This would provide a more effective comparison for the impact of $\epsilon$ values."	An experiment with real empty labels would be needed (cases or patches with only background, instead). Here the results show that the model starts learning when to predict empty labels when psi and epsilon are designed to do so, but only as a negative result (lower DSC). In the presence of real empty labels, we should see an increase of performance by reducing false positives in the empty maps.
507	The Intrinsic Manifolds of Radiological Images and their Role in Deep Learning	I could not identify any. However, code release that allows to reproduce the findings would be nice.	"Theoretical contribution of this work is weak. The ID computation is based on the existing method by [17,18], and directly used on medical and natural images. There's another concern is that maybe the ID computation for natural images is different from medical images. For example, the natural images are bounded by 0-255 and use 3 channels, while radiological images are not bounded and normally only contain 1 channel. 
Also, although Normalization layer might help to train the neural network, the distribution of radiological image pixels are very different from natural image pixels. So the current existing neural networks like resnet etc. may not be a good method to measure the test accuracy.

The ID computation is based on the assumption of Poisson process and MLE. This is slightly concerning as lacking of evidence. For example, one can show that auto encoder can/ cannot learn a reasonable reconstruction error for different dataset to show the empirical ID for each dataset. This paper directly uses the existing method to this specific domain and shows no toy data example to support the ID estimation. Therefore, the overall estimation of ID might not be exact and the finding of different scope of negative rely may come from very different aspects other than ""ID difference and domain difference""."	
508	The Semi-constrained Network-Based Statistic (scNBS): integrating local and global information for brain network inference	"It is not entirely clear to me if there is some potential issue of circularity (a.k.a. double-dipping, bias) in the proposed methodology. For example, in Section 2.c, where the cut-off selection is described, the Authors explain that ""the final threshold is the one that gives the largest effect among all possible cut-offs"". Typically, steps like this one, requires a careful nested cross-validation implementation to avoid that selection of parameters is operated in a biased way. Unfortunately, the manuscript is not clear on this (and other) implementation details but they claim to disclose the code of their experiments.
A second source of concern is the experiment's section, which is entirely based on ""synthetic"" data (I'd say semi-synthetic, since it heavily relies on actual fMRI data). The noise models are quite elementary and may not properly represent physiological situations."	"The algorithm performed after subnetworks were predefined. If the subnetworks were not known, how could the proposed method work?
The cut-off values, t_{g}^{+} and t_{g}^{-} were obtained by the correlation with y, and the final inference was also estimated by the relationship between V_{g}^{+}(t_{g}^{+}) and y, and between V_{g}^{-}(t_{g}^{-}) and y. The results obtained during the process of the proposed method were reused for the final results of the proposed method.
It would be better if the results of the consistency analysis of the other methods was also shown in Fig. 6."	"*	I do not see major weaknesses in the paper"
509	Thoracic Lymph Node Segmentation in CT imaging via Lymph Node Station Stratification and Size Encoding	Three super LN stations are used in this paper but it is not clear how many super stations are optimal in general.	"One would be tempted to think why a CNN would not be able to figure out by itself the grouping method for the super-stations, nor the large/small differentiation.
While the criteria for the super-station grouping is intuitive, perhaps the authors should try different criteria and see the change of performance.  Also with the large/small differentiation, in which values other than 10mm could be tried.
The datasets used in external testing are substantially different in terms of slice spacing, as these are 5mm vs 1.2mm in training data.  Also, these are all CT scans from patients with esophageal cancer, and I am not sure if there is any bias because of this."	"In a word, this paper is not well organized, and the writing is needed improved thoroughly. And the English of the manuscript should be significantly polished before further consideration for accept.
The architectural settings are rarely discussed. The reason of choice of nnunet blocks should be explained. And the structure of nnunet block should be described simply.
The images offered in the manuscript are low-quality. It is recommended to improve with high-quality images. Moreover, it is better to repaint some illustrations with unclear intentions. For example, such Fig 2, the contents are too small."
510	TINC: Temporally Informed Non-Contrastive Learning for Disease Progression Modeling in Retinal OCT Volumes	"The paper is not self-contained. I had to go to the original VICReg paper to understand the technical details of the method, and the intuition behind it. Also, there are additional unclear issues, which I detailed in Q8 below.
The evaluation is done on a single dataset."	discussion around misclassifications and possible ways to improve missing	"Only one dataset has been evaluated in this paper, which may limit its generability evaluation. If the authors only has one dataset available, cross-validation would be more appreciated.

The written can be improved. The author states that they have exploited the meta-information. However, what kind of meta information has included, and how they are included is not clear."
511	TMSS: An End-to-End Transformer-based Multimodal Network for Segmentation and Survival Prediction	"Images are cropped to 80x80x48. This potentially helps as a sort of attention/mask mechanism where the tumor region is now in focus. Using the entire image downsampled to the smaller size (for easier training) would have been fairer.
No ablation studies on the patch size, contribution of modalities etc has been done."	"It would good to provide more results.
The hypothesis behind using the network is not clear"	The authors have chosen to use a cross-validation and additionally conducted a metaparameter optimization. In this combination, however, the authors might have partly done a circular analysis.
512	Toward Clinically Assisted Colorectal Polyp Recognition via Structured Cross-modal Representation Consistency	The proposed method is evaluated on a relatively small datasets, and no information regarding the computation costs as well as no discussion on the real-time application in clinical settings is provided.	"The methodological innovation should be stated clearly
The research hypotheses of the work should be stated (e.g., which is the hypothesis behind using SAM?)
Limits in the literature should be highlighted.
The discussion of the results can be improved to give more insights to the readers."	Need more visual representation for result section.
513	Towards Confident Detection of Prostate Cancer using High Resolution Micro-ultrasound	The main weaknesses of the paper is that the comparison with other studies should be added, especially the final heatmap.	"A major component of the study is to compare different uncertainty methods. While this reviewer appreciates the trend and comparisons of different methods presented in Figure 2, the lack of data in tabular form makes it challenging to assess and gauge a full performance profile. In particular, the upper bound of confidence threshold for Evidential approach in Figure 2(a) is interesting in comparison to its counterparts.
There appears to be lack of discussions on the performance difference between EDL and EDL + Co-teaching for sensitivity and specificity, and whether such differences are clinical relevant, especially given the impact of co-teaching is a particular interest of the study.
For results presented in Figure 3, it would provide additional information and discussion if the heatmaps are compared with confidence threshold via quantification such as Jaccard or Dice against a baseline configuration."	Performance comparison with conventional ultrasound would be an appropriate gold standard, if data were available in a similar patient population. While performance in terms of confidence and accuracy are compared across model variations a more clinically relevant performance assessment should include actual decision making across a group of operators. For instance, if an operator is given control to adjust the confidence threshold for given images in a test set 1) would they take a biopsy and 2) where would they take the biopsy.
514	Towards Holistic Surgical Scene Understanding	Dataset only contains only 8 instances of surgeries (but total runtime of 19.1 hours, which is average compared to other datasets, see Table 1.). The cross-validation is 2-fold, 4 surgeries for training, 4 for validation, which even further reduces the amount of training data.	"TimeSformer and Swin transformers are two of the state-of-the-art models recently introduced in the computer vision community for video analysis tasks. For completeness, authors need to compare TAPIR with such methods to show their method is actually the state of the art.
It would be interesting to see how TAPIR performs on other publicly available datasets like Cholec80."	"The structure of the transformer is not shown in Figure 2. This is not conducive to the reproduction of the model.
All tables have no bottom border. It's not pretty.
The authors used too few methods for comparison to well verify the advanced performance of the proposed method."
515	Towards performant and reliable undersampled MR reconstruction via diffusion model sampling	"One work that the authors should have compared or at least cited is this: robust compressed sensing mri with deep generative priors, which use a GAN to learn the distribution p(y_full) in the paper, and use Langevin dynamics to perform the reconstruction. Please comment on the advantages of using diffusion model compared to GAN model.
If possible, please add the std statistics to Table 1.
As mentioned in the paper, the inference time is quite long for DiffuseRecon, can you provide some potential ways to accelerate it?"	"This paper is unclear about some implementation details:  As the authors claimed ""We follow [1] with some modifications to train a diffusion model that generates MR images.""  What are the detailed modifications? Since this step is the fundenmental of this work, I can not understand the training process from current version.
The authors also set all the medthods to produce 2 consecutive slices which is not similar to the original MR reconstruction baselines, why? All the figures in current paper are drawn to generate 1 slice along with the formulations. Do other models (D5C5 and KIKI) are also modified to  produce 2 consecutive slices?
The diffusion model is computationally expensive [1, 2]. The comparison of running time and parameters should be included in table 1. Although the authors disscussed the problem in page 8, excessive computational cost can also limit the applicability of the proposed method.
[1] Nichol A Q, Dhariwal P. Improved denoising diffusion probabilistic models[C]//International Conference on Machine Learning. PMLR, 2021: 8162-8171.
[2] Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in Neural Information Processing Systems, 2020, 33: 6840-6851."	"The description of the refinement in the coarse-to-fine module is not very clear.
The implementation detail of training DiffuseRecon is not elaborated."
516	Towards Unsupervised Ultrasound Video Clinical Quality Assessment with Multi-Modality Data	"Some information about the dataset is missing
Statistical analysis of the results is missing"	"Some important details about the proposed model, e.g. parameters, and the used dataset are not given
Code/datasets were not provided. Thus, given the missing information above, reproducibility of these results is hindered"	"The size of training dataset is quite small.
Details regarding dataset is missing. 
It is not clear how robust the method is to input domain shifts.
The clinical application of the proposed method seems to be minor."
517	Tracking by weakly-supervised learning and graph optimization for whole-embryo C. elegans lineages	The main shortcomings of this work are (i) incremental methodological novelty of the proposed method that only combines previous concepts ([14], [21] and [8]) into a single framework; (ii) insignificant superiority of the tracking performance in terms of DET and TRA compared to Linajea [14] that is named as JAN-US in the Cell Tracking Challenge; and (iii) very limited performance comparison with the state-of-the-art methods. Based on the scores listed in Tables 1 and 2 and available on the Cell Tracking Challenge website for the JAN-US (i.e., original Linajea) method (http://celltrackingchallenge.net/participants/JAN-US/), it does not seem the integrated cell state classifier would improve the tracking performance by any statistically or practically significant margin. Furthermore, it is unclear why the top-performing methods for the Fluo-N3DH-CE datasets (i.e., KTH-SE (1) and KIT-Sch-GE (1) publicly available on the Cell Tracking Challenge website) were not taken as the baselines for the mskcc-confocal and nih-ls recordings.	"* Improvement in terms of two established validation measures, related to detection (DET) and tracking (TRA), respectively, looks marginal.
* Modified version of the algorithm does not results in consistent performance on all the validation measures: while it improves the scores in some categories, the results with respect to other validation measures actually deteriorate."	"I'm wondering why the additional metrics were not computed for the cell tracking challenge data set and why it is mentioned separately in the text, rather than combining it to Table 1.
There was a recent submission to the cell tracking challenge that outperforms the proposed approach. While this is to be expected for an ongoing challenge, it would still be good to correct the statements in the results section accordingly.
The improvements over the previous methods seem to be quite modest. It would be good if the authors could describe the implications of the improved scores. Can this be somehow quantified? For instance: How many manual corrections are saved by these minor improvements? How many more complete lineages are present?
I'm aware that it would unveil the identity of the authors, but for the final manuscript it would be great to include a link to the annotated data set as the authors propose in the abstract. This would indeed be a valuable addition for the community."
518	TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers	"Writing is a bit hard to penetrate. I had to read it a few times.  Intro could be more focussed and define more clearly what the actual task is - it takes until we get to the experiments before that becomes clear.

Could do with some ad-hoc baselines in the experiments e.g. a simple tract-density image."	"Choosing the first two eigenvectors of the affinity matrix as the coordinates of the 2D embedding grid might lead to information loss.
The interpretability of CNN can also be achieved by Class Activation Maps (CAM). Authors does not provide experiments to show superiority of the proposed method against it."	"The introduction of TractoEmbedding method is not clear. First, spectral embedding as an important step, even citation is given, should give some basic introduction. Second, in the second step, only first two dimensions for each point was used to generate the  2D embedding grid. There is no experiments to discuss if the two dimensions can effectively represent the spatial information of the 3D fibers. Third, the paper mentioned ""multiple fibers that are spatially proximate are mapped to the same voxel"",  based on this kind of representation, the differences of multiple fibers which are mapped to the same pixel are ignored. It is better to conduct experiments to measure similarity and differences of the fibers at the same pixel.

Exp 1: Synthetic data. To evaluate if the proposed method can identify the fibers with group differences in the WBT data for interpretation, this paper generated two
synthetic groups of G1 and G2, compared G1, the mean FA of each CST fiber in G2 was decreased by 20%. This kind of group difference is too simple compared the complex change caused by SCZ. The results that the proposed model can get 100% acc for G1/G2 classification but a much lower acc for HC/SCZ (table1) can also illustrate this problem. Therefore, the interpretability of the results may be not very accurate."
519	TransEM: Residual Swin-Transformer based regularized PET image reconstruction	"This work uses the swin transformer as the image regularisation model in an existing reconstruction framework. Demonstration of the impact of incorporating the swin transformer needs to be stronger. It is hard to see the advantage given the low image resolution involved in the work. The swin transformer is supposed to capture long-range dependencies in the image, however this benefit is not clearly shown in the results. Also, with the swin transformer as an image quality improving tool, the authors did not justify the need to incorporate it into the reconstruction, given that it can be used post reconstruction as well.

This work is done in 2D, where PET image reconstruction is a 3D problem.

Only simulated data were used to train and assess the proposed model. As the authors mentioned in the discussion, clinical evaluation will greatly strengthen the impact of this work."	"1) Limited novelty: the proposed work is a special case of the swin-transformer, the current work has no obvious innovation in network architecture, except the additional shortcut added before and after the convolutional layer.
2) Without clinical evaluations: All the experiments are performed with simulated data. It is hard to check whether the proposed method is applicable in real applications. In addition, for practical use, TransEM may encounter difficulties in collecting abundant clinical raw data for the network training."	"The main weaknesses are more about clarity and organization rather than scientific aspect.
There are lots of mistakes which catch the eye when reading: some misprints (""reconstrution"", page 3, ""Possion"", page 3, equation (1)), ""the"" missing several times, capital letters in the middle of sentences, commas missing to better understand the structure of the sentence, spaces missing between words and their acronyms or after colons.
There are lots of imprecision in the chosen values (cf detailed constructive comments) and it seems that there is confusion between training set and validation set when the authors talk about hyperparameters.
The analysis of the results is light, sometimes unclear (the authors said TransEM does not perform so good, and then, it is the best in addition to DeepPET), not argued and not scientific enough: ""relative not so good results"", or subjective: ""lots of excellent works"".
The different number of counts in the experiment is unclear. The experiment seems to be a ""low count"" simulation. The network is trained with ""high dose"" images, which seems to correspond to the ""high count"" description from the authors. A second level of count is presented in the ""robustness analysis"" part, but a table presenting those results is shown above. In this part, it is not clear whether the number of data is the same as in the low count simulation, or if the data are ""downsampled"" or not. Moreover, the PSF of the high count simulation is different from the low count one, and not specified for the robustness analysis, whereas the scanner is always the same, which defines a unique PSF. All of this should be clarified."
520	Transformer based feature fusion for left ventricle segmentation in 4D flow MRI	Overall a pretty good paper, I am not sure what to criticize ;-)	Minor concern: since this is a relatively recent application, the work has little related work for comparison. Moreover, the compared methods are more considered baselines, hence, the evaluation against SOTA is not available.	The writing is not rigorous enough in some sections of this paper.
521	Transformer based multiple instance learning for weakly supervised histopathology image segmentation	"-The paper does not present a significant novel concept but rather combines existing approaches in an efficient manner to the given task. It builds upon the Swin Transformer and deep supervision for the side outputs. The latter has already been used for the CNN-based segmentation approaches. Addressing the correlation of MIL instances was the theme of a few recent papers, e.g., the TransMIL method. However, since they are mostly addressing the classification task rather than segmentation, I still believe the minor contribution is valuable.
-Since there is no separate validation set, I wonder how the authors determine optimal values for the hyperparameters, e.g., the parameter r of the generalized mean function and the weights of the three side-output layers. It is also unclear how much performance improvements are due to hyper-parameter tuning."	"The introduction of Swin Transformer to pathological image segmentation is novel, which considers the relations between instances in MIL. However, the paper lack further explanation or visualization about how Swin Transformer model the relations and benefit segmentation.
Deep supervision is also an existing method in reference paper.
Most parts of the method is described in detail. However, some details are still unclear and need to be further clarified.
The method is compared with some MIL methods, but only one FSL method, U-Net. Also, the experiments are conducted on a single dataset with 2 metrics. It might be better to provide experiments on more datasets, such as camelyon16, with more metrics like dice coefficient."	"1.The use of deep supervision to better use image-level annotations and strengthen constraints is already prevalent in other works and cannot be described as a contribution proposed in this paper.
2.The interpretability of the fusion strategy is insufficient and the exact fusion of the side-outputs is unknown,which means the definition of is not clear. In other words, the features of the side-output of each layers are different, which means that different fusion strategies bring different effect.
3.There is an ambiguity between the visualization results and the experimental results in this paper. The visualization results of UNET are very close to the results obtained by Swin-MIL , and it seems that the results of the Swin Transformer Based MIL outperform those of the UNET, which is inconsistent with the comparison of the performance on F1-scores and Hausdorff Distance between them."
522	Transformer Based Multi-task Deep Learning with Intravoxel Incoherent Motion Model Fitting for Microvascular Invasion Prediction of Hepatocellular Carcinoma	Maybe, the most significant problem could be the very reduced number of samples. 114 HCC are a really small dataset and I cannot be sure of the performance of this proposal with a dataset of proper dimension for this kind of technique.	"This paper is an incremental improvement of the method described  in [21]. The additional aspects are not clearly stated in the paper which makes it difficult to assess its novelty.
The paper is also not very well written and not easy to read, follow. It would benefit from a careful English proof reading."	"Not appropriately justify the methodological motivation of the work
No IVIM parameter maps have been shown."
523	Transformer Based Multi-View Network for Mammographic Image Classification	"1.Literature research is insufficient. The novelty of the proposed method needs to be explained.
2.The network process description is not clear enough. How do the three sub-networks evaluate the results as a whole?
3.In the experimental part, the method in this paper should be compared with the methods mentioned in the introduction to demonstrate the effectiveness of the method.
4.Figure 2 does not have acceptable image quality.
5.The English expression of the article is very poor."	"Key details are not explained well. For example, this paper does not provide details (e.g., equations or figures) for the proposed shifted window-based cross-view attention block.
Ablation study. This paper does not provide the experimental results of Swin-T with feature concatenation. Therefore, we are unable to directly compare the proposed cross-view attention with the simple concatenation. As a result, we are unable to know how much improvement does the proposed cross-view attention bring.
On page 5, there are some issues with the fonts. For example, Q, K, V, \alpha, \beta in the last paragraph."	It would be interesting to compare the performance on other tasks as well, such as lesion detection tasks.
524	Transformer Lesion Tracker	"The registration benchmark is somewhat out-dated and some new reference could be added. The lesion tracking is similar to video motion tracking, and the searching/template image pairs is similar to 2 frames in the video. Some references in video motion tracking/optical flow: 
Teed, Zachary, and Jia Deng. ""Raft: Recurrent all-pairs field transforms for optical flow."" European conference on computer vision. Springer, Cham, 2020.
Qin, Chen, et al. ""Joint learning of motion estimation and segmentation for cardiac MR image sequences."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2018.

Motivation/results mismatch. It seems the largest gain is achieved from SSS, but it seems the motivation for SSS is reducing computation cost. Further discussion why SSS improve performance is appreciated.

Some reasoning is missing. It is not clear why affine transformation is used. For lesion deformation, it is more like a non-rigid registration task. Adding reasoning on why affine is used is appreciated."	"The paper misses a discussion about weaknesses or limitations of the presented work and give
more insights into the method (e.g. for which lesion types does the method work well or in which cases does the method fail)
Overall, in my opinion, there is an imbalance between the sections. The introduction and related work section are relatively long, but the results are only briefly presented and not discussed at all.
some information is missing: The output is the predicted center coordinate and a classification result. However,
after reading the paper, I don't know what is classified (maybe I missed it?!)
some related works are missing: 
  Hering, A., Peisen, F., Amaral, T., Gatidis, S., Eigentler, T., Othman, A., & Moltz, J. H. (2021, August). Whole-Body Soft-Tissue Lesion Tracking and Segmentation in Longitudinal CT Imaging Studies. In Medical Imaging with Deep Learning (pp. 312-326). PMLR.
 Moltz, J. H., D'Anastasi, M., Kiessling, A., Pinto dos Santos, D., Schulke, C., & Peitgen, H. O. (2012). Workflow-centred evaluation of an automatic lesion tracking software for chemotherapy monitoring by CT. European radiology, 22(12), 2759-2767."	
525	Transforming the Interactive Segmentation for Medical Imaging	"The descriptions of the method is not specific enough and sometimes confusing, see detailed & constructive comments"" below."	"-The paper defines a method with only tests on challenging datasets Users did not try their method on real applications or in collaboration or supported by medical personnel of a final user.
-The assumption of clicks on working progress is not new and other studies have used them before with simpler method. 
-Perhaps the authors could elaborate why their results are significant against previous methods."	I  am missing a technical aspect. I fail to understand why the label assignment brings such an addition to the click encoding only, and why click encoding only  perform so poorly. As I understand the label assignment includes the click encoding step. However based on the results illustrated in table 2, label assignment leads to a stunning +8% dice in cancer segmentation compared to click encoding only, while results using click encoding do not seem to benefit at all of increased number of clicks. (-1% Dice between 5 and 10 clicks). Maybe this balance between the two steps should have been clarified better for non proficient readers.
526	TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation with Transformers	"The average runtime for each result, or estimated energy cost is missing. 
The motivation for choosing transformers based architecture for the task is not well elaborated. Since the model is evaluated on cardiac MRI only, it is not appropriate to claim that it is a medical image segmentation model. Accordingly, the title and text should be updated and made specific."	"Following are the weakness:

It would be great, if the authors would provide the computation details like the training time of all the methods use to compare with TranFusion.
Different types of U-Net (TranU-Net or Res-U-Net or vanilla U-Net) were compared. It would be nice if it is compared with U-Net having attention block. Though, Trans U-Net has the transformer component but  still it would be interesting to see the comparision with Attention U-Net."	"The Fig.2 misses some detailed explanations, and it is hard to understand.
The proposed DiFA and MSA modules seems to be the increment from the single input to the multi-view inputs."
527	TranSQ: Transformer-based Semantic Query for Medical Report Generation	"Failure modes of the approach have not been discussed.
Current NLP metrics (e.g. ROUGE-L and BLEU-1) will fail when there is finding uncertainty or absence. Perhaps using CIDEr is a better metric to compare against to detect the presence of abnormalities (Pino 2022 AIIM-D, SSRN).
Grammatical mistakes are seen in the paper (e.g. Sec 2.3, paragraph 1 - don't start a sentence with a conjunction like 'and'). But, overall the paper is clear and easy to follow."	"The use of this part based approach leads immediately to questions about overall ordering of the sentences in the result.  The method for ordering is defined, but does this affect the score?
Some of the example sentences in Fig 2 seem to refer to priors e.g. ""stable"" ""have increased""- but there are no priors fed to the system, so the system has picked up this language from the training data.  Would be good to discuss."	"Please see the Q8 for details.

Some very important analyses are missing, e.g., the ablation study.
Some important implementation details are missing."
528	Trichomonas Vaginalis Segmentation in Microscope Images	"There lack experiments on public datasets, which limits the reproducibility of the proposed method.

The HRF module is similar to the channel and spatial wise attention mechanisms, which have been proposed in the following publications:

L. Chen, et al, ""SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning"", CVPR 2017, pp5660-5667.
S. Woo, et al, ""CBAM: Convolutional Block Attention Module"", ECCV 2018, pp3-19.

The paper lacks computational complexity analysis on the proposed method in comparison with other image segmentation methods."	"References to previous work on Trichomonas Vaginalis segmentation/detection are missing. For instance, check ""Wang, X., Du, X., Liu, L., Ni, G., Zhang, J., Liu, J. and Liu, Y., 2021. Trichomonas vaginalis Detection Using Two Convolutional Neural Networks with Encoder-Decoder Architecture. Applied Sciences, 11(6), p.2738."" and its corresponding dataset: https://github.com/wxz92/Trichomonas-Vaginalis-Detection
In the comparison with the state of the art, the number of execution trials and hyperparameter exploration is unclear.
In the ablation study, the Dice and IoU values are missing."	"The description of the segmentation method should be improved (see ""detailed & constructive comments"" along with some minor issues)."
529	UASSR:Unsupervised Arbitrary Scale Super-resolution Reconstruction of Single Anisotropic 3D images via Disentangled Representation Learning?	"It is unclear in the experiment what ""507 MR images"" mean. These are 507 slices of the same exam? Or 507 exams of different knees? The same for the spine.
The quantitative assessment is input dependent. It is uncertain if the improvements will repeat with other data. The differences in the comparison with SOTA methods are small.
The qualitative evaluation is limited to visualizing one sample image and only in the authors' opinion. There is no independent assessment by a population of experts. It is also uncertain how the method will perform on natively low resolution images or on increasing resolution of natively high resolution images.
The fusion part is unclear. Interpolation and SR can be applied in different orders to obtain arguably different results. It seems that for lack of space the authors did not detail that part.
The paragraph ""ablation study"" does not make sense for me. I do not see ablation there as my perception is that fusion is an extension and not part of the method. It is unclear in table 2 how the metrics are applied to ""with fusion"" and what ""arbitrary"" means in that context.
Limitations and applicability are not discussed."	Some of the experiment details are not clearly shown, such as the train/val/test split for the datasets that are used in the experiment is not clearly presented. Since the inhouse dataset of spine CT scans is a new dataset, we can better assess the model better and apply other methods on this dataset if train/val/test splits are shown in the paper.	"Simply modeling resolution space as a Guassian distribution seems rather simple.
The big picture Fig. 1 could be improved with more information.
The comparison between the proposed and SMORE should be discussed."
530	ULTRA: Uncertainty-aware Label Distribution Learning for Breast Tumor Cellularity Assessment	Learning the distributions can be time consuming compared to single value labels.	"The main weakness of this paper is the limited novelty of the method. The implementation of ULTRA is very similar to the method proposed by Tang et al. [1]: (1) regarding the regression problem as a label distribution problem; (2) utilize normal distribution to model the uncertainty. 
It feels like that this article has just slightly changed [1] and used it on the TC score estimation task.
[1] Tang, Y., Ni, Z., Zhou, J., Zhang, D., Lu, J., Wu, Y., Zhou, J.: Uncertainty- aware score distribution learning for action quality assessment. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. (2020) 9839-9848"	"statistical analysis is rather weak for evaluating generalization
important parameters (sharpness of the Gaussian distribution, importance of branches, relative loss of KL and MSE losses)  are set empirically and their influences on the outcome are not discussed or investigated.
the rationale for multiple branch is rather weak: augmented version do not really reproduce the kind of variability that would change the prediction of an expert."
531	Uncertainty Aware Sampling Framework of Weak-Label Learning for Histology Image Classification	"The authors should compare their approach to other current state-of-the-art techniques, such as applying the Blue Ratio to extract only the most relevant patches from each WSI.
Apart from that, the amount of WSIs used is very small, and should be improved."	"-The novelty may be overstated. Uncertainty is already used for histopathology image segmentation, i.e. [1], 
-The dataset is small and incomplete. There are in total 85 WSIs used in the experiments. The 85 WSIs are categorized into 3 grades but the cancer-free slides, which are usually important for weakly supervised WSI analysis, are absent. This absence affects a lot to the models trained with noisy labels, i.e., the baseline methods.
-Lacks comparison with related works. There are quite a few studies in the domain aim to solve the problem of weakly supervised histopathology image classification, e.g. [2-4]. The goal of these studies is very similar to this paper but none of them is compared.
[1] Thiagarajan P, Khairnar P, Ghosh S. Explanation and Use of Uncertainty Obtained by Bayesian Neural Network Classifiers for Breast Histopathology Images. IEEE Transactions on Medical Imaging, 2021.
[2] Li J, Chen W, Huang X, et al. Hybrid Supervision Learning for Pathology Whole Slide Image Classification[C]//International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021: 309-318.
[3] Lerousseau M, Vakalopoulou M, Classe M, et al. Weakly supervised multiple instance learning histopathological tumor segmentation[C]//International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2020: 470-479.
[4] Lerousseau M, Classe M, Battistella E, et al. Weakly supervised pan-cancer segmentation tool[C]//International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021: 248-256."	No major weaknesses, only minor comments (see below).
532	Uncertainty-aware Cascade Network for Ultrasound Image Segmentation with Ambiguous Boundary	"The UAM idea is confusing (see detailed comments).
Some descriptions, figures, and equations need clearing up (see detailed comments).
The authors lack analysis of design ideas or experimental results."	"1) Some technical details are not clear. e.g., what is 'ultrasound feature' in Fig.2 and Sec.2.1?
2) There are no qualitative results to show the effectiveness of proposed method in boundary issues, especially in recurrent edge correction."	Some training details are missing, e.g., the number of layers and channels in the network.
533	Uncertainty-Guided Lung Nodule Segmentation with Feature-Aware Attention	"It is not clear what is the formulation of the Gabor and Otsu operators and how this impacts backprop.
It looks like the network is trained end-to-end with the loss L = LBCE (S, GTS ) + LUAM. is that correct?
The learning objective LUAM = LBCE((GT)',(GT)) + LBCE((GT)',(GT)) + LBCE(GTS' ,GTS) contains terms that are in contrast with each other, therefore a balance may be found by the optimisation algorithm which takes into account all the different GTs without necessarily modelling the effect of different ground truths. Of course, since the authors seem to use different prediction heads for the computation of the three components of LUAM, this effect might only be mild. In the supplementary materials the authors show ablation studies which seems to confirm that the presented version of the network is actually better. they mention two other versions V1 and V2, without really specifying their respective configuration. I suggest to at least mention the result of ablation studies of 1) FCN trained with LUAM without separate prediction heads, 2) FCN + UAM only and 3) FCN + UAM + FAAM
It is unclear to me how certain experiments are performed. For example, the standard UNet used for comparison is trained on single ground truths. It achieves different results on single annotations, intersection and union between annotations. These results are usually inferior compared to UGS-Net. But, was the normal UNet trained on unions and intersections when performing experiments relating to unions and intersections (resp)?"	"As described in Sec. 1, line 5, the LIDC-IDRI dataset provides multiple annotations. If a method conducts experiments on this dataset and only chooses a single annotation as the learning target, the method is not a conventional method or a traditional method, it is a wrong method. The motivation of this paper is confusing;
Figures in this paper are relatively small and are hard to see.
Nodules that are smaller than 3mm in the LIDC-IDRI dataset are commonly not used for experiments. It seems that the authors don't know the dataset enough;
There are many grammar and spelling errors. For example, in the Abstract, line 13, ""LIDC-IDRI"" should be ""the LIDC-IDRI""."	"""UGS-Net's input is the lung nodule CT images, and its final learning target keeps consistent with the current mainstream methods, which is a single annotation GTs."" While the multiple annotations are provided, how to choose which annotation will be conducted in the uncertainty aware module and the final output?
As the intersection and union provided, did the author consider calculating the average of the GTs, as the intensity of the average mask may include the agreement probability of the nodule region?
No comparison with other uncertainty based methods ref[ 8- 10].
In table 1, how to implement the baseline U-Net with multiple annotations? 
Minor:
Some fonts in Fig. 1(B), Fig. 2, Fig. 4(B) are too small to read"
534	Undersampled MRI Reconstruction with Side Information-Guided Normalisation	One major weakness is that the evalution of the methods should include comparison with other training mechanisms.	The author should mention the ways of how to insert SIGN in other backbones.	"If the reconstruction network has enough capacity, it should be able to model the heterogeneity in the dataset
Possible redundancy in the selected side information"
535	UNeXt: MLP-based Rapid Medical Image Segmentation Network	"The method is not sufficiently or clearly described. I had to search for UNext code online to understand it.
There is no comparison to, or even mention of, other works that reduce the computational complexity of UNet-like models."	"The authors state that ""It is worthy to note that we experimented with MLP-
Mixer as encoder and a normal convolutional decoder. The performance was not
optimal for segmentation and it was still heavy with around 11 M parameters."" Although this work is not a segmentation based work, there is enough similarity that the authors felt it justified to add an entire paragraph ""Difference from MLP-Mixer"". Given that, the authors should include these results in the main table rather than a vague statement that its ""performance was not optimal.""

It would be worthwhile to move the two additional dataset experiments from the supplementary to the main paper. The introduction can be cut down some, there is a bit more space spent on motivation than is really necessary."	"Some similar ideas of adopting MLP-based networks are recently proposed under heated discussion, thus, the unique novelty or technical contribution (made in this work) should be strengthened and emphasised.

Comparison with the counterparts from MobilNet family is missing, which is an important milestone established in the domain of edge/mobile computing. Adding a comparison with them, in terms of both efficiency and effectiveness, would be more convincing. (Some available resource and hope it helps: https://coral.ai/models/semantic-segmentation/)

Cross-validation (i.e., 3-fold, 5-fold or etc) is suggested. The current experimental setup is specified as ""We perform a 80-20 random split thrice across the dataset and report the mean and variance"". However, the variance of the proposed method (within Tbl.1) looks a bit unstable among the others, especially when comparing with the competitive ones. There is no doubt regarding the efforts paid in reducing the computation burden, but it would be very useful and practical to the community if a statistically solid benchmark can be built.

Qualitative analysis on more popular medical image segmentation datasets is preferred, such as MoNuSeg. The qualitative analysis in Fig. 5 looks meaningless and relatively pale. Honestly, the claimed superiority of the proposed method cannot be told from a personal perspective, which brings me the idea that whether more popular and more competitive medical image segmentation datasets are suggested."
536	Uni4Eye: Unified 2D and 3D Self-supervised Pre-training via Masked Image Modeling Transformer for Ophthalmic Image Classification	-The dataset contains very heterogeneous data types . Although the authors perform different types of ablation studies, it is sometime hard to disentangle the impact of the different patterns of the heterogeneous datasets, eg does performance on some type of images (eg fundus from GAMMA) benefits only from the latent representations learned on additionitional images from the same type (eg fundus from EyePACS) or also from additional images of different patterns (eg 3D OCT). Might be interesting to add some ablation study training the SSL network on the different subtypes of images. (Similarly to what was done in Table 2 of the Appendix when considering a separate training on 2D or 3D data). At least, discussion should be enlarged to address this point.	"Slight lack of clarity in the evaluation: authors denote the downstream by the related challenge or dataset, which do not necessarily describe what is the actual classification task. The 3D results only report 1 comparison method (which is not state-of-the-art) whereas the 2D comparison methods may be easily expanded to 3D. Moreover the ablation studies results are reported each for 1 different downstream task (certainly for time / resources consideration), authors could explain further their choices. 
Although it is a proxy task for the self-pretraining of relevant features, the reconstruction task results (only qualitative) seem quite moderate on visualization on Fig.4, even with the lowest mask ratio (25%)."	"Some parts of the method are not well-explained.
The novelty of the method is limited. The method is based on masked autoencoder (MAE). The novel components are Unified Patch Embedding (UPE), which does not make complete sense based on their current description, and the dual-branch decoder, which is not quite novel."
537	Unified Embeddings of Structural and Functional Connectome via a Function-Constrained Structural Graph Variational Auto-Encoder	"-Presentation and Writing Quality: The quality of the paper needs to be improved: figures/ diagrams, and the overall presentation need significant modification before publication.
-The Experiments (datasets\ ablation experiment) of the proposed methods can be further validated.
-The discussion (weakness and limitation) of the proposed methods can be further revealed."	While the author claims that their model can establish a unified spatial coordinate system for comparing across different subjects, their analyses still rely much on the registration process in preprocessing stage. In addtion, the necessity of including an AE model is questionable and needs further clarification.	"Major Concerns:
1). Methodological validation
The reviewers are curious about validating proposed methods with other peer deep neural networks.
2). More details of the sampling technique in this paper need to be included.
3). More explanation that orthogonality cannot allow for better compression is needed."
538	Unsupervised Contrastive Learning of Image Representations from Ultrasound Videos with Hard Negative Mining	"The sampling of positive and negative frames involves many hyper-parameters, which may have a significant impact on the results. The paper should explain how the parameters are selected.

The proposed method resembling the classical idea of CPC[1] and DPC[2], which should be added in the related work.

Some details: the format is not allowed, as the authors have used  a lot \vspace and parallel table.

[1] Contrastive predictive coding for video representation 
[2] Video Representation Learning by Dense Predictive Coding"	"My major concern is regarding the generalizibility of the algorithm. It is not clear to me whether the method can work in other medical modalities like 3D CT images. I will suggest to refer to the paper [1] for more insights on this problem.
[1] Contrastive learning of global and local features for medical image segmentation with limited annotations, NeurIPS 2021."	"The Butterfly video dataset only contains 1533 images, which is small for contrastive learning.
Some details are missing. e.g. what is the size of the memory queue."
539	Unsupervised Cross-Disease Domain Adaptation by Lesion Scale Matching	"1) This method is proposed based on existing unsupervised domain adaptation
By adding the lesion scale matching the performance can be improved. However, the authors did not discuss the scenario when the knowledge of target data is not available. 
2) The authors need to provide a detailed ""algorithm"" in a table to show the whole process.
3) The authors haven't compared with existing ""batch normalization"" based method."	Perhaps the choice of ResNet50 needs to be motivated	"However, I have to point out, the scale mismatching issue is a typical problem in the computer vision community.  Namely,  the classification /regression are based on samples from the bounding box.  Namely,  we will not use a city view street image for car classification but the car image patch crop instead.
It is also a standard pipeline in most of the data augmentation pipeline to introduce the randomness in scale which is the latent variable z in this work.
Though it shows nice theoretical insight about the random scale (LS) here from prior perspective and employs monte carlo EM.
It at least missing a strong baseline based on the conventional data augmentation pipeline to random scale z instead using a fixed  z in the ablation study."
540	Unsupervised Cross-Domain Feature Extraction for Single Blood Cell Image Classification	"There is no comparison with state of the art methods. 
The authors does not report any insights on the computational requirements. 
The motivation of the pipeline is not clear. Why choose what they choose (for the different components of the model. 
Mask R CNN is a semantic segmentation model, not a detection model."	"Introduction and motivations to the work: There is a great deal of nuance involved in haematological classification. The authors chose as a use case a very narrow and highly simplified problem of distinguishing white blood cell types on images containing individual white blood cells. Although the results obtained are of considerable interest and realistically applicable on further domains, ideally I believe the introduction lacks detail and characterisation of the problem at hand.
For example, what is the rationale for classification on images consisting of only white blood cells? To cite an example (please note that this is not a request for citation, if the authors do not consider it necessary): in some works, such as this one (https://www.mdpi.com/2076-3417/12/7/3269#) recently, it has been discussed how reliable CAD systems based on CNNs are for the analysis of globules on ""whole"" images, i.e. composed of a multitude of cells, and how it is basically impossible to have a reliable diagnosis on the basis of a direct classification carried out with CNNs.
Therefore, I would suggest to the authors to improve the introduction so that it provides a deep overview of the study.
In fact, I think it is unclear what unique challenges are associated with this task.
In Section 2: from the figure, it seems that the Mask R-CNN training was performed on the three dataset merged. This aspect is not stated in the text. Could the authors be more precise in this sense?
Section 2 again: why do you call it 'reconstruction'? It sounds more like image generation to me. If it were a reconstruction, I would expect to see the WBC clean and not something ""polished"".
Still Section 2: ""In our experiments GN was effective in image generalization."" How and why?
Section 2.1: What is the anchor dataset D0? Please explain.
Section 3.2: why was the constant b in equation 3 set to 5?
Section 3.2: the training procedure is not entirely clear to me. Did you train on the 80% of the original images? Or on the ""reconstructed"" ones?
In reference to Table 1, the authors state ""AE RF: random forest classification of features extracted by a similar autoencoder trained on all datasets with no domain adaptation."". However, I think it should be useful for reader to give, even if brief, a detail about the similar autoencoder, also considering it reached best results in same-dataset experiments.
With reference to the classification method used, Random Forest, I think the authors should better motivate his choice (Note: this is not a criticism of the method itself, just a request for clarification)"	"Major:

The baselines are not strong enough. In my opinion, the main contribution of this work is in the domain adaptation area, where no label from the target dataset is given. Also this is the place where the proposed method shows better performance (Train on one dataset, and directly test on another without fineturning). This method should be compared with domain adaptation method. But we cannot find this kind of the baselines in the experiments.
Inconsistant description for the ResNet-RF method. In Table 1, it seems it is trained on different datasets in each experiment. If so, where does the classification label come from? And in the main text, it said ""ResNet RF: random forest classification of the features extracted with a ResNet101 [9] architecture trained on ImageNet dataset"". The training set is inconsistent here.
Minor:
The application of the proposed method is limited as the accuracy on the cross domain seems far way from training supervisedly."
541	Unsupervised Deep Non-Rigid Alignment by Low-Rank Loss and Multi-Input Attention	The main weakness of this work is that the authors didn't give a good description of their methods, the whole method part is difficult to follow. In addition, the application of this method may be limited.	"The discussion in many places seems not objective enough and lacks supporting materials, such as the authors claim it is the first low-rank loss for alignment, but it seems that some articles for registration already exist: A low-rank representation for unsupervised registration of medical images."" arXiv preprint arXiv:2105.09548 (2021); although it is not stated in terms of loss. The author uses multi-attention to complement the sparse corruption, what is the motivation between attention and sparse.
And the authors claim the deep-learning-based methods can not have a denoise
function, but for the learning of high-frequency information (noises) and low-frequency information, NN can reduce noise very well: Deep image prior, CVPR2018; Training deep learning based denoisers without ground truth data. NeurIPS2018.
The experiments can not effectively illustrate their method, I will list it in the comments.
A more clear outline of the next steps in research would be appropriate."	"There are some curious points for evaluation and comparison.
Main concern is that whethere the comparison was fair."
542	Unsupervised Deformable Image Registration with Absent Correspondences in Pre-operative and Post-Recurrence Brain Tumor MRI Scans	"1 1 The proposed method lacks novelty. It seems to be a combination of a few ""tricks"" from existing works. The forward-backward consistency is a widely used optical flow technique [19,21,28,29]. The inverse consistency is also a well-known constraint in the registration community.
2 Unfair comparison and incremental performance improvements. 1) Methods used in the comparisons are not designed for registering images with absent correspondence. MICCAI has a community dedicated to methodologies for pre-op and intra-op brain tumor images registration (CURIOUS'18, TMI), the authors should compare the proposed method to the best-performing methods in the CURIOUS challenge. 2) To the best of my knowledge, in Image-guided neurosurgeries, TRE (for landmarks near tumors) within 2mm is considered good. The TRE achieved by the proposed method is larger than 3mm, which might be too large to be clinically acceptable.
3 Lacks discussion potential. The proposed method doesn't show clear innovations and contributions over the state-of-the-art methodologies. I'm doubtful that the MICCAI audience would be interested in this work."	The quality of estimated regions with absent correspondence could be evaluated in more detail, providing evidence for the proposition 'The results demonstrate our method is capable of accurately locating the regions without valid correspondence': How do the masks compare to automatically computed masks for pathological regions (that were used for '-CM' baseline methods)? Can false-positive regions sometimes be a problem?	It would be helpful to discuss the extraction of accurate masks in the proposed unsupervised learning scheme, considering the smooth regularization of registration fields. There lack some descriptions of the regularization term (Eq. 7) and mask threshold.
543	Unsupervised Domain Adaptation with Contrastive Learning for OCT Segmentation	"As said in the paper, the final pairing strategy is first generate P_slice and then apply augmentation on them.
Actually, such operation will miss the original P_augm which only does the augmentation on the same slice.
The results under P_augm + P_comb would be interesting.

Actually, the improvement of updated Projection head did not bring too much improvement from 27.21 to 27.77 in Table 1.

""In Table 1 and Table S2, UpperBound results for a supervised model trained on labeled data from the target domain are also reported for comparison. This labeled data, used here as a reference, is ablated for all other models."" According to the description, the dice performance of SegCLR(Pcomb ,Cch ) should be 29.32 + 27.77=57.09, right? Isi it too low for a segmentation task?

Only one dataset is utilized for testing. The generalization ability of the proposed may need to be verified on other datasets.

Missing comparison with other unsupervised domain adaptation methods."	"some formulas are not clear
validation compares only with contrastive learning methods"	"-In Table 1, only the scores relative to the baseline are given. This is confusing, as the reader has no idea how good or bad the performance of the baseline method is. In the text, it is stated that the baseline method has a poor performance on D_t, but this performance is shown nowhere in the main paper. Looking it up in the supplementary for each class is a bit cumbersome.
-It is unclear whether this approach is done for the 3D volumes or only 2D slices. In the abstract and introduction, it is stated that this is a 3D segmentation. However, the pairing is performed on the slices, and the cited U-Net is also originally implemented in 2D. Is the segmentation performance then computed in 2D or 3D?
-In Figure 3, the relative scores are reported, which is somehow confusing in my perception. It would be nice to compare the real performance on D_s and D_t, instead of a relative loss of performance. 
-In Section 3.4, the authors state that their method with unsupervised domain
adaptation remarkably produces results within the ranges of experts' variability. However, I do not see that in Figure 4. If the authors want to make this statement, they should explain more about this in Section 3.4.

The proposed projection head C should be included in Figure 1.
-Some comment on the model complexity (e.g. number of parameters) should be given.
-The authors only compared themselves against similar contrastive learning approaches. It would be interesting to see the performance for other UDA methods such as disentanglement- or adversarial-based methods."
544	Unsupervised Domain Adaptive Fundus Image Segmentation with Category-level Regularization	"A concept akin to the prototype-guided discriminative loss was proposed previously, including transferable prototypical networks and subtype aware UDA approach, e.g.,
[1] Pan et al. ""Transferrable prototypical networks for unsupervised domain adaptation"" CVPR, 2019
[2] Liu et al., ""Subtype-aware Unsupervised Domain Adaptation for Medical Diagnosis,"" AAAI, 2021

It is unclear how to achieve edge localization."	"In section 2.1, what is segmentation model G? The segmentor? Eq (2) is just a simple Euclidean distance. Why not choose some domain adaptation distance functions, such as MMD and CORAL?

In source domain category regularization, margin delta is the key parameter. It is better to show how to set the best value of 0.01. What are the effects of different values?

There are too many loss functions in the training procedure. Without a detailed algorithm, it is difficult to know how to train the whole pipeline.

In terms of the results, the paper did not compare with SOTA methods [1-2]. Some results are significantly better than the source-free model [1], in which no source domain labels are used. In RIM dataset, 0.905 VS 0.908 [2]. Also, the results of the Drishti dataset are exactly the same as the results as in [2]. These performances should be further explained, at least it is not better than [2].

There are many parameters in the models. It is better to add parameters analysis section to show how to get these optimal values.

In the ablation study, there are many variants that are not compared.

L_{inter}
L_{dis}
L_{aug}
L_{inter} + L_{dis}
L_{inter} + L_{aug}
L_{dis}+ L_{aug}
Without these results, it is difficult to know the effectiveness of different loss functions on the model.

[1]. Chen, C., Liu, Q., Jin, Y., Dou, Q., & Heng, P. A. (2021, September). Source-Free Domain Adaptive Fundus Image Segmentation with Denoised Pseudo-Labeling. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 225-235). Springer, Cham.
[2]. Lei, H., Liu, W., Xie, H., Zhao, B., Yue, G., & Lei, B. (2021). Unsupervised domain adaptation based image synthesis and feature alignment for joint optic disc and cup segmentation. IEEE Journal of Biomedical and Health Informatics."	"Only a dice score is applied to evaluate the performance.
Target domain category regularization is similar to strong and weak augmentations in semi-supervised learning."
545	Unsupervised Lesion-Aware Transfer Learning for Diabetic Retinopathy Grading in Ultra-Wide-Field Fundus Photography	"The loss function for UWF lesion detection is quite weak. It is not very clear how accurate the lesion detections are (for UWF modality).
The design of  LEAM is not fully justified."	The paper does not talk much about the benefit of using UWF over CFP. Is there any specific study that authors can cite showing the at broader field of view of peripheral retinal pathology of UWF resulted in better diagnosis or detection of specific biomarker of DR ? This can be cited along with the citation 3.	"Most natural baseline (supervised UWF model with label) appears missing
Performance improvement appears marginal (and dependant on metric); it is unclear whether the contribution of the method would diminish with more (labeled) UWF data (currently, only 904 UWF images available, of which about half are normal)
No direct (if partial) evaluation of lesion segmentation for UWF data"
546	Unsupervised Nuclei Segmentation using Spatial Organization Priors	"Missing the description of the HE database.

Lack of comparison with other adversarial learning methods for nuclei segmentation[1][2].

[1] Liu, Dongnan, et al. ""Unsupervised instance segmentation in microscopy images via panoptic domain adaptation and task re-weighting."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020.
[2] Mahmood, Faisal, et al. ""Deep adversarial training for multi-organ nuclei segmentation in histopathology images."" IEEE transactions on medical imaging 39.11 (2019): 3257-3267."	"The method proposed in this paper may be better termed as unsupervised nuclei detection. It may be hard to make a reliable segmentation method. Segmentation is usually used to extract the shape information of the cells or nuclei. If the segmentation is not accurate enough, it could extract misleading information for diagnosis.
What are the training settings for the supervised and unsupervised in Table 1? Actually, the proposed method also makes use of the labeled data. What data and how many samples are used to train the supervised methods? What and how many labeled and unlabeled samples are used to train the proposed method? Is the proposed method using less labeled data than the Unet and Nuclick in Table 1?
Precision and Recall may not be good metrics separately. Why not computing F1 score based on the precision and recall?
How is the accuracy balanced?
How is the object level segmentation achieved using Unet?"	"impact of postprocessing included in proposed method is not discussed
lack of evaluation in relation to tissue with different compactness"
547	Unsupervised Representation Learning of Cingulate Cortical Folding Patterns	"Technical improvement is limited. Technical novelty can only be found in adding a decoder and proposing methods in the preprocessing steps.
Interpretation of the results is relatively shallow. Only description of the morphology of clustered folding patterns and whether they are present in the reports are found.
The choice of cluster number seems arbitrary.
There are no clear conclusion by comparing SimCLR and b-VAE. Also, its application in clinical scenarios is not very clear."	"The technique contribution is low. The authors seems to perform a contribution of two methods, in a special data form.
This paper seems to be more like exploring folding patterns, rather than developing a new method."	"Lack of validation experiments.
The description is not very clear, especially for the comparison of patterns discovered by different models.
Lack of a comprehensive comparison of cortical folding patterns discovered in this work and related papers."
548	Usable Region Estimate for Assessing Practical Usability of Medical Image Segmentation Models	"The authors mention the main strength of using their metric over assessing accuracy and usability independently is that they are combined in a single metric. However, the combined metric is dependent on several tunable parameters (i.e., the URE metric depends on the choice of a threshold and choice of statistical metric). Since the ranking of models may depend on the choices of these parameters, it may not be clear which is actually superior for a task.
The evaluation shows the relative performance of several models on different public datasets. This evaluation only shows that the ranking of these models changes based on the metric used, but does not demonstrate that the proposed metric's ranking is superior. An additional usability study with clinician feedback would be helpful here to show that the proposed metric is more aligned with how users evaluate model output."	"(1) The advantage of CCRC over ECE is not clearly justified;
(2) The limitations of URE is not identified and discussed;"	"No comparison with other model evaluation methods
No justification on why these new index should perform better than existing ones"
549	USG-Net: Deep Learning-based Ultrasound Scanning-Guide for an Orthopedic Sonographer	"The main weakness of the work is lack of clarity in some areas. Some information needed for better understanding the method is given later in the paper, and at least one additional figure explaining some of the definitions (e.g. directions, tangential plane, etc) is needed.
Also, there is no comparison in this work (with similar methods or clinical evaluation)"	"A formal mathematical formulation of the problem is recommended. The paper lacks the basic formulation, starting from the detailed dataset.
More comparison with other methods is recommended, only the proposed method and its variants are compared.
Training epoch in ablation studies can be more in the experiments parts."	"Clarity: Some sections of the work are unclear, particularly in the Methodology where some variables have not been defined and hyperparameters are not stated.
Novelty: As acknowledged in the literature review provided in the Introduction, the idea of providing US probe movement directions to sonographers is not novel and has been explored in other applications. Although this is acknowledged, the limitations of the existing work should be described further to justify the unmet need addressed by this contribution."
550	Using Guided Self-Attention with Local Information for Polyp Segmentation	Some rewriting process may be need to improve readability. Also, there is no explanation or presentation of polyp segmentation from the medical aspects. How about flat polyp detection rate? How about concave polyp? The paper is heavily depends on public dataset.	"(1) Some details are missing in the paper, e.g. how does the transformer decoder work and how is the attention map in figure 1 generated?
(2) Lack of comparison in terms of model size and FLOPs;"	Limitations of the approach are not presented, nor are failure cases. Moreover, no statistical analyses have been performed, and the variance of results is also missing. This makes me skeptical of the reported results and the superiority of the presented methodology, although the differences in mean metrics indicate that this might not be a concern.
551	USPoint: Self-Supervised Interest Point Detection and Description for Ultrasound-Probe Motion Estimation during Fine-Adjustment Standard Fetal Plane Finding	"The proposed method might not be able to be used in the actual fetus standard plane finding. For this method to work, the image of the standard plane of the scanned subject will need to be acquired in advance. This method is simply a pose estimation method between two ultrasound images and the models do not have a conception of what a standard plane image should look like in the learned weights.
There seem to be some fundamental issues with the design of the method. In the motion estimation module, from my understanding, the authors want to first lift the matched 2D pixel pairs to 3D Euclidean space so that a closed-form point cloud registration method based on singular value decomposition can be used to obtain the 3D relative pose. However, the authors seem to use a learning-based network ""Transform Net"" to make the conversion from 2D pixel locations to 3D Euclidean locations. Wouldn't there be a non-learnable way to directly obtain the 3D location of the pixels with respect to the imaging source based on the imaging mechanism of ultrasound? In addition, what the authors present here is to estimate the motion of the point where IMU was installed during data collection instead of the actual ultrasound probe. Should there be a calibration process to bridge this gap? In addition, both problems above may suggest that, with the current design, the learned models may not be able to generalize across different types of ultrasound probes.
The novelty is relatively limited. As the authors pointed out, the design of the local detector and descriptor and the pre-training method follow the design in citation 3. The graph matching method to find feature matching is based on the work named superglue. There are also methods discussing how to handle singular value decomposition differentiably. The main technical novelty seems to be combining these modules into an end-to-end pipeline for the task of relative pose estimation between two ultrasound images."	"The main criticism is that this paper has too much content for a short conference paper - this leads to some parts being not very clearly explained.
Although the keypoint detection part doesn't need any ground truth, you do ultimately use ground truth IMU or simulation based data, to indirectly train it right? Could you comment on how you would make it fully self-supervised? If you apply the transformation predicted on the source image, and calculate loss with the target image, and backpropagate that, would it work?
It is clear that you're solving for subtle linear probe motion. What happens if there's a large motion and/or there are other compounding non-linear motion such as breathing, cardiac motion, etc?
Won't SVD be an expensive operation to carry out each time? How long does training your algorithm take in the hardware you describe (which seems quite good)?
How reliable are IMU signals? They are subject to some noise/drift issues too right?"	"The term self-supervised for this work is misleading, I think the method is unsupervised not self-supervised. Unsupervised is used when the network is trained by comparing the first image and the second image whereas, Self-supervised term in motion estimation is used when the network corrects by comparing with its own prediction in a teacher-student fashion.
The loss function is poorly presented, the authors mentioned groundtruth in loss function equation while, as authors mentioned the training does not require any annotations.
The authors violated the rule that there shouldent be any text in supplimentary materials."
552	Vector Quantisation for Robust Segmentation	"While the paper is successful in justifying the robustness claim, it is still not clear why there is an increase in performance from the baseline U-Net on a single domain? The paper says in the CONCLUSION section that a potential reason is codebook behaving as an atlas in the latent space for segmentation of anatomical parts(which are very structured), but unfortunately no experiments/visualization has been performed to claim this. The paper also says that because anatomy is structured, the quantization bottleneck helps in capturing this structure by limiting the latent space through discretization. The reasoning doesn't seem to be plausible, with no visualization of the codebook to support it.

Working only on basic U-Net architecture doesn't prove its applicability on other sophisticated segmentation architectures.

No results on datasets where the segmentation region is small which limits the evaluation of the technique. For instance, I would like to see some results on the ACDC dataset (Automated Cardiac Disease Diagnosis) where the number of pixels of interest are less as compared to background.

The method particularly use Group Normalization and Swish Activation which is different from basic traditional U-Net architecture. I would like to know if the success of the method is contingent on using Group normalization and Swish Activation?"	"If the paper aims to claim that vector quantisation could be effective in the segmentation task universally, the author should validate this on a more general platform, such as nnUNet. Additionally, the author should try at least one more architecture except for U-Net to approve the generalization.
As VQ-VAE has claimed, the discrete representations fit for discrete latent variables, like planning and language tasks. However, the author claims that the VQ is also an
The author claims that the network could be robust by minimizing Ph(x+e)-Ph(x), this assumption could be true for the noise perturbations. However, for domain shift, the problem should become Ph(f(x))-Ph(x) instead of simply adding a small value of e. The function f() can range from a renormalization function to a non linear mapping, like in the paper shown in [1] [2].
There are many other works for solving the domain shift problem in medical images, such as [3][4]. While the author only compares VQ-Net against U-Net. So what is the advantage of VQ against domain adaptation methods?
In the task of the domain shift experiment for lung segmentation, as shown in Figure 1, I didn't see there is an obvious difference between NIH and JRST. And in table 2, the segmentation performance improves from JRST to NIH encounting domain shift. Therefore this experiment is not convincing enough to prove the effectiveness of VQ for domain shift.
Reference:
[1] Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation
[2] Synergistic Image and Feature Adaptation: Towards Cross-Modality Domain Adaptation for Medical Image Segmentation
[3] The domain shift problem of medical image segmentation and vendor-adaptation by Unet-GAN
[4] A Closer Look at Domain Shift for Deep Learning in Histopathology"	"Use of UNet as a baseline (from 2015) instead of a more updated approach, such as nnUNet or TransUNet.
I found Section 3.1 a bit confusing."
553	Video-based Surgical Skills Assessment using Long term Tool Tracking	It seems unfair to compare the proposed method with the GOALS assessment tool (or at least arguing that it can replace GOALS). The reason is that in GOALS one gets continuous value, representing the assessors' feedback, but the proposed method only considers two classes. In this context, from a trainee's prescriptive, GOALS gives more useful information compared with the proposed method.	"From my point of view, the main drawback of this paper is the lack of comparison of its results with other previous works that could be highly related with this topic. The authors used bytetrack such as baseline for performance comparison but I will suggest to include some others:
doi: 10.1007/s10916-020-1525-9
doi: 10.1109/WACV.2018.00081
doi: 10.1007/s11548-016-1388-1
Additionally, the main contributions (cost function and track recovery) have not been discussed or compared with other approaches of the previous studies.
Finally, I cannot clearly see the novel contribution stated by authors ""classify skill directly from the tool tracking"" in the discussion section. Maybe, I missed something but for me 'classification"" implies to group these skills in any way (novice, intermediate, experts... or something similar). As far as I read the paper I cannot directly understand that the algorithm can provide these groups."	"The evaluation dataset is very small and limits the reliability of the obtained results.
The rationales for architectural choices are omitted.
Ablation studies is needed to justify lots of modeling in this paper."
554	Vision-Language Contrastive Learning Approach to Robust Automatic Placenta Analysis Using Photographic Images	"The comparied baselines is not enough and strong, i.e., the resnet50.
No experiments on public datasets.  It would weaken the confidence in their improvements on the tasks of placenta image analysis."	"The new loss function is not adequately motivated. For example, why not simply use something like
x
instead of log(cosh(x))?

There were no statistical tests performed to ascertain if the proposed model is significantly better than others. Also no confidence intervals are presented.
The writing could be improved. There are a lot of notational inconsistencies as well as logical gaps (see detailed comments below)"	"The results table only reports one absolute result for each method, and not an average over several runs, or a k-fold training. Making it difficult to evaluate the robustness of the method to initialization/ augmentations used during training.

Details and discussion of qualitative prediction results are lacking.

There is a baseline missing: Clinical text reports only.

It would have been interesting to show what are the more important areas for the classification using a saliency method such as Grad-cam (despite the well-known issues of it)."
555	Visual deep learning-based explanation for neuritic plaques segmentation in Alzheimer's Disease using weakly annotated whole slide histopathological images	"No major contribution in the method of the study but the application is interesting.
The dataset size is small (8 whole slide images).
The dataset is not publicly available so the merit of the method cannot be established and compared to any state of the art method.
Quantitative analysis of different factors  like different modalities, context effect and attention maps are missing. A table showing the contribution of each of the factors will be helpful."	I am concerned for the technical novelty because the proposed method uses attention UNet for segmentation. Perhaps emphasizing more on why this problem is important and why others have not done this would highlight this work.	"I could not see any technical novelty in terms of the approaches and methods used even though the methods used are solid and well accepted within the community;
A bit more of explainability exploration could have helped to make the paper stronger, what exactly the model learned."
556	Visual explanations for the detection of diabetic retinopathy from retinal fundus images	From controlling the trade-off between adversarial and plain training schemes aspect, Equation 2 (adversarial training) and Equation 3 (ensembling) are similar. Therefore, the authors should provide more argument with more detailed discussion on why Equation 3 is effective.	"The work lacks novelty. The only novelty element of this work is the use of an ensemble model.
The two saliency map techniques used in the experimental setting are two of the most simple ones (Guided-backprop and Integrated Gradients).
Introducing an ensemble increases complexity and makes the model less interpretable.
It is not clear how the saliency maps were computed (as presented, the network has two branches, each one with the image as input, how were the gradients propagated?)
It is also not clear how the oversampling of the minority class was performed."	"The paper (correctly) notes that saliency maps are used to help justify a decision made by a neural network model to a clinician. However, this paper only shows something similar to a saliency map for two cases (Fig 1 and Fig 2) in which the diagnosis is very visually apparent and the classification is correct, which is not clinically relevant. What would be clinically relevant are cases that are either non-apparent (with a correct prediction) or ones in which the prediction is incorrect. Without those, it is hard to tell if their technique has value to a diagnostic workflow.
From Figure 1, it is hard to see what the advantage of their VCE approach is compared to the other approaches. Qualitatively, it seems that Guided Back-Prop is more specific than the proposed method. From Table 2, it seems that both comparative methods on average outperform the proposed one, although that might not be stastistically significant. (The authors can easily check this with paired non-parametric tests such as signed-rank tests.)"
557	vMFNet: Compositionality Meets Domain-generalised Segmentation	"Experiments on both datasets were all conducted in 2D networks. It would be interesting to understand the effectiveness of vMF modeling in 3D contexts.
Tests of statistical significance are missing: for example, in Table 1, in SCGM 20% (Dice), the vMFNet achieved about 1.5% Dice improvement compared to DGNet but with a standard deviation of 8.8."	"The theoretical motivation of the paper is somewhat unclear. The concept of compositional components is defined in vague terms. In practice, the proposed method seems like a regular clustering prior on features based on a mixture model. The advantage of employing vMF kernels instead of Gaussian components as in GMM is unclear.

Some elements of the method could be better explained / motivated. For example, I am not sure that the definition of L_vMF achieves the desired goal of aligning mu_j to z_i (missing minus sign?). Also, I fail to see how the image can be reconstructed from such a low dimensional representation, or how this reconstruction can be useful.

The proposed method is similar in essence to have a auto-encoder loss on the features as in [35]. I believe this approach should be included as baseline in the experiments to demonstrate the advantage of the vMF compositional model."	My main concern is that I find a lack of sufficient explanation / intuition as to why the proposed method should help for domain generalization. Despite the additional loss function L_{rec} and L_{vMF}, and despite the additional decomposing-composing pipeline, I do not exactly see why a neural network trained on a source distribution will provide correct predictions on a shifted target distribution. Specifically, the feature extractor trained on the source domains may not necessarily extract similar features from a test image. Thus, the fixed kernels may not necessarily correct highlight different regions in different likelihood channels. Indeed, the fact that test-time-training improves performance shows that there are some errors that TTT can alleviate. Nevertheless, the proposed training seems to provide improved robustness even without TTT. Can the authors provide any pointers that can help me better understand this?
558	Vol2Flow: Segment 3D Volumes using a Sequence of Registration Flows	Many important method and experiment details are missing in the paper.	The actual applicability in real scenario worth an in-depth discussion. Additionally, the method is suffered from severe error propagation problem due to slice-wise registration.	"1) the displacement field between the neighboring slices may not exist.  For instance, Z-spacing of CT images could be 5mm, and the object boundary changes significantly from slice to slice.  it seems the algorithm most likely will fail in such cases
2) it unclear how the algorithm propagates the segmentation when the organ boundary is reached and it disappears from the next 2D slice. It seems to be another limitation. 
3) Comparisons are insufficient.  Optical flow algorithm chosen is very basic and old, its results looks artificially weak. A better deformable image registration method (based on Mutual Information) should be tried for a fair comparison. 
Another comparison lacking is to other semi-automatic segmentation methods, such as segmentation based on extreme points (e.g. in 3D) , at least a discussion about such methods would be nice."
559	Warm Start Active Learning with Proxy Labels & Selection via Semi-Supervised Fine-Tuning	The manuscript suffers from several weaknesses. First, the figures are small and have a poor resolution. Secondly, the experimental section does not compare the proposed approach against others in the literature. The authors could consider similar approaches that have been proposed for other applications and try them for 3D medical data.	"It is not clear what are the different settings compared in the experiments. Especially, how the variance and entropy methods are used together with semi-supervised method. In algorithm 1, line 6, 10, 12, there are three places where data needs to be selected and the comparison/ablation study setting is not clear.
""proxy ranking"" has also be introduced but the exact definition is not clear."	"It seems that from figure 2, the improvement / performance gain of semisupervised method with proxy is tied to choice of data and acquisition function. For example, the ProxyRank-Semi performed well above supervised method and other semi-supervised method (e.g. entropy) for Liver & Tumor but for Hepatic Vessel data, the ProxyRank-Semi came close to ProxyEnt and at the end of fourth round was well below it as well as ProxyRank-EntSemi

Limited discussions

From figure 3 (eg. 3C and 3F), the combination of gain of semi-supervised method with proxy ranking was seen more on Hepatic dataset than liver and tumor which could have been explored and explained better.
Semi-supervised added with proxy better only for early rounds (fig 3C)?"
560	WavTrans: Synergizing Wavelet and Cross-Attention Transformer for Multi-Contrast MRI Super-resolution	No weakness, but a minor suggestion, I hope the authors can add another ablation study to discuss the importance of K-space data consistency loss.	The paper indicates to use wavelet packet decomposition but it seems that it hasn't benefited from WPD but instead the simple wavelet transform. It needs to be clarified which one out of the two it was and why. It also used simple Haar wavelet, the choice of which wasn't justified and may need to be explored.	The transformer model is difficult to optimize, the proposed is implemented with 4 V100 GPUs, which is unpalatable for clinical application.
561	Weakly Supervised MR-TRUS Image Synthesis for Brachytherapy of Prostate Cancer	"Qualitative results show that the proposed method still has a lot of room for improvement.
Weakly supervised learning is emphasized in the title and contribution of the paper, but there is too little description about weakly supervised learning in the main body of the paper.
The authors did not conduct ablation experiments. This is not a big problem, but the author uses too many modules, I am not sure whether all these modules could work."	-	Need to specify how to select the weights in the loss function.
562	Weakly Supervised Online Action Detection for Infant General Movements	"1) It relies heavily on Openpose performance.
2) The paper insists that inference with only 20% of the entire video is sufficient, but other methods also show similar results. 
2) The dataset used is not disclosed, but detailed explanation is still lacking."	The consequences of ablation are unclear. In the case of the branch and local feature extraction module proposed by the author, it seems meaningful in video-level classification but does not affect action detection. It is encouraging that FM detection has been attempted from the point of view of action detection, but the overall performance is low despite the single class of action target. Even if a proposed dataset is constructed in a very limited environment, scaling to action detection does not appear to be effective.	"The paper also needs improvements. The weaknesses of the paper are listed below.

The detailed information of the proposed method may need more elaboration. For example the size of the CNN used in clip-level pseudo labels generating branch.
The proposed framework was only tested on an in-house dataset. The cross-dataset generalization would be an issue. If the proposed framework could be tested on more publicly available dataset it would be great.
There are some typos.  For example, on page 4, ""in detial"" should be ""in detail"".
How the clip-level annotations were generated and used should be presented."
563	Weakly Supervised Segmentation by Tensor Graph Learning for Whole Slide Images	"*	While the evaluation is done on two datasets, which is a clear plus, there are some shortcomings in the evaluation which reduce its value of it. For example, the comparison methods are not in the current state of the art but were published in 2019 (with one exception); only fractions of the datasets have been used during training, and no measures for uncertainty are given. 
*	The overall idea of using superpixels and graph networks is not new. Also, the type of annotation has already been used. (Of course, the authors do not claim this as a novelty) 
*	The authors used only parts of the training data due to hardware limitations. In general, the method seems to be rather computational expensive compared to the benefit. 
*	It is unclear how the meta-parameters were obtained."	"The explanation of the proposed method is hard to understand. The paper is not easy to read.
For the tensor graph learning, it is hard to follow the explanation without some clear definition. For instance, ""Graph adaptive and aggregation module (GAAM) learning combines and adjusts between different relations encoded in multi-relational graphs."" There's no equation to explain how different tensors interact.
Also for the tensor graph, it is not clear how to define the edge?
Regarding the loss, it is defined as a trace of a matrix, as it is not a traditional loss such as cross entropy, it would be better to explain the meaning of this loss, or cite a reference.
For the node reweighing, what does \hat{y}_m represent in the loss l_3?"	Please see below
564	Weakly Supervised Volumetric Image Segmentation with Deformed Templates	I personally do not understand the purpose of the image reconstruction component. Certainly, forcing the network to reconstruct the original image from the segmentation mask may help learn some of the network parameters. But I was expecting this feature being used to create some sort of feedback loop. Using the mse loss for reconstruction of image content is also very risky in this context: a perfect reconstruction is only possible as a result of overfitting because the model won't really learn the distribution of plausible images but just to associate segmentation with image. This can at best generate some contours with plausible greyscale levels near the boundaries of the object but blurry details elsewhere. Since the authors do not seem to use this in a feedback loop, I still am puzzled about why it is part of the training.	Although effective in terms of labor cost, the performance seems not to be very outstanding compared to the baseline approach.	"*	Doesn't mention how many images in each dataset, nor how the ground truth was obtained.
*	Experimental results are not explained properly.
*	Section 4.3 ""Full supervision"" does not seem to be clearly defined.  Do you mean you train a U-Net with fully annotated volumes? If so, how many were used for each set?
*	How exactly are the numbers in table 1 achieved?  Was a pair of networks trained on weakly supervised data, then tested on a different set of test images? If so, how many were in each set?
*	Is it the case that the U-Nets are only trained once? The user performs point annotations until the template is satisfactory, then the U-Nets are trained?  Or is it that the U-Net is trained as the user goes along, so that they can assess the performance and add points to poorly annotated regions? If the former, how will the user know that they have placed enough points?
*	Is the approach to train a pair of networks for each image to be segmented, or does one train a pair of networks for a set of images (each with its own sparse annotations)?"
565	Weakly-supervised Biomechanically-constrained CT/MRI Registration of the Spine	What is the likelihood this method will extend to other areas of the body, or general CT vs. MRI registration?	"1, The notations used in this text are confusing. Many characters have no definition.
2, Experiment results are not promising. Fig. 2 and Table.1 do not show significant improvements with compared methods, and it is difficult to find the impacts of different functions from the results.
4, Writtings and layouts should be improved. For example, Table 1 was mentioned before Fig.2 and Fig.3, but it appears in last pages."	Although the introduced rigid dice loss and rigid field loss perform better than the baseline method w/o such losses, the relative advantage is small. The ablative studies with properness condition and orthonormal condition also show varying results, with different loss terms having their own benefits. It makes it hard to draw a conclusion from such a work.
566	Weakly-supervised High-fidelity Ultrasound Video Synthesis with Feature Decoupling	"How occlusion and deformation are obtained as ground truth for this work?
Detecting sparse features as keypoints within US should be a challenging task, how robust is the proposed module to noise and outliers?
What is the level of similarity between the source and driving image? What does it need to be for this framework to prevent any failure?
Why a single module such as optical flow has not been used instead of keypoint and dense motion?"	"It is not quite fair enough to compare the proposed model with FOMM, given that FOMM learns features in a totally unsupervised manner. It could be more convincing if the authors conduct more extensive ablations on examining the contributions from each proposed component in the network.
The novelty of this paper is somewhat limited theoretically, given that FOMM has been already well-explored in the cv domain."	"Novelty seems to be limited. The major frameworks in fig 2 and fig 3 comes from [11], including the self-supervised keypoint detection, optical flow and occlusion prediction, and the occlusion aware conditional generation. The novelty parts are (a) introducing the manual label (b) introducing the GAN loss and VGG loss.
Lack of ablation study. There are limited experiments showing the newly add components works. For example, compared with [11], it is unknown if the manual label and GAN loss helps to improve the final results.
Lack some illustrations of acronyms. For Table. 1, it is not clear what is P, PT, and PTG."
567	Weighted Concordance Index Loss-based Multimodal Survival Modeling for Radiation Encephalopathy Assessment in Nasopharyngeal Carcinoma Radiotherapy	"(1) The author conducted ablation experiments on their own models on the two modal data and did not use other current models for comparison;
(2) Regarding the processing of image data, I hope that the author will introduce more details, such as: how does the author obtain the ROI region, manual or automatic detection? What is the ROI area size? Is the number of CT, RS, and RD pictures the same per patient? Three kinds of picture information can be directly controlled by the operator?
(3) The author's model is evaluated on a private dataset, is the public dataset used for experiments?"	"The dataset is highly imbalanced (1:10), a confusion metric or similar might be worth adding to show what kind of mistakes the model is more likely to make.
I am not sure if these improvement on CI is statistically important. The authors may want to add some analysis on the statistic significance."	"The weaknesses of this paper include:

Claim of ""best accuracy"" despite no statistical testing and/or analysis between ROC AUC or CI values for significant differences.
The weights may have been made parameters of the model to train (w_nv, w_v)."
568	What can we learn about a generated image corrupting its latent representation?	- There is no need to do experiments, We can also be sure that the scores of these two indicators(variance and MI) will be very poor in Sec.3.2. The generator has a high generation freedom with a black patch. So the author claims 'The results suggest the possibility of finding a potential confidence threshold to eliminate uncertain synthesized images.' is also worth researching.highly reproducable	The validation experiment of the proposed confidence score correlates with the quality of downstream task, e.g., segmentation, on the synthesized image is not enough and straight forward. Segmentation tasks may not be enough, more tasks such as classification and detection can be implemented.	The experiements shown in Figure 3 are somehow vague. I feel hard to understand the relationship between segmentation mask and the generated confidence score maps. Better to overlap both images to show correlation.
569	What Makes for Automatic Reconstruction of Pulmonary Segments	"The contribution is consider limited. The introduced definition is not novel.
The efficiency of the ImPulSe network is very convincing. Using the dice score is not very sufficient for the reconstruction task.
Labeling the pulmonary segments should be a challenging part of the task. A induction of the labeling procedure and ground truth qualify will be preferred."	"While the introduction and the title refer to pulmonary segment detection, the authors go beyond that, and also detect arteries and veins. This should be stated from the beginning. It comes as a surprise in the result section. 
The main justification of the method is the ""ability to output reconstruction at arbitrary resolutions"", with the side nice features of having less parameters and faster training time. However, I should be critical with the method. The authors have a continuous function that they use to define the surface of lung segments. However, the features used as input of such continuous function are obtained by interpolating the feature space, which has the same resolution as the input image (and therefore the output image of standard methods, such as the u-net). It seems as if the authors are doing interpolation in feature space rather than interpolation in the resulting output image. 
Following in that train of thought, Figure 2 is not a fair comparison between voxel-based methods and implicit functions, since the gaps between voxels makes them look artificially bad. Also, in the illustrative 2D example, a simple interpolation in the voxel space would have given a soft border.
The authors up sample the reconstructions using nearest-neighbour interpolation. What would have happened if the authors used other interpolation methods (as trilinear, as used for the features)?"	"1) In section 2.3, it states that the decoder predicts 19-class (including 18 segments and background). However, the Dice of pulmonary segments, bronchi and arteries are shown in the evaluation results. Does the model predict bronchi and arteries as well? It's confusing, please clarify it.
2) In section 3.3, the authors evaluate the effect of different inputs in reconstruction of pulmonary segments. It's confusing by when given inputs of bronchi and arteries, the validating the predicting of bronchi and arteries. 
3) In Table 2, predicting pulmonary segments from the inputs of lobes mask (or binary mask of bronchi and vessels) doesn't make sense.  I suggest to try with I, I+L, I+B, ...
4)  During inference, ImPulSe works with the original resolution of CT images, however, the fully convolutional models work with down-sampled images (128^3) and up-sample the predictions back to the original resolution. Will the down-sampling and up-sampling operation affect the results? Why do the authors not try with sliding-window strategy, for fully convolutional models?"
570	White Matter Tracts are Point Clouds: Neuropsychological Score Prediction and Critical Region Localization via Geometric Deep Learning	"No statistical testing is performed. Therefore, the performance of the model cannot be evaluated properly. It is just shown, that the MAE is slightly lower compared to the baseline. In Table one, at least the standard deviation should be reported to display stability of the predictions. A box-plot with significant differences indicated would be best.
The authors compare their approach against relatively simple regression models. Why where theses baselines chosen and not more advanced approaches such as random forests or neural networks?
The authors missed to discuss their method and its limitations. It is also not discussed that classical along tract analysis also enables localization, albeit only along the tract.
The authors do not correct the density of their tractograms, e.g. using SIFT. Therefore, tract densities will not be representative and some regions will be over- and others under-estimated.
Incorrect definition of loss function: The definition of the loss function seems to be displayed incorrectly and should probably be changed to Loss=L_pre+wxL_ps
It is not explained, why the authors introduced a weight for L_ps and why it was set 0.1"	"The motivation and evaluation of the study are limited in most clinical applications.
The explanation from the results about neuroscience seems a bit far-fetched."	"The point cloud representation does not include shape information, such as curvature, critical points etc. Hence, the network may not be able to detect alternations due to subtle shape variations.
The motivation of using a point cloud based model is not sufficiently justified by its novelty. This work may be better motivated if critical region localization on fiber streamlines is a main task of interest. 
The discussions and conclusions drawn regarding the critical regions are weak based on the limited quantitative results of the critical region localization. These discussions can be reduced significantly. 
The intersection region figure in Fig. 3 is not clearly presented. What do the three colors (green, red, yellow) mean? Besides, the surface of white matter-gray matter interface may be a more proper choice for visualizing the intersection. 
In the caption of Fig.2, the synonyms are not properly written. Lpre -> L_{pre}, Lps ->L_{ps}, (MSE -> (MSE)
The definition of Loss under Eq. (1) is wrong. 
Minor grammatic errors can be found. For example, 4 lines above Section 4, ""our method is non-invasively localizing..."""
571	Whole Slide Cervical Cancer Screening Using Graph Attention Network and Supervised Contrastive Learning	Lack of strong evaluation to show the superiority over other whole slide cervical cancer screening methods.	The authors mentioned the importance of whole slide images, but there is no evidence of using whole slide images in their manuscript. The authors didn't include qualitative image-based results.  Hence, it is hard to decide if the analysis is correct or wrong. Technical novelty is none or limited. The authors worked on Graph Attention Network and Contrastive Learning. These approaches may be new to the healthcare domain but have been used in other image analysis platforms. Hence, the author's technical novel contribution is limited. In Fig. 2, two different types of images have been used. The image in Slide seems H&Es, which doesn't match with tile images. It will be better if the authors introduce their data first. Then show their qualitative results in whole slide images. There is no clue on how the screening will be performed. The t-SNE plot is not sufficient as a performance measure.  The manuscript lacks sufficient qualitative and quantitative evidence. The authors should share their source codes, trained models, and a small amount of test data for review.	Some parameter values are not justified. For exmaple, what is the criteria to determine the appopriate patch size? How to determine the number of top-k patches?
572	Why patient data cannot be easily forgotten?	The experimentation is limited to a single medical application which suffers from the problem of limited data. While this might be a particularly challenging example, as only less than a hundred patients are used for model development, the more interesting real-world applications are ML models trained on large-scale data (e.g., chest X-ray disease detection). The paper would have been stronger if such an application would have been explored.	"I would have liked to see a more in-depth discussion on the implications of the findings of the paper. Is forgetting patient data a realistic method to deal with withdrawn patient consents? What is more important: respect data protection or ensuring model performances?
I'm wondering about the correct definition of when a dataset is correctly forgotten. Tabel 1 suggests that data is forgotten if the accuracy is 0.0. But isn't an accuracy of 0.5 (random classification) better suited?"	"The main difference of this paper compared to [9] is that the patient data is divided is two categories: edge cases and common cases. It is unclear how prevalent this division is for medical imaging data. The authors justify this with experiments in one dataset and even in this one dataset, no confidence bars are computed. With this level of empirical validation it is hard to assess the practical impact of the proposed method.
In the medical imaging dataset that was used, there is a large discrepancy between the test set performance (reported as 0.19 error) and the results reported in Fig 3. that show that ""By considering a threshold of 50% on
the error of the golden model, we find that > 60% of patients in ACDC can
be considered to belong to the edge case hypothesis."". The big difference in between these numbers and the test set performance indicate that the model training may have not been done properly (possible overfitting issues). This raises some additional concerns about the generalizability of the empirical results.
As a general remark the authors should include error bars in all their reported results, following the standards of paper [9].
For the medical imaging datasets, the authors should use an NN architecture that has close to state-of-the-art performance. It is unclear whether this is the case with the architecture that was employed."
573	XMorpher: Full Transformer for Deformable Medical Image Registration via Cross Attention	The discussions on the existing deep registration and transformer-based registration models are inappropriate. There is a lack of architecture descriptions of the proposed full transformer-based registration model and the integration with existing registrations models.	"Some of the comments the author express on existing methods are subjective. For example, I don't agree with their comments on the CNN-based registration methods including fusion-first and fusion-last, because the registration network is to learn the spatial deformation between the paired images, thus, the moving-fixed features extraction and matching should not split, while the authors claimed these two steps should split. Please list more experiment or reference to support your point.
The proposed XMorpher seems like require a huge training dataset, thus, the generalization seems like questionable. I think the large deformation might be limited by window size if you remove the affine transformation?"	Figures and their captions need to be improved. And more information can be acquired in Q8.
574	Y-Net: A Spatiospectral Dual-Encoder Network for Medical Image Segmentation	"There are some minor questions that could help better understand and justify the contribution of the work
-- Since there is global and local features extracted from the spectral encoder, how important/good is the segmentation without the spatial path? in the  is the depth of the network chosen in terms of applying CNN/GCN network? A U-Net is only a spatial model and Y-Net with only spectral model can help to highlight the spectral path
-- Was any data augmentation used for this work? U-Net requires data augmentation to learn the data distribution and could gain some performance with this.
-- The numbers for fluid segmentation is very less for RelayNet compared to the results reported in the paper. The authors of RelayNet report performance >0.75 dice for the fluid region. 
-- When the spectral features range is altered, how does the performance correlate to the SNR of the OCT images?"	Overall, it is lack of novelty, and its experiments are far more satisfied.	"-In this paper, the authors have used only one dataset to evaluate the proposed method.  For an architecture paper, I think that additional datasets should be used to evaluate the generality of the model.
-I think the design of the comparison experiment is not reasonable.  Compared to U-Net, the proposed method has an additional encoder. How can we determine whether the performance improvement is due to the extraction of spectral domain features or to the increase in the number of parameters?
-The novelty and contribution of this paper are limited."
