category		contribution	contribution	contribution	strenghts	strenghts	strenghts	weakness	weakness	weakness	repro	repro	repro	detailed	detailed	detailed	justifictation	justifictation	justifictation
review		review 1	review 2	review 3	review 1	review 2	review 3	review 1	review 2	review 3	review 1	review 2	review 3	review 1	review 2	review 3	review 1	review 2	review 3
id	title																		
001-Paper1347	3D CVT-GAN: A 3D Convolutional Vision Transformer-GAN for PET Reconstruction	This paper aims to reconstruct standard-dose PET (SPET) images from low-dose PET (LPET) images, via a  3D Convolutional Vision Transformer GAN (3D CVT-GAN), which extends the 2D Convolutional Vision Transformer under the framework of 3D Transformer-GAN, with the extended 3D CVT and TCVT blocks equipped into both generator and discriminator of GAN.	The paper proposed a 3D convolutional vision transformer-GAN model for low-dose PET synthesis.	This paper proposes a novel 3D convolutional vision transformer GAN framework to reconstruct standard-dose PET image from low-dose PET image.  Specifically, they utilized 3D CVT blocks as the encoder for feature extraction and 3D transposed CVT as decoder for final SPECT reconstruction. Furthermore, the proposed 3D CVT and TCVT blocks employ 3D convolutional embedding and projection instead of using linear embedding and project which overcomes the semantic ambiguity problem.	This paper extends the 2D CVT to 3D CVT, which is useful and helpful for 3D data. This work builds a 3D CVT-GAN for reconstructing SPET images from LPET images, and the experimental results are satisfactory.	The method is clear and makes sense. The results include comparison with multiple state-of-the-art low-dose PET synthesis methods. The method was evaluated on real low-dose PET instead of simulated low-dose PET.	Ablation study is provided to evaluate the effectiveness of each proposed modules, including patch-based discriminator, 3D CVT blocks, 3D TCVT blocks and convolutional embedding and projection. Comparisons with state-of-the-art methods show the superiority of proposed method. This paper is clearly written and easy to follow.	The writing and organization need to be improved. Specifically, the methodology can be simplified while the visual comparison of ablation study can be appended. Except for the number of parameters, the FLOPs can be shown for further evaluation of model complexity.	There are so many transformer-based methods. And the method is not considered to be novel anymore. The performance improvement is subtle. The dataset is too small.	Although the proposed method outperforms other methods in terms of PSNR, SSIM and NMSE, there is no too much difference in the visualization results as shown in Fig.2 for method of 3D-cGAN Transformer. It's better to provide some zoom-in regions to highlight the better result got by proposed method. Statistic evaluations such as PSNR and SSIM may not be sufficient enough for proving the reconstruction effectiveness in clinical aspect, this paper can consider adding reader study for clinical diagnosis of NC and MCI using either GT SPECT or reconstructed SPECT respectively. There is no explanation why the model size of proposed method is smaller than other methods.	This work is reproducible.	Code is not provided.	Not sure, if the authors agree to share the code to the community, I would consider change my rate	The expression and figures can be improved for better understanding and clear illustration.	The quality of the low-dose PET in Fig.2 is actually quite good. It leads to the question whether this application is meaningful. This can be proved by either the similar performance on a much higher dose reduction rate (visually worse image quality) or the comparison of the diagnosis (e.g. classification of NC vs MCI) on low-dose and each synthesized standard-dose PET. Even for a patch-based model, a dataset of 16 subjects is still very small. The improvement in Table 1 is very subtle. In table 1 and 2, add statistical test to show significance.	It's better to provide more clinical evaluations. Consider to use a bigger dataset.	This work has novelty on extending 2D CVT to 3D CVT for building a 3D CVT-GAN to reconstruct SPET images from LPET images, which is helpful for diagnosis. Thus I recommend to accept now.	The novelty is the main concern.	A lot of similar methods have been proposed to tackle the standard-dose PET reconstruction from corresponding low-dose PET images, improvements on PSNR and SSIM are not the key factors to show the effectiveness of proposed method. The authors should consider how to evaluate the robustness and generalizability of proposed method on a larger dataset.
002-Paper1233	3D Global Fourier Network for Alzheimer's Disease Diagnosis using Structural MRI	The paper proposes a deep learning model for Alzheimer's Disease Diagnosis from MRI images which works in the Fourier domain.	This paper proposes a 3D Global Fourier Network (GF-Net) to utilize global frequency information in predicting the Alzheimer disease (AZ). Authors used exist techniques to show the importance of frequency filter by comparing to other techniques in predicting the AZ.	This paper proposed to utlize the Fourier transformation to catch the global information to improve the AD diagnosis, so-called GF-Net. It divides each image into several patches to meet the patch embedding operation and use a sequence of global Fourier/ inverse Fourier transform with element-wise multiplication to capture global information in the frequency domain. Multi-instance learning (MIL) strategy is utlized to randomly drop patches to augment samples for training.	Using Fourier networks for AD diagnosis is novel. The method outperforms a large number of competing methods. The authors present an interesting ablation study.	-Used public datasets like ADNI -Compare with state-of the art	Method is described clear with good experimental results.	Details about the competing methods are missing. There is no time comparison. The relevance of the shapely analysis is not clear because it is not done for other methods, for example CNNs.	The  idea of frequency filter is already proposed and used (ex., https://ieeexplore.ieee.org/document/7504251, https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6485015/).	(1) Motivation is not clear. Why the global information in the frequency domain is useful, What if just enhence the global information, as done by multihead attention? (2) Not report results on MCI classificaiton.	Code will be available but the authors will not provide pre-trained models or evaluation code.	Could be reproducible if the code will be available since the data are public.	It is possible to be reproduced.	"in page 2 correct ""explain f_ff in details"" to ""in detail"" In Fig 1 correct layer nrom to layer norm."	-Why the proposed approach only applied to Alzheimer data. We suggest testing the pipeline on many public data...,etc. The paper is almost close to clinical paper. We suggest author to note the novelty in technical aspect.	(1) It would be better to verify how much the  frequency domain improve the performance. (2) It would be better to report AUC values for such binary classification. (3) It would be more useful to report results on MCI classificaiton. (4) How many patch extracted in each image? The classification results decrease along patch size, it is because of the reduce of patch counts? (5) Some typos, e.g., 0.915+-0.031 should be 91.5+-3.1.	Novelty of the method and convincing results.	Technical aspect.	No results on MCI classification. Not report AUC values.
003-Paper1512	4D-OR: Semantic Scene Graphs for OR Domain Modeling	This paper proposes to represent the OR via semantic scene graphs to achieve a holistic understanding. A new 4D-OR dataset is constructed and will be made public. Several state-of-the-art computer vision methods are pipelined and tested on the new dataset.	This paper presents a neural network-based approach to generate semantic scene graphs of the activities happening in the OR during a surgical procedure. The principal goal of the proposed approach is to accurately predict the role of every human present in the OR.  A quantitative evaluation is performed on a dataset composed of simulated total knee replacement surgeries recorded with six RGB-D cameras, with annotations of human and object poses, SSG labels and clinical roles. The authors propose to make this dataset public, which can be a big contribution to the field. A complex methodology is presented to process the images and point clouds, and also to automatically obtain human and object pose information using off-the-shelf approaches, and then generate an SSG to be able to predict the role of everyone in the scene. The approach achieves good performances especially for patient and head surgeon role prediction.	The paper tackled the problem of understanding surgical scenes. It proposed semantic scene graph to construct a holistic knowledge of the operating room. The paper generated a multi-view dataset and relied on state of the art model to detect 3D human and object poses. These were then used to build a scene graph and predict roles.	A new 4D-OR dataset is built and will be released, which will benefit the community. The idea of using semantic scene graph for holistic OR understanding is significant, which has the potential to make an impact. The paper achieves scene graph generation by assembling several state-of-the-art computer vision methods into a reasonable and effective pipeline.	The paper is clear and well-written. The methodology is clearly explained, and its explanation makes for most of the content of the paper. The proposed method appears to be reproducible thanks to all the details provided and that is a good thing. This paper can contribute to future context-aware systems with an automatic and holistic understanding of the activities happening in the OR. A rich dataset is introduced, which can be used by the community for developing and evaluating similar approaches and go one step closer towards a smart OR. Indeed, the presented dataset is complex and is challenging to obtain. Synchronizing and calibrating six RGB-D cameras, with all the post-processing needed, is not an easy task. The dataset also provides several kinds of annotations. The quantitative results provided will represent a baseline for future papers. I also find this paper interesting because MICCAI has already seen similar works but rather dealing with images coming from minimally invasive surgeries (such as laparoscopic videos or robot-assisted surgeries).  Yet this paper deals with recordings from a conventional orthopedic procedure from ceiling cameras. This is promising because today around 90% of total knee replacement surgeries are still performed using conventional instrumentation (no navigation, cameras, or robot present). Hence, this paper shows that conventional approaches can also benefit from smart context-aware systems in the near future.	Focused on a problem of holistic scene understand in the OR that can benefit many applications Generated a novel dataset with annotation Making the dataset publicly available Achieving high performance on detecting roles and relationship	"As for the clinical role experiment, I wonder why the authors did not embed the role into the scene graph where the relations will be like ""Assistant surgeon assist head surgeon"" instead of ""human1 assist human2"". What is the benefit of a role-agnostic scene graph? The human poses and object boxes in the new dataset are automatically generated by the Kinetic SDK. What is the accuracy of these generated annotation? How does this source of error affect the evaluation of this paper?"	The paper lacks details about how realistic the surgeries included in the dataset actually are. This is important for the reader to get an understanding of how challenging the clinical role prediction task is from the data. Also, since the surgeries are simulated, it is not clear if the most important parts of the knee replacement procedure are actually included in the data, namely everything that happens after the knee incision (placement of cutting guides, bone resections, knee joint assessment, trial implants, cement preparation). Furthermore, how different are the 10 recorded procedures? Knee replacement surgery can be performed through different clinical approaches depending on the desired alignment (mechanical or kinematic), the type of implant, etc. Are all these differences represented on the dataset? Or is it the same surgical approach repeated ten times? Are the actors in the videos different from one video to the other? Has the OR changed somehow from one video to the other? Moreover, the size of the test set seems small for an evaluation of the performances of the proposed approach, especially since we do not know how different each of the surgeries are. Do you believe your approach would generalize well to variations in the surgical workflow? The evaluation metrics and results are difficult to interpret and understand. More explanation should be provided. What is the purpose of the ablation studies that are presented? The discussion section is short and as a reader you feel like more can be discussed about this work. Especially about how the approach would translate to a real clinical setting. No recommendations for this are provided or about how would this approach be implemented in a clinical setting.	Limited technical contributions It would be interesting to get the performance by perturbing pose estimation prediction. It would be interesting to see on camera view layout the paper does not include any information on the calibration. It would be interesting to provide some information on how the cameras are calibrated.	Although the authors promise to release the codes, the reproducibility of this paper is at a high risk due to the huge pipeline of existing methods and many ad-hoc components. I would recommend the authors package all the pipeline components together into a for example Docker container.	Details about implementation, all the off-the-shelf approaches, hyperparameters, data split are provided. However, the paper lacks explanations about how clinically realistic the simulated surgeries are.	The author committed to release the code upon acceptance.	Please see weaknesses.	Please find below a set of comments/questions that I believe can help improve the paper if addressed: Did you consider having additional data as input besides images and point clouds? I am talking about sound or signals from electronic equipment (e.g. oscillating saw or drill). It seems complicated to imagine future OR to have six RGB-D cameras installed. Regarding the RGB-D cameras: how was the placement decided? How is the calibration between the cameras done, I mean the extrinsic parameters enabling to fuse all point clouds properly? More details about this could be useful to the reader. Could you give more information about all pre-processing steps for the point clouds. The fused point cloud seems noisy in the figures. Did you consider working directly with the depth images instead? Could you discuss how much ambient illumination can be an issue for your approach? The scialytic lamps were always off during the recordings? As mentioned before, I think the paper could benefit from more details about data. Knee replacement surgeries can have different workflows and surgical gestures depending on many things such as surgical planning (mechanical or kinematic alignment), type of implant, surgical technique. Were all simulated surgeries the same? Is the ancillary used (such as intramedullary rod, cutting guides, pins...) also included in the simulation? It seems to me that the specific parts of a knee replacement surgery happen when the knee is exposed, and your dataset rather considers the activities happening around the OR but not directly the surgical gestures. How would this method be applied in a clinical setting? How do you envision its integration to routine clinical practice? Could you provide the list of relationships and activities considered? I do not see things like bone resection, implant placement, cement .... Also providing the list of objects considered (only large medical equipment?) can be interesting.	Can the author provide any information on depth interference among the views. One can imagine that the accuracy of the model decreases on non-simulated cases. I think it would be interesting to see the affect of perturbed 3D poses on the performance of the model. Considering the model have achieved high performance, do the authors plan to make the dataset more challenging by adding either extra data or tasks? The model had achieved almost perfect prediction for patient and head surgeon. Can the author comment if this could be due to the fact that their 3D location are very similar across the data. Have the authors simulated both left and right knee replacement to get different location for the head surgeon.	"This paper is a good proof-of-concept of scene graph in the surgical field. Besides, a new dataset will be made public. Therefore, I recommend an ""accept""."	This paper brings interesting contributions, but it seems more as a proof of concept. Hence I recommend rejection and resubmission for a future MICCAI. The dataset used for evaluation does not seem clinically realistic (at least from the scarce details provided in the paper). The evaluation sections and discussions seem shallow and also lack information on how this approach could translate to clinical practice.	I think the paper would be stronger if the effect of data perturbation is analyzed more and also more information on scenarios captured for this dataset.
004-Paper2655	A Comprehensive Study of Modern Architectures and Regularization Approaches on CheXpert5000	The paper presents a methodical study of modern architectures applied to a fixed low data regime of 5000 images on the CheXpert dataset. Specifically, it studies the BiT and ViT models through experiments, as well as established regularization methods Mean Teacher and MixUp. These are compared to the well-known and frequently used ResNet50 architecture. They find that models pretrained on ImageNet21k achieve a higher AUC and larger models require less training steps. All models were quite well calibrated and performed well. Regularization of Bit-50x1 with MixUp or Mean Teacher improves calibration and accuracy. Vision Transformer achieve comparable or on par results to Bit-50x1.	The general computer vision literature is moving towards larger models pretrained on increasingly larger scale datasets, much larger than ImageNet-1K which is still the standard approach in medical imaging. Although the role of transfer learning has been repeatedly questioned, recent results support this widespread practice especially when the size of the target dataset is small, and when using architecture with weak inductive priors such as transformers [1]. In this paper, the authors experimentally evaluate the performance of Big Transfer Model (ResNet50, BiT50 and BiT-101) and Visual Transformers (DeiT and ViT) pretrained on ImageNet-1K, ImageNet-21K and JFT when transfer learning to the medical domain. They experiment on CheXpert downsampled to 5000 images to focus on the small data regime. Results show that BiT outperforms ResNet50, and show the advantage of ViT over DeiT, which is inline with previous results  [1].	The authors conduct an extensive set of experiments to demonstrate the feasibility of large-scale transfer learning on chest x-ray classification.	The paper is a solid set of experiments on using modern architectures on relatively small datasets and evaluating performance. The discussion of the findings are thought provoking and interesting. It provides good guidance for fertilizing future work.	The paper brings a new class of models to the attention of the medical imaging community. Results can be easily applied by other researchers Transfer learning from ImageNet is still the standard practice in the medical domain, especially when dataset size is small, and thus the results are relevant Experiments are comprehensive and provide practical pointers e.g., related to the optimal batch size	The experiments are thorough - they are well designed and thoroughly executed. It well backs the papers claim and contribution. The topic and experiments are relevant - the network architectures (BiT, ViT) that the authors experiment with are probably relevant to the readers interest.	No significant weaknesses to comment on.	Experiments are reported only on one dataset (CheXPert) and only in the small data regime (5000 training images) The paper is somewhat rushed and some parts need to be clarified	Lack of novelty - there is no major novelty either technical or on data contribution.	Good	The paper relies on publicly available pre-trained models and datasets. New splits defined but will be provided for the final version. The methodology is described in detail, but a few details are not very clear, in particular: The description of the fine-tuning setting is not very clear (see detailed comments to authors) Which pretrained models were used in the experiments and the link to retrieve them.	The reproducibility of the paper may be limited due to the requirement of large compute and data requirement. The authors also do not publish their code or data used.	Nothing significant. I would have liked to see domain specific models in comparison also.	"Major comments: I agree with the authors that the small data regime is the most interesting one when exploring transfer learning from the RGB to the medical domain. However, given the large data imbalance in the CheXpert dataset, the authors should report how the dataset was sampled (if random or stratified), and what is the resulting label distribution. The authors report the results on the standard split, as well as a new split with larger validation and test set. This is based on previous work by Mustafa et al. who pointed out that the standard validation set is too small to allow meaningful comparison. At the same time, it seems to me deeply unrealistic that a practitioner would train on 5K images, validate on 17K and test on 25K images. A more realistic comparison would be for instance to sample multiple test set from the 25K sequestered images to include the variability due to the test set into the confidence interval. I understand that fully solving this problem would be another paper in itself, but in my opinion it is still a limitation and should be at least addressed Which pretrained models were used in the experiments? The authors mention the timm library, which does not include BiT models. In Table 1, there are two models BiT-50x1, one trained on 89944 images, and one trained on ""all"".  I would report the actual number of images, and clarify what is the different between the two sets. In Table 2, the number of iterations for REsNet50 appears to be much larger than those for BiT-* models. It is not clear why and it would be interesting to compare how the different models converge The description of the fine-tuning setting is not very clear. For BiT, the papers first state that the BiT-HyperRule was used, but then concluded that training with a batch size of 32 with the same plateau scheduler as ResNet yielded better results. As a consequence, it Is not clear which protocol was used exactly for each experiment in Tables 1 and 2. Minor comments: The confidence intervals are lacking as experiments for some models were still underway at the time of writing. The results are unlikely to change substantially, therefore in my opinion this is not a major limitation The paper feels somewhat rushed with several typos. For instance deramatology -> dermatology (page 4), presumambly -> presumably (page 4), it's small size -> its small size (page 5), smalles datasets -> smallest? Datasets (page 6) Some sentences have grammar issues or lack the subject. For instance, at page 5, Split to intra: automatically created from report, and valid: 234 manually annotated X-rays [1] Matsoukas, C., Haslum, J. F., Sorkhei, M., Soderberg, M., & Smith, K. (2022). What Makes Transfer Learning Work For Medical Images: Feature Reuse & Other Factors. arXiv preprint arXiv:2203.01825."	ViT's benefit from large pre-training dataset yet the authors conducted experiments on the same dataset. It would be good to obtain more data - perhaps not exactly chest x-ray but other related medical images - to see the benefit of ViT pre-training.	Good complete work.	This overall appears to be a good paper with rigorous methodology and clearly presented results.  The results are of interest to the medical image analysis community at large. The paper would have been stronger if additional datasets were included in the analysis.	The paper is well organized and well written. The experiments are well constructed and thoroughly conducted. The methods are relevant and up-to-date.
005-Paper1525	A Deep-Discrete Learning Framework for Spherical Surface Registration	Deep neural nets-based surface registration Reshaping existing methods to a CNNs framework Validation on a large dataset with other spherical registration methods	This paper proposes a deep learning framework for registration on a sphere. The problem is framed as a discrete matching problem - where each control point on a sphere is assigned a label corresponding to one of a possible set of displacements, with regularization by a conditional random field network. The approach is evaluated in a large dataset of brain surfaces against both deep learning and conventional algorithms, and has competitive performance.	This paper presents a learning-based pipeline to co-register two Cortical surfaces using spherical representations of the surfaces. Followed by a learned rigid alignment of the surfaces, the author's apporach includes the successive prediction of displacement fields eventually aligning one spherical surface to the other.	The paper proposes a neural nets-based spherical registration. The underlying theory is based on existing spherical registration [22,23] and MoNet architecture [9]. Although theoretical contributions seem weak in this work, the authors integrated existing approaches into a single framework. The proposed method was validated on a part of the HCP dataset (N~1.1k) with other spherical registration methods. The results show comparable performance (especially to Spherical Demons) while reducing computing time.	The proposed method uses different architecture choices compared to the closest existing work, S3Reg, that simplify the approach, eliminating the need to average multiple solutions. The overall design of the approach makes sense for the registration problem at hand. The method is very clearly explained. The evaluation is excellent, using the very large Human Connectome Project dataset, and comparing to several conventional methods and S3Reg under different parameter settings. The comparison considers both quality of matching and distortion, making it more thorough.	The authors address a weakness of a prior learning-based approach, i.e. lacking equivariance of filters, by incorporating a suitable solution from another paper. The authors' design is creative and well tailored to the problem at hand. Furthermore, the authors provide sensible explanations about the components of their method and present evaluations providing insight into different dimensions of their results, e.g. looking at distributions of strain etc..	The paper presents promising results. Yet, some points are not clear; in particular, the motivation of using rotational equivariance seems weak. Please see my comments in the section below.	It is not clear whether diffeomorphism of the spherical deformation is guaranteed, as in Spherical Demons. It may be so if the local displacements are constrained to be small enough and enough iterations of fitting are performed, but this is not stated, and in description of the coarse-to-fine architecture it seems like the coarse and fine level models are only executed once per pair of input spheres. So it seems like it would be possible for the deformation to cause folds on the surface. I wasn't sure whether this is captured by the strain variables - they all are plotted on the log scale so one has to assume there were no negative values. The overall contribution is somewhat incremental.	No major weaknesses.	The proposed method seems reproducible.	Ok	The method's architecture should be reproducible from the descriptions in the paper. The dataset used for training and evaluation is also public. However, to directly reproduce method and results, parameters of the method would need to be published.	"The authors pointed out rotation equivariance does not hold in S3Reg, in which a filter orientation is inconsistently represented at the poles. This might be motivation of this study in using MoNet. I like a rotation-equivariant approach but was disappointed in how rotational equivariance was used in this work. Although I do understand the orientation problem, the MoNet approach (or rotation equivariance) seems not critical in the proposed method. In particular, MoNet was used for rigid alignment as preprocessing in this work, which may be able to capture the global orientations because of rotation equivariance. However, rigid alignment is relatively quick and generally works well without learning in conventional methods such as FreeSurfer or Spherical Demons. When it comes to spherical registration, rotation equivariance seems not valuable as the input data are already rigidly aligned. Also, I think ""good"" rigid alignment is key in the proposed method because of the label order, which may not be modeled even with rotation equivariance. In this step, MoNet and Spherical Unet (or other spherical CNNs) perhaps make no huge difference in terms of the label inference. It is unclear how critical the orientation failure at the poles is on overall registration performance from a practical view. The overall registration framework is heavily based on [22,23]. The proposed registration is clearly faster than conventional surface registration. Since diffeomorphism is implicit yet, how about the regularity of the deformations (e.g., self-intersection)? Does the proposed method unfold flipped triangles (triangles with negative area)? In the introduction, the authors claim that ""there is a limit to these improvements since cortical topographies vary in ways that break the diffeomorphic assumptions of classical registration algorithms"". I am unsure if this is overcome by the proposed method. Since the framework follows [22,23], what makes methodological difference from the classic methods? Any supporting examples? In the results, Spherical Demons seems better in all benchmarks except for computing time. Although the proposed method is fastest, the computing time is often not more important than registration accuracy in many neuroscience studies. Can the authors comment on the results? Along this, some manual labels (e.g., cortical parcellation) would be also helpful to understand the performance of the work, though I am unsure if this is feasible in this work."	Overall this is a very nice paper, I would recommend addressing the question of diffeomorphism. Some additional specifics could be provided, for example it wasn't clear until seeing Figure 3 that the features being matched were sulcal depths; the loss function could also be more explicitly stated. It would also be interesting if the approach would also work well for non-learning based registration, i.e., for optimizing the loss function for a given pair of inputs. After all one really wants the best registration possible, even if it takes a bit more time (i.e. a few iterations of gradient descent) to compute the solution.	I enjoyed reading this paper (see comments above). A few minor points: More details about the loss function desirable. While the authors state loss function components for different parts of their pipeline, it is unclear whether all components are jointly or independently trained (Rotation + DDR1 + DDR2). It would help to state the loss function including the cross correlation term (maybe instead of equations (1) and (2) if space is an issue). Formulas for J and R are the same (page 6)	Please see my comments above.	Good evaluation and valuable method for a real-world problem. However, enthusiasm tempered by the two weaknesses noted above.	The authors' approach is interesting, addresses shortcommings or prior work, and is well explained in this paper.
006-Paper1155	A Geometry-Constrainted Deformable Attention Network for Aortic Segmentation	This work is dedicated on the automatic segmentation of the aorta from injected CT-scan using a deep learning approach that is geometry-constrained.	The paper introduces a geometry-constrained pipeline for the automatic segmentation of the aortic lumen in healthy and diseased cases.  The diseased cases include coarctations and aortic dissections.  The proposed method uses geometry-constrained deformable attention which consists of an extractor and a guider. The former generates variable-size patches and, thanks to self attention, captures long-range dependencies.	The paper proposes a geometry-constrained deformable attention network (GDAN) for segmenting aorta. There are two main contributions: morphological constraints and deformable attention mechanism are added to the segmentation network.	The main strengths of this work are : Considering the shape of the aorta in the segmentation process, and then by extent, the fact that the segmentation must be anatomically plausible Validation on a database with different diseases Evaluation of the method according to the part of the thoracic aorta (ascending, arch and descending) Good results	"The work proposed a novel pipeline for the automatic segmentation of the aortic lumen.  The authors show that this pipeline outperforms previous works on both healthy and diseased aortas. It is of potential interest to the community given the limited amount of work on image analysis of chronic aortic syndromes [1,2]. [1] Fleischmann, Dominik, et al. ""Imaging and Surveillance of Chronic Aortic Dissection: A Scientific Statement From the American Heart Association."" Circulation: Cardiovascular Imaging 15.3 (2022): e000075. [2] Pepe, Antonio, et al. ""Detection, segmentation, simulation and visualization of aortic dissections: A review."" Medical image analysis 65 (2020): 101773."	Strong evalutation: The proposed method is applied to the aorta segmentation and  diameter measurement. The paper gives a reasonable detailed comparison with other methods which shows the performance of the proposed method.	"There are some weakness in this paper, however : Who did the manual segmentation ? Making an evaluation according to the part of the aorta is a good idea, but it could be interesting to have the same kinds of result according to the pathology The Hausdorff distance is a metric to detect the outliers, and only considering the 95% of the Hausdorff distance is a non-sense, or something to hide bad results. Please provide the ""true"" HD The standard deviation must be indicated in the Bland Altmann plots in the figure 5. The diameters indicated in the discussion are not good. According to the Bland Altmann plots, the maximum error is higher than 0.98 mm The units are not specify for the HD"	"The introduction is rather long for the format of the paper, but while it brings attention to some problems of cardiovascular radiology, it does not provide much information on the current state of the art, its limitations and why the reasons behind the architectural choice.  Perhaps recent reviews on this topic can support the authors in editing the introductory part [1,2]. I would also recommend the authors to split the introduction from the related works. In 2.1, ""h"" is not defined. Also, what do the authors mean with ""base fact""? While comprehensive, I find the method description (section 2) to be too focused on the mathematical description of some aspects, while other aspects are left without any details (e.g. ""At the end, a lightweight decoder is used to fuse multi-level features to decode."" -> it is not clear what the decoder actually is and how it exactly works). Dataset: Was there an ethical approval for the access of the 204 CT scans? Please report statistics regarding volume size and spacing. Train and test: after how many epochs was the learning rate reduced? Diameter meausurements were not compared against state of the art but only against ground truth. [1] Fleischmann, Dominik, et al. ""Imaging and Surveillance of Chronic Aortic Dissection: A Scientific Statement From the American Heart Association."" Circulation: Cardiovascular Imaging 15.3 (2022): e000075. [2] Pepe, Antonio, et al. ""Detection, segmentation, simulation and visualization of aortic dissections: A review."" Medical image analysis 65 (2020): 101773."	The experimental section lacks analysis in the discussion of comparison method.	It would be interesting to have access to the database, but I know that it is not evident. Moreover, the manual segmentation is questionable	Some further implementation details are provided as supplementary materials.  Do authors do not plan to publish code or trained models. CT data is also not publicly available.  Details on ground truth generation are also limited.	Although the data and code is not public, the authors list most of the details of the method in the paper.	My main comments to improve this article are : Provide the Hausdorff distance Indicate the results according to the diseases Some errors in the text, particularly the values in the discussion Specify the units for the Hausdorff distance and the RMSE	"For major points please see 4) and partly 7). In addition: Minor typos/punctuation errors are present throughout the manuscript. Although minor, I would suggest the authors to read once more through the text and figure captions.  E.g. sometimes it says ""extracter"" and some ""extractor"". I assume the latter is correct? What do the authors mean with ""reference point"" in 2.2? Figures need to be replaced later in the text. Ex: fig.3 is mentioned much later. Figure 4 is currently not mentioned in the text. conclusions: the reported error should have a +/- sign? It would be beneficial to add more details about image statistics (e.g. size) and about ground truth generation protocols/processes. While it is meaningul to show that the proposed approach outperform other methods, it would be also interesting to show how the different methods perform on the different classes (dissection, coarctation, etc..), perhaps this could be part of a larger journal version."	The methodology is in general well described. The following comments could be taken into account for further improvements. In the experimental part, what technologies are mainly used in other methods[2,3,4,5,18.27]? Comparison should be added in related work or the experimental part to better demonstrate the advantages of the method in this paper. In clinical application, is the vascular segmentation and diameter calculation real-time? It is suggested to increase the calculation amount compared with other methods. There are some expressions which should be clarified, though, as they are sometimes difficult to interpret:  Sec 2.3: The number of center-points, M, should not be a component of C, which would make formula(5) ambiguous. M should represent the number of centerpoint information tuples. If my understanding above is correct, I think to rewrite for C^m = (l_m, v_m, d_m), C = {C^m m = 1,... M}	Good article, good study, but some correction to do.	While results show some merit, a clear comparison for diameter meausurements is currently missing In the current state, a solid round of editing is needed to increase readability of the manuscript. The editing phase should also try to clarify all aspects in the methodology section to ensure reproducibility.	The method has high clinical application value in the field of medical image analysis  and also has strong experimental comparison.
007-Paper1247	A Hybrid Propagation Network for Interactive Volumetric Image Segmentation	This is a well written and organized paper that presents a hybrid (two network) approach for interactive, semi-automatic segmentation of 3D medical images.	The paper proposes a method for interactive segmentation of 3D medical data. The method uses two modules, 1) a slice propagation network (SPN) for transferring user interactions to adjacent slices and 2) volume propagation network for transferring user hints over the entire volume. The main motivation of using SPN is to avoid having to run a 3D model on whole volume, which would require higher memory usage. The proposed method improves upon existing interactive segmentation methods.	This paper introduces a method for interactive segmentation of 3D medical scans. The pipeline relies on two networks working iteratively on the input volume, the user scribbles and the previous segmentation: one in 3D working on a crop of the volume, and one in 2D that propagates the segmentation from one slice to another. The former is based on a 3D Unet, while the second one is based on the Deeplab architecture with a memory module that takes as input both the 2D input and the corresponding slice of the 3D feature map of the first network. The method is evaluated on four segmentation tasks from public CT databases: kidney, tumour, lund and colon.	The approach leverages a slice propogation network to propogate information from use scribbles on one slice to adjacent ones using memory modules. A volume propogation network is used to perform volumetric segmentation and generate features for the memory modules. This is an interesting approach that is novel to my knowledge and is broadly applicable. The approach is tested on segmentation of multiple organs and compared to multiple methods. Quantitative results are impressive. The method appears to generalize well to a variety of applications.	The interactive segmentation of medical images is an important but under-studied problem. The proposed method improves performance over existing interactive segmentation methods on multiple datasets. The paper is well written and relatively easy to follow.	This paper addresses a relevant (yet not particularly researched) problem, namely interactive segmentation. This is particularly important nowadays to help clinician annotate images more efficiently. The method is compared on several datasets and to several baseline approaches.	The validation study is designed to automatically simulate user interaction scribbles. It's not clear if the method would perform as well with a human scribbler.	It is unclear how many interactions are used for results in Table 1. Is same method used for generating user interaction for all methods?	My main concern is that I am not sure whether I can see such a complex approach easily deployed in clinical use. It is an interactive method, but it still (1) requires networks training and therefore specific to the anatomies it has been trained for, (2) requires some computational power on the user-end to run those networks. In order to be used in practice, I feel like an interactive method should be swift and lean. Here, there is no mention of the computational time of each prediction, or the overall time it takes an actual user (not simulated scribble) to segment in a satisfactory way the organ of interest, or a study on generalization to new structures. The complexity of the method makes it difficult to reproduce; for instance it uses both a 2D and a 3D network, the training has 4 different steps where various parts of the network are trained, etc. The method could have been better explained. I don't understand why one part is in 3D and the other in 2D. Figure 1 is overall a bit confusing: the order of the different operations is not clear.	The methods are detailed and reproducible	"It would probably be difficult to reproduce the method as it includes multiple modules, implementation details of which are not completely clear, e.g. It is unclear what ""Semantic segmentation loss"" means? It is unclear which model wights are referred to be ""the pre-trained weights on video object segmentation is used for initialization"" The details of scribble generation are missing."	"Training/evaluation code, as well as pretrained models and data, are specified as ""available"" but no link has been provided."	Adding a small user study showing better accuracy with less interaction time/clicks with this method compared to others would move this paper from good to great. Another way to expand the validation would be to include other image modalities in the analysis.	"In Fig. 1, space character is missing for some labels. The language needs some improvement: ""refine the volume segmentation result a time"" ""both two networks for slice and volume propagation are included in our method"" ""This two networks collaborate with each other for segmentation."" In Fig. 3 including the performance for one or two baseline methods would have been nice."	"Overview I am not sure to understand why we need both a 3D and a 2D network. How does the network know when to stop the propagation (at the of the ""end of the organ"")? How is the 3D crop first defined? How does the network know the extent in the out-of-slice direction? Is the volume network trained at a fixed resolution in pixels? (or in mm?) or do you train it with varying input size? Figure 1: The slices shown are upside/down (CT table is on top of the images), it's not very important but it does look weird. Figure 1: What do the yellow arrow mean above/below the memory modules? The experiment numbers are not very detailed. Reporting the mean is not enough: we need more details (at least standard deviation, quartiles or even better full boxplots or violin plots) No statistical tests were performed Dice coefficients are only one part of the picture, other complementaty metric Hausdorff distances How many interactions were necessary to obtain the results from Table 1? Failure cases/areas of improvements are not discussed (also no future work is mentioned in the conclusion) Misc ""Given a 3D volumetric image, the user interaction can only be performed on one of all slices,"" I disagree with this statement, several commercial and open-source software (Slicer, ITK-SNAP) offer 3D brushes. I appreciate the comparison with several other baseline methods, but it seems to me that in the context of interactive segmentation, the current standard is not necessarily deep learning-based approaches. It would have been even nicer to compare with other standard approaches like watershed, grabcut [https://docs.opencv.org/3.4/d8/d83/tutorial_py_grabcut.html], random walker [http://vision.cse.psu.edu/people/chenpingY/paper/grady2006random.pdf], etc. Figure 2 Which cases are shown here? median ones? good ones? Please indicate how they were selected Some images with a particularly low quality (first two crops), please regenerate them A 3D visualization of the different segmentations would also be nice Does Figure 3 contain average numbers over the whole dataset or just one example? There are a number of typos: ""This process will stop when reach a stop slice,"" ""Note that the VPN is not work in the first iteration"" ""both two networks for slice and volume propagation are included"" ""This two networks"" ""to crop and generate volume patch input of our VPN"""	This is a good paper that is of interest to the MICCAI community but has a minor weakness.	The problem is somewhat under-explored and the proposed method is evaluated on 4 datasets and it out-performs baseline methods on all 4 of them.	This paper has merits but I am a bit concerned with the complexity/reproducibility of the method, as well as its applicability. Since I also had a number of remarks on the clarity and experiments parts of the paper, I am leaning towards rejection.
008-Paper0063	A Learnable Variational Model for Joint Multimodal MRI Reconstruction and Synthesis	In this paper, a learnable variational model is proposed for joint multimodal MRI reconstruction and synthesis. Numerical experiments are performed to validate this method.	In this paper, the authors present a learnable optimization algorithm to jointly do the image reconstruction and image synthesis, the method takes undersampled radial k-space MRI acquisition and outputs the reconstructed image as well as the synthesized third-modality image.	Based on the theoretical analysis, this paper proposes a new deep model, which can simultaneously reconstruct the source modal image from partially scanned k-space MR data and synthesize the target modal image without any k-space information. In addition, the network proposed in this paper adopts the double-layer optimization training algorithm to optimize the network parameters and super Cen book, and a large number of experiments on different modes of brain magnetic resonance data verify the effectiveness of the proposed algorithm.	(1) The proposed joint MRI reconstruction and synthesis comes with theoretical analysis. (2) A bilevel optimization is used for parameter updating. (3) Experimental results, especially some visual results zoomed in on tumor regions, shows its potential utility and value in practice.	This paper delivers an solid work on jointly reconstruct undersampled MRI images and perform the image synthesis, the paper is well written and include a thorough analysis and metric comparison. The method is also well described, In general, a very solid work. In terms of both quantitative metric, as well as the visual image quality, the proposed method provide superior results compared to other methods, also, including the results in the regions of pathologies (Fig 2) delivers more diagnostic value of the paper.	In this paper, the convergence of multi contrast joint reconstruction and generation learnable algorithm is guaranteed by controlling the super parameters, and the idea of meta learning is used to optimize the super parameters to obtain the balance of multiple regular terms.	(1) The Introduction is not well organized. It seems more like stack the review of compressed sensing MRI and MRI synthesis without presenting their link. What is the value of study MR image synthesis under the setup of partially acquired MRI measurements of other modalities. In my opinion, this is the key issue to be addressed to show the value of this paper. (2) Following the first concern, is this paper the first work to explore the joint MRI reconstruction and synthesis? If so, it should be stressed since it is a major contribution. If not, related works should be discussed. (3) Another major issue is that the rationale or analysis should be provided on why the third term in Equation (1) is a reasonable design? Citing related papers, visualized analysis or explanations should be useful. (4) The technical details should be better organized and concise to make the paper more readable and understandable to MICCAI community.	"One weakness is that the paper doesn't clearly describe how the k-space data was generated, since, as far as I know, the BRATS 2018 dataset is not for raw-kspace, please carefully describe the data processing steps. Also, please check carefully on this paper: Implicit data crimes: Machine learning bias arising from misuse of public data, which shows some potential data crime for MRI reconstruction. The undersampling strategy for this paper is radial sampling, and 40% sampling rate is kind of considered as a ""low"" acceleration factor. Would be nice to elaborate more on how does the undersampling matters and what happened if the sampling is cartesian."	This paper proposes to use the idea of meta learning to optimize the hyperparameter, but it does not show that the parameter is the main limitation affecting the problem, and there are not enough experiments to verify the advantages of the optimized parameters over the general hyperparametric design.	The method presented in this paper is loaded with technical details. I think it would be challenging to reproduce this work without the help of publicized codes.	The authors didn't claim that they will release the code.	This paper can be reproduced.	(1) The authors are encouraged to strengthen the introduction section. (2) A detailed analysis and explanation of the proposed model is recommended.  (3) Paper should be reorganized to make the presentation of the optimization uncluttered.	Please add clear description on how you pre-processed the data and generate the k-space inputs, which is important for people to use it and evaluate it. I have a comment on those contrast synthesis works in general: usually, deep learning model can succeed in medical image synthesis because the output information is encoded in the inputs (e.g., High Fidelity Direct-Contrast Synthesis from Magnetic Resonance Fingerprinting in Diagnostic Imaging). But for those T1w -> T2w, I don't think the contrast information of T2w is encoded in T1w, nor FLAIR, which means that its hard to guarantee that the output contrast is authentic, can you elaborate on this point? In the paper, you showed a few examples on from 2 contrasts to 1 contrast, I wonder do you have any experience on generating 2 contrasts from 1 contrast?	The convergence of the learnable algorithm is proved in the paper, which can show the convergence curve of the optimization objective, so as to ensure the consistency of the theoretical and experimental results. Add experiments to verify the advantages of double-layer optimization superparametric design over general fixed superparametric design.	The theoretical analysis, the novelty w.r.t. application and the presented experimental results are advantageous for this work. However, the listed weakness of the work prevented a higher rating.	The authors provide a novel framework for joint reconstruction and image synthesis, which is of interest to radiologist. Though more evidence/analysis on different sampling patterns and acceleration rate is needed, this work is insightful and solid.	This paper proposes a learning algorithm with theoretical analysis, and the generation accuracy of reconstruction accuracy has been greatly improved. However, the main contribution of this paper has been proposed, so the innovation of this paper should be improved.
009-Paper1374	A Medical Semantic-Assisted Transformer for Radiographic Report Generation	The paper introduces a bilinear pooling-assisted sparse attention block and embed it into a transformer network to capture the fine-grained visual difference existed between radiographic images. It can explore higher-order interactions between the input single-model (in the encoder) or multi-model (in the decoder) features, resulting in a more robust representative capacity of the output attended features;  2.The  paper proposes a medical concepts generation network to provide enriched semantic information to benefit radiographic report generation; The paper extensively validates the model on the recently released largest dataset MIMIC-CXR.	First, this paper proposed a memory augmented sparse attention block to capture the higher-order interactions between the input fine-grained image features. Besides, a novel Medical Concepts Generation Network is proposed to predict fine-grained semantic concepts and incorporate them into the report generation process as guidance. At last, this method outperforms multiple state-of-the-art methods on MIMIC-CXR.	The authors propose a medical report generation model using various components such as  sparse nonlinear attention in the transformer, pseudo-medical concepts using RadGraph, and reinforcement learning. This study was evaluated on a MIMIC-CXR dataset, and the importance of each component was justified through structured logic from literature logic as well through experimentation.	"1.The paper injects bilinear-pooling into the self-attention to capture the 2nd or even higher-order interactions of the input fine-grained visual features. 2.To record the historical information, the paper extents the set of keys and values with additional ""memory-slots"" to encode and collect the features from all the previous processes.The paper utilized ReLU instead of softmax unit to prune out all negative scores of low query-key relevance, automatically ensuring the sparse property of the attention weight."	1.Memory-augmented Sparse Attention module combines the memory mechanism and squeeze-excitation operation to capture the higher-order interactions. 2.CLIP is used to extract features of the image, which is not used in this task before. the performance of this method is good.	The motivation for using higher order interaction for extracting fine-grained detail in x-ray images for report generation was well justified and well verified through the experimentation. Coupled with sparse attention, was able to improve the efficiency. The proposed MSA module having memory slots with sparse attention was able to perform significantly better than vanilla self-attention. The medical concepts from RadGraph enhance the performances by providing semantic information. Thorough comparison with state of the art  and ablation with various components introduced in the paper	1.The article only assessed language fluency, not Clinical Accuracy Performance, and we cannot judge whether it has clinical significance. 2.Does the CLIP module work when testing the model? If it works, where does the report that matches the image get it; if it doesn't work, how to extract the regional features of the image. 3.Part 2.2 and Part 2.3 Are the same as the content in Attention(), but the results are different.The loss function calculation formula in section 2.3 does not explain how pk is obtained. 4.The model proposed framework in the paper does not correspond very well to the formula proposed in the article. The inputs and outputs of the modules in the framework are not clear, and the corresponding modules are not labeled for what they do. 5.The MSA module in part 2.1 is very confusingly written, and it is difficult to understand what it means if you have not read the reference [13,19].	1.Fig 1 is not clear. I don't know the meaning of the part of module near the input image. Because the input and output(e.g. Q,K,V) are not clear. Part of Memory-augmented Sparse Attention is similar to [1], but it is not mentioned in paper. Medical Concepts Generation Network is common used in this task, such as [2], although the supervised words are not same. Lack the experiments on IU X-Ray dataset, which is common used in this task. [1] Meshed-Memory Transformer for Image Captioning. CVPR 2020 [2] Visual-Textual Attentive Semantic Consistency for Medical Report Generation. ICCV2021	I found it very hard to follow notations used for defining various variables and weights of a network. It would be helpful to include a chart for it in the supplementary.	The paper did not provide the code, part of the introduction is not clear, not easy to reproduce.	The reproducibility of the paper is good, because the author says source code and the pre-trained models will be made available to the public.	The dataset is publicly available. Authors have said that source code and pre-trained models will be made available to the public.	1.You can draw the frame diagram more carefully and mark clearly what work each module has to do. 2.When you write formulas, you can list them separately instead of confusing them with textual content. You should clearly describe the working mode of the model testing phase. You should explain the working process of the MSA module in more detail. You should evaluate the content of the production report with clinical accuracy.	Some weaknesses are listed in question 5.  The figure may needs edition and paper needs some explainations about similar work before. The experiment on IU X-Ray is lack.	Not clear if metric scores are increasing while using RL is due to training more for 20 epochs or actually because of using RL. For better comparison please provide a score for training the model without RL for 60 + 20 epochs. Dc and nm not explained when defined in section 2.1. mathbfVc in section 2.4 is a typo from latex, please fix it. Organization and notations for various variables are not well explained and are hard to follow. This needs to be fixed.	This paper introduces the image-text matching task into the regional features used to extract images, and the addition of bilinear pools improves the high-order interaction between fine-grained features of images.	The performance of the paper is good, and the problems this paper tends to solve are clear. However, some modules are similar to previous works. Therefore, this paper may need to explain the differences. The experiment part is insufficient.	The paper proposes an elegant way to solve various problems pertaining to image captioning in medical imaging which usually looks very similar. Using higher order interaction for fine details coupled with memory slots improves the representation capabilities for report generation. Further the use of pseudo-medical concepts and RL help in pushing the performance further.
010-Paper1785	A Multi-task Network with Weight Decay Skip Connection Training for Anomaly Detection in Retinal Fundus Images	This paper proposed a new encoder-decoder based architecture that uses (scheduled) skip gating (what the authors called weight decay) and multi-task learning with HOG feature prediction. The proposed method has been evaluated on one public dataset for anomaly detection.	This paper explores the applicability of skip connection and multi-task learning to anomaly detection tasks and presents a multi-task encoder-decoder network with weight decay skip connection (WDMT-Net). The authors first design a weight decay training strategy to alleviate the identity mapping problem of U-Net architecture as well as leveraging its strong capacity in feature representation learning. Then they integrate an auxiliary histograms of oriented gradients (HOG) prediction task to the anomaly detection framework for fully exploiting the significant commonalities of normal fundus images. The quantitative and qualitative experimental results indicate the advancement of the proposed method compared with other state-of-the-art method.	In this paper, the author explore the applicability of skip connection to reconstruction-based anomaly detection.Specifically,a weight decay skip connection training strategy is presented to mitigate the identity mapping problem of the U-Net architecture and meanwhile leverage its advantage on feature representation learning. Then, the author integrate an auxiliary task, i.e.,HOG prediction,to the anomaly detection framework, which can fully exploit the significant commonalities of normal fundus images.	Exploration of HOG feature prediction for medical image reconstruction is indeed interesting and novel. The proposed method achieved the SOTA results on the dataset.	a)   Different from the previous reconstruction-based approaches which are built upon auto-encoder architecture without skip connection, this paper proposes a novel WDMT-Net with an auxiliary HOG prediction task that adapts U-shape network to anomaly detection task. b)   A weight decay training strategy and an auxiliary HOG prediction task are respectively designed to suppress the identity mapping problem and make full use of the shared commonalities of normal fundus images.   c)   Experimental results demonstrate that the proposed method outperforms other state-of-the-art anomaly detection methods. d)  The detailed description of the model and experiment in this paper is accurate and clear. The writing of this paper is coherent and easy to understand.	1.This paper comes up with two questions and do validation experiments : whether the skip connection can be helpful for improving the anomaly detection performance?whether the HOG prediction task can serve the image reconstruction (main task) as auxiliary and assist the anomaly detection? 2.This paper's method demonstrates its effectiveness for detecting abnormal regions in retinal fundus images.	"The technical contributions are not sufficient for a good MICCAI paper. The intuitions brought by this paper are limited, and there are many aspects that the authors could study further. (1) In Eq. 1, weighted feature fusion has already been well studied in the vision domain (a similar study can be traced back to ""Identity Mappings in Deep Residual Networks"", He etal, ECCV2016) and many following U-Net based papers have tried such weighted skip connections since then. However, the only interesting part is that \alpha is gradually decreased in this paper instead of being learnt as others. In this way, the authors should spend more efforts to study the behavior of such decreasing schedule: linearly scheduled, cosine schedulerd piece-wise scheduled etc. (2) The HOG prediction in this work seems like a direct combination of [13] and [17]. Although there is indeed a new attempt (using different resolution HOG as GT), it is more like a training trick instead of a major technical contribution. Experiment on one single dataset does not seem sufficient to fully evaluate the proposed method. Since the primiary contribution claim is the network with skip connections, the authors should compare with more SOTA skip-connected networks (e.g. Attn U-Net, BiO-Net etc.) in Table 1. Also, these works should be cited in the related work section."	"a)  In ""Introduction"" Section, the description of the motivation for the proposed auxiliary HOG prediction task is unclear and insufficient. The authors only stated the effectiveness of HOG prediction for self-supervised representation learning, without explaining the correlation between this auxiliary task and the main anomaly detection task.  b)  In the comparison experiments, except Sparse-GAN and Proxy Ano respectively published in 2020 and 2021, the other competing methods are somewhat outdated. Comparing with these methods can not fully demonstrate the advancement of the proposed method, though it achieves the best performance. In addition to methods designed for anomaly detection, some classic medical image reconstruction framework and method should also be reviewed and re-implemented as comparison methods."	"1.""The skip connections at higher levels tend to mislead the model to bypass the lower levels of features and essentially learn an identity mapping function.""How do you come to the conclusion that the model bypass the lower levels of features? Can you give any theoretical or experimental support it? 2.This paper proposes a weight decay skip connection training strategy. You've not give any references. What is the purpose of the strategy? What is the advantage of the weight factor a?"	Codes are promised to be released soon.	Since the authors described their method and implementation details clearly, this paper has good reproducibility. Besides, they also intend to release their source code related to this work.	This paper seems reproductive.	I hope the authors can have a more thorough study on the proposed components, and conduct more convincing experiments on more datasets and compare with more advanced skip-connected networks.	  The authors are supposed to clearly explain the motivation for the proposed auxiliary HOG prediction task and illustrate the correlation between the auxiliary task and the main anomaly detection task in more detail.   In addition to the reconstruction results and the prediction A_M displayed in Fig.3, we hope that the authors can provide visualization results of the auxiliary HOG prediction task to verify its effectiveness.   The authors should evaluate the advancement of the proposed method by further comparing it with the appropriate state-of-the-art approaches for medical image reconstruction.   	"1.In Section 2.3,""with the normal images Taking a gray image"".You miss a period before the word ""Taking"". 2.In the introduction part of the paper, you mentioned the reconstruction-based and non-reconstructionbased methods. References of the reconstruction-based method you've not mentioned."	Overall, this paper indeed proposed a new approach in this domain, however the contributions are limited and the experiments are not enough. I will consider to improve my rating if the authors can address most of my concerns as stated in the weakness section.	This paper is well and logically written. The main contribution of this paper is that the authors proposed a novel U-Net architecture-based WDMT-Net with a weight decay training strategy and an auxiliary HOG prediction task to simultaneously combat the identity mapping problem and fully exploit the shared commonalities of normal fundus images. However, the description of the motivation for the auxiliary HOG prediction task is unclear and insufficient. Besides, appropriate state-of-the-art approaches for medical image reconstruction should also be reviewed and re-implemented as comparison methods.	The weak reject has been made till current manuscript due to the ambiguity of the motivation and importance of the discounted shortcut. Moreover, the experiments cannot justify the effectiveness of the proposed method.
011-Paper1016	A New Dataset and A Baseline Model for Breast Lesion Detection in Ultrasound Videos	The authors report on the development of a network for breast lesion detection in ultrasound video clips. In addition, the authors collected and annotated a video dataset consisting of 118 videos for use in breast lesion detection and classification.	The authors focus on DL for breast nodule detection and classification in sequences of ultrasound images. They provide a data set of expert annotated 188 videos. They also design and propose a network using local information of consecutive US frames and shuffled ones, aiming at using both local and global features. The network is well explained. The results are nicely analyzed through a comparative and an ablation study. The authors offer the data set and their code to the community.	This paper presents an empirical study on detection and classification of breast lesions in ultrasound videos by utilizing transformer. The general topic of the paper is very interesting specially in the domain of ultrasound videos. The authors further proposed the release of their annotated dataset upon the acceptance of their work. The paper proposed inter- and intra-video fusion blocks based on attention mechanism prior to transformer block.	The dataset and its annotations would be of value to many investigators. The detection algorithm performs well in cased when the lesion is known to be in the video.	The method is novel and well justified and presented in details. The data set is nicely gathered and annotated by experts. The experiments are well done and the results are valuable. The authors offer their data and their code to the scientific community.	Releasing of the data upon acceptance Use of transformers for lesion detection	The use of the method for dtecting a lesion when a lision is known to be present is not clear.	For a conference paper, I do not see a major weakness. For a journal paper, more explanation of the cost functions would be nice.	Use of private dataset Lack of intuitive reasons for the proposed structure design, especially the proposed inter- and intra-VFB modules	OK	These seems to be no problem with reproducibility. The random shuffling and then taking the consecutive frames within the randomly ordered frames could be improved. The fact that the authors will provide data and code definitely supports the reproducibilty.	There has been no downloadable link for the dataset used in the paper.	Experimental results In the introduction, you mentioned the need for detection and classification. However, for detection you would need to include video clips without lesions. Since all the training videos contained  lesions, the task you are solving is a detection in the case that a lesion is known to be present in the frames. It would be good the make that clear. As well, to use your algorithm, a physician would need to identify a lesion first and then use your method to detect it in the frame. Since the physician would need to collect a video of a region with a lesion, then its identification has already been done and an algorithm is not needed. Thus, the value of a detection method would be if the algorithm could detect lesions in videos with and without lesions with a high true-positive rate and low false-negative rate. An explanation of the value of detecting a lesion in videos, in which it is known that a lesion is present would help too clarify the value of the method. How many images were typically collected in the videos. How did the physicians annotate the lesions as benign or malignant? Were the lesions classified by a biopsy or visually? If visually, what is the variability in generating the annotations of the images.	Please either try to justify better your choice of the random shuffling or think of providing a more systematic approach making sure that the method can always take advantage of the global features, if you strongly beleive that these play crucial roles. Thanks for providing enough details and doing the ablation studies.	"Despite the authors' interesting comparison and ablation study, the fact that they used a private dataset, even though the dataset is supposed to be released upon acceptance, makes the judgement of the true effectiveness of the proposed structure difficult, especially given the ever-growing number of studies in the field.  Of course, half of the paper's contribution is devoted to releasing the dataset, the other half is devoted to the proposed structure design, which lacks specific explanations on how and why the proposed modules are useful. Although, the main focus of the paper is on ultrasound videos, it will be highly beneficial to the impact of the paper if authors experiment their proposed architecture design on different video datasets (either natural or medical). Because several aspects of the methodology were difficult to follow, giving more specifics would be advantageous. For example, it was stated that: "" For a current video frame (It), our CVA-Net takes three neighboring images (denoted as Ik, Ik-1, and Ik+1) "", and ""Figure 3(c) shows the details of our intra- video fusion module, which further integrates three output features (Pk-1, Pk, and Pk+1) of inter-video fusion from three adjacent video frames.""  The use of subscripts (k-1,k,k+1) and superscipts (1,2,3) for ""neighboring images "" and ""adjacent video frames"", respectively, is confusing.  If subscripts relate to consecutive frames/images, please make sure your manuscript is consistent in this regard."	The use of the method for detection breast lesion in cases that it is know that a lesion is present requires justification.	Well justified and novel methodology. Excellent annotated data. Well done experiments and ablation studies. Offerning the annotated data and code to the community.	The anticipated dataset release was the main reason of my comments. The justification of the effectiveness of the proposed structure is limited.
012-Paper0202	A Novel Deep Learning System for Breast Lesion Risk Stratification in Ultrasound Images	The paper presents new architecture for training a cancer prediction in breast ultrasound images. This architecture has several key-elements:  (1) dual-task prediction of cancer label (pathology) and the BI-RADS score (radiologist)  (2) teacher-student architecture, with a potentially novel way to derive the soft labels from the teacher's prediction (3) consistency loss on cancer and BI-RADS predictions, as the two labels are usually correlated (4) cross-class loss on BI-RADS predictions, as the 6 BI-RADS classes are ordinal, so the loss depends on the distance from the true class and the predicted class. Evaluation results include comparison to single-task state-of-the-art architectures, and an ablation study.	The paper presents a framework for classifying the BI-RADS score (malignancy risk stratification) as well as true (path-proven) malignancy of breast lesions from ultrasound. The framework uses a teacher-student model, where the teacher provides soft labels to the student. The student is also required to output consistent predictions for BI-RADS and path-malignancy (Consistency Supervision Mechanism), and is penalized more strongly for greater errors (Cross-Class Loss Function). The authors demonstrate this framework outperforms current approaches, which typically train a model to jointly classify BI-RADS and path-malignancy.	The paper presents a novel use of soft-labels to improve the breast lesion malignancy prediction and BI-RADS category prediction. The soft labels are generated using a teacher network which intern trains a student network. Two loss functions are proposed, CSM to enforce consistency between the 2 tasks and CCLF to penalize large deviations in BI-RADS class.	Adding BI-RADS as an additional task to the pathology is novel, to the best of my knowledge, as well as the consistency loss. In the presented results the proposed architecture has superior performance compared to other architectures.	Very easy to understand, reasonable model design. Great datasets. Comparisons and ablations are fairly thorough. Results show substantial improvements on many metrics	The paper is well written and has good experimental validation of claims. The proposed approach shows good improvements over other baselines and can have broader applicability to other domains. Abalation study showing the gains from the proposed loss functions and student-teacher method is provided. CAM analysis shows the intended benefits of the loss functions are observed.	There are several parts in the paper that were unclear to me, such as the derivation of the soft labels, handling multiple images by the same patient, and handling inconsistencies between BI-RADS and pathology.  See detailed comments below. It is unclear why different tests were done on the 3 datasets. The results were much stronger if the proposed architecture was showing superior results in all the three datasets - using the same tests. In other words, I would expect the comparison to state-of-the art and ablation study to be conducted in the three datasets that were used in this study.	"Only minor weaknesses: there are 3 different techniques used in this paper, each of which is reasonable, but none of which are particularly novel or explored in great depth. they also seem to be generally applicable to many multitask learning problems (beyond medical vision), so it would be interesting to know how they have been used elsewhere clinical utility is maybe unclear. Comparison to radiologist performance and analysis of failure cases would help on this front. It may be helpful to have a sense of how well radiologists can predict whether a lesion will be malignant, as well as inter-reader agreement. Is this framework also useful for mammograms? I'm not familiar with multitask learning but I suspect there are stronger baselines than just ""train on both tasks simultaneously"""	The equations of the proposed loss functions are not clearly specified, leading to the following confusions: In equation-1, it is unclear if the SLB, SLP are calculated per lesion. It is confusing while reading as the equations depict that the soft-labels aren't a function of neither the lesion nor the image. This needs to be clearly stated. In equation-3, the loss always sums up to a negative value? As the terms are all probability values in the interval [0, 1]. It is more adequate to call it a reward than a loss. In eqn-1, its unclear why Ni and Nj are different, shouldn't they be equal? This suggest that images of the same lesion have different BI-RADS scores? In equation-3, it needs to be clarified that the benign and malignant probability sum to one i.e. \(p_B + p_M = 1\) as the check for benign is \(p_B > 0.5\).	Most of the presented methodology is clear and easy to reproduce. However, reproducing the results may be difficult due to the following unclear parts that I mentioned above, namely: (i) derivation of the soft labels, (ii) handling multiple images by the same patient, (iii) handling inconsistencies between BI-RADS and pathology.	No code provided. 2 public datasets, 1 private.	The paper states relevant hyperparameters and present results on 2 public datasets. Though the code and inhouse dataset aren't released.	"It is unclear how the authors handled cases in which the BIRADS (radiologist's assessment) and pathology (cancer/benign) don't agree, i.e., cancer with low BIRADS, or  benign and high BIRADS.  In many cases a suspicious finding is found to be benign, and mammography may reveal additional suspicious findings that are not shown in ultrasound (e.g. calcifications). Were such cases excluded from the training? How were the probabilities in Figure 1 computed?  If they were taken from a paper, please add a citation to the reference in the caption of the figure. The computation of the soft labels is unclear: a. Equation 1 is unclear:  what are the two summations on?  is the summation limited to a single patient, or involves all patients? what does the ""while"" mean?  Is there any iteration involved?? b. The following sentence is unclear: ""N_i and N_j represent the total of the images being counted in corresponding sum formula.""  What is the criteria of these images?  Does N_j stands for N_0 and N_1, and these are the number of cancer and no-cancer patients or images respectively? c. The task-correlated labels are then used to train the student model that has the same How did you handle the issue of multiple images by the same patient? Was the presented evaluation (AUC^P, AUC^B, and other measures) made on the set of images or the set of unique patients?  Did you perform any kind of aggregation on the images from the same patient?"	Would be nice to have an ablation for CSM + CCLF Consider renaming Table 4 - Teacher to RepVGG-A2 to remind readers that it is the same as that row in Table 2. How do you calculate AUC^B since BI-RADS is multi-class? What does [3] do to get better performance on AUC?	"The average number of images per lesion should be mentioned. In ""Implementation Details"" section the authors state that the images are resized to 224x224 but the ""aspect ratios remained to unify input size"" this statement needs more clarity as its not evident how the aspect ratio is maintained while resizing to a fixed size. In Fig-5, the CAM method used should be cited."	The two factors that affected my decision were the unclarity of important issues in the methodology, and the inconsistent evaluation on the three datasets.	This paper is a solid approach for classifying malignant breast lesions. Probably most useful for clinical researchers and clinicians, but other medical vision researchers who work on multiclass learning tasks can also draw inspiration from this approach. Weaknesses are fairly minor.	The proposed method is novel, improves over baselines and has sound experimental validation. The results are validated on public datasets and ablation is conducted to show the utility of each component of the method.
013-Paper0975	A Novel Fusion Network for Morphological Analysis of Common Iliac Artery	This paper proposed a new method for morphological analysis of the common iliac artery. This paper provided a new framework that combines CNN and Transformer, addressing the challenging task of CIA morphological analysis. This paper is the first automatic approach to morphological analysis of the common iliac artery.	The paper proposed a new module Cross-Fusion block and Transformer structure. And the paper designed to provide a new solution for common iliac artery segmentation and morphological analysis tasks.	The authors propose a novel fusion network, combining CNN and Transformer is proposed to address the morphological analysis of common iliac artery issues. The experiment demonstrates that their method outperforms the best previously published results for this task.	The application is novel. Morphological analysis is a sound idea. It is good to see that the authors attempted to use the ablative analysis to back up the claims regarding the proposed network modules.	Clear description: The structure diagram and formula are clearly described, and the method part is clearly organized. Clear motivation: Fluent writing and clear clinical meaning.	The paper designs an interesting way that combines CNN and Swin Transformer for the ends of the common iliac artery and the edge pixels segmentation. The authors propose a morphological analysis algorithm to obtain anatomical information on the common iliac arteries (the minimum inner diameter, access diameter, and tortuosity).	"I feel that there is little methodology advance from a technical point of view. Although I found the work interesting and potentially very useful, my major concern is on whether analysis morphological algorithm can ""obtain access diameter, the minimum inner diameter, and the maximum curvature of CIA with arbitrary shapes."" Otherwise, I think the experiment is far from validating this strong claim."	The comparative experiment is not sufficient and lacks the comparison and analysis of the number of parameters and computation (e,g, FLOPS) There were some minor mistakes.	The design of the method mainly focuses on the combination of CNN and Swin Transformer, which is weakly related to morphological analysis of the common iliac artery.	Paper is clear that an expert could reproduce	No source code is given. But, the method section of the paper is introduced so that readers can hopefully repeat or refer to it.	The description of the proposed method is clear and the paper is reproducible.	"Limited innovation in methodology. The U-shaped and Cross-fused is actually not new. They are simple modifications and applications of [12] and [3], respectively. Insufficient results for validation. Could you add more experiments to show the accuracy of morphological? The paper is the lacks evaluation related to comparing the quantitative results between anatomical information and the ground truth. The author claims ""the shape and size of the iliac leg are important indicators"", which require some references. The method is complicated and has included many modules. However, there are modules, such as Hybrid Decoder, that are not well validated. There are some misspellings in the manuscript. e.g."" the maximum value of the remains represent the largest tortuosity of the vessel""."	Is there any comparison between the method in the paper and criss-fusion Block[15] or other attention methods, such as spatial attention, CBAM(Convolutional Block Attention Module), etc. It is recommended to add comparison of speed or calculation amount with Unet or other networks. Minor problems: Page 4: Ps R^(H+W-1)x(WW) , I think it should be HW Is there a quantitative analysis for calculating vessel diameter and curvature? For example, comparison with manual measurement through radiologist, or, clinical significance and guidance.	"The proposed morphological analysis algorithm isn't outlined in Abstract. ""make it difficult"" is changed to ""makes it difficult"" in Abstract. The chaotic topological order of modules in Fig. 1 makes the figure unclear at a glance. The relationship between the method and task can be better explained. ""represent the largest tortuosity"" is changed to ""represents the largest tortuosity"" In Diagnosis Algorithm"	Although the clinical hypothesis is meaningful, the paper lacks technical innovation. More importantly, the paper is the lacks evaluation related to comparing the quantitative results between anatomical information and the ground truth.	This paper has a clear motive and structure, and the network design is also innovative. There are details that can be improved and modified in the experimental section.	The proposed network that combines CNN and Swin Transformer is appealing. However, the task relevance of the method can be better designed.
014-Paper1405	A Novel Knowledge Keeper Network for 7T-Free But 7T-Guided Brain Tissue Segmentation	The proposed method fuses features from 7T-like images and 3T images to segment brain tissues. As 7T images are not available simply compared to 3T, the proposed method use 7T features without directly using 7T images.	This work proposed a knowledge keeper network to guide brain tissue segmentation by taking advantage of 7T representations without explicitly using 7T images.	The author proposed a novel knowledge keeper network (KKN), trained via KD and KT, to guide and train a brain tissue segmentation model using 7Tlike representations in a 7T-free domain.	1) Clear presentation and well written manuscript	This work presents a knowledge keeper network to extract 7T-like representations from 3T image, without paired-7T images during training.	The method is novel and effective. They propose a novel knowledge keeper network (KKN) to extract 7T-like representations from a 3T image that can further guide tissue segmentation to take advantage of the contrast information of 7T without 7T images.	"1) Novelty is limited. Knowledge transformation and synthesizing 7T from 3T images have been used a lot in the literature. 2) I expected to see comparison results with methods that ""synthesizing 7T-like image first and then conducting segmentation"" to justify the proposed method. 3) In Table 2, the authors only showed the effectiveness of plugging 7T-like features in the segmentation frameworks, e.g., the 3D UNet framework. However, their proposed method has used extra data to train the teacher network compared to 3D UNet and DSC improvement is not significant (around 1%)."	"Figure 1(c): what is the difference between Input 7T images, Reconstructed 7T images and Target 7T images? More information are required to understand how the teacher network works. Section 2: ""Through two-stage KD methods, KKN learns to infer 7T-like representations from a 3T image using a paired 3T-7T dataset {X,Y}"", why paired 3T-7T dataset is needed during training (Figure 2(b)). This conflicts with authors' statement. Figure 3: By comparing the result of the proposed method and ground truth, I do not think the result has 7T representations. Only the intensity distribution is similar with 7T images but it does not have tissue details that 7T images have."	The author claims that existing methods lack applicability and generalizability on datasets not including 7T images because they require pairs of a 3T image and its corresponding 7T image for training. But in this manuscript, the author still needs paired data to train the teacher model. How to explain that? Besides, I don't know what the practical application is. In my opinion, the task is to segment brain tissues, why don't you segment those directly by developing a better approach? Although the performance will be better if you translate a 3T image to a 7T image, you need 7T images to train the KNN, which is almost not available. So I think the practical application is limited.	based on the checklist, yes.	No source code was provided.	Good. The author has opened the code.	"I suggest appending the comparison results with methods that ""synthesizing 7T-like image first and then conducting segmentation"" to justify the proposed method."	Provide more information to verify the effectiveness of the teacher network. Please further specify how obtain 7T representations from 3T images. (Figure 2 (b)) Recovery tissue details of 3T images are more important than simply simulating intensity distribution.	The author claims that existing methods lack applicability and generalizability on datasets not including 7T images because they require pairs of a 3T image and its corresponding 7T image for training. But in this manuscript, the author still needs paired data to train the teacher model. How to explain that? Besides, I don't know what the practical application is. In my opinion, the task is to segment brain tissues, why don't you segment those directly by developing a better approach? Although the performance will be better if you translate a 3T image to a 7T image, you need 7T images to train the KNN, which is almost not available. So I think the practical application is limited.	Lack of proper comparison results.	The simulation of 7T representations for 3T images is not clear.	I am confused about the practical application.
015-Paper1881	A Penalty Approach for Normalizing Feature Distributions to Build Confounder-Free Models	The authors propose an alternative optimisation scheme to impose the orthogonality constraint proposed in the MetaData Normalization paper.	This paper proposed a penalty term for MetaData Normalization. Unlike the original linear regression based method, the PMDN learn the projection \beta using a neural network. Experiment shows improvement over the baseline method MDN on several dataset.	This paper proposed a Penalty MetaData Normalization (PMDN) method. PMDN extend the conventional MDN whose performance is confined by the sample size of a mini-batch, by using a penalty method so that the linear parameters within the MDN layer are trainable  and learned on all samples. The results show that the proposed PMDN method can improve the classification performance compared to other four methods.	Learning representations that are free of confounding factors is an important aspect for a wide range of tasks in medicine and beyond. The evaluation is extensive and illustrates the proposed scheme is less affected by smaller batch sizes than the original approach.	This paper slightly improved from MDN to learnable and trainable version, to better perform the regression and handle the difficulty of large batch size.	Compared to conventional MDN, the beta in proposed PMDN is learnable and  is not confined by the batch size. The figure 1 is very clear and is helpful to understand the overview of this paper. The writing and organization of this paper is good and easy to follow.	"Related work on learning confounding/bias-free representations is very limited. The authors only refer to adversarial learning schemes (refs. 11,18), but a large amount of literature exists beyond that. Most importantly, the authors should refer to other works that are based on orthogonalisation too: (i) Tartaglione et al. ""EnD: Entangling and Disentangling Deep Representations for Bias Correction"". CVPR 2021. (ii) Neto. ""Causality-aware counterfactual confounding adjustment for feature representations learned by deep models"". (iii) Liu et al. ""Projection-wise Disentangling for Fair and Interpretable Representation Learning: Application to 3D Facial Shape Analysis"". MICCAI 2021. The smallest batch size that has been studied is 80, however for many tasks, in particular involving multiple 3D volumes, a batch size of 80 is infeasible. Studying very small batch sizes (<20) would be helpful to understand whether the proposed approach would still be effective in this setting."	Theoretical analysis of PMDN is not enough. In section 2, PMDN seems to be using a neural network to replace the linear regression and simply by adding a loss term during training. The contribution of this method is not enough unless further analysis, including convergence rate, theoretic bound of the error, etc., is developed. Experiment baseline is weak. There are many other method (from Fairness deep learning) can be performed and compared with the proposed method. For example, Canonical Correlation Analysis (CCA), and Renyi Fair Inference. This aspect is not considered in the current paper, nor does the author discuss the pro and cons of this aspect.	"According to f - Mb, the metadata should have the same dimension as f. But the representation of different metadata has different dimensions: acquisition site  is one-hot encoded, the age is z-score and the sex is a binary number.  It is not clear how to feed the metadata into the model. As beta is trained based on both the classification loss and the penalty loss, it is not clear if Mb contains all and only metadata related information. Figure 3 only compare the results under a small batch size equals to 80 which is unfavorable for MDN. For fair comparison it is better to also compare the tSNE results under other batch size. There are some typos, such as ""... the matadata of each group separately and report ..."" This paper has no keywords."	Reproducibility is fair, except for the choice of lambda, which has not been described.	This paper should be reproducible.	The code and data can be released according to the reproducibility checklist.	"The proposed work is heavily inspired by the previous work entitled ""MetaData Normalization"" (MDN). The MDN approach is removing bias from learned feature representation by using metadata, capturing confounding variables, to estimate the residuals of latent representation and metadata, such that the confounder-free representation is guaranteed to be orthogonal the space spanned by the confounding variables (metadata). While residuals can be obtained in closed-form, such an approach becomes unstable when using batch-based learning. The authors propose to address this problem by replacing the hard constraint (orthogonality) with a soft constraint and performing alternating optimisation. The proposed approach seems to be effective in removing confounders and to be more stable than the original work. However, there are some issues that if addressed could improve this work. As mentioned above, the discussion on related work is insufficient and has to be extended to give a comprehensive overview on learning confounding/bias-free representations. The authors write that the benefit of MDN, and therefore the proposed PMDN, over adversarial approaches is that it can remove confounding effects from multiple layers. I am not sure whether this is a valid argument. If I remove the bias only from the latent representation prior to the network's final prediction layer, I would guarantee that the network's prediction is confounding-free. From a theoretical perspective, I do not see any benefit from removing bias from intermediate representations. In fact, ""Causality-aware counterfactual confounding adjustment for feature representations learned by deep model"" by Neto follow this approach. Several assumptions implied in MDN are not made explicit or are discussed. Three crucial assumptions are that (i) the confounding variables (metadata) must block all backdoor paths between image/latent representation and the outcome to be predicted, (ii) the confounding variables influence the latent representation linearly, and (iii) that the confounding variables must be linearly independent. It would be best to be upfront about these assumptions and discuss their rational. Since the hard constraint of orthogonality is replaced with a soft constraint (via regularisation), lambda in eq. (7) effectively determines to which degree this constraint is enforced. A study on the effect of lambda on learning a confounder-free representation would be helpful to understand whether there is a downside to going from a hard to a soft constraint. The authors claim that the original MDN suffers from instability, yet the experiments on synthetic data in section 3.1 have not been carried out repeatedly to demonstrate the variance of the proposed PMDN is indeed lower than that of MDN."	The iterative updating of W and \beta can be performed as Block Stochastic Gradient Decent, and thus can have better theoretical convergence. Another important baseline is Canonical Correlation Analysis (CCA) which maps the MetaData and the learned features in the same domain and compute the correlation. If we minimize the correlation, we can guarantee that the data is independent to the MetaData, which shows the same benefit as the proposed method in this paper.	Q5	The proposed approach seems to be effective, however, more related work and a discussion on the assumptions implied in MDN are required.	The method is slightly beyond the MDN baseline and lacks the analysis of the proposed method. The experiment requires further comparisons between different other methods.	The writing and organization of this paper is clear. But some part of the paper does not follow the template (no keywords). The evaluation of this paper  is not enough, figure 3 only show the tSNE under some specific batch size.
016-Paper2636	A Projection-Based K-space Transformer Network for Undersampled Radial MRI Reconstruction with Limited Training Subjects	The authors propose a new  deep-network, based on Transformers, to reconstruct  radially-sampled K-space data.	The main contribution of this paper is the projection of k-space and the application of the transformer in utilizing both the real and imaginary parts of the k-space in prediction in the k-space domain.	o The paper proposes a method for reconstructing GRASP MRIs by arranging the radial k-space lines as a sequence (in order of acquisition time). This sequence is then fed into a transformer network which outputs the fully-sampled radial k-space as output. Further, the paper explores multiple data augmentation techniques to improve performance. This model obtains superior results compared to previous methods on a small dataset containing scans from multiple anatomies.	The network seems to perform well. Using the raw k-space data to predict missing samples is a really interesting approach and the application of  Transformers in this context seems very relevant.	The main strength of this paper is the application of the transformer network in predicting the skipped radial spokes. The radial spokes were acquired sequentially which makes the transformer network a good fit.	The idea of treating the radial acquisition lines as a sequence is novel. It allows the authors to use the transformer network for this problem which is not obvious.	The description of the method is, at times, hard to follow. The authors could have  used cross-validation to evaluate their  results.	The real and imaginary part was stacked together in a big array for network input. Should they be separated for two networks and combined? If this is the better case, a comparison is needed.	The model was compared against baselines like U-Net and squeeze and excitation network that were not designed for this problem. A better comparison would be to compare against a model like XD-Grasp. Without such comparison, it would not be possible to determine how well the proposed model performs.	No data or software  provided. Hard to reproduce the algorithm based on the  description.	The authors did not comment on the reproducibility.	The authors used a private dataset which makes it impossible to reproduce. They do list some of the hyper-parameters used for training the model.	Given that the authors effectively interpolate data in  k-space, it would be interesting to see the accuracy on that domain. How well does this mehtod generalize to further undersampling? Is the ordering of the data essential for MR reconstruction?	In Fig. 1b, the spokes were inverse FFT transformed to generate a projection vector. What is the projection vector mean and how does the vector be generated? Was the data consistency be implemented via the projection step? Based on Fig. 1e, it's not clear where the data consistency was implemented. In the 3.1 data sets section, it's not clear whether the radial imaging was acquired in a full sampling scheme or undersampled. Were the training and testing based on fully sampled images and retrospectively undersampled images? After using the transformer network to predict the radial spokes, how the final image was reconstructed?	Provide comparisons against relevant baselines, so it would be possible to properly judge the merits of the proposed method.	This is a very interesting approach with possible extensions in the future.	The recommendation is based on the method description and the results presentation.	While the presented method is novel, it was compared against previous models like U-Net that were designed for 2D images and not for handling radial scans of MRIs. As such, it is not possible to properly judge the merits of this method.
017-Paper0545	A Robust Volumetric Transformer for Accurate 3D Tumor Segmentation	This paper proposes a 3D pure transformer architecture, called VT-UNet, for volumetric medical image segmentation. The network can directly work on 3D volumes. The authors design an encoder block with two consecutive self-attention layers for feature extraction and a decode block with parallel cross-attention and self-attention to recover the learned features for segmentation. Experimental results on a large MRI dataset demonstrate its superior performance compared to other baselines. The author also did a robustness analysis to show the VT-UNet is more robust to artifacts.	The paper proposes a transformer architecture for volumetric segmentation with a new design that encodes local and global features. The method obtains competitive performance in BraTS data.	The author proposed a decoder block, which uses one shared projection of the queries and independently computes cross and self-attention. Besides, The author combined a lot of mature and well-known structures, and the performance of the network outperforms other SOTA.	1) The idea of designing a 3D pure transformer for medical image segmentation is novel, given that 3D transformer is not easy to train and requires careful design like the parallel cross-attention and self-attention. 2) The effectiveness of the proposed method is demonstrated on a large MRI dataset with better performance than all the other pure CNN baselines and CNN+transformer baselines. 3) The paper well-written and structured, the figures help a lot on illustrate the ideas.	1) VT-UNet has very less number of parameters and computational complexity when compared to the other recent works while getting better performance. 2) The concept of introducing parallel cross-attention and self-attention in the expansive path to create a bridge between queries from the decoder and keys & values from the encoder is good. 3) The proposed method is purely transformer based and improves upon the previous works and shows clear distinction when compared to them. 4) The paper is neatly organized and clear to understand.	The high performance comparing with other methods is the key strength of this paper, and the model size is also excellent. The shared projection of the queries and independently computes cross and self-attention is an interesting contribution.	"1) In the introduction, the authors mention that the proposed method has fewer model parameters and lower FLOPs compared to existing method. While from Fig.1, there is no information about the FLOPs, the number of parameters and the FLOPs are not necessarily the same thing. 2) It seems strange that the authors did not mention any data augmentation in the experiment setting. If no data augmentation is used for all the comparison, this won't be ""fair"", given that data augmentation is a common technique that people use to trained DNN models nowadays. With data augmentation, the gap between different methods could be smaller because almost all the methods will become better. 3) Transformer usually needs a large dataset for training, when there is only limited data available, the proposed method may not be better than CNN, which could limit the usefulness of this method."	1) Ablation study is missing. I believe experiments which show the increments in performance obtained because of the 3D shifted windows, K,V skip connections would be useful. 2) Not much novelty in terms of encoder. 3) I am still surprised by the drop in complexity. Structure wise I do not see anything much different that would actually reduce the complexity as reported in the paper. I read the supplementary where the authors discuss about the same. I think more clear discussion about the complexity reduction and the reason behind the same should be present in the main paper.	The authors spent a lot of content introducing the existing work, although these structures are parts of the network. For example, the shift window,  relative positional bias, patch merging, and patch expanding are very famous existing work, but the authors focus on these contents as a subtitle. The author declared that they propose a convex combination approach along with Fourier positional encoding, while there are few detailed introductions about the Fourier positional encoding in this paper.	Code is given and data is public,	Code is present in supplementary.	Great. The used dataset is public data, and the code is available.	1) Some baselines' results deviate too much from their original papers. For example, the Transunet and UNETR, in their original paper, they are both better than UNet and nnUNet. However, in Table 1, their accuracies are much lower than nnUNet. The authors need to discuss a little on these results. 2) Why are Fourier feature positions used in the decoding process? Why not use no position information or normal position information? The authors need to do ablation study on this design choice. 3) In Equation 5, why alpha=0.5?	Ablation study and more discussion on the reason behind reduction in parameters and complexity can be added to make the paper look better.	It is better to spend more content to introduce the new-designed work and the true contribution of this paper. The authors should add some explanation and analysis of the model parameters and size. There are few detailed introductions about the Fourier positional encoding in this paper. The authors should add more details about it.	Overall, the paper is well-written. The idea of designing a pure 3D transformer for medical image segmentation is interesting, this paper could be interesting to the audience of MICCAI 2022. My biggest concern is that no data augmentation is used in the experiments. This would affect all the comparison with other baselines.	Performance and Efficiency.	The performance of this network is great, but the content of this paper did not focus on the true contribution.
018-Paper0114	A Self-Guided Framework for Radiology Report Generation	They presented a self-guided framework to obtain the potential medical knowledge from text report without extra disease labels. It can assist the network in learning fine grained visual details associated with the text to alleviate the data bias problem.	* The paper proposed to use labels from unsupervised clustering of radiology reports to learn visual feature-extractor from medical images. This approach alleviates the need of supervised image-level labels to train the visual feature-extractor.  * The learned visual features are then used in a transformer-based caption generator to write the corresponding report. * The report generator further used a supervised cosine similarity loss to compare the generated and ground-truth report.	Novel use of mixed image and text based strategies to build robust models that can generate reports without the need for tedious pixel level annotation and image captioning.	(1) The writing of the paper is very standardized and logically clear, and it is easy to read. (2) Figures and tables are very clear and detailed explanations are given. (3) The experimental part is adequate with detailed analysis.	* Unsupervised clustering acts as knowledge extraction from domain-specific knowledge source i.e, radiology reports. * The reuse of report-embedding extracted from sentence-bert in report generator for supervised cosine similarity loss is interesting.	The paper is written methodically and provides step by step description of the various stages of model development and demonstrates validation by help of visual representations ( Fig 3, heat maps) which allows easy co-relation between observing visual features and corresponding text generated.	(1) Many mature algorithms are used in the article, and the innovation is not outstanding enough. (2) How are multiple parameters in the network determined? Some clarification should be given.	"* The paper lacks motivation on why the proposed approach is good for medical report generation. I understand the approach don't require image-level disease labels, but the main building blocks of the models are not necessarily trained for medical domain. They are mostly pre-trained on natural images and just applied on medical data.  * The experiments don't evaluate performance on preserving negative mentions. A major differentiator between medical report generation and image-captioning on natural images is ""negative mention"" in medical reports. Two sentences can have high similarity score (calculated based on word overlap), while having an opposite polarity. For example, ""Lungs have pleural effusion"" vs ""Lungs have no pleural effusion"".  * The authors claimed to ""extract fine-grained visual features associated with text"" is supported by qualitative experiments in Figure 3. That is not enough to support the claim. * The heatmaps in Figure 3 not localized and covers most of the image. For ""no"" the heat map is highlighting upper lobe. It's not clear how this is a good result? The ""no"" in the sentence is in context with pleural effusion and pneumothorax. Both diseases are in lower lobe regions as highlighted in the last heat-map. * The method section of the paper reads more like putting different blocks together and lacks motivation on the need and the design of those blocks. For example, knowledge distiller is required for unsupervised clustering of the reports, but why it is designed to have a Bert based embeddings? Are these embeddings better than other methods for medical reports. Does fine-tuning Bert on medical reports would help in getting better domain-specific embeddings? Why was dimensionality reduction used? Why not cluster the reports using the report embedding? Is there some transformation that UMap can learn but not Bert? Why HDBSCAN clustering was used? How to define number of clusters? How to make sure the clusters are distinct enough?"	The self guided framework is a novel and useful approach to overcome some of the described challenges, however few questions remain.	The paper does not provide a link to the code. The description of the paper is relatively clear and generally reproducible.	Paper will be reproducible if the authors release their code.	The authors describe the model development methodology in detail and demonstrate ablation study results. Would be ideal if access can be provided to code or tools used for study.	(1) Please clearly state the difference from previous studies. (2) The authors need to carefully discuss for which types of image samples your method is more effective and for which samples it fails. (3) The authors should state what the flaws of the method in this paper are and what future work is possible.	"* It's not clear what ""data bias problem"" the authors are referring to, throughout the paper. * In Knowledge distiller (KD), the dimension reduction is mapping the report embedding to a 2-d space. It is not clear why authors mapped to a 2-dimensional space. Are the UMap embeddings used in some 2D visualization of reports clusters? If the final goal is to do unsupervised clustering, then the authors should consider doing a hyper-parameter tuning to evaluate the best embedding dimensions before clustering.  * The experiments don't evaluate the quality of clusters extracted from knowledge clustering. Figure 3(c) is too small to draw any meaningful conclusion. * At multiple times, the manuscript lacks proper details and have very generic statements: o Recently, some works [7-9] have been proposed for more specific tasks. Which tasks? o Researchers have made a lot of attempts and efforts to fill these gaps. Which gaps? o SB is a multi-layer bidirectional transformer that has been well pre-trained on two large and widely covered corpora [19, 20]. Which corpora? o One reasonable explanation is that our method ignores some meaningless words and pays more attention to the long phrases used to describe diseases. Which meaningless words? Please provide examples. o KMVE can help the framework to distinguish image details and alleviate the data bias problem. It's not clear how? * In Ablation study, how image-features are calculated for Base transformer without KMVE? * The text reads: For the meaningless words like ""the"", ""are"" and ""."", our model assigns relatively low probabilities. While in Figure 3 (a), the probability of . is over 0.9.  * The paper is over-loaded with abbreviations that makes it difficult to read."	1) How does the similarity comparer in RG overcome the image-text data bias? If one image can be reported in numerous ways by different radiologists, then which one are the model generated reports compared to?  2) The three step process- KD, KMVE and RG should work well for a given imaging modality and a specific lesion detection or classification task. Would it generalize across modalities or transfer to new tasks? To what extent would the model need retraining? 3) Does the number of neighbors and minimum distance set in Dimension Reduction have any bearing on why BASE+SC performs better than SGF for BLEU 1? Would setting larger number of neighbors allow KMVE to understand semantic relationships between distant words?	The structure of the paper is very clear, the introduction is very detailed, and it is easy to reproduce.	The paper is novel interns of its pipeline and application.	Novel mechanism of using a mix of report and images for model training and using similarity comparer as QA step (alleviating need for humans for labelling or QA).
019-Paper0199	A Sense of Direction in Biomedical Neural Networks	"The authors propose an interesting convolutional module that combines local rotation and local scale encoding. They build upon [1] and propose three novelties to improve the design of [1]:  A calibrated Response shaping instead of the response shaping proposed in [1].  An additional loss term that promotes the rotational symmetry of the learned kernels by computing a distance from the kernels at different orientations to a set of reference Gabor functions. An additional module, called index map featurization allows encoding the local scale and local orientation of the input and propagates this information to downward layers. [1] Liu, Zewen, and Timothy Cootes. ""MASC-Units: Training Oriented Filters for Segmenting Curvilinear Structures."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021."	The paper presents an improvement of the MASC method which introduced computation of oriented filters in CNN networks to detect curvilinear structures while significantly reducing the needed parameters compared to having to learn each orientation individually as in standard CNNs. The main improvements of the method proposed in this paper are a new regularisation function added to the loss which aims at better balancing orientational relationships between kernels as well as integrating pyramid representations to handle responses at different scales.	This paper proposes a network, called G-MASC, to encode rotation and scale into the convolution to adapt the model to vessel segmentation and histology segmentation tasks. This explicit encoding can reduce the redundancy of kernels in traditional CNN, thus largely lowing the size of the network. Information on the direction and scale can benefit the analysis of curvilinear structure, e.g., retinal vasculature, and authors evaluate the proposed model with MoNuSeg, DRIVE, and CHASEDB1 datasets.	The two strengths of the paper are the novelty and relevance of the approach as well as the experimental evaluation which is thorough.	it improves an already well performing state of the art method a comprehensive evaluation has been done on public datasets which highlights improvements and limitations of the method	This work points out that MASC [8] sometimes is unstable and proposes to solve it by calibrated response shaping and a phase-offset rotational error (PoRE). This paper combines the convolution operations with the orientation information, like using Gabor filters. The proposed model has about 1% parameters to U-Net and shows comparable performance. Experiment datasets contain two retinal fundus image datasets and one histology image dataset. The proposed G-MASC shows comparable performance on DRIVE, and works better on MoNuSeg, comparing to MASC. PoRE design is reasonable to regularise the filters to keep local relationships, which is also verified in the ablation study.	The authors failed to convey clearly and precisely their methodology. A lot of details are missing to fully understand the approach.	on the tested datasets, there are only minor differences between MASC and the presented method which raises the question whether the addressed limitations of MASC are major the new formulations to handle scale differences do not result in significant performance improvements, at least not on the used dataset	It is claimed that the original MASC used the response shaping function which authors found to be unstable in some cases, but this has not been verified or clarified in which scenarios. From the experiment results, it is hard to say that MASC is unstable (MASC even performs slightly better than the proposed G-MASC on CHASEDB1). The difference between the original response sharing and the proposed calibrated response sharing is unclear to me. The original one calculates the response across different filters w, while the proposed one seems successively update M with learnt kernels W. What is the benefit brought by the new way? In the ablation study, the performance degrades only when IMF and PoRE are both removed. With either one, the performances are close to the final performance. This raises a question - Do IMF and PoRE substantively or functionally overlap? I think this paper requires a clearer comparison and analysis between MASC and G-MASC, both theoretically and experimentally. For example, why does G-MASC only show considerable improvement over MASC on MoNuSeg (the authors claim G-MASC work better when using fewer directional kernel sets and smaller kernels MoNuSeg, but this is an observation rather than an informative analysis)?	The authors mention that the code and dataset will be made available.	"The paper generally presents sufficient detail and references for reimplementation. ""In practice we use a set of Gabor reference functions.."" It would be good to provide some details about the parameters used."	Although some information is missing, such as training epoch, learning rate, and footprint, the authors claimed in abstract that they will release the code and data, so the reproducibility is good.	"My main comment is that the current manuscript is not clear enough to understand the methodology, and a lot of important details are missing. More precisely: A better motivation/introduction on the calibrated response shaping is necessary. In the current version of the paper, it is hard to understand without reading [1] what is the motivation of this and what is the novelty in comparison to [1]. The paper is written as if the response shaping was something obvious. Without a better explanation, I would remove Fig. 2 (a) since it is impossible to comprehend without context (furthermore, in the legend the authors use the terms ""stress"" and ""node"" which are not defined). The parametrization of the kernels is not clear and if they used weight sharing between the different orientations of the same kernel. It is also not that clear why they need two base kernels to cover 8 directions, I think it is to avoid rotating the kernels on smaller angles than pi/2, a comment on that point would be welcome since I think it is a strength of the method to avoid relying on steerability which can lead to resampling issue. Furthermore, it is not clear from Section 2, what is the position of the proposed approach toward the state of the art (especially toward [1]) and also how the current approach compares to the steerable CNNs. I understand that the information of the local orientation is encoded and propagated throughout the network, but what would be the advantages/disadvantages in comparison to a fully rotation equivariant network? I have other minor points and questions: Probably that I am missing the point, but If the authors used weight sharing of the two base kernels for the 8 orientations. Why does the PoRE loss need to be applied to the n=16 kernels? If they are rotated/mirrored versions of each other, I would expect Eq. 5 to reduce to something like 4 times the distances to the two base kernels.  A quick definition of the metrics would be welcome, for instance, the F1-score can be computed the instance-wise for the MoNuseg dataset, but I think the authors used it pixel-wise. The authors mention doing an experiment with non-Hybrid G-MASC. First, the term hybrid was not clearly defined (either change the formulation or clearly define it). Second, I would be interested to see the result of the same experiment but keep only the Gabor part. I would like to see a discussion on the computation efficiency of the current approach compared to the other baseline. I expect it to be a strong selling point. What is the architecture of the U-Net (Double Conv)? In Fig 3. f) the rescale is not explained. To conclude, the proposed method is very interesting in terms of novelty. Furthermore, the experiments are convincing. However, the authors fail to clearly explain and motivate the approach. I had to read [1] to have a better grasp of what the method was."	"The paper is well written and provides a very good evaluation. The MASC approach is an interesting way to incorporate orientation to CNN filters in order to reduce redundancy and parameters and an improvement would definitely be of interest to the community in my view. The presented extensions, in particular the regularisation function could have been better motivated, though. There are some indicators like ""..the response shaping function which we found to be unstable in some cases"" and ""..it helps maintain a stable orientation relationship amongst its kernels"", but the evaluation does not show major differences between MASC and G-MASC. So I am wondering whether the original formulation really has significant drawbacks. At least these drawbacks do not materialise in the performance on the tested datasets. As for the integration of scale representations, the ablation study does not show significant differences compared to not having them. The authors note that the tested dataset may not be suitable as there are only minor scale differences. However, I think as the scale integration is one of the proposed novelties, it would have been good to provide an example to demonstrate an improvement."	"Those names across figures and main text should be consistent. For example, ""Rotational penalty reference node"" in Figure 2 has not been mentioned in the main text. Also, some illustration is missed. What do (c) in Figure 3(b) and Figure 3(c) mean? What does ""c kernel sets"" in Figure 3(f) correspond to? The authors mention that ""a smaller set of training patches (4000 samples) was used in the ablation experiments to show differences more clearly"". Does it mean the proposed modules only show obvious efficacy when the dataset is small? What will happen if the data size is large? I appreciate authors have reported the parameters, but considering the proposed network involves some matrix calculations, I believe some metrics like FLOPS can complementarily show the efficiency of the proposed method. Supplement more comparison between G-MASC and MASC, e.g., it would be good if the authors compares some directional kernels of G-MASC to those of MASC in Figure 1."	This paper could be accepted on the basis of the novelty of the approach and the satisfying evaluation. However, it would greatly benefit from clearer writing.	I think the paper is very solid in its overall presentation, writing and evaluation. However, the improvements over MASC seem to be minor and as presented on the tested datasets, the proposed method is more of an alternative to MASC rather than addressing major drawbacks.	This paper integrates the information of the rotation and scale into a convolution neural network, which largely lightens the model parameters while achieving comparable or even better performance on three datasets. Nevertheless, a few points summarised in weaknesses and comments need clarification, e.g., what does the calibrated response shaping bring and how?
020-Paper0395	A Spatiotemporal Model for Precise and Efficient Fully-automatic 3D Motion Correction in OCT	The paper introduces a novel model and an optimization for volumetric distortion correction in OCT. The method allows for performing. It estimates A-scan positions in a single mapping, combined with a forward warping that allows correction in all 3 dimensions while avoiding the need for repeat scans. It also performs parameterization with respect to time, which allows for fully continuous parameters. Finally, by structuring the feasible distortions in OCT, a forward interpolation scheme is utilized, providing more accurate results while requiring less computational time than competing methods	The authors proposed a motion model and a corresponding fully automatic, reference-free optimization strategy for volumetric distortion correction in OCT.	eye emotion during scanning OCT images causes artifacts, This paper, introduce a registration method to compensate 3D-OCT eye motion.  The author claim that their result are promising and improved dramatically  compare to the state-of-art.	The paper provides a good introduction, and a good explanation of the differences between the proposed method and the competing ones. The proposed method requires a different protocol for scanning, with a single mapping, which takes less time, so it potentially leads to less errors caused by saccadic movement or by wandering eye from the patients, as well as less discomfort. The proposed method is 5 times faster than the fastest method it was compared against The proposed method supposedly provides a dramatic improvement over competing methods on registration accuracy.	The authors proposed a novel strategy to correct OCT volumetric distortion. The authors made experiments based on patients to prove the effectiveness. The authors proposed the first metric quantification of errors in OCT motion correction.	using non-rigid registration compare to rigid-registration, using regularization term , using a-scan and B-scan information , motion compensation throughout fast microsaccadic eye movements.	There were no accuracy comparisons with competing methods, just time comparisons. Although estimations are provided according to the characteristics of the diverse methods, the lack of hard numbers decreases the confidence on the provided results, and it does not really facilitate obtaining an objective comparison by the reader. While this is justified and explained by the authors, one wonders if one or some of the competing methods could have been replicated, or the authors contacted to run experiments in the utilized dataset. The dataset is not available and there is no word on if it will be made available in the future. While its characteristics are described in good detail, having a common dataset with ground truth metrics would facilitate that future methods can be compared with the presented one.	What is the meaning of right part in the equation (1)? In the Fig.3, how did authors take the top row two images almost at the same position, as the eye is moving? If the positions are different, how can you get the final OCTA. Would you further explain the eye motion trajectories? The runtime comparison is shown in Table 1. The authors reproduced the comparison algorithms and gave out the results based on the same GPU? The references of Zang17, Makita21 and Chen21 in table 1 are missed. How are the results from other diseased patients?	missing reference of Table 1 lack of enough comparison with more methods in terms of accuracy	Nor the code, nor the dataset are publicly available. The framework is said to be made available in the future, once further work is performed and the framework is considered closer to completion The mathematical support of the method is clear and detailed enough to try to attempt an implementation somewhat close to the one the authors might have done. While the dataset is not publicly available, it is sufficiently well described to be able to acquire a similar one, if access to a suitable device is had.	How to get the X/Y fast motion volumes may determine the reproducibility of the paper.	however the code is not provided , but provided visual inspection is satisfactory	The paper was quite clear and well organized. The introduction section was very detailed and contained good details on what the state of the art is, and what are the differences introduced by the proposed method. The method is well defined and described. The contributions are made clearly, and the method provides several improvements in speed, accuracy and in comfort to the patient, as less acquisitions are required. The weakest part of the paper would be the experiments section. There were no accuracy comparisons with competing methods, just time comparisons. Although estimations are provided according to the characteristics of the diverse methods, the lack of hard numbers decreases the confidence on the provided results, and it does not really facilitate obtaining an objective comparison by the reader. While this is justified and explained by the authors, one wonders if one or some of the competing methods could have been replicated, or the authors contacted to run experiments in the utilized dataset. The dataset is not available and there is no word on if it will be made available in the future. While its characteristics are described in good detail, having a common dataset with ground truth metrics would facilitate that future methods can be compared with the presented one. The discussion is quite interesting. It goes into detail on why there was no direct comparison, and it makes an effort on providing estimates on the range of error of competing methods. It also provides a good analysis on what the future steps for the framework might be.	There is no conclusion in the paper.	1- author should provide complete reference which are written in the paper (table1, Zhang? Ploner? Makita? Chen?) 2-  in terms of accuracy comparison , some team have worked on this topic for long term which are missing in references such as Martin F. Kraus et al, your results should be comparted to them	The give score is due to the proposed method being faster than alternatives, supposedly more accurate, and simper given the need for just a single mapping, which reverts on more simplicity, as well as more comfort for the patient. However, I find the experiments section not to be as strong as it could have been. With proper experimentation and comparisons, utilizing hard numbers when comparing with alternatives, I would have definitely rated the paper higher.	The detailed explainations of the paper and the results seems good.	1-The novelty paper should be more strong 2- the registration parameters are not described competently, for example in literature mentioned about Mutual information or SSD cost function. these should be included too 3-
021-Paper0893	A Transformer-Based Iterative Reconstruction Model for Sparse-View CT Reconstruction	This paper proposed a novel transformer-based iterative reconstruction model for sparse-view CT reconstruction problem. The authors combined convolution and transformer branches to simultaneously extract local and non-local regularization terms. Besides, an iteration transmission module was introduced to further promote the efficiency of each iteration. The experiment results demonstrated competitive performance in artifact reduction and detail preservation.	This paper proposed a sparse-view CT reconstruction network incorporated with transformer. The proposed network introduced FBP into the iterative schemes instead of back-projection to accelerate the convergence.	1 combine both local and nonlocal regularizer with neutral networks to achieve general prior. 2 apply IT module to connect the iterations to improve deep feature extraction.	This paper introduced a novel unfolded iterative reconstruction model, which combined convolution and transformer branches to simultaneously extract local and non-local regularization terms.	a. The motivation to introduce transformer is clearly described. b. A new module called iteration transmission is proposed to connect the iterations to improve the deep feature extraction and structure preservation. c. The results look promising.	1 design an iteration block to simultaneously learn the local and nonlocal regularizer.  2 IT model is proposed to build the communications between different iterations.	1) In section 1, the authors indicated that one of their contributions was replacing back-projection with FBP to improve the performance of network. However, many previous works, e.g., AirNet, have already demonstrated the efficiency of FBP operator. 2) In fig. 4, the PSNR of FISTA-Net was higher than TGV, while the SSIM was lower. Could you please explain the contrary result? 3) In section 4, the authors indicated that independent iterations made the network extract only shallow features. Since the proposed network was built by unfolding iterations, these iteration blocks were cascaded to form the whole network. Thus, the network was supposed to be capable of extracting both shallow and deep features. 4)  The manuscript does not give much discussions about why the method could improve the results, and how could the method be further improved.	a. The filter of FBP is fixed with RL filter, a learned filter may be better. b. The training is supervised learning and unsupervised training will be more clinically valuable. c. More task-based evaluations will better demonstrate the performance.	1 2d and small dimension images are reconstructed. Not sure if the method can handle the real and large 3d data volume.  2 using FBP to replace the backprojection destroys the conjugate operation requirement on AAt. The authors are expected to perform careful evaluations. 3 As shown in Figs 4 and 6, subtle image quality improvement is observed. The real impact of the work is thus doubleful.	The manuscript has clearly described the network architecture and the reconstruction workflow. Most of the work is reproducible.	The implementation of the network looks simple, but we still hope the authors release the codes.	Well done.	1) In this paper, one of the main contributions was the iteration transmission module. AirNet contained dense connections between iterations, which was a different form of iteration transmission. It is recommended to compare the proposed network with AirNet. 2) Ablation study was needed to verify the effectiveness of the proposed iteration transmission module. 3)  As shown in figure 3, local and nonlocal regularizations are alternatively conducted in the proposed method. Does it work better than the workflow using only nonlocal regularization? Please give explanations or make a comparison. 4)  In Result section, only 64-view-reconstructions are presented. Could the proposed method handle the reconstruction with fewer projections with little degradation? This should be added to the manuscript. 5)  According to the paper, both nonlocal iteration block and the iteration transmission improve the reconstructions. Whether the NLIB or the IT makes more contributions to the reconstruction improvement? Please make the comparison. 6)  The authors might modify their writings. There were some grammatical errors, e.g., in section 2.2, 'After the merged windows are fed into ...'	a. The authors should clarify the motivation of IT module, i.e. why existing networks cannot extract deep features. b. Task-based evaluations should be added. c. Clinical metrics should be adopted.	The authors are expected to provide the evaluation using full dimension dataset, usually with more than a few hundreds of 2d projection data, each of more than 1024*1024 dimension. 3d CT images are also expected to fully demonstrate the calculation speed and memory consumption of the proposed network. The FBP operation is not a common way to perform backprojection though the literature mentioned this operation (ref.28). It destroys the conjugate property of the forward and backward operations. Not sure if the authors derived the formula which may justify the FBP operation. The presented results are in perfect low noise level. The authors are expected to evaluate the algorithm when facing high noise fluctuation.	This paper proposed a novel hybrid network in unfolding manner, which combined convolution and transformer branches to simultaneously extract local and non-local regularization terms. However, more comparison results and ablation study were still needed to verify their main contributions.	This paper introduced transformer architecture into unrolling-based CT reconstruction network, proposed a novel module to extract deeper features, and achieved promising results.	Similar publications were found in literatures within the past five years. Everyone claimed nice results but not readily usable in clinic. Many authors did not understand the clinical requirement and only focus on algorithm parameter tuning. This manuscript is still following the old way though it includes detailed description of the network structures and results.
022-Paper0036	AANet: Artery-Aware Network for Pulmonary Embolism Detection in CTPA Images	The authors propose a novel 3D fully convolutional network for pulmonary embolism (PE) detection with artery-aware. An artery context fusion block is embedded in the network to generate artery context to guide PE detection. Even-dice loss is introduced to avoid gradient exploding. The method was evaluated with a public dataset (CAD-PE), and achieved state-of-the-art performance.	The authers proposed an artery-aware 3D fully convolutional network (AANet) that encodes artery information as the prior knowledge to detect arteries and PEs. They developed An artery context fusion block (ACF) generating the context of artery as in-network prior knowledge to guide PE prediction. The methods are evaluated on the CAD-PE dataset with the artery and vein vessel labels which shows good performance.	Clinically, this paper aims at pulmonary embolism detection. Technically, this paper puts forward a way to introduce attention to arteries in the segmentation networks.	A novel artery-aware framework for pulmonary embolism detection is proposed.  A loss function even-dice loss is introduced and balances the artery / PE loss. The experiment with public data and various settings is persuasive.	The experiment is of great significance for the PE diseases, and the technical method is reliable. In this paper, an artery-aware network (AANet) to segment PE in CTPA image that fully utilizes artery context is proposed. The method has certain novelty in application.	Authors put forward a way to introduce attention in the segmentation networks, together with a loss function (Even-Dice Loss) which could work even if there is no foreground in the sampled batch.	1) The network was pretrained on a large-scale dataset LUNA16. As the LUNA16 dataset is for lung nodule detection, the pre-training processing is not straightforward to understand.  2) A challenge dataset CAD-PE was used. The authors made extra annotations (pulmoanry artery) on the public data, which was used as auxilary class for PE detection. The improvement is noticiable, but it might be unfair to compare with others participants of the challenge 3)  Based on the network prediction, post-process (morghology closing) is used after threshold. How much does post-processing improve the performance.	At the end of the first paragraph of the introduction section, the author only stated that the high false positive PE diagnosis was partly caused by doctors, and did not discuss other reasons in detail, such as the impact of the gray, size and other characteristics on the CTPA images. At the end of the third paragraph of the introduction section, the data set mentioned by the authors should be cited. In the AANet of the Methods section, the authors did not explain the role of using different numbers (two or three) residual blocks, the difference between stdConv and Conv, and the role of fusion multiscale modules for PE segmentation. There are too few method measures to confirm the accuracy of the method. In the results section, the PE 3D or 2D segmentation results should be compared with the ground truth.This method should be compared with other methods. The discussion and conclusions are insufficient, and more should be discussed about its advantages over other methods and its possible future applications.	The method itself requires more annotation than traditional methods, and the annotation (as shown in Fig 2) requires lots of work. Although authors mention that this network and weights could be used as a pretrained model for other tasks, it is still questionable whether this method could be applied to other similar diseases. The study/experiment design could be modified to make it clear to readers. To make a meaningful comparison with all methods, it would be better if authors could mention whether all methods were pretrained with LUNA16. And although mentioned in the paper that citation [13] missed some small cases, I would still want to see how AANnet performs on the 80+ cases subsets (as mentioned in [13]).	The reproducibility of the paper is OK.	The reproducibility of this article is possible.The network structure parameters and loss function are described in the article.	Authors claim that the dataset and the network/weights will be made public. I believe that the reproducibility is good.	None	At the end of the first paragraph of the introduction section, the author only stated that the high false positive PE diagnosis was partly caused by doctors, and did not discuss other reasons in detail, such as the impact of the gray, size and other characteristics on the CTPA images. At the end of the third paragraph of the introduction section, the data set mentioned by the authors should be cited. In the AANet of the Methods section, the authors did not explain the role of using different numbers (two or three) residual blocks, the difference between stdConv and Conv, and the role of fusion multiscale modules for PE segmentation. There are too few method measures to confirm the accuracy of the method. In the results section, the PE 3D or 2D segmentation results should be compared with the ground truth.This method should be compared with other methods. The discussion and conclusions are insufficient, and more should be discussed about its advantages over other methods and its possible future applications.	"in Table 1, I believe ""SDiceLoss"" should be ""PSDiceLoss"" based on this paper's context. Mention if all methods were pretrained with LUNA16. Show AANnet's performance on the 80+ cases subsets (as mentioned in [13])."	The novel methodology and experiments	This paper describes the network structure and the experimental process completely, but the experimental results and discussion part are insufficient.The authors added artey-vein prior knowledge as a novel weight for PE segmentation, but did not compare with other methods.	The performance of the method is good, which outperforms existing methods, and the loss function mentioned could be generalized to other tasks. However, the study design could be improved (at least with better description in the paper).
023-Paper2394	Accelerated pseudo 3D dynamic speech MR imaging at 3T using unsupervised deep variational manifold learning	The authors pursue improving the spatio-temporal resolution (and image quality) of dynamic speech imaging using a manifold approach to collect 2D slices and time to arrive at a composite 3D volume over time.	The authors propose an unsupervised deep variational manifold learning approach for reconstructing pseudo-3D dynamic speech MRI from sequentially acquired under-sampled (k-t) space measurements of multiple 2D slices. The propose method shows better performance than conventional compressed sensing reconstruction methods in 2 initial subjects.	In this paper, an unsupervised deep variational manifold learning was applied for temporal-spatial fast speech MRI. The experimental results show some subjective superiority of the proposed method over other TV-based methods.	"The motivation to pursue unsupervised learning is well founded given that it is challenging to collect a ""gold standard"" dataset in such acquisitions. The use of preparing a ""pseudo-3D"" seems practical and exploits k-t space rather than either a 2D only or 3D only approach"	The proposed approach does not need large-scale fully sampled Pseudo 3D dynamic speech MRI for supervised training but reconstructs the image time series only from the measured under-sampled (k-t) data	(1) The authors explored the speech MRI which to my knowledge is less intensively studied in fast MRI research, yet it is a meaningful task with challenges. (2) The proposed unsupervised learning spares the need for fully sampled training references. (3) The paper is well organized and presented with good image quality.	The challenge in assessing these reconstructions is a clear definition of the requirement of the image quality and target spatio-temporal resolution that is necessary (adequate). These are directly tied to the speech task at hand and defining these constraints might enable the algorithm to optimize better The comparison to earlier CS methods while important to show improvement do not represent the state-of-the art methods for accelerated spatio-temporal acquisitions such as dictionary approaches (atom based), including simultaneous multi-slice methods, etc. Given the niche application, the exploration of these methods may be limited to describing them in the discussion section for the reader's comprehension. In future experiments (or in discussion), it will be interesting to visualize and interpret the latency vector in line with the speech task to enable an understanding of the generator's outputs.	The proposed method requires subjective iterative optimization, and the number of iterations should be carefully selected to avoid overfitting Effectiveness only demonstrated in limited number of cases	"(1) From the perspective of method originality, this paper offers little novel idea, which seems apply generative manifold learning in this problem. The ""Methods"" section is not explained and analyzed in detail, which is the major component of this paper. Novelty should be addressed here. (2) No objective evaluation is applied. If the ground truth is not available, some subjective rating scores could be used."	The reproducibility criteria have been met	The authors have attempted to make the code publicly available, which should ensure good reproducibility.	This paper is presented clearly and the model is easy to reimplement although the code is not publicized.	"The authors have strived to improve image reconstruction quality compared to previously available compressed sensing reconstructions. A clear understanding of the target acceleration and image quality required will provide meaningful insights into algorithm development. The approach to leverage a ""latency vector"" is intuitive as an input to the generator and perform the data consistency step. The strengths and weaknesses have been listed above"	1: According to the data acquisition parameters, the acceleration factor of the multi-slice speech MRI is 9-fold? Could the proposed method be applied for higher acceleration factors or even 3D accelerated speech MRI? Please comment? 2: The reconstruction time of each method should be provided 3: The number of subjects recruited in the study should be clarified in the Section of 3.1	(1) The model novelty should be further clarified and stressed. What is the key difference between this model and references [23.24], except for the applications? In my opinion, the vocal tract shaping during speech has regularity, and different letter has different pattern. However, cardiac imaging show distinct regularity. (2) The equation (1), the gradient operation is applied on both s and t, while in the text it seems only t is differentiated. (3) The authors claimed the network learns noise in the reconstruction when certain number of epochs is reached. I wonder if it is due to the self-supervised nature of the proposed method (over-fitting). More analysis would be helpful.	The demonstration of the developed algorithm in vivo is appealing. The lack of a gold standard reference is a challenging yet important step to accomplish and compare.	No fully sampled data is required for supervised training which is difficult to acquire for the speech MRI	The major factor is model novelty.
024-Paper2344	Accurate and Explainable Image-based Prediction Using a Lightweight Generative Model	The model presents a Lightweight Generative Model. It is not a deep learning model, and consequently has as its main advantage its linearity and invertibility. The model is illustrated in prediction of age and gender from brain scan images.	The main contribution is a demonstration that an extremely simple Gaussian model for image-based exogenous variable prediction may perform as well or better than far-more-complex peers that have significant disadvantages (larger number of free parameters, lower interpretability), especially when the training set size is relatively small.	The paper proposes an alternative to non-linear discriminative methods for predicting variables of interest such as a subject's brain age from images. The alternative is formulated as a generative model that models the subject's image as a linear function of the subject's covariates (such as age, gender, etc.). This is then used as a likelihood model and a posterior of the variable of interest is constructed for discriminative analysis. To learn the generative model, maximum likelihood method is used and closed form solutions are employed. For tractability of the covariance of the noise matrix (which is underconstrained), the noise variable  is modeled using factor analysis with a smaller latent noise vector. To estimate these new variables, Expectation-Maximization is used. Experiments are performed on the UK Biobank dataset.	The main strengths of the paper are its originality (new derivation of an algorithm). The fact that it is not a deep learning based strategy is quite refreshing (and even brave). Another advantage is the existence of only hyperparameter that needs to be tuned and/or experimentally set.	"The main strength is conceptual.  The broad trend is toward ever-more-complex computational machines for this type of task, without considering the disadvantages.  The conceptual trap, for computer scientists, is that cheap and simple alternatives like the proposed one are not ""novel,"" but novel doesn't necessarily mean better in any and all ways. The attempt to compare the simple method against linear/nonlinear generative/discriminative alternatives is also a strength, with some limitations.  This appears to be a good-faith effort to replicate prior work."	The method proposes a more tractable and lightweight generative model which is linear, and admits closed-form solutions for the weight matrix (common with most linear models). The method performs very well for smaller datasets, but this could also be attributed to model capacity (which is not directly explored in the paper). The paper addresses the problem with deep learning methods early on in the paper, regarding issues with smaller training datasets, difficulty in model interpretation, and extensive tuning  required to make the method work, which also can lead to brittle performance.	As for weaknesses I would highlight the difficulty in reproducing the method. The authors do not mention the code to be made available and the mathematical derivation might be quite heavy for unfamiliar audiences.	"The weakness in the evaluation is that the leading method, SFCN, was not reimplemented and tested using the current method's computational pipeline; this paper just re-prints previously published performance numbers.  Differences in the random partitioning of training and test sets, as well as differences in preprocessing pipelines, could have accounted for the reported performance differences. Another weakness is that claims in the paper are not well supported.  Pointing to publications showing deep learners applied successfully to huge image sets, the assertion is made that deep learners must have such large training sets to perform well.  Poor performance of deep learners on small training sets has not necessarily been shown in these papers. In the evaluation, we see the performance of the re-implemented methods, but not whether that performance is similar to that of prior publications with those methods.  In other words, it's not clear whether the author's reimplementations of these methods achieved the highest performance possible. The argument that the proposed method is more readily interpretable than deep learners is not very convincing either.  It is possible to apply various schemes to ""trace"" the influence of each voxel through the machinery of the neural network, and thus get a quantitative sense of whether higher/lower intensity at that voxel tends to be associated with higher/lower age. The argument that discriminative learners have more free parameters than generative ones is suspect.  Surely it is possible to assemble complex generative regressors and simple discriminative ones; people just choose not to.  The real point of this paper is not about generative vs discriminative, in my view- it is about complexity vs simplicity."	"The model proposes a causal forward model expressing the effects of variables of interest on brain morphology. However, the model is a linear model. Although a linear model is easy to interpret, the work doesn't motivate why a linear model is sufficient to capture all the nonlinear variation in structure and morphology given only variables like age and gender. The extensive research around deep learning (for both generative and discriminative models) has been to find the right ""features"" to generate/extract to faithfully capture the conditional probability distribution (of the brain image given other variables, in this case). Moreover, deep models also lend themselves to explainability and interpretability ([1, 2, 3]) and it is incorrect to say that discriminative models are harder to interpret. Moreover, the interpretability of the proposed ""causal model"" is not substantiated with experiments that show how the proposed model is different/better than other baselines in explaining different factors. For example, Figure 1 is really unnecessary and its not clear to me what exactly the figure is trying to convey. A comparison between baselines is required in the experiments to support the claim that the proposed method is a causal model that lends itself to better interpretation compared to other methods like VAE, SFCN or RVoxM).  The paper has very little technical novelty. Linear generative models have been proposed before in the literature and the derivation of the posterior follows from the basic generalized linear models formulation. The major drawback of the linear model is also its lack of flexibility. Since the model only uses upto two covariates (age and gender) the model doesn't capture, for example, multi-modal behavior or non-linear deviations in the given generative model formulation. Comparison is also slightly unfair with SFCN. For the same input (affine T1), SFCN has a consistently better than the proposed method for all training subject sizes. The tradeoff between accuracy and some other factor (training time, hyperparameter tuning) is not shown to justify the use of the proposed method rather than SFCN."	I belileve the method to be difficult to reproduce for unfamiliar audiences, not comfortable with the needed mathematics (since as far as I am aware, no code will be made available).	The authors go to pains to tell us how they implemented and evaluated competing methods, in quite some detail.  In that sense reproducibility is high.   The data set is open source.	Most implementation details are provided in the paper. Methods are easy to re-implement, although no code is provided in the supplementary details.	It is not clear to me why age is used as covariate for gender classification while no other variables are employed when doing age prediction. I would also like to have seen mentioned the training times of the SFCN and RVoxM methods. And what is the age range on the UK Biobank.	See above for my comments.	"The paper is generally very  well written and easy to follow. I didn't see any major change in flow either. However, the paper initially focuses a lot on an ""explainable"" generative model, and terms like ""causal forward model"" that are not used in the paper whatsoever. Deep learning models also follow a ""causal structure"", the only difference being the model/hypothesis space, which has implications on learning capacity and optimization.  However, I think the Bayesian method proposed is overly simple. Given only a upto two raw covariates (age, and gender), the generative model is simply a linear combination of the two variables with additive noise, which doesn't seem to have sufficient model capacity.  The statement ""... naive Bayesian classifiers can empirically outperform more powerful methods when the training size is limited ..."" which is also the premise for the proposed method is a commonly known fact - naive bayesian classifiers have low model capacity, and are less likely to overfit and have better performance than high-capacity models (deep networks). However, then the paper goes on to say ""...even when the number of training subjects is the thousands, our lightweight linear generative method yields prediction performance that is competitive with state-of-the-art nonlinear..."". So it is not clear which case the proposed method is handling - the low or high data regime. Experiments however, do not support the claim as SFCN outperforms the proposed method at every training set size. In page1, the phrase ""In a recent survey on single-subject ..."" the survey itself is not cited. Please cite it.  In the subsection ""Discriminative methods are hard to interpret"",  explainability of attention models is questioned but the proposed method doesn't solve or handle the issue itself. [1] LIME: Ribeiro, Marco Tulio, Sameer Singh, and Carlos Guestrin. ""Why should i trust you?: Explaining the predictions of any classifier."" Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining. ACM, 2016. [2] Shapley sampling values: Strumbelj, Erik, and Igor Kononenko. ""Explaining prediction models and individual predictions with feature contributions."" Knowledge and information systems 41.3 (2014): 647-665. [3] Selvaraju, Ramprasaath R., et al. ""Grad-cam: Visual explanations from deep networks via gradient-based localization."" Proceedings of the IEEE international conference on computer vision. 2017."	I have chosen a strong accept, since I believe this to be a good paper, only with minor weakness. It is a well written document, the method is novel and it is illustrated in a useful and important application.	The main reason to accept is that it is a refreshing change of pace from the ever-more-complicated deep learners that dominate the landscape.  Carefully and thoughtfully evaluating a simple-minded alternative and its advantages is a unique contribution to the field.	"The paper proposes a simple framework for estimation of age and gender from brain images using a linear generative model as the likelihood function to derive a posterior for the covariate given the image. However, most methods (including those based on deep learning) are usually derived using a similar formulation (a likelihood term transforms into a ""loss function"") and the prior term turns into a ""regularization term"". Most Bayesian methods also use a similar framework and then optimize parameters. The proposed method is not very motivating or technically novel. Although both linear and non-linear benchmarks are used, and the paper proposes better predictive performance for low training samples, the performance of SFCN is consistently better than the proposed method. Moreover, the claims about a causal forward model are not substantiated with experiments showing if the proposed method provides any advantage compared to other baselines in interpretability and explainability."
025-Paper0249	Accurate and Robust Lesion RECIST Diameter Prediction and Segmentation with Transformers	This work proposes an architecture and set of training objectives for RECIST lesion diameter prediction on DeepLesion data. As in some other works, the architecture involves a convolutional encoder followed by a transformer encoder (with positional encoding). As in prior work, the architecture includes a convolutional decoder path that outputs a weakly supervised segmentation and heatmaps which indicate RECIST diameter keypoint positions. This work proposes the addition of a second decoding path which uses a transformer decoder with a keypoint regression output. Furthermore, this work proposes two consistency losses between the two decoding paths. Ablative experiments show that the transformer encoder is particularly useful (this has been proposed before for other tasks but not validated on this task) and the transformer decoder path with consistency losses yields an additional (though small) gain in performance.	The paper proposes a model for semi-automated RECIST diameter prediction and segmentation. The model is applied in two, consecutive steps, in which the first step aims to predict a rough bounding box, while the second aims to predict a segmentation map, heatmaps for long- and short-axes diameter keypoints, as well as an additional direct regression of these points. The paper evaluates the algorithm on a seemingly manually chosen and annotated subset of the DeepLesion dataset and achieves superior performance in comparison with a variety of other algorithms.	the authors propose a transformer-based network (Meaformer, Measurement Transformer) for lesion RECIST diameter prediction and segmentation (LRDPS), which involves three related and complementary tasks, including lesion segmentation, heat map prediction and key point regression. Two consistency losses are introduced to explicitly establish the relationship between these tasks.	New decoder path for RECIST diameter prediciton in DeepLesion : keypoint regression via transformer. Novel consistency losses. The ablation studies are thorough and informative. SOTA performance. Thorough comparisons to other methods.	Although not the first to propose this, the submission addresses an interesting combination of CNNs and Transformer networks with a similarly interesting application in a clear radiological environment and with a straightforward practical use. The manuscript contains a variety of descriptive figures, which facilitates a better understanding of the authors' arguments.	a transformer-based network (Meaformer, Measurement Transformer) Two consistency losses are introduced to explicitly establish the relationship between these tasks.	Some technical details are unclear (eg. step 1 outputs: do they use a segmentation objective, do they output only two heatmaps, do they use the same consistency losses and are these suboptimal for bounding box prediction, as opposed to keypoint prediction?)	Overall, the impact of the method felt rather difficult to assess. - While the idea of single-click RECIST diameter estimation via the use of a combination as proposed by the authors is rather novel, single-click lesion segmentation is not, and is part of the regular clinical workflow, e.g. when using clinical reading software (single-click segmentation for example is part of the MM Oncology Suite from Siemens, but similarly exists for other vendors). - While I do not believe that these vendors currently use state of the art deep learning and transformer networks for this purpose, the question arises whether the conducted comparison is therefore adequate, as it seemed to only take into account deep learning-based methods, and with 5 out of 9 (namely [2] and [15-18]) having a very strong overlap of contributing authors, some optimized for the same dataset (e.g. 17,18). As a result, the representativity of the conducted comparisons felt somewhat vague to me. Further, the choice of the data should have been somewhat better motivated. It did not become clear to me, how the 1,000 test samples have been chosen and how they were distributed across the various types of lesions in the DeepLesion dataset. Finally, I was not completely convinced about both methodological aspects of the proposed solution (see below) as well as the significance of the achieved results, on which I would like to ask the authors for further explanation.	What's the internal structure of the transformer you are using? It's best to show it with figure. For example, whether the position encodings are all added to Q, K and V or only added to K and Q. What kind of structure is the encoder-decoder attention you mentioned in the decoder?	What were the Adam optimizer hyperparameters (other than learning rate) and what was the batch size? While not mandatory, I strongly encourage the authors to release the code for reproducibility - this is the most productive and useful way to allow reproducibility.	Overall, the reproducibility form in my perspective well matched the manuscript. Regarding a few points, I felt that the authors might adapt their answers, as pointed out in the following: A clear declaration of what software framework and version you used. [Yes] -> No I felt, that this was not described in detail. While it was mentioned that the authors used PyTorch, no versions have been documented. For all reported experimental results, check if you include: The range of hyper-parameters considered, method to select the best hyper-parameter configuration, and specification of all hyper-parameters used to generate results. [Yes] -> No This in so far as basically no hyper-parameter optimization seems to have been conducted. Details on how baseline methods were implemented and tuned. [Yes] -> No While the authors have stated to have trained the methods on the same data, they did to the best of my understanding not comment on whether they used one of the original implementations or not, nor whether they conducted any kind of tuning for the data at hand. An analysis of situations in which the method failed. [Yes] -> No While I might have overlooked this, I did not find an error-case assesment in the manuscript.	yes	Transformers on CNN is not a new contribution - as noted in the paper, this is based on prior work. Instead, the authors can list SOTA performance as a contribution. How did you select the loss weights (lambda)? Do you only predict two heatmaps in step 1 (bounding box)? Please clarify in the text. Do you apply the second consistency loss (distance to segmentation boundary) for the bounding box corner prediction in step 1? Please clarify in the text. If you do, this seems suboptimal as bounding box corners can be expected to typically be outside of the segmentation boundary. What is the impact of omitting this consistency loss in step 1? The ablation on consistency losses and model components is useful. Please perform a statistical significance test since results are so similar. Please better detail the weak supervision method. Is the first pseudo-mask an ellipse based on RECIST diameters? Are the second and third pseudo-masks the intersection of the prediction with the previous pseudo-mask? Is there an ignored region? Is step 1 (bounding box prediction) trained without the segmentation objective?	"(Note: In the following, original references are referred to as [X] while newly introduced are referred to as [rX]) Results As pointed out above, while I absolutely understand the clinical motivation of the method at hand, I did not feel completely convinced regarding the practical impact of the method at hand. - This is, first, as a click-based segmentation is part of typical clinical assessment software. Would it be possible to point out the main differences to these solutions? - Secondly, while deep learning-based solutions often constitute the state of the art, especially for this purpose well-functioning more classical algorithms are already existing in a clinical deployment. - Would it thus be possible for the authors to point out why only deep learning-based solutions were taken into account for comparison? - Would it further be possible to briefly comment on the very large authorship overlap across the chosen comparison algorithms (5 out of 9), namely [2] and [15-18]? - Could the authors briefly comment why they did not compare to other state of the art semi-automatic segmentation approaches, such as [r1-2]? Further, to me the claimed superiority seemed to be rather questionable: - While the authors provided standard deviations, they did not conduct any statistical testing. Many of the depicted standard deviations, however, imply that the results are not significant, in particular if taking into account values such as the Dice coefficient of AHRNet in Tab. 1, but also the long- and short-axes deviations in comparison to PDNet, or the segmentation-based results in comparison to TransFuse. As a result, I was not able to rule out the possibility of mere difference by chance. - If possible, I would like to ask the authors to add significance tests, such as t-tests, in order to rule out this possibility. Regarding the nnUNet results, it did unfortunately not become clear to me why the authors did not provide segmentation values, since nnUNet by its very nature is a segmentation approach. - Further, on p6, the authors state that the listed results have been copied from the related nnUNet paper. The paper, however, does not contain an evaluation on the DeepLesion dataset. Could the authors briefly comment on that? Method To me, the value of L_cons1 and L_cons2 might have been better motivated and assessed in more detail. Due to their formulation, both L_cons1 and L_cons2 are likely to provide only sparse feedback. - While the authors have assessed them in an ablation study, the results to me seemed to be mostly inconclusive. This is especially, as the results of the heatmap accuracy did not seem to change significantly while already being better than the regression output, neither did the Dice coefficient. Thus, finally the introduction of L1 and L2 does not promise a significant improvement, but a significant additional effort during the training. Drawing the axis the other way around, i.e. exchanging start and end point for each axis, would lead to a total of 4 permutations, which all would describe the same axes. - How do the authors ensure consistency across the keypoints to combat this issue? Some of the hyperparameter choices seemed rather arbitrary and barely motivated. - The choice of N_en=6 layers seemed somewhat arbitrary. Why did the authors chose exactly 6 layers? - Similarly, why was the weighting of lambda_1 to lambda_5 on p5 optimal? How was this assessed? Discussion I felt that in light of the above mentioned points some of the statements in the discussion were in my perspective not sufficiently shown. - I would recommend to mitigate some of the claims in the result discussion, namely the consistent improvement (p8), and that long-distance information is encoded, which was not directly assessed (p8). Further, the derivation of RECIST diameters from segmentations in my perspective is mostly state of the art, and is done by likely all large vendors as part of their clinical assessment software, as well as by a variety of institutes who have previously published on lesion segmentation. In a publication from 2008 [r4] this has already been part of the used software.  - I would therefore recommend to not state this as a major contribution of the manuscript at hand (cf. p7). The results on the DLS dataset felt somewhat difficult to interpet without further context. While an accuracy of 91.7% at first sounds positive, this is a rather low value if 95% of the lesions would show a similar response pattern (and thus the method would be worse than informed guessing). - As the dataset has not been further used in the manuscript, I would recommend to leave out this information, or to add a separate supplementary with a more thorough evaluation on that, which may then be referred to in the manuscript. Dataset The choice of the data should have been somewhat better motivated. It did not become clear to me, how the 1,000 test samples have been chosen and how they were distributed across the various types of lesions in the DeepLesion dataset. - Would it be possible to briefly add some explanation on this? Minor The table styling might be refined. A good resource for this purpose is [r3]. On p6, there is a formatting issue with a larger reference. The DLS dataset should be briefly described at its first mention, introducing the abbreviation would further help the reader to understand the sentence without having to look into the reference. Where does the stated accuracy of 99.1% come from (p5, bottom)? Why was this 2% better than the previous result? Could the authors add a note on where to find this data? Fig. 2 seemed to be a bit small, which first made it difficult to me to follow the discourse in Sec. 2. The manuscript currently contains a few typos and language issues that the authors might want to fix prior to publication: ""On the other hand, transformer is designed [...]  which is able to [...]"", p2; ""which attracts tremendous attentions increasingly"", p2; ""[...] matrix Q is token as input"", p4; ""The purposed MeaFormer"", p4; References [r1] Lin, Zheng, et al. ""Interactive image segmentation with first click attention."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020. [r2] Mahadevan, Sabarinath, Paul Voigtlaender, and Bastian Leibe. ""Iteratively trained interactive segmentation."" arXiv preprint arXiv:1805.04398 (2018). [r3] https://people.inf.ethz.ch/markusp/teaching/guides/guide-tables.pdf [r4] Jolly, Marie-Pierre, and Leo Grady. ""3D general lesion segmentation in CT."" 2008 5th IEEE International Symposium on Biomedical Imaging: From Nano to Macro. IEEE, 2008."	"I have read this paper. My advice is minor repairs. In this paper, the authors propose a transformer-based network (Meaformer, Measurement Transformer) for lesion RECIST diameter prediction and segmentation (LRDPS), which involves three related and complementary tasks, including lesion segmentation, heat map prediction and key point regression. Two consistency losses are introduced to explicitly establish the relationship between these tasks. Existing problems:  In the abstract, you should point out the existing problems in the current research, and then put forward the methods of this paper.  What is the meaning of the last sentence ""All figures are best viewed in color."" in Figure 1.  Is there a prediction head in the MeaFormer in step 1? The MeaFormer difference between step 1 and step 2 is not indicated.  In 2.1, the image I_c and I_d  should be displayed graphically.  In the backbone in 2.1, why is HRNet-W48 used to extract features?  What's the internal structure of the transformer you are using? It's best to show it with figure. For example, whether the position encodings are all added to Q, K and V or only added to K and Q. What kind of structure is the encoder-decoder attention you mentioned in the decoder?  In the final objective function in 2.2, how to set the weight factors of different loss? Is the weight factor set in this paper the best combination of weight factors?"	While modest, the new model contributions are clear and yield state of the art performance. Clarity could be improved. While some ablation studies could be added, the current set of ablation experiments is pretty good.	Overall, I am not fully convinced by the value of the method at hand: Most importantly, many of the results seemed inconclusive, some remained unclear, and a few seemed inconsistent. The idea in my perspective is current, but still has only limited novelty. The practical value of some of the method's concepts (e.g. the consistency loss) remained somewhat unclear to me. Regarding these issues, the manuscript unfortunately does not provide a statistical evaluation, which could easily eliminate some of my reservations. The sum of these issues combined with a variety of minor issues and the expected quality of MICCAI as a flagship conference therefore leads to my final recommendation.	a transformer-based network (Meaformer, Measurement Transformer) for lesion RECIST diameter prediction and segmentation (LRDPS), which involves three related and complementary tasks, including lesion segmentation, heat map prediction and key point regression.
026-Paper1270	Accurate Corresponding Fiber Tract Segmentation via FiberGeoMap Learner	This paper proposes a fiber classification method to identify fiber bundles from the whole brain tractography. The proposed method uses a new descriptor to represent the shape and position information of the fiber bundles, which are combined and then fed to the network for training a transformer-based deep learning model. Experiments have been performed to investigate the effectiveness of FiberGeoMap. In addition, the proposed method has been applied to autism data.	The authors propose a tractography segmentation method. This is a nicely designed method and shows a good result.	A novel fiber tract segmentation approach is proposed. First a novel descriptor, called FiberGeoMap, encodes the geometric features of the fiber by transforming to spherical coordinates and binning to predefined bins. Second, a computational framework, based on Transformers and multi-head self attention, is proposed for segmentation. Extensive evaluation using 103 fiber tracts plus no-tract categories on 205 HCP subjects are shown. Comparison with state of the art approaches shows significant improvement in dice scores over Tract-Seg and WMA. In addition, the application of proposed method in a clinical setting shows reduced fiber density (ratios) in Autism versus controls.	The two main advantages of this paper are as follows: A new method of fiber information description is proposed. This method, which is based on spherical coordinates, computes local and global information separately and combines two kinds of information together as a whole sent to the subsequent deep learning model. The transformer module is used to resolve the fiber segmentation problem with FiberGeoMap as input for the transformer.	This is a nicely designed method and shows a good result.	A novel geometric descriptor for fiber tracts capturing global and local information. A novel Transformer and self attention based segmentation framework. Extensive validation, including ablation experiments. Evaluation using 205 HCP subjects and comparison with state of the art approaches, where proposed method significantly outperforms state of the art methods, namely, Tract-Seg and WMA. Application of the proposed tract segmentation method to clinical setting (Autism versus controls). The concepts, results, are well illustrated. Code and video for tracts shared via GitHub.	Experiments seem unfair, such as inconsistent training and testing data. The baseline methods are proposed in 2018, not the latest ones. A qualitative analysis should be also provided.	"some major concern about the paper. The experimental evaluation seems not fair to either TractSeg or WMA. The author combined the two atlases together. The two methods have different definitions even for the same tract. This is a known issue due to the lack of concuss. Please refers to  https://doi.org/10.1016/j.neuroimage.2021.118502. So when comparing the tracts that are overlapped in these two atlases, there should be bias introduced. Also the two methods are performed with different tractography algorithms. Second, the computation of the evolutions is not clearly described. Dice is for volumetric overlap, while prevision and recall are class prediction. Did the authors convert the streamlines to masks somehow? The application on autism seems not necessary and redundant. Is ""the proportion of fibers"" a fair measure for ""abnormities""? Do we expect ASD individuals have such abnormities? As this is a technical paper, I would suggest adding experiments on additional datasets from multiple acquisitors as performed in TractSeg and WMA studies."	See some minor issues in detailed comments.	Good.	good	The code and video for predicted fiber tracts openly available via GitHub.	In the experimental part, the proposed method completes the segmentation of 103 fiber bundles in total, resulting in an average dice score of 0.93. Some of the fiber bundles segmented by the proposed method are not included in the other two comparison methods, therefore it is unreasonable to directly compare the average dice of the other two methods with the average dice of 103 fiber bundles. The evaluation should be improved by comparing the dice scores of fiber bundles belonging to the results of all methods. Are the models trained and tested on the same sets of HCP data? Taking one of the baseline methods, TractSeg, as an example, TractSeg uses 105 subjects for training and testing, and used five-fold cross-validation, which is different from the setting of the proposed method, i.e., 205 subjects. The paper must include more experimental details to ensure fair comparisons of different methods. The two comparison methods in the experimental part were both proposed in 2018. There are new methods proposed in this field in recent years, such as DeepWMA (proposed in 2020). These new methods should be considered in the evaluation. Why is there only quantitative analysis in the experimental part? It will be more convincing if the qualitative analysis can be included in the experimental part. The resolution of Fig. 6 should be increased. Currently, if you zoom in to see the performance of individual fiber bundles, they are blurry and hard to see. Evaluation with only one dataset is not very sufficient to demonstrate the effectiveness of the proposed method. More datasets should be considered.	"The authors propose a tractography segmentation method. This is a nicely designed method and shows a good result. Below are some major concern about the paper. The experimental evaluation seems not fair to either TractSeg or WMA. The author combined the two atlases together. The two methods have different definitions even for the same tract. This is a known issue due to the lack of concuss. Please refers to  https://doi.org/10.1016/j.neuroimage.2021.118502. So when comparing the tracts that are overlapped in these two atlases, there should be bias introduced. Also the two methods are performed with different tractography algorithms. Second, the computation of the evolutions is not clearly described. Dice is for volumetric overlap, while prevision and recall are class prediction. Did the authors convert the streamlines to masks somehow? The application on autism seems not necessary and redundant. Is ""the proportion of fibers"" a fair measure for ""abnormities""? Do we expect ASD individuals have such abnormities? As this is a technical paper, I would suggest adding experiments on additional datasets from multiple acquisitors as performed in TractSeg and WMA studies."	"Congratulations on this great work. It was easy to read and follow. Please consider some minor suggestions, mainly pertaining to some complex sentences.  Minor: Please check for complex sentences, and break into multiple sentences to improve readability. See examples below. Complex sentence in the Abstract: ""Experimental results showed that the FiberGeoMap combined with FiberGeoMap Learner can effectively express fiber's geometric features, and differentiate the 103 various fiber tracts, furthermore, the common fiber tracts across individuals can be identified by this method, thus avoiding additional image registration in preprocessing."" Not clear, what is implied. Please check the following sentence on page 2:""therefore, these fibers need to be clustered or segmented into a relatively small number of fiber tracts, the fibers within each tract are similar and each fiber tract should have the relatively independent meaning, namely fiber tract segmentation or fiber clustering."" Complex sentence on page 2: ""Previous studies have focused on three cate-gories of features, which are geometrical [4,5], anatomical [6,7] and functional [8,9] features in chronological order, and seem more and more reasonable and in line with the requirement of fiber clustering, but the uncertainty also increased in sequence, for example, anatomical feature based method depend on anatomical segmentation and registration, while the anatomical atlases are various, registration techniques are also not mature."" Complex sentence on page 2: ""Accordingly, for FiberGeoMap, we proposed a revised Transformer network, called as FiberGeoMap Learner, which can efficiently explore the FiberGe-oMap features, and then we trained the model with the all fibers from 205 HCP sub-jects [12], the experimental results showed that our method can obtain the accurate and corresponding fiber tract segmentation across individuals."" Please define the names of the tracts before the first use of acronyms on page 4, and Figure 3. ""3 fiber tracts (STR_right, STR_left, and MCP) from...."" Typo on page 6: ""The hyper-parameters of mainly include:"", remove of or modify sentence. Complex sentence on page 6: ""In order to quantitatively demonstrate the results, we used dice score [14], accuracy, precision and recall as the evaluation criterion, and computed these values between each predicted fiber tract and the  corresponding fiber tract atlas from each subject in the test set, and averaged the values among fiber tracts and individuals"" Complex sentence on page 8: ""Considering an extreme situation, for a fiber streamline, our method can only classify the all voxels on the fiber streamline as one fiber tract, but TractSeg may classify voxel #1 as tract #1, and voxel #2 as tract #2, and so on, but actually these voxels should belong to the same class, this apparently did not conform to the common sense in neuroscience and may resulted in the higher dice score than ours."" Complex sentence in Conclusion on page 8: ""In this paper, we proposed an effective fiber tract segmentation method, which can identify the accurate and common fiber tracts across individuals based on a novel representation of fiber's geometrical feature, called as FiberGeoMap, and we also tailored Transformer neural network to meet the input FiberGeoMap"""	Please refer to the comments in box 5.	This is a nicely designed method and shows a good result.	The paper proposes novel fiber tract segmentation approach including a novel geometric descriptor for fiber tracts and use of Transformers and self-attention for segmentation. The extensive evaluation shows significant improvement over state-of-the-art approaches. In addition, clinical application is demonstrated by comparing fiber density in autism versus controls.
027-Paper0159	ACT: Semi-supervised Domain-adaptive Medical Image Segmentation with Asymmetric Co-Training	Introduces the problem of semi-supervised domain adaptation (SSDA) into medical image segmentation Proposes a co-training framework that integrates UDA and SSL (semi-supervised learning) The empirical results show that the proposed approach performs well on the BraTS2018 dataset.	This paper focuses on semi-supervised domain adaptation (SSDA) in medical image segmentation. It first discusses the unsatisfactory performance achieved by unsupervised domain adaptation (UDA). Inspired by the recent success of SSDA methods on natural images, this paper proposes a new SSDA framework that is tailored to medical image segmentation. The proposed method is termed asymmetric co-training (ACT). Different from traditional co-training methods that describe each example using two different sets of features, ACT decouples SSDA into semi-supervised learning (SSL) and UDA, then applies the co-training strategy. Furthermore, this paper proposes exponential mixUp decay (EMD) to reduce the noise in generated pseudo labels. The proposed framework is evaluated on the BraTS18 database and outperforms previous UDA and SSDA methods by a large margin.	The authors propose an asymmetric co-training~(ACT) strategy to decouple labeled data into SSL and UDA for semi-supervised domain adaptation (SSDA). Two models are trained with labels from different domains and then boosted together with pseudo labels generated from each other. And exponential MixUp decay is proposed for gradual co-training. They validate the effectiveness of the proposed method on cross-modality brain tumor segmentation tasks, outperforming other SSDA and UDA methods.	The problem formulation of semi-supervised domain adaptation (SSDA) is highly relevant for medical imaging The proposed asymmetric co-training approach provides a framework for making use of both UDA and SSL The empirical results demonstrate ACT outperforms the baseline UDA and SSL models	The idea of asymmetric co-training is interesting and novel. Rather than describe each example using two different sets of features that provide complementary information about the instance, this paper decouples the source data and the target data into two sets which are then fed into an SSL module and a UDA module to achieve the co-training.	The idea is novel and works well. It is reasonable to generate pseudo labels for each other and boost each other for SSDA. The methodology is easy to follow. Gradual co-training is suitable and helpful for asymmetric co-training. The experiments are well-designed, which demonstrates the effectiveness of ACT. Ablation studies and sensitivity analysis can help further evaluate the feasibility of decoupling labels to SSL and UDA for SSDA.	The methodology lacks motivation from a machine learning perspective. It is unclear how the proposed approach can appropriately integrate the two components given the different assumptions in UDA and SSL. The empirical evaluation is on one dataset. It would be more convincing to include more datasets in the evaluation to ensure the proposed approach does not overfit to the dataset. Confusion between ACT and ACT-EMD. If ACT includes EMD, what is ACT-EMD denoting.	"This paper mentions that previous SSDA methods suffer from ""source domain supervision dominates the training"", is any reference or analysis to support this statement? What is the detailed workflow of SSL and UDA? There are lots of implementations of these two tasks, but this paper does not provide more details. Therefore, it is not clear whether the improvement comes from the new framework or more advanced SSL or UDA implementations. The meaning of SSDA:1 or SSDA:5 is not clear. Does it mean the number of labeled subjects (as claimed in the second paragraph of Section 3) or the number of samples (as claimed in the fifth sentence of the first paragraph on page 7) in the target domain? What does the ""subject"" mean? Since the labeled target samples are quite limited in SSDA: 1 and SSDA: 5, is that enough to train a model? Even so, the generated pseudo labels would be quite noisy which may result in negative transfer problems. Do other baseline methods share the same labeled target examples with ACT in the SSDA: 1 and SSDA: 5 settings? This paper is not well written. There are a lot of long sentences which are hard to understand. For example, ""In order to prevent a segmentor, jointly trained by both domains, from being dominated by the source data only, we adopt a divide-and-conquer strategy to decouple the label supervisions for the two asymmetric segmentors, which share the same objective of carrying out a decent segmentation performance for the unlabeled data"""	"Unlike the statement ""no SSDA for medical image analysis (MedIA)""There are many works about SSDA for MedIA [1]. And decoupling labels to different parts, such as SSL and UDA, is not new to SSDA, e.g., [2] [3]. The authors should discuss more recent SSDA works in the introduction section. Accordingly, the authors should compare the proposed method with more recent  UDA and SSDA works such as [2-4], besides these UDA and SSDA methods for natural images. [1] Guan H, Liu M. Domain adaptation for medical image analysis: a survey[J]. IEEE Transactions on Biomedical Engineering, 2021. [2] Zhao Z, Xu K, Li S, et al. MT-UDA: Towards Unsupervised Cross-modality Medical Image Segmentation with Limited Source Labels[C]//International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021: 293-303. [3] Li K, Wang S, Yu L, et al. Dual-teacher++: Exploiting intra-domain and inter-domain knowledge with reliable transfer for cardiac segmentation[J]. IEEE Transactions on Medical Imaging, 2020, 40(10): 2771-2782. [4] Chen C, Dou Q, Chen H, et al. Unsupervised bidirectional cross-modality adaptation via deeply synergistic image and feature alignment for medical image segmentation[J]. IEEE transactions on medical imaging, 2020, 39(7): 2494-2505."	Code is provided.	The manuscript misses some technical details, for example, the implementations of SSL and UDA in the proposed ACT framework. Fortunately, the code is provided.	The author submitted the code as the support material. There are no reproducibility concerns.	I think the paper needs a stronger motivation for the proposed approach instead of simply stating this is what we propose and it works.	"The writing needs to be substantially improved. A suggestion is to avoid using very long sentences which are unnecessary in most cases. Page 6: change ""Without loss generality"" to ""Without loss of generality"" Page 7: ""ACT-EMD"" in ""The better performance of ACT over ACT-EMD demonstrated the effectiveness of our EMD scheme for smooth adaptation with pseudo-label"" is confusing. Suggest using ACT (w/o EMD). It is suggested to briefly introduce ""Target Only"" and ""Supervised Joint Training "" in Table 1 and 2. It is not clear how they are implemented. For example, what is training data used in these two settings?  Table 1: change ""SSAD"" in Table 1 to ""SSDA"""	The authors should discuss more recent UDA and SSDA works for medical image segmentation, and compare with these methods in the experiment section to further prove the effectiveness of the proposed method. More cross-modality datasets, such as MM-WHS and Chaos, can be implemented for further evaluation.	The paper has good empirical results but lacks justification in the approach relating to the underlying distribution assumptions.	This paper proposes an interesting and novel idea to apply the co-training strategy to the SSDA problem. The performance is impressive. My concerns are (i) the experimental settings need more justification, (ii) some important technical details are missing in the manuscript, and (iii) the writing is not clear and needs to be improved.	The idea of the divide-and-conquer strategy for SSDA is well-motivated. The methodology and experiments are well-designed, and the results are promising for cross-modality segmentation. I highly suggest that the authors include more discussions on recent UDA and SSDA methods. Also, the authors should give more comparisons with these methods.
028-Paper2509	Adaptation of Surgical Activity Recognition Models Across Operating Rooms	This paper proposes a domain adaptation method for surgical action recognition.	The authors have proposed a method to overcome the lack of generalization problem on models trained to recognize the surgical activity across different operating rooms. The authors propose to adapt the model that was originally trained on the source domain by predicting pseudo-labels on the target domain and using them to retrain the target model using augmented versions of the pseudo-labeled clips. The pseudo-labels are generated on the target domain based on the most confident labels from the source domain. The level of confidence is based on a threshold that is determined for each class during the model training on the source domain by taking into account the class imbalance. The authors evaluated the domain adaptation strategy by only providing labels in the source domain (unsupervised) and by also providing labels to a small portion of the samples in the target domain (semi-supervised). Both non temporal and temporal features are used for the surgical activity recognition.	This paper tackled the problem of phase recognition from external ceiling mounted cameras. The proposed approach wanted to address the problem of generalization from one operating room to another one. They have explored the use of both unlabelled data and labelled data from the target OR.	The proposed pseudo label sampling is somewhat helpful; experimental results are good; presentation is clear and well-organized.	This paper presents several strengths, the state-of-the-art is complete, the method is well explained, the validation compare to state-of-the-art methods and an ablation study is done	generated a big dataset proposed a model for domain adaptation well written	The technical novelty is weak; Code is not available while the reproducibility responses are checked; Only one dataset is used.	The main weakness of the paper is the data description. There is no information about the type of surgeries and the annotation protocol	limited technical contribution limited information on the dataset, like procedure, video duration, clinical team, etc not sure how balanced the dataset is and only reporting accuracy and mAP might not be enough. I would expect to see precision, recall and F1 score experiment parameters are given in the supplementary material and not sure if it will be published with the paper	Code is not available while the reproducibility responses are checked.	The information provided will allow the reproducibility of this paper. The release of the dataset will be a plus to allow further comparison.	Experiment parameters are given in the Supp Materials, not sure if it will be published with the paper. The model trained and assessed on private dataset make it difficult to reproduce the results.	The paper is an incremental work for AdaMatch which is properly cited. Compared with AdaMatch, the contributions are 1) queued predictions; 2) video level augmentation; and 3) pseudo label sampling. However, the authors also remove the random logit interpolation from AdaMatch.	The description of the dataset could be improved. For example, there is no information about the 28 types of surgery. Did their all come for the same specialty, as gynecology or digestive? What are the 10 phases and how are they defined? How many observers have annotated the data? How the data were merged if there are several annotations for the same surgery?  What are the instructions given to the observers? The authors did not discuss the limitation of their model. Did some phases are more difficult to recognize than others? The authors talked about the execution time of their model (supplementary material) but not of other models. Are these execution times similar? and if not, does the performance increase justify the use of the proposed method?	the paper misses information on the dataset and we need to see other evaluation metrics that are often used in the literature.	The first and second contributions are clearly trivial and straightforward. As for the 3rd contribution, pseudo label sampling, it is clearly functionally similar to distribution alignment which is proposed by AdaMatch. While the authors compared their method with AdaMatch in Table 2, I still encourage the authors to provide ablation studies on which one (distribution alignment vs pseudo label sampling) is the key. Morevoer, I suspect that the slight disadvantages of AdaMatch compared with the proposed method may be attributed to the prediction cache (1st contribution). The authors are encouraged to also apply this trick to AdaMatch to fully justify the effectiveness of Pseudo Label sampling. Overall, the technical novelities of this paper are weak and the experimental results do not properly justify the necessarity of the proposed pseudo label sampling.	The weaknesses of this paper are minors, it is very clear and provide all information to understand and reproduce the work. Moreover, it addresses an important issue in workflow recognition methods: the lack of generalization.	Limited technical contribution,  lack of information on the dataset and would like to see more metrics.
029-Paper2006	Adapting the Mean Teacher for keypoint-based lung registration under geometric domain shifts	This work is built on top of the mean-teacher-based framework (which was initially proposed for semi-supervised learning), where it adapts the perturbation (e.g., scaling, translation) and enforces consistency as well as introduces GCN to extract features. They also adapt the integration-based LBP to obtain the displacement fields. As such, the method can perform well in exhale-to-inhale lung registration.	In this paper, the author proposed a novel approach for domain adaptation registration. They introduced a method based on key-points registration, the Mean Teacher paradigm and graph convolution network. They experimented on a different lung registration dataset and compared with last published method.	A new registration to cope with domain shift was proposed via a keypoint-based registration model alongside self-embedding within the mean teacher framework.	The paper is well written and easy to follow. Adapting the mean-teacher consistency for registration is interesting. The method is simple and shows effectiveness in exhale-to-inhale lung registration.	The method introduced in this paper propose a new method using Mean Teacher for registration adapation. The method is novel, simple and obtained very good results on two public datasets. The author proposed adaptation of the Mean Teacher framework to work in the context of registration problem.	The proposed method is based on the mean teacher framework alongside domain adaptation, overcoming two main drawbacks of deep learning, including the assumption of i.i.d. between source and target domain data and the need for massive training data. Domain adaptation for registration is not many compared with UDA for segmentation and classification	"Unclear and somewhat misleading motivations. As for the domain shift, it is somewhat incorrect to argue that ""The domain shift consists in exhale scans from the target domain exhibiting a cropped field of view such that upper and lower parts of the lungs are partially cut off"". This is not common sense for domain shift (CT-MRI, obviously different intensity distribution, etc). Particularly, the cut-off problem seems like because of pre-alignment preprocessing in the learn2reg challenge. I think it is somewhat misleading to regard this issue as a domain shift problem. This work is built on top of the mean-teacher-based framework (which was initially proposed for semi-supervised learning). Adapting the mean-teacher method into registration is interesting, yet, the motivation is not strong and also ambiguous here. Here, it seems like the paper just follows a semi-supervised paradigm, and tries to use another dataset as unlabeled data. Therefore, I will just regard the method as a semi-supervised registration method. The claim for tackling the domain adaptation problem is not strong. However, in the image registration community, acquiring ground truths are always infeasible, especially for those not-landmark-based methods. That is why unsupervised registration becomes popular. Thus, this method may lack practical values. Following the above concerns, the experiments are weak. Especially, since the proposed approach is inherently a semi-supervised registration method that enables learning with both labeled data and unlabeled data (w/o the landmark labels), it is unfair to just compare their methods with source-only, target-only. The method exploits more data (i.e., unlabeled data) during training, so it is not surprising to see the improvements. I mean, when you train with your limited labeled data, the method may struggle with overfitting, so it is not surprising to see a performance drop in your test set. Therefore, it is hard to evaluate the efficacy of this paper. This is just a discussion point regarding the motivation of using the mean-teacher framework in registration since I noticed an interesting work [1] that utilizes mean-teacher design to achieve adaptive regularization weighting during training for unsupervised registration. From [1], another insight of using mean-teacher design in registration is that the solutions to this ill-posed problem may vary greatly among different training steps, therefore [1] enforces the consistency to exploit the ""temporal"" information related to the ill-posedness. Authors can further discuss similar works, although having different motivations.  [1] Xu, Z, et al. ""Double-Uncertainty Guided Spatial and Temporal Consistency Regularization Weighting for Learning-based Abdominal Registration."" arXiv preprint arXiv:2107.02433 (2021)."	The proposed pipeline requires ground truth deformations phi to use the supervised loss. The author did not explain how they obtain/calculate these ground truth deformations. With a classic registration algorithm or a deep learning based algorithm ? Can this method be expanded to unsupervised registration ? The optimisation is performed without regularisation losses on the produced deformation. Is it a choice of the author ? Concerning the regularisation, the author do not discuss the performance of the regularisation in term of folding or standard deviation of the Jacobian. Do the proposed method produce smooth or noisy deformation ?	The utility of reverse augmentation is not clearly demonstrated via an ablation study.	Good. Authors have provided the code.	The method seems fully reproducible, and the code is provided.	* The authors shared their code, which thus is highly reproducible.	The motivation should be further clarified and re-considered. From my experience, it is inherently a semi-supervised registration method that attempts to utilize the unlabeled data via consistency. Claiming this a domain adaptation problem is strange to the image registration community. Experiments should be re-designed to support their arguments and motivations.	The quality of the paper could be improved by discussing the following points : Could the proposed Mean Teacher approach be expanded with CNN instead of GCN and with full volumes instead of points clouds ? The regularity of the proposed method (negative Jacobian/foldings) The adaptation to unsupervised registration without groundtruths.	This paper fills in the gap of the need for establishing local correspondence in the context of UDA for registration which is deemed new and innovative. Specifically, the authors combined the optimization registration approach (i.e., LBP) with the mean teacher framework. It is surprising that the proposed method even outperformed the target-only methods (including VM++ and target only), which is typically considered an upper-bound in other UDA tasks (e.g., segmentation or classification).	The motivation is a little misleading with insufficient examples and experiments to support the arguments.	The method is novel, obtained very good results and the paper is very well written and presented.	The paper deals with the area of UDA for registration which is underexplored. The methods are new and solid, built upon well-validated approaches. The experiments are through and the results are convincing.
030-Paper0272	Adaptive 3D Localization of 2D Freehand Ultrasound Brain Images	The authors developed a method to localize a 2D ultrasound image in the head of a fetus during an ultrasound examination.	This work contributes with a method to adapt a previously trained CNN for the localization a 2D ultrasound plane inside a 3D volume. The contribution allows for the application of the trained CNN to different sets of ultrasound volumes and video sequences.	Authors designed a framework that adaptively localizes 2D ultrasound images in the 3D anatomical atlas. They also fine-tuned their method after getting instances which provides double accuracy in quantitative results.	The algorithm is able to localize the plane in the head of a fetus The analysis is sound	The method for adaptation of previuosly trained CNN is a novel contribution. Testing was performed in an adequate set of 3D volumes (17) not used for training, as well as testing in 7 video sequences of free hand 2d ultrasound with more than 1000 image frames (planes) in total	The method requires limited number of frames to be manually annotated which is good. Unsupervised fashion of the method alow the technique to adapt to 3 different datasets from different US machines. Authors claimed that the overall displacement of a video in the 3D anatomical atlas is equal to the displacement from the first image to the last in that video. Authors claimed that compare to the baseline methods they fine tuned their method to produce more accurate localization accuracy.	The clinical motivation is not clear The statistical analysis requires some clarification.	"In my opinion the main weakness is the testing performed on free hand 2D ultrasound images, since the location of the image planes inside the volume is unknown. The authors propose a ""rate of change"" index to estimate accurate localization of planes, it seems to me that this index can result in optimum values for an smooth sequence of contiguos images, which are wrongly located in the US volume (e.g. an smooth sequence of planes parallel to the trancerebellar plane may have optimum values of ""rate of change"" for the bottom half of the cerebellum as well as for the top half of the cerebellum)"	This might be a little personal idea but it is a fact. In medical imaging, when we are reconstructing 3D image from voxels from 2D freescan Ultrasound frames, we need to be around 100% accurate in reconstruction. The reason is that we are looking at patient organ or lision. We cannot use estimation like the author's idea or train a model to generalize one image to other image. maybe in test data, we have a new problem in patient which was not in training dataset at all. Therefore, still using sensors and 3D ultrasound probes is the best way to solve this problem, and community should work on decreasing the cost of those techniques and increasing the performance and accuracy. It is not acceptable that every problem can be solve by training a deep learning model. For this reason, the proposed idea is not suitable for medical imaging. Estimating and predicting location of a 2D frame can be used for animation design or other engineering applications. Now this generalization issue can be see more clear when we are generalizing from Atlas to real human organs. This become even more worse when we are using unsupervised techniques for localization of frames in 3D slace.	OK	The work seems reproducible	I couldn't fully understand Regression ConvNet because authors didn't provide any details. Supplementary data was helpful for better understanding the work.	It is not entirely clear why localizing the 2D image in the application you described is needed. Is it to find the optimal plane to perform measurements? Although the 2D ultrasound approach can be variable, does it take much time and is the variability significant? Although plane detection can be useful in many clinical applications, I think the motivation for this work needs some more justification. What is used to normalize NSTD (normalized standard deviation)? Is it the mean value? Table 1. It would be good to understand ED in units of mm to give the reader a better understanding of the error. Table 1. Since it appears you performed statistical tests, it is not clear whether you corrected for multiple t-test to avoid a type 1 error, i.e., Bonferroni correction. I am not sure how to interpret a NSTD of 0.553. It is lower than what was generated in Yeung at al, but I am not sure if it sufficiently low to be used clinically. It would help to know what NSTD was normalized with.	A possible evaluation of free hand 2D US image location inside a 3D volume, could be performed if the set of 2D images is acquired with a tracking device attached to the US probe. Then the set can be reconstructed accurately and all the plane positions calculated could be evaluated against this reference volume. What is the voxel size in table 1? I suggest to include table 3 in the main text	"The manuscript was written smoothly, and I like reading that. It is better to focus more on qualitative results than providing many formula for training objectives.  Literature has two great papers in this field that can be cite in introduction: Mohamed, Farhan, and C. Vei Siang. ""A survey on 3D ultrasound reconstruction techniques."" Artificial Intelligence--Applications in Medicine and Biology (2019). Mozaffari, Mohammad Hamed, and Won-Sook Lee. ""Freehand 3-D ultrasound imaging: a systematic review."" Ultrasound in medicine & biology 43.10 (2017): 2099-2124."	My main concern is the clinical motivation. The reason for developing the method is not convincing.	The original contribution is significant and has a wide scope for application and further improvements, maybe trackless 3D reconstruction if adequately validated.	In my opinion, working at datasets from different ultrasound machines is valuable. Especially when the goal is training novice sonographers. Although novelty of the work is not completely enough for MICCAI, but publication of the work will attract many attention in ultrasound community readers.
031-Paper2848	AdaTriplet: Adaptive Gradient Triplet Loss with Automatic Margin Learning for Forensic Medical Image Matching	"This paper improves triplet loss by imposing a panelty on the ""hard"" triplets whose negative sample stands close to the positive sample and the anchor in the feature space. The panelty was implemented by an adaptive gradients depending on the difficulty of negative samples. The proposed method greatly improved the matching results on the selected datasets."	This work proposed a new AdaTriplet loss modified from the Triplet loss to improve the image matching problem over the hard negative samples. It also proposed an AutoMargin method to adjust margin hyperparameters during the training.	This paper investigates image retrieval for forensic medical image matching by introducing a new triplet objective with corresponding margin adaptation method. In particular, the authors propose AdaTriplet, which combines the standard triplet loss with a simple regularization on anchor-negative distances, and AutoMargin, which uses distance statistics to automatically adapt both the standard triplet margin and the additional regularization margin parameter. The performance of these methods is evaluated on two FMIM benchmarks, showcasing convincing performance especially when the subject time differences increase.	The proposed method was mathematically proved in detail and the main concept of the target problem and resolving ideas were clearly descripted and visualized. The proposed method was effective on the selected datasets throught reasonable experiments, and make significant improvement with minor modifications.	The writing is very good. The problem is clear, the existing method is analyzed, and the proposed method is well motivated. The figures in the manuscript help a lot in understanding the work. Both theoretic analysis and the experimental results are presented in the manuscript. Alation studies shows the effectiveness of the proposed method.	The paper is very well written and structured, with the proposed methods themselves being well motivated. The performance of Adaptriplet + Autogmargin is quite convincing (see Section 5 for some issues with this particular aspect.).	As an alternative of the general loss function in field of metric learning, the proposed method should be evaluated on more mainstream architectures and datasets. Some expressions are confusing and need to be modified.	"Minor: The purpose of th AutoMargin is to choose the hyperparameter margin automatically. However, the AutoMargin method (equation 7 and 8) has new hyperparameter K_delta and K_an. How to select these two hyperparameters? More explanation on this statement ""we want to increase the virtual thresh- olding angle between anchors and negative samples, which leads to the decrease of b(t)"" ? Is b(t) the threshold in AdaTriplet loss to ensure f_n is far from f_a?"	"While the FMIM problem is incredibly interesting, it is not entirely clear how AdaTriplet specifically tackles this problem, and it would be important to clarify/investigate this in more detail. Instead, it is proposed as a standalone novel Deep Metric Learning method. However, for this to work, much more extensive discussion of other existing ranking objectives, but in particular negative mining strategies, have to be discussed in conjunction, with AdaTriplet and Automargin sharing similarities to existing works such as margin loss (Wu et. al, ""Sampling Matters in Deep Embedding Learning"") or Smart Mining (Harwood et al., ""Smart Mining for Deep Metric Learning""). The issue of less informative/reductive triplets can be often already addressed with a tuple mining approach. In general, there are many more recent approach in Deep Metric Learning to account for - given that classification-based approaches perform much worse than sample-based methods, it would be important to compare to stronger sample-based methods with tuple mining (c.f. e.g. Roth et al., ""Revisiting Training Strategies and Generalization Performance in Deep Metric Learning"" or Milbich et al, ""Characterizing Generalization Performance under Out-of-Distribution Shifts in Deep Metric Learning for a list of more Out-of-distribution capable extensions). Particularly the automargin approach should have similar effects as adaptive mining methods such as Smart Mining (Harwood et al., ""Smart Mining for Deep Metric Learning"") or PADS (Roth et al., ""Policy-Adapted Sampling for Visual Similarity Learning), of which at least one should be compared to show whether AutoMargin performs competitively. It would help the readability if some of the formulas and symbols are replaced or extended with full sentences/descriptions. Currently, the paper is very densely packed with a large collection of different variables and notations, which often requires multiple additional passes over various text passages. Small note w.r.t 2.2: The reason to operate on the hypesphere (i.e. for normalized embeddings) is the much better scalability to higher-dimensional representation spaces (see e.g. Wang et al., ""Understanding Contrastive Representation Learning through Alignment and Uniformity"" or Roth et al., ""Revisiting Training Strategies and Generalization Performance in Deep Metric Learning""), and not just the fact that the margin is better behaved."	This paper is highly reproducible.	The code is provided so it is reproducible.	All relevant hyperparameters and pipeline settings are clearly listed, giving me no reason to doubt the reproducibility of the paper results.	Some sentenses are confusing and affect the readability. For example, in Introduction, 'Unlike general CBIR, longitudinal medical imaging data of a person evolves in time due to aging and the progression of various diseases'. In Section 2.4, 'The prior work [22], considered incorporating an additional term that is minimized when a hard negative example is detected.' Some abbreviations need to be written in full with proper explaination when they first appear, such as CV.	This a great work. My questions according to the AutoMargin method is listed above in the weaknesses section. The authors mentioned that they will test more models and datasets. I totally agree with that.	The paper is very well written and structured, and both AdaTriplet and AutoMargin make intuitive sense. Unfortunately, the paper disregards the large corpus of existing DML methods, in particular more recent sample-ranking objectives and tuple mining approaches, which effectively tackle what both methods aim to do. As such, the paper would heavily benefit from an extended experimental section which compares to some of the more recent sample-based objectives alongside respective tuple-mining methods.	The methodology of this work is concise and of elegant simplicity with good portability. It achieves significant results.	The method is novel, the analysis is comprehensive, the writing is clear, and the results are good.	Given the aforementioned points, I am currently opting for weak reject - while the paper is very well written, and both proposed methods are well motivated, the lack of comparison to the current Deep Metric Learning literature, as well as it not being clear to me how the proposed method in particular addresses the problem of FMIM make it hard to go for acceptance. I am however certainly open to have potential misunderstandings of mine pointed out during the rebuttal to update my score.
032-Paper1371	Addressing Class Imbalance in Semi-supervised Image Segmentation: A Study on Cardiac MRI	This paper proposes an approach to improve the performance of semi-supervised learning for under-performing classes in imbalanced datasets. It proposes to record confidence indicators (entropy, variance, confidence) during training for every class. The indicators are combined using fuzzy fusion. A class sampling scheme uses the confidence score to sample classes. A dynamic training stabilization scheme is also proposed, which redistributes the losses from convincing and under-performing samples.	The paper presents a novel approach to address the class imbalance problem in semi-supervised image segmentation.	The submission presents a training strategy for student-teacher network to deal with the class imbalance problem in cardiac MRI segmentation. By investigating class-wise confidence and class-wise sampling rate, authors improve commonly used cross entropy loss by focusing more on less confident classes, and they further reach better training stabilization by utilizing dynamic modulation of weights.	The ablation study demonstrated the value of RCS, fuzzy fusion, and DTS, when added to the baseline model.  The proposed model was evaluated on two different datasets. Performance is not very sensitive to the values of hyperparameters beta and lambda.	The main strength of the paper is its novelty in algorithm formulation. The proposed training scheme includes several novel elements such as category-wise confidence scores, fuzzy adaptive fusion, class-wise resampling, and dynamic training stabilization.	(1) The application of cardic MRI segmentation, the semi-supervised algorithm and the proposed training strategy are tightly matched. (2) Most of the formulas are written clearly and detailed. (3) Experiments are fully conducted on hyperparameters, ablation studies, and method comparison. (4)Two public datasets are utilized to demonstrate generalizability.	The paper aims to improve the performance of the under-performing class. However, the results do not show the benefits for the under-performing class, as metrics such as DSC are used. There are no results/discussions of the effects on the under-performing/minority class. The performance improvement of the proposed method for various levels of class imbalance (imbalance factors) needs to be demonstrated. How effective is each component when the classes are extremely imbalanced / fairly balanced? The proposed method was not convincingly superior to SOTA on ACDC. Average DSC was lower for L=1.25% and 2.5%, and very similar to PCL and Global+Local CL at 10%. PCL results for MMWHS are missing. The performance improvements due to each of entropy, variance, and confidence are not clearly demonstrated. In the supplementary, the evaluation appears to use DTS in the model (judging from CC results). What is the performance of each indicator without DTS?	The main contribution of the paper is to address class imbalance during training. The application is, however, limited to the semi-supervised learning, and the evaluation is only performed on cardiac MRI datasets.	(1) Too much formulas and notations which are hard to follow. (2) It's unclear whether the improvement on under-performing clases will destroy the performance on other classes.	Satisfactory. Datasets are publicly available, and methods are described with acceptable detail.	The paper presents enough details to reproduce the results on public datasets.	No code sharing was mentioned in the submission. The datasets are publicly avaible.	The paper may consider the following to improve: Demonstrate effects on the sensitivity of each class (especially the minority and under-performing class). Demonstrate the value of the proposed method for various levels of class imbalance (imbalance factors). Further improve the proposed method to convincingly beat SOTA on ACDC.  Include PCL results for MMWHS. The writing needs to be polished to improve readability and fix grammatical errors.	"The proposed method is novel and interesting. Can the method be used with fully supervised or self supervised learning? Can the method be applied to radiological images other than cardiac MRI? In Table 1, the PCL method also shows good results for L=2.5% and 10% in ACDC. But there is no result of PCL for the MMWHS dataset, can the authors explain why? On page 4, how the range for R_c^k was determined? How about the penalty values  for P_c^CCF and P_c^FR? ""ACDC"" needs to be referenced first time in the text."	(1) It's highly recommanded to make Figure1 more detailed to include all of the main notations and show all of the components of the proposed training strategy. (2) Please clarify in Equation (6) that how entropy, variance and confidence are fused. (3) Please add comparisons on each single anatomical object in ablation studies and/or method comparison to demonstrate that the proposed strategy won't let those good-performing classes decay. (4) Please have a discussion on, after applying class-wise sampling rate, how choosing pixels on boundary, inside the object, or purely randomly will influnce the segmentation accuracy.	The main weakness is that the paper aims to improve the performance of the under-performing class, but the results do not show the benefits for the under-performing class. There are no results/discussions of the effects on the under-performing/minority class.	The paper focuses on a new training algorithm including a set of new mathematical formulation, and thus the methodological novelty is strong. The details of the experiments and parameter tuning is provided in the paper and in supplementary material. The method was well evaluated by comparing with other methods, and an ablation study was performed to show the effects of different components of the method. Finally, the result looks promising although the evaluation was only carried out on cardiac MRI datasets.	The application, the algorithm and the proposed strategy are tightly matched. It can be better after clarifying some details.
033-Paper0332	Adversarial Consistency for Single Domain Generalization in Medical Image Segmentation	This paper proposes synthesizing new (unseen) domains in an adversarial manner for domain generalization in medical image segmentation. An adversarial domain synthesizer (ADS) is developed to generate images with the hardest perturbations. Mutual information is calculated to preserve the semantic information in the synthesis image. The performance has been evaluated on various organ segmentation tasks.	The authors propose a single domain generalization method that can be trained on a single source domain while being generalizable to various novel testing domains. The proposed method is based on an adversarial domain synthesizer that plays a contradictory role as a segmentor, making the segmentor sufficiently generalizable after model convergence.	The paper proposes a strong assumption for single domain generalization that the distribution of unseen domains belong to the distribution of synthetic domains. Under this assumption, they design a new adversarial augmentation method for organ segmentation. The experimental results achieve the state-of-the-art and show the effectiveness under different settings. In general, the organization and overall motivation are clear.	This paper is nicely organized and easy to follow. The authors have done a good survey and analyzed the limitations of existing methods. The proposed adversarial synthesizing method with the guidance of mutual information regularization is relatively new in DG. The algorithm is clearly presented. Both quantitative and visualization results are reported to verify the effectiveness of the proposed method.	The concept of the adversarial domain synthesizer is intriguing and novel.	1.The method is novel. This paper designs an adversarial domain synthesizer with a mutual information regularization, which can reduce the negative effects of adversarial learning and achieve high generalization performance. 2.An interesting use of contrastive loss. Using the patch-level contrastive loss as a surrogate for mutual information estimator, which can reduce computational cost and achieve similar results.	The mixup ratio is not clearly introduced.  Are there any strategies for collecting patch pairs from the source and the synthesized images? Experiments are only conducted for CT->MRI. How about the results of MRI->CT?	"The authors make such a bold assumption that the real unseen domains are subsets of the collections of synthetic domains. However, the experiments are not sufficient to support such an assumption. Many key questions have not been fully addressed: 1) Are there any theoretical guarantees that support the above assumption? 2) Is there bias between the synthetic domains and the real unseen domains? For example, do the synthetic images show low noise level? 3) What is the segmentation performance on source domain where the segmentor was trained? How does the segmentation model perform after domain generalization compared with being trained on real target domains? I don't think using an adversarial framework can avoid overfitting to a regular pattern, as claimed by the authors. It is almost practically impossible for a GAN model to converge to its global optima. In real application scenarios, it is very likely that the adversarial synthesizer only covers a subset of all possible ""styles"" in medical imaging. The experiments section is not well written. Specifically, Secs 4.3 and 4.4 are very short and Fig. 3 is very confusing. It is unclear for the general audience which subfigure corresponds to which experimental settings (CT/MR or multi-scanner). The paper lacks some insights about model design. For example, why the authors use two different $T$ networks but not one with different $z$ samples? The authors should talk about the limitations of their method. I believe the domain generalizability of the proposed method is not unlimited."	1.The reproducible findings are not clear from the current submission. The detailed information of how to use the random parameter z during training is not mentioned. 2.The motivation of using patch-level contrastive loss is a little bit weak, the contribution would be more clearly if the paper can compare several different mutual information estimators. The assumption is too strong and needs more explanation between the assumption and proposed method.	The reproducibility of this paper is relatively high because most implementation details, such as batch size, learning rate, and parameters of the optimizer, are provided. The authors also describe the details of data prepossessing and evaluation metrics.	Good. The authors listed the data source, model architecture, and promised publishing their code after acceptance.	clear for the experiment settings and network architecture.	The descriptions of ADS network architecture are some implementation detail, which is better written in the experiment section to increase the model's flexibility. There is a gap between the image X and its bathes (X_p and X_n). More explanations are needed to clarify equation 2. The paper argues that the proposed method outperforms the meta-learning-based methods on computation cost. It would be better to give some theoretical or experimental evidence. As the proposed method includes an adversarial network, the authors should consider analyzing the convergence of the algorithm.	"The idea of using adversarial domain synthesizer for better generalizability is interesting. However, the authors should narrow down their focus and application scenarios (e.g., which imaging modalities, healthy or pathological data, etc) and show experimentally and (hopefully) theoretically that the method holds the water. Given that the objective is domain generalizability, showing cases that the method fails is particularly critical. The authors should present some examples of the synthetic images and explore how samples $z$ affect the ""style"" of the synthetic images. Figure 2 is inconsistent with the text. Fig. 2 left (blue boxes) shows negative examples are selected from $X$, while Fig. 2 middle shows negatives are selected from $\hat{X}_n$. Additionally, $f_q$ in the text is inconsistent with $f_n$."	I think overall the paper is good and interesting, but the detailed motivation of loss function and network architecture design need to be reported for providing decent insights.	As mentioned in the strength section, this paper is satisfactory in the aspects of novelty, algorithm design, and experiments.	Although the method itself shows some merit, the paper overall is not clearly written. The experiments being insufficient to support the hypothesis of the paper mutes the excitement.	The organization of the paper is good. Some technical novelty.
034-Paper1829	Adversarially Robust Prototypical Few-shot Segmentation with Neural-ODEs	The paper proposes a novel framework/architecture PNODE that can be employed to train adversarially robust few-shot segmentation models.	This submission proposes a novel adversarial defense framework against attacks on few-shot segmentation. Authors demonstrate the superior performance of their method to the traditional adversarial training. They also show the proposed framework generalizes well to common adversarial attacks such as FGSM, PDD and SMIA as well as to multiple data sets in both in-domain and cross-domain settings.	In this work, the authors propose to use Neural-ODE for Few-shot Segmentation, which is robust to adversarial attack. The proposed PNODE method, based on Neural ODE, doesn't require prior knowledge on the type of adversarial attacks. The authors perform experiments comparing with multiple baseline methods, the results show that the results are more robust to adversarial attack.	The topic of adversarial robustness is an interesting and relevant topic to the computer vision as well as the medical community, thus the motivation for the work is solid.	Novelty: the adversarial attacks on few-shot segmentation (FSS) with deep neural networks and their defense mechanisms have not yet been explored. Clearly there is a need for such robust models in clinical setting. Literature review: systematic and complete reviews on neural ODE, FSL, Adversarial robustness Experiments: extensive experiments using three benchmark data sets across multiple segmentation models and attacks methods.	The authors perfroms extensive experiments comparing with different baseline methods, the evaluation results show that the proposed method outperforms baseline methods. The idea of using neural ODE for prototypical few-shot segmentation is interesting. The writing quality is good.	"The introduction starts with a number of bold statements that come with no references and, to me, without those references, it appears the entire premise of the paper is incorrect. The authors state that the lack of available data makes models vulnerable to adversarial attacks and proceed to cite three references in the paper (references 1,2,3 in the paper) that do not support this statement. In fact, availability of the data has nothing to do with adversarial robustness of a model. If more data would make a model more robust to adversarial examples, models trained on ImageNet (more than a million training samples) would be more robust to attacks as compared to CIFAR, which is not the case. What confused me the most in the paper is the usage of the terminology ""framework"", as in, the authors claim that they propose a framework to make few-shot segmentation segmentation models more adversarially robust. However, in the experiments section, the proposed framework PNODE is detailed as having a CNN backbone following a Neural-ODE block, where this backbone is different than all other compared architectures. So, does it mean that the authors propose an architecture rather than a framework? Because if it is a framework that is applicable to any feature extractor, the expectation is to see performance (clean accuracy, robust accuracy) of (a) model, (b) model trained with SAT, and (c) model with PNODE, where the robust accuracy results obtained with (c) is hopefully better than both (a) and (b) while clean accuracy is comparable. Authors also claim that PNODE's clean accuracy is even higher than other architectures, but, is this improvement in the accuracy attributed to Neural-ODE or the superior architecture in the backbone? I do not understand why the authors did not provide the same results for the model employed in the backbone so that we can make a fair comparison between models/framework. As it is, experiments section leaves much more comparative results to be desired. Authors also do a poor job of literature reviewing for robust few-shot model models. The paper could use a discussion on how PNODE differs (advantages vs disadvantages) compared to some other work in the field [1,2,3]. Finally, the references are not up to the standards of the MICCAI. Sometimes the first names are shortened (Paschali, M.) other times full name is writte (Cihang Xie). Also the venues of publications are not consistent (only acronym, full name, full name + acronym). Please refer to the publication guidelines for the correct and consistent reference format. [1] Tan et al., Towards A Conceptually Simple Defensive Approach for Few-shot classifiers Against Adversarial Support Samples [2] Goldblum et al., Adversarially Robust Few-Shot Learning: A Meta-Learning Approach [3] Liu et al., Long-term Cross Adversarial Training: A Robust Meta-learning Method for Few-shot Classification Tasks"	No major weakness as far as I can see. Authors may consider citing more related papers, for example, Kang, X et al., Adversarial Attacks for Image Segmentation on Multiple Lightweight Models, IEEE Access Vol. 8, 2169-3536 Daza, L, et al, Towards Robust General Medical Image Segmentation. MICCAI-2021 Authors may consider testing your method using an ensemble of attacks such as AutoAttack.	"The authors claim that the adversarial robustness of proposed method come from the fact that the integral curves of Neural-ODEs are non-intersecting. The authors should expand on this and give more details to explain it. There are a few pervious work that already propose to use Neural ODE for adversarial robustness. e.g. [1] [2], the authors may discuss them in related work. [1] Shin, Yu-Hyun, and Seung Jun Baek. ""Hopfield-type neural ordinary differential equation for robust machine learning."" Pattern Recognition Letters 152 (2021): 180-187. [2] Yan, Hanshu, et al. ""On robustness of neural ordinary differential equations."" arXiv preprint arXiv:1910.05513 (2019)."	No issues on reproducibility that I can think of.	Authors provide github website so it is acceptable.	The authors agree to release the code when the paper is accepted, and the datasets they used is publicly available.	1- Support the claims made in the introduction with correct citations. 2- Improve experimental section with correct comparisons. 3- Compare the work to the existing methods of adversarially robust few-shot learning methods.	"As above, citing relevant references and test your method with auto attack  Minor: Defence should be ""defense"""	The authors can perform ablation study to demonstrate the effectiveness of proposed components. From Fig.2 (right), the proposed PNODE method's performance seems to drop the most as the attack intensity increase, can authors provide some explanation for that?	See the weaknesses section.	Novelty, presentation quality and experimental results.	I think overall extending Neural ODE to enhance the adversarial robustness for FSS is interesting. And the authors conducted extensive experiments and compare against a range of baseline to demonstrate the effectness.
035-Paper1248	Agent with Tangent-based Formulation and Anatomical Perception for Standard Plane Localization in 3D Ultrasound	This paper proposes an RL based approach to search for the standard plane in 3D ultrasound. Contributions/claims: This paper introduces a tangent-point based Standard Plane (SP) localization method that restricts the search space making optimization easier. Instead of relying on pre-registration it uses imitation learning to initialize the agent, thereby making optimization further easier. It proposes an auxiliary task to enable differentiation between non-SPs and SPs. It proposes a reward function that uses spatial and anatomical information to guide the agent.	The paper defines a new tangent-point-based plane formulation in RL to restructure the action space and significantly reduce the search space; designs an auxiliary task learning strategy to enhance the model's ability to recognize subtle differences crossing Non-SPs and SPs in plane search; and proposes a spatial-anatomical reward to effectively guide learning trajectories by exploiting spatial and anatomical information simultaneously.	The authors propose a localisation framework that uses RL with a novel tangent-based formulation and a novel anatomical landmark-based reward. Additionally imitation learning-based initialisation and an informative auxiliary task of state-content-similarity prediction, are used to aid  learning.	Strength: The paper is very well written The idea of using a tangent point to uniquely represent a SP is pretty nifty as the search space is reduced to the surface of the sphere of a given radius. The paper clearly formulates the problem as a Reinforcement Learning Problem by clarifying the state and action space and reward function The figures are clear and well-produced. The supplementary training video is quite cool!	The formulation of the SP localization is essential for optimizing the RL framework. This work proposes a new formulation for SP localization in RL. The proposed formulation is unrestricted by directional cosines coupling with less action space than the previous ones (6 < 8). The reward function affect the optimal searching policy with the action. Considering the abnormal data, the reward function encourage the agent to perceive anatomical information.	A novel way to frame the problem of localisation with a novel tangent-based formulation and a novel anatomical landmark-based reward; combining an auxiliary task with RL problems is also fairly uncommon and thus has some novelty Sufficient details are provided in order for the paper to be reproducible Ablation studies and experiments comparing different methods are useful and insightful	"The agent is pre-trained by supervised learning with the ground truth data. The auxiliary task learns a representation of the SP close to the target SP by supervised learning. Although they are great tools, they raise doubt to what degree the success of the agent depends on the RL algorithm, but not due to the supervised learning segments. That also poses the question ""do we even need RL here?"" An ablation study might be useful to answer these questions. The results section compares the method with a regression based and registration based method, but it does not say anything about what would happen if we only use the imitation learning module along with the auxiliary task to refine the features. How is a tangent-point based solution better than the directional cosine and distance from the origin based translation? Fig. 2 makes some effort at explaining it through some great visualizations, but the benefits are still not clear in Section 2.1. ""The coupling among the directions makes actions dependent""- It's not clear since x, y and z are orthogonal and thus have no impact on each other."	The anatomical structure reward is essentially a reward based on landmarks reward. For some designed module, there are lack of ablation study to demonstrate the significance of the tangent point formulation and imitation learning. It only show the impact of SCSP and SAR. Compared with model which without SCSP and SAR, the performance of proposed model has no significant improvements especially in SSIM, in ablation studies. .	"Proper justification is not provided for some claims e.g. ""rely on initial registration to ensure data orientation consistency. They are easily trapped when pre-registration fails""; evidence in the form of citations or preliminary experiments would be useful in order to justify claims like this Some unclear wordings should be clarified e.g. ""ability to recognize subtle differences crossing Non-SPs and SPs in plane search""; what does 'crossing' mean here? how do you define subtle (maybe provide an example of what is subtle)? etc. The components of the formulation e.g. imitation learning-based initialisation, anatomical feature-based reward and tangent-based formulation are mostly well justified but would have been good to see some more citations of previous works that utilise some of these techniques Statistical tests to compare methods are not performed; these help to justify claims of superiority of one method over another, without them it is difficult to draw meaningful conclusions from the results; however, spread is reported which aids in drawing some conclusions"	The paper mentions of the training process some relevant hyper-parameters in section 2.3 and 3.1. But the submission did not accompany any code. In the absence of a codebase, reproduction might be challenging.	The model is described in sufficient detail, and the authors outline their experimental procedure clearly. The supplementary provides a video which shows the navigation SPs in 3D US.	Sufficient details are provided in order for the paper to be reproducible	"Some clarifications that might improve readability: The actions are defined as change in the tangent point location along the x, y and z axis. The section after that definition is confusing: ""we model the agent-environment interaction as a multi-stage motion process by progressively scaling down the step size from 1.0 to 0.01 when the agent appears to oscillate for three steps"". So, what is the final definition of the actions? Explain how the 2D image is reconstructed from the tangent point. It's not clear how the ground truth heat-maps were computed. Were those available with the dataset? If not, briefly describe the heat-map generation process. Improve the Fig. 1 caption. Currently it says nothing about the figure. Figures should be self-sufficient."	The paper is well-written and proposes a solution to a challenging clinical problem.They demonstrate significant improvements over the state-of-the-art methods. Their model has robust performance on abnormal data. In ablation studies, the performance of proposed model has no significant improvements especially in SSIM. The initialization of agent is important to learning performance. The proposed model adopt imitation learning as an initialization of the agent. There is also lack of ablation study to demonstrate the efficiency of the imitation learning. In the comparison experiments, the proposed model compare with  the RL_wsadt and RL_avp which are based on the traditional formulation (8 parameters).  I wonder the performance of the proposed model based on traditional formulation with SCSP and SAR module.	Overall this is a strong contribution to the field with only minor weaknesses, which if addressed, would make the paper stringer. In particular if some of the comments about justifying some claims and performing statistical tests, that would make the manuscript even stronger and more self-contained.	Well written paper, clear description of the methodology.	The formulation of the SP localization is essential for optimizing the RL framework. This work proposes a new formulation for SP localization in RL. The proposed formulation is unrestricted by directional cosines coupling with less action space than the previous ones (6 < 8). The reward function encourage the agent to perceive anatomical information. The tangent point formulation has been applied in other image application such as 3d reconstruction of  fetal brain in MRI. In results of comparison experiments, the proposed method outperforms all of the others on the most of the metrics. For the ablation studies, performance of proposed model has no significant improvements especially in SSIM. but some designed module lack ablation studies.	The paper is well-organised, presents a novel methodology and mostly justifies the methods well, whether it be in the design choice stage or post-results; some small weaknesses are present (see above), which if addressed, would make the manuscript stronger.
036-Paper1552	Aggregative Self-Supervised Feature Learning from Limited Medical Images	The manuscript presents two strategies of aggregation in terms of complementarity of various forms to boost the robustness of self-supervised learned features, i.e., a principled framework of multi-task aggregative self-supervised learning from limited medical samples to form a unifed representation, with an intent of exploiting feature complementarity among different task and an auxiliary loss function based on a linear centered kernel alignment metric to self-complement an existing proxy task. These two strategies can effectively boost the modeling robustness and capability.	Authors propose strategies for various aggregate forms of pre-defined self-supervised learning tasks to boost the performance. The prior papers mostly combine all SSL tasks together, which can potentially harm performance. This framework is helpful in general when we have several options for SSL tasks and want to find a way to combine these SSL tasks.	This paper targets self-supervised feature learning (SSL) for low-data regime. To achieve this goal, two techniques of aggregation in terms of complementarity of various SSL tasks. The first technique is a principled framework of multi-task aggregative SSL. Tasks are iteratively added to exploit feature complementarity among different tasks. The second technique self-complements an existing proxy task by an auxiliary loss function. Experimental results on two medical image classification dataset show improved accuracy over the existing works.	The proposed method is novel and effective. The authors conducted extensive experiments to verify the effectiveness of the two proposed strategies. The manuscript is well organized, clear and easy to follow.	Their multi-task aggregative SSL (MT-ASSL) uses linear centered kernel alignment (LCKA) to align the feature representation of two neural networks to look interested and novelty. Self-Aggregative SSL makes sense and is able to apply to other settings. In general, Reviewer thinks that the novelty of the method in this paper is good enough.	This paper is well-motivated. SSL needs a large amount of samples to train, and collecting medical images is expensive. Achieving good performance of SSL by using limited data is necessary. The idea of aggregating multiple SSL tasks to improve the representation learning is interesting. The writing is clear.	The dataset split and backbone selection might be not appropriate.	One of the main weaknesses of this paper is the baseline setting. In particular, the main contribution of this work is the strategy to combine SSL tasks. The author also mentioned that this work is different from conventional methods in which all SSL tasks are combined and trained together. Therefore, one of the crucial experiments shows the difference between their approach and default settings. For instance, in Table 1, it would be useful if the authors could provide an experiment for training together all SSL tasks and compare them with the proposed method (SRC, SimCLR, 2D Rot). A similar result should be done for the 3D Brain hemorrhage dataset as well. Given this evidence, the contributions of this paper will be more convinced.	The training cost of SSL aggregation is high. It needs iterative training of each SSL method. The reported accuracy of existing SSL works is low and inconsistent with existing works.	The reproducibility of the paper is good since most of the important implementation details are provided in the manuscript.	Authors well-described experiments in this paper; thus, it is possible to reproduce their experiments.	The experimental setup is described in detail. Following the description can reproduce the results.	(1) There is a typo in Eq.(2), please correct it. (2) In Fig.1(b), it is suggested to highlight the iteration of training. So please modify it. (3) Please correct line 8 in Algorithm 2. (4) The two datasets are separated into training and testing sets according to the ratio of 80:20. However, in the training process, an extra validation set is needed to determine whether a further training iteration will continue. It is not appropriate to use the testing set for model tuning during training. Please elaborate on it. (5) The authors use the average classification accuracy as the evaluation metric. Why not use the overall accuracy (i.e., the number of correctly predicted samples/the total number of testing samples)? Please also provide the confusion matrix of the proposed method on two datasets. (6) From Table 2, we can observe that the VGG network outperforms ResNet18. But the authors mentioned in Section Implementation details that they use ResNet18 as backbone. Why not VGG?	As mentioned above, the Reviewer strongly recommends that authors provide further experiments for the conventional approach where all SSL tasks are combined. The experiment described in Table 1 at the 'MT-ASSL ACC' is confusing to the reader initially. Therefore, the authors should detail how these results are computed. It is unclear how the method trains the feature representation  \theta in Eq.(5) given a selected subset A. Is \theta trained from scratch given all SSL tasks in A, or \ will theta be fine-tuned given a new SSL task added to A after each iteration? What is the difference between these two options? After each iteration, rather than purely combining all SSL tasks in A and training with equal parameters, the authors may apply to set proper weights for each SSL task, e.g., using a grid search to find the best combination.	If I understand it correctly, by minimizing Eq.(9), L_com is minimized and the similarity between \phi and \phi^\prime is maximized, which contradicts the goal of learning a self-complementary representation. Could you please explain more on this equation? The data augmentation (i.e. horizontal flip) is very weak compared the ones used in SOTA self-supervised learning approaches such as [1]. By using stronger data augmentations, [1] can be greatly improved. Could you compared the accuracy of the proposed method and [1] with strong augmentations? SSL training has high computation cost, and iterative SSL training will be higher. Could you compare the total training cost (in terms of training time or FLOPs) of the proposed method and the baselines? [1] A simple framework for contrastive learning of visual representations, ICML 2020	The proposed method is of technical novelty, and the experiments are very comprehensive to verify the effectiveness of the proposed method.	In general, the method in this paper is good enough for the MICCAI. It can bring more benefits by adding new SSL tasks into a pool for consideration. Though the experiment to compare with the standard method (training all SSL tasks together) is missing thus, it would not be comfortable to validate the effectiveness of this approach.	The overall method is well-designed, and my main concerns is the reported performance in the experiments. The performance of the baseline method SimCLR is lower than expected. For example, the accuracy of all the SSL baselines in Table 2 is almost the same as w/o SSL, which is inconsistent with results reported by existing works. Carefully tuning the hyperparameters of SimCLR may improve its performance and even outperform the proposed method. Therefore, the effectiveness of the proposed methods is not well evaluated in current experimental results.
037-Paper1305	An Accurate Unsupervised Liver Lesion Detection Method Using Pseudo-Lesions	The main contribution of this work, applied to anomaly (tumour) localization, is a new form of data augmentation where lesions are simulated on healthy liver slices in CT. Each simulated lesion is a downscaled, randomly oriented, and jittered liver from some other slice/volume. An ablation study shows that this greatly improves lesion localization. By applying a CycleGAN for image-to-image translation between healthy and diseased images, applying a multi-scale gradient magnitude similarity deviation loss, and applying a UNet discriminator, performance is further improved.	"This paper proposes an unsupervised liver detection method. Unlike standard unsupervised anomaly detection models which are based on autoencoders trained on normal images only, the authors propose to use an auto-encoder like architecture that can reconstruct lesion-free images from input lesion image. As for standard anomaly detection models, the reconstruction error between the input and output data outlines the lesion localisations. The first step is to train a cycle-GAN like architecture based on couples of normal and pseudo-lesion images. Pseudo-lesion images are derived by adding ""lesions"" to the corresponding normal slice. Once the model is trained, the generator enabling to generate lesion-free image serves as for the anomaly segmentation model. The other generator of Cycle-GAN model serves for data augmentation. The proposed architecture is evaluated on different datasets : The LiTS public dataset containing 131 CT scans and a private dataset of 90 CT scans."	The paper describes a data augmentation approach to improve lesion detection. By learning to generate liver CT images with lesions from non-lesion images (via CycleGAN), a UNet segmentation network is trained to perform anomaly detection, i.e., detect lesions regions from the input images. Evaluation is performed on public (LiTS) and proprietary datasets.	The proposed lesion simulation strategy is novel, simple, and apparently very useful. It should not be difficult to adopt for other tasks. The paper is clear and the ablation study is fairly convincing. The method is compared against multiple anomaly localization methods on two similar datasets.	-An ablation study is performed to evaluate the contribution of the different loss terms. -Comparison with state-of-the art method is performed Use of the gradient magnitude similarity deviation (GMSD) in both the consistency loss term and for the computation of the reconstruction error index.	The key strength of the paper is the clever application of unsupervised learning/unpaired image translation techniques to lesion identification. The approach can also be extended to other region identification tasks in medical imaging, beyond lesions. The method is compared to several popular anomaly detection frameworks, and a detailed ablation study of various components of the system is presented.	"The training strategy and the exact jittering and other parameters used to simulate lesions are not detailed, which impedes reproducibility. While the data augmentation is novel, the UNet-based discriminator and image-to-image translation for anomaly localization have uncited prior work. The discriminator has been proposed in multiple works, including ""A u-net based discriminator for generative adversarial networks"" by Schonfeld et al. Anomaly localization works that rely on image-to-image translation between healthy and diseased data include: (1) ""Towards annotation-efficient segmentation via image-to-image translation"" by Vorontsov et al.; (2) ""Visual feature attribution using wasserstein gans."" by Baumgartner et al.; (3) ""Pathology segmentation using distributional differences to images of healthy origin"" by Andermatt et al. While the proposed method shows great performance, the metric is not defined. It is the AUC of a ROC curve but what is the measure over which you vary the operating point?"	"-The idea to use a Cycle-GAN model to translate normal to pathological data and vice-versa was proposed in Sun et al (JBHI 2020) for neuroimaging application to the segmentation of glioma based on the BraTS dataset. This reference should be added and discussed. -Some important methodological details are lacking: -1) The use of a UNET based discriminator should be clarified. It is not clear from Fig 1 what the output of this discriminator is (UNET-like architectures usually output images with dimension similar to the input image input...which is different from standard GAN discriminators outputting label (true or fakes). The authors mention ""This enables the discriminator to learn both global and local differences between real and fake images"" but it is not clear how. The wording of the whole section 2.2 should be reworded as it contains typos impacting the understanding. 2) The authors should provide the backbone architectures of the different networks (generator, discriminator)  3) A clear definition of the loss terms, including the general GAN loss (different variants exist) and discriminator loss, should be provided. As stated above, introduction of the GMSD consistency loss term is interesting but requires clarification regarding its differentiability, for instance, which is not obvious. 4) GAN training is likely to be challenging, the authors should clarify stopping criterion and describe the validation dataset, if any."	It seems like the main weakness of the approach is the lack of detailed insight into when the method will succeed and will fail. It would be important to report statistics and examples of failure and success cases of lesions the method can handle.  Please see detailed feedback section.	Please describe the training strategy, including the optimizer, the optimizer's hyperparameters, the number of epochs used, the learning rate schedule (if any), and any other details required to reproduce the results.	The authors answered that they will make the code available which is not mentioned in the paper.	Most details of the networks and datasets are available. Please also include details about hyperparameters, and mage pre-preprocessing steps for both CycleGAN and the main anomaly detection network.	"How is the AUC computed? What is the measure over which you vary the operating point? Is AUC computed considering per-pixel detection? The anomaly map looks like a pretty good segmentation! Please evaluate the Dice score with respect to the reference segmentation masks so that this fully unsupervised method could be compared to supervised and semi-supervised methods. Please discuss other (tumor) anomaly localization works that rely on image-to-image translation between healthy and diseased data, such as: (1) ""Towards annotation-efficient segmentation via image-to-image translation"" by Vorontsov et al.; (2) ""Visual feature attribution using wasserstein gans."" by Baumgartner et al.; (3) ""Pathology segmentation using distributional differences to images of healthy origin"" by Andermatt et al. Is the unaltered CycleGAN architecture used in this work? Thanks for the ablation study; please add a test without SSIM and possibly a test with non-multi-scale GMSD."	-Please see above Some details regarding the implementation  of the SOTA algorithms (eg fANOGAN etc should be provided) at least in a supplementary section to improve the soundness of the comparison. -The rationale of the method proposed to create pseudo-lesion creation should be motivated. Why did the authors not consider more simple shapes, eg spherical lesion?  These pseudo-lesion have normal liver pattern, how do these patterns compare to lesional patterns? -The authors should detail if the AUC is computed at the voxel level and  if some kind of processing is performed on the reconstruction maps (eg clustering, removal of small clusters etc..) The paper should be proofread by a native English speaker. Fig 1 is not correct, should X and Y should look similar, ie have the same background?	An inherent limitation of the cycleGAN method is that it may not learn realistic lesions, or may learn a very polarized distribution of outputs. It would be important to analyze and discuss the types/sizes/ characteristics of lesions that can and cannot be detected with the proposed approach. It also seems like the the liver regions that are input to the algorithm are segmented from the CT. It would again be important to explain that the method is a proof-of-concept, and addresses a specific problem in the more general pipeline for lesion detection.	A good, simple data augmentation trick that makes a big difference in at least one task (anomaly localization - so possibly useful for segmentation, too) is a valuable contribution to the community because it is easy to adopt.	The novelty is moderate, the paper lacks too many methodological details, which impairs the soundness of the study.	The paper shows how to use an unsupervised image translation technique (CycleGAN) to address the lack of paired data for accurate lesion detection. The AUC and other results show a demonstrated improvement of the proposed technique over other anomaly estimation baselines.
038-Paper1415	An adaptive network with extragradient for diffusion MRI-based microstructure estimation	The authors proposed a deep-learning based method to estimate microstructure maps from undersampled diffusion MRI data. The main contribution of the work is the introduction of an extragradient that warrants the convergence of the network.	The authors proposed a novel network for microstructure parameter estimation from diffusion MRI. Specifically, they show their network reduced the prediction error when using few diffusion MRI measurements compared to other methods. Overall, the proposed method shows the least error when compared to the gold standard parameter maps. Results are shown on 2 in vivo datasets. The experimental design is sound, and the paper is well-written.	The paper approaches the problem of dMRI microstructure estimation by an adaptive network with extragradient. An integrated algorithm AEME is proposed, which can adaptively determine the number of iterative units and incorporate with extragradient unit (EG-Unit).	The introduction of extragradient is not new per se, but it is innovative in the context of estimating microstructure from dMRI. The paper is very well written, easy to follow. Statistical tests are sufficient.	-The experimentation and results are well presented and adequatly compared to other learning-based methods. -The in vivo datasets used have different diffusion acquisition protocols and were acquired at 3T and 7T, supporting the generability of the findings. -The manuscript is well written.	The topic is interesting and clinically significant. A series of experiments (ablation study, the performance comparison against several previous methods) demonstrated the superiority of the proposed method. The adaptive mechanism of iterative units selection seems effective and provides an efficient training strategy.	There is no main weakness. Please see my comments below for minor concerns.	-The proposed method showed a slight improved fitting error benchmark against previous techniques. The manuscript shows incremental work with low novelty. Results are limited to the prediction of gold standard NODDI maps in an in vivo setting. It would be of interest to measure the performances of the proposed method on other method parameter maps. Although, I understand the conference format limitation.	The extragradient sparse encoding is originally proposed in Ref. [9], so the technical contribution of AEME is limited. The motivation of using extragradient in this study is unclear. Some implementation details are missing. (e.g., batch size, the maximum epochs, etc.) The experiment setting of Fig. 2 need further clarify. According to the method section, adaptive mechanism will determine the number of iteration block. How could authors achieve experiments in different fixed number of iteration blocks.	Results can be reproduced	The 3T in vivo dataset is freely available. It is unclear if the 7T dataset will be made available. The network will be made publicly available on Github. The experimental setup, the method description, and networks architecture structures are sufficiently described.	Authors mentioned that a demo will be provided at https://github.com/Tianshu996/AEME after this work is accepted. The availability of pre-trained models is not clear.	It is unclear why the downsampled data lack the b=3000 shell. Since the HCP data have 3 shell, it is better to equally downsample all 3 shells. Is the number of shell or which shell used affect the prediction results? For example, will the error be higher if one just use 60 directions in b=1000 shell and no other shell to test? While the numerical results show significance, there is little improvement in Fig. 3. Is there any better way to make the results in Fig. 3 more convincing? When compare AEME with MEDN+ or MESC2, it is impossible to notice the differences in Fig. 3	Results. The authors used NODDI parameter maps computed on high angular resolution data as a reference gold standard to test their and other methods. This is done and reported with various subsampled diffusion data. The issue here is that the authors also compared their results with the AMICO methods. Although the NODDI toolbox and AMICO both estimate the same parameter maps, they do it in a different fashion and as such, it is not fair to report the difference between AMCIO and NODDI toolbox as the AMICO error. It is also not clear why the authors selected the NODDI toolbox as the gold standard and used AMICO on the subsampled data. The same method should be used for both to clearly highlight the error due to using fewer diffusion measurements. Please either replaced AMICO with the NODDI toolbox results on the subsampled data or use the AMICO maps computed on the fully sampled data as the gold standard. P7. Figs 3 and 4 and too small to appreciate the difference on the maps. Please increase the size or focus the figures on selected brain areas. P8. Fig 5 is too small. The axis labels and text are too small to read. P8. Conclusion. The authors report a reduction in the needed diffusion measurements of 11.25 (270/24). The manuscript suggests the lowest q-space sampling tested was 12 measurements per shell on the 3-shells HCP data, which would be a reduction factor of 270/36. Please correct, or otherwise clarify.	The adaptive mechanism is the main contribution of this study. Authors may provide a pseudo code algorithm in methods part. Fig. 1 (b) is not informative. For example, what's the 'reuse EG-Unit'? Author may make it specific (reusing the network architecture or the trained weights; how to reuse when there are several EG-Units). Extragradient-based method was originally proposed to address LISTA in [9]. Authors may further clarify the motivation of using it in this study and provide the justification of the performance improvement brought by extragradient.	Overall a decent paper. The technical difficulty is not high but good implementation and sufficient validations. A minor concern is marginal improvement in Fig. 3.	The manuscript is well-written and interesting to the MICCAI community. The method and the experimental design are sound.	The rating is based on the good performance of the proposed approach in dMRI microstructure estimation. As stated in previous sections, authors should resolve the several concerns in Q5 and Q8.
039-Paper2756	An Advanced Deep Learning Framework for Video-based Diagnosis of ASD	This work is a dataset release paper with an accompanying model which performs surprisingly well on such a small dataset. The work also suggests a strategy for such dataset collections.	The paper presented a method for ASD diagnosis in children, based on the analysis of videos and extracting head-related information. The proposed method was evaluated using a unique dataset, which will be made publicly available as the authors promised. The experimental results showed superior performance as compared to baseline approaches.	This paper aims to ASD detection on videos. To do that, a new dataset is collected. It describes the video acquisition strategy in detail. It also designs a new pipeline to detect ASD. Specifically, openface features are extracted from a small segment of videos. A designed HRC module is to enhance features. Finally, scores from different segments are aggregated to obtain a final result. It also has ablation studies to verify the effectiveness of the proposed module.	The collected dataset is valuabel for the ML community.	The paper has a sound experimental setup and strong evaluation. Another major contribution of the paper is the dataset, which will be made publicly available.	The paper is well organized It collected a dataset for ASD detection, and claims that dataset will be released. A new pipeline is proposed for detection. It exploits openface features and introduces HRC attention.	While the input to the model has a temporal dimension (i.e. it's a video), the architecture is not designed to take the variable length of videos into accont (e.g. using a recurrent model). Instead, they have chosen to subsample the video frames to a static size of N.	The paper lacks technical novelty as it utilizes existing methods or software (OpenFace) to extract the features followed by simple approach for reduction and classification using regular CNN. But the paper presented strong evaluation.	It says the frames are deleted if the landmark detection confidence is less than 0.75. If that, how to keep the number of consecutive frames N in each snippet. Does it use the empty frame or all-zero landmark feature? It seems there is no description on details of the dataset. How many videos are included in the dataset? Is it the same as the number of children? If not, are videos of the same child included in both training and test split? It would be better to have an ablation study on the number of segments. As the child with ASD may(not) have response to different moving directions, is it possible that max operation would have a better performance to aggregate the score from different segments? I feel the technical novelty is marginal. Extracting face features for ASD is explored in previous works. HRC attention module is very similar to channel attention in SENet.	The dataset and code will be released; all good.	OK	Details of the framework/experiments are provided in the paper. If the dataset will be released, it would be enough for reproducibility.	"Please avoid using vague and self-gratifying terms (such as ""advanced deep learning framework"", where the term advance is not well-defined)."	The paper lacks technical novelty as it utilizes existing methods or software (Open Face) to extract the features followed by simple approach for reduction and classification using regular CNN. But the paper presented strong evaluation.  The paper has a sound experimental setup and strong evaluation. Another major contribution of the paper is the dataset, which will be made publicly available.  The fact that the dataset will be made public is a huge PLUS. The research community is of a great need for such datasets. I just have few comments: why the faces are blurred in the figure? Due to human protection agreement? If this is the case, how will you share the dataset publicly? Can you provide more information about dataset accessibility condition and agreement? Although the paper is well-written, there are several typos here and there. Please do several rounds of proofreading before final submission.	There is a previous work[1] that also does video-based autism detection. It would be better to discuss the relationship. [1] Machine Learning Based Autism Spectrum Disorder Detection from Videos	The work introduces and releases a new dataset with accompanying model. They do a thorough job of analysing the model and its performance. They also switch different components in/out of the architecture to assess their impact.	Although the paper lacks technical novelty, it has sound experimental setup and strong evaluation.	This work proposes a new dataset and pipeline for video based ASD detection. I think they are helpful for the community. However, the technical novelty is marginal. I prefer the weak reject as my rating at the current stage.
040-Paper0031	An End-to-End Combinatorial Optimization Method for R-band Chromosome Recognition with Grouping Guided Attention	This paper proposes a novel chromosome recognition method to improve the existing recognition performance. To address the recognition issues of karyotypes with numerical abnormalities, deep assignment module is proposed. Also, a grouping guided feature interaction module is proposed for feature aggregation. These proposed modules address the current challenges in chromosome recognition and as a result the proposed method outperforms the state-of-the-art algorithms.	* This paper proposed an end-to-end deep learning method to recognize both normal and abnormal karyotypes, without needing any feature extraction backbone. * A grouping guided feature interaction module (GFIM) is built for feature aggregation between similar chromosome instances to reduce confusion between chromosomes with similar lengths.  * A deep assignment module (DAM) is designed for flexible and differentiable label assignment. * An empirical study was performed on a large-scale R-band chromosome dataset collected and labeled by clinical cytogeneticists. The proposed method outperformed competing methods on both normal and abnormal karyotypes.	The authors state that they propose a novel approach for chromosome recognition, specifically R-band chromosome recognition. R-band chromosome recognition is, according to the authors, understudied. The authors claim that the method outperforms state-of-the-art for this problem.	The problem analysis is done well. Then, the method is designed and described in great detail. The proposed DAM and GFIM modules address the problem very well. GFIM enhances the feature aggregation between similar chromosome samples. DAM enhances the recognition of chromosomes with numerical abnormalities.	* This paper studied an interesting and important imaging computing problem for chromosome identification in karyotyping. The application is relatively unique, in comparison with typical image computing problems studied in MICCAI. * The proposed end-to-end method is innovative. * The empirical study yielded improved prediction performance compared with competing methods.	The paper is generally well written and concise. The figures and table are mostly self-contained, meaning that they can be understood in isolation, without the context of the surrounding text. The authors are clearly well versed in the technicalities of both the biological side, and the technical side concerning deep learning, this is both a strength and a weakness of the manuscript. The authors show promising results on a decently sized dataset. The authors compare with reasonable baseline methods and do an ablation study of their proposed approach. The performance is good and generally convincing that the method could indeed be better than what is compared to.	The loss function used in the neural network is not given in full detail. It is a weighted combination of chromosome grouping loss and no other details are given. The authors should either provide with a reference or a clear formulation for this loss function. This is definitely needed for the reproducibility of the proposed method.	* The chromosome recognition problem is relatively new to the MICCAI community. It was not clearly specified in this manuscript. * It is unclear why the input data can be treated as a sequence. There is a lack of discussion on any spatial, temporal or other sequential information related to the chromosome data. I understand that chromosomes are numbers, but that is the output label the method aims to predict. Where is the sequential information for the input data? * In Table 1, the patient number is not equal to karyotype number. What is the relationship between them? * In Tables 2-3, the performance improvement is minor. The conventional Hungarian algorithm seems to be a good strategy. It is unclear whether the minor improvement would remain for other independent data sets. * There is a lack of ablation study. The individual contributions of GFIM and DAM were not quantified.	"The paper's main weakness is that it is telling a complex story in a very short format. The paper combines a couple of fields, (general problem of MICCAI submissions). Here we have a niche within a specific biological problem (karyotyping) and the technical solution (deep learning and graph matching). Quite a lot is assumed from the reader. The problem itself lacks a bit of motivation, but not much. It needs be clearly stated which diseases or disorders benefit from the R-band staining. It is stated that: ""R-band chromosome of bone-marrow cells can help identify the abnormalities occurring at the end of chromosomes"". This is not convincing me that the problem is of clinical relevance, name at least one particular case where this is important. There is an attempt to explain why this problem seems to be understudied compared to work on other staining methods. More blurred bands are mentioned. This also relates to the clinical relevance. I cannot understand whether this problem is interesting because it is challenging or because other staining approaches are superior and thus more data is available for those. The motivation for why R-band is used is also lacking here. The reported performance is convincing from the tables in the experiments sections. Performance on another dataset, comparing to reported metrics would certainly aid here as well. Some repetitions to provide error bars on the metrics would also aid the reader in assessing the differences between the approaches. The performance needs to be summarized a bit at the end of the introduction, the only mention of results is in the end of the abstract. It is better to have numbers than just stating ""state-of-the-art"". The dataset itself looks like it is not open source. Maybe this is not very specific due to anonymity. The dataset lacks a description of the abnormal cases. Do these all have the same abnormality? How is G=7 chosen in section 2.2 ? Equation 4 looks more like programming assignment equal operator than an equation. Consider giving the normalised weights on the left hand side a different name. An ablation study is required to convince the reader that the DAM is needed, this is done but not mentioned until section 3. This should be mentioned in the end of the introduction. For the results, the dataset should ideally also be stratified by disease/disorder/abnormality to show whether the improvements are specific to a particular disorder. Maybe there are few cases in the abnormalities for doing this, but this should be addressed. I cannot see that a specific test set is used for the normal karyotypes. The reported accuracy seems to be from a 5-fold cross-validation, this needs to be more clear, specifically in the table caption. English language is generally good, one minor comment: 1) Sentence in first paragraph of section 2.1 does not sound right: ""For example, one can hardly to identify and distinguish these two chromosomes..."" Remove the ""to"", and it makes sense."	The method is described in great detail. However, the loss function definition is missing. Also, the hyperparameters of the Bi-RNN training are missing.	Data and code are not available.	"The results are not reproducible without the code and the dataset. It seems that the authors intend to release the code, but the dataset does not seem to become available. I do not agree with all the claims of the authors: 1) The authors mark yes to the following: ""For new data collected, a complete description of the data collection process, such as descriptions of the experimental setup, device(s) used, image acquisition parameters, subjects/objects involved, instructions to annotators, and methods for quality control."" There is absolutely not sufficient description of this in the paper. The description is just from the point of receiving the images, there is no description of the acquisition process. 2) The authors also mark yes to the following: ""Whether ethics approval was necessary for the data."". I cannot find anything about ethical approvals in the manuscript. 3) I generally do not agree with the statements under section 4), e.g. there is no statistical test for comparing the significance of the difference between the methods, although the authors state that the method significantly outperforms state-of-the-art in the conclusions, this is simply misleading... Also I could not find a description of the hardware used."	The research is conducted in a proper way, from problem formulation to experimental results. It is nice to see an example of a properly-conducted research and well-written paper.	"* Please address the weaknesses mentioned above. * ""one can hardly to identify"" should read ""one can hardly identify""."	Please go over the weaknesses and reproducibility issues. The paper is generally good for a MICCAI paper. The reproducibility report does not accurately represent what is in the manuscript, and that needs to be addressed. I don't think that you need to run any extra experiments, just be a bit more clear on exactly what you did. Stating that the results are significantly better than state-of-the-art without a formal statistical test or comparison to results on a already reported dataset, is misleading.	The problem is defined well. Then, the proposed modules address these problems directly and successfully. Method description is proper and clear. The paper is written and organized well. Finally, the experiments are conducted properly including comparison with others and ablation studies. The contribution of the paper is shown clearly. There are just some minor weaknesses that can be addressed during rebuttals.	* The studied application is interesting and under-explored in the MICCAI community.  * The computational problem should be clearly described.  * The performance enhancement is modest.	The paper is generally well written and the problem is relevant for MICCAI. Some motivation for the problem is lacking and the paper can be slightly better organized. The authors are a bit liberal in the reproducibility report.
041-Paper0049	An Inclusive Task-Aware Framework for Radiology Report Generation	Paper proposes a structure level description of X-ray images which is an interesting idea. In addition, the paper also conducts abnormality detection of each structure alongside an auto-balance loss to solve for the skewness of data availability between normal and abnormal patients.	This paper studies the medical report generation tasks. Different from existing approaches, it shed light more on the special structure of the medical report by introducing a task distillation module. This TD module leverage prior knowledge (keywords) to group sentences in reports into a set of anatomical structures. The text block of each anatomical structure will later individually be used for training a transformer decoder for text generation. The encoder part is built on top of a CNN feature extractor as well as a classer token embedding pool. Features extracted from the image and abnormality type are fed to a transformer encoder. During training to better battle the data imbalance issue, the authors also propose a special sampling method named Auto-Balance Mask. Extensive experiments on two benchmark datasets confirm the superiority of the solution.	This paper describes a framework for radiology report generation. A novel task-aware framework is proposed that is composed of a task distillation module turning the image level report to structure-level description. There is also a classification mechanism  to identify and emphasize the abnormality of each structure.	Interesting idea to solve the problem structure wise instead of as a whole inspiring from how a real doctor would do the job. Using one head for each structure in a multi-head attention network is also a meaningful proposal. Auto balance mask loss is a useful idea.	1, the structure-aware idea makes great sense. It can help improve the coverage of the generated report on a very large scale. In the real world, many anatomical components of integral reports are omitted simply because no issue is found. This causes a great volume of context missing for training a model. In this paper, the structure-aware idea is realized by combining the task-distillation module and specialized decoders. Shedding light on retaining all anatomical components is the main contribution of this paper. 2, as discussed above, context is omitted intentionally in the real world. To counterattack, the authors propose to randomly attach the normal descriptions (from other reports), this is a big augment for the training 3, moreover, the imbalance of abnormality parts of reports also causes troubles. As a remedy, the paper proposes the Auto-Balance Masks Loss for better training. 4, comprehensive experiments both confirm the superiority of the whole solution as well as the necessity of each technical component	novel task-aware framework is proposed where various information from the dataset is extracted and group the descriptions into several anatomical-structure-specific aspects instead of generating the overall report at once	Fig(1) is not clear. Output after each step is not clear. Notation alone is not clear.  Task distillation module is not explained properly. Few other questions need to be addressed: What information knowledge graph is providing? What is the type of the data that's generated after building the knowledge graph? How is it being used by this network in terms of dimension of the data? Are the image embeddings and classification tokens processed parallely in the encoder. What is the difference between classification embedding and token? It's not clear. There is no mention of ground truth description for each structure. There is no mention of ground truth labels between normal and abnormal for every structure. How much performance variation has been observed with auto-balance loss function and weighted loss function can be added.	Despite well formulated ideas and system designs, I have following questions: 1, how are the prior keywords determined? It seems like these keywords are predefined before the TD module. Any discussion in this regard or examples of keywords should be helpful. 2, the interaction between TD and the transformer encoders are not discussed very well. Even though I have the rough idea of the interaction as discussed in the strength section, I am still not very sure my understanding is correct or not. 3, how to evaluate the TD module at the first place? From my understanding and the ablation study (Table 2), this TD module help provide to training data (?) for the decoders in TRG. If TD fails over, presumably the whole model should degrades. 4, during inference, the abnormality types are also need for the report generation. How would performance change (presumably degrade) without knowing the abnormality?	discussion about misclassifications and places where the reports are not correctly generated.	Reproducible with some efforts.	positive if code are released	yes	Fig(1) is not clear. Output after each step is not clear. Notation alone is not clear.  Task distillation module is not explained properly. Few other questions need to be addressed: What information knowledge graph is providing? What is the type of the data that's generated after building the knowledge graph? How is it being used by this network in terms of dimension of the data? Are the image embeddings and classification tokens processed parallely in the encoder. What is the difference between classification embedding and token? It's not clear. There is no mention of ground truth description for each structure. There is no mention of ground truth labels between normal and abnormal for every structure. How much performance variation has been observed with auto-balance loss function and weighted loss function can be added.	"Please address the questions listed above. Other than that I have the following suggestions (non-decision related): 1, do multiple rounds of proofreading especially for breaking down long sentences into small pieces. e.g. in the abstract, the sentence that starts with ""Therefore, we propose"" spans 9 lines, making it really hard to follow. 2, Fig 1. seems to have typos in the red blocks. Since this is the main illustration of a solution, it is not acceptable that misinformation exists."	This paper is interesting. Comments below: Discuss about the possible missclassifications and mislabelling (wrong report generated). Possible ways to handle them and how severe are those errors.	Paper is well written but need to answer a few question and clarify them with some reasonable justifications.	The main idea of this paper is to break down medical report into anatomical parts and generate the documents for each part instead of generate them as whole. This idea is adds small complexity while efficient to largely improve the performance.	novel approach
042-Paper1549	An Optimal Control Problem for Elastic Registration and Force Estimation in Augmented Surgery	The manuscript describes a framework for non-rigid image registration and estimation of surface forces that generate the deformations observed in the images. The framework formulates image registration and surface force estimation as an optimal control problem. It utilises the hyperelastic material model to describe tissue constitutive behaviour. The Authors envisage application in force estimation for robotic surgery when direct force measurement (and haptic feedback) are typically not available.	The authors propose a method to estimate surface forces needed to load a biomechanical model of the liver, so that the deformed model will fit observed data. An optimization problem is formulated along with an adjoint scheme to resolve it, with surface loads as the optimization variable. This registration approach is evaluated on experimental phantom data from the Sparse Data Challenge. While the evaluation it not complete, the proposed approach currently ranks second in the challenge leaderboard. An additional contribution of the method is that since the applied forces are estimated, the magnitude of these forces could be used as haptic feedback in a robotic system.	This paper introduces a non-rigid liver registration method between pre-operative and Intra-operative biomechanical models. The surface of the liver is registered and compared in rigid and non-rigid registration. The TRE approach is utilized for currency measuring.	The proposed method attempts to compute physically (biomechanically) plausible deformation field and surface force distribution. The error measures used (Euclidean distance between the actual positions of selected points and the positions predicted through registration) has a clear geometric interpretation and appears to be clinically relevant (accurate information about target position is crucial in surgery) The manuscript structure is very clear.	"the problematic is well stated the optimization problem and adjoint solving are sound and clearly formulated introducing ""admissible controls"" is very relevant, to restrict the location and magnitude of possible load forces the force estimation study is clearly a plus; this part may actually be more significant than the registration result itself"	The paper is appropriately written. The structure of the article is properly placed.	The role of the tetrahedral mesh in the proposed algorithm is unclear. It appears that the computation is done on the continuum/organ (liver in the example analysed in the study) discretiseed using a point cloud rather than the tetrahedral mesh. Synthetic tests cases for liver: it is unclear how the deformation field resulting from the forces applied to the liver surface is computed. How it was ensured that the computed/predicted deformation field is plausible (i.e. physically correct)? The reported computational speed/performance (update time of under 2 s using off-the-shelf PC with i7 processor) is impressive, but compatible only with real-time constrains of image-guided surgery. For haptic feedback (estimate foces for haptic is one of the motivations for the study, an update frequency of around 500 Hz would be required). The registration errors and errors in estimating the surface forces are reported. However, no attempt is made to interpret these errors in the context of the accuracy and robustnessn that would be required for clinical applications. The propose approach requires patient-specific information about the tissue material properties (Young's modulus). Elastography is mentioned as a possible way of determining such properties. However, in the context of determining patient-specific properties of soft tissues still remains a subject of active research. It's accuracy and robustness are hotly debated topic and are from being commonly accepted as amenable to clinical applications.	your results are only compared to a rigid solution and are thus obviously better. Being ranked second on the dataset/challenge website is clearly positive, but without comparison with other non-rigid baseline methods it is not possible to really assess the added benefit of the method you should at least compare your method with the previous solutions proposed in your group as in [17, 18]. (yes, references and the challenge dashboard are enough to easily break anonymity; fair enough).	1-No novelty 2- why the ICP is used , what about CPD approach (Coherent Point Drift)?  3- The landmark selection of TRE selection is not described well , However im not sure if TRE is the best measuring tool for your data.	The relavent form has been filled out by the Authors. Appears to be adequate.	The optimization problem and ajoint problem are well explained and could thus be reproduced in another simulation context.	noting special for this paper  for reproducibility	Well written manuscript on important topic of image registration and estimation of forces that causes intraoperative organ deformation. The manuscript would benefit from: (a) More detailed/clear explanation of the methods used (including the role of tetrahedral mesh); (b) Interpretation of the reported accuracy, robustness, and computational efficiency (computation time) in the context of time constrains of clinical workflows; (c)  Clear statement of how patient-specific material properties of soft tissues required for the proposed framewrok can be reliably obtained using methods (and equipment) available in clinics/clinical settings (or offer perspective of the use in clicnical setting in a reasonable future).	"""The box plot shows that significant improvement is achieved by the elastic registration step with respect to the rigid registration result."" -> Well yes, that was expected! It would be more relevant to compare your method with other non-rigid registration methods. Are statistics for other teams available on the website of your dataset/challenge? ""As these datasets were used to calibrate the method, results are better for these sets than in average."" -> this is clear bias, but thanks for noticing. Obviously you should avoid estimating errors on already seen datasets, although my understanding here is that no other data is available. -> what do you mean by ""calibrate"" here, did you tune some parameters with this datasets? Which ones, the admissible forces properties? rheological parameters vary significantly with the experiment: (E=1Pa, nu=.4) and (E=20kPa, nu=.45) in experience 1 and 2, respectively. Could you explain this huge difference, especially for the stiffness? why limit yourself to a linear elastic model if you can simulate a hyperlastic law? Computation time is not a factor in this study, and the ""small deformations"" threshold is never clearly known/real. ""avoid the inverse crime""? Strange sentence in this context, although we can get the point. proof read before the last version (""registration registration"" in the abstract, ""onyl"", ...)"	1- Dice or Jaccard indices could be included for measuring 2- Jacobian Determinant tool is a powerful tool for measuring registration accuracy. The JD of borders value could describe the expansion and shrink of border points.  3- Hausdorff distance is another more suitable tool for accuracy measurement	The study addresses clinically relevant problem and the proposed method appears, in general, to be scientifically sound. However, before the manuscript can be accepted for publication, more detailed description of the methods used needs to be provided (see the comments in sections 5 and 8 of this review)) and specific description of how patient-specific material properties required by the proposed method can be reliably obtained in a clinical environment. It appears that this can be achieved by careful revision the manuscript without any need for obtaining additional results.	The method is clear and interesting, and the problem of finding the loads leading to a solution absolutely relevant. The major limiting factor is that the proposed method is not directly compared to other simulations or other loading approaches.	This method is very simple with no novelty. I suggest to utilize deep learning approaches
043-Paper1625	Analyzing and Improving Low Dose CT Denoising Network via HU Level Slicing	The authors improve the performance of denoising CNN in low-dose CT by slicing the problem into different dynamic range levels, significantly improving the performance of different denoising networks.	Authors proposed the use of HU level slicing to improve the performance of the vanilla convolutional network. Authors first use different CT windows to slice the input image into separate HU range. Then different CNN networks are used to process each slice. Then, a feature fusion module combines the feature learned by each network and produces the denoised image. Quantitative (PSNR, SSIM, RMSE) and qualitative experiments are presented comparing the proposed technique and state-of-the-art methods. The experiments show an good performance (PSNR, SSIM and RSME)	The authors point out the importance of intensity resolution in LDCT. To tackle the issue by large dynamic range in LDCT and improve the deep neural network for LDCT denoising, the authors propose to use HU level slicing where feature fusion mechanism from multiple deep neural network for different HU levels is developed.	This is a very good idea that solves a complex problem with a simple add-on to denoising strategies. The results are very good and have been validated with different networks and different datasets.	Novelty: The novelty of the work lies in the use of HU level slicing on CNN networks. The method shows a good performance (in terms of PSNR, SSIM and RSMEI) when combined with some state-of-the-art methods.	The main strengths of the paper is that they bring up the intensity resolution issue in LDCT denoising. In particular, most deep neural network uses min/max normalization for preprocessing of the input image, the relatively large dynamic range needs to be adapted for the LDCT denoising network. The proposed method is validated on multiple public dataset.	The threshold selection and number of dynamic of the ranges seems ad-hoc Slicing the problem adds complexity to the networks that may become more difficult to training and with more parameters and hyperparameters	Some important information is missing in the experiments: The computational time is not presented. A declaration of what software framework and version authors used is not presented. A description of the computing infrastructure used (hardware and software) is not presented. English could be improved.	The main weakness of the paper is that HU level slicing is not novel method. This is the traditional thresholding with heuristic parameters. The method needs ablation study for HU slice values and theoretical analysis why this HU level slicing is beneficial. Most importantly, HU values have a physical semantics in terms of attenuation so normalizing after HU slicing needs to be justified. In particular, the reconstruction loss is well balanced between different HU bins.	ok	The average runtime for each result, is not presented. A declaration of what software framework and version authors used is not presented. A description of the computing infrastructure used (hardware and software) is not presented.	Since the authors used or updated the existing network such as U-Net, REDCNN and CycleGAN, it is easy to reimplement without source code release. It will be trivial to implement HU level slicing if the HU level values are given.	In fig. 5 one subfigure seems to be missing Table 3 could have in bold the best results for a better visualization A discussion on how to choose number of ranges and the thresholds would be interesting	It would be interesting that authors gave some information about the computational time required by the proposed method. Authors should include a description of what software framework and version used. Authors should include a description of the computing infrastructure used. Give a reference (and/or formulas) for RMSE, PSNR, SSIM. Summarize the research limitations and future research directions. English needs a revision.	The authors need to justify why HU value slicing is working theoretically. In addition, they need to discuss the physical meaning of HU value in terms of attenuation. Moreover, they need ablation study with different HU values and discuss how to set HU values. Theoretically, they can make infinite number of HU slices and their linear assumption on performance with the number of HU slices may be violated. They also need to consider weighting in terms in L_recon for different slices. Moreover, learning procedures and implementation details after fusing the features should be illustrated.	This is an idea that can be widely applied for many different neural networks and approaches, even for different applications. The enhancement of performance of the  is very significant.	The topic of the paper is relevant and interesting to the MICCAI community. It presents an innovative idea (HU level slicing ) that can be used in existing methods to enhance the denoising performance in Low Dose CT Denoising. Some important information is missing in the experiments, but it's not complicated or expensive for authors to include it. This missing information affects to the reproducibility of the experiments. The code is not available, but enough details to reproduce it are given.	The major factor for my recommendation is the novelty of the proposed method. HU level slicing is heuristic and validated with many different settings for robustness check. Theoretical analysis should be included for justification of using HU level slicing.
044-Paper1703	Analyzing Brain Structural Connectivity as Continuous Random Functions	The authors propose a spatially-continuous connectivity model for white-matter connections, to analyse dMRI data for possibly salient connections. The method does not need pre-existing atlases of connectivity, giving it more freedom to find new relevant inter-region connections.	This paper proposes to use low-rank embedding of continuous random function to represent the connectivity intensity matrix of white matter fibers. The paper is written in excellent English. The introduction part is educative and clearly summarizes the limitations of state-of-the-arts methods. The methodology section gives a good mathematical description of the algorithm, and the experiments clearly demonstrates the merit of this method.	This work theoretically analyzes brain structural connectivity as a continuous function.	"The exposition is good, the method is well-motivated and laid out, and the writing is clear (up to where the math lost me around theorem 1, but that is usual for me). So the paper is a pleasure to read.  The approach is creative and compelling, one of those papers that make you smile and say ""wow, that's neat"".  The removal of the atlas constraint is valuable."	Compared to the exiting methods, this work extends the classical discrete representation to continuous space and develops mathematics schemes to solve the computation task. Experimental results prove the advantage of this new method in group-wise inference from SC data as well as in localizing group differences to brain regions and connectivity patterns on the cortical surface. Overall, this paper contributes a novel method to the field of brain functional connectivity analysis.	1). In this work, the authors comprehensively employ real analysis and functional analysis to prove the brain structural connectivity as a continuous function. 2). This paper is written well.	"In the results (Fig 1), it's unclear to me if the higher rate of ""significant"" discoveries is simply due to CC's higher resolution, or if CC has more significant findings at the resolution of current methods."	The method and experiment sections needs a few more details, as I list in the detailed comments below.	1). Topic Issue  From the reviewer's perspective, this work concentrates on real analysis, functional analysis, and statistical Theory to discuss the continuous brain structural connectivity. This is a theoretical research work that probably does not match the topics of MICCAI. Furthermore, there are fewer medical, clinical translational, and imaging analytics provided in this work. Therefore, the reviewer would suggest submitting this work to NeurIPS, ICML, or COLT. 2). Inconsistent Format This paper is organized inconsistently. At first, the reference citation is not consistent with the original template. The authors utilize (1) as a citation of reference #1, but this results in the confusion as an equation (1). Furthermore, the Bibliography should be replaced as a Reference. There are many inconsistencies in the format of this work. 3). Theoretical Issues/Questions Although authors provide many theoretical analytics to brain structural connectivity, some theoretical issues lead to confusion among reviewers. At first, the authors proposed the concept as Borel measurable regions, but the authors utilized these regions to denote a Lebesgue calculus. What is the definition of Boreal measurable regions? Are these regions could be covering via Boreal sets? Or what is the relation between Lebesgue and Boreal measure? Why cannot authors adopt the Lebesgue measure to denote these regions? Unfortunately, the authors do not provide more details and explanations. Furthermore, in Section Reduced-Rank Embedding of a Sample of Continuous Connectivity, it seems that the authors try to conduct a novel space. This novel space is defined based on a standard Euclidean metric and finite dimensionality. Why can the authors directly definite all operators in a Banach space? Is the novel space complete? Can authors prove the completeness of the proposed new space?  Finally, the authors denote the target function as Eq. (1). However, there is no detailed description to introduce the optimizer and the convexity of the target function. Can authors prove the proposed target function a convex or non-convex problem? 4). Technical Issue. In section Algorithm, the authors employed SVD to perform the matrix decomposition. However, there are several shortcomings of SVD. For example, SVD cannot implement the decomposition of a sparse matrix; SVD is difficult to solve the over-complete problem; if the input matrix is not square, SVD could be time-consuming. Therefore, the reviewers provide the following research works for authors as references: [1] Wen, Z., Yin, W., & Zhang, Y. (2012). Solving a low-rank factorization model for matrix completion by a nonlinear successive over-relaxation algorithm. Mathematical Programming Computation, 4:333-361. [2] Shen, Y., Wen, Z., & Zhang, Y. (2014). Augmented Lagrangian alternating direction method for matrix separation based on low-rank factorization. Optimization Methods and Software, 29:239-263.	No indication in main paper about reproducibility.	The method is novel, the availability of the author's original code may greatly help with the reimplementation. The authors admits promised the availability of the code, but I did not see the download link in the paper or in the supplementary file.	No reproducible experiments provided in this paper. No source code is released.	"Methodology: -Would there be any advantage to defining U_i as bounded (vs potentially infinite)? In practice of course it is bounded. -Is the Borel-measurable a required condition? All the regions E_i are closed sets in practice. If not required, why is it mentioned? -""for any pair (omega_1, omega_2)"": Is this a point, or a small discrete region with dimension dx, dy (eg a pixel)? -Re ""underlying random function"": Is it practical to encode known biophysical connectivity information about the cortex as a prior (a function with pre-defined shape), instead of assuming no information? -I was not clear about the interaction between the non-discrete assumptions in the math and the discrete reality of the data: Is the infinite dimensional, continuous surface assumption necessary to develop the method? If no, would it be clearer to define the problem over discrete space (eg a certain sized voxel/pixel)? Reduced-Rank Embedding: ""infinite dimensionality of the continuous connectivity"": Is this due to a continuous (ie not discretized) assumption on the surface? If yes, is the continuous assumption necessary? Definition of V_K: Is this effectively imposing a discretization on the 2-spheres? Misc items: ""assume the U has been centered"": you could just state this, plus ""WLOG"" Fig 1: Bigger axis labels would help readability Define ""HCP"" Square brackets are clearer for references. A few typos: -diffomorph -> diffeomorph -for for -> for -it's -> its -analysis are known -> analyses are known -analysis N -> analyis of N -Between subject -> add hyphen, or say ""Inter-subject"""	The choice of algorithm parameters needs more explanations, e.g., alpha 1, alpha 2 in page 6 and the thresholds for subnetwork discovery in page 7. Why did you choose those specific values? Is the algorithm sensitive to the parameter choice? Page 6, from the implementation aspect, you gave the computer configuration but still need to report the computation time. Page 7, $200, $40 and $160 seems typos or latex error. Page 8, the color bar of fig.2 B is too small and numbers on it are vague. Please enlarge it. Page 6, 'the fours panels' should be 'the four panels'.	Validation Issues: The authors only validate proposed embedding methods with the other three peer algorithms. Nevertheless, further validation is required. The reviewers hope that the authors can validate the proposed technique with current machine learning embedding techniques such as word2vec.	The creativity of the proposed solution to the atlas problem. The clarity of the exposition.	The methodology novelty is good.	Authors need to clarify their theoretical analytics and proof with more details. Authors need to validate their proposed method with peer machine learning algorithms.
045-Paper1726	Anatomy-Guided Weakly-Supervised Abnormality Localization in Chest X-rays	This paper proposed an anatomy-guided weakly-supervised abnormality localization model for chest x-rays. Specifically, the model consists of a cascade of two networks, one for identifying anatomical abnormalities and the other one for pathological observations. The model also utilizes an anatomy-guided attention module to guide the observation network to focus on the relevant anatomical regions generated by the anatomy network. Experiment results on two large public chest x-ray datasets show superior performance against other methods.	The authors proposed a weakly supervised disease localization approach for chest X-rays by integrating the classification of anatomical mentions (also extracted from the radiology reports) in addition to the disease/observation mentions. The CAM maps from the anatomy classification branch are further utilized as the weight mask of refined regions for the disease classifications. Two large-scale public datasets, i.e., MIMIC-CXR and ChestX-ray8, are employed here for the experiments. The radiologists also annotated bounding boxes of diseases in the MIMIC-CXR datasets. Superior results of the proposed framework (in both classification and localization) are reported compared to some prior arts (e.g., CAM-based localization).	This paper proposed Anatomy-Guided chest X-ray Network (AGXNet) which consists of two networks: Anatomy Network and Observation Network to make use of both anatomy mentions in reports and image-level label. An anatomy-guided attention (AGA) was then adopted to bridge these two networks. The proposed method achieves competitive results on two public datasets.	The topic of adapting positive-unlabeled learning to chest x-rays is interesting, probably making the contributions useful for medical communities. The results appear strong (although some comparisons are missing). The paper is well-written.	The paper is overall well-prepared and easy to follow The authors explicit model the anatomical mentions in the report for the localization and classification purpose Two large-scale datasets are utilized to demonstrate the effectiveness of the proposed AGX module	The authors incorporated anatomy mentions into Weakly-Supervised Abnormality Localization in Chest X-rays through an anatomy-guided attention (AGA) module. Positive Unlabeled (PU) learning was used to alleviate the noise of in CXR reports.	The novelty is limited. Utilizing reports to aid abnormality localization in chest x-rays has been popular in this area. Baselines are not complete and state-of-the-art, especially some existing works that also utilize the medical reports are not considered for comparison. Ablation experiments are not complete. For example, this model contains a few hyper-parameters, but many were not discussed in the experiments.	I have a couple of concerns and suggestions as follows: I found a directly related reference [R1] is missing, which tackles the same problem and also extracted the attributes of disease (including anatomical locations) from the report as a form of supervision. The comparison to prior arts in weakly supervised localization is relatively weak, where only one previous work based on vanilla CAM is included. There are other methods, e.g., [R2] (another missing reference), which should be included and compared. Also, the localization results for other IOU values shall be included, at least in the supplementary. Especially results on IOU 0.5 are important. The PU module for the uncertainty learning seems only to be effective in some scenarios, e.g., better results for pneumonia but worse for pneumothorax in MIMIC-CXR. It would be helpful if the authors could further discuss it. There are public datasets with bounding boxes/segmentation masks for pneumonia and pneumothorax. It will be more convincing to adopt those datasets for the evaluation, considering the current GT set from MIMIC-CXR is a bit small (a couple of hundreds vs. thousands). [R1] Bhalodia, R. et al. (2021). Improving Pneumonia Localization via Cross-Attention on Medical Images and Reports. In: MICCAI 2021 [R2] Li, Z., et al.: Thoracic disease identification and localization with limited supervision. In: CVPR 2018	In the experimental part, some state-of-the-art methods need to be compared in Table 1, 3 and 4. Many important information is missing in the comparative experiment part. The authors violated the guideline for supplementary submission.	Can be reproduced.	The results seem to be reasonable and reproducible.	The reproducibility of the paper is credible.	Please refer to the main weaknesses.	see 5	The proposed proposed method should be compared with [23]. More state-of-the-art methods need to be compared on NIH Chest X-ray and MIMIC-CXR dataset. Authors should give more details in how the contrast algorithms were trained such as RetinaNet. Which part of DesenNet-121 is the observation feature map f_o?	The experiments are comprehensive and the results are strong. But the idea of using reports to aid disease localization in chest x-rays is not novel.	Good quality paper with a clear introduction of the method and well-organized experiments, though there are some possible improvements in the comparison study and evaluation data.	According to paper submission guidelines, for supplementary, authors should not submit text materials beyond figure and table captions, definition of variables in equations, or detailed proof of a theorem. The authors violated this guideline.
046-Paper2430	Anomaly-aware multiple instance learning for rare anemia disorder classification	This paper proposes an  interpretable pooling method for MIL to address the poor machine learning performance on rare anemia disorders classification from blood samples. Experiments demonstrate the superior performance of the proposed strategy over standard MIL classification algorithms, with providing a meaningful explanation behind its decisions.	The authors proposed an interpretable MIL network to increase the contribution of anomalous instances. This work shows SOTA performance when compared with other MIL approaches.	This paper proposes an anomaly-aware pooling strategy for multiple instance learning. The key idea is to design a latent space that uses Bayesian Gaussian mixture models to estimate the distribution of negative instances. Extensive experiments are conducted to validate the effectiveness on bag/instance classification and the anomaly analysis.	This paper aims to address a very important and meaningful problem. The idea and the overflow of the proposed framework is clear.	The paper is mostly well-written and clear. The method achieves good performance on the authors' private dataset. Most of the aspects are described in sufficient detail to enable the reproduction of results. The anomaly-aware GMM modeling using the negative bag is novel, and provides good potential impact in real-world clinical applications.	+The idea to model the distribution of negative instances by using Bayesian Gaussian mixture models is interesting and in-depth. As traditional MIL is capability of finding the positive samples, how to alleviate its omission error rate is always an important topic. Hence, I believe the idea in this work can have impact on the MIL community, vision community, explainable community and the medical imaging community. +The discussion of the proposed method, especially on the behavior of the anomaly score is in depth and solid. +This paper is well written and easy to follow.	For the introduction section, the contributions are not clearly summarized. Why the proposed strategy can overcome the limitations of the attention mechanism? For the Mask-RNN, is it pretrained on another large-scale dataset? or it is directly applied to the dataset used in the experiments. For the Anomaly scoring, why using the Mahalanobis distance in Eq. (3). The number of comparison methods is relatively few. It is difficult to understand the difference between the Anomaly method and attention method in Fig. 3 and Fig. 4.	It would be better if the authors can compare with some SOTA anomaly detection approaches for anomaly recognition. Authors may consider training some of the MIL methods from computer vision on microscope data and comparing them. A public dataset can be used to further justify the contribution of this work. The novelty is a bit limited, given that [14] proposed to use attention and MIL classification for such problems as well. The loss function is the same.	-The motivation of this work in its current form is not clear enough. Why the negative instance estimation is necessary when using MIL need more specific justification. -Regarding the technical framework. The aggregation from instance representation to bag representation is required to be permutation invariant. Will the proposed anomaly score warrant this aspect? The author needs extensive effort to justify this issue in the rebuttal stage. -This work lacks multiple strongly related deep MIL based works in the past few years. For example: [1] A multiple-instance densely-connected ConvNet for aerial scene classification. IEEE Transaction on Image Processing, 2020 [2] Multi-instance multi-scale CNN for medical image classification. International Conference on Medical Image Computing and Computer Assisted Intervention, (2019) [3] Local-global dual perception based deep multiple instance learning for retinal disease classification. International Conference on Medical Image Computing and Computer Assisted Intervention, (2021) [4] Loss-based attention for deep multiple instance learning. AAAI 2020. The authors are suggested to enrich the related work accordingly.	The idea in this paper is clear and the overflow of the proposed framework is also clear. So i think this paper has good reproducibility.	Authors claim they will release the code upon acceptance.	Either the dataset or the code is available. I don't think I have the confidence to reassure that this work is reproduceable.	This paper aims to address an important problem, the idea and the overflow is clear, but i still have some concerns as follows: For the introduction section, the contributions are not clearly summarized. Why the proposed strategy can overcome the limitations of the attention mechanism? For the Mask-RNN, is it pretrained on another large-scale dataset? or it is directly applied to the dataset used in the experiments. For the Anomaly scoring, why using the Mahalanobis distance in Eq. (3). The number of comparison methods is relatively few. It is difficult to understand the difference between the Anomaly method and attention method in Fig. 3 and Fig. 4.	The paper is well-written and the novelty can be deemed as sufficient given the use of anomaly GMM module. However, there are still a few things can be strengthen during rebuttal.	Please refer to the weakness part in Sec5 for details. The major issues to improve this work include: justification of motivation; more theoretic insight on whether the MIL pooling function; and the enrich of some strongly relevant MIL based related work. Also, the authors are suggested to validate the proposed method on more publicly available benchmarks.	Please see the weakness of this paper.	See above.	Both the strength and the weakness are obvious in this work. The strength is slightly over the weakness. I would recommend weak accept.
047-Paper1683	Assessing the Performance of Automated Prediction and Ranking of Patient Age from Chest X-rays Against Clinicians	In this work the authors present a study comparing the performance of three radiologists on chest X-ray age prediction and ranking tasks, comparing with data-driven models trained on a highly heterogeneous non-curated set of chest X-rays from a variety of clinical settings in six hospitals. The authors conclude that (a) the radiologists are significantly more accurate at detecting age-related changes in a single patient than at estimating age in single images and (b) the models significantly outperform humans on both tasks. The author'work indicates that accuracy gains are likely to be small from larger datasets, and that the majority of age-relevant information is present at resolution in this modality.	The authors present a framework aimed to determine patient age from a chest x-ray. The model was trained on a large database (1.8M chest X-rays). An ablation study investigating the model accuracy based on changing training size and image resolution demonstrates the generalizability of the approach. The approach is based on a conditional Generative Adversarial Network and allows the visualisation of the predicted scan that can be used to identify semantic features learned by the model.	In this study the authors  present a chest X-ray age prediction model trained on a large heterogeneous set of chest X-rays, with sensitivity analysis to training set size and image resolution, and  generalisation performance on the public dataset NIH ChestX-ray[14]. Moreover they present a study comparing the performance of human radiologists against their model on two tasks: (a) ground truth age prediction from a chest X-ray; (b) ranking two time-separated scans of the same patient in age order.	1.The authors demonstrate a GAN-based 'explainable AI'solution to visualise age-relevant features identified by the model, comparing with those identified by the radiologists based on their clinical experience. This paper is relatively experimental on demonstrating the effectiveness of each components of proposed method.  2.The author use GAN-generated synthetic chest X-rays conditioned on patient age to intuitively visualise age-relevant features via simulated age progression.	An impressive large dataset is used in this work. I found the ablation study and the comparison with radiologists (used to validate the proposed work) quite interesting. The use of the predictive model to visualize semantic features is quite relevant and this solution can be used to discover new potential biomarkers.	The study is very interested. Well written organised. They deliver and answer their hypothesis questions. There is a novel idea of the network and how to combine GAN with regression models to predict the age of a patient from X-ray.	"1.In Section 2.2, the paper says ""whether the ability of radiologists to order two images of the same patient by age is superior to their ability to estimate true patient age from a single image"". But the experiment to compare these two abilities was not introduced and What is the final conclusion. 2.In Figure 1(b), it is not clear how the pixel difference map by the model is implemented.  3.The presentation of the purpose of separating aging features is insufficient and unclear. 4.The expectation of ranking success rate for humans is 59.7%+-2.8% in Section 3.2, What is the basis of the ranking success rate for humans? 5.In the model architectures comparison, the MAE of 3.33 from Efficient+LR just only 1% improvement compared with Efficient+CL and Efficient+OR in Table 1. A minor improvement is not enough to indicate that Efficient+LR network is optimal. Which method did you ultimately choose for Age Prediction? Minor 1.Figure 3 (right) and its description in the Generalisation Performance on Public Datasets of the Section 3.1 is ambiguous. 2.98 years refer to your own dataset results or after fine-tuning on the Chest14 training data."	The paper should be improved in clarity and has different grammatical errors/typos. Some of the results are not described adequately and the results section must be reorganized/rewritten.	My only concern is in the human AI comparison. The authors used test cohort for this which may include images from the six hospitals protocols whic the AI network already know through training. This si not a fair comparison with human experts. The authors need to compare in a cohort where the AI tool never show before the hospital's protocol modality resolution and quality of the images. By this way the comparison is fair with the human experts.	Maybe the results can be reproductive.	Details required to train the network are missing. This includes some important hyperparameters such as the learning rate, batch size, etc.	easily reproducible	Please refer to the weakness.	"Some of the results are not clear and not well described. Below are my main concerns: 1) Please define some of the metrics used during evaluation (i.e. MAE ME, and R2) 2) Some of the results mentioned in the text are missing from the tables. For example, a) ""...CNN is saturated with this size of dataset and the specific modeling approach is only marginally relevant. Using DenseNet- 169 CNN (14.1M parameters) in place of EfficientNet-B3 (12M parameters) led to a slightly lower performance (3.40 vs 3.33)..."" b) ""....Taking a mean ensemble estimate from models trained at the four resolution levels reduces MAE to 2.78 years, which to the best of our knowledge is the best accuracy reported in a heterogeneous non-curated dataset...."" 3) Some sentences on the results are very confusing. For example what the following sentence means? Please rewrite it: We observe actual success rates of 67.1% for the humans and 82.5% and 85.5% for the regression and ranking models; with respective p-values of 0.001, 0.201 and 0.029 we find that the radiologists significantly exceed the baseline expectations, no evidence that the regression model outperforms (as expected) and weak significance for the ranking model exceeding the baseline regression expectation. Why in Fig. 4 the ""model incorrect"" results are not provided?"	Very nice study and idea. Well design and delivered. My only concern is in the human AI comparison. The authors used test cohort for this which may include images from the six hospitals protocols whic the AI network already know through training. This si not a fair comparison with human experts. The authors need to compare in a cohort where the AI tool never show before the hospital's protocol modality resolution and quality of the images. By this way the comparison is fair with the human experts. Please kindly verify if there was images from the six hospital in the test cohort which already excisted in the training cohoirt of AI. If yes please remove these images and test again in the new test cohort and observe the differences with the experts. Thank you.	The proposed method is motivated and the results are convince, expect for some issues. Currently, I recommend to accept and expect the replies in the rebuttal.	The paper uses an impressive large dataset and some of the presented results are interesting. However, the clarity of the paper must be improved and some of the results are not reported correctly in figures and tables. The description of the framework should also be extended so that relevant hyperparameters required for training the system are correctly described.	The study is very interested. Well written organised. They deliver and answer their hypothesis questions. There is a novel idea of the network and how to combine GAN with regression models to predict the age of a patient from X-ray. My only concern is in the human AI comparison. The authors used test cohort for this which may include images from the six hospitals protocols whic the AI network already know through training. This si not a fair comparison with human experts. The authors need to compare in a cohort where the AI tool never show before the hospital's protocol modality resolution and quality of the images. By this way the comparison is fair with the human experts. Please kindly answer the above question.
048-Paper1796	Asymmetry Disentanglement Network for Interpretable Acute Ischemic Stroke Infarct Segmentation in Non-Contrast CT Scans	"This work develops a segmentation method for stroke infarcts in brain CT based on asymmetry. It differs from previous works on asymmetry by developing a method to differentiate between 2 asymmetry types: Pathological (stroke) and non-pathological (anatomical). The steps are: 1. A registration net learns to align the CT with mid-sagittal. 2. Asymmetry map made via CT's reflection along mid-sagittal. 3. A second net takes the CT and predicts only the pathological asymmetries. This is learned via an intuitive regularizer designed based on domain-knowledge (main technical contribution). 4. Using output of (2) and (3), a synthetic ""asymmetry compensated"" image is made, where non-pathological asymmetries are removed. This is beneficial cause it can serve as human-interpretable side-output (secondary contribution). 5. A third network learns to segment given the ""asymmetry compensated"" image (4). Evaluated is done on a public database, AISD, outperforming 3D (Res)Unet and DeepMedic."	the authors presented an asymmetry disentanglement network to provide extra supervision for the stroke segmentation on non-contrast-enhanced CT images. the authors demonstrate the effectiveness of the disentanglement module.	Developed a novel network to disentangle asymmetries from non-contrast CT images for AIS segmentaion, trained with a tissue-awareness loss function to make the model more interpretable.	The main premise of the work, developing a method for leveraging asymmetry to detect pathology, is a very good one. It could potentially be useful for the detection not just of stroke, but others as well. And developing such methods could be useful to enhance the strong general-purpose learning-based models (networks) commonly used nowadays. The method is well thought, well-designed, and well implemented. For this, I identify 2 components: 1) the regularizer for learning to predict a pathology-asymmetry is intuitive and nicely-designed for the purpose. 2) The framework for combining image X, all-asymmetries map A, and pathology-asymmetries map P, to get a get a synthetic image (asymmetry compensated X_comp) where the non-pathology-asymmetries are (pseudo-)removed (X_comp = X+A-P) is a well-designed one for the purpose. The method gives a secondary output, the asymmetry-compensated image, where only the pathology-asymmetries should be visible, not the non-pathology ones. This is the image that is given to the segmentation network (primary output being the segmentation). This image makes the segmentation model more interpretable by humans. (shows why the segmenter predicts what it does). I find this a very nice advantage. The results seem ok, outperforming 2 standard general-purpose segmentation models, the 3D CNN DeepMedic and the 3D (Res)Unet, and some stroke-specific segmentation models [9,18]. I liked that the authors re-implemented and adapted some of the previous methods [9,18] to make the comparison fair (e.g. [9] from 2D to 3D). Makes me more confident that they put some effort to make good baselines. The paper is very well written and explains the method clearly. It was a pleasant read.	an asymmetry disentanglement network based on domain knowledge to provide extra supervision. improved numbers by comparing with existing methods.	Overall a nice paper to read some aspects that were nice to see: Tissue type aware regularisation term Different asymmetry maps to try and seperate/disentangle pathologies - like this concept, so not focusing on one aspect but potentially have the ability to highlight a few pathologies as one tries to make the model more explainable Nice detail in their methodology and explanation of the parts of the network	"The method is task (Stroke segm) and modality (non-contrast CT) specific. This limits the audience of interest. It is based on (at least) 2 assumptions: 1) All the asymmetries of interest are dark. This is mentioned briefly in method section and discussion (doesn't find bright blood). 2) The pathologies appear as ""additive"" intensity changes (X_comp=X+A-P). For example, it cannot deal with pathologies that create morphological changes (e.g. tumor). The work would benefit if a) task-specific assumptions were discussed very explicitly, b) if it would discuss how and to what tasks it could be potentially extended. E.g. multiple-sclerosis / leucoaraiosis in Flair MRI (though bright). Evaluation is rather limited. Only 1 experiment per method. No runs with multiple seeds. So we aren't sure whether improvements are by a ""lucky seed"" or consistent. No train/val/test setup, only train/test. So it's not clear whether improvements are ""best run"" with optimally configured hyper-parameters on test set. This could be improved by averaging over multiple seeds per method, and by discussing how hyper-parameters were found for the method and the compared methods. It is unclear whether some components of the method are adopted or proposed. In good scientific writing, this should be crystal clear. This needs fixing. Examples: In Sec.2, Transformation Network: Eq.1 is adopted from [18] but this is not explained. The text only mentions the difference that the model here is 3D whereas [12] is 2D. Please state explicitly that Eq.1 is adopted from [18]. Also, in rebuttal, please state explicitly if any other parts of the method are adopted (Eq.3? Parts of regularizer?), and same in text. The regularizer has multiple parts. There is no ablation study whether each individual part is important. Knowing which part is important would help extend this work in the future. For example, one could see if the parts that are most important are applicable to another task and perhaps employ them for a different pathology/modality. Parts of the regularizer could be straightforwardly applicable as a regularizer of the segmenter, and not just the asymmetry-predictor D. In fact, some of them may be the cause of segmentation improvement, instead of the asymmetry map generation. Example: For example, penalizing the pathology when it appears on CSF (1st part), or the regularization of the average size (2nd part). This has not been studied. There could be a comparison/ablation where those parts are tried on 3D ResUnet to see if they improve it, to identify whether the improvements come from the asymmetry-image generation or the regularization of the size/location of pathology. The authors & article could explicitly discuss this."	Narrative application Although the proposed method is interesting, aiming to disentangle two kinds of asymmetries and reduce the artefacts of input images, this approach might be narrative to stroke segmentation on noisy CT images, which rely on such a prior on asymmetry to improve the input images. Potential issues:  The authors propose to modify the input image X^ by X^ = X + Q = X + A  - P. This might be problematic as it might generate false positives due to the intensity range difference between X and Q (or X, A and P).  A clarification or discussion on this might be needed. Results and Clarifications.  x. The author mentioned that the transformation network T can be unsupervised. I am missing the details of such a training strategy. From my understanding, this is similar to regress the rotation angle of an object in an images. One could simulate an angle (transformation matrix in the authors' case). However, this would require some 'object-centered' images. Essentially, a question would be: how good the network is to regress the parameters? The transformation module is very important in the proposed framework as it generate A, which is base for next steps.  x. For training D, is there any weighting strategy for four loss terms?  x. How much does the segmentation maps (WM, GM, CSF) help as a regularization? x. For self-mirrored version X', which plane/view is used?	The idea I think is novel for the application, I think a weakness is potentially the lack of significance testing between the methods - which I think is nice to utilise in the evaluation, especially as you are comparing your method to current state of art methods.	(based on the article alone) I think the article describes the method sufficiently clear, and gives some of the hyper-parameters, so that a reader can sufficiently reimplement the core method. Although the 3D ResUnet itself may not be able to be replicated (not all hyperparameters are here such as width of layers etc). Less reproducible are likely the results themselves, as they are only 1 run per experiment, which means improvements may be subject to the initialization seed, etc. Overall, average.	seems reproducibable.	Dataset available, parameters chosen given, liekly it can be impelemented whilst following the methodology.	"In the rebuttal, the authors should aim answering primarily about the weaknesses I raised in previous section. I below provide some additional points, that they should also clarify at least within the article in case of publication, if they do not find space to clarify them in the rebuttal. Sec 2, ""However, this can lead to a trivial solution... network."": I dont think that this would happen, because (I think) gradients of D are not a function of A (because of the addition). So D does not receive information about the shape of A. It just processes X and learns to ""highlight"" specific parts of it (pathology assymetries) to enable segmentation. E.g. if output of D is 0, then X+A-0 = an image without asymmetries, so segmentation is impossible. Therefore to enable segmentation, D learns to ""highlight"" (P) the pathological-assymetries. It would be good to have an experiment to study this (perhaps I am wrong), so that the reader understands the method's behaviour: Just run the whole framework with regularizer = 0. I believe it would still predict the pathology-assymmetries, but with more false positives, which the regularizer resolves via the size/location penalties. If the authors agree that this sentence of the article is not 100% true, then consider replacing this statement. Sec 3, it is unclear how the tanh activations predict rotation and translation degrees, and how rotation is restricted to 60 degrees. Do you associate -1 and +1 of the output tanh to correspond to e.g. -60 and +60 degrees rotation, and something similar for translation? Please clarify. ""warm-start strategy ... for training D."": Please clarify this a bit more. Do you add supervised cross-entropy loss with G at the very output of D? Which output of D do you penalise to look as G? P? E.g. via L_bce(P,G)? If so, please clarify within text, e.g. by writing ""L_bce(P,G)"". Will help reproducibility. In the regularizer of ""mean size"", is mean(T(G)) computed for each case (G) separately at each iteration, or is this mean computed over the whole database (mean of all subjects)? Please clarify (also in article). Note that if it is over whole database, it biases the network, which is less likely to predict extreme cases (too small or too big infarcts), in which case it should be discussed. (minor, in case you find this helpful) I think the title is a bit confusing. Most readers, and this reviewer included, when they see ""Disentanglement Network"", they will assume it is a paper focusing on methodology how to learn to disentangle in a data-driven manner. Here, the method does not really learn how to disentangle one different asymmetries. Instead, the asymmetries are actually modelled (total asymmetry = X-reflect(x), pathological asymmetry via the regulariser, non-pathological via A-P) and the model learns to predict pathological asymmetry P. A less confusing title could be something like ""Pathological Asymmtry Prediction Network for..."" or something like that."	Applications  Not sure if it is relavant to include MR image for stroke segmentation such as ISLES-2018/2016. More datasets on demonstrating the idea would be helpful. Potential issues:  The authors propose to modify the input image X^ by X^ = X + Q = X + A  - P. This might be problematic as it might generate false positives due to the intensity range difference between X and Q (or X, A and P).  A clarification or discussion on this might be needed. Clarifications.  x. Details of the training strategy of the transformation network T. Essentially, a question would be: how good the network is to regress the parameters? The transformation module is very important in the proposed framework as it generate A, which is base for next steps. A clarification on this would be needed.  x. For training D, is there any weighting strategy for four loss terms?  x. For self-mirrored version X', which plane/view is used? Presentation X in second row of Fig. 4 is not transformed while X+Q is transformed.	I was interested to find out how the parameters were set - I would like to see some strategy performed to find lambda - unless this was done, if so how? If the work aims to improve AIS assesment - if it can't detect the right bleeding spots, do you think that the outcomes are overconfident? I would like to see it perform on a larger dataset, is that int eh scope for future work?	The paper has significant strengths, an intuitive idea, clear presentation. It has weaknesses, but the strengths outweigh them I believe.	Good presentations of a sophisticated approach.  However, some details and potential issues need clarifications.	Overall a nice paper, the idea to utilise dientanglement to separate symmetries is interesting, I think its a paper that looks to exploit the available data and at the same time improve current models so I did enjoy reading it.
049-Paper2434	Atlas-based Semantic Segmentation of Prostate Zones	This paper proposes a novel segmentation approach for prostate zones, by integrating the anatomical prior into an atlas map. Besides, the weight of fusing with the atlas could be adjusted in the testing phase.	The authors propose a new deep learning approach for automated semantic segmentation of prostate zones by including prior shape information from an anatomical atlas.	Multizonal prostate segmentation is performed on T2w MR images using a combined 3D anatomical atlas and deep learning (U-Net based) segmentation algorithm. The impact of each method on the final segmentation may be adjusted in real-time to optimize results by tuning a hyperparameter. This approach requires a whole-gland prostate segmentation as input, then predicts the central gland segmentation, and through elimination identifies the peripheral zone segmentation.	The idea of integrating the spatial probabilistic prior into an atlas to help the segmentation framework is interesting and novel. The writing and organization is clear, making the whole process easy to understand.	In general, this is a well organized and clearly written paper. I like the incorporation of a probabilistic atlas to provide semantic context.	This work integrates two segmentation approaches to perform two-zone prostate segmentation. Novelty lies in allowing the user to adjust each method's influence to improve segmentation by adjusting a hyperparameter in real-time.	"As the author mentioned, the assumption of having the available WG mask is very strong, and the impact of employing the WG mask as input is non-negligible, so this may harm the transferability of this method in practice. Besides, the author uses an external dataset to evaluate the method, while the atlas is built upon the source dataset, if the data bias is huge, will this strategy still work out. There are some atlas-based methods for prostate segmentation that are not included in the reference list, such as [Jia, Haozhe, et al. ""Atlas registration and ensemble deep convolutional neural network-based prostate segmentation using magnetic resonance imaging."" Neurocomputing 275 (2018): 1358-1369.], [Ma, Ling, et al. ""Automatic segmentation of the prostate on CT images using deep learning and multi-atlas fusion."" Medical Imaging 2017: Image Processing. Vol. 10133. International Society for Optics and Photonics, 2017.], [Padgett, Kyle R., et al. ""Towards a universal MRI atlas of the prostate and prostate zones."" Strahlentherapie und Onkologie 195.2 (2019): 121-130.], [Singh, Dharmesh, et al. ""Segmentation of prostate zones using the probabilistic atlas-based method with diffusion-weighted MR images."" Computer Methods and Programs in Biomedicine 196 (2020): 105572.]. Similar methods should be at least taken into discussion to better address the main contribution of this work."	"Minor problems with English grammar. Please edit. Setting different weights of the atlas during testing means ""human in the loop"" and might make the algorithm operator dependent."	Whole gland prostate segmentation is required as one of the inputs to the model, limiting its application to only cases in which this is already available. The anatomical atlas was developed by registering images with an affine transform, but a deformable registration may achieve better results. Figure 2 compares the Dice of the baseline method with and without mask, in addition to the proposed methods in which the hyperparameter that determines the influence of the semantic segmentation v. atlas is varied. Clearly, adding a mask improves the model performance, but in absence of a mask the model performs significantly worse than typical two-zone prostate segmentation methods in which no mask is used. It would be interesting to see how this U-Net model performs on two-zone prostate segmentation when trained without a mask on this dataset. Only a couple comparisons of their results to other work are included, but multi-zonal prostate segmentation is a well-studied topic, and inclusion of comparison to more recent work would be beneficial.	The reproducibility of this paper is good.	The authors provide specifics and point to the data they used.	This work utilizes publicly available datasets, Prostate x and Prostate 3T. The methods are clearly described and code publicly available, making this work reproducible.	The major concern has been listed in my comments to question 5. Also, the comparison with other similar methods (section 4) is not specific enough, more study is needed in this aspect.	As mentioned above, lambda could be viewed as human in the loop. Fig 3 does not clarify for me why lambda=0.4 was chosen.	This paper is clearly structured, with the motivation for the segmentation work well defined.  Fig. 2 depicts the Dice for segmentations produced at baseline and with different lambda values for the dataset as a whole. However, it is unclear from this figure if there are some types of cases in which small lambda values yield better results and others in which large lambda values yield better results. For example, does prostate size, presence of BPH, or location of cancer influence which lambda value would be best for use in a specific prostate segmentation? While classifying what types of cases perform best at given lambda values is beyond the scope of this work, it would be interesting to see how the ability to fine-tune results by adjusting lambda on a case-by-case basis rather than setting it as a fixed and evaluating on the entire dataset improves results. If someone who had not seen the ground truth segmentations were to adjust lambda to optimize segmentation accuracy on a case-by-case basis, how would these results compare to others in Fig. 2? One advantage of automated segmentation is that it minimizes the bias of an individual in performing segmentation. Allowing users to modify the influence of each segmentation technique re-introduces user bias while also possibly enabling more accurate segmentations. It would be beneficial to mention inter-reader variability in this work, including DSC for two-zone prostate segmentation.	The idea of building the voxel-wise probabilistic atlas to help the network capture anatomy prior is interesting, and the author has explained their approach clearly. The experimental results demonstrate superior performance against the baselines as well as some other methods.  However, as mentioned in Question 5, some weaknesses affect the judgment of the contribution with this work.	Good paper, relevant topic but a concern that needs to be addressed.	While the approach has merit, the fact that the prostate segmentation is needed in order to segment the central gland reduces the enthusiasm for this work.
050-Paper0345	Atlas-powered deep learning (ADL) - application to diffusion weighted MRI	The paper proposes a novel framework to unify atlases and DL for dMRI biomarker estimation. It was used to estimate fractional anisotropy and neurite orientation dispersion from down-sampled data and achieved superior accuracy .	This paper proposes a novel method that exploits deep learning together with atlases of brain microstructure parameters. This is used for the estimation of microstructure scalar parameters from diffusion MRI.	The major contribution of this work is a framework that leverages atlas-based registration methods with deep learning for the estimation of dMRI of FA and orientation dispersion from tensor and NODDI estimation. Specifically the authors propose the use of atlas-based registered features, together with atlas-reliability map (as given by standard deviation) and registration error (proposed formulation). These three are concatenated to the diffusion signal as inputs to the DL fitting model.	extensive comparison with competing methods according to their resuls, the proposed method is faster and more accurate than existing ones;	This paper proposes to learn from an atlas in addition to the diffusion images to evaluate on a new set of data microstructure scalar parameters. This approach seems novel to me and results are of interest.	"* Paper contributes with a novel approach to leverage atlas-based derived information with dMRI signal for the estimation of FA/OD scalars. To my knowledge, such idea and approach are both novel. * Authors use dHCP data for atlas-construction (230) and have a ""pure"" testing set of 70 subjects through different ages (31 to 45 weeks). * Comparison with existing approaches and statistical significance of the results"	implementations of other DL methods might not be correct	there is no justification as to why learning from an atlas would be a better idea than learning from the individual images themselves, although results tend to show interest (see tables) in methods, the atlas construction process is missing important details. Overall the clarity should be improved. learning only scalar parameter values seems to be the lower end of what could be achieved, why not learning complete models ?	* Limited proof that outperformance of the provided framework is due to the atlas * Limited analysis and discussion of the results, eg is there any bias in the results as per age or which is the limit point where atlas is no longer useful? (see my details comments below) * Questionable choice of registration method (also acknowledged by authors)	Although the architecture is simple (U-Net), the full method is rather complex. Without the code, it seems unlikely one can reproduce the experiments. Although the authors mentioned in the reproducibility checklist that the code was made available, they did not mention it in the paper.	Nothing to say	Authors state training code will be made available, and already publicly available data is used. Parameters of compared methods are not clearly stated (probably due to space limitations), authors claim were chosen according to original paper description but this does not support reproducibility of SOA results. Unclear if statistical tests are done for the ablation study.	It is never easy to validate implementations of other's methods. It would be better to use the original code/model for comparison purposes;	"In addition to comments above, the paper lacks details in the methods explanation. In particular, is the atlas built in an unbiased way or towards a given reference ? If the second option, which one ? How re the average tensor images computed, what type of tensor rotation, etc. ? Also it seems to me that there are mismatches in notations between section 2.2 and eq 2 (what is \bar{T}_k) On a more phylosophical point of view, there is no clear justification as to why DL would work better on atlas than on the input images themselves. Is there also a reason why not considering full models rather than ""just"" microstructure maps. This is a bit deceiving in terms of novelty. There should be a discussion on those aspects. That being said the results section is well constructed and the evaluation shows the difference between results without and with the atlas, showing improved results."	"Overall, I have found this paper very interesting and as raised in the strong points there is the novelty of the proposed framework. The paper is clearly written and organized. The proposed framework is methodological sound though some choices are unclear to me. The major drawback of the presented work is the lack of discussion and other analysis that would have been in my opinion valuable to better illustrated the contribution of this work. But I do of course acknowledge the limited space of the MICCAI template. Still, I will raise here below my concerns/comments that could maybe help the authors improve their work in the future. The authors did a very nice work by including several existing methods for comparison an adding statistical test on the results. I was wondering on why the authors selected the given downsampling factors that indeed ending with very low number of measurements overall. It would have been interesting to explore which is the number of directions if any where the proposed method does not outperform the others. Or is this systematically the case? For instance 32 directions is still plausible setting for new born data and should be included. Equally, this reviewer is very curious to understand if the results vary across age of scan? Would have been interesting to see boxplots (in order to understand also if there are outliers) through different age. Maybe if the authors already explored this, some comment can be included. Similarly, can the authors discuss if there is any specifc anatomical region where method is outperforming? Given that atlases are less accurate at GM/WM cortical interface given large cortex variability, maybe in those areas atlas-prior is less accurate and then not so relevant? How the accuracy change for WM structure where overall these models are most appropriately defined? It would have been interesting also to push further the study of the influence of the registration error. Authors could have included rotation/transaltion errors as to see how this influences the performance of the proposed method. I do appreciate the ablation study that is indeed very needed and should be clarify if the results on Table 3 are also statistically significant or not. It is not so obvious though which is the contribution of each of the steps given those values on table 3. At least in my opinion two first columns seem quite equivalent. Please clarify. Also why not including also n= 12 (FA) and n=30 (OD) in that table? Maybe Figure 2 panel c can be removed to give more space for ablation study results. As the authors already acknowledge the choice of diffusion tensor registration can be questionable. As for accurate registration the use of T2w image seems more appropriate. Though still T2/dMRI is then needed. If space allows some more discussion /justification on this registration choice could be included. Minor Authors could clarify since end of introduction page 2 that the down-sample dMRI refers to diffusion gradient direction downsampling and not spatial resolution. Typo in page 6, 2nd paragraph, twice ""to"" before refernces 30, 17 could the authors clarify if all the methods work at the same spatial resolution?"	Good paper with solid results. Reproducibility could be improved by sharing the code.	Overall, I find the paper interesting but brought a bit downwards by novelty and some missing parts in the methods, thus my recommendation of weak reject	I think this is a fair conference paper, given the novel contribution of combining different aspects of atlases with DL for solving a widely practical problem of scalar diffusion and microstructure estimation from dMRI. Despite the limitations and comments I raised, this paper is valuable for discussing during the conference.
051-Paper1798	Attention mechanisms for physiological signal deep learning: which attention should we take?	The paper investigates several self attention mechanisms in a range of CNN and multi-head self attention based architectures in application to ECG signals combined with other physiological data. Experiments are performed on a classification test for predicting hypotension and a regression task to predict intraoperative cardiac output. The authors identify best performing self attention mechanisms in each task and provide hypotheses as to why these models work the best.	Four attention mechanisms were compared.	This paper comprehensively investigates four attention mechanisms fused with three convolutional neural network (CNN) architectures for two processing physiological signal prediction tasks. This paper aims to provide a good guide for researchers who use attention-based convolutional networks for physiological signal prediction tasks.	The paper seeks to compare the performance of several self-attention mechanisms in CNN architectures in 1D physiological (mostly ECG) signals. While extensive experimentation has been done in the computer vision literature regarding such architectures, the benefits and drawbacks are less well established for 1D signals. As a researcher in EEG and ECG I have personally searched the literature for papers in the physiological signal domain addressing similar architecture design related questions and found a dearth of resources. I believe that work presented here addresses questions that may be common in the 1D physiological signal domain that there are currently few resources for. The applications in clinical/medical practice are generally well motivated. 1D signal monitoring is commonplace in many clinical settings and the work presented here could be of relevance to researchers working in a variety of application areas. While the paper does not propose new methods or architectures, the way it evaluates and compares existing methods is intuitive and well thought out. The experimental procedure is well described, results are presented in an interpretable way and discussed understandably in ways that might be informative for other researchers in the field. The paper is clear and free of misspellings and other errors.	The manuscript was written clearly and organised well.	The problem studied in this paper is important and needs to be solved in ECG Extensive case studies	The paper is not novel. No methods are presented that have not been presented before by the literature. However, it is my opinion that experiments presented are valuable and informative. I'm not sure the inclusion of the multi-head self attention model makes sense in the experiment. Transformer architectures and multi-head self attention has recently become widespread in the literature. Their applications are widely varied. They have been used as sequential models, models that pool information spatially, enhanced with convolutional blocks, and applied in a wide variety of novel architectures. As such it is hard to draw conclusions about the overall efficacy of transformer models from the single architecture explored in the paper. Similarly, each of the CNN architectures employs some method of aggregating over the final output of the CNN encoders (flattening or GAP). Architectures that replace this aggregation and fully connected classification layers with transformers have already appeared in the literature. Therefore it is possible to envision combining the CNNs presented in the paper with transformers and multi-head self attention. Given these two points it may be better to limit the scope of the experiments to just CNN based models. It would be good to perform the experiment on a wider number of tasks. The authors choose two tasks with different that require information from separate phenomena in the ECG for the network to identify. Different self attention mechanisms are more effective in each task according to the phenomena of interest. The inclusion of more experiments in different ECG applications would help verify that the results presented in the classification and regression tasks hold in a variety of situations. The paper relies on citations for explaining the self-attention mechanisms investigated. The paper does a decent job motivating these mechanisms in writing but as these mechanisms are central to the understanding of the paper, further detail should be presented. This detail could come in the form of equations or schematic diagrams.	There are no deep insights.	Need more justifications about the novelty claims Need to include more related work that are highly important Need to check for grammatical errors and typos.	The paper seems to be pretty reproducible. I would be concerned that if there are any necessary adjustments to apply the self attention methods in the 1D setting as opposed to the 2D setting where they were proposed, these details have been omitted. Otherwise, network diagrams are given for each of the CNNs and the dataset used seems to be open source.	It is reproducible.	The dataset and code are not disclosed in this paper, so the reproducibility of this paper needs to be further improved.	The self-attention mechanisms should be described in mathematical detail or schematics. More citations for the conditions of interest should be provided. The authors cite a few deep learning papers that are applied to similar problems but it would be good for the authors to include some more clinical citations and background. Some more description of the data would be good. It is not clear if the ECG is 12 lead. Similarly it's not clear how the PPG was collected. It would be good to include these details. The dataset is not cited. In figures 2 and 3, the fact that higher is better and lower is better, respectively, lead me to a bit of confusion. Explicitly stating this, as the figures look very similar, would be good. Similarly, the discussion mentions that different self-attention mechanisms work better in each task because of the phenomena that must be identified for the tasks to be solved. Clinical citations for these phenomena would be good to verify these assertions.	"The manuscript presents the comparison results of four attention mechanisms: squeeze and excitation, non-local, convolutional block attention module, and multi-head self-attention. Although the samples were randomly split into the training set and testing set, it still has bias. Cross-validation is a more fair way to evaluate the models. The demographic data were used as features for the classification. Statistical analysis should be performed to check whether or not the differences between patients and healthy people in these demographic data were significant. The results shown in Fig. 2 demonstrate that the attention mechanism did not significantly improve the performance. The authors should provide insights. Does it imply that the attention mechanism is not effective in this case? It is better to acronymise area under the receiver operating characteristics curve as AU-ROC. I strongly suggest that the authors identify typos and correct them. For example, ""mean percentage absolute error (MAPE)"""	Comments: Need to include more related work that are highly important [1] Chen W, McDuff D. Deepphys: Video-based physiological measurement using convolutional attention networks[C]//Proceedings of the European Conference on Computer Vision (ECCV). 2018: 349-365. [2] Zhu X, Cheng D, Zhang Z, et al. An empirical study of spatial attention mechanisms in deep networks[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2019: 6688-6697. [3] Wang S, Li B Z, Khabsa M, et al. Linformer: Self-attention with linear complexity[J]. arXiv preprint arXiv:2006.04768, 2020. The authors need to introduce some related work on the self-attention mechanisms in the introduction. Need more justifications about the novelty claims This paper comprehensively investigates four attention mechanisms fused with three convolutional neural network (CNN) architectures for two processing physiological signal prediction tasks. Although this paper can provide a good guide for researchers, the novelty still needs to be further justified and improved. It would be better if the authors could devise some novel ways of combining attention mechanisms with convolutional networks efficiently. The authors should explore the combination of efficient self-attention mechanisms with convolutional networks. Need to check for grammatical errors and typos The editorial quality of this paper is largely unsatisfactory. It contains quite a lot of inconsistent/non-precise description, as also reflected in the above comments.	While the paper does not propose new methods, the authors identify an area of the literature that has been scarcely investigated and perform an intuitive and interpretable experiment. The experiment is likely to be of use to other researchers who seek to apply CV methods to CNN architectures for 1D physiological signals. Because of the clarity of the experiments and the potential value to other researchers working on similar problems, I would recommend this paper for acceptance.	Although the manuscript's contribution is not significant, it still provides help in the selection of the attention mechanisms.	Although this paper provides a high-quality experimental guide on combining attention mechanisms with CNNs, novel designs and ideas are still lacking.
052-Paper2482	Attentional Generative Multimodal Network for Neonatal Postoperative Pain Estimation	The paper proposes an automated approach to asses pain in the neonatal intensive care unit. It is a multi-modal approach designed, developed, and validated for assessment of posoperative pain. The multiple modalities include visual and auditory inputs.	Authors propose a novel approach for neonatal postoperative pain assessment. The proposed model is consist of three stages: spatio-temporal feature learning (Stage 1), joint feature distribution learning (Stage 2), and attentional feature fusion (Stage 3). The main technical contributions are as follows: develop a deep feature extractor followed by an RNN Auto-Encoder network design a novel generative model that combines all the modalities use a transformer-based attentional model Compared to other state-of-the-art, the proposed method outperforms in the classification task. Authors also show the results of the pain intensity estimation.	The paper proposes a novel multi-modal approach for the assessment of postoperative pain of neonates. A deep feature extractor followed by an RNN Auto-Encoder network is used to extract spatio-temporal features from both visual and auditory modalities. A novel generative model is designed to combine all the modalities while learning to reconstruct any missing modalities. Instead of using early or late fusion techniques, a transformer-based attentional model is adopted to learn cross-modal features and generates final results.	This paper is well written, the target application is excellent and it combines multiple modalities in the analysis. The architecture design seems appropriate for the task of combining multiple modalities and fusing them in a lower-dimensional latent space.	Thorough evaluations: Authors evaluate the proposed method by using many types of scores and conducting ablation experiments. These results show the efficacy of the proposed approaches in the paper.	1) This paper handles the problems of missing modalities by reconstructing them using a generative model.  2) It is novel to use a transformer-based attentional model to generate final pain label and its intensity.	The approach is novel and derives from similar work using CNN-LSTMs. The innovation is a small step as it also uses a combination of similar techniques.	Lack of detailed information about the experiments: For example, authors do not describe how many data is used for the evaluation, how many pain/no pain cases there are. These information would help to understand the results exactly. The results of the pain intensity estimation are seemed to be not good enough.	1) The performance of the proposed approach when dropping each modality was discussed in the paper, however, if some samples of one modality and other samples of another modality were missing at the same time, can the model reconstruct features? 2) Did the authors consider different weights of three modalities during the training process? As shown in Table 3, when the sound modality was dropped, it seems that the performance decreased more.	The data used is public and the methods are clearly explain. With these 2 elements the results should be reproducible.	Authors use many public dataset and pre-trained models such as USF-MNPAD-I. The implementation details are well described.	Implementation details have been described.	I enjoyed this paper and it was hard to find issues in the experimental design as well as any criticisim in the techniques used to accomplish the given task of pain estimation.  The ablation experiment is interesting, although I would expect that the system should require all inputs as it achieves the higuest classification accuracy.	There are many types of scores in the results. However, which is the most important score from clinical point of view? What cases can the model classify correctly (but other state-of-the-art fail to classify)? These discussions might help to understand and improve the model. The pain intensity needs to be estimated so accurately? If not, is it usuful to estimate the range of the pain intensity? For example, range1(0-1), range2(2-4), and range3(5-7), it become a bit easier, and the model might achieve the performance good enough.	"1) Page 2, Paragraph 3, ""i.e., it makes final assessment of pain"" should be ""i.e., it makes the final assessment of pain"" 2) Page 4, Paragraph 2, ""Finally, feature sequences of each modality are used to train the LSTM-based AE ..."" should be ""Finally, feature sequences of each modality were used to train the LSTM-based AE ..."".  3) Page 6, Paragraph1, ""All the models are developed based on PyTorch using GPU"" should be ""All the models were developed based on PyTorch using GPU"". The past tense is suggested to be used, please modify the tense of the whole paper."	The paper is clearly written, the problem is challenging and uses multiple modalities which are combine using autoencoders and attention layers. The architecture is interesting and the proposed approach reached the higuest reported accuracy in the literature.	Lack of detailed information about the experiments: For example, authors do not describe how many data is used for the evaluation, how many pain/no pain cases there are. These information would help to understand the results exactly. The results of the pain intensity estimation are seemed to be not good enough.	The paper proposes a novel multi-modal approach for the assessment of postoperative pain of neonates.  Instead of using early or late fusion techniques, a transformer-based attentional model is adopted to learn cross-modal features and generates final results.
053-Paper1971	Attention-enhanced Disentangled Representation Learning for Unsupervised Domain Adaptation in Cardiac Segmentation	This paper addressed the problem of domain-specific information inherent in the domain-invariant features, especially under large domain shifts. Hilbert-Schmidt independence criterion (HSIC) is used to restrict the independence and complementarity	The paper proposes a UDA framework for cardic segmentation, working with: i) Alignment of Imaging Characteristics. ii) Channel-wise Disentanglement. iii) Attention Bias for Adversarial Learning. The proposed method archives better performance when adapting between MRI and CT on the cardic segmentation task.	This work presents an Attention-enhanced Disentangled Representation (ADR) learning framework for cross-domain cardiac segmentation, where Hilbert-Schmidt independence criterion (HSIC) is adopted for feature disentanglement and the attention bias module is used for alignment of task-relevant regions. The proposed method is demonstrated its superior performance on the MMWHS 2017 dataset.	Novel channel-wise disentangled representation learning was presented as opposed to dual-path disentanglement. An attention bias for adversarial learning was proposed to emphasize task-relevant domain invariant features	1.The paper is well-motivated, well-written, and easy to understand. 2.The topic of unsupervised domain adaptation and application to cross-modality is highly important in clinical practice.  3.The proposed method is a combination of disentangled representation learning and attention mechanism. Both are hot topics with a lot of literature and both are interpretable. Such a combination has limited novelty, but the paper demonstrates its effectiveness in unsupervised segmentation of medical images.	The problem of cross-modality segmentation is important and interesting. The proposed HSIC  and attention bias modules are proven useful. Fig.1 is clear and helps to understand. The paper is well written and nicely organized.	* The attention bias module and the use of Hilbert-Schmidt independence criterion (HSIC) are not new (see ref [14] for the Hilbert-Schmidt independence criterion).	1.The experimental results are not solid enough. Since the main innovation of this paper is the framework rather than the pre-training manner, all UDA methods involved in the comparison should be treated under the same configuration. According to the description in the #section2.5, the pre-training parameters, learning rate are not same. (The proposed ADRs are fine-tuned on the basis of SIFA, while others are initialized from scratch.) 2.The compared UDA methods are not SOTA now. It is recommended to add more comparison with SOTA methods.	"In section 2.2, the authors only used the GAN to achieve the image-to-image translation, which theoretically can not preserve the anatomical structure in the translation process. That is the reason why the cycle consistency loss is proposed in CyleGAN [1]. In this case, the subsequent cross-modality segmentation for anatomical structure does not seem to make sense. Before the feature disentanglement, the authors firstly perform image translation, which really confuses me. The disentangled representation learning is based on the assumption that the image from different domains share the same domain-invariant features and have their own domain-specific features [2,3]. In this work, the domain-invariant feature and domain-specific features can be considered as the shared anatomical content and specific image style (CT or MR). Taking MR as the source domain, the style of translated image x^{s->t} should look like CT image, as shown in Fig.1. In this case, for two CT images (real CT and pseudo CT), how do disentangle their content and style as they share the same content and style? Therefore, it seems contradictory to perform image translation before feature disentanglement. In section 2.5, the generator and discriminator used in image alignment are fine-tuned with a learning rate of 1e-10. My question is, with such a small learning rate, whether the optimizations of these two networks can be negligible? Whether the translated images are visually better or worse? The author should plot the loss function curves of these two modules and visualize the translated images. [1] Zhu, Jun-Yan, et al. ""Unpaired image-to-image translation using cycle-consistent adversarial networks."" Proceedings of the IEEE international conference on computer vision. 2017. [2] Lee, Hsin-Ying, et al. ""Drit++: Diverse image-to-image translation via disentangled representations."" International Journal of Computer Vision 128.10 (2020): 2402-2417. [3] Huang, Xun, et al. ""Multimodal unsupervised image-to-image translation."" Proceedings of the European conference on computer vision (ECCV). 2018."	* The authors shared their code, which thus is highly reproducible.	The authors included model details in the section 2 and released the main code on the GitHub.	I'm not sure if it can be reproduced as some modules seem to be unreasonable in my opinion. But the authors provides the code, which is a plus.	This paper proposed a new and innovative UDA for segmentation by means of an attention-enhanced disentangled framework. This paper presented a few key innovations for the considered application, though not entirely novel, including (1) the embedding space is disentangled in a channel-wise manner into domain-invariant and domain-specific subspaces; (2) attention bias is proposed to boost capturing task-related domain invariant features. The experiments are comprehensive and the results are encouraging.	Although #Alignment of Imaging Characteristics# is not your main contribution, it is recommended to add references on cross-domain alignment. (section 2.3) Some figures are too small to catch all details, and improvement or re-design is recommended. (Fig. 2-5) As the main contribution, how the HSIC is applied to disentangle representations extracted by the encoder is not clearly explained. It is recommended to describe more details than Eq.(2).	The authors should discuss the issues I mentioned. I encourage the authors to visualize the domain-invariant features and domain-specific features and discuss the level of disentanglement between domain-invariant and domain-specific features.	Novel channel-wise disentangled representation learning alongside extensive experimental results.	The topic of unsupervised domain adaptation and application to cross-modality segmentation is interesting. The weakness of this paper is that the experiment results are not solid.	Some modules seem to be unreasonable as I mentioned before.
054-Paper0676	Attentive Symmetric Autoencoder for Brain MRI Segmentation	The paper proposes an attentive symmetric auto-encoder based on ViT for MRI segmentation. The method does pre-training and resorts to the prior of brain structures and develop a Symmetric Position Encoding.	The paper proposed a 3D brain segmentation model based on Masked Auto-Encoder self-supervised learning scheme, with a novel symmetric positional encoding (SPE) to add anatomical symmetric prior, and a novel attentive reconstruction loss based on histogram of gradient reweighting in order to emphasize the informative patches. Experiments shows the sota performance on 3 benchmarks and the effectiveness of each novelty.	The authors proposed an attentive reconstruction loss function. The symmetric position encoding seems useful for this task. The experimental performance looks good.	1) Paper is clearly organized and neatly written. 2) Masked Pre-training for medical segmentation is novel and makes sense as well. 3) Attentive reconstruction loss and symmetric position encoding help use some key properties of medical data to do the pre-training. 4) Experiments show a decent improvement in performance.	The paper has clear motivation for its novel ARloss (add importance to informative patch reconstruction) and SPE (induce anatomical symmetric prior to positional encoding). The proposed model/methods are evaluated on multiple benchmarks and different metrics (Dice & HD95), and achieve sota performance compared with other sota transformer-based models and SSL methods. The attentive reconstructed loss takes importance score for each patch using 3DD VHOG, which is reasonable and easy to implement. Ablation studies demonstrate the effectiveness of SPE and ARloss separately. The 3D brain MRI is aligned to a template anatomical space during pretraining so that the left-right symmetric could be guaranteed.	Considering that MAE needs a lot of data, it is difficult to apply it to medical images. The high performance of this paper may be the main strength. Symmetric Position Encoding is an interesting work.	1) Novelty in terms of network architecture is not great. The authors use shifted windows in UNETR architecture.	There are several concerns about the paper: The paper makes a strong assumption that the input 3D brain MRI is left-right symmetric, and from this point, the author develops SPE to induce the anatomical symmetric prior in SW-ViT encoder. This is okay for the pre-training phase if all the brain volume is aligned to BIDS. However, I'm concerned about the brain MRI with disease/lesions that can significantly change the local region and appearance of a brain. For example, Alzheimer's Disease and brain tumor can both affect brain structure in a certain region in one side of the brain, and cerebral atrophy can even severely change one side of the brain structure. When the brain structure is asymmetrically affected by some neurodegenerative diseases, the anatomical symmetric assumption is not held anymore, does SPE still have any advantages over the vanilla PE? There's no discussion about this in the paper, however, this issue should be concerned, since the proposed ASA is a general brain segmentation model and should be able to take care of diseased brain. One question about the downstream finetuning on BraTS 2021: this benchmark has 4 MRI modalities so the input volume will be 4 channels, however, the SW-ViT is pretrained using only T1 MRIs which is single channel. When fine-tuning the pretrained model on BraTS, how does the model handles 4-channel input? More specifically, how is the input patch embedding layer being initialized, given the gap of different channel number. From Table 4 ablation study, comparing SPE&SSL with SSL-only setting, there's only marginal improvements in ET Dice and HD95 scores, and the Dice scores for WT and TC is even lower in SPE&SSL setting. Therefore, it looks like there's no obvious advantages using SPE under SSL pretraining scheme. Although the paper emphasizes that SPE improves the symmetric details in segmentation, but there's no evaluation on how good the symmetric structures are preserved and segmented in the 3 downstream tasks. I suggest adding some segmentation results which contain symmetric brain structures and segmented better than that from other SOTA models/methods.	"As we know, ViT is a patch-based structure, and swin Transformer is a pixel-based structure. It is better to add more details about Shifted Linear Window-based Multihead Self-attention(SLW-MSA).  It may be confusing for the reader whether it is computed on pixel-level or patch-level, so the author should add more details. This paper is similar to ""Self-Supervised Pre-Training of Swin Transformers for 3D Medical Image Analysis"" (https://arxiv.org/pdf/2111.14791v1.pdf) The author should add more discussion to distinguish it from that paper."	Authors state code will be released	As the authors indicate in Abstract, the code and model will be released after accepted. The datasets are all open-accessible and the hyper-parameters of pretraining, fine-tuning and ASA model are also provided in paper, so this work should be reproducible.	The author did sufficient experiments on different public dataset, so it is easy to reproduce.	This paper shows the masked pre-training can improve the performance of the segmentation. Experimental results validate it and I believe it is a good finding for 3D medical image segmentation	I'd suggest some discussion and possibly adding some evaluations on the model's ability for handling challenged diseased brains with asymmetric structures, and explore/explain whether SPE is still effective on these brain volumes. It's better to add some visualization results to demonstrate that the proposed model can generate better segmentations on symmetric structures compared with other SOTA methods, in order to verify the motivation and assumption of the proposed method. Please explain how the SW-ViT backbone pretrained on 1-channel input is adapted to the 4-channel BraTS image in Methods part.	It is better to add the ablation study about the influence of P. It is better to add more details about Shifted Linear Window-based Multihead Self-attention (SLW-MSA). I wonder it is computed on patch-level or pixel-level.	Pre-tranining, new additions to the SSL method.	Considering the SOTA performance on multiple benchmarks, the clear motivation as well as the novelties in SPE and ARLoss, although the paper does not fully verify and demonstrate the effectiveness and advantage of its symmetric novelty, I think the paper is definitely above borderline and has the potential for acceptance. I would like to see the rebuttal response from the author and adjust my rating of the paper.	This paper is an interesting work, and the experiment is sufficient.
055-Paper0121	Autofocusing+: Noise-Resilient Motion Correction in Magnetic Resonance Imaging	In this work, the authors proposed the Autofocusing algorithm using a CNN extracted prior knowledge on specific k-space motion corruption models. The authors evaluated the methods using fastMRI datasets.	"The authors proposed an autofocus method based on a neural network to remove motion artifacts. Similar to autofocus and gradMC (L2 regularized autofocus method), the method does ""blind"" motion correction. However, it is constrained by a deep learning-based regularization term. The fast MRI database was used to compare the performance of the proposed method with other motion correction methods. The motion corrected images were compared using four quality metrics."	The work proposes a deep learning prior for MR motion artifact reduction that is used inside an autofocusing strategy. Rigid (translation and rotation) motion parameters are estimated together with scaling variances modelled by a UNet prior. Rigid motion simulations were conducted on the fastMRI database.	The main strength of this paper is around formulation of the optimization by including k-space trajectory estimates.	The proposed method does not require motion detection or estimation to perform motion correction; The proposed methods was compared with several other motion correction methods using four image quality metrics;  -The images obtained with the proposed method look good even in the presence of severe motion; -The source code is provided;	Iterative-based motion correction using the concept of autofocusing with an automatic update of translation, rotation and scaling parameters. Combination of optimization-based refinement of motion parameters (translation, rotation) and an image-derived scaling prior extracted by a trainable UNet to prevent artifact inpainting or unrealistic restorations.	The main weakness is that the proposed method is seems a trivial step in addition to autofocusing methods. The newly introduced Unet loss mechanism is similar to a GAN setup as critic so the authors should compare their methods with a GAN based motion correction model.	"-It is not clear how the method works. In particular, it is not clear how eq (5) is related to the autofocus method proposed in [2], which is based on an entropy metric. -The proposed method takes 7-9 minutes to generate motion corrected images.  -Comparison with other state-of-the-art motion correction deep learning methods missing. For example, [13] was discarded because of the ""associated training routine"" and [29,30] because they do not take ""the physical nature of the MRI artifacts into account"", but these methods provide results within seconds, which is an advantage over the proposed method."	The proposed method was investigated for simulated motion which may not necessarily reflect the actual true underlying motion behaviour and impact on the image. Rigid motion correction for a single imaging sequence and imaging contrast was performed and hence generalizability of the autofocusing concept (task independent metrics/prior) is not known.	The authors included scripts but this reviewer encourages the authors to make them public accessible.	Source code is provided. Data from fastMRI database.	The study was conducted on a publicly available dataset. The method is described and could be reimplemented. The authors do not report if source code will be shared.	The authors should clarify the novelty of the methods in addition to the autofocusing methods. The authors should compare their method with the existing methods using GAN setup since their construction of loss mechanism is similar to an GAN based critic. Therefore, it is important to validate both the novelty and benefits of the proposed model.	"Other comments: ""Compensation for the motion artifacts is referred to as demotion"". I cannot say I have heard the term ""demotion"" before in the field of motion correction for medical imaging. The autofocus method is vaguely described. Please provide more detail. The authors mention that the disadvantages of MedGAN are the ""extra adversarial low functions and the associated long training routine"". How does the training time compare with the proposed method? It would be interesting to see how the outputs of both methods compare as well, since MedGAN provides images in seconds and the proposed method takes 7-9 minutes. Is autofocus+ conceptually similar to medGAN (expect for the deep regularization)? Is the performance of the proposed method superior to medGAN with a L1-norm regularization? It is not clear how blind motion correction can be performed from solving eq. 5. How is this related to autofocus, which uses entropy as a quality metric? It is not clear what is Sp, the output of the U-Net. Is it an image or the motion parameters? Section 3, ""10374 scans for training and 1992 scans for validation"". How many for testing? The fastMRI database provides about 1500 knee k-space data. However, 10374 were used for training. Please clarify. If DICOMS (magnitude images) were used, please clarify how eq 4 was applied. Please provide the network architecture details. Please provide the training time for all deep learning methods and computation time of motion correction for all methods. Please provide more details about the other methods. How was the regularization parameter for gradMC selected? Are the SOTA and autofocus+ U-NETs the same? GradMC seems to perform poorly for all cases and sometimes is worse than the corrupted image. Would results improve if the regularization parameter was optimized? Fig 3. Visually gradMC seems better than the corrupted image, but the PSNR and SSIM indicate otherwise. Similarly, gradMC looks better than autofocusing but the latter has better PSNR and SSIM. Please comment. Fig 3. Please add a zoomed section to the ""clean image""."	"Motion artifact reduction is more commonly known as motion correction/compensation/reduction than ""demotion"". Please clearly highlight in Introduction and Abstract that this work only addressed rigid motion artifacts. Please clarify if motion vectors were selected randomly from the reported ranges. Please specify how many different motion trajectories were generated per image and on what grounds the motion trajectories were selected and taken. Why were the rotation, translation and scaling parameters not jointly estimated inside one network or at least inside the same optimization steps (instead of shifting between rotation/translation and scaling estimation)? Was the UNet operated on the magnitude-only or on the complex-valued image? Were separate networks trained for each motion type or was a joint network trained? If the former, were cross-testings performed (train on motion type A, test on motion type B)? Was the amount of inner autofocusing iterations empirically optimized or selected? Did the authors investigate the proposed method's performance on out-of-distribution data, i.e. stronger motion amplitudes, different imaging contrasts etc.? At least for increasing noise levels, the network seems to be only affected mildly. Was a GPU-based NUFFT implementation used? What is the main computational bottleneck for the proposed approach? Please report if source code is shared."	The overall methods are interesting but this reviewer finds it is limited in novelty and comprehensiveness in experimental comparison.	"The authors proposed a neural network-based regularization term to improve the autofocusing method. More detail about the method is necessary to appreciate the contribution of this work. It is also important to understand how gradMC, ""a powerful optimization-based autofocusing method"", performs so poorly even with minimal motion."	The work is nicely and comprehensively depicted. Some methodological choices require further justification or investigation. Overall, the work describes an interesting new concept for addressing motion correction via autofocusing and deep learning.
056-Paper0386	AutoGAN-Synthesizer: Neural Architecture Search for Cross-Modality MRI Synthesis	Authors propose a new MRI synthesizer, called AutoGAN Synthesizer, which automatically discovers generative networks for cross-modality MRI synthesis.	Aiming at the recovery of realistic texture while constraining model complexity, authors propose a GAN-based perceptual searching loss that jointly incorporates the content loss and model complexity. To incorporate richer priors for MRI synthesis, we exploit MRI K-space knowledge containing low-frequency (e.g., contrast and brightness) and high-frequency information (e.g., edges and content details), to guide the NAS network to extract and merge features.  Considering that the low- and high-resolution of multi-scale networks can capture the global structure and local details respectively, they use a novel multi-scale module-based search space which is specifically designed for multi-resolution fusion. Their searching strategy can produce a light-weight network with 6.31 Mb parameters from module-based search space only taking 12 GPU hours and achieve state-of-the-art performance.	This paper presented a novel AutoGAN-Synthesis framework for cross-modality MRI synthesis by designing a generator to extract and fuse multi-resolution features. Moreover, the authors proposed a GAN-based perceptual searching loss to balance the model complexity and synthesis performance. Experiments on 2 datasets demonstrate that the proposed methods can outperform other cutting-edge methods qualitatively and quantitatively.	The proposed model can  search for a remarkable and light-weight architecture with 6.31 Mb parameters by occupying 12 GPU hours. Prior knowledge is introduced into AutoGAN the  for MRI synthesis.	1) Authors propose lightweight but effective model for cross-modality MRI synthesis. 2) They provide comprehensive experiments to verify the advantage of their method. 3) They propose several loss functions and present the visualized results, which is convicing.	The direction of investigating neural architecture search for cross-modality MRI Synthes is interesting. The experiments are comprehensive. Code will be made publicly available.	1: The main ideas of the paper are known or incremental advances over past work.  Besides, the incorporation of the K-spcace has also been employed in some medical image processing works. 2: The number of images are too small to obtain meaningful training results. There are only 75 samples of BRATS2018 dataset and 25 samples of IXI dataset for training. 3: The experimental setup details are incomplete. For example, the value of  and  in Equation(3) is unclear. The authors should provide the value of the hyper-parameters for the reproduction. 4: The code and the data are not available to aid reproducibility.	Table 1 is not suitable to place in Page 5, which makes readers hard to read this papers. Authors just compare the model weights but not whole computational complexity. Most models contain less parameters but take longer inference time.	The technical novelty is limited and incremental, as claimed in this paper, the two main contributions of this work are GAN-based perceptual searching loss and K-space learning while those two are off-the-shell techniques. The authors use 2D axial image for training and testing, they didn't consider the consistency in the coronal view and sagittal view.	The code and the data are not available to aid reproducibility.	I believe their experiments are reproducibile.	Since the authors mention they will provide the code to public, it is highly possible to reproduce this paper.	1: More experiments are needed: comparison to more recent and SOTA cross-modality methods; the convergence of the GAN model. Besides, the ablation studies are insufficient. For example, the learning rate. 2: There are not comparison of the InstanceNorm and BatchNorm. It's claimed that the InstanceNorm is superior to the BatchNorm in low-level image processing task	"This paper is well organized. Authours deal with the challenging tasks.  They provide comprehensive experiments to verify the advantage of their method and detailed methods in supplementary materials. I find the Equation(2) is hard to read. First, the ""Upsampled Fr"" is not the equation. Second, authors don't mention how to implement Upsample and down-sample. Please explain your methods."	It's not sure that by using proposed method whether the authors need to train different models for different number of modality MRI images as input? Or is it a uniform model that no matter how many modality MRI image there are, only one model is trained. How is the K-space data acquired? I think for those two public datasets they only give the magnitude image. As K-space Learning has been proposed for fastMRI in other works, I won't think adopting this strategy for cross-modality MRI synthesis can be summarized as a contribution. In figure 3 and 4, it's better to give the synthetic T2 images by different methods, not just the difference map. In figure 6, a part of the image is missing in Flair, but after adding adversarial and k-space learning, there is some context predicted in this region, why? Why do the authors choose to synthesizing T2 from Flair, rather than the opposite?	Both MRI synthesis using GAN and neural architecture search is not new. This  combination seem new.	I focus on experiemental results and methods. Both of them are good.	Please check the detailed comments
057-Paper1988	AutoLaparo: A New Dataset of Integrated Multi-tasks for Image-guided Surgical Automation in Laparoscopic Hysterectomy	This paper presents and describes a new data set for Laparoscopic Hysterectomy. The data set includes 1388 minutes of surgical activity. It is aimed to be used for research in multiple areas including workflow analysis, laparoscope motion prediction and instrument and anatomy segmentation. The authors reported results from applying several machine learning methods on their data set for each of the areas mentioned above.	This work contributes an integrated dataset AutoLap with multi-tasks to facilitate learning-based automation in hysterectomy surgery. A series of experiments are carried out on AutoLap with SOTA models. The dataset will be released to public after the paper is published.	This paper presents a dataset for facilitating ML-based approach development laparoscopic video understanding. The dataset contains sub-datasets designed for three different tasks including workflow recognition, laparoscope motion prediction and instrument/anatomy segmentation. It is stated that this dataset is provided to encourage advances towards surgical automation. Example uses of the dataset were demonstrated by benchmarking multiple state-of-the-art DL approaches. The authors claimed that the dataset will be shared along with the publication of the work.	There is a bad need for more publicly available data sets for applications such as automation in surgery. The main strength in this paper is that it aims at filling this gap by providing a data set from real surgeries.	This dataset is collected from real surgeries in hospital. The single dataset works for three tasks: workflow recognition, laparoscope motion prediction, and scene segmentation. SOTA models are evaluated on the dataset to present the benchmark references.	The paper is well-written, its structure is clear and easy for readers to follow. The topic of this paper is closely relevant to MICCAI interests, in particular focusing on fostering the CAI research field. This paper aims at presenting a newly annotated laparoscopic video dataset focusing on three important sub-tasks towards surgical automation. The details of the dataset are provided and it does seem straightforward to use this dataset upon its release. Benchmarking examples are demonstrated. Several state-of-the-art approaches have been tested on the dataset, and this further proves the usability of the dataset.	In my view, the main weakness in this paper is that the authors made many assumptions regarding their data set that need to be properly justified.	"The major weakness is that the value of ""multi-task"" dataset is not fully presented. There have been plenty of surgical vision datasets. The key feature of AutoLap is multi-task. So, what is the relationship between the three tasks? How to leverage the correlation over tasks? How is the feasibility? Without deep analysis and discussion about ""multi-task"", the value and innovativeness of another new dataset are limited. The AutoLap dataset should be positioned in the surgical research area. A table is suggested to compare the features of AutoLap and existing datasets. See https://arxiv.org/pdf/2011.02284.pdf. The sample number for segmentation task is sufficient. But are the sample numbers for the other two tasks sufficient for training DNN?"	"""Towards surgical automation"" has been stated as the motivation of providing this dataset. However, the videos used for this dataset were collected from non-robotic laparoscopic surgeries. Given that robotics is an essential component for automation, it is not clear that how evaluating the ML approaches on this dataset can be directly transferred to robotic video datasets. As for robotic surgical tools would appear differently in the videos compared to conventional laparoscopic tools and the generalizability of ML approaches is still doubtful, the value of this dataset for surgical automation would still be questionable."	There are some improvements that need to be made on the reproducibility aspect such as: Sharing the data set or at very least a sample of it, especially since this is the data set is the main contribution of this paper. A clear explanation of some of the assumptions made with respect to the data set is needed, especially the parts of the data set on laparoscope motion prediction and anatomy segmentation.	The dataset will be released upon paper publication.	The authors stated that the dataset will be released upon the publication of the paper.	"I strongly recommend renaming the dataset. The current name ""AutoLap"" is the trade mark for an autonomous camera system for laparoscopic surgery, see: https://www.youtube.com/watch?v=_fXPgRgTEAY&ab_channel=MST-MedicalSurgeryTechnologies The term ""Image-guided"" implies that you provide data from imaging modalities such as ultrasound. That is why I suggest the authors use the term ""vision-based"" instead throughout the paper. On the title: Is the use of this dataset restricted to just automation? If not, I would change the term ""automation"" to be ""activity"". This should widen the possible use cases by researchers in areas other than automation as well. It is really difficult to reason how this dataset is useful without actually seeing the dataset itself (or at least a sample of it). Would it be at all possible to find some way to share it so that we can also review it? In the introduction: the authors wrote: ""To enhance the surgical scene understanding towards image-guided automation, the most promising solution is to rely on learning-based methods"". I would say ""one promising solution"" instead of ""the most promising"". The latter is a very strong statement that need substantial and clear evidence. I find the word ""task"" very confusing in the context of this paper, with respect to the use of this term in automation for surgery papers. In these papers, the term tasks or subtasks refer to tasks such as suturing, knot tying and so on. To avoid this confusion, I recommend that the authors use another term. On the data set collection: Were all the videos coming from one surgeon's practice or from multiple surgeons? If it is the latter, did the authors account for the individual preferences of surgeons (such as their preferences on moving the laparoscope ) by any means in their dataset and models? On the data set collection: Did the imaging platform used provide 3D view of the scene? And if so, which video channel was eventually used to record the video (the left or right channel)? Please add this to the description of the dataset. On the data set collection: Were there cases of disagreement between the annotations of the gynecologist and specialist? If so, how did the authors handled these cases? The part of the data set on the laparoscope motion can be significantly improved to avoid confusing the reader. This is mainly because of the design decisions that the authors made in this part that are not justified such as: a. using part of the data set and not the entire data set for motion prediction.  b. Discretization of the motion of the laparoscope into 6 motion types instead of treating the motion as continuous variable.  c. The choice of the types of motion which seems to miss cases such as moving the camera diagonally. Currently, I guess this can only happen by discretizing the diagonal motion into two types of motion (e.g., up and then left). d. Setting T to be 5 seconds What not considering the entire data set for the part on anatomy segmentation as well? What is the value of presenting the results in table 4 especially since the accuracy is not very high?"	The authors should revise the paper according to the above weaknesses.	"This reviewer is generally pleased to see a paper that contributes a comprehensive dataset to the CAI field. As the paper is well-structured and provides details/examples of how to use the proposed dataset for benchmarking, this reviewer would think this paper is acceptable and only have a couple of comments listed as below: Please mention the differences between laparoscopic videos vs robotic laparoscopic videos (tool appearances, etc), this would help the readers to understand that how learning on such a dataset could be transferred to ""towards surgical automation"". It would be good if the authors can also provides insights how the size of the this particular dataset is correlated to the success of evaluating ML approaches? Is the size of each sub-dataset fair enough for judging the model performance? It was nice that the authors have provided the benchmarking results of the several existing frameworks. As the dataset is provided with clinical relevance, it would be better that if the author can discuss what are the clinically-acceptable accuracies in these sub-tasks? One common question for most of these existing datasets is that after a researcher evaluate his/her approach on the dataset, how would the researcher know if the approach is doing good enough for the tasks? The readers would appreciate if the authors could provide more discussion on this."	"The need for having more publicly available data set justifies my leaning towards accepting the paper. The several questions I outline above justify why it is a ""weak"" accept."	"This work provides a novel dataset featured by ""multi-task"". However, I expect more contents to expose the value of ""multi-task"". At least the authors should tell why the three tasks are coupled and how to practically benefit the surgical automation as a whole."	This paper presents a newly annotated dataset to the CAI field. The paper is well-written, and the description and details of the dataset are adequate. Several state-of-the-art approaches have been tested on the dataset as example uses. Overall, this paper is acceptable.
058-Paper1919	Automated Classification of General Movements in Infants Using Two-stream Spatiotemporal Fusion Network	This paper proposes an automated framework for GMs classification consists of preprocessing networks and motion classification network. With the former network getting rid of the background information and aligning the body orientation, the latter network processes the spatiotemporal features. The proposed method outperforms the listed comparing methods.	"The paper addresses the problem of Automated Classification of General Movements in Infants. This problem is known to be still difficult and ""unsolved"". Authors propose an image based approach (using videos) to rate the GM category. In the proposed approach, first a focus is made on removing the background of the video and the pose of the infant is normalized. From these processed images, authors use a two-stream spatio-temporal network to predict the GM category. Authors evaluate on a proprietary dataset and make the code available. Results outperform current state of the art."	Focus on the infant general movement classification problem in this work, a preprocessing network is introduced to remove unnecessary background.from GM videos and adjust the infant' body position.  A two stream structure for motion classification is proposed based on both spatial and temporal information.	The motivation is reasonable and interesting. The paper is well-written The GM classification data set was collected and labeled.	"The proposed preprocessing networks, while not technically novel, are an effective way to normalize against background, pose and scale. Authors leverage existing work on two-stream spatio-temporal architectures which seems particularly well suited for GM classification authors release the code for comparison. One major difficulty of comparing automating GMA methods is that the data (recordings of infants) are often ""private"". The release of code will allow the community to compare on other cohorts."	Detailed ablation study to demonstrate the effectiveness of the proposed components including the two stream design and the preprocessing network.	"In general, the contribution and novelty is limited. The proposed method is a combination of the processing networks and the two-stream network. Is backgroud removal a necessary step? For one thing, it's very difficult to remove them all. Then, based on the most recent human motion and video content analysis research, it's unncessary to do that. Lacks of some detailed of compare methods such as paper [26], [18] and some necessary analysis of why they(sota) work much worse here. In Table 1, the preprocessing networks were applied to both ""Baseline"" and ""Ours"" and all outperform [26] and [18] a lot. It seems that preprocessing is playing a major role. But the ablation study in Table 2 shows that it is not so important. It confuses me here the single stream network without preprocessing works so good. And it may be more convincing to test the proposed method on other public datasets. In table one, the comparing methods are [7], [18], [25] and [26]. But they are not SOTA method in human motion analysis, and two methods are published in 2015. Latest video analysis and human motion classification methods from computer vision fields should be considered and compared."	"There is no clear novel technical contribution. It is at the boundary of a  ""Methodological studies"" and an ""Application study"". The evaluation and comparisons of the method could be improved: Preprocessing (3.1) + final evaluation and comparisons."	"It is not clear if the dataset will be publicly available.  WM, FM and PR should be clarified what they are.  The angle and scale preprocessing is similar to the idea of  ""In-Bed Pose Estimation: Deep Learning With Shallow Dataset"" , not cited."	The code is released but the dataset is not.	The code is made available with all steps.	code is avialalbe, dataset not.	It is a good work to solve the GM task with explicit and reasonable motivation. And experiments prove its effectiveness. But I think it may be better to show the details of implements of compared methods and give more analysis of the experiments result. And experiments on more datasets is encouraged to make the conclusion sound. And some latest methods could try to applied in the motion classification network to replace the two-stream network for processing spatiotemporal infomation.	"evaluation on the preprocessing networks (3.1) is missing. One wonders what is the quality of the output. In Related Work authors write ""The pose-based approach [5,18] ... performance depends on the accuracy of the pose estimation algorithm."" to motivate the use of a video-based approach. However, the proposed process is based on OpenPose (and later on Farneback optical flow method [10]) which may have errors. Could they be quantified?  The underlying question is: is it worth to improve these preprocessing steps to increase the performance or should one focus on the two-stream networks? No evaluation of the computed flow (Farneback method [10]). How reliable is it? Are outputs noisy, temporally consistent? Same underlying question as before. It is unclear if STAM [18] was retrained with the new dataset to have a fair comparison with the proposed method. It is unclear how authors compare to [26]. To the best of my knowledge their code is not available. Did authors re-implement it? Did authors retrain on the new  dataset to have a fair comparison with the proposed method? Authors do not evaluate on the [26] dataset, which is available ""upon reasonable request"" (https://www.nature.com/articles/s41598-020-57580-z#data-availability). It would have been interesting to have a second comparison."	See weakness.	The research motivation is clear and the paper is well-written. In general, the idea of proposed method is not new and contribution is limted. Also, the comparison and ablation study cannot fully show the effectiveness of proposed method.	The automated General Movement Assessment is a difficult and yet clinically very relevant problem. The proposed approach is sound, relatively simple (in the good meaning of the word simple)  and the obtained results (even with the mentioned evaluation weaknesses) are improving the state of the art.  At this point the merits slightly weight over the weaknesses.	Detailed ablation study and technical sound.
059-Paper2247	Automatic Detection of Steatosis in Ultrasound Images with Comparative Visual Labeling	The authors propose a comparative visual labeling (CVL) + RankNet approach to develop comparative and reliable labels for training and testing computer aided diagnostic systems.	This is an interesting study about the fatty liver disease diagnosis in ultrasound. Authors investigated problems related to the lack of proper reference labels for the development of fatty liver disease diagnosis methods. To address the problem, a comparative visual labeling (CVL) along with the RankNet method were used to improve the quality of the labels determined by three annotators. Moreover, authors trained a CNN with differently obtained labels to classify fatty livers to confirm the quality of differently obtained labels.	The authors have proposed to use a RankNet to improve the healthy/pathological label for Steatosis detection using Ultrasound images. The inputs to the RankNet are randomly selected paired images from the dataset, and they are trained on binary labels provided by the annotators showing first or second image has the highest degree of pathology. They have thresholded the scores generated by the RankNet to acquire labels and evaluated their quality by comparing to histopathology results, outperforming the visual labels provided by the annotators.  The new labels did not enhance the classification performance using CNNs.	This is the first time CVL is applied to diagnostic labeling of medical images. It provides a reliable way of generating labels for medical diagnostic tasks, which are often subjective and difficult to obtain. The authors show that deep learning models trained with these labels achieve similar performance to histopathology labels. The paper is well-written with nice, illustrative figures. There are sufficient experiments to support the findings.	Authors addressed an important problem related to the fatty liver disease diagnosis. Sometimes the reference labels for the fatty liver disease diagnosis are not available. In this case, radiologist assess the US images to provide the labels. An approach to the robust image labeling would be therefore very useful. Authors used the RankNet algorithm in an interesting and novel way. Authors compared the proposed approach with the conventional method. 3 annotators participated in the study. Table 1 suggests that the proposed method improved the quality of the labels to some extend. good reproducibility. Authors plan to release the codes and the dataset.	The paper is well-organized and easy to follow.  The authors have presented an innovative idea of using RankNets for their specific application of improving the label quality in Steatosis detection, which can be extended to any other usage of biomedical images for disease detection.	"There are some typographical errors, e.g. page 6 ""A drawbacks..."". Also some abbreviations are used without explicitly mentioning them, e.g., DL in the abstract. It may not be evident to all readers that DL is 'deep learning'."	"the technical novelty of the manuscript is limited, authors used well-known techniques for the ranking (RankNet implemented as a feed-forward network) and for the image classification (InceptionResNetV2 pre-trained on the ImageNet). the proposed method is more time consuming compared to the conventional labeling. The usefulness of the proposed method would be probably limited for a larger dataset. Authors performed multiply experiments to calculate various cut-offs, performance metrics and to determine hyper-parameters. It gives an impression that an independent test set was not used for the overall method evaluation. experiments were performed on relatively small datasets, which makes it difficult to assess the usefulness of the methods (see the next two comments) ""Two classification CNNs were trained on Dataset 2 using SVL and CVL+RankNet labels, and tested on Dataset 1 ... ROC-AUCs were 0.89 (CVL+RankNet) and 0.86 (SVL), and the difference was not statistically significant (p = 0.34)"" It seems that the classification performance did not significantly increase thanks to the proposed method. The same issue is associated with the results presented in Table 2. the proposed method did not improve the F1 scores for 2 out of 3 annotators, according to the McNemar's test and Table 1. It seems that the labels obtained with the conventional approach were of good quality. For a dataset of 50 cases the 10% drop in accuracy would correspond to 5 mis-classified examples, which is after all a small number. This suggests that the proposed method improved the labels for only several cases compared to the conventional approach."	According to figure 2, what is fed to the RankNet are one-hot encodings of the image indices, not the data from the images themselves, and the network creates the scores based on these one-hot encodings. I believe the authors should elaborate more on this, to clarify whether this is really the case, and if it is how it leads to the generation of better labels. Both datasets used by the authors are very small ones, and since the authors have not presented any information about the data stratification, I believe the networks are trained on very few samples, which in my belief, affects the credibility of the results. Authors should mention if they have augmented the dataset, or if they have trained and tested on the same dataset (For the results of Table 2, for example), which they shouldn't have!	Reproducible. Code and data will be made publicly available.	Good reproducibility. Authors plan to release the codes and the dataset.	One of the datasets used in this paper is public, and the data provided about the networks structure and hyperparameters are enough to reproduce the code. Although the authors have mentioned that they will make the code public.	Please do a thorough proof-reading of the paper.	The general idea of the work is very interesting. It would be interesting to evaluate the proposed approach on several datasets from different medical imaging modalities. Evaluations on a larger liver US image dataset would probably better highlight the usefulness of the proposed approach. Authors separated the dataset into four groups for the evaluations, Fig. 3. I think that it would be interesting to directly related the error rate with the liver steatosis level. The mild group included cases with the steatosis level between 5% and 33%. In practice, I would expect the labeling errors  mostly for the cases with the steatosis level between 5% and 10%. Since the radiologists need to assess pairs of liver US images, the proposed method is more complicated and time consuming than the conventional approach. Results presented in Table 2 show that the proposed labeling method does not improve the classification performance. Unfortunately, this suggest that the conventional method would be probably preferable in this case.	"I believe the text should be further edited. For example, In the abstract the authors do not need to mention SVL because it is never used again in that section, and this sentence ""Code and data will be made publicly available."" should be moved to probably after you've talked about your data and models in the Methods section.  The acronym inside the parenthesis in line 12 of the second paragraph of page 2 should be SVL. And also I believe this sentence is missing a verb. Or in the following line it should be surgical ""skill"" assessment. And ... For this sentence in the results section, ""This indicates that the CNNs have some inherent robustness to training label errors in this task."" authors should either provide a justification or a reference."	My recommendation is based on the novelty of the task and its immense applicability in the field of medical imaging where generating good quality labels is often difficult and impedes machine learning training. The paper is very well written and can stimulate research in application of comparative visual labeling to medical diagnostic labeling tasks more.	Authors proposed an interesting approach to the labeling problem in fatty liver disease diagnosis. However, the usefulness of the proposed method was evaluated on relatively small datasets, which makes it difficult to assess the results. Moreover, the usefulness of the proposed method might be limited in practice for larger datasets due to the requirement to generate image pairs.	The use of the Ranknet for this application is innovative but I think the authors should provide more information on the type of inputs to this network, and the way they have evaluated their performance.
060-Paper1361	Automatic identification of segmentation errors for radiotherapy using geometric learning	X	The authors present a combined CNN-GNN model to perform segmentation error prediction. The model consists of three parts. First, a CNN encoder generates features from image patches extracted along the boundary of the contour, the intention is to include appearance information into the task. Second, a GNN operating on the meshed contour with the features of the CNN encoder attached to the nodes, updates each node's representation according to its local neighborhood. Third, a MLP classifies each node's features into five classes representing different bins of signed (inside/outside) distances to the surface of the true contour. The model is trained and evaluated with synthetically perturbed contours of parotid glands.	For developing a tool to automatically identify errors in 3D OAR segmentations without a ground truth, this paper proposes a novel quality-assurance (QA) architecture combining a convolutional neural network (CNN) and graph neural network (GNN) to leverage the segmentation's appearance and shape. Experiments demonstrate the effectiveness of the proposed QA method. Paper structure is good. Figures are nice.	X	The major strength of the work is the novelty of the hybrid CNN-GNN for contouring error prediction. The method does not require secondary training of alternative models as related ensemble approaches or statistical model information for local error predictions, and differently to classification approaches only finding failed contours, the method offers with the node-wise classification richer information regarding potentially erroneous parts of the segmentation.  Also, the training time for a new organ of only 10 minutes is very interesting for a potential practical application as an additional QA tool for both manual and automated contours.	Topic is relatively fresh. Lots of paper talk about how to delineate targets, while limited number of them think about error control.	X	Although, the work is verry interesting methodically a few major weaknesses can be identified: Terminologically the authors claim their method to be self-supervised (training with synthetic data) and unsupervised (pre-training of the feature extraction), however neither is the case. I would categorize the creation of the perturbed contours as a smart form of data augmentation and the pre-training to be supervised as the class of each patch to be on the contour or not is directedly extracted from the reference contour.  The usage of both terms self-supervision and unsupervised pre-training is confusing and does not follow their definitions.  The authors use the public Dataset of Nikolov et al. as basis for their work. The dataset contains human bias freed reference contours (per-reviewed by expert radiologists and oncologists), and contours of a radiologist which are used in the clinical study of Nikolov et al. as a human reference. The authors simply claim to use one of the contours. Although the dataset is only used as a proof of concept for the work, the authors should make clear which contour they use and not train on human biased contours.  The method is trained and validated on the same synthetic dataset, as the approach is methodically novel there is no reference method available, however, it would add a lot of value to validate the method on real erroneous contours i.e., the second contours of the used dataset created by an independent human annotator or the output of a model. The selection of the hyperparameters for the creation of the synthetic dataset as well as the process of meshing the contour seems to be arbitrary. It could make sense to perturb the contours within the range of a meaningful observer variability or an expected error range of a current automated segmentation approach, both values that are studied in the work of Nikolov et al.. Although, the authors claim in the discussion that a future step is to generate training data based on real observer variations, could a meaningful parametrization of the perturbations already be integrated. In this sense, also, the resolution of the mesh could be identified rule based regarding the size of the organs, while the chosen values seem to work well for the tested organ, it is not clear how the mesh resolution would suit much larger or smaller organs. Finally, also the selection of five bins/classes of distances is missing an argumentation. The ablation study shows that the pre-training of the feature extracting CNN does not lead to an increase in performance, dedicating less than a whole section for that process could be considered.	Idea is clearly presented. However, it seems like only five level of errors are analyzed, from larger than 2.5mm to smaller than -2.5mm. Is there any reason for you to use the two threshold of 1mm and 2.5mm? The dataset description is rare. At least we should know the resolution of the original CT scan, since you talk about errors less than 1mm. If the pixel spacing of CT is large than 1mm(especially in axial direction), the results become  meaningless. The paper is more clinical driven instead of technique itself. As mentioned in radiotherapy, normally there is a margin between PTV and CTV. The margin is sometimes 5mm. If so, the importance of 1mm-2.5mm error check is not that important. So, talk about the motivation. From Fig 4, subfigures a d are easy to understand that large errors are easier to be recognized. However, from b, it seems like the decrease of performance on small errors(confusion matrix value of -1mm-1mm)is not that obvious comparing with that of large errors. How do you comment that?	X	The dataset is publicly available, and the authors claim that the code will be made available as well. However, the architecture of the networks is not becoming clear with Fig.2 and 3 only. What are the values in the CNN, GNN and fully connected blocks in Fig.2+3 indicating? Also, the dimension of the features generated by the CNN and later the fully connected layer for the classification is relevant information that is missing.	Due to the provided implement details, I guess it is reproducible.	X	"Next to the identified major weaknesses only a few minor problems are identified: Page 2, Introduction at the end: An ablation study is probably not a contribution, self-supervision terminology Page 5, Unsupervised pre-training: pre-training vs transfer learning terminology, self-supervised vs unsupervised pretext task terminology Page 7, Ablation tests: what is an erratic validation? Loss curves could probably be included, but in combination with the training loss. The validation loss is really smoothed by the pre-training, seeing the curves also helps to understand the statement in the discussion, claiming that the training is smoothed. Page 8, Discussion: The sentence: ""However, these approaches require the adoption of the new segmentation models themselves."" is unclear. Also: ""... fills a void as most ..."", what is meant by fills a void? The statement that the Parotid Gland is a difficult organ to segment needs a reference. General: Most of the paper is written in past tense, which, if at all, should only be used while discussing related works."	see  main weaknesses part	This paper attempts to train a solution for identifying segmentation errors. Segmented parotid glands were perturbed with a tiny amount of random noise, and then smoothed.  The network architecture is interesting, containing both a CNN and a graph NN.  Graph elements are classified into 5 groups related to their distance from the original segmentations. Overall, this is a worthwhile paper in that it targets an important problem and the approach is plausible.  The evaluation is fine, but without a proper user  study we cannot really know if this will be useful.	The novelty and practical relevance of the introduced method outweighs the weaknesses of the work. The selection of the hyperparameter can be adapted to meaningful contouring errors and the evaluation extended to at least the human annotator in the used dataset (better to results of a current automated approach), in order to additionally get an evaluation on practical but not only synthetic contours. The partly incorrectly used terminology must be corrected, but overall, the methods innovative combination of CNNs and GNNs to consider both appearance and topology very efficiently for contouring error prediction justifies an accept.	The idea is relatively new.
061-Paper2252	Automatic Segmentation of Hip Osteophytes in DXA Scans using U-Nets	This paper deals with the automatic segmentation of osteophytes in hip dual X-ray absorptiometry scans (DXAs). The proposed approach uses U-Net.	The article presents a semi-automatic method for segmentation of hip osteophytes from DXA images. The first step of the method is manual localization of hip joint keypoints (can be done automatically with BoneFinder tool, as mentioned for a subset of the data). The second step is a vanilla UNet with minimal modifications applied to patches extracted around certain keypoints.	The authors proposed 3 deep learning networks (U-Nets) for automatic segmentation of osteophytes in DXA scans at three different sites: inferomedial femoral head, superolateral femoral head, and lateral acetabulum. The proposed framework provide good sensitivity and specificity for the detection of osteophytes and average Dice metric in terms of semgentation accuracy.	The original idea of the paper is the automatic segmentation of osteophytes on the hip from DXA scans using neural networks.	Osteophytes are one of the most distinct signs of osteoarthritis and they are included in different OA grading schemes (for hip, knee, hand, etc). Their clinical picture and overall etiology are not well understood. Since this research direction is covered in the literature very sparsely, this work provides new evidence on feasibility of automatic osteophyte detection and quantification. Subsequently, the work and the developed method open opportunities for further epidemiological studies, at least with the UK BioBank data. These are the primary contribution of this work. The methodology applied in the study is principled at all steps (dataset creation, annotation, data standardization, model training, analysis) and is well shaped to approach the related clinical quesiton. Large sample size. The article is very well structured and written in a clear language. The graphical materials are informative and sufficient.	Novel application: automatic segmentation of osteophytes from DXA scan is new and if successful could open of new avenues for screening population for a risk of total hip arthroplasty (THA). Testing the method on UKBB dataset with 41,160 left hip DXA scans is a strength. The sesisitivity for detection of osteophytes was excellent >95% but the specificity was fair (>70%). The dice metric was also fair ~0.65.	The contribution is rather fair. Use of the existing U-Net to segment osteophytes on DXAs.	While the osteophyte detection performance is relatively high, the segmentation one is rather low for downstream clinical applications (Dice score of 0.6-0.7). One of the potential reasons is that the applied segmentation method is very basic (essentially, UNet without hyper-parameter optimization, as stated by the authors). Overall methodological novelty of the work is unclear. Otherwise, no notable weaknesses.	The proposed U-Nets require a priori landmark detection step using bone-finder software. The authors mentioned on page 6 that landmark positioning error in this step may adversely affect the U-Nets performance especially in the lateral acetabulum (corresponding landmark point 78). Given the power of deep networks, would it be possible to learn directly from the raw DXA scans and integrate the landmark placement algorithm into the U-Net architecture? 820 scans out of 41,160 subject were excluded due various reasons including image quality. How did you assess the image quality? -The dice metric for segmentation accuracy is fair suggesting potential improvements would be possible by optimising the network further. You may report the dice index for manual annotation between the two radiographers and discuss the proposed framework utility in light of this results. Minor comments: In figure 4 (second row), it is better to overlay the ground truth mask on the actual DXA scan. Page 5, 'with He normal initializer' -> 'with the normal initializer'?	A public database is used, the UK BioBank. No proof of reproducibility.	Overall, I consider reproducibility of the study as high. Several remarks below. The exact model architecture is not given explicitly, eventhough it is understandable from the text to some extent. The authors have checked the code release in the checklist, however, there is no link to the source code in the article. Software versions are not specified for the critical components of the pipeline.	The results should be well reproducible. The authours employed the standard U-Net architecture for segmentation and the initial landmark detection was also carried out using bonefinder software which is also available upon request. The dataset is from UKBB which is also available upon request.	This paper deals with the automatic segmentation of osteophytes in hip dual X-ray absorptiometry scans (DXAs). The proposed approach uses U-Net. The paper is well written. The contribution is rather fair. Contributions of the study are not well described in the paper. As contributions, only the performance of the proposed system are reported. The cropping method of the patches is not clear enough.	"Page 3: While most of the studies in the domain are mentioned, please, add a citation to the recent study on segmentation of ostephytes from MRI - https://www.oarsijournal.com/article/S1063-4584(21)00484-2/fulltext . Also, please, include those results into Discussion. Page 5: It would be appropriate to elaborate on how the manual and automatic keypoint annotations differ. Perhaps, by showing a sample from each of the groups. Page 5: The authors say ""Automatic point placements were available for a subset of all images"". Please, provide more informative details on how the subset was selected - certain cohort? / random? / etc. Page 5: Please, specify software versions for BoneFinder, Keras, and Tensorflow. Page 5: ""optimized with Adam [28] (default parameter values used)"". Please, state explicitly the default parameter values. ""dropout rate, with probability 0.3"" -> ""dropout rate of 0.3"" / ""dropout with probability 0.3"". Table 2: The authors use osteophyte detection Sensitivity/Specificity as the performance metric. However, it is not clear from the text what is the detection threshold (> 0 voxels? other?). Please, state explicitly. Table 2: The standard deviation of the Dice scores are considerable in comparison to the mean. For the reader to better understand the extent of the segmentation errors, it would be informative to visualize additionally either or both: (I) the distribution of sample Dice scores - histogram where (x) sample Dice score, (y) number of samples, (II) the distribution of sample Dice scores at different osteophyte severities - scatter plot where (x) ground truth osteophyte area, (y) sample Dice score. Please, consider adding those plots in the article or as a Supplemental material. Page 6: The authors make a hypothesis ""could it be underfitting due to insufficient training examples?"", which eventually does not hold. This assumption sounds somewhat questionable, in the first place, since the number of the training/validation samples is actually the highest for ""Lateral acetabulum"". I would suggest to remove this point from the text."	As discussed by the authors, the initial landmark detection step can adversely affect the U-Nets performance. I would integrate the landmark placement step into the U-Nets architecture to segmentent osteophytes directly from DXA scans. Approximately 2% of scans were excluded due to poor image quality. Identifying these scans in large dataset with 40,000 scans could be demanding. will you consider automatic image quality assessment?	The contribution is rather fair. Direct use of the existing U-Net for the segmentation	Analysis of osteophytes (morphological and whatnot) in a rapidly emerging and important topic in scope of understanding the clinical picture of osteoarthritis progression. The study provides new evidence into the performance of automatic methods in the task, thereby complementing the sparse existing literature on the topic. The presented results have a good potential to facilitate a discussion on the best practices in annotation of osteophytes, longitudinal studies of their morphology, etc.  The study is conducted in a very principled way (excellent justification and description of the method components and the performed steps).  The overall novelty of this study in terms of methodolgy is rather limited.	This is a well-written paper with an interesting topic and results, but the methodology is not quite novel so I rate this paper as 'accept' rather than 'strong accept'.
062-Paper0295	Automating Blastocyst Formation and Quality Prediction in Time-Lapse Imaging with Adaptive Key Frame Selection	This paper proposes a deep learning method to predict the viability for IVF of an embryo, based on microscope video timelapses spanning 3 days, and with groundtruth taken as an embryologist evaluation at day 5/6. The base structure of the method is a spatial-temporal CNN+LSTM with additional positional encoded features. The core novel contribution is a selection mechanism that decides which frames are relevant for the classification task, which shows an improvement in classification when compared to using all frames or other selection methods.	The paper proposes to adaptively select informative frames from the TLM video, to improve the blastocyst classification. A policy network with Gumbel-Softmax sampling approach and a reward function are developed for frame selection. Experimental results on the in-house dataset show the effectiveness of proposed method, outperforming the SOTA across various evaluation metrics.	In this paper, the authors proposed a novel deep learning framework to adaptively select informative frames for blastocyst formation prediction using time-lapse monitoring (TLM) videos on D3. Extensive experiments were conducted on a large TLM video dataset to evaluate the proposed method; experimental results demonstrated its superiority over the latest state-of-the-art methods.	The paper is clearly written. The frame selection mechanism is interesting and can be potentially useful in other relevant MICCAI topics (e. g. surgical workflow segmentation / action recognition).	Novel method about key frame selection for improving the automatic blastocyst formation. Promising results on the in-house dataset are achieved.	The proposed framework is novel. It addresses two issues in existing methods: input videos need to be manually annotated; redundant and irrelevant information in TLM videos can overwhelm the informative ones. Extensive experiments on a large-size dataset were conducted, which includes comparison with state-of-the-art methods and ablation studies, to systematically evaluate the proposed method. The manuscript writing is very good and clear.	The main weakness is the limited analysis of the experimental results, and in particular it would be useful to expand on the frame selection results. (also refer to detailed comments). More qualitative results could be provided as supplemental material, including failure cases and interpretation of the corresponing selected frames. Additionally, there are no comments on limitations of the proposed method, current limiting challenges, and future research. Conclusions could be improved by incorporating such comments rather than just summarising the paper.	Some statements and experimental settings should be clarified	It would be nicei if the authors could discuss about potential limitations about the proposed method.	The authors state they will release the code. Also, the description of the method is clear and simple enough to replicate the proposed architecture and training methodology. The authors state they will release data in the reproducibility form. This would be great as to my knowledge there is no equivalent public, open data.	Not good, as they evaluate the method on their in-house dataset, while do not mention the release plan of both dataset and code	Dataset and codes are not publicly available.	"The authors mention ""kinetic parameters"" as a data input, however, this is never explicitly described/defined. Is this just a frame index / timestamp? Are these manual annotations, or are they automatically extracted? If manual, then it means the method is not fully automatic which is not clear in the current submission. It would be important to clarify. Table 1: Could you report the number of selected frames for competing methods? At least one of them is mentioned to do frame selection. Table 2:  I do not understand LSTM having #SF=32. From how the dataset is described, I would expect the number of frames to be in the thousands (every 5 min, over 3 days), so why is LSTM only using 32? Is there any subsampling? Fig. 3: Images have visible person names on the top. I don't know if this is any sensible information, should it be hidden? Fig. 3: Could you display which frames (index/time) are being selected? Qualitative evaluation: ""AdaKFS can select a small number of informative frames of different embryo development stages, such as 2-5 cell and 8-cell stages"".  A more in-depth analysis of selected frames would be useful: showing more results, including failure cases (in suppl material). Additionally, it would be useful to have more quantitative statistics of specific events that are detected (i understand this is a lot of work though). Can embryologists make an accurate prediction based on selected frames? I wonder if the selection mechanism could be useful just by itself and make embryo analysis faster / more convenient. Any comments?"	Do you plan to release the dataset as well as the code? The definition of morphokinetics parameter should be clarified. There are several statements which are imprecise: e.g., 'using a small number of frames and their morphokinetic parameters, without any extra annotations' in method. However, the method still requires the classification annotation for training model. The determiners should be added. What are the dimensions of f_t and k_t? It seems that the numbers of selected frames for different sequences are different with the proposed method. It is better to illustrate the range of the total length of different sequences, as well as the range of the numbers of selected frames. In the ablation study, how to determine whether the frame are selected or passed when only using policy network? It is interesting to see the separate efficacy of kinetics features. The ablation using kinetics with only LSTM could be added.	Although the authors implicitly indicate how the image features ft and kinetic features kt are generated in the framework shown in Figure 2, it would be better if the authors could explicitly mention that in the method section. How did the authors tune the hyper-parameters for different methods in experiments? Page 6, 'We uniformly sample T = 32 frames ...'. Was the same sampling process performed in the testing period? Will the sampling rate affect the prediction performance? Page 6, last line: The authors may want to cite a reference for 'ImageNet-pretrained weights'.	My overall impression of the paper is positive. The selection mechanism is clearly explained and could have potential application in other video analysis problems targeted by MICCAI. I need a rebuttal to form a stronger opinion.	Interesting method with promising results	The proposed framework is novel. It addresses two issues in existing methods: input videos need to be manually annotated; redundant and irrelevant information in TLM videos can overwhelm the informative ones. Extensive experiments on a large-size dataset were conducted, which includes comparison with state-of-the-art methods and ablation studies, to systematically evaluate the proposed method. The manuscript writing is very good and clear.
063-Paper2315	Automation of clinical measurements on radiographs of children's hips	The authors propose an automatic method to determine two radiographic parameters for the monitoring of hip anomalies in pediatric AP hip radiographs. The author propose to use the concept of Random Forest Regression Voting to detect automatically anatomical landmarks for acetabulum and femur. These landmarks are then used to determine the radiographic parameters (AcI, RMP). The proposed system was tested and validated on a clinical dataset of 200 images (400 hips).	The authors propose an automatic system for calculating the acetabular index (AcI) and Reimer's migration percentage (RMP) from paediatric hip radiographs. These two clinical measures are used for the diagnosis of developmental dysplasia (DDH) of the hip and monitoring hip migration for cerebral palsy. The approach uses the Random Forest Regression-Voting Constrained Local Models (RFRV-CLM) framework to automatically locate landmarks which are in turn used to automatically calculate the measures. They test their approach on a challenging dataset of pelvic radiographs of children containing cases of severe disease and occlusions. They report high conformation between the measures as automatically determined by their approach and those measured by clinical experts.	In this paper, the authors introduce a Random-forest-based method for measuring on coronal radiographs angular parameters of the pediatric hip that have clinical value.	The proposed method relies on automatically detecting anatomical landmarks, which are then used to determine the radiographic measurements. This generates a system which I believe has a great potential for easy adoptability by clinicians, due to its ability to be explainable. Furthermore, the authors tested and validated the proposed method extensively on an independent dataset with multiple manual measurements.	Significant clinical feasibility for monitoring DDH and achieves SOTA for at least one of the measures, namely RMP. The approach is tested on a challenging dataset which includes cases of severe disease. Furthermore, the method is tested on a replication dataset to indicate performance in the wild. The authors describe a reasonable effort to establish ground truth dataset with multiple clinician landmark annotations and they also report on the agreement of the landmarking.	This study is of clinical value. This article takes into account the reproducibility of manual measurement in the evaluation process.	The Random Forest Regression Voting Constrained Local Models framework was previously applied to segment proximal femur from AP hip radiographs, by identifying anatomical landmarks around the femur contour. The contribution in this work is in extending the idea to also include acetabulum landmarks and proposes the application in a new patient population. As such, the work is making a contribution to the field, but does not have a unique novelty.	This seems to be a direct application of known method (RFRV-CLM). The novelty is The authors do not describe any of the models' details, model training etc. The methods section focusses largely on experimental design. As the authors self report there is a large spread in their ground truth annotations. No comparison to other methods is provided.	Lack of methodological novelty The evaluation is not based on appropriate metrics and results are not fairly interpreted and discussed. It is impossible to reproduce this work based on the methodological details provided in this paper.	Methods are well described and/or referenced and can be reproduced. Results are not reproducible since the dataset is not publicly available.	To the authors' own admission the dataset is lacking examples of cerebral palsy although the the RMP measure, for which SOTA is indicated from the results, is an important measure in cerebral palsy monitoring. As the approach applies a known method, the code is not provided (not applicable)	There is no detail about the random-forest hyperparameters as well as visual descriptors chosen. Therefore this method is barely impossible to reproduce.	"The manuscript is very well written and easy to read and follow. I just have a few comments: I would like to bring the following publication to your attention: Xu W, Shu L., Huang C, et al. A Deep Learning Aided Diagnostic System in Assessing Developmental Dysplasia of the Hip in Pediatric Pelvic Radiographs. Frontiers in Pediatrics, 8 March 2022, Volume 9, Article 785480, doi: 10.3389/fped.2021.785480.  The authors of this paper published results of a deep-learning approach to classify Hip Dysplasia, based on radiographic parameters, including AcI. For AcI, the intraclass consistency for the automatic generated and the manual measured value was >0.75.  However, in all fairness, with a publication date of March 2022, I would not considers this a ""missed reference"" :-) However, since it seem to be relevant to your work, I'm adding it to my comments. The calculation for AiC relies heavily on a small subset of acetabulum landmarks (9,39,5,8). I therefore think it would be meaningful to add the point-to-point errors for just these selected landmarks to the evaluation of landmark detection accuracy. Can you provide an average (or approximated) runtime for the proposed method?"	Provide some clarification on which data was used for the initial model training. Is it the data from the 50 randomly selected images? If not, it is not exactly clear what these 50 were used for? Some clarification on which models were built from which data in the main text would be useful. No comparison to other methods is provided.	"The notion of replication dataset is not clear to the reviewer in this context. It looks more like a hold-out test dataset. The level of expertise of operators that annotated dataR is not described. As on this dataset, there is only moderate agreement between operators, this should be discussed somewhere in the paper. While ICC provide interesting information, reproducibility of a measurement method should be quantified with a confidence interval of reproducibility (and repetability) when possible. See for instance ISO 5725-2:2019. There is no detail about the random forests' hyperparameters and features. The authors should be more specific about their models. It is not clear to me what is actually the ground truth for AcI and RMP in dataI and dataR. Is it the average of manual annotations or the annotations of just one chosen operator ? there is no evidence in the paper that the manual annotations provided to the random forest lead to robust estimates of AcI and RMP. The values of ICC give here misleading information about the real performance of the algorithm in terms of agreement with manual placement. Figure 4 illustrates that there many patients for which the difference between mean manual and automatic placement is over 10deg for AcI for instance. It can also be seen that ""moderate agreement"" in dataR is in fact associated with a large confidence interval of reproducibility.  Finally, it can also be seen in these graphs that for similar mean manual values, the automatic algorithm can provide variable results. These points should be fairly discussed. The authors claim SOTA results on RMP but this claim is not supported by proofs in the text."	It is a well written paper with a strong evaluation, but only moderate novelty.	Although the clinical significance for the work is high, and the results of the proposed approach are compelling, this paper seems to be a direct application of known method (RFRV-CLM) limiting the innovation Furthermore, the authors do not describe any of the models' details, model training etc. The methods section focusses largely on experimental design.	While such a method could have a significant clinical impact, it is not fairly evaluated and discussed. Even if the model is not novel, more methodological details should be provided to the reader.
064-Paper1731	BabyNet: Residual Transformer Module for Birth Weight Prediction on Fetal Ultrasound Video	The authors propose an end-to-end method which called BabyNet for birth weight estimation based directly on fetal ultrasound video scans. The authors design a novel residual transformer module by adding temporal position encoding.	The paper proposes a hybrid neural network called BabyNet, for automatically predicting fetal birth weight based on fetal ultrasound video scans.	The paper describes a method to estimate birth weight of fetus from ultrasound scans performed one day prior to delivery. The architecture (BabyNet) includes a residual transformer module in a 3D Resnet-based network (hybrid CNN and transformer) to analyze ultrasound videos. It is evaluated using 225 fetal ultrasound videos from 75 patients. It is compared with state-of-the-art methods ad human experts.	An end-to-end birth weight estimation method based directly on fetal ultrasound video scans. The BabyNet is trained and validated with data acquired one day prior to delivery. The experiment results are shown to be competitive compared to SOTA on 225 2D fetal scans from 75 pregnant women.	BabyNet efficiently bridges CNNs and transformers for end-to-end estimations of fetal weights from US videos. It avoids the high computational complexity of pure transformers and also allows local and global feature learning.	The paper addresses an interesting problem to estimate birth weight of the fetus from ultrasound videos. Experimental results show that the proposed method outperforms state-of-the-art methods and is comparable human experts. Combining human and AI outperforms individual methods. The paper is well-written and organized.	The network is not innovative enough. The proposed network only extends the temporal encoding based on the BoT. Some details about the dataset and experiment is ignored (see detailed and constructive comments).	Descriptions about the data processing mode of the BabyNet lack detail.	"The clinical significance of the paper is unclear, and further explanation of how it could be useful in clinical practice could be included. The authors state that ""Accurate prediction of FBW is critical in determining the best method of delivery (natural or Cesarean)."" however this statement needs appropriate citations and more detail, e.g., Is this decision solely based on fetal birth weight or other factors are also considered? Is this widely adopted in practice? When is the decision made (the paper evaluates videos only from 1 day before delivery)? The paper does not discuss the interpretability aspect of the proposed machine learning model. Quantitatively the results look promising compared to existing architectures, however, it is not clear what makes this architecture unique and suitable for the given problem. Do the authors analyze how the model makes the decision qualitatively? Which visual features does this model learn to make the accurate estimation? Where were the highest and lowest attended parts of the video for the MHSA layers? The method was evaluated on a dataset on 225 videos. In general, vision transformers require large-scale datasets for training and suffer from overfitting problems on smaller datasets. The paper does not mention this aspect and how was it overcome (e.g., regularization and augmentation). Future directions of the work are missing in the paper."	The code is publicly available, but the dataset is not.	Authors have provided their source code.	The authors have provided source code at anonymized Github repository. They state that the dataset and pre-trained models will be made available after acceptance.	"The network is not innovative enough. The proposed network only extends the temporal encoding based on the BoT. What is the sweep mode of the fetal video used for trained and validated? Is linear or sector scan? The mean frame number of scan videos is 852, while the temporal sequences input to BabyNet have only 16 frames. Would better performance be obtained if the temporal sequences are longer? The authors describe in the Discussion section that the way to combine BabyNet with clinicians results is to take an average, which is best described in the Experiments section. In the penultimate paragraph on page 6, ""Clinicians (this work) BabyNet"" should be ""Clinicians (this work) & BabyNet""."	Section 2.3 Images should be uniformly partitioned into patches (tokens) before they are input into transformer layers. What is the patch size in the proposed RTM architecture? Section 2 What is the loss function adopted in the proposed method? Section 3, Implementation Details According to authors, acquired ultrasound images size 960x720 or 852x1136 pixels. Input video frames size 64x64 in heightxwidth. If I understand it correctly, lots of spatial information would be lost during the image resize process? Section 3, Implementation Details The input sequence to the BabyNet sizes 16 frames in the length. The number of frames in a US video is about 852 frames. Do the sequences from the same video overlap? Figure 1 It seems that one weight number is predicted for a sequence with 16 frames. A US video has about 852 frames. Then, does one US video scan correspond to multiple predicted weight numbers?	The authors may provide more details and citations about the clinical significance of the proposed work. Is this decision of natural/cesarian delivery solely based on fetal birth weight or other factors are also considered? Is this widely adopted in practice? Is this decision usually 1 day before birth i.e., from when these videos are analyzed? The model is trained using 225 videos. In general, vision transformers are trained using large-scale datasets and have shown to suffer from overfitting with smaller datasets. Was any overfitting observed as the data seems to be small-scale. How was the model regularized in this case? How was the network or its components initialized? Length of the video or frame rate could be provided (currently only total number of frames are provided) What is the computational complexity of the different compared models? This could be additionally mentioned in Table 2. It is not clear how clinicians obtain their estimates of the fetal weights. Do they use only the ultrasound video or additional patient metadata?	An end-to-end birth weight estimation method is proposed and the proposed method is trained and validated with data acquired one day prior to delivery. The experimental results of the proposed method is superior to the SOTA methods, but lacked innovation in the methodology.	BabyNet efficiently bridges CNNs and transformers for end-to-end estimations of fetal weights directly from US videos.	The paper addresses an interesting problem to estimate fetal birth weight from ultrasound videos. A novel architecture combining CNNs and vision transformer module is proposed. However, some points still remain unclear, e.g. interpretability of proposed models, overfitting on smaller datasets, and potential to use the method in clinical practice
065-Paper0280	Bayesian dense inverse searching algorithm for real-time stereo matching in minimally invasive surgery	This paper describes an approach to stereo reconstruction of surgical video building on Bayesian searching of correspondences as an extension to dense inverse searching. The approach is tested on synthetic and clincal data set and evaluated with respect to computing requirements and performance.  The evaluation is done quantitativelywith respect to a diffuse and non-Lambertian ilumination and qualitatively wrt the achieved 3D reconstructions.	This is paper introduces a stereo matching approach for minimally invasive surgical videos. The proposed approach includes a Bayesian Dense Inverse Searching method and a Spatial Gaussian Mixture Model to deal with textureless or no-Lambertian surfaces. The proposed approach runs fast in run-time and has been evaluated both synthetic and in-vivo dataset. The comparisons to state-of-the-art are also provided and results have shown the approach archives close performance to ELAS and has doubled run-time speed on processing same-sized images.	The authors tackle the problem of disparity estimation in stereoscopic pairs of surgery scene. Their intent is to have a computer efficient approach. The proposed algorithm is a modified version from standard patch-based matching. The key points of the modification are the introduction of a Bayesian computation to estimate posterior probability and to associate a confidence to the pixels.	A very nice paper, nicely developed and presented methods, well evaluated and discussed.	The technical novelty of this paper is adequate. Combining Bayesian model into Dense Inverse Searching provides the confidence of identifying textureless and non-Lambertian surfaces, and this is particularly useful for dealing with specular highlights in surgical videos. The proposed approach presents good performance on both synthetic and in vivo datasets and has been compared to both classic feature matching and deep learning approaches. It is worth noting that the approach runs in real-time 25Hz on 360x288 images. Given current trend of using deep learning for depth estimation, this reviewer is delighted to see the practical approach proposed in this paper. Although DL-based approaches present outstanding performance in MIS data, these approaches however have not yet addressed their generalizability issues. Researchers are currently experiencing poor performance when using the trained networks to process unseen data.	The main strength of the paper is a rather complete experimental work (synthetic and real data) and comparison with different approaches.	-	Quantitative evaluation was not performed on either  in vivo or ex vivo data.	In the result, one may question the interest of including the DNN approaches. As underlined by the authors, these methods have a sensitivity to the training. From a quantitative point of view, the numbers demonstrate an improvement which is quite modest. The qualitative images are interesting, but it is difficult to assess if the improvement is significant for a clinical user.	The results are reproducible.	The authors have provided their code of the implementation.	The reproducibility is standard. The authors offer to provide all useful material.	accept as is	"This paper is overall written well. The flow of the paper is easy for readers to follow. I have minor comments listed as below:  The second equation on Page 5 is missing a label. In addition, in this equation, there might be a typo in the second part:   .   ^2_F instead of   .   ^2_2.  In the first paragraph of Section 3 /gamma was set to two different values for two different sizes. Can the authors provide more details how this value is chosen? There are regions in Fig 4 being highlighted in red circles, however it is not clear in the text why these regions are highlighted and what particular points the authors would like to discuss. I would recommend the authors to include a brief description of this in the figure's caption. Currently, quantitatively results are only provided on synthetic data. Although qualitative in vivo results have been provided, it is not clear what the actual accuracy numbers are. Please have a look at the SACRED ""Stereo Correspondence and Reconstruction of Endoscopic Data Challenge"" dataset where ground truth data are available."	"Few comments to specific points: In eq 2, the computed weights for the fusion of the different patches may include a division by zero, in case of perfect egality between the left and right intensity. This point is not mentioned in the submitted article. ""We compared ELAS, BDIS, DIS, and SGBM on the in-vivo data sets. Since no ground truth is provided, DNN-based methods cannot be implemented."" -> do not understand since it is just about doing inference on these data and not training?"	The overall impression, presentation and completeness of the evaluation.	This paper presents a nice framework for stereo matching in surgical videos. The presented approach has adequate technical novelty that includes a novel Bayesian Dense Inverse Searching and a spatial Gausian Mixture Model. The validation is well conducted and results have shown their approach provides competitive performance to ELAS whilst retaining real-time processing speed.	The proposed methods are classic but the authors made a true experimental work.
066-Paper2505	Bayesian Pseudo Labels: Expectation Maximization for Robust and Efficient Semi-Supervised Segmentation	In this paper, authors present semi supervised segmentation method based on psudo labels of the unlabelled images. The authors present the  approach where the model and psudo labels are updated iteratively where in E step the psudo labels are generated and in M step the model is updated using both label and psudo label. Dice loss used by authors need thresholding of psuolabels, so author variational inference based approach to compute the the threshold T.	This work presents a new formulation of pseudo-labelling as an Expecation-Maximization (EM) algorithm for clear statistical interpretation.	The paper proposes a novel semi-supervised segmentation method using pseudo-labels with an extension to variational inference. The method jointly trains a neural network on few labeled images and more unlabeled images in a two step approach. The authors link pseudo-labeling to the expectation-maximization framework. The extended method uses variational Bayesian inference to also estimate the label threshold used for generating pseudo-labels.	The idea if optimizing the threshold parameter during training is really interesting.  Since the threshold parameter is the output of the network, with this method, different pseudo labels can have different threshold value to be used in the next iteration. This also eliminates the hurdle of manual optimization of the hyper parameter, This approach can be used for computing the segmentation uncertainty  and is also shown to be robust against adversarial attack. The experiments and results are extensive.	This work proposed a new formulation of pseudo-labelling as an Expecation-Maximization (EM) algorithm for clear statistical interpretation. And the authors further introduces a probabilistic extension of SegPL using variational inference, which learns a dynamic threshold during the training. The two formulations are novelty and may provide some new insights in the future.	The paper is well written, the method is novel and it is empirically shown that the method is effective. The link between EM and pseudo-labeling is interesting. The method is compared to different consistency-based baselines and yields better results. Moreover, SegPL is robust towards distribution-shift and adversarial attacks. The paper addresses an important problem, as expert annotations are especially costly in medical imaging.	Authors have not mentioned how many iteration of EM is performed in the experiment. While SegPL significantly outperforms existing semi supervised approach, the SegPL-IV method gave only minor improvements over SegPL.	The idea is novel and interesting. But the some descriptions are not precise enough. In addition, the experiments datasets are too small, the experimental results are different from many previous works (the reported basline and comparison results are too bad). (1) To best our knowledge, there are too many pseudo label based semi-supervise segmentation methods, this work is not the first one!!! (2) The two datasets are too small, why not use the whole BraTS dataset?And the results of BraTS is too bad, it's not convincing and reasonable. The CARVE dataset just has 10 volumes, why not use a big dataset? There are too many large-scale and open-access datasets. (3) If possible, please provide the results of distance based metric, like HD or ASD. The statistical analysis results also should be reported. (4) The methods missed too many recent works about the same topic[1,2,3,4,5, ......]. [1] Semi-supervised learning for network-based cardiac MR image segmentation, In MICCAI2017. [2] Semi-Supervised Segmentation of Radiation-Induced Pulmonary Fibrosis from Lung CT Scans with Multi-Scale Guided Dense Attention, in TMI2021. [3] Efficient Semi-supervised Gross Target Volume of Nasopharyngeal Carcinoma Segmentation via Uncertainty Rectified Pyramid Consistency, in MICCAI2021. [4] Semi-supervised Left Atrium Segmentation with Mutual Consistency Training, in MICCAI2021. [5] Semi-supervised Medical Image Segmentation through Dual-task Consistency, in AAAI2021.	No major weaknesses. I only have some minor comments (see below).	Authors have provided necessary information to be able to reproduce this work.	Maybe.	The method is described well enough to be reproduced. The paper uses publicly available datasets and the code will be published.	Please mention  the number of EM iterations used and how the average value of T changes over the EM iterations? In Equation 8, the number 50 seem to be typo. Do you mean 0.5?	See the weakness comments.	Minor comments: If T should be between 0 and 1, why not use a Beta prior and/or Beta likelihood? Using a Normal distribution as variational distribution can lead to values outside [0, 1]. How do you deal with invalid values? I appreciate the Bland-Altman plot. However, I did not know that statistical significance can be derived from it. Please elaborate on that or use a proper statistical test and report p-values. Tab. 1 and 2: I assume that the values are mean +- std. Please state that. Uncertainty estimation with SegPL-VI: As far as I understand, only the estimation of T is implemented in a probabilistic/variational manner. I find it quite a stretch to argue that the deterministic segmentation network can produce reasonable stochastic segmentations with that. The problem with Brier score as calibration metric is that it heavily depends on model accuracy. I think that a proper calibration metric such as (classwise) ECE or adaptive calibration error would be better suited.	The idea of threshold optimization using variational inference for pseudo label based semi-supervised training is novel. This method can also be used to compute segmentation uncertainty.	This work proposed a new formulation of pseudo-labelling as an Expecation-Maximization (EM) algorithm for clear statistical interpretation. And the authors further introduces a probabilistic extension of SegPL using variational inference, which learns a dynamic threshold during the training. The two formulations are novelty and may provide some new insights in the future.	This is the best paper in my batch and I really enjoyed reading it. The method is novel and well described, and the results are good. It will be a nice contribution to MICCAI.
067-Paper1462	Benchmarking the Robustness of Deep Neural Networks to Common Corruptions in Digital Pathology	This paper propose to synthetically generate corruption to pathology images. Nine type of corruption was considered at five severity levels each. Ten CNNs are trained and their performance are tested on the regular validation set, the corrupted validation set, and a held-out test set. The authors found that 1) the corrupted data leads to higher error rate 2) the model confidence increase with level of corruption severity 3) different corruption affected the models differently 4) early stopping helps with robustness 5) error corrupted validation set is more predictive of generalizability of the model.	This paper presents two new benchmarks to measure evaluate how deep neural networks perform on corrupted pathology images. Specifically, corrupted images are generated by injecting nine types of common corruptions into validation images. Two classification and one ranking metrics are designed to evaluate the prediction and confidence performance under corruptions. Furthermore, this paper validates the poor robustness of modern CNNs towards input corruptions.	The paper builds an ensemble of corruption methods to be used as a standard suite for evaluating model robustness in histopathology. It evaluates the methods on a pretty extensive list of standard architectures, including the more modern vision transformers.	The experimentation is extensive, and the conclusion from the experiments are mostly useful. It's interesting to see that the error on the corrupted validation set is a better prediction of generalization. This suggests that the corruption being propose gets the image closer to real test set. The paper is relatively easy to follow.	This paper explores an important topic, robustness problem of CNNs, which is significant in the real-world deployment; The design of corruption types are close to reality; The experimental results shows poor robustness of modern CNNs is valuable. Another interesting phenomenon is that DNNs have been constantly improved over the past decade, but their classification performance on corrupted pathology images is slightly changed.	The idea of the paper is intriguing. It is well-known that model confidence is not a good predictor of probability, especially given out-of-distribution pertubations. I like the idea to run a standard suite of tests on models to evaluate which of them are more robust and/or show reliable confidence estimates.	Crucial detail of how the corruptions are implemented is missing and not available even in the supplement. No promise of code or dataset release is made. This has strong negative impact on reproducibility. This also make it hard for us to judge the true quality of the corruption. While the experimental results seem to suggest that the corruption is working as intended, there could be some unexpected deviation from reality (for e.g., not enough marking included, or deviation from non-random nature of pathologist marking). Releasing code and having more details about the method will improve the strength of this paper considerably. I find each experiment to not be thorough enough. While high level conclusion drawn are interesting, there are many questions left unanswered, leaving that high-level conclusion rather weak. For example, I find correlation to be a weak metric to show that if a model is more robust to corruption, it is more generalizable. It could just be that the corruption creates more out-of-distribution samples, so it happens that the errors on two sets are correlated. I think more investigation is needed. One thing that comes to mind is to train with corrupted images and see if generalization actually improve. In my opinion, this would be a stronger evidence that the proposed corruption is working as intended. Another example is the reverse relationship between severity and the model confidence. Isn't this expected result? As the corruption push the image outside the distribution, the model is likely to make higher confidence negative prediction in this case? There are some discussion, that I think would be better left in supplementary to leave room for more important points (such as one discussed above). In particular, the exact formula for the metrics could be left largely in the supplement. The basic idea is relatively straighforward and do not need to be explained in depth in the main paper, in my opinion.	Authors claim that the proposed corruptions are easy-to-use in practical settings since they are implemented by being plugged into the dataloader class.  Although Sec.3.1 describes how it implements in brief, it is still unclear how it implement and why it is easy-to-use. The minor mistakes (e.g., the label of y-axis in Fig. 3.) should be corrected.	"I don't understand why a local dataset could serve as a benchmark, given that it was not published. As known from domain shift investigations, model robustness towards a covariant shift in the input is highly dependant on training runs. Single training runs, as done in the reported experiments, are thus very likely to not really be conclusive for such investigations. The authors write that the AlexNet scores the best in terms of their rCE metric. However, looking at Table 1 I would only infer that the CE metric is similar across all model architectures, so given the formula for the rCE metric it is natural that the worst performing model on clean data achieves the best value here. This, however, does not imply (as the authors motivate) that this metric is a good proxy for model robustness. I also don't agree with the notion ,,although CNNs are constantly improved in the past decade, their performance on corrupted images changes little while causing the incredibly worse robustness"": Some of these corruptions deteriorate the image severely, and a reduced metric might be highly correlated with a reduced performance by human experts as well. Thus, this is not a sign of weak robustness, but might be related to just information destroyed in the image. Robustness is only within the limits of information being still contained within the image. If the corruption scheme destroys diagnostic information within the image, a reduced performance is to be expected and cannot be attributed to a lack of model robustness. The authors state that one of their findings is that overfitting harms the robustness of the models. But that's actually the very definition of overfitting. The paper structure is also a bit unclear. Parts of the experimental results are already reported in the introduction. Further, the authors did not discuss the limitations of their approach in any way."	Poor. No detail on implementation anywhere, and no code to be published.	This paper seems to be simple to realized but still be a bit unclear.	No code is given (although the authors state this in the questionaire). The authors relate to one of the datasets as a possible benchmark, yet it is not available and no link is provided. It will thus be hard to reproduce the results.	See my weakness section.	I suggest that authors publish the code. I suggest authors further investigate whether the existing robust studies work on the proposed benchmarks.	The biggest weakness IMHO is that the authors compare single shot trainings of various architectures, that are hardly comparable. If the authors want to evaluate the robustness of an architecture, then they should IMHO use several training runs of one architecture (and please also report the distribution). I would recommend to include a subjective evaluation. If the relevant information in the input image is destroyed, a model deterioration can not be called as missing robustness. Hence, this is only possible if a human expert can still retrieve information from that. I think the authors should try to limit the pertubations to effects that only effect model robustness and not general (e.g. by experienced experts) recognition. I would also question the sense of the rCE metric in that sense. If all models have the same (mediocre) results after the corruption of input images, it is not really informative to set that into relation against the original performance. The metric that I liked the most was the CEC metric, as it tackles the model confidence. It would be interesting to compare that against a standard metric such as rank correlation.	Overall, I think this is still an interesting paper despite all the weaknesses listed.	This study study an significant problem when the model is deployed. The plenty of experiments is convincing.	While the general idea of the paper is good, I think that the methodology is not sufficient to support the claims. It is not surprising that corrupting images severely leads to a reduction in recognition, and that can't be attributed to model robustness issues. I think that model robustness amidst realistic pertubations is an issue, yet the methodology is insufficient to investigate this, as also reflected by the highly variant results in the paper.
068-Paper2856	BERTHop: An Effective Vision-and-Language Model for Chest X-ray Disease Diagnosis	The authors propose a transformer-based vision & language (V&L) model based on a recent image feature learning method named, PixelHop++ and BlueBERT which has been trained on biomedical and clinical datasets. The results show that the proposed method can better capture the associations between clinical notes and medical images, gaining higher classification results.	The paper proposes a Vision and Language model for better capturing the associations between clinical notes and medical images. In particular, PixelHop++ is used to extract features from images, that are then processed with a visual transformer together with language embedding, extracted from a text report. The results demonstrate the validity of the method	The paper proposes BERTHop, a transformer-based Vision and Language model that is applied to medical images. The visual encoder of the V&L architecture for BERTHop is implemented with PixelHop++. This is unsupervised and reduces the dependencies of labeled data.	This research shows the feasibility of performing V&L analysis and disease diagnosis on small medical datasets without labels. Various comparative experimental analysis shows the effectiveness of the proposed method.	The paper is well written and easy to follow. Dataset and training procedure are well presented. The effectiveness of the choices made is demonstrated by testing different feature extractors and different transformer backbones.	BERTHop implements PixelHop++ for unsupervised visual feature learning for medical images. This helps specifically when labeled data is not available which makes it a perfect use case for medical tasks. The paper shows detailed disease-wise labels and compared the results with two baselines. The improvement in AUC is significant. Also, the comparison of different transformer backbones is shown. The argument for replacing BUTD as it fails to detect certain medical abnormalities is supported with convincing results in Table 1.	While the results show the effectiveness of using PixelHop++, I was not able to clearly understand why the use of PixelHop++ is improving the learning outcome. The description of PixelHop++ needs to be articulated - what do you mean by image at different frequencies? why does this improve the result?	How PCA is applied to PixelHop++ channels? The pre-processing should be explained, despite only say that is the same as TieNet Trying the method also on other datasets could confirm even more its effectiveness.	Although the results are supporting the claims of the paper, the reviewer is concerned about the generalizability of the proposed architecture on other datasets. Also, the reviewer is interested to understand the change in results when the degree of unlabelled data is changed. Implying what will be the disease-wise AUC when 10%, 20%, 30%, and so on amount of the data is labeled. This is because a mixture of labeled and unlabelled is common in medical datasets. Some areas of the paper were gramatically difficult to be followed.	It looks it is possible to reproduce of the proposed method as it is based on the combination of existing methods.	The dataset is public and the hyper-parameters have been listed.	The authors did not declare whether code will be made public. The individual model components are publicly available making the proposed architecture reproducible. The dataset used is publicly available.	While the results show the effectiveness of using PixelHop++, I was not able to clearly understand why the use of PixelHop++ is improving the learning outcome. The description of PixelHop++ needs to be articulated - what do you mean by image at different frequencies? why does this improve the result? The writing of paper needs to be improved; there are many grammatical errors and repetitions throughout	Some aspect could be better explained	The simplicity of the architecture cannot be countered with a lack of novelty perspective. Also, detailed comparisons are reported with different backbones.  Please clarify whether the method is an extrapolation of the concept of visual Q/A, and if the authors have tested it previously on non-medical datasets.	The paper addresses an important research topic. The experiment presented in the paper is good but the description/contribution of proposed method is not well articulated.	The paper is well written and easy to follow, and a good evaluation of the architecture has been conducted.	The proposed architecture is simple and intuitive. The results are convincing and strongly support the arguments presented.
069-Paper1459	Bi-directional Encoding for Explicit Centerline Segmentation by Fully-Convolutional Networks	A bi-directional endocing scheme without autoregressive blocks is proposed for various shapes and orientations of lines.	This paper proposed a encoding schema for representing/segmenting tube-shaped objects in 2D medical images. The representation can be directly infered by a neural network.	"The authors address the problem of centerline segmentation in 2-D images and propose a bi-directional point-based centerline encoding as target for a neural network (HRNet) which circumvents an ""implicit"" pixel-based segmentation. The authors evaluate their approach on three datasets (synthetic, semi-synthetic, CLIP) and show improvements compared to recently published segmentation-based methods for most investigated centerline types."	The centerline based n-connected points to cover the segmentation seems interesting.	Although there are already many works that also use connections of key points for different tasks (e.g. CurveNet for point cloud processing, deepsnake for semantic segmentation), the proposed centerline encoding in this work is somewhat novel. Also, the bi-directional design is also interesting and sound. The emprical experiments and comparisons seem satisfactory. Qualitative results are also meaningful.	The authors present an interesting encoding approach to improve centerline segmentation which utilizes a point-based encoding of the centerline. The authors provide results on three different data sets (synthetic, semi-synthetic, CLIP) The authors compare their results to multiple recent methods for centerline segmentation.	Limited Novelty, such heatmap based methods are widely used in landmark detection/pose estimation tasks [11]. The author applied it to such a specific task. Inefficient evaluation. The author verified their methods on synthetic data. Why not evaluate it on a real-dataset? overclaimed. The author argues that segmentation-based methods can generate some false positive pixels. However, the proposed method can also generate an incorrect centre line via the wrong keypoint locations.	In page 3, the authors mentioned that 'p_i and p_{i+1} connect with an edge'. Does this denote a direct connection with a straight line? Or the edge can be a curve depends on other characteristics (if so, how?)? The 'edges' in Fig.1 prediction look like curves to me. In page 3, the authors mentioned that non-end points are sampled from the centerline. I though those key points directly predicted by the HRNet? How is this sampling related to the predicted n key points? In page 3, the authors mentioned that 'we fix n to be identical for all tubes regardless of their shape and length'. If so, the problem is essentially a signal sampling problem, where the authors use the HRNet trying to capture the most significant waves. Therefore, for smaller n, there will be an under sampling problem. For larger n, it will become oversampling. Although the authors provided an ablation study w.r.t n (Table 2), the evaluation was bound by the specific dataset and specific tube geometry and hence is not generic. This problem is actually an limitation of the proposed method, the authors should at least have some discussions on this. Also, the width of the centerline can be another important factor for the performance. Apparantely, assumptions on all tube-shaped objects are in the same width is invalid. It would be better if the authors can explain their choice on width selection and even better if an ablation study can be provided.	From my understanding, the paper's main contribution is the formulation of the bi-directional centerline encoding. The evaluation, however, does not focus on showing a benefit of this encoding. Specifically, different architectures are used for the segmentation tasks and the proposed method. Is is therefore not possible to judge whether improvements are mainly due to a better suited architecture ( or due to proposed encoding. This could have been easily avoided since most (all) of the reference methods could also work with the proposed encoding as targets. It is not clear how / whether the hyper-parameters used for the different methods were tuned. I would expect that especially aspects like learning rate, #epochs, etc. may be different across architectures. The focus and the presentation of the results is not ideal from my perspective. Important results (standard deviation, quantitative values) are only presented in the supplementary material, an analysis of failure cases (or non-optimal results) is missing.	The implementation code is unavailable and  some part of implementation are unclear.	Reproducible.	According to the reproducibility checklist, the authors do not plan to provide their training or evaluation code. The method itself is fairly straightforward and well described, and a re-implementation should be possible. Most hyperparameters used for training the models are provided in the supplementary material, however, only the CLIP data set is available for reproducing results.	Typos:  'we propose a different data structure for for the efficient segmentation of tube-shaped objects' -> 'for for'	Please add more discussions (or ablation studies) and fix the unclearness as stated in the weakness section above.	"General comment: I have to admit that I have a hard time understanding why the methods works with apparently very good performance. The network is required to obtain a substantial understanding of the global structure of the target object (with partially very different sizes and shapes) to be able to place landmarks equidistantly on the target image. I would have liked to see a more detailed discussion and potentially analysis on this aspect, e.g., is this dependent on the receptive field of the network?, is the HRNet an essential ingredient for this task? what happens to the predictions of a structure if an image is cropped or stretched? If I am overlooking something and this is a rather straightforward insight, I am happy to stand corrected. The description of the method can be improved: It is a bit tricky to say that no post processing is required when there is an obvious step to get from the heatmaps to the line. I would encourage the authors to diffuse such sentences. From my perspective, the description of the ground truth heatmap should be part of the method description (not of the encoding) - I was missing this in ""Training and Inference"" and was surprised to find this later in the text. The clarity in the method description could be improved: I was not sure what the authors mean by ""horizontal coordinates of the endpoints are closer than some threshold""? Closer to what? How were these coordinates extracted? The output of the network should still be a heatmap. Also, how are s (scaling) and t (threshold) selected? Why n=31? There is very little information on the selected architecture (see also comment above). One additional sentence that summarizes the concepts of this would help put this into context. It semi-synthetic data is described very briefly, and makes it rather difficult to assess how realistic these images are. Also, for the real and semi-synthetic data, multiple structures are segmented, however, it is not clear how this was performed. Are separate networks trained for each centerline type? Was there a multi-task setting? The evaluation and description of the results can be improved: The experiments do not reveal what role the architecture itself plays - potentially in combination with the proposed encoding scheme (see also comment above). The results on the synthetic data are - from my perspective - not particularly interesting. They are okay in supplementary material, however, I find the added value of Table 1 in the paper to be relatively limited. Figure 2 is rather hard to read/interpret (also: variance of results?). I understand that the authors want to also illustrate the ccs-aspect, but this overloads the figure from my perspective and contains little additional value (e.g., if there are a few false positive pixels, this will have limited impact on a line fitting afterwards). A bar or box chart (that includes the standard deviation) would have been more expressive from my perspective. The authors show impressive qualitative results, however, I am missing an analysis of failure cases. I am missing an analysis of cases where it didn't work. At least for some structure, a SDR of 15% indicates that there are some points missing. Also, in the real and semi-synthetic examples shown, the centerlines seem to be rather well behaved. It is not clear how such a method would behave in case of loops. Did the authors observe any outliers? Additional comments and typos: abstract: ""mainly addressed via dense segmentation"" - this is a bit simplified. There have also been regression approaches that formulate this as regression tasks in Kordon et al., MICCAI 2019, https://doi.org/10.1007/978-3-030-32226-7_69 abstract: ""agnostic in the number of points in their centerlines"" - this seems to contradict the statement that the authors propose to use a fixed number of points. This should potentially be rephrased to improve clarity. Convexity as such doesn't really play a role for pixel-based segmentation. It is true that it may be easier to design postprocessing for some convex objects, but this can be formulate more precisely in the text. p. 1: ""and with the attention mechanisms [18] to address a guidewire segmentation and tracking in endovascular aneurysm repair problem. A U-Net model with the spatial location priors was used in [10]."" - these sentences reads a bit bumpy and could potentially be improved. p. 2: ""to reconstruct [the] true shape of [the / a] device"" - expression. p. 2: ""for for"" - typo. p. 2: ""n-connected points"" - no dash, otherwise this distorts the meaning (compare n-connected graphs) p. 3: ""We use the conventional for landmark detection heatmap regression loss"" - expression. p. 4: ""fracture of points"" - typo. p. 6: ""Hausdorf"" - Hausdorff."	The novelty.	Overall, this paper proposed a novel and interesting approach for an existing problem. The intuition was justified and the experiments are good. I think this paper has met the standard of MICCAI.	It is appreciated that the authors compare their proposed method to multiple previously published methods, however, from my perspective, the experiments do not really assess the contribution proposed in the paper (e.g., the centerline encoding). Additionally, a description on how (whether) the reference methods were tuned and an analysis of failure cases are missing.
070-Paper1108	BiometryNet: Landmark-based Fetal Biometry Estimation from Standard Ultrasound Planes	The author proposed an end-to-end network BiometryNet for automatic fetal biometry estimation on the ultrasound images. It uses simple landmark annotations instead of complex mask annotations. Moreover, a dynamic orientation determination mechanism was proposed to reduce variabilities and improve landmarks' localization. The results on the two independent datasets demonstrated the effectiveness of proposed method.	This paper reports a contribution to a previously reported method for automatic detection of landmarks in medical images. The contribution allows the method for adaptation to rotating landmark pairs and was evaluated in 3 biometry tasks (biparietal diameter, occipito frontal diameter and femur legth) in two large data sets of fetal ultrasound images with accuracy errors within acceptable clinical values.	This paper introduces a landmark regression network for directly computing biometrics in US. Specifically, it modifies the original landmark class reassignment with a novel dynamic orientation determination to generalize it to multiple scenarios. This work validated the proposed method on a large dataset, and the results are sound and promising.	(1) The author proposed an end-to-end landmark regression framework BiometryNet for fetal biometry estimation, which only used simple landmark annotations for training. It reduced the time for manual labelling and high inter-and intra-operator variabilities.  (2) The Dynamic Orientation Determination mechanism was further introduced to determine measurement-wise orientation and provide consistent landmark class for various measurements.	In my opinion the original contribution is significant and the extensive validation in fetal ultrasound biometry tasks provides enough evidence to support the possible clinical application of the methods.	The idea of this paper is simple but effective. The results are strong where the proposed method outperforms other methods and ablation studies	(1) I supposed the third part of contribution should not be considered in your main contribution if the annotated dataset is non-public. (2) The author did not discuss the influence of simple landmark annotations and fine annotation on accuracy. In addition, there are differences between different obstetricians in the labeling process, and the protocol among them should be given in the paper.  (3) For different detection tasks (BPD and OFD), do you train one network separately for each task or train one network simultaneously for multi-task detection? If one network is trained for each biometry, the process of this work is complex. (4) A comparison with the advanced methods on fetal biometry estimation is missing.	Previous works on automatic landmark detection in medical images, including ultrasound are not mentioned in the introduction.	The motivation of the landmark class reassignment should be detailed.	The part of Landmark Regression Network seems to be reproducible and it is challenging to reproduce the part of Dynamic Orientation Determination. Ideally, the source code should be released.	The work is fully reproducible since public data sets were used	It is highly recommended for the authors to release their dataset and code for better reproducibility.	(1) In section 4, the description of proving the robustness of proposed approach was not detailed enough. It should be added the results of training on FC dataset and testing on HC18 dataset. (2) In section 3.1, the details of landmark annotation should be given. Moreover, the author extracts the BPD and OFD biometry from the major and minor axes of an ellipse least square. Did this approach affect the accuracy of result?  (3) The comparison with the advanced methods on fetal biometry estimation should be added. (4) The influence of simple landmark annotations and fine annotation on accuracy should be illustrated. Moreover, the protocol of annotation among obstetricians should be given in the paper. (5) The femur length prediction task is not clear in the paper and the author should declare why the femur Length image are inputted in the network.	"I just would like to suggest to the authors to revise the introduction and include previous works on automatic landmark detection in medical images, such as: Amir Alansary et al., (2019), ""Evaluating reinforcement learning agents for anatomical landmark detection"", Med. Im. Ana., vol53, pp.156-164. where was reported a 3D landmark detection method based on deep Q-network (DQN) architectures  which was  evaluated on the detection of multiple landmarks in three different medical imaging datasets: fetal head ultrasound (US), adult brain and cardiac magnetic resonance imaging (MRI)."	The authors should detail the motivation of the landmark class reassignment module as it is the main novelty of the proposed method. It is hard to follow the necessity of this module without reading the reference [3] in the original paper. Performing the experiment to demonstrate the superiority of the method can not be assigned as a contribution, please remove it. The metric curve in Fig.4 is hard to recognize. Please put the zoom-in patch of the convergence stage on it for better visualization	Due to less convincing experimental results, the reviewer thinks this paper cannot meet the standard of MICCAI and rates this paper as weak accept.	In my opinion the validation is remarkable and allows for an objective assessment of the clinical viability of the methods, which in turn seems to be very high.	Overall, this paper is good. It proposes a novel dynamic orientation determination to generalize the landmark class reassignment to multiple scenarios. It evaluated the proposed method on a large dataset. The results seem sound and promising.
071-Paper1852	BMD-GAN: Bone mineral density estimation using x-ray image decomposition into projections of bone-segmented quantitative computed tomography using hierarchical learning	This paper proposes a method to estimate BMD (Bone Mineral Density) from a plain X-Ray image. The proposed approach combines the QCT in training and decomposes an x-ray image into a projection of a bone-segmented QCT.	The paper is focused on estimating bone mineral density using plane X-ray.  The main contributions are 1) the greatly improved method for estimating BMD from x-ray that is presented, 2) the method that allows combining QCT data, and segmentations using hierarchical learning to improve performance.	I congratulate the authors for this great work. The authors proposed a novel hierarchical learning approach to predict bone mineral density (BMD) from plain radiographs which provides opportunistic screening for osteoporosis.	The main idea of the paper is to estimate BMD using QCT and plain X-ray images widely available and more accessible. Unlike previous methods on BMD estimation from X-ray images, the proposed method uses information in the training phase from QCT.	Novel methods for applied that make use of related data in a different representation.  Specifically the authors use a hierarchical learning approach, which allows them to take advantage of available quantitative CT data.  The algorithm and the training approach allow the algorithm to greatly improve performance.  The investigation presents a novel way to use the QCT data, it is used as a way to measure ground truth BMD, but also for training a GAN for image translation from x-ray to a synthetic DRR that is useful or BMD assessment. The article is well written The clinical need is well established The authors create a dataset, create a novel algorithm, investigate different training regimens and then demonstrate impressive performance.  There is lots of novel and interesting work here	The proposed hierarchical framework was novel to learn a Digital Reconstructed Radiograph (DRR) from X-rays, and then estimate BMD from the corresponding DRR generated using a generative adversarial network (GAN). Extensive validation using different backbone architecture was performed. The paper is very well written and I enjoyed reading through it.	The paper is a hard bit to follow, due to lack of details and lack of precision in the definition of the equations and variables used.  Several steps of the proposed method are not described, e.g. cropping, registration, etc. The original data used for the study are not described. Experiments were achieved on a limited dataset and were not validated on a different data.	The dataset could be better described.  How much variability in vendor, time subjects?  What is the image quality required, particularly for the X-ray?  This is an important question and it is not well dealt with.	While it is an strength for the proposed framework to be trained on a small dataset with N=200, this is a weakness when it comes to validation. I would suggest testing your method on larger datasets without paired QCT to see how this may be generalised to other studies.	No proof of reproducibility. Experiments achieved on an in-house dataset. Code should be available.	The code is made available. The data is not made available. There is an extensive note about the parameters used in training Reproducability is likely, dependent on the code released.  There are a lot of details in creating the dataset that are probably crucial to reproducing this work.  So without the dataset it may be hard to reproduce. The source of the data is not well described, what is the fidelity (resolution, contrast, reconstruction) of the underlying images?  This lack of clarity could limit reporducability The methods are well described.	Data does not sound to be available but most codes will be available upon acceptance.	"This paper proposes a method to estimate BMD (Bone Mineral Density) from a plain X-Ray image. The proposed approach combines the QCT in training and decomposes an x-ray image into a projection of a bone-segmented QCT. The paper is a hard bit to follow, due to lack of details and lack of precision in the definition of the equations and variables used.  Several steps of the proposed method are not described, e.g. cropping, registration, etc. The original data used for the study are not described. Experiments were achieved on a limited dataset and were not validated on a different data. It is unclear what GAN brings to the proposed method? The structure of the paper could also be improved by restructuring the ""Proposed Approach"" section into different sections linked to the different steps of the proposed approach. Too many variables are not defined in the different Equations. Please fix. Details are lacking about how the different models were implemented. Were they tuned favorably?"	Figure 1 - landmarks are present on some of the 3D bone surfaces, however, these landmarks are not explained. Figure 1 / dataset - It is unclear if the x-ray images and QCT are matched.  I think they are because of the comparisons of BMD derived from DXA and QCT with the synthetic BMD measurements.  However for the GAN training it would not be requirements that the x-ray and QCT be matched. o Comment on: Are all the x-rays taken from a consistent orientation?  How much variability is there in the x-ray image orientation and how will this affect the result? Can you comment on the magnitude of the errors in BMD in z-score terms, which is often how osteoporosis is reported.	-Please provided the baseline characteristics of the cohort? age, gender, mean BMD T-scores,... The precision of the method is not reported here. If you can collect repeated X-rays from the same subject for a subset of data, for example N=30, you can also compared computed BMDs and report coefficient of variation (CV) to reflect the precision of the compuatioanl framework. We should have CV<3% for practical use; the lower the better.	The proposed method is interesting The paper is not well written The paper deserves acceptation if slightly improved for the readers	Excellent paper, a useful goal, novel implimentation, improvement over SOTA, consideration of clinical need.	The paper is well presented with a novel formulation, interesting application, and comprehensive results.
072-Paper0820	Boundary-Enhanced Self-Supervised Learning for Brain Structure Segmentation	This paper proposes Boundary-Enhanced Self-Supervised Learning (BE-SSL), leveraging supervoxel segmentation and registration as two related proxy tasks. The former task enables capture boundary information by reconstructing distance transform map transformed from supervoxels. The latter task further enhances the boundary with semantics by aligning tissues and organs in registration. Experiments on CANDI and LPBA40 datasets have demonstrated that our method outperforms current SOTA methods by 0.89% and 0.47%, respectively.	This manuscript proposes two pretext tasks for self-supervised pretraining for the downstream task of brain structure segmentation, i.e., regressing unsigned distance maps defined with respect to supervoxel over-segmentation, and volumetric registration. The proposed pretext tasks are evaluated on two public datasets and demonstrate superior performance to several SOTA methods.	This paper proposes a boundary-enhanced self-supervision method that is able to learn from supervoxel segmentation and registration tasks. The supervoxel branch is refined for the main task to get the final segmentation. The experiments on CANDI and LPBA40 datasets show the efficiency of the proposed method.	The motivation is clear and convincing. How to enhance the boundary segmentation result using self-supervised learning is a promising direction in medical image analysis. The idea of employing the distance transform map (DTM) based on supervoxels to emphasize the the edges and boundaries sounds reasonable to me. Also, applying self-supervised learning to predict DTMs is novel. Learning the registration from each volume to the mean volume is an interesting way to incorporate the semantic information, where I suppose the mean volume incorporates the semantics of the whole dataset.	The proposed pretext tasks are straightforward. The pretext task of regressing unsigned distance maps defined by supervoxels seems novel.	This paper proposes a new self-supervision method that consists of supervoxel segmentation and image registration as proxy tasks to enhance boundary segmentation for the main task. The paper is well written and easy to follow.	The major weakness lies in the experiment section. I do not understand why the rest 5 baselines (3D-Rot, 3D-Jig, 3D-Cpc, PCRL, Genesis) perform worse (most of the time) than training from scratch in the fine-tuning stage, especially when the labeling proportion is 10%. Because self-supervised learning has been shown to be more effective when the amount of annotations is quite limited. The authors should conduct more analyses about why RubikCube performs better than other baselines, because the effectiveness of Genesis and PCRL have been validated on other challenging tasks.	I have three major concerns, elaborated below. One of the main claims of contributions that the registration pretext task matches and enhances boundaries concerns me. According to Eqn. (3), the authors employ voxel-wise similarity loss for the registration, which does not emphasize boundaries by definition. Several aspects affect the reproducibility of the paper. First, the hyperparameters used to generate the supervoxels are not given, nor is the impact of different hyperparameters on performance investigated. Second, it is unclear how the supervoxels of interest (and background) are identified. Third, there is inconsistency regarding the evaluation setting (train/val/test split versus five-fold cross-validation). Fourth, it is unclear how many epochs are trained for both pretraining and fine-tuning. Lastly, no code is submitted, nor do the authors promise to publish codes. The improvements upon existing SOTA seem minor.	"The main weakness of the paper is the limited technical contribution and experiment performance. The authors claim they are the first to introduce registration as a proxy task for self-supervised learning, which is true to my knowledge. However, image registration has already been used in a self-supervision manner in [1], which limits the technical contribution of the paper. In experiments, the improvement compared to other self-supervision methods is very limited (less than 1% in most cases) and no information on the significance test is provided. This makes the method less convincing on boundary segmentation. [1] Li, Hongming, and Yong Fan. ""Non-rigid image registration using self-supervised fully convolutional networks without training data."" 2018 IEEE 15th International Symposium on Biomedical Imaging (ISBI 2018). IEEE, 2018."	The proposed method is reproducible.	Reproducibility is doubtful. Please see my response to Q5.	The authors would like to make all codes publicly available, which ensures good reproducibility.	Please address the problems in the weakness section. Overall, this is a good paper with promising technical novelty.	"Eqn. (1): what is the ""inf""? Table 1 and Table 2: (1) how do you obtain the numbers for methods in comparison? (2) please add a space between ""Dice"" and ""(%)"", and (3) ""3d"" -> ""3D"" Page 7: ""..., for CANDI and LPB40, Suggesting that ..."" -> ""... , suggesting that ..."" Page 7: ""..., one enhancing the fundamental boundaries and the other enhancing the semantic boundaries."" Please differentiate the fundamental and semantic boundaries. Ref. [12]: ""cnns"" -> ""CNNs""; please also check other references for similar cases."	*Why not use one decoder and two output channels to train the two proxy tasks? Is there any technical limitation to doing that? I think this can save model capacity and learn more fused task-relevant features. *Significance test should be conducted for the results. *Report the final convergence points in Figure 4 and 5. They seem quite close to each other in later epochs. *Highlight the boundary improvements if there are any in Figure 6, such as over-segmentation and under-segmentation. *Add necessary citations that use supervoxel and registration in self-supervision. The literature review is not complete enough.	The motivation is clear and intuitive. The proposed method sounds reasonable to me. The experimental results are satisfactory.	The idea of regressing distance maps defined by supervoxels is somewhat interesting. However, considering the major weaknesses mentioned in Q5, I rate it weak reject.	The limited technical contribution and performance are the main weaknesses of the paper. The method should be further refined to enhance boundary segmentation and more solid results should be provided.
073-Paper0016	BoxPolyp: Boost Generalized Polyp Segmentation using Extra Coarse Bounding Box Annotations	The authors present a polyp segmentation framework. Their main goal is to achieve accurate polyp masks leveraging datasets with mainly polyp box annotations and few pixel level annotations. In particular, the paper proposes: Fusion filter sampling (FFS) as a preprocessing module to (i) convert box annotations into pixel level annotations,  (ii) exclude difficult/wrong training samples, and (iii) ignore uncertain regions of the image during training. A pretrained model on a segmentation dataset is needed to perform this task. Mixture of Annotated and Pseudo labels (MAP): it's a variation of Cutmix where regions of polyps with pixel level segmentations are pasted onto images with pseudo labels (arising from FFS). This is used to (i) suppress the negative effects caused by the errors in pseudo labels and (ii) to upsample the fully annotated polyps. Inter-image consistency (IIC) loss	This paper presented a polyp segmentation method, which leverages the cheap bounding box annotations to alleviate data shortage for a polyp segmentation task. The authors presented fusion filtering sample, mixture of annotations and pseudo, and Inter-image consistency loss to boost a generalized polyp segmentation model through extra bounding box annotations.	The authors propose to use leverage bounding box annotations for polyp segmentation task model. For Fusion filter sampling aimed at generating pseudo labels the authors used pertained SANet (previously proposed, [22]) to generate the coarse segmentation map prediction and compare with bounding boxes. They then use a similar technique to cut mix where they mix the patches with pseudo label on random images with true label. Inter-image consistency loss between different view predictions is also proposed. Authors did show some improvement over previous methods. An open question would be does these improvements comes from the use of large LDPolypVideo dataset for their coarse prediction or technique implemented that mimics more of a data augmentation techniques e.g. cut mix and loss function comparing different views. Also, is the preciseness of few more percent improvement clinically relevant.	"The amount of data present in recent datasets (such as LDPolypVideo) is needed for robust translatability of polyp detection models, but has the problem that the labels are ""softer"" and contain noise. The paper focuses achieving state-of-the-art results (usually obtained on smaller, curated datasets) on these larger and noisier datasets. The authors employ well thought techniques that are easy to implement and can be applied to a variety of situations. The experiments show the benefits of the proposed improvements in a consistent, detailed and thorough way. The methods are implemented using open datasets. Particularly, the results are evaluated on 5 available datasets showing increased results when compared to the counterpart networks on all of them. The proposed architecture is compared to 9 state-of-the-art networks, and implemented on top of 2 of them. The authors provide additional ablation studies on 2 datasets. Qualitative examples are also provided, showing the benefits of their proposals."	The paper is well-written and organzied. The experimental results and analysis are sufficient.	A simple technique to leverage the dataset with bounding boxes The proposition is well ablated	Why are accurate pixel-level segmentations needed clinically? Even if a polyp is detected perfectly, the resection techniques are very coarse. In my opinion, it is important to find polys, but an exact boundary prediction is not necessary clinicaly.  Perhaps it would be good to put the clinical context in the paper to balance the technical contribution - e.g. could this be needed in an autonomous robot setting, etc? The models were only trained and tested on polyp frames, so specificity has not been evaluated. A pretrained model on a segmentation dataset is needed to perform FFS.	The paper presented several strategies to improve the polyp segmentation performance through make full use of the pixel-wise annotations and extra bounding box annotations. However, the connection and relationships among the main components in the proposed method are not clear. Additionally, some important details about the experiments are missing.	The paper lacks clarity and motivation of the paper is not clear. Please see constructive comments for details. The paper uses previously proposed techniques in their model and propositions Math symbols and equations needs to be checked. For e.g., images and labels needs to be a vector/matrix of dimension d and hence represented either in bold or capital Question remains, is the method generalisable to other unseen datasets?	Some clarification needed.	I believe that the obtained results can be reproduced.	Authors have checked yes for the reproducibility of paper.	"Please, clearly say what data was used for training (particularly in section 4.1). I assume the models were trained with LDPolypVideo but this is not clearly stated.  Aditionally, a dataset with segmentation masks is needed for the ""pipeline with masks annotations"". What data is used for this? Please state if the baselines (from other methods) were retrained by the authors and if the same data was used. If so, please also state if randomness was accounted for (the same seed was used so the same augmentations, and data loading order was kept) Figure 4 is unclear when the authors refer to ""Dice values of the above models under different thresholds."" What threshold is modified? Is this showing the dice score when using different thresholds over the predicted maps? If so, it's interesting that the Dice score tends to increase with higher thresholds. What threshold was selected for the results shown in Table 1? In 3.1. ""Meanwhile, a pre-trained SANet [22] model (trained on small segmentation dataset) is applied to get a coarse prediction P for I."", please explain how the  pretrained model was pretrained or obtained. What data was used, was there any data contamination with the LDPolypVideo dataset, etc. Minor: ""In particular, the widely adopted training set [8, 22] contains only 1,451 images"" (please mention dataset names for clarity) In the introduction, the authors fail to mention other segmentation methods that use coarse polyp boxes uniquely as ground truth [refs] In the first paragraph of related work, it sounds like UNet architectures are a subtype of FCN networks. This can lead to misunderstandings, as Fully Convolutional Networks are a segmentation architecture separate from UNet. Typos: ""a generalized polyp segmentation model is urgently needed"" -> a ""generalizable"" But box annotations in LDPolypVideo exist two -> But box annotations in LDPolypVideo have two There might be more"	The connection and relationships among the main components (i.e., fusion filtering sample, mixture of annotations and pseudo, and Inter-image consistency loss) in the proposed method should be elaborated. More details of the comparison algorithms should be added. For example, What type of the annotations were used in the network training? And what's the number of the pixel-wise annotations and bounding box annotations used in experiments respectively.	The reviewer would argue on following points and addressing these could improve the paper: 1) Subjectivity in labelling is more of a problem than erroneous labels. Several available datasets that are well-annotated but assessing there subjectivity and finding an agreement is the way forward. Direction of research should include a clear distinction between what does author mean by accurate mask and erroneous mask.  2) Overfitting of previous segmentation models - how do the authors know that these models overfit unless they perform generalisability tests. The same question would be for this work, does the model generalise on unseen datasets. E.g., if you train on one dataset could you try inference on the other dataset?  3) Data shortage in which sense? Argument - there are many publicly available datasets for polyp segmentation are available that can be used to develop methods. How much is sufficient? Authors could cite some papers that reflect to this and make an argument on that? 4) Authors did comment about time consuming in related work but they did not provide the inference time. Also is the network end-to-end trainable	The study addresses a problem arising from new publicly available datasets, and is therefore new and timely. It focuses on a problem rooted on clinical translatability (rather than improving results on curated data) which is of great importance. The proposed modules are innovative, particularly the FFS methodology to generate pixel-level labels handling uncertain pixels.	The paper is well-written and organized, but some method details should be added.	The method shows some benefit especially in using large detection video dataset. However, the arguments on doing this are slim. Alongside, the generalisability assessment is lacking. I think rationale behind developing a segmentation model with the detection labels doesn't fit right because you could just make a detection method which is already of interest in clinical practice, why leverage for segmentation?
074-Paper2706	Brain-Aware Replacements for Supervised Contrastive Learning in Detection of Alzheimer's Disease	"The paper introduces an augmentation technique along with a contrastive learning method for pretraining classifiers for Alzheimer's disease. The study uses augmented labels as ""soft labels"" for supervised contrastive objective."	The authors combine the advantages of relaxed contrastive learning and use case specific data augmentation operations (BAR and BAM) to solve the Alzheimer disease detection. The performance of the presented method is reported on the Alzheimer's Disease Neuroimaging Initiative dataset.	The authors propose a new data augmentation strategy for the contrastive learning framework to train a better pre-trained model. Their method produces a great variety of realistic-looking synthetic MRIs with higher local variability compared to other mix-based methods, such as CutMix.	The paper is well organized and it presents the methodology very clearly. It is addressing a significant problem without any unrealistic claims. The authors have formulated the problem and the study's hypothesis is clear. The proposed method is an incremental improvement on the literature, which is sufficient for publication. The experimental setup is designed to validate the hypothesis of the paper, with sufficient ablation study.	The paper is well written. The BAR and BAM augmentations are novel: BAR is a new version of CutMix which takes benefit from the anatomic properties of the brain while BAM is a new version of Cutout.	"It is an interesting idea to produce a great variety of realistic-looking synthetic but not simply mix up some image patches in ""CutMix"". ""CutMix"" has been proved to be effective in natural images. But, in the medical image field, we pay more attention to Interpretability. It is important to input the cases matching the anatomy structure so ""CutMix"" strategy may not be suitable for the medical images.  Meanwhile, they train a supervised contrastive loss with the soft labels and synthetic images, leading to very powerful representation learning."	"I did not see any major weaknesses in the paper. Only a couple of minor points: The paper claims: ""Also, BAR implicitly forces the model to pay attention to the relationship between medically relevant brain regions, thus making it more clinically relevant."" This statement implies causality. Although the authors have presented some attention maps in the supplementary materials, causal feature learning requires deeper analysis. Although the authors have targeted a very specific problem to solve i.e., AD detection in MR, but the general idea behind the method is not specific to AD. It would've been better if the authors included other clinical problems of significance."	The applicability of the BAR augmentation is very limited and can not be generalized to other use cases since it sets hard constraints on the data: alignment and pixel precise information. The evaluation is very limited since it does not compare to any of the state of the art methods. Since the method builds on  top of ViT, I am missing the comparison with methods using convolutional networks.	They can compare the attention maps (grad-cam) from the different models for some more detailed analysis but not just some metric values. Since they focus on the AD classification, they can compare the performance of replacing brain regions related to with AD (e.g., hippocampus) or other unrelated regions. They should compare with more SOTA pretrain methods. In this paper, they just compare with some simple baselines, such as naive contrastive learning, cutmix.	The dataset is publicly available and the authors will publish their code. So the results should be reliably reproducible.	The paper reports architectural and experimental setups thoroughly. Thus the results should be with little effort reproducible	The author will release their codes in github.	I very much enjoyed reading the paper. I suggest the authors expand their scope of their method to other problems as well.	Since the introduced method is architecture agnostic and convolutional networks are very successful  in solving object detection,  including results for convolutional networks should make the contribution even stronger. (ViT are successful on image classification but way less efficient on other vision tasks). Also, reporting the performance of previous state of the art methods should help evaluating the importance of the introduced augmentations. An even better scenario would be to take the old state of the art method and include the augmentations in an appropriate way and report the performance.	"They can generate the grad-cam maps from the different models for some more detailed analysis but not just some metric values. They can compare the performance of replacing brain regions related to AD (e.g., hippocampus) or other unrelated regions. They should compare with more SOTA methods of model pretraining, e.g., moco [1]. [1] He, Kaiming, et al. ""Momentum contrast for unsupervised visual representation learning."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020."	The paper is well organized and the objectives clear. It has sufficient technical novelty for MICCAI and the presented method is thoroughly evaluated.	The limited applicability of the introduced augmentations and the weaknesses in the evaluation push toward not accepting the paper. Minor improvements might lead to upvoting the paper.	"I appreciate that they focus on the problems of ""Cutmix"" in the medical image field. Their method produces a great variety of realistic-looking synthetic MRIs with higher local variability."
075-Paper2270	Breaking with Fixed Set Pathology Recognition through Report-Guided Contrastive Training	Existing frameworks for computer-aided diagnosis tools rely on a fixed set of predefined categories automatically that are extracted from medical reports. This paper proposes a framework to go beyond this assumption to make diagnosis more context aware. Methodologically, they employ a contrastive global encoder-decoder designed to uncover latent concepts from unstructured medical reports, while still retaining the ability for free form classification. They also investigate properties of such free -form recognition and propose a method to employ weakly annotated data to improve training. They evaluate on large-scale chest X-Ray datasets such as MIMIC-CXR, CheXpert, and ChestX-Ray14 to demonstrate the efficacy of their method in comparison to methods employing direct supervision.	The authors propose a method for training a framework to detect different diseases in chest X-ray images. The training is performed with report supervision using local (sentence) and global (full report) levels.	This paper aims to enable the neural networks used to medical images less reliant to label supervision. To this end, this work proposes a novel contrastive language-image pre-training method. The extensive experiments and analyses on four datasets, i.e., MIMIC-CXR, CheXpert, ChestX-ray14, and PadChest prove the effectiveness of the proposed approach.	The paper is extremely well written. The explanation of the problem and proposed solution is precise, yet well motivated, and detailed enough. Evaluation is performed on large-scale datasets and extensive investigation has been performed on various factors impacting model performance, such as prompt engineering, ablations on loss components and model heads. The experimental design is thorough, which is a big plus for potential translational applications. Extensive and relevant baseline comparisons have been provided	This work presents a novel technique for integrating radiology reports into model training for different disease detection in chest X-ray. The authors propose a  method for adapting the well known contrastive language and image pretraining for recognition of natural images to more complex text such as radiology reports. The analysis provided and results is clear and well defined.	The proposed approach is well-motivated and novel for medical images. The experiments and analyses are very extensive.	The model has four different hyper parameters \lambda_1 - \lambda_4 to weigh the contributions of various loss terms. How do they arrive at the settings mentioned below Eq. 5? Which of these parameters is the performance most sensitive to? Some of the differences reported in Table 1 are rather minor and do not report a measure of standard deviation. How consistent are these differences and would they hold up under a statistical comparison?	In some cases, the improvement in the results is not very big. A statistical analysis would be helpful to better understand the significance of the obtained improvement	The novelty of the framework is limited. In my opinion, this work mainly adopts the existing contrastive language-image pertaining model, i.e., CLIP [1], from computer vision to a new domain or a new task. However, introducing large pre-trained models to solve a new downstream task cannot bring new insights to the community. It's very important to explain why the proposed approach can improve the performance and what problems can be solved by the approach? There are some previous works, e.g., [2], that have attempted to adapt the contrastive language-image pertaining model into the medical image analysis field. The authors neither cite nor compare with it. [1] Learning Transferable Visual Models From Natural Language Supervision. 2021. [2] Contrastive Learning of Medical Visual Representations from Paired Images and Text. 2020.	This looks alright to me.	Good reproducibility. Architecture is well defined and results are demonstrated in open source datasets.	I believe that the obtained results can, in principle, be reproduced. Even though key resources (code) are unavailable at this point, the key details (e.g., proof sketches, experimental setup) are sufficiently well described for an expert to confidently reproduce the main results, if given access to the missing resources.	Minor : Typo in the line below Eq. 5, there are two settings for \lambda_4 mentioned The explanation above Eq. 1 for breaking the symmetry is a bit hard to parse. Perhaps a few lines of explanation can be provided on which portion of the loss differs from the traditional MILNCE construction	This is a well written manuscript that presents relevant work on a widely studied topic. The reduction in the need of data annotation is a very helpful tool for any other medical image application.  A more in depth statistical analysis of the results may be helpful to better understand the impact of the improvements achieved	The novelty of the framework is limited. In my opinion, this work mainly adopts the existing contrastive language-image pertaining model, i.e., CLIP [1], from computer vision to a new domain or a new task. However, introducing large pre-trained models to solve a new downstream task cannot bring new insights to the community. It's very important to explain why the proposed approach can improve the performance and what problems can be solved by the approach? I recommend the authors add a Related Work section to help the readers better understand the differences between this work and previous works, .e.g, [2]. Besides, I recommend further discussing the advantage and disadvantage of each previous work, instead of just listing them, which can help the readers understand the strengths and weaknesses of this work. Many important hyper-parameters are missing, such as learning rate, batch size, and the number of epochs, which hinder reproducibility. Besides, it is necessary to report some model details and training details for the proposed approach, for example, the training time, numbers of model parameters and memory cost, and so on. The paper is written in an optimistic tone that leads the reader to assume the proposed approach is rather good. However, I am more interested in knowing if the approach brings errors? And what type of errors does it bring? And why? I would like to see a statistical significance test, due to the performance gap between the proposed approach and the previous state-of-the-art methods is small. [1] Learning Transferable Visual Models From Natural Language Supervision. 2021. [2] Contrastive Learning of Medical Visual Representations from Paired Images and Text. 2020.	The paper is well structured, with careful and thorough experimental design with the claims being made supported by the results. There are a few minor points that need addressing (see weaknesses). However, overall the paper is in good shape.	The authors present an elaborated framework for multimodal training that involves both image and text, obtaining text information from radiology reports both locally and globally. This framework is novel and very valuable for the community, along with the introduction of self-supervised learning and a more wide range of diseases that do not require explicit annotation.	The novelty of the proposed framework is limited. In my opinion, introducing large pre-trained models to solve a downstream task cannot bring new insights to the community. It's very important to explain why the proposed approaches can solve the claimed problems. It's not clear why the proposed approach can improve the fluency, adequacy, and fidelity of novel object captions. Some important analyses should be perfomed to prove the contributions and claims of paper.
076-Paper1095	Building Brains: Subvolume Recombination for Data Augmentation in Large Vessel Occlusion Detection	The authors propose a new augmentation method for training deep learning models to automatically classify large vessel occlusions by combining parts of relevant images from different patients.	"Propose ""recombination"" (generates artificial training samples by recombining vessel tree segmentations of the hemispheres or hemisphere subregions from different patients) as a simple but effective data augmentation strategy for large vessel occlusions classification. On a private dataset, the method is showing better performance than baseline augmentation methods."	The authors present an augmentation method for large vessel occlusion (LVO) classification that recombines subvolumes of vessel segmentations from different patients.	interesting idea and novel method for this application. good evaluation	* An interesting way of augmenting the brain data * Comprehensive experiments: five fold cross validation on three models, each with ablation study	The presented method is a simple yet effective and clinically relevant solution to solve the problem. The ROC AUC is significantly improved with respect to the compared method. The paper is very well written and structured.	Paper is hard to follow and clarity could be improved. Motivation for this work can be improved No actual images showing the recombination of vessels	* The augmentation method might only be useful to limited applications, thus the impact is limited to a small field.  * No public datasets. No access to codes.	"The motivation for developing an automatic classification method is not clear. Section 1 reads ""Despite common anatomical patterns, the individual configuration and appearance of the vessel tree can differ substantially between patients, hence automated and accurate methods for LVO detection are desirable"". I don't see why differences between vessels trees across patients justifies a desire for automated methods. Can a radiologist perform this task quickly and accurately? If yes, why do we need an automated method?"	Limited reproducibility because of the lack of clarity in the methods section	No code or data available from the paper.	No code, data or models are shared. The reasons for this are not mentioned. Most hyperparameters are shared, but the strategy to choose them is not described.	* Overall, the motivation for this work is not very clear. It is really not that challenging to identify if a patient has an LVO. It would be clinically more useful to locate the LVO. * The authors argue that data availability may be limited. However, the previous research papers in this domain were able to collect a vast amount of data. Thus, the claim that data availability is a problem is not well justified. * Only a subset of the patients included actually suffered of an LVO. What was the reason for imaging in the other patients? * The pre-processing section could be improved by adding more details rather than just citing other papers. * Figure 1 shows multiple discontinuities after recombination, which are not biologically plausible and may lead to false detections of clots. It is unclear what benefit these simulated datasets have. * More generally, the artery tree is highly variable between individuals and the branching pattern can be very different. It may be questioned what value such an augmentation has. * For the Recombination of ICA and MCA Subvolumes, I don't see the information that if the ICA is affected there also should be only reduced signal (at most) for the connected MCA branches.	"The paper is a natural extension of [14], which introduced deformation and mirroring as augmentation methods to help large vessel occlusions classification from the vessel segmentation masks. With the new recombination augmentation method, the healthy parts of the brain vessels from all the training set are shared and thus improve the classification performance. This idea is especially useful when the dataset is limited and the task is relatively simple.  The DenseNet used in three models was not the same (number of parameters), which introduces another variable in comparison between three models.  Considering the dataset was from the same source as ref [14] (151 patients in this paper, 168 in ref [14]), it is important to make clear whether this is using the same or partial data (why cherry picking if that is the case) as ref [14]. It would be ideal if this paper follows the same dataset as previous ones, if available.  In ref [14] segmentation masks were deformed 20 times as deformation augmentation (10 times with 4 random anchors + 10 times with 5 random anchors), but the authors implemented with ""Each data set is deformed 10 times with a random elastic field"". Is that an implementation difference? All the cases were successfully segmented and registered? How the performance is impacted by segmentation and registration quality? The basilar arteries might be tortuous and in both sides of the brain mid-plane. Mirroring all left-sided hemisphere's vessel trees in sagittal direction might cause broken/irregular basilar arteries in the artificial images.  Grammar errors:  it can supplemented system enables to split"	"Please check for typos and incongruences: ""as time is brain"", ""it can supplemented"", ""models specifically designed exploiting""... Please check for missing or undefined acronyms (I recommend the \acronym LaTeX package)."	While the methods is interesting, the methods description is hard to follow and could be improved	The paper is well written (despite some grammar errors) but the innovation might not be high. Also the specific application on large vessel occlusion classification limits its reader group/impact.	The paper is well presented and the proposed method is clear and effective. Although the motivation is not properly explained, it might be guessed. For example, the method could be used for triage. I would have liked to see more significant efforts in terms of reproducibility.
077-Paper0239	CACTUSS: Common Anatomical CT-US Space for US examinations	The authors propose a pipeline to segment the (healthy) aorta in US images without having to use labelled US data. They do so by training a contrastive generative network to translate between intermediate representation (from CT) to ultrasound images. Given that the CT scans are labelled, they can then train a network to segment the aorta in the IR.	This paper describes a way to bridge between CT and US to enable AAA screening using US.  This is done through an Intermediate Representation of the anatomy. Simulated US is generated from CT, and then trained with real US images unpaired. Also, a segmentation network is generated from the simulated US images.  This framework can then take real ultrasound image for segmentation for the purpose of AAA screening and diagnosis.	The authors demonstrate a framework for automatic aortic measurements on abdominal US doppler studies using a domain adaptation technique leveraging an intermediate representation space and deep learning models trained on labeled CT studies of the aorta. They then show favorable performance of their approach compared to a Unet model trained on a small number of labeled US studies alone using a Unet.	Clearly written, well explained, and good figures make the concept easy to understand. Novel IR methodology using conv. ray tracing instead of simple edge detection Clinically acceptable results on volunteer data	The use of unpaired image-to-image translation network is interesting for this application.  It demonstrates the feasibility of using ultrasound for AAA diagnosis.	Accurate domain adaptation of deep learning models between CT and US modality holds considerable value. CT-based models are more plentiful, well curated, and reproducible. US can be performed in any setting (office, emergency room, OR), requires less capital and can be leveraged by robotic interventions to provide real-time guidance. The approach of using an intermediate representation space that can leverage CT-based labels for US applications is novel to this reviewer. Importantly, the accuracy results for a specific clinical task-aorta measuring-is reported as better than if a model built only with US imaging were used.	No patient data with AAA diagnosis or borderline AAA. Therefore the generalizability of the proposed method for actually detecting an aorta >8mm diameter is questionable, as this case is not tested. No mention of statistical testing on the results. Focus on Dice score is not correct for this clinical application, as distance errors are more relevant to the AAA diagnosis.	"The real ultrasound images came from relatively young individuals.  These people are probably ""easy"" to scan.  People who need AAA screening are much older and they are probably more ""difficult"" to scan and could have worse image quality.  This could greatly impact the performance. Commercial solution for AAA diagnosis is already available using 3D ultrasound alone (no RUS or camera).  There is no mentioning of this."	A relatively small number of patients were used for evaluation and many of the same patients appear to have been used to train the comparator US U-net model and serve as test patients. The full heterogeneity of anatomy encountered in practice cannot be captured with so few patients and so failure modes appear unexplored. The clinical need for automatic aortic segmentation on US is not entirely clear. US doppler studies are typically performed by a trained US technologist for whom it takes minimal effort to make aortic measurements. While CT has superior accuracy, discrepancies in US measurements of aortas in a screening context are rarely clinically meaningful. The framework is complex and it can be challenging to follow which component is being discussed (US simulation vs CUT vs IR space aorta segmentation), especially with regard to data used for training/validation/evaluation.	Good, but of course would be better if the code could be made available and US data could be made available.	It is adequate.	The authors reference the US simulation algorithm and CUT network and provide parameters for each. As noted in their checklist code is not provided . Overall this limits reproducibility of their work. It would be of great interest to the community if they provided access to their framework to allow others to leverage well-curated CT (or MR?!) datasets for US applications.	Section 2 - What is meant by 'anisotropic properties' of the IR? Section 2 - The authors should briefly explain the conv ray-tracing approach in addition to citing [16] Section 2.1 - Was the test set of 100 frames acquired from a separate volunteer? Section 2.1 - No unhealthy (i.e. patients with AAA) in CT or US datasets. Figure 3 - Labels of what is simulated US/IR ('fake') and what is real would be helpful Section 2.4 - What about an experiment where no IR is required? Could you use CUT to go between abdominal CT and US directly instead of using an IR? What would the performance be? I expect the authors did some preliminary testing with this even if it wasn't a thorough experiment, and it would be good to report these preliminary results. Table 3 - Was any statistical testing done for significance? Table 3 - On the siemens machine, an MAE of 7.6+1.5 seems that in some cases the mean error is exceeding the 8mm clinically acceptable threshold? Is this correct? Table 5 - Please include MAE as well. Section 3 - Overall I would argue that DSC is not a good measure for AAA diagnosis compared to a distance error such as MAE or Hausdorff. I would emphasize the distance metrics more than DSC. In particular, for the Siemen's machine it seems that a supervised U-net is better at distance errors (although not sure if this is statistically significant as mentioned above). Section 3 - FID was measured but not reported. Would be interesting to read how well CUT does in this application. Section 3 - A better way to present the results in Table 3 would be to do a Bland-Altman style plot where it is clear if/how the error changes as the ground-truth diameter varies.	"The author should try ultrasound images with different body habitus for the learning. I am not sure if the proposed solution is a good candidate for using ultrasound for AAA diagnosis. Some minor typos (e.g. ""Germangy"")"	Overall this is impressive work with a novel concept that if further validated in a larger and more diverse number of patients, and for different clinical tasks, could create a paradigm for accurate US imaging segmentation and classification models. Further understanding of the how training sample sizes and hyperparameters can affect the performance of the CUT and segmentation models would be enlightening.	I recommend an accept because the authors describe a clear methodology and novel pipeline. The one major weakness is that there is no AAA patient data, and therefore it remains to be seen if this technique would work for AAA diagnosis for the proposed clinica	The data used is not quite suitable for applications like AAA diagnosis.  This however is a good feasibility on a cross-modality application.	The major factors were the use of an intermediate space generated from US simulation and linked with real US images through a CUT algorithm that is a concept novel to this reviewer and appears to perform at a clinically acceptable level. Further validation in a larger, diverse patient cohort is needed.
078-Paper0017	Calibrating Label Distribution for Class-Imbalanced Barely-Supervised Knee Segmentation	Novel proposed techniques to improve semi-supervised learning.  Specifically the techniques are 1) using weights for the loss that are dependent on the class, designed to address class imbalances that are common in segmentation problems. 2) patch selection that is dependent on the class. 3) sampling that is dependent on an estimate of uncertainty of the patch.	In this paper, the authors regard the MR knee bone and cartilage segmentation as a class imbalance problem with barely labeled data. In order to handle the segmentation under this situation, they utilize the cross pseudo supervision (CPS) to build a baseline of semi-supervised segmentation. Then the authors adjust the label distribution using the proposed probability-aware random cropping, class-aware weighted loss, and dual uncertainty-aware sampling supervision (i.e., the proposed CLD method). In the experimental part, the CLD obtains the best performance by comparing with other related approaches.	This paper focuses on a class imbalance problem in automatic deep-learning based multi-class knee structure segmentation and proposes a novel solution by combining class-aware weighted loss, probability-aware random cropping, and uncertainty-aware sampling supervision. The ablation study supports the addition of each approach.	Novel methods for semi-supervised learning within a cross pseudo supervision framework are proposed and applied to a knee segmentation task o Specific methods include  A weighted loss that puts more weight on volumes that have fewer volumes.  This favours the smaller classes, which is particularly important for the task of segmenting cartilage  Training patch selection that is dependent on the classes present that again is designed to favour improving the segmentation task  Patch selection based on the class distribution in the z direction o The observation that these features of the data and problem can be exploited is novel and clever.  It is useful to a broad audience because these aspects could be useful in other image analysis tasks ablation studies are considered to examine the effect of the 3 different strategies to improve performance comparison with state-of-the-art methods is presented the authors demonstrate significant improvements particularly in the cartilage segmentation tasks	I think the proposed method is an effective extension of the baseline CPS. The proposed strategies to calibrate the distribution of labels could work well for knee bone and cartilage segmentation. Especially for the knee cartilages, even under the few-sample condition, the proposed modules could obtain relatively high results for the FC and TC in a semi-supervised framework. From an engineering point of view, this is an effective work.	This is a well written manuscript focused on a common problem of class imbalance that often occurs in multi-class segmentation processes. The study provides equally novel solution based on a model centric strategy of combining weights to distribute class imbalance, probabilities for image cropping, and sampling supervision for uncertainty arising due to unlabeled data. The experiments demonstrate the clinical utility of the approach on a publicly available dataset with a strong evaluation of network predictions and using an ablation study.	"There is little consideration of clinical translation.  The methods developed do not seem to be designed specifically for knee cartilage segmentation this is rather a task to demonstrate the utility of the methods for improving semi-supervised learning.  However the authors do not comment on how these methods can be translated for other tasks or how they affect clinical translation The ablation studied do not consider all combinations of the methods.  It is unclear how well PRC or DUS work in isolation The authors do not present cross-fold validation results which would be helpful to understand the variability of the effect.  It seems that the methods would be highly dependent on the choice of labeled and unlabelled specimens particularly because of the increased dependency on the segmentation distribution.  Are the methods increasing performance at the expense of greater variability? There are other methods based on similar ideas that could be commented on, focal loss or oversampling R. Zhao et al., ""Rethinking Dice Loss for Medical Image Segmentation,"" 2020 IEEE International Conference on Data Mining (ICDM), 2020, pp. 851-860, doi: 10.1109/ICDM50108.2020.00094. R. Mohammed, J. Rawashdeh and M. Abdullah, ""Machine Learning with Oversampling and Undersampling Techniques: Overview Study and Experimental Results,"" 2020 11th International Conference on Information and Communication Systems (ICICS), 2020, pp. 243-248, doi: 10.1109/ICICS49469.2020.239556."	"The basic semi-supervised framework used in the paper is based on CPS [3] (i.e., section 2.1), and its ethodological contribution is not notable. The major contribution could be in the section 2.2, to relieve the class imbalance problem in the segmentation of knee bones and cartilages using 3D MR data. In section 2.2, the ""Probability-aware random cropping"" may be effective, but this part could only be counted as an improvement for the preprocessing part, not for the core part of the semi-segmentation framework. And the ""Class-aware weighted loss"" module is a minor improvement of class weighting, comparing with some traditional way, like the inverse rate of the volume of one class to the whole. At last, other papers, e.g. [5], it also had some similar adaptive/dynamic weighting approaches. I do not find some substantial improvement in yours."	The conclusion is not at all written in the manner it should be and it repeats the contribution of the study as already mentioned in its introduction. This is the only weakness of this paper. Apart from this, all sections are clearly written.	Reproducibility of the paper seems adequate.  The data used is from an open dataset so would be accessible to future investigators wishing to reproduce the result.  The methods and experiments are thoroughly described. Issues with reproducibility: Some of the methods could be more clearly described (see constructive criticism below).  The code does not appear to be open.  There is not very much detail on the software used to implement the algorithm and experiments.  It is stated that Pytorch was used, the version was omitted, further the operating system was omitted.  The hardware used as has only 1 detail, that a 3090 was used.	I think the reproducibility is OK, for the baseline has opened their code, and the implementation of the three modules for addressing the class imbalance problem is not difficult.	Data is available as a public repository. Algorithms are not available, but methods are described very well, so their implementation will be very easy.	"There are grammatical errors throughout for example: o Abstract  ""we statistic"" should be ""we did the following"" o Introduction  ""no ionizing radiation"" should be ""without radiation"" the word ""statistic"" is often misused Acronyms should all be defined with their first use, for example CPS was not defined with its first use Methods Script L and capital L in the equations seem to be used interchangeably.  Pick one and be consistent or better define the distinction. Sub-volume vs cropping patch vs volume.  Are these terms used to refer to the same thing or is the distinction important?  This should be made clear with definitions for each term or by using the same term.  In the description of DUS and WL, the term volume is used; in the descriptions of PRC sub-volume is used; and in the experiments description cropping patch is used. It may be they are different things and this could be an important part, but this reviewer is confused on this point Experiments How does the patch size relate to the thickness of the segmentations and the original image size? Figure 3 Is this a case where both CPS and the proposed method did not do well but the proposed method did much better.  Can you quote DSC here? Table 2 Why did you not examine the effect of PRC or DUS or the combination of PRC and DUS (without WL) The analysis would be improved if these experiments were included to better understand the effect of each technique. How sensitive is the result to the samples used as the labels?  Did you do any cross-fold validation or experiments where you chose different samples as the labeled data.  If these could be included this would help  better understand the robustness of the result and also the reproducibility. The paper would be improved by a conclusion or discussion that puts the results in better context in terms of translation or generalization.  It seems that the authors are not that concerned with the specific segmentation task but this is more of a convenient dataset to use to test out the methods.  What barriers exist to using this method for other more challenging image analysis tasks?  How useful will these approaches be beyond this task?  How will the authors use the methods developed?"	"Although your experimental results beat some related papers, I do not find a clear description about how your modules have substantial improvement or difference to others, and how your method obtains the increase in quantitative values for the knee segmentation problem. Visual comparisons in 3D are important. With these comparisons, I can clearly ""improved"" segmented results in detail in the 3D space. If possible, you should also give segmentation results with different number of labeled and unlabeled data, respectively. And the unlabeled data could also be different modalities (The OAI usually uses DESS MR data, but in hospital, T1 and even T2 are very common). With these experimental settings, you could show your method have a higher extensibility and effectiveness. In the second section in the introduction part, you did not clearly state how your work addresses the shortcomings of these related articles. In Fig. 3, ""Comparison of segmentation results with CPS [13]"", the CPS is citation [3]?"	This paper is well written. There are not many comments from the reviewer. However, there are many typos in the paper that need author's attention. Also, abbreviations used should be defined at their first occurrence. As mentioned earlier, the conclusion is not at all what is expected. Conclusion should focus on what this study achieves - specifically, and can it be made general in certain way. Also, few discussion points on why you see improvements in the ablation study is important.	Significant improvement in performance Novel approaches within an existing framework to improve semi-supervised learning An important topic with potential for broader application however this could be better explained Experiments could be more thorough Discussion of the importance of the result could be better not very much consideration of clinical translation	Some effectiveness for a medical application/problem.	Most of the paper is neatly written with a good problem definition and novel methodological solution. Only conclusion needs rework.
079-Paper0963	Calibration of Medical Imaging Classification Systems with Weight Scaling	The approach tackles the important problem of networks calibration. This is specially important when medical staff takes decisions based on networks confidences. The paper proposes Confidence based Weight Scaling (CWS), a technique for calibrating the outputs of deep learning classification networks. The approach achieves state-of-the-art calibration with a guarantee that the classification accuracy is not altered	A weight scaling calibration method that does not alter accuracy of a predictive model like the common post-hoc temperature scaling method often utilised, the approach achieves improved calibration when compared to current calibration methods and can be applied to any trained model.	The paper investigates calibration methods for DNNs trained for medical image classification systems and proposes a weight scaling methodology that helps calibrating models.	The proposed calibration technique is well described and formulated.  The calibration can be potentially applied to any network and classification dataset. Multiple networks, and datasets are used for the validation. The validation is overall fair and reliable. Performing the validation on three public datasets makes the obtained results stronger. Results seem to show a clear benefit of this calibration method with respect to others existing works The paper is clearly written and easy to follow	The idea is interesting for a calibration method but the approach has been utilised before, similar to a paper released last year: https://arxiv.org/pdf/2108.00106.pdf, which is my only concern re novelty. However, the premise of not altering the accuracy of confident samples is different/not given as guaranteed in papers as the paper indicates.	The motivation of the paper is sound. Models being calibrated is a property that is important, especially in the medical field. The paper uses a number of different architectures and datasets to showcase their experiments, which is a plus.	Adding some description about the limitations and problems of the proposed method would be desirable; as well as insights about future line of work	The concept of keeping the same samples' confidences high is the aspect that is not exactly clear to me when reading the paper - if its a weight scale but then fed back to the classifier, the values would change for predicted confidence, as you still pass through a soft-max layer?	The notation used in the paper is not clear. Please use vector notation to make variables clear to the reader (for example, some variables are referred to as being vectors but they are written as constants, the same goes for subscripts). Also, a number of symbols are used for denoting different purposes which makes reading the manuscript confusing (x and y are referred to as inputs and outputs, respectively but later on, both variables are used to refer different patients [e.g., patient x and patient y]). The main problem I see in the paper is the usage of ECE as a metric to showcase results. As noted by the authors, although ECE remains to be a top-contender as a metric of study, shortcomings of ECE as a metric of calibration is well documented[1]. In light of this information, having better results showcased (only) with ECE does not mean much. Authors also note that some of the shortcomings of ECE is alleviated with adaECE but refrain from providing experimental results on this metric, why? While reading those lines, I had the expectation to see results for both ECE and adaECE. Yet it is not there. This is especially confusing since the authors themselves acknowledge that ECE is not a good metric of evaluation. Then why would a new method that shows superior results showcased with ECE be useful? A number of alternatives of ECE has been discussed in the works of [1] (also referred to in the paper), would it be possible to show results with metrics discussed in this paper (for example adaECE, TACE, SCE)? Does the proposed method still achieve better results when measured with these metrics? If the usage of those metrics is not possible, what is the justification? [1] Nixon et al., Measuring Calibration in Deep Learning	The formulation seems clear and reproducible. However, code will not be provided. The validation is performed in three publicly available datasets.	The equations are provdided and the data available as well as code links and in this way I think it has a high chance of reproducibility.	It would be desirable to have algorithm 1 in the form of a function implemented in any language and any framework of choice for the sake of reproducibility.	Making the code available will be beneficial for the community	I enjoyed reviewing this paper and only have a few questions: A bit more clarity re the method and why only the ECE for top-1 predictions was looked at? Not clear what you refer to here, apologies if something is missed. On Page 7: The 'WS calibration was lower than the ECE... by more than half', I think here it should be worth noticing that the HAM1000 dataset did not perform well for any of the TS->WS, but perhaps you have an indication as to why that would be? Also try some statistical significance testing? the level of confidence in Fig 2, how come there is no representation for the COVID group?	1- Employ vector notation for mathematical equations and be concise with the variable usage. 2- Showcase results with other metrics. 3- Discuss the differences (if any) obtained with different metrics. 4- Please be consistent in your referencing style, capitalization of venues, abbreviations etc.	The paper tackles a very important problem in deep learning, network calibration. A novel calibration procedure is proposed and the fair evaluation shows a clear improvement with respect existing calibration procedures.	Well done on trying the approach, interesting and a nice read, my only concern is that perhaps I see it closely related to another paper (as mentioned above), but the methodology is interesting and perhaps even though similar, the findings are interesting. Perhaps I am a little sckeptical re the final results, but perhaps more clarity on the method re points raised would be good to clear up.	Two main problems of the paper is the concise math notation and the lack of experimental results using other metrics.
080-Paper1145	Camera Adaptation for Fundus-Image-Based CVD Risk Estimation	The manuscript proposes a novel method to adapt fundus images captured by two different fundus cameras to explore the domain discrepancy issue. First, the authors collect a dataset (FCP) containing pair-wise fundus images captured by two cameras with different image quality of the same patients. Second, the authors propose a cross-laterality feature alignment pre-training scheme and a self-attention camera adaptor module. I think the work is of importance for device adaption for fundus image analysis.	This paper makes a fundus pair (left and right eyes) dataset with both high-precision and low-precision equipments. The paper proposes a cross-laterality feature alignment method for model generalization in the task of cardiovascular disease risk estimation. The paper design a self-attention camera adaptor module for domain adaptation, bridging the domain gap for data from different OCT cameras.	This paper proposed a cross-laterality feature learning training method and a camera adaptor module to improve a fundus image-base CVD risk predicting algorithm. In addition, they collected a Fundus Camera Paired (FCP) dataset containing pair-wise fundus images captured by the high-end Topcon retinal camera and the low-end Mediwork portable fundus camera of the same patients. This dataset is of great significance to the study of data in different domains for CVD risk prediction.	The constuction of the dataset is important for research on the camera adaption.  The proposed scheme consider the camera adapation problem from a novel view.	The proposed cross-laterality feature alignment pre-training scheme is realized by minimizing the CVD risk difference between left and right fundus photos, hoping to extract common features in left and right fundus photos. And the self-attention camera adaptor modul is realized by minimizing the predction value between fundus photos from two equipments. This research show potential application value in disease prevention.	In this paper, domain adaptive technology is used to increase the accuracy of CVD risk prediction from images collected by low-quality portable fundus camera, which is of great significance in clinical application. Moreover, experiments in this paper are abundant.	The motivation of selection of CVD risk estimation after the camera adaption is not clear. The organization and the writing needs improvement.	"The description of the method is not clear and accurate, for instance, ""o^r, o^r, l"" in chapter 3.2 paragraph 1 is a spelling mistake and the meaning of the superscript of variable ""y"" in chapter 2.1 paragraph 2 is not mentioned. Most references of this paper are review articles, lacking of references that related to ophthalmic images or similar research. The experiment data is not sufficient and the results are not convincing."	The loss function in Fig. 2 is not described in detail.	The description of the proposed proposed method is clear.	The paper is believed to be reproducible.	I think this article is reproducible.	The manuscript proposes a novel method to adapt fundus images captured by two different fundus cameras to explore the domain discrepancy issue. First, the authors collect a dataset (FCP) containing pair-wise fundus images captured by two cameras with different image quality of the same patients. Second, the authors propose a cross-laterality feature alignment pre-training scheme and a self-attention camera adaptor module. Overall, this work is of importance for device adaption for fundus image analysis. Suggestions and questions are as follows: The motivation of selection of CVD risk estimation after the camera adaption is not clear. The manuscript doesn't compare the proposed method to Cycle-Gan, which is famous for image translation. The organization and the writing needs improvement.	"Is the assumption that ""the visual clues of CVD risk have invariant representation over the two eyes"" supportted by any literature? Or this can be verified by your experiment result. There are some loose expressions in this paper, which are easy for readers to misunderstand, for example expression ""S={x_i^l, x_i^r, y_i^r, y_i^c}"", in chapter 2.1 paragraph 2, the symbol ""r"" has two definitions (right fundus photo and regression). The collapsing performance with SimSiam is confusing. Have you analyzed the reason? Your approach is not limited to a specific network architecture. Thus, more typical network architures need to be evaluated. Besides, the proposed methods do not compare with recent domain adaptation methods. The data provided is insufficient, such as the results of multi-task network. What's more, the accuracy metrics of CVD risk estimation mentioned in the title is not demonstrated in the article. I think the accuracy is one of the most importanct factor to determine the application value of this work."	The quantification results are suggested in the abstract.  Please provide the detailed description or reference of the stop-gradient operation in Section 2.1. In Fig.2, z^s and z^t were input into the loss function, but there is no description of this operation. Please specify the relationship between the loss function and z^s and z^t.	The main idea of this work and the construction of the dataset are of great importance. The propose scheme considers the camera adaption problem in a new view.	In my opinion, the experiment setting in this paper is generally reasonable. However, the investigation of related resarch is not enough. The experiment results is not convincing and do not compare with state of the art methods.	I think this paper is of great significance for clinical application, and the method is feasible and the experimental results are promising.
081-Paper2332	Capturing Shape Information with Multi-Scale Topological Loss Terms for 3D Reconstruction	This paper uses persistent homology, a popular approach in topological machine learning, to design a loss function for 3D reconstruction of 2D image slices.  The proposed loss function is suitable for neural networks (fully differentiable), and assesses the topological persistence (via persistence diagrams of image cubical complexes) of a predicted likelihood function measuring the probability that image voxels are part of the 3D object's shape.  This multi-scale loss incorporates novel shape information to help improve reconstruction, and is compared to other reconstruction method loss functions on cell shape prediction from 2D microscopy images.	This paper introduces a topological loss into a deep neural network for 3d reconstruction from 2d images. The purpose of introducing this additional loss term is to capture multi scale information about the general shape of an object in addition to the more standard geometric information. The new loss is fully differentiable and is added to the SHAPR model for image reconstruction. It is shown that this improves the accuracy of image reconstruction.	The submission focuses on a challenging task that aims to reconstruct the 3D objects from 2D images and masks. Upon the existing SHAPR framework that was optimized with a combination of Dice and BCE loss, this submission proposes a topology-aware loss (L_T) consisting of two terms: a Wasserstein distance between two persistence diagrams and a noise reduction term that incentivizes the model to reduce overall topological activity. The experiments are conducted on reconstructing 3D red bed cells and nuclei using the SHAPR framework with or without the proposed topology-aware loss. The topology-aware loss improves predictions in relevant metrics.	The paper concisely explains topological data analysis and persistent homology in a clear way, and makes the reader aware that not only is this loss function more suitable for reconstruction, but it can be implemented in pre-existing modeling architectures due to its desirable properties.  TDA methods are also stable under noise perturbations, which is useful for image data.	The topological loss, formulated as the Wasserstein distance between persistence diagrams plus the total persistence, is well justified. That it is also differentiable makes it especially valuable. This is a potentially significant advance for this field that provides a general mechanism for the inclusion of topological information which has been neglected by the field.	To my best knowledge, the proposed topology-aware loss is novel. As discussed in related works, the idea of topology-aware loss functions has been explored for image segmentation tasks, but the application of the 3D reconstruction from 2D images is novel. Besides, the method is supported by solid mathematical analysis, including the error bounds in loss calculation. This submission is generally well-organized, with informative illustrations. Although the method section is a bit mathematically heavy, it's easy to follow with intuitive explanations.	"Improvement seems fairly marginal for the nuclei dataset, as compared to red blood cells, and there is not much explanation for why this may be the case.  This would be relevant, as it is discussed that computation can be a bottleneck (though speed-ups are proposed) in optimization with respect to this topological loss function, and so providing suggestions, based on dataset, on when this extra computation may not be ""worthwhile"" would be helpful for a practitioner."	The construction of the cubical complex is not adequately explained. What do the nodes of the complex correspond to? The description of topology and persistent homology may not be sufficient for the majority of readers who are likely to be unfamiliar with the approach.	The main weaknesses are in the experiments. First, Sec. 4.2 mentioned that the regularization strength parameter \lambda for L_T is optimized in [10^-3, 1x10^2], which is a very wide range. \lambda = 0.1 is used for all experiments after the hyper-parameter search. This implies that performance degradation should happen at some point when \lambda > 0.1. The problems are: There are no results on the robustness of L_T for different values. This is no explanation for why it starts to decrease the performance at some point. Does L_T directly work without BCE or Dice losses (L=L_T instead of L=L_G+\lambda L_T)? If yes, what's the performance of using only L_T? If not, why does L_T itself not achieve better or comparable performance than BCE or Dice? Besides, although the loss is topology-aware, the 3D masks shown in the submission all have a Betti number of 0, which means a single connected component. It is not very clear if the L_T loss can improve the reconstruction of masks with a more complex topology.	Code does not appear to be provided, though the authors do use a pre-existing deep learning method for 3D image reconstruction with its corresponding datasets which have been made publicly available.  Some of the TDA methods (computing persistent homology, along with the optimal transport algorithm) would be difficult to reproduce from scratch.	I do not think I would be able to reproduce this paper from the information provided. Some key details are omitted that would be needed to reproduce the work. There are no statements about code availability.	The proposed method is mathematically sound, and the baseline model SHAPR has an open-source implementation. Experimental settings are clearly described. Generally, I have a positive impression of reproducibility.	It would be interesting to see how things might compare to other persistent homology representations (e.g., persistence landscapes), or even simpler topological descriptions.	"Please provide full details about the construction of the cubical complex from the image data. The choice of cubical complex should be explained (I assume this is to align with the voxel basis but this should be made explicit). A brief introduction to the key aspects of persistent homology could usefully be included for the majority of readers. On P2, microscopy images are referred to as having a multiscale nature. It was unclear what this meant, especially since microscopy covers a very broad range of techniques. This should be clarified. Some small corrections to the text are required: P1: ""permit to draw conclusions"" should be ""permit conclusions to be drawn"" P2: ""likelihood of a voxel x to be part of"" should be ""likelihood that a voxel x is part of"" P7: the panels in Figure 2 are not aligned (the graphs for the nuclei are vertically offset)"	As described in the weaknesses, providing more experimental results on the robustness of the proposed L_T under different strength parameters and analyzing its impact on the final reconstruction performance will make the submission stronger.	This contribution could be deemed innovative, and incorporating topological/shape information into reconstruction appears valuable, but would need to do a more exhaustive comparison and discuss implementation/computation further.	This paper makes a valuable technical contribution - a differentiable topological loss function - that could enable significant further work in this area if the descriptions of certain aspects of the process are clarified to support reproducibility of the work. Although other attempts have been made to do this in the past, this is the most convincing I have seen.	The proposed method is novel with solid mathematical analysis. The submission is generally well-written and clear to follow. The main weakness is the lack of experimental verification on the robustness of L_T with different weights, which is of practical value. Thus I recommend a weak acceptance before rebuttal.
082-Paper2156	Carbon Footprint of Selecting and Training Deep Learning Models for Medical Image Analysis	The paper raises awareness and provides informative results about the carbon footprint of deep learning in medical image analysis.	The authors propose clearly defined guidelines for reducing carbon emissions during the development of machine learning models. They use a well-known segmentation framework and well-known datasets in the medical image analysis (MIA) community to estimate the energy consumption used by the community during training.	This paper presents carbon footprint of selecting and training deep learning models for medical image analysis, which seems to work from the test results.	Timely and important topic. Presenting the carbon footprint of DL models in medical image analysis in terms of distance travelled by car is an excellent way to convey the results.	The paper originally addresses an extremely important concern for the community and the general global population, from the point of view of the medical image analysis (MIA) community. This is very interesting because it is not a topic frequently discussed within the medical community, despite its relevance. It presents and compares multiple methods that can be used to measure energy consumption. The manuscript is well written and structured, and experiments correctly corroborate the authors' hypotheses, showing that training using large 3D/4D images means a larger energy consumption.	The experiment results are provided.	No major weaknesses. The empirical part is limited in scope, but sufficient to make an important contribution.	The methodology itself is not novel, as different existing tools to measure carbon emissions are used. However, I appreciate that the point of the paper is to make the community aware of this issue and recommend guidelines.	1 The main contributions of this paper must be further summarized and clearly demonstrated. This reviewer cannot distinguish the new findings of this paper and the existing methods/approaches in the literature. This reviewer suggests the authors exactly mention what is new compared with existing approaches and why the proposed approach is needed to be used instead of the existing methods.  2 The theoretical depth of this paper must to be strengthened. The principle of the proposed approach is not clearly explained, and there is no equation throughout the manuscript. 3 There is no comparison with other state-of-the-art methods in literatures. The effectiveness and superiority of the presented method in this paper should be verified through such comparisons. 4 This reviewer would like to suggest the authors add a flowchart of the presented method and the corresponding description to enable readers to have a better grasp of the approach as a whole. 5 The novelty and contribution of the present work need further justification. Authors need to add more results with more discussions to thoroughly support the main findings. 6 The practicality of the approach should be further discussed.	Can be reproduced, but probably shouldn't in order to reduce CO2 emissions.	For all models and algorithms, check if you include A clear declaration of what software framework and version you used. [Yes] Software versions are not reported. For all code related to this work that you have made available or will release if this work is accepted, check if you include: Specification of dependencies. [Yes]  Training code. [Yes]  Evaluation code. [Yes]  (Pre-)trained model(s). [Yes]  Dataset or link to the dataset needed to run the code. [Yes]  README file including a table of results accompanied by precise command to run to produce those results. [Yes] I have not seen any references to shared code or data in the manuscript. For all reported experimental results, check if you include: The average runtime for each result, or estimated energy cost. [Yes] The training of models in this work is estimated to use 39.948 kWh of electricity contributing to 11.426 kg of CO2eq. This is equivalent to 94.898 km travelled by car. Excellent! I hope reporting this will soon be a trend.	The reproducibility of the paper is not good.	One way, as a community, to reduce CO2 emissions would be to avoid unnecessary, repeated experiments. Many works present similar baseline results (e.g., running the same U-net model on same data), over and over again. The paper discusses the briefly as part of the idea of open science. I think this point is important and could be made stronger and highlighted a bit more. Maybe a way forward would be to construct a library of trained models that can be shared and re-used for comparative analyses avoiding many training runs of similar models, and thus reducing CO2 emissions. But this would need to go hand in hand with recommendations for publications, reviewing, etc. I would suspect that many reviewers are asking for (sometimes unnecessary) comparisons which require the authors to run many more model trainings than needed. We should be more careful, as a community, to ask for ablation studies, etc. Any additional experiment should come with a clear justification and trade-off analysis of added (scientific) value over the increased carbon footprint. Similar applies to cross-validation, etc.	"Please double-check for grammar, e.g., ""an year"".  Please replace ""f.x"" with ""e.g.,"". In Table 1, abbreviations are probably not necessary. Fig. 3: It says ""total [...] energy [...] over the five-fold cross validation"", but it seems to me that it is the mean consumption, not the total. On the right graph, please modify the bars so they are separated instead of stacked (i.e., three groups of three bars, where each group has a bar for each region). The current visualization suggests the consumptions add up and each region is responsible for a certain percentage. Some labels are barely visible when printing the manuscript in black and white. The paper claims AMP made a large difference when training with brain images, but this is not clear in Table 2. I think either the table or the text should be corrected. In Section 6, it is not clear where 2.89 comes from (the number used to calculate the carbon emissions from MICCAI papers)."	The presented approach seems to work from the test results. However, the contributions of the paper are not clear and not sufficiently significant to be published.  In addition, the overall technical quality of this paper is below average as this work is not well presented.	The paper makes a strong point that should be of interest to the whole MICCAI community and beyond.	The paper is very original and relevant for the community. It is very well written and structured. Results are reported clearly. I am glad that teams within the community are looking into this.	The contributions of the paper are too limited.  In addition, the overall technical quality of this paper is poor.
083-Paper0783	CaRTS: Causality-driven Robot Tool Segmentation from Vision and Kinematics Data	The authors proposed to perform semantic segmentation via 3D pose estimation and rendering of surgical instruments. They propose that this makes the method robust to challenging image based situations such as smoke/blood.	This paper presents a novel framework for robot tool segmentation. The key novelty of the proposed model is that instead of assuming a causal relation between an image and its segmentation mask, it rather assumes a direct causal link between robot kinematics measurements and the segmentation mask. Removing the direct link between the image and its segmentation mask, that is the standard assumption in prior works, is hypothesized to render the segmentation model robust to image-domain shifts. The latter is experimentally validated on both real and simulated data captured under controlled settings using the dVRK platform.	The paper describes, CaRTS, a surgical instrument segmentation algorithm. CaRTS proposes a novel framework for estimating the segmentation of surgical instruments using images and kinematics. The results on binary segmentation seems to be well above other existing methods.	The idea of using a 3D model tracking to provide a segmentation mask is not really new, it is a different approach from what people commonly try and the authors correctly identify that it could provide some benefits in challenging cases where pure image based methods break.	1) The paper explores a model that goes beyond the standard paradigm for robot instrument segmentation. Removing the direct causal link between the image and the segmentation mask is hypothesized to improve performance when unseen image-space conditions are encounted (ex low brightness, smoke etc). This is supported by the experimental results of the paper, as the proposed model is tested on several unseen image-domains with varying conditions. It outperforms a standard U-Net baseline and a method that also leverages kinematic parameters combined with Convolutional network. 2) The proposed model links various parameters of the robotic platform (camera, tool semantics, kinematics). Thus it constitutes a flexible framework that can be used to estimate missing parameters via gradient descent, given all involved operations are differentiable. This is leveraged to iteratively refine the measurements of the kinematic parameters using gradient descent while also resulting in improved segmentation performance across domains.	The proposed framework, which combines image information and kinematics, is interesting though not novel as existing works have already explored this framework The proposed algorithm seems to obtain high segmentation performance, including under different settings such as low brightness, bleeding, smoke, background change, and simulated smoke The chosen dataset, and metrics are adequate The paper is well presented and easy to read	I think fundamentally this paper is solving a more difficult problem than segmentation, which is 3D pose estimation. Although the idea is interesting, I am not convinced that this approach will yield better results in the long run. The challenges of 3D pose estimation, when the instruments are moved into complex articulations or when the tool movement is fast will cause this method to fail. Additionally, this method requires kinematics access (not guaranteed), 3D CAD model availability (again, not guaranteed) and a very powerful GPU to even get close to real time (which the authors admit is far away). Additionally it won't be able to handle laparoscopic instruments which are often used in robotic procedures. I don't see the comparison to a vanilla UNet as convincing. For this approach to be comparable, it should be compared with a UNet (or ideally a newer architecture) that has at least been trained with augmentation for smoke and other artifacts so that the fall off can be compared. Looking at the failures of the UNet in figure 1, I don't see these as hugely challenging cases representing a fundamental limit of the state of the art. These are quite easy images that a well trained network should be able to handle.	"1) Despite the fact that the model does not directly link the image to the segmentation mask, it requires a pretrained segmentation network (a UNet) to extract semantically-rich features over which the difference between the rendered and observed image forms the utilized loss function (page 6). This network leverages the standard paradigm of mapping input images directly to segmentation masks. Conclusively, the overall method implicitly employs the ""contemporary"" causal model, in the form of a pretrained feature extractor. This merits some discussion in the paper."	"The authors state that the proposed model is very flexible, potentially capable of ""direct estimation of robot kinematic parameters, including joint angles, base frame and camera transformations, in an end-to-end fashion."". However, the authors cast the model experiments for surgical instrument segmentation. When focusing on the evaluation, the authors only compare the model against one segmentation model, U-Net, which is already 7 years old. I encourage to authors to compare against more recent segmentation models to compare against (e.g. EfficientDet, HRNet, Swin Transformers, ...). In addition, the model seems to be designed to only work with binary segmentation (background and instrument) and in absence of occlusions. These are important limitations which makes the proposed framework interesting but its potential and real applicability is unclear"	Paper seems like it would be reproducible since the code will be released.	The authors stated that the code and dataset publicly available upon publications, thus it can be assumed that reproducing the results in the paper will be possible.	The code and data seems that will not be released	I think the authors should try training a state of the art segmentation model and add augmentation before continuing with attempting to perform segmentation via 3D pose estimation. I think the approach would be useful/interesting evaluated as it is, which is 3D pose estimation. There are some previous works (e.g. Real-time 3D Tracking of Articulated Tools for Robotic Surgery, Ye et al, MICCAI & 3-D pose estimation of articulated instruments in robotic minimally invasive surgery, Allan et al, TMI) which would be good comparison points.	1) The baseline UNet's performance significantly drops when tested in unseen image-space conditions. However, it is trained without any augmentation (as stated in supplementary). It would be interesting to explore the limits of this baseline when data augmentation is employed, in the form of simulated image-space alterations (such as smoke, bleeding etc). Including a data augmentation pipeline is essential in most standard tool segmetation approaches and thus provides a stronger baseline to compare the proposed method to. 2) In page 6, it is mentioned that the feature extractor is trained on collected images and hybrid images where the average image background is added to rendered images. The latter should be justified in the paper. 3) It would greatly benefit the reader's understanding of the method section, if a figure describing the various parts of the robotic setup and linking them to the referenced variables in the text, was added to the paper.	In addition to the suggestions above, authors might refine the paper by improving the readability of the figures by defining all variables in their caption. For example in Figure 2.	I think the idea is fairly creative but I don't see a clear path to improving the state of the art in segmentation (which this paper is addressing) using this approach.	This paper presents a novel and flexible framework that questions the standard approach to robot tool segmentation. Both the novelty and the adequate experimental validation outweigh the lack of justification and discussion around some methodological choices. Therefore, I incline towards acceptance.	The proposed framework is very interesting as it defines and employs multiple sources of information that are commonly available in real settings (kinematics and vision). Upcoming works should follow this trend and employ all information available to generate algorithms that are more robust and reliable, in 'any' scenario.
084-Paper1369	CASHformer: Cognition Aware SHape Transformer for Longitudinal Analysis	This paper proposed a  Cognition Aware Shape Transformer for longitudinal shape analysis.  The CASHformer uses a frozen pre-trained Transformer, where only LN layers are fine-tuned in small Alzheimer's dataset, to predict the mesh deformation along time. Congnitive embeddings and congnitive decline asare loss are also introduced as regularization.	This paper proposed a method named CASHformer, a transformer-based framework for the longitudinal modeling of neurodegenerative diseases. CASHformer consists of the mesh network, frozen pre-trained transformer, cognitive embedding, and cognitive decline aware loss. The results show CASHformer reduces the reconstruction error by 73% and increases AD disease detection by 3% to the baselines.	Authors propose CASHformer, a transformer based approach to model hippocampus deformations across time. MRI follow-ups of hippocampus are segmented using an already existing software (FIRST) and embedded using a mesh-based neural network encoder (SpiralResNet). A pretrained transformer is trained to predict hippocampus deformations embeddings (only fine-tuning the Layer Normalization blocks). Authors propose to incorporate clinical knowledge to the model by including a cognitive score embedding (similarly to positional encoding) to modulate the hippocampus latent representations, as well as a Cognitive Decline Aware Loss based on cosine similarity to enforce larger deformations for patients with a higher cognitive decline. Authors evaluate their model on three proposed longitudinal shape modeling tasks: interpolation, extrapolation and trajectory prediction. An ablation study on the proposed contributions and a discussion on the size of the models are also presented.	Using large pre-trained Transformers as the universal computing engine and finetune it in the small  downstream tasks is an interesting research topic, which might helpful to deal with the limited data problem in the medical field.	This paper effectively incorporates the transformer model into its framework to solve the low-resource data issues with detailed experimental discussion. CASHformer designed cognitive decline aware loss for the longitudinal analysis.	Overall clarity: the context, goal, contributions are clearly described and the article seems well written to me.  Clinically aware: the effort to anchor clinical relevance and medical expertise throughout the introduction, methods and evaluation is highly appreciated. Generalizable methodology and contributions: although it may seem contradictory with the previous comment, the approach presented seem to be easily transposed to the analysis of other longitudinal data (for different pathologies or clinical need) using any encoding/decoding network (relevant to the task) and the same training strategy of the pre-trained transformer backbone, aswell as some task specific loss / embeddings.	The ablation study is not very sufficient. The first two rows of the bottom Table 1, proof that finetune LN layers with pretrain weights is better than train from scratch, but we can't know if only finetuning LN layers is better than finetuning all layers.	"I notice the frozen pre-trained transformer is also similar to the ideas of Adaptor [1], Prompt [2] in the natural language process. [1] Houlsby, Neil, et al. ""Parameter-efficient transfer learning for NLP."" International Conference on Machine Learning. PMLR, 2019. [2] Schick, Timo, and Hinrich Schutze. ""Exploiting Cloze-Questions for Few-Shot Text Classification and Natural Language Inference."" Proceedings of the 16th Conference of the European Chapter of the Association for Computational Linguistics: Main Volume. 2021."	"Overlap with ref. [24]: TransforMesh [24] is the main state-of-the-art comparison method, and when reading the original paper, some considerable overlaps with the proposed submission are noticeable: in the structure (section 2.4, evaluation...), notations, figures (fig. 2 in [24] vs fig S2 here), and quite some amount of text (transformer architecture description, ""missing shapes"" paragraph, etc.). Still the proposed method is showed to outperform [24] and some clear methodological novelties are provided with the Cognitive Embeddings, Cognitive Decline Aware Loss and Frozen pre-trained Transformer training. Authors could provide further discussion on the use of such modelization: predict diseases among neurodegenerative pathologies in clinical practice ? if yes, in what time horizons ? or maybe to better understand those complex diseases for researchers, neurologists ?"	The authors agree to release the training code.	The results are reproducible with some effort.	All parts of the model architecture blocks are well described as well as the training procedure (mostly in the supplementary materials). The dataset is exhaustively detailed as well as the splits and the proposed evaluation process. As stated in the reproducibility checklist, not all hyper-parameters tuning / setting is reported, e.g. the loss weight lambda was empirically set.	Provide more experiments to address my questions in the wakness question.	Please provide more details on the dataset. How to choose the subset from ADNI, which would help researchers follow and reproduce your works. The author could provide more exploration in their further works on the frozen pre-trained transformer, e.g., fine-tuning partial layers instead of layer normalization (LN), prompts.	"Major comments: an alternative evualuation idea could focus on trying to predict the ADAS score with different time horizons ? which seems to be highly valuable for clinical pratice. Minor comments: p.5 line 3: ""between the the line"": remove 1 the p.5 line 16: ""The increase of the classification accuracy by 3%"" : percentage point increase does the mean absolute error (metric used) between meshes has a unit ? (mm?) fig S3. authors could add other usual binary classification metrics (e.g. precision, recall, f1, roc)"	Overall, the paper has good quality, however, the comparison and ablation is not sufficient.	Overall the paper is well written, and the experiments are good and solid.	The overall clarity, medical significance and clinical anchor are appreciable. The methodological contributions and outperforming results are clear, as well as a good generalization potential to approaches for the processing of longitudinal data. Nevertheless, the considerable overlap with ref [24] is quite remarkable and remains problematic to me.
085-Paper0894	Censor-aware Semi-supervised Learning for Survival Time Prediction from Medical Images	The paper presents a deep learning method for survival predictions based on images. It presents a methodology to leverage the use of censored data which is often ignores or sub optimally used.	The paper addresses the challenge of survival time prediction with censored data, either by patients leaving the study or by patients living longer than the study ran. The authors suggest a pseudo-label approach to estimate labels for the missing data and establish different loss functions that can be used in this case. The proposed approach is evaluated on two datasets and compared to other methods.	This paper proposes a semi-supervised learning method for patient survival time regression with censorsed and uncensored pathology and x-ray images based on the lower-bound time to death and peudo labels. The proposed method is evaluated on two public datasets and achieve improved performance.	*The paper is very well written and it is easy to follow. *The authors proposed a pseudo labelling approach to leverage the use of censored data, which seems to be novel and very useful. The paper shows a good ablation study of the proposed model as well as a good comparison to other methods. The paper claims to achieve SOTA in both datasets used.	* The proposed solution is straightforward, comparatively simple, and can be combined with other technical approaches.  * The proposed solution seems to be superior to existing solutions.  * The problem that is addressed in this paper is under-investigated.	The introduced ELR loss and censor-aware ranking loss shows promissing results and could be adopted in related tasks. Rich ablative studies are conducted and show detailed performance impacts of th e proposed modules.	The methodology of the images used in the pathology case is not clear. The paper cites the work in https://www.pnas.org/doi/epdf/10.1073/pnas.1717139115 mentioning  1505 patches. However, in the cited work, there is no mention of these 1505 patches, and the only data available are the whole slide images.	* The authors use only two datasets for their evaluation. I would have expected more.  * The authors reported only the best run when comparing their approach to others.  * The authors used only image data, while the approach should be usable with other data types as well.	For the CA-MSE loss, why the authors adopt MSE loss instead of MAE loss considering the evaluation metric is based on MAE? It would be better to conduct more ablative studies on the impact of the amount of noisy labels on the proposed method. How to overcome the influence brought by the difference of proportions of censored/uncensored cases in different datasets which causes different performances in this paper?	The authors mention that the code will be shared upon acceptance. The raw data used is public; however, the preprocessing to obtain the patches in the pathology case is not described at all, making it challenging to fully reproduce the work.	The paper is clearly written and most of the relevant information is within the paper. The authors also promised to publish their code and conducted their experiments on public data. The results should therefore be reproducible.	The authors declare the code will be publically available.	This a very well written paper with some very interesting results. I think it would benefit from a clear explanation on the preprocessing of the images e.g. how to go from whole slide images to patches instead of just citing the work where this was done. If there is space problem, this can even go in the supplemental material.	Thank you for the interesting paper. I enjoyed reading it. However, I found a few small points that could be improved for the revision of the paper: Not for MICCAI but generally: Include more datasets in the evaluation. Further clarification is needed for the combination of the losses: Are all losses weighted equally? If not, how were the weights estimated? Table 1 lists results from the best runs. I found this slightly irritating as it seems to give the proposed approach an unfair advantage (Which should not be necessary, based on the table) Please comment on how the configuration selected for Table 1 was chosen. It seems that this had been done after the ablation study, which then could lead to methodical overfitting. In the discussion, the authors claim that l_ca_rnk and l_elr are usually beneficial or not worse than not using it. Judging from the results, the same is true for not using them. The corresponding comments should be rewritten.	Please refer to the previous questions.	This is a very well written paper with some strong results in public datasets. It can improve by including more details about the data.	The paper is quite strong. The presented idea is straight forward and easy to combine with other approaches. A real problem is addressed and the solution is well evaluated. However, while the paper is solid research, I've missed the surprising element or the additional effort to make it an outstanding paper.	Overall, I think this is an interesting paper. The proposed method leverage censored data for survival prediction and show improved results. Please be able to address my concerns.
086-Paper0497	CephalFormer: Incorporating Global Structure Constraint into Visual Features for General Cephalometric Landmark Detection	The paper presents a two-stage framework for 2D and 3D landmark detection. It first detects the coarse landmark by a unet network inserted with CephalFormer Block and then refines the landmark by a sequence of CephalFormer Block, where the Transformer explicitly takes the global structure constraint. The method outperforms the state-of-the-art methods on two public cephalometric landmark detection datasets and a real-patient dataset.	The manuscript proposed a two-stage method with transformer-based neural networks for 2D/3D cephalometric landmark detection. The proposed method utilizes a transformer-based network for coarse predictions of landmarks, and uses self-attention layers to refine landmark prediction combining information of high-resolution image appearance and low-resolution predictions. Moreover, the experimental results validated model performance of the proposed method on three different data sets.	To adaptively encode the landmark's global structure constraint into the representation of visual concepts and avoid large biases in landmark localization. This paper proposed CephalFormer, which exploits the correlations between visual concepts and landmarks to provide meaningful guidance for accurate 2D and 3D cephalometric landmark detection. By evaluation on two public datasets, experiments show that CephalFormer significantly outperforms the state-of-the-art methods.	The method is carefully designed and significantly outperforms the state-of-the-art methods. the use of local group attention and global group reduction attention is an efficient way to capture the long-range dependency and alleviate computational burden at the same time.	* The submitted contents are related to the application of neural networks in the field of anatomical landmark detection in 2D/3D medical images, which is highly relevant to the MICCAI audience. * Experimental results support the claims made in the paper. * The paper is well-organized and well-written.	(1) The authors proposed a general Transformer-based framework that naturally handles both 2D and 3D scenarios for the landmark detection.  (2) The authors studied and innovatively proposed a way to represent visual features and landmarks into a coherent feature space to explicitly incorporate the global structure constraint for accurate cephalometric landmark detection. (3) The method in this paper outperforms the state-of-the-art methods on two public cephalometric landmark detection benchmarks and a real-patient dataset.	It is confused about the landmark embedding. What is the definition? How is it calculated in this paper? Is it a coarse visual feature from the coarse predicted heatmap? To strong statement about the global structure constraints. It hardly tells the technical novelty compared with it proposed in [p1p2]. Some method details are not clear in the Fine-Scale Coordinate Refinement section. For example, what is C of patches P? R^{g}? and label embedding? It is suggested to define a notation before using it. [p1] Structure-aware long short-term memory network for 3d cephalometric landmark detection (TMI 2022). [p2] Structured landmark detection via topology-adapting deep graph learning (ECCV 2022).	* The technique in the manuscript might not be sound. The two-stage method directly predicts the coordinates from the models, which might not be generalizable when applying the models to images with different (e.g., larger, or smaller) field-of-review, especially for 3D CT. And the second-stage model heavily depends on the predictions from the first-stage model. If the first model misses the landmark, it is very difficult for the second one to locate it.	No obvious weaknesses found for this paper.	The code is promised to be public.	The reproducibility of the implementation is not easy to achieve.	No code were made public in the paper.	the landmark embedding should be well defined and evaluated,  Is it a coarse visual feature from the coarse predicted heatmap? the statement about the global structure constraints should be turned down. More details are needed in the section on Fine-Scale Coordinate Refinement.	Are the models in the proposed method trained end-to-end? Or are they trained sequentially? Why not predict the coordinates of the landmarks from the first-stage model? Or predict the heatmap from the second-stage model? If the first model misses the landmarks in the predictions, is it possible for the second-stage model to get them back in the final predictions? In experiments with 3D CT, are all volume resampled to the same size or the same the resolution/spacing? If resampling to the same spacing, how does the model address the situation that image shape is different from model input shape? In general, will more CephalFormer blocks help for better model performance? What is the ground truth for heatmap prediction? Why is cross-entropy loss used here since it has been used classification tasks in the literature? It is unclear about the regression formulation in the manuscript. Comparing GPU memory footprint (which is critical in 3D medical image analysis) with other state-of-the-art methods will help understand model efficiency. How does the typical failure case in the prediction look like? What is the cause of the failure?	Except for the landmark detection, can the LGA and GGRA be used for other purposes? For example, use it for the normal semantic segmentation? I think the author should give some discussion of this topic. The figure 3 seems quite unclear when for comparing with other methods for details. Either the author split it to two different figures and zoom in the landmark area to let the reader see clear how the CephalFormer make it better.	Considering the impressive performance and the careful network design, I tend to accept the submission in pre-rebuttal. However, some details need to be elaborated. The authors claim that they are the first to explicitly model the dependencies between all visual features and anatomical landmarks. But the landmark embedding is not well defined and evaluated, i.e., what is the definition? How is it calculated? Moreover, What is the insight? How does it affect the performance? (without the landmark embedding) Besides, the way to model global structure constraints is very similar to the [p1p2]. Considering the unclear definition and lacking evaluation of landmark embedding, it is doubtful that the current representation supports such a strong statement. The final score is depended on the response. [p1] Structure-aware long short-term memory network for 3d cephalometric landmark detection (TMI 2022). [p2] Structured landmark detection via topology-adapting deep graph learning (ECCV 2022).	The manuscript presented a new neural network based method for anatomical landmark detection. But its technique is not sound. More explanation and description are required for further improvement.	This paper has quite strong innovations and also quite easy for readers to follow. A very nice paper to be accepted.
087-Paper2197	Cerebral Microbleeds Detection Using a 3D Feature Fused Region Proposal Network with Hard Sample Prototype Learning	This paper proposes a single-stage 3D deep learning method for automatic detection of cerebral microbleeds based on susceptibility-weighted imaging (SWI) and the phase images. Compared to the literature, the study removes the need of a second-stage learning for reducing false positives. Instead, it adds a feature fusion module (FFM), as well as a hard sample prototype learning (HSPL) approach. Collective strategies seem to outperform existing models.	The authors propose a novel single-stange deep convolutional neural network that combines a 3D-Unet with the Region proposal network (of Faster R-CNN) as well as a feature fusion module and a convolutional prototype learning-based loss term to detect cerebral microbleeds in SWI MRI.	This paper proposes a single-stage deep learning framework for the automatic detection of cerebral microbleeds. The structure of the net consists of an initial encoding based on the 3D U-Net while the decoding part is merged with a region detection network (YOLO-based).	1) Introduces a 3D single-stage deep convolutional neural network for automatic detection of cerebral microbleeds, which can be time-consuming and subject to error in clinical practice. 2) Includes comparison of the proposed method with 2 approaches without addition of the customized learning components 3) Overall results appear promising	* The authors combine several known techniques in a sensible manner and show that their method is outperforming simpler baselines. * The results may indicate a preferable performance compared to the cited state of the art. * The authors performed quantitative as well as extensive qualitative evaluations employing the visualization of feature maps and probability maps.	Paper merges u-net with object detection architectures to detect and classify all in one step (avoiding the usual detection + false positive reduction step). Paper uses feature fusion model to incorporate contextual information- A hard sample prototype learning module is introduce to gauge the false positives location and use this information in the metric to reduce the number of false positives.	1) The description of RPN is overly brief, and the mechanisms and utility of HSPL is not clear 2) All the image examples shown seem to be SWI scans. How the phase images are used are not illustrated.	* A direct comparison of the performance to methods of the state of the art is not possible because different data sets are used for every method including the proposed one, which was trained and evaluated on an in-house data set. The authors did not compare to a state-of-the-art method on their data set. * The authors state correctly that ratings of expert neurologists are error-prone and that there are substantial inter-rater differences, yet this potentially important issue is not addressed in the paper. The method was (supposedly) trained using labels provided by a single rater (of a group of experts) per subject. * No limitations or future work are discussed or mentioned. * more quantitative evaluations wrt. to the choice of a Unet backbone as compared to the original Faster R-CNN architecture are missing	It seems that the results are obtained with images of patients with CMBs. What would be the performance in healthy brains? Generalisation analysis (i.e. testing in a different dataset)  is not shown. Analysis of the loss contribution is not performed.	No strategy is mentioned	* the method is insufficiently described as crucial details such as the used optimizer, the number of epochs or training iterations and batchsize (if any) are missing. * The reproducibility relies on the availability of the used code and the data set as well.	Difficult to replicate. There are a large choice of hyerparameters which are not explained in detail.	This paper presents a new deep neural network approach for detecting cerebral microbleeds based on a 1-stage implementation instead of 2 stages as seen in the literature. With the demonstrated performance, this method definitely warrants further validation. Additional attention to the following points should help enhance the paper. Introduction 1) It is worth noting that neuroradiologists do not always have trouble to detect the microbleeds, and often do not need to quantify them if not clearly indicated. See end of 1st paragraph, page 2. Method 1) In section 2.1, are the microbleeds incidental findings in the 114 subjects or under some specific diseases? What are the size ranges of the microbleeds? That would impact model performance. 2) In the same section, are there any co-registration procedures applied in data preprocessing, why and why not? What does 'random cropped data' mean - referring to any location of the brain? 3) In section 2.2, the term 'number of lengths' is confusing. E.g. how would a bounding box sized 20x20x20 give 'the number of lengths for each dimension to zero'? A related question, in Fig. 1, do all the feature maps to be fused have the size of 32 with 16 channels? The middle part is unclear. 4) In section 2.3, please explain this sentence: 'due to the sparse and tiny properties of CMBs, the HSPL crops the data based on the rule that the number of crops containing CMBs corresponded to the crops not containing the CMBs'; e.g. how would the two crops correspond to each other? Overall, as mentioned above, this and the RPN sections would benefit from additional information. Experiments 1) The method proposes to use both SWI and phase images but it is unclear how that is implemented, alone or together.  2) (Minor) For figure captions, start with a brief title would help improve clarify.	"* The most promising concurrent method of the references should be trained and evaluated on the author's data set to be able to directly compare performance conclusively. * The limitations of this methods should be shown and discussed. * The wording should be improved, e.g. * stating that with two-stage models ""there is an annoyance"" is hardly objective and valid criticism and objective arguments should be found to motivate a one-stage approach. * The sentence ""Obviously, the proposed net significantly detected the CMBs [...]"" should be rephrased as well. * All important details on the training procedure need to be added. * A cross validation-based evaluation scheme would be beneficial and avoid a possible testset bias"	"In this paper authors aim to detect cerebral microbleeds in MRI images. They fuse ideas from the U-Net and the YOLO-based architectures. A total of 114 subjects including 365 CMBs were used to train & test the approach. The technological proposal seems feasible, although authors needed to add an ""artificial"" loss (ie, concentration loss) to enhance the results. Without this term the results were improved with respect the U-Net, but not significantly. The performance of the simple U-Net using the concentration loss is not shown. I have some issues regarding the dataset. Firstly, since authors used only diseased brains, I'm not sure if the approach will be able to detect non-diseased brains (ie, if the brain is healthy, would the net do not detect any lesion?). An analysis with normal brains should be added in the experimental section. My second main concern is about generalisation. Using images from the same dataset is a very favourable scenario for deep learning algorithms. Authors should test what is the performance of the algorithm in images from a different dataset."	This paper includes several innovations as seen in the strength, and the proposed method can be useful for research in different topics. A few weaknesses need to be addressed.	* A discussion or at least mentioning of the limitations, possible improvements and future work is missing entirely. * Furthermore, a prominent issue is that the results cannot be directly compared to the state-of-the-art methods hence the significance of them is somewhat unclear	The aim of the paper is interesting. However, the experimental evaluation is limited and specific to a single dataset.
088-Paper0402	CFDA: Collaborative Feature Disentanglement and Augmentation for Pulmonary Airway Tree Modeling of COVID-19 CTs	Authors propose to augment features in a siamese-like Unet between healthy and COVID-19 affected CT images. Specifically, healthy and COVID-19 image crops are passed into the neural network and their feature representations are permuted and concatenated at different resolution levels. This approach allows to learn the structure of the airway tree and improve segmentation quality for damaged lungs.	A collaborative feature disentanglement and augmentation framework is proposed for the segmentation of airway trees. This method can jointly exploit labelled clean CT scans and a small amount of labelled noisy CT scans to train a bias-discriminative encoder for the segmentation.	This paper utilizes a disentanglement way to tackle clean and noisy domains, aiming to synergistically learn intrinsic features and independently learn unique features for  airway segmentation of noisy COVID-19 CTs.	Authors propose novel method to introduce structural information from healthy lungs' images to noisy damaged by COVID-19 lungs. The pipeline improves airway tree segmentation quality compared to multiple baseline methods.	1) It is interesting to exploit information from both clean and noisy CT scans, and deal with the domain difference problem of the airway segmentation for these two kinds of CT scans.  2) The disentangle technique is novel and reasonable.	It is an interesting task to consider generalization to noisy domain datasets. Comparison methods are rich to demonstrate the improvements. The total framework is very clear with Feature Disentanglement and Augmentation.	Authors claim about transferring topological structure from healthy to noisy images seems optimistic, especially considering the absence of any registration between noisy and healthy patches and their random flipping/rotation.	The augmentation and training procedure is not described quite clearly.	It's better to visualize the disentangled features to support the idea. The feature disentanglement approach is widely used in domain adaptation segmentation, what's the difference compared with them. For feature disentanglement, domain separation network (DSN) [1] designs shared encoder (similar to Ec in your work) and private encoder (similar to En in your work) to capture sharable features and bias features. A reconstruction network is proposed to ensure the feature completeness, which means that both share and private features are useful (avoiding trivial solutions). However, in your proposed CFDA network, there are no constrains to ensure feature completeness. How to prove the learned features are shared or private. [1]  Bousmalis K, Trigeorgis G, Silberman N, et al. Domain separation networks[J]. Advances in neural information processing systems, 2016, 29.	Authors complete experiments on publicly available datasets. Authors claim to publish code and pretrained models, however no github link is provided in the text.	This work could be reproducible	Sufficient details for reproducibility.	For possible future research direction I would suggest testing this approach to improve segmentation for other similar problems, e.g. Airway tree for patients with pneumonia and/or for noisy images, e.g. low-dose CT.	It could be helpful for readers if the authors could explain more details about the augmentation and the training procedure.	FLA is applied in every layer of the feature extractor. What if it is applied on only one layer or some layers? More visualizations are needed to demonstrate the effectiveness of  disentanglement. What if the network input is clean + clean and noisy + noisy? I'm curious about the performance of them.	Novel method; experiments on publicly available data; thorough comparison with existing solution to the problem	The motivation of this work and the technique about the disentangle are the two major factors make me give the very positive evaluation on this work. However, the unclear description related to the augmentation and training procedure slightly lower the evaluation.	Interesting idea and good technical quality.
089-Paper1601	Characterization of brain activity patterns across states of consciousness based on variational auto-encoders	VAE applied to dFC	In this paper, the authors proposed a dense variational auto-encoder (dVAE) to generate low-dimensional representations which map dynamic functional connectivity to probably distributions. The dVAE model was validated on dataset with five rhesus macaques in different arousal states. A connection-wise receptive field (FR) analysis is then to visualized and interpret encoded trajectories between states of consciousness. The VAE-VIENT framework provides a complete definition of the latent space in terms of wakefulness status and dynamic brain trajectories.	This paper proposes characterization of brain activity patterns across states of consciousness based on variational auto-encoders, via analysis of the resulting low-dimensional latent feature space. The proposed method is experimented on a primate dataset and the results show that it can reconstruct the average pattern of each brain state with high accuracy, generate a latent feature space stratified into a base of brain states, and reconstruct new brain states coherently and stably.	The authors further associated the encoded information with different brain states	This work provides an excellent framework to instill interpretability and exploitability of the latent space, which unveil the biological insights on the states of consciousness. In this model, receptive field analysis leverages the regions to perturb for inflecting trajectories and perhaps restoring wakefulness. The VAE framework for Visualizing and Interpreting the ENcoded Trajectories (VAE-VIENT) is then used to describe the latent space dynamic.	Writing is generally concise and clear. The topic is of interest to the sub-field.	No quantitative comparison to other methods	Firstly, defining more equations are necessary, such as the loss function for dVAE, receptive field analysis.  Secondly, the paper introduces too many contents but fails to explain them clearly. And the writing of the paper is poor. For example, the order for the subfigures of Fig.3 is a mess and Fig. 4 have few explanations in context.  Thirdly, the sample size and the splits of training and testing dataset are not clear. 5-fold cross validation is used on training dataset. What is the test set?	"Contributions of the paper are unclear. It seems the dVAE has already been applied to generative embeddings of brain collective dynamics in [21]. The authors should be more specific about their contributions. Fig. 2: the performance on state 2 is bad compared to others. Please comment. Experiments: ""..., we select the trained weights associated with fold 3."": given similar performance with fold 2 and fold 3, why the latter is selected? Effect of the b regularization parameter: please provide more details about the grid (interval, log scale if employed, etc.). It is better to also present the search results. More description and explanation are needed for Fig. 4(B). The manuscript does not follow the MICCAI format, e.g., the first lines of paragraphs are not indented in Introduction."	no concern	The paper meet the crieria on the checklist.	Reproducibility seems OK.	It seems reasonable by visualizing how different the consciousness state and how different brain states separated in the embedded space. But what is missing here is some quantitative analysis Why did the authors choose to apply VAE to dFC? There are a large amount of debates regarding the sliding window based estimate, e.g. window size, reproducibility, etc. Is it possible to work on the original time series?	Firstly, defining more equations are necessary, such as the loss function for dVAE, receptive field analysis.  Secondly, the paper introduces too many contents but fails to explain them clearly. And the writing of the paper is poor. For example, the order for the subfigures of Fig.3 is a mess and Fig. 4 have few explanations in context.  Thirdly, the sample size and the splits of training and testing dataset are not clear. 5-fold cross validation is used on training dataset. What is the test set?	"Page 2 ""A small number of animals was investigate as it is advised in nonhuman primate studies."": investigate -> investigated Page 5 bottom ""The highest the difference, ..."": highest -> higher"	A solid paper but missing some quantitative results	The novelty of this paper focuses on the description of the latent space. However, more detailed information about the methods and experiments are required.	The contributions are unclear.
090-Paper1593	CheXRelNet: An Anatomy-Aware Model for Tracking Longitudinal Relationships between Chest X-Rays	The authors present CheXRelNet, an anatomy-aware model for tracking longitudinal relationships between chest X-rays. The proposed approach allows the authors to perform diagnosis and monitoring of a patient through comparisons of sequential chest X-rays. The method takes 2 sequential chest X-ray images of a patient and evaluate disease change.	This paper works on monitoring the changes of pathological findings in Chest X-rays (CXRs), i.e., given a pair of two sequential CXRs, to predict whether a finding is improved or worsened. The authors propose to leverage both global information from the whole image and local information from the given anatomical region bounding boxes for the prediction. They use pathology co-occurrence to construct a graph and build CheXRelNet based on the graph neural network to merge information. Experiment results show that the proposed CheXRelNet outperforms the separate global and local baselines on the Chest ImaGenome dataset.	The study proposes using a graph attention neural network to learn the correlations between the follow-up CXRs to track the pathological change. The model considers both local and global visual features. The experiment compares the proposed approach (local+global) with the local-only model and global-only model.	The proposed model utilizes both local and global anatomical information to output accurate localized comparisons between two sequential chest X-rays examinations. The authors came up with the graph construction to capture correlations between anatomical regions from a pair of chest X-rays. The proposed model outperforms baselines.	Clinical relevance: The sequential monitoring of pathological findings in CXR can potentially help patient follow-up. This topic has limited research before. Global-local relation consideration: It is promising if the prediction from the neural network can leverage more local information since the pathology is usually localized in specific regions. The inter/intra-image relation proposed in the paper can provide complementary information for the global decision. The proposed CheXRelNet is based on the graph neural network that can merge information from different regions and from past to current.	The study is nicely explained and formulated.	There is no ablation study for the classification module.	"Not significant improvement compared to the Global baseline: From Table 2, we can see that the proposed CheXRelNet slightly outperforms the Global baseline (0.68 vs 0.67 for All). Lack of statistical analysis: Given such a small gain, the authors should give some statistical analysis, e.g., confidence interval, to verify the effectiveness of the proposed method. Limited interpretability: A critical factor in using local information is that it can provide precise localization, which can largely help clinicians understand the outcome. The authors also claim that the CheXRelNet can output accurate localized comparisons (contribution 1). But no such demonstration is provided. The result is still largely performance-driven. The way to build the graph: The authors use pathology co-occurrence to build the graph. However, this process is somewhat ""hard-coded,"" i.e., a manually defined threshold is used to determine the connectivity."	I could not see any major weakness in the study.	The paper seems to be reproducible since it is written pretty clearly, the authors intend to release the code and an open-source dataset was used for evaluation.	The authors have shown assumptions/ implementation details, etc. The evaluation is on a public dataset [26]. The authors are encouraged to make code publicly available.	the study is developed and test on a publicly available dataset. the implementation details and parameters are provided in the manuscript. The codes did not shared.	The authors present CheXRelNet, an anatomy-aware model for tracking longitudinal relationships between chest X-rays. The technical novelty of the paper is at a good level. Evaluation is reasonable but lacks an ablation study for the classification module. The paper is well written and easy to reproduce. The authors investigated an important problem in the field of medical image analysis and achieved state-of-the-art results.	For the graph construction, the authors should give some ablation study or discussion on how to choose the threshold. I also wonder if the authors can propose some soft version to merge information among regions. For the comparison, the proposed global-local network needs more computation and parameters compared to the plain Global model. So it is not clear whether the performance gain is from the global-local consideration or more parameters. I recommend that the authors think of a way to justify the proposed network's effectiveness further. Zero-shot evaluation results are presented for comparison. The authors can show more details for the inference during zero-shot evaluation for completeness. The authors use the ResNet101 autoencoder [26] to extract features, and the autoencoder is pre-trained on several imaging datasets. I am not clear if the features are extracted from the encoder. For the two off-diagonal k by k blocks, the authors can further clarify the explanation, e.g., with the s, t index range. It is a bit hard to understand at first glance.	nice study.	Technical novelty, reproducibility, and results achieved.	The monitoring topic and global-local consideration are interesting for CXR. But, there exists some weakness in the method design and experiment results.	nice study.
091-Paper1588	ChrSNet: Chromosome Straightening using Self-attention Guided Networks	The proposed method straightens images of curved chromosomes, to allow easier analysis of their properties. It applies a DNN, self-attention layers, a U-Net refinement layer, and a loss function tailored to the use-case. Ironically, a key finding (though not acknowledged as such) is that the extra architectures beyond the basic DNN have little-to-no effect (table 2). To the authors' credit, they do not obscure this finding.	The paper proposes a novel chromosome straightening approach using self-attention guided networks. The method combines low-level details and global contexts to recover banding patterns. This study creates mappings between straight and curved chromosomes for chromosome straightening.	this method introduces a new neural network based machine learning method for chromosome straightening, with good contribution of its machine learning method and its biological impacts.	The motivation is well-stated.  The prior work is well and succinctly covered (I do not have expertise to assess if it is complete). The loss function is well-described and thoughtfully incorporates domain-specific metrics. The clarity of the paper is good.	Novelty: This study creates mappings between straight and curved chromosomes for chromosome straightening for the first time. Experimental evaluation: well conducted and presented. Much appreciated the comparison.	the beauty of this paper is about formulating an important biological application into a pixel-to-pixel prediction problem, solved by a simple but effective learning paradigm.	The use of self-attention is not well-motivated. In particular, it is not clear that CNNs' inability to model long-range relationships matters in this use-case, and the division of spatial patches into a 1-D sequence (to enable application of attention methods) seems like a backwards step - throwing out highly salient spatial (neigbor) relationships.	I would suggest to the authors to improve the introduction so that it provides a deep overview of the study. In fact, I think it is unclear what unique challenges are associated with this task.	One major issue is that for real-world validation, it is not clear if ground truth is available or not. I am a little confused. Meanwhile I have two minor comments: Fig 2 is a little bit contradictary to the results in Table 1, or at least may not fully represent the evaluation in Table 1. For example, to my eyes, the Unet result on synthetic data seems to have better S score and L score than ChrSNet, at least for this example. Why Unet performs so badly on real-world example, but does a very good job on synthetic data?	The main text makes no mention of code being available.	The reproducibily is adequate. Perhaps, the authors could give more details regarding the key parameters involvedin their method.	In the authors' answer to the reproducibilty checkliost, the authors claim to include all the code. But, I don't see any placeholder in the manuscript.	"The loss function is well-described and thoughtfully incorporates domain-specific metrics. Reading the results, I wondered whether length is an important metric - does it affect the later analysis of chromosomes, or is the most important point clear banding? As mentioned above, is the use of attention networks indicated for this use-case? (a) are there any salient long-range relationships that a CNN will not encode? (b) Putting the patches into a sequence (pg 4) seems to lose ground, since relevant spatial neighbor relationships are lost. In the results (table 1), evaluating length and straightness is arguably a bit unfair, since this is exactly what ChrSNet explicitly optimizes. LPIPS is a more neutral metric - does it relate meaningfully to the needs of later analysis? (I don't know). On a related note: Are there metrics about clarity of banding that derive from the needs of later analysis, that would make good assessment metrics? Providing uncertainty intervals in the tables is well done - a vital element for assessing the findings. A key finding, seen in table 2, is that the various added-on architectural blocks do not add value vs using just RG (all accuracies vary by less than 1 std dev). It seems that this should be discussed. In fact, I would argue that the paper could be restructured to decrease the description of the other blocks, because they were experimental deadends (in the best sense) - worth reporting, but in the context of ""ideas that did not work out"". Miscellaneous: -deep features: what does this mean? -a type from 1 to 12: what does this mean (unclear to a domain outsider) Fig 2: what is the stippling artifact in UNet Real-world (bottom row, second from right)? Fig 1: y is supposed to be a straightened version of x. Perhaps modify the figure to show this, for clarity. Miscellaneous typos: complimentary -> complementary flat it into -> flatten it into agained -> again slop -> slope -Unet -> U-Net, or UNet the from -> from -parts[19] -> add space A careful review of grammar would be good (eg some definite articles are dropped). As an aside: if English is not your first language, I congratulate you on a far better command of English than I will ever have of your native tongue, whatever it is :)"	Dear Authors, I read your manuscript with great interest and I found it of very good quality. Also the results are quite impressive and opens the field for further improvements. I congratulate with you also for the conduction of the experimental evaluation: very clear and well presented. I have no major concerns. My only suggestion is the following: I would suggest to improve the introduction so that it provides a deep overview of the study. In fact, I think it is unclear what unique challenges are associated with this task.	Both predictions from Unet and ChrSNet are more or less very noisy, comparing to real images. I think adding some adversarial training to the learning step would help further imcrease the performance. For Fig 2, I would suggest report the S score, L score and LPIPS for the example. In general, I would suggest to do more investigation on why Unet performs so badly on real-world example, but does a very good job on synthetic data	Much of the paper describes architectures that were shown by the experiments to be of little value, and the lack of added value of certain architectures is not discussed. While negative findings are useful to report, the paper should be shaped accordingly.  There were quite a few typos.	Novelty of the proposal, and novelty of the task. Extremely well conducted experimental evaluation.	The paper is good. I am willing to change from 6 to 7 or 8, if the AC can verify on the code release and reproducibility.
092-Paper2297	CIRDataset: A large-scale Dataset for Clinically-Interpretable lung nodule Radiomics and malignancy prediction	The authors propose an end-to-end trainable deep learning architecture for combined lung nodule malignancy classification as well as vertex-wise spiculation and lobulation classification from thoracic CT scans. The architecture is a slightly extended version of Voxel2Mesh. The extension mainly focuses on adding the classification heads. Moreover, the authors provide lung nodule segmentation masks, spiculation/lobulation annotations, and area distortion maps for the publicly available LIDC and LUNGx data sets. Those data sets are also used to quantitatively evaluate the proposed method.	almost 1000 annotations of lung nodules for open datasets, including segmentation masks and classification w.r.t. spiculation approach to segment lung nodules, classify speculations, and estimate malignancy in one network architecture	The authors propose a method combining shape features and deep features (both based on Voxel2Mesh) in an end-to-end model for the automatic prediction of lung cancer malignancy. The motivation for the use of Voxel2Mesh is to extract spiculations on the lung nodule surface, which are predictive of malignancy, without smoothing the contours. The authors therefore extend the use of Voxel2Mesh to a multi-class problem to segment nodules, and classify vertices into 3 classes: nodule, spiculations and lobulation. Spiculations annotations are performed on the public LIDC-IDRI dataset for training/validation (annotations and code will be made publicly available upon acceptance). The models are evaluated on one internal and one external test set.	Novel idea for lung nodule malignancy classification Special focus on challenging vertex-wise spiculation and lobulation classification Derived masks and annotations will be made publicly available	I like the idea of the presented approach: very clean solution and includes explainable features for radiologists. Plus: the application of lung nodule analysis is clinically very relevant.	The annotated spiculations will be shared publicly. The method is simple and makes good use of the existing Voxel2Mesh method with a good design (multi-class Voxel2Mesh and malignancy prediction). Mesh classification and final malignancy prediction results are reported, using only mesh features vs mesh + deep (encoder) features	Extensions of the original Voxel2Mesh architecture appear to be minimal (limited novelty on that end) Results are hard to assess as no baseline numbers or comparisons are provided Description of the architecture and the training process could be improved	Evaluation not convincing: some smaller issues (see below in comments), and malignancy prediction results look not competitive with state of the art (e.g. NoduleX)	There is no comparison with SoA best results on the datasets.The benefit of the deep features is not evident from the results, particularly on the external test set.	The authors will make the masks/annotations that were used publicly available. The same is true for the code and pretrained models. Most of the parameters that are important for training are given. I, therefore, believe that the results presented will be reproducible.	Apparently, the authors have not understood how to fill out the checklist: all questions are answered with yes, yet most of the information is missing in the paper (e.g. only everage values are given for comparison and no variation, no tests for statistical significance, ...) On a positive note, datasets and code will be released, which should answer many of the open questions.	Public dataset and annotations will be shared, together with the code and model weights.	"While I really like the contribution, I see two major problems with the paper in its current form: (1) The actual novelty is hard to assess as the paper is not really clear on how much the proposed architecture differs from a vanilla Voxel2Mesh network. It seems like that the major difference between Voxel2Mesh and the pipeline used here, is the addition of the classification heads to perform (global) malignancy classification as well as vertex-wise spiculation and lobulation classification. I don't think that this would actually be a major problem as the proposed application scenario seems to be novel, but the authors should make this more clear then. I am also wondering how the Voxel2Mesh part is trained. Is the whole pipeline trained end-to-end using the BCE loss? That seems to be unlikely, but I am not able to find additional information in the paper. It would be, for example, interesting to learn how the authors accurately segment the spiculations, which will most likely be smoothed out by Voxel2Mesh. Is this achieved by just removing that part of the loss? Is that what the authors refer to in Sec. 2.1 when saying ""We did not apply regularization terms to the deformations to capture their irregular and sharp surfaces."" (2) If find it hard to assess the results of the quantitative evaluation. The authors neither compare their approach to any baselines nor do they discuss their numbers wrt to existing work. While I understand that (most likely) no comparable approaches exist for vertex-wise spiculation and lobulation classification the performance of the malignancy classifier should have been discussed and compared to other methods. The authors indicate in Sec. 5 that ""Although the segmentation performance was better than the previous deep learning methods, [...]"". However, I cannot find any comparisons to other DL methods in the paper. Minor comments: Fig. 2 is not referenced in the paper and at least parts of its caption should be moved to the main text of Sec. 2 The first two sentences of Sec. 4 are not really results In terms of reproducibility it would be better to directly mention the GPU(s) being utilized instead of saying that ""Nvidia HPC clusters"" were used"	"""Voxel2Mesh for multi-objects, this paper single object"": unclear what this means or what the challenge is Which 32 mesh features are used? Sec 3.1: unclear how classification of peaks was done - by radiologists? Generally good description of experiments with many details. Sec 4, about Table 2: ""On the external LUNGx testing dataset (N=70), the hybrid voxel classifier model does better in terms of both the metrics for all three classes."" -> Jaccard index for nodules is actually worse. Table 3: LIDC-PM results are surprisingly good, better than on training set?!? LUNG-X results much worse, this might indicates open issues, would be nice to have a discussion on this. No comparison with other state of the art methods wrt malignancy prediction. Fig. 3 not that convincing, unfortunately - is this a typical example? Sec 5: ""the segmentation performance was better than the previous deep learning methods"": I could not find a comparison in the paper."	"I think the title is not well suited to the paper. There is no mention of lung nodule and it is not a toolkit that is proposed. Although mentioned, it is not clear from the beginning that the method is end-to-end. I first thought it was a mistake. It could be worth briefly mentioning early in the paper how it is trained end-to-end. I would tend to contest the motivation in the abstract ""tend to smooth out ... making subsequent outcomes prediction difficult"" Deep models can capture it, not necessarily needing precise segmentations.  Some typos to fix: e.g. ""to classy"" Maybe the Voxel2Mesh part, that is most of Fig. 2, could be evidently split in the figure from the novelty that is taking the encoder features for the Malignancy prediction. The results seem promising, yet I agree with the limitations stated by the authors on the vertex-level classification. (This comment could be merged with the Limitations and Future Work section)"	I like the idea, but I'm not entirely sure (1) if there is enough novelty and (2) if the evaluation supports the author's claims.	Not an easy decision, but in the end I did not find sufficient evidence for the impact of the new approach, as performance was a mixed bag overall.	The method is simple and well designed. The results are difficult to assess without comparison with state of the art methods.
093-Paper1320	Class Impression for Data-free Incremental Learning	The author presents a data synthesis strategy using learned neural network parameters for data-free incremental learning. Multiple loss functions are introduced to mitigate the catastrophic forgetting problem. The proposed scheme is applied in echocardiogram view classification and demonstrates its efficacy.	The authors propose a data-free class Incremental learning method, and they show that the proposed framework which combines the pseudo images generation and three novel losses can provide the accuracy improvement for sequences of medical images classification tasks.	The authors propose Class Impression, a novel data-free class incremental learning framework. In Class Impression, instead of saving data from classes in the earlier tasks that are not available for training in the new task, they synthesize class-specific images from the frozen model trained in the last task.	A. The authors suggest to initialize the neural network with the average batch value of the input. The proposed scheme is an interesting attempt that is distinct from the existing deep inversion method. Through a comparative study, the paper demonstrates that the initialization contributes to the classification accuracy B. The performance of a neural network greatly depends on the scaling of multiple loss functions. The presented automatic loss weighting method mitigates the imbalance problem of multiple losses.  C. The authors conduct extensive comparison experiments with diverse incremental learning methods. The proposed class impression (CL) method outperforms existing methods including LUCIR, ABM, and CLBM. The ablation study is presented and demonstrates the efficacy of using multiple loss functions.	The authors apply data-free rehearsal-based class incremental learning to the medical image analysis task. The implementation seems interesting. This paper leverages three additional loss terms, which are specifically designed for medical image analysis, to address the catastrophic forgetting problem. One of them, named margin loss, is the first time to be applied to data-free class incremental learning. Large improvements have been made on a datasets, which demonstrates the effectiveness on this task.	The authors propose novel 1) cosine normalized cross-entropy loss for imbalance issue, 2) margin loss to encourage robust decision boundary, and 3) intra- domain contrastive loss to alleviate domain shift, which empowers Class Impression in addressing the catastrophic forgetting problem. - They conduct extensive comparison experiments and ablation analysis on the echocardiogram view classification task to demonstrate the efficacy. Class Impression out-performs the SOTA methods in data-free class incremental learning with an improbable gap of 31.34% accuracy in the final task and get comparable results with the SOTA data-saving rehearsal-based methods.	A. The incremental-learning method is assessed with tasks of classifying five views of echocardiography. Such an echocardiogram view classification task has a limited number of classification classes (<=5). The efficacy of the class impression to complicated tasks is concerned.  B. The clinical effectiveness of the echocardiogram view classification is limited. C. The exact value of quantitative assessment is not provided.	"There are parts where it is not clear if the authors are using something ""off the shelf,"" or if it is part of their contribution. For instance, what is the novelty of Class Impression compared to DeepInversion? It seems they have exactly the same optimization objective. Is the Class Impression specifically designed for medical images? What new challenges does the medical image introduce compared to the popular datasets widely used in computer vision, such as CIFAR, ImageNet, and so on? And how does the Class Impression address them? The organization of the paper is incomplete. Since the paper contains various existing methods, the authors should provide more details in the literature review and background. Besides, the experiments section lacks the necessary discussion to analyze the results. The evaluations are not solid. The four-task setup containing only five classes may be too simple to cause the catastrophic forgetting problem. The longer sequence of learning tasks with more classes should be used to evaluate the effectiveness of the method. From the current experiments, I still doubt if using pseudo images in task T, which are generated from the model in task T-1, for replaying can avoid catastrophic forgetting. They also should double-check the bold statements and references. For instance, the authors claim that Smith et al. train a generative model to synthesize images without considering preserving class-specific information. As far as I am concerned, they not only did not train a generative model but also improved the DeepInversion which is used in this manuscript. Why are the images generated by Smith et al.'s method look so bad as shown in Fig. 1. (d)? Ref: Smith, J., Hsu, Y.C., Balloch, J., Shen, Y., Jin, H., Kira, Z.: Always be dreaming: A new approach for data-free class-incremental learning. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. pp. 9374-9384 (2021)"	This is a strong paper. There are no main weaknesses detected.	Author agrees to disclose code and thus the result will be reproducible.	Good. All methods used for the proposed framework are depicted clearly and noted with appropriate references.	Highly reproducible.	A. The efficacy of the paper would be emphasized with a broader range of applications in medical incremental learning. For example, breast lesion classification, or chest x-ray classification could be a good application to show clinical effectiveness.  B. Additional ablation study would help demonstrate the effect of the mean initialization method, and data synthesizing scheme.	Please double-check the correctness of reference [22] in the manuscript. The experimental evaluation could be expanded, especially for the ablation studies.	This is a very interesting approach with real potential in medical imaging applications due to privacy regulations of data storage.	The proposed class impression is an interesting approach that mitigates catastrophic forgetting problems in incremental learning. The experiment demonstrates that the class impression outperforms existing schemes. However, the clinical effectiveness of the experiment is not sufficiently demonstrated  	The decision are mainly  made according to the novelty of the proposed framework and the organization of this paper.	It is challenging to deploy class-incremental learning for medical image analysis. The proposed approach is novel. To the best of my knowledge there are no existing data- free rehearsal-based class incremental learning work on deep neural networks specifically designed for medical image analysis.
094-Paper1363	Classification-aided High-quality PET Image Synthesis via Bidirectional Contrastive GAN with Shared Information Maximization	The paper proposed a framework for low-dose PET reconstruction. The method consists of three components: a domain alignment module to regularize the consistency of the shared information between low-dose and standard-dose PET, a contrastive learning strategy to enhance domain-independent information, and a classifier to ensure the accurate diagnosis-related features.	In this paper, the authors hypothesized that the abundant shared content and structure information between LPET and SPET can help improve image synthesis performance. Based on this, the authors proposed a BiC-GAN framework that contains a master network and an auxiliary network to extract shared information from LPET and SPET. Both networks implement intra-domain reconstruction and inter-domain synthesis tasks, aiming to extract shared information from LPET and SPET domains, respectively. Additional contrastive learning and classification tasks were also used to boost the performance. The proposed method achieved results comparable to the state-of-the-art methods.	The paper studies how to generate high-quality standard-dose PET synthesis from low-dose PET images. The authors propose a bi-directional contrastive generative adversarial network (BiC-GAN), including a master network and an auxiliary network, for intra-domain reconstruction and inter-domain synthesis tasks. Moreover, a domain alignment module is designed to maximize the mutual information from two domains. Also, the mild cognitive impairment (MCI) classification task is incorporated into PET image synthesis. The authors demonstrate the robustness of the method compared with the state-of-the-art qualitatively and quantitatively.	The paper is well-written and well-organized. The motivation of the method is very clear and well-explained. The method is relatively novel.	The idea to use shared semantic content and structure information between LPET and SPET in training is novel. Extensive evaluation with comparison with the state-of-the-art methods.	The paper is well organized, and the motivation behind low dose PET noise reduction is well justified. The work introduced a novel end-to-end bidirectional contrastive GAN (BiC-GAN), including a master network and an auxiliary network, for intra-domain reconstruction and inter-domain synthesis tasks. The authors proposed a domain alignment module to maximize the maximum mutual information between the two domains. The work achieves better results compared with the state-of-the-art methods quantitatively and visually.	The evaluation needs improvement. The dataset is too small. The performance improvement is subtle.	The dataset in this study is small, with only 16 subjects in total. The authors also have to do 2D image processing to obtain more 2D slice samples, which is a limitation. The proposed method is likely not statistically different from the SOTA AR-GAN. It would be informative to discuss the pros and cons of the proposed method compared with the SOTA AR-GAN in different aspects, or how the proposed method can outperform AR-GAN in certain scenarios.	This paper combines several existing technologies to form a new framework, thus the novelty is not significant. This is OK if the performance is extremely expressive, or the analysis is sufficient. Although the quantitative and qualitative performances of the proposed method described in the paper have shown better than state-of-the-art methods, the performance gains seem marginal, as shown in Table 2 and Figures 2,3. The magnitude of the improvement of the proposed method remains unclear. Although the overall architecture is novel, each individual components are largely inspired by previous works. To overcome marginal improvements, the authors should compare more recent works. Furthermore, the experiments on more benchmarks should be presented to verify robustness. Lack of enough ablation study analyzes the contributions of individual components to the final performance.	Code is not provided.	The method description is clear.	Will code be available? It is not mentioned that code is made available.	"The quality of the low-dose PET in Fig.2 is actually quite good. It leads to the question whether this application is meaningful. This can be proved by either the similar performance on a much higher dose reduction rate (visually worse image quality) or the comparison of the diagnosis (e.g. classification of NC vs MCI) on low-dose and each synthesized standard-dose PET. The model is quite big for a dataset of 16 subjects. Any overfitting problem? In table 1 and 2, the results show limited improvement. Statistical tests are needed. Since the classifier of NC vs MCI is already a part of the model, it would be natural to show the diagnosis of low-dose, synthesized, and ground-truth PET, and compare them. In Fig.2 and Fig.3, please add zoom-in image. It's hard to differentiate the quality between methods. In introduction, it says current methods ""do not take into account the applications of the synthetic images in analytical and diagnostic tasks"". This paper actually used the similar method. Ouyang, Jiahong, Kevin T. Chen, Enhao Gong, John Pauly, and Greg Zaharchuk. ""Ultra-low-dose PET reconstruction using generative adversarial network with feature matching and task-specific perceptual loss."" Medical physics 46, no. 8 (2019): 3555-3564."	"The proposed method obtained similar results compared with the current SOTA method AR-GAN, which is good, but it would be better if the authors can discuss the pros and cons of the proposed method compared with the SOTA AR-GAN in different aspects, or how the proposed method can outperform AR-GAN in certain scenarios. In Eq.(2), in the last term, I think it should be D_M(l, s_syn) instead of D_M(s, s_syn), according to Fig.1. In the ""Contrastive Learning Module"" section, the local features are processed by a 3x3 conv kernel. Since the local feature spatial dimension is 2x2, why use a 3x3 conv kernel? The symbol ""C"" is referred to both the classifier (in Section 2.4) and the positive constant in Eq. (6). I suggest the author to change one of them."	"This paper combines several existing technologies to form a new framework, thus the novelty is not significant. This is OK if the performance is extremely expressive, or the analysis is sufficient. Although the quantitative and qualitative performances of the proposed method described in the paper have shown better than state-of-the-art methods, the performance gains seem marginal, as shown in Table 2 and Figures 2,3. The magnitude of the improvement of the proposed method remains unclear. To overcome marginal improvements, the authors should compare more recent works. Furthermore, the experiments on more benchmarks should be presented to verify robustness. Although the overall architecture is novel, each individual components are largely inspired by previous works. Lack of enough ablation study analyzes the contributions of individual components to the final performance. The authors seem to have missed some relevant literature. Specifically they miss out on several relevant citations, e.g. "" CT Super-Resolution GAN Constrained by the Identical, Residual, and Cycle Learning Ensemble (GAN-CIRCLE)"", """"Unpaired brain MR-to-CT synthesis using a structure-constrained CycleGAN"", and ""CycleGAN denoising of extreme low-dose cardiac CT using wavelet-assisted noise disentanglement""."	The method is ok.	Novel idea and strong evaluation.	Originality: Generative adversarial network and contrastive learning have been investigated in PET image synthesis. However, this paper combines them for PET analysis tasks. Novelty is limited. Quality: The paper introduces the model and provides few experimental evaluation. Especially, another contribution claimed by the authors is the loss function. However, the authors did not provide complete experimental results to analyze the impacts of each components of the loss function. Clarity: The paper is well-structured and mainly well-written. Significance: The ideas used here could be utilized in different medical image analysis tasks. The paper studies how to generate high-quality standard-dose PET synthesis from low-dose PET images with a bi-directional contrastive generative adversarial network (BiC-GAN).
095-Paper0431	Clinical-realistic Annotation for Histopathology Images with Probabilistic Semi-supervision: A Worst-case Study	This paper proposes a new semi-supervised segmentation method for histopathological slides. The originality consists in annotating only a single polygon on the major tumor site and give a rough estimation of the tumor/tissue area ratio.	"This paper proposed a annotation strategy for histopathology images. A semi-supervised learning method with probabilistic week supervision is designed to verify its effectiveness on Camelyon 16 dataset under ""worst-case"" setting. The idea seems interesting and refreshing."	This paper studies an interesting topic in clinical-realistic annotation for histopathology images. It poses several key components that are neglected but could be critical in histopathology image annotation, including localization and boundary delineation. To address them, this paper proposes different annotation strategies for slides with different annotation burdens. Especially, this paper proposes a probabilistic weak-supervision training pipeline, which is novel to me. The overall results are promising. In particular, different baseline methods, e.g., MixMatch or SimCLR can be improved by the probabilistic ratio supervision with a considerable margin.	The proposed architecture dealing with both a rough polygon containing positive samples and just an estimation of the tumour/tissue ratio is very original and could be useful in clinical routine	This paper proposed new annotation strategy that may be insightful for clinical annotation. Designed semi-supervised learning method proved its effectiveness on Camelyon 16 under worst-case setting.	"Well-organized and well-motivated ideas. This paper starts by posing domain-specific limitations and challenges in the pathology image annotation process and derives its motivation and idea along with the text. I can easily follow the main idea and find it solid. The proposed probabilistic weak-supervision training method is novel, to me. Using the sorted prediction scores to match the expected labels is a clever way to exploit the ration information. Although it does strongly rely on good stage-1 training (in Table 2, ""skip"" stage-1 fails), the proposed probabilistic framework can improve the upstream model's performance on the downstream task of interest, which is acceptable to me."	"The method could be compared to more classical semi-supervised approaches  The tumour/tissue ratio is not so easy to estimate, a study on the influence of a ""bad"" estimation could be given"	"The assumption behind the designed annotation strategy (""easy"" and ""hard"" figure) is not very theoretically reliable. 2.The strategy is only evaluated on Camelyon 16 with small number of samples and limited tumor types, thus the generality is limited. 3.The method is only compared with limited self-supervised and weakly-supervised methods. It would be better to have comparison with latest semi-supervised methods, such as FlexMatch, SimMatch, etc."	"If the authors would address my concerns below, I would further adjust my score. Major concern: estimation of ratios and multipliers.  1) How do you obtain the 4-point or 10-point polygons for experiments? Is the proposed method sensitive to the quality of polygons?  2) Estimating the ratio for small/medium focal tumors seems to be straightforward by computing poly_area/otsu_foreground_ared. But how do you estimate the multiplier for polygon area v.s the whole tumor area? An accurate estimation of the ratio, in my opinion, is the key to the success of the proposed method. Using the ground truth label to directly obtain this information can be seen as label leakage, but I'm okay with it if the leakage is not much. Authors should provide the information on the distribution tumor_area v.s. tissue_area, or the distribution of the number of small/medium and large tumors. This helps to inspect the degree of leakage. Missing details about implementations.  1) MC Dropout. Where do you put the dropout layer, and how many times of MC dropout are performed? How do you estimate the uncertainty U? Is it the standard deviation of multiple passes? How much time is increased for Probabilistic + feedback (use MC Dropout for feedback)? There are way more passes for MC dropout. And the gain seems to be marginal, (0.02 from table 2)  2) Training details in each step. The paper has not presented the training details, e.g. how many epochs SimCLR / MixMatch are pre-trained and how many epochs those models are tuned, either using patch Camelyon or other annotation.  3) Consider specifying what is stored in the sample queue. Are they images? Besides, is Y+ a vector full of ones (eq 3)? 4) Will the code be made available? Minor issues: The tumor region ratio indicates tumor_area/slide_area or tumor_area/tissue area. These two concepts both exist in the text, e.g., ""while large lesions can occupy more than 50% of the slide"" and ""the tumor region occupies 51%, 3%, and 0.01% of the foreground tissue"". Consider clarifying it more. Comparison between MixMatch-variant [8]. In table 2,  the last line, is it equivalent to MixMatch+Polygon+Probabilistic+feedback? Is there any difference in stage-1?"	Codes, data and trained networks are given. Very good reproducibility of the study	This paper gives detailed explanation of its method and dataset. However, in order to reproduce this work, further details about experiment setting (such as choice of hyperparameters) and data sampling results should be provided.	More details about each training step, e.g., pre-training, fine-tuning, MC dropout configuration, need to be clarified for reproducibility. The authors checked almost all boxes in the checklist, in which it says the code would be made available. If that is the case, reproducibility is not an issue.	As already stated, I think that the paper presents a very clear, new and original approach for semi-supervised segmentation of whole slide images. The results seem to prove that this approach provides best results than others.  I would have like to read a more complete study on the influence of the parameters of the method and especially the ratio given by the user on tumour/tissue. What if the pathologist underestimate or overestimate this ratio? Maybe a second experiment on another publicly available dataset would also strenghthen the evaluation of the method.	"I wonder the assumption (""hard"" and ""easy"") in Figure 1 is theoretically supported by pathologists. Can you provide references or further explanation? 2.The experiments were conducted on a portion of Camelyon16, which limits the generality of the strategy. Would it be possible to experiment on more datasets? 3.The method is only compared with limited self-supervised and weakly-supervised methods. It would be better to have comparison with latest semi-supervised methods, such as FlexMatch, SimMatch, etc. 4.All the tables illustrate FROC, and it would be better if annotation time cost of the methods is also listed in a table for comparison."	Please refer to the weakness section. In addition to that, I suggest doing another ablation that uses the full amount of WSIs in Camelyon16 to pre-train the SimCLR encoder. This also meets the realistic scenario -- you don't have to pre-train only on labeled data.	Original approach Good results Interesting topic	The method introduced an interesting annotation strategy for histopathology images, which is proved effective on Camelyon16 compared with a few self-supervised and weakly-supervised methods.	The well-motivated idea. The novel probabilistic ratio-based weak supervision training pipeline. However, an explanation for my Major concern (2) is needed for my final rating.
096-Paper2314	CLTS-GAN: Color-Lighting-Texture-Specular Reflection Augmentation for Colonoscopy	The paper presents a method for creating synthetic colonoscopy images using CT imaging and that the authors term a CLTS-GAN.  the CLTS-GAN is trained using an unsupervised approach, it allows texture and lighting to be disentangled.  The authors also demonstrate the utilty in using the CLTS-GAN to improve segmentation in colonoscopy images	This paper proposes a one-to-many image-to-image data augmentation model for colonoscopy whereby image attributes such as colour, lighting, texture and specular reflexion are controlled by means of 1D vectors and 2D matrices.	The authors propose a method for the augmentation of colonoscopy images by changing color, lightning, texture and specular reflections of the original images which is realized by controlling noise parameters for color/lightning and texture/specular, respectively. The framework maps optical colonoscopy images (OC) to virtual ones (VC), extracting color/lighting and texture/specular feature representations (G). A second generator (F) generates OC from VC using color/lighting and texture/specular noise. The framework is trained end-to-end using a combination of GAN loss, cycle consistency loss, and additional regularization losses. The authors evaluate their method in the context of data augmentation for three datasets.	impressive augmentation results that create realistic looking colonoscopy images, generated by using a image to image translation framework that can transfer textures from real colonoscopy images to synthetic ones created by 3D modelling. the authors present a clear clinical task and 2 routes for use in training and in improving segmentation methods well thought out unsupervised method making use of a lot of useful concepts for applying textures and lighting to colonoscopy images adversarial loss cyclic loss identity loss novel architecture allow lighting and texture disentanglement	Control of features for data augmentation using the proposed approach generating good results Evaluation of performance on other models by introducing augmentations resulting from this work showing improved results Code will be available upon publication	The authors present a sophisticated and flexible framework for manipulating colonoscopy data. The proposed framework allows a user to gain control over the relevant image capture settings after acquiring the videos and map from one image to many other appearances. Not only the data augmentation but also the training application has great potential.	training and implementation specifics are limited. what preconditioning was done?  What frameworks were used to impliment the methods the algorithm training explanation could be clearer, particularly figure 1 the main advance is that texture and lighting are effectively transfered onto virtual colonoscopy frames, this is small advance on the styleGAN.  The novelty above that could be better explained	Overall the mathematical description supported with Fig. 1 intuitively is correct but I think there are few mistakes that make the reading very confusing. I will list some of this in the detail comments below. It is unclear from the paper how the user will get satisfactory results when sampling randomly from a uniform distribution While differences can clearly be appreciated for colour and lighting, attributes such as texture and specular reflexion are not noticeable in the examples shown in the paper. I think the paper lacks a quantitative approach to measure this.	As the augmentations are performed on a frame-level, temporal consistency of the frames cannot be guaranteed. Therefore, the augmentation technique is only valid for frame-based downstream tasks. Especially for training applications, the temporal coherence of the generated video is crucial for realism. This is only rudimentally discussed in the paper. The proposed framework has a lot of potential; however, the authors only evaluate one specific use case of the method. A more extensive evaluation would strengthen the work. Furthermore, a discussion of the different options how to use the framework for augmentation (last comment in the detailed feedback section) and also for training applications and colonoscopy simulation (using the VC data as reference) would clearly improve the presentation of the work. Even though the method is very interesting and relevant, the presentation of the proposed method is lacking clarity (also because there are many different options how to use the framework and the presented approach is very complex). Instead of showing a lot of different applications of the framework without context, it might have been better for the limited space of a MICCAI submission to focus on one application and explain it in more detail.	The dataset and the code will be released.  The methods have been well described.  So the work should be reproducible.  The dataset is small sot the performance may differ with different imaging.	Source code will be available upon publication	The authors promise to publish the code and pretrained models upon acceptance. With only the explanations from the paper it would be very complicated to implement the presented approach.	figure 1: F and G should have arrows in and out.  It can be confusing to understand the flow for simulation of colonoscopy do the augmentations need to conform to physical constraints?  if a video fly through of a virtual colon is made do the video and individual frames look to be a coherent set?	"When creating the VC data similar to [18], is this by using Blender? if not please describe the details of how this is done Also, it is unclear what the inverse square fall-off property is for or where you modify this. The reason there but what does it mean more accurately and why Mathematical description, Fig. 1 and text: In second sentence of Sec 4, isn't it related to G rather than F? and later on F instead of G? Define G_im, G_cl and G_ts in Eq 2 ""The discriminator has the network use random ..."" is very confusing After Eq 7, ""G may ignore Z_ts"" is confusing since G only takes OC images as input Define I in L_text ""Two VC images are passed to F"" is confusing since F only receives one In the definition of L_t, shouldn't it be F rather than G? F does receives as inputs the image, z_cl, and z_ts When setting values to parameters, did you mean lambda cyc rather than lambda T? But overall, how are these values found/set? are there any experiments to support their choice? Define VC at the beginning of Sec 3"	"The abbreviation ""VC"" is never explained (I assume it is virtual colonoscopy). The authors should make clear already in section 3 why the VC frames are needed and what they are used for. While reading the paper it is confusing that they are introduced in section 3 without giving any context. The authors should explain what is the ""number of matrices"" in z_ts? The authors should explain the forward and backward cycle in the paper and how exactly a train step is implemented. Furthermore, it should be explained how exactly the non-corresponding OC and VC frames are used in the framework and how they are fed to the model in the training process. How do the authors determine the number of epochs? How is the quality of the generated samples assessed? There is a typo in the caption of figure 4. The authors should explain why a novel dataset based on VC is introduced even though there are no corresponding ground truth images available. Why did the authors not use an existing synthetic dataset? The authors illustrate different use cases of the proposed method in qualitative figures, which include: passing an input image to F and random sample z_ts and z_cl (figure 4) extract z_cl from reference images with G and then passing it together with an input image to F (figure 3) colon-specific color and lighting, while polyp specific textures and speculars like it is used in the evaluation of the presented work Furthermore: combining input images and CLTS latent vectors from different datasets (figure 4) would also be possible ... However, only one specific configuration is evaluated and discussed in the paper. The authors should at least discuss the different options of using their framework for augmentation."	a clear clinical need is identified an interesting method for transferring lighting and texture style separately from real to synthetic images good results manuscript is mostly well written but some parts could be more clear.	The definition of the model is at times very confusing between figure, text and equations	The authors present and interesting and flexible approach for the manipulation of OC images. The paper is well written, but hard to follow (also due to the complexity of the presented approach). A more structured way of presenting the use cases of the proposed framework would improve the paper.
097-Paper0726	Collaborative Quantization Embeddings for Intra-Subject Prostate MR Image Registration	This paper proposed a HiCo-Net for prostate MRI registration. The proposed HiCo-Net introduced a hierarchical quantizer with a collaborative dictionary to regularize the global and local feature maps of the CNN to generate a better deformation field. Experimental results showed that the proposed method can outperform other state-of-the-art methods in multiple evaluation metrics.	This paper describes an approach for (prostate) image registration. Conjecturing that deep networks for this task are overparameterized and that the feature space can be clustered into few groups, the authors devise a vector quantization approach improving performance.	This paper proposes a hierarchical collaborative quantization embedding for the image registration network to address the potential overparameterization problem. The authors also validate their proposed method on a private dataset.	This paper is well-written and easy to follow. The network architecture design showed enough technical novelty. The motivation is clearly stated in the introduction section.	The paper is very well-written (relating to language variety, broad content organization, level of detail of explanation and reasoning). The author's approach is innovative, well-motivated, well-documented and thoroughly validated. As the approach is complex, the authors provide an ablation over method components allowing the reader insight into how different components work together and influence performance. The method out-performs other approaches for this task.	The authors attempt to address a quite challenging but important topic to address the potential overfitting problem for image registration. The idea of using collaborative and hierarchical quantization methods to reduce overparameterization is quite interesting and the formulation seems to be quite novel.	The major problem of this paper is the lack of important experiments, which might undermine the main motivation.	I don't have major weaknesses to list.	"Format violations. The authors clearly modify the template in several places to gain extra writing space:  1) In section 2, the vertical spacings below section title ""Method"" and the following subsections including ""2.2 Model overview"" etc are clearly modified and reduced, which are strictly prohibited. 2) Text is wrapped around Fig.1, Fig. 3, and Fig. 4., which are strictly prohibited as well. 3) The supplementary file also exceeds the limit by 3 pages. The effectiveness of the proposed framework (collaborative + hierarchical) is not well-supported by the experiments.  1) From Table 1, the combination of Dv, Dh, Dc yields lower dice conformance compared with Dv plus Dc, which seems unable to support the effectiveness of the proposed hierarchical quantization.  2) Interestingly, the vanilla quantization (Dv) alone also yields better Dice and DC scores than the proposed combination (Dv, Dh, Dc), which shows that the combination of Dh and Dc's effectiveness under further exploration.  3) The proposed method also does not show superior performance compared to Dv plus Dh in terms of DSC and CD in Table 1, leaving the effectiveness of pre-trained Dc questionable. Potential misselection of baseline models. From Table 2, the NiftyReg method yields even worse Dice conformance compared to without registration in Table 1. This shows that NiftyReg doesn't work on the dataset or the authors didn't implement it correctly. This also disqualifies it from the ""state-of-the-art"" methods, making the argument of superiority less supported."	It is easy to reproduce since the authors provide detailed architecture design in the supp file.	The method's architecture is reproducible. However, as the method is learning-based, it is only fully reproducible with access to the processing pipeline and the model parameters. The dataset seems to be private.	The method is validated on a private dataset and thus it is hard to comment on the reproducibility given the marginal improvement.	-- The author showed enough ablation study to prove the effectiveness of the model design. However, when compared to existed works, they only compared two works that are not SOTA. Please provide more comparison results with the following SOTA works in registration: They are in unsupervised settings but could be adapted to supervised/weakly supervised setting with public code. [1]. Balakrishnan, G., et al.: Voxelmorph: a learning framework for deformable medical image registration. (2019) [2]. Meng. Y, et. al.: DeepTag: An Unsupervised Deep Learning Method for Motion Tracking on Cardiac Tagging Magnetic Resonance Images. (2020) [3]. Liu, L., et al.: Contrastive registration for unsupervised medical image segmentation. (2021) Minor modification:  -- Please change the name of section 2.6. from Training to Loss Function, since you have another subsubsection called training in section 3.1. -- It is better to modify the paper title so that the hierarchical quantizer idea is included in the title.	As is apparent from my other comments, I enjoyed reading this paper very much. I liked the overall organization, the motivation with t-SNE embeddings, the way quantization was introduced and built into the approach. Following the methods section, I expected to see an ablation which is exactly what the authors provided. In addition, their method out-performed other approaches which is a huge plus. I hope that this work finds it's way into a journal at a later point.	"The authors might consider including the VAE and its variants in the literature as it shares similar goals with the discrete quantization methods. Please double-check the typos and grammar throughout the paper (e.g. section 2.2, decoder G()->D()?). Please add some detailed support when making big arguments. For instance, ""However, deep registration models are over-parameterized..."" is not supported by any papers."	This is a good submission with clear organization, and enough technical contributions. If the authors solve my major concern above, I will keep my rating.	The paper is well-written. The approach is well-motivated, well-described, and innovative. The evaluation includes an ablation of method components and a favorable comparison against reference methods. For me, this paper checks all  the boxes.	The recommendation is based on the format violations and the main weaknesses listed above.
098-Paper0779	Combining mixed-format labels for AI-based pathology detection pipeline in a large-scale knee MRI study	This paper proposes a method for training neural nets from positional-class pair labels. The authors develop and validate their idea on knee MRI datasets.	The paper presents a two-stage method for classification of 4 abnormalities from knee MRI. The first stage performs rough detect localization, the second - binary classification. The authors put focus on the data-centric aspects - heterogeneity of the available MR images and the corresponding annotations across institutions - and study the relative importance of positional (point location) and categorical (defect grade) labels in detection of knee abnormalities. The results are three-fold: (I) combining positional and categorical labels was shown to be beneficial for the overall performance, with the proposed automatic method reaching the level of inter-reader agreement, (II) data augmentation done also between the stages lead to further performance increase, (III) training with diverse multi-institutional data (25 locations) yielded high performance also on the unseen public dataset obtained with a different MR protocol, which support the importance of combining the datasets.	Interesting study exploring use of both text level global labels and pixel level annotations for anomaly detection on multiple tasks. The study is conducted on a large dataset making ablation results comprehensive. The validation of model with public dataset is notable.	External validation of the developed method Conceptually right direction - adding positional information is naturally a way to ensure that the network learns from relevant spots	While the proposed method is essentially a two-step localization-classification pipeline with multi-task supervision, it is meticulously designed and evaluated by the authors. Consequently, the study provides a range of insights into the value of integrating the diverse annotations, performance of automatic defect localization, performance of automatic defect grading, and value of multi-institutional (multi-view, multi-protocol) MRI data. Overall, the study presents a very strong clinically-relevant evaluation. While the multi-institutional data is one of the core assets behind the study yet it is private, the authors make a focused effort to describe the dataset in detail, including the protocol- and the defect-related details. The article is written in an excellent way considering all the parts - problem formulation, structure, data description and preprocessing, method description, experiment design, statistical analysis, interpretation of the results, conclusions, valuable supplemental materials. Excellent graphical materials.	The main strength is the ablation study of the various training methods and the comparison with inter-reader agreement.	Too heuristic approach, and it is unclear how clinical relevance of the developed method can overcome this The authors re-invent object detection, and should have tried single-shot object detection architectures as baselines Statistical correctness of the claims has to be verified before this work is published.	The architectural novelty of the proposed solution is rather limited. The study also does not review the state-of-the-art methodological solutions for the task. Subsequently, it is not clear, for example, why an end-to-end is not considered. The paper is missing a discussion on the comparison of the proposed method against the prior studies, at least, in scope of ACL injuries - Namiri et al 2020 (https://doi.org/10.1148/ryai.2020190207), Astuto et al 2021 (https://doi.org/10.1148/ryai.2021200165).	The strategy and rationale in using peak finding algorithm in Stage I is unclear and needs to be explained better in the main manuscript.  Also the methodology of obtaining categorical or global level labels is unclear.	The paper is very difficult to reproduce due to a) private dataset and b) highly complex multi-stage pipeline.	Overall, very high reproducibiliy score. The checklist provided by the authors is in a agreement with the provided details. Few aspects regarding the private dataset and the data management are to be clarified. See the comments in next section.	The authors satisfactorily illustrate the model development methodology, various training strategies, data used and how positional labels were obtained and the comparison with inter-reader agreement between MSK radiologists.	Dear authors, thanks for submitting your work to MICCAI. While in overall the paper investigates an interesting direction, there are clearly some limitations. As I have mentioned earlier, this paper re-invents object detection. The authors should compare to methods like SSD and YOLO. The labels can be easily converted to the required format. In my opinion, it is fairly straightforward to build a 3D implementation of YOLO. Please, run statistical testing of your results, i.e. compute a standard error over runs, and preferably execute a statistical test itself.	"Page 2: ""even different image types"". The previous paragraph actually prioritizes the difference in the imaging protocols, so it would be better perhaps to rephrase ""even"" here. Page 2: ""However, methods to combine categorical labels with other positional label types, such as point-landmarks, were not addressed."". This is a false statement. A direct counter-example - Mask-RCNN (He et al), in knee domain - KNEEL + DeepKnee (XR, Tiulpin et al) and Namiri et al 2020 (MRI, https://doi.org/10.1148/ryai.2020190207), broad overview of the methods - Crawshaw et al 2020 (https://arxiv.org/abs/2009.09796). Please, rephrase. Page 2: ""A first model, to our knowledge, trained for ACL injury-age and subchondral edema underlying the cartilage defect pathology detection."". Firstly, according to the paper, the model is not trained for the tasks simultaneously but 1 model per task. Secondly, please, see Astuto et al 2021 (https://doi.org/10.1148/ryai.2021200165) and adjust the claims accordingly. Page 3: Please, elaborate on how the splits are done - balancing w.r.t. number of samples controls/cases, other? Page 3: ""25 different institutions"". Please, provide a reference to the dataset or the prior publications with it. For camera-ready version, please, strongly consider also elaborating on the demographic info (at least, age range, sex balance, geography of the institutions). Page 3: ""Labels used by models"". Please, provide few references on the rationale behind pooling of the grades, i.e. why are ACL and MCC injuries were pooled in a certain way. Page 3: ""1398 studies"". Please, rephrase to not start the paragraph with the number. Page 3: ""detected using a deep reinforcement learning model"". It would be great to also briefly provide the detection performance in the text. Page 6: ""only the ""best"" candidate was selected"". It may read as only one defect per knee was considered. Please, elaborate here what ""candidate"" means. Page 7: ""Stage I training was designed to achieve high sensitivity, since false positive studies would be filtered by stage II. Indeed, in three tasks we observed sensitivity exceeding 95% (Table 2). However, in the Cartilage Edema task we obtained 89%."". There is actually not a single task where sensitivity is >=95%. For MCC, 89% is only in AUC, not sensitivity. Please, review and fix this paragraph. Page 7: ""4.9 mm localization accuracy"". Please, provide the credible intervals, if available. Page 7. ""Two models only used positional-labels in training (Labels = Posit. in Table 2)."". Please, elaborate on how the classifier is trained in this case. These two experiements are rather difficult to understand."	While the authors demonstrate a clever strategy in using positional labels to improve accuracy of categorical labels, the assumption is there will be real world gains in decreased need for pixel level annotation to build robust models. The paper falls short in exploring this practical aspect (e.g. time taken to used mixed format or reduction in computational resources) How are the categorical labels obtained? Please expand on this crucial aspect. Are they labelled prospectively by the MSK radiologists or are they retrospectively obtained from radiology reports? If later, how are the labels for the 4 task obtained-, eg. NLP based extraction for key words. How was resizing done in Stage II, if more than one lesion in different locations in the same study? The application of peak finding algorithm in Stage I is unclear. Is the best candidate the central point on a given slice, with the shortest distance to lesion of interest? One of the claimed main contributions of the paper is that this is the first manuscript to detect subchondral edema. There are already multiple state of the art papers which quantify, detect and classify subchondral edema in multiple knee compartments, not just MCC. Please rectify this claim. Finally, the biggest focus of the paper is that use of positional or pixel level labels augment categorical labels by helping it localize the lesion. Please provide some visual proof of how the model attention performs (eg saliency maps) if possible.	No statistical testing, lack of comparison to baseline methods, difficulty to reproduce.	Method for grading knee lesions from multi-view multi-protocol MRI.  Strong clinically-relevant evaluation of the proposed method.  Interesting results showing the value of (I) combining positional and categorical annotations, (II) multi-institutional data for model robustness.  Limited methodological novelty of the work (not end-to-end, not multi-label).  Limited analysis w.r.t prior art, both methodological (deep learning) and clinical (lesions in knee MRI).  Excellent quality paper, with very high reproducibility score.	Interesting paper that tests a novel strategy to use pixel level labels to improve locational accuracy of text labels. The hope is such models can reduce need for time and labor intensive pixel level annotations while building ML models. The authors do a good job of providing statistical metrics on various ablation methods but unfortunately do not any real world stats such reduced annotation time without reduction in model performance.
099-Paper2128	Combining multiple atlases to estimate data-driven mappings between functional connectomes using optimal transport	This paper proposed a all-way optimal transport algorithm that combines information from multiple source atlases to estimate a connectome close to the target one. As an application, predicted connectome was used to predict IQs and yielded a equivalent performance as the target one.	"This paper addresses a problem when you have a large rsfMRI dataset processed to form a connectome using one atlas, and you want to compare with a connectome generated with another atlas. Normally, you would have to reprocess your dataset with the second atlas in order to make a comparison. This problem exists because there are several widely used atlases used in connectomics research. Previously it was proposed to use optimal transport algorithm to transform connectome based on a single atlas into a connectome based on a second atlas, using just a subset of ""training"" subjects processed with both atlases. Here, the authors show that a connectome can be predicted better if the training dataset is processed with multiple existing atlases."	The authors explore the optimal transport method to estimate the mapping of brain connectomics form multiple source atlases, unlike widely used single source atlas, to a target atlas improving better estimation. For this a paired time-series is taken from 'k' different source atlases with a linear combination of ns regions and compared to the target atlas distribution (as distance minimisation of two pdfs). An iterative Sinkhorn algorithm is used to solve the equation using existing Optimal Transport toolbox [11].	The proposed algorithm is novel. An all-way optimal transport algorithm was proposed based on single-source one. The information integration ability of the proposed algorithm was well demonstrated by comparison with single-source one.	The paper is very well written. It addresses an interesting problem, for which only one solution seems to have been proposed so far. The solution is a natural extension of the single atlas to target algorithm. The evaluation is comprehensive and shows that the proposed multi-atlas strategy predicts the true connectome much better than single-atlas. Particularly impressive are the IQ prediction results because they show that synthetic connectomes generated using multiple atlases are just as good at predicting IQ as the true connectomes.	1) The paper is well written and easy to follow 2) Using optimal transport for multiple source atlases is interesting  3) Results show clear improvement in correlation values with target ground truth 4) Provided details on parameter sensitivity is worth noting	It is a bit weird that the predicted connectome has a better IQ prediction performance than the original one. Isn't the algorithm was designed to predict a connectome as close as the original one? That is, if the original one has a bad IQ prediction performance, the predicted one is expected to have a bad one, too. If the predicted one has a better performance, as shown in Fig. 4, this connectome is a 'hybrid' connectome, rather than a predicted one. The authors may provide some in-depth discussion on the performance.	The only drawback is that it is somewhat unclear how practically useful this method will be. It still requires one to process some subset of the target dataset with the target atlas, and at that point, once you've set up all the scripts, doing it on the full dataset might just be a matter of computing power. But still, the method is elegant, so this is not a major weakness.	1) The paper lacks insight into how many number of source atlases are optimal and what is the effect of increasing or decreasing these numbers 2) More details on choice of regions could help readers. What are ns and nt regions and how these are included?	The authors didn't provide that source code. Datasets and atlases used are publicly available.	Ok	The authors have agreed to most of the reproducibility questionnaire.	For the first comment in Q5, the authors may consider some connectome-related metrics to evaluate the performance, such as graph metrics (degree, clustering coefficient, etc.) For the second comment in Q5, some discussion may be provided. For example, why other connectomes only have a prediction accuracy around 0.5 while Craddock has it as high as 0.75. Also, it seems that Craddock always yields the best single-source performance. Does the choice of atlas strongly affect the performance?	See above	1) It is hard to understand from abstract how the authors have validated their approach and what improvement they get. I think briefly mentioning the dataset and a sentence with key finding would help reader.  2) Authors could replace training and testing word with 80% for  optimal 'parameter estimation' which was then applied on 20% remaining data for measuring the efficacy of the method. This will help readers not to be confused with the deep learning works.  3) Authors should take special care with regard to anonymity and not put the acknowledgment during submission of double blind reviews.	The novelty of the proposed algorithm.	The positives outweigh the negatives. An interesting method with thorough evaluation and something different to complement all the CNN papers at MICCAI.	Use of multiple source atlases to better generalise the mapping of target atlas is interesting and the results show promise in the proposed method. Also, the experiments are done thoroughly along with the parameter sensitivity test.
100-Paper1882	Computer-aided Tuberculosis Diagnosis with Attribute Reasoning Assistance	This paper introduces new large-scale TB dataset for attribute-assisted X-ray diagnosis and help models conduct weakly-supervised TB detection. This paper also proposes an multi-scale attention-based feature interaction module to enhance TB detection.	It proposes a new large scale tuberculosis (TB) chest X-ray dataset, namely tuberculosis chest Xray attribute dataset (TBX-Att), and establishes a attribute-assisted weakly supervised framework to classify and localize TB by leveraging the attribute information to overcome the insufficiency of supervision in WSL scenarios.	The paper describes a new chest X-ray (CXR) tuberculosis dataset that contains disease attributes in addition to TB labels. A novel multi-scale feature interaction model for TB attribute detection and classification is also described. Majority of existing TB CXR datasets contain only disease identifying labels (TB, healthy, sick/no TB). However, there are many other clinical features that may be of interest to clinicians during diagnosis. To address this, the paper presents a TB CXR dataset with attributes (e.g., fibrotic streaks, pulmonary cavitation etc.) to facilitate computational analysis and reasoning about different TB properties.	4.(1) The proposed dataset provides attribute information which can help model detect TB areas in a weakly-supervised style. This dataset may receive well focus in the future research if provided publicly. (2) The proposed multi-scale feature interaction module effectively leverages the attribute feature to conduct self-attention and cross-attention, resulting a better performance. Such method is useful for the weakly-supervised framework, and have promising potential for wide-spread application.	It presents the TBX-Att dataset for attribute-assisted X-ray diagnosis for TB. It proposes a method to fuse the attribute information and TB information, including the feature pyramid network, the attribute classifier and the feature interaction module. It faces many difficulties to collect the dataset of attribute-based TB X-rays, and it combines the TBX11K dataset to construct a totally new attribute-based TB dataset.	Majority of existing TB CXR datasets contain only disease identifying labels (TB, healthy, sick/no TB). However, there are many other clinical features that may be of interest to clinicians during diagnosis. To address this, the paper presents a TB CXR dataset with attributes (e.g., fibrotic streaks, pulmonary cavitation etc.) to facilitate computational analysis and reasoning about different TB properties.	5.1) The components in the feature representation module, such as group convolution[1] and multi-head attention[2], are directly transferred from general image recognition framework, which may be lack of novelty to some extent.  [1] Krizhevsky A, Sutskever I, Hinton G E. Imagenet classification with deep convolutional neural networks[J]. Advances in neural information processing systems, 2012, 25. [2] Dosovitskiy A, Beyer L, Kolesnikov A, et al. An image is worth 16x16 words: Transformers for image recognition at scale[J]. arXiv preprint arXiv:2010.11929, 2020. (2) The insufficient experiments can not provide evidence to show the effectiveness of some operation in the module, such as relative position encoding.	1.Fig.1 and Fig.2 can not explain the usage of features in each step. The branches of detection and classification are commonly used in precious work and there are not novelty parts in the network. The method is only evaluated on one dataset, that can not certify the effectiveness.	While the motivation is compelling, the paper seem to miss important details about the data collection protocol. For instance, what ages, genders, and other demographical information does the dataset represent? How was the dataset annotated? The paper mentions the TBX11K dataset, but it is not clear how the two datasets are combined.	CREDIBLE: I believe that the obtained results can, in principle, be reproduced. Even though key resources (e.g., proofs, code, data) are unavailable at this point, the key details (e.g., proof sketches, experimental setup) are sufficiently well described for an expert to confidently reproduce the main results, if given access to the missing resources.	The paper is simple with good reproducibility.	Most details of the proposed computational model are presented, so it should be possible to reproduce it.	8.(1) In the ablation study part, please show the effectiveness of relative positional encoding which is adopted in the multi-head attention module. The author needs to discuss why relative positional encoding is needed for TB detection. (2) The loss function for detection branch is not clarified. Please provide specification. (3) The influence of the level of multi-scale feature pyramid is not well studied. Directly using attention module in the low-level with large resolution feature map will cause great computation and memory burden, although a multi-head style and stride down-sampling are adopted. Please show the experiment results under different choices of depth and provide conclusion about the relation between performance and pyramid level, like whether the high-resolution low-level feature maps contributes much less the higher levels.	1.There are some clerical errors, e.g., Loss_seg has no been provided before in equation 4.  2.The attribute feature representation should be proved validity. The method is not compared with enough current work, so it is not convincing.	There were several points that I found unclear/missing. I did not understand why the presented task is presented as a weakly supervised learning, when the dataset contains both bounding boxes and labels for every data point. Also, what is the advantage of the proposed attribute relational reasoning network over more standard region proposal networks (e.g., RCNN, Yolo, etc.)? Results in Table 2 also need a lot more description. Is it possible to evaluate the attribute classification and detection separately? Is it also possible to perform analysis on each attribute separately and compare and contrast?	The whole proposed attribute-guided method is novel for TB detection, but the components need more innovative modification for medical image recognition. And the insufficient experiments can not provide evidence for the effective of some operation.	1.The dataset proposed combines the former dataset TBX11K. It offers a playground for attribute-assisted X-ray diagnosis for TB. The model proposed inherits two branches thought without much novelty. The experiment is not sufficient.	Overall, the paper presents a compelling limitation of previous work and presents a dataset and an approach to overcome it. However, the evaluation and description of both the method and the dataset is quite limited.
101-Paper0377	Conditional Generative Data Augmentation for Clinical Audio Datasets	The paper presents a GAN based method for augmentation of clinical audio data by synthesizing log-mel spectrograms. Furthermore, the paper demonstrates that the augmented data can help to improve the classification of various clinical actions flow by using a resnet-18 classification model.	The paper introduces a new clinical audio dataset, which records the sounds of different surgical phases during the Total Hip Arthroplasty (THA) procedures in the real-world operating room. A GAN-based model equipped with Wasserstein loss and Gradient Penalty is proposed to enlarge the dataset. Based on the augmented dataset, the method shows the improvement on the phase classification task, compared with other data augmentation strategies.	The authors present a Wasserstein GAN based method for augmentation of audio based classification tasks for medical applications.  The authors show an improvement in classification performance of a classifier trained with the data augmented with the GAN.	The paper presents a new direction.  Overall, well organized and well presented.  The flow is easy to follow.	Introducing a new audio dataset recording the sound of surgical action Propose to enlarge the dataset with GAN	The authors present a novel method for augmenting audio datasets in medical applications. The authors show improved performance over existing techniques The analysis of audio signals is a potentially rich source of insight in medicine that is not well researched.	No comparison with other methods (the authors claim there are none?) Clinical application is not very clear. and so it is not clear what motivates the work.	Limited method novelty with just applying the GAN method to do the augmentation The experiments are not that sufficient, which only can be stated as some preliminary results for this new topic.	The dataset analysed is very small, therefore the generalizability of the method is limited. The authors do not well describe and justify the used of the the dataset being analyzed.  What is the purpose of analyzing the THA audio dataset? The description of the results needs to be improved.  Only mean accuracy is presented, which is very limiting in terms of interpretation.	No code provide. But the authors have promised to make it public.	Good for the reproducibility, as they promise to release the dataset and code	The authors state that they plan to release the code upon publication.  The methods are reasonably described so that a scientist with the code could reproduce the paper.  The dataset is small, it is not that well described, so reproducing these results may be challenging without the dataset.	"The authors have presented an interested applications of GANs for clinical audio data. However, the data collected in this work is through a controlled process. And has been manually edited/segmented to extract audio segments for each action. In a real clinical settings, the audio data will be a lot different. And it will involve human conversations, ambient noise etc. The authors did not provide any insights on this or how this can be addressed. Did the authors consider using SSIM too for comparison? Check for typo in conclusion: ""can to improve..."""	The main weakness is the novelty of proposed method. It seems that just applying GAN techniques with Wasserstein loss for generating clinical audio, while do not consider the unique properties of clinical audio when designing the method. -As shown in Fig.1, 'Suction' seems always being overlap with other phases, therefore, how do you define the recordings for 'Suction' class? Do they also belong to other classes? How many samples for each class do you generate when doing the comparison in Table 1? If only doubling for each class, why you say the generation can help tackle the class imbalance problem? The results on all five folds are better to be shown, rather than only the average results. Actually, from the Fig.3, it seems that the generated samples are not well aligned with the GT in each class. Maybe you could show the samples generated by other GAN-based approaches for comparison, to validate the effectiveness of your method.	Abstract The authors should make the primary use case of this approach more clear.  Is this meant for A/V surgical annotation of the stage, is it meant to guide interventions, is it mean for training?  It is even unclear what the nature of the sounds that will be analysed are, is it the speech of the surgical team, the heart beat of the patient, or the pitch of the drill?  It is unclear what labelling the dataset collected in an automated fashion will be useful for. Introduction The reason for analysing the THA procedure is unclear.  The authors successfully make the case that there are many useful reasons to analyse audio in medical application, drilling sounds as guidance in ortho procedures, lungs sounds as a diagnostic measure.  However the authors need to point out why they are working with the audio dataset that they are.  It is not clear what this dataset is useful for or if it is a good surrogate for other audio datasets. Methods The dataset seems small (N=5), which would likely affect generalizability. Do the crossfolds split based on the different recordings, 5 folds, 5 different recordings? What is the frame rate of the recordings?  L=16380, how many seconds does this correspond with? Why is the spectrogram distribution and the dataset recording distribution differ?  The percentage of samples that are suction in the spectrogram data is far smaller than in the recording dataset. How is the data split for training the GAN vs training the classifier?  This could be made more clear Figure 2 A more thorough description in the caption is needed.  It should be made clear in the description that this is the GAN.  The input should be made more clear, the spectrogram output should be more clear. Results The results section should state the main result, rather than only refer to the figures and tables. Why is mean accuracy reported, rather than F1 for the multiclass classification?  Some sample confusion matrices would be helpful in interpreting the results.  It is unclear from only accuracy which classifier is best. Discussion The authors should not comment on the method improving robustness or generalization because neither of these things were investigated in this study.	The topic is interesting. The approach is easy to follow and will encourage good debate in the conference.	Weak novelty of proposed method, as well as the insufficient experimental results	Novelty of the approach The dataset was small and not well justified The results are promising but could have been better described
102-Paper1704	Conditional VAEs for confound removal and normative modelling of neurodegenerative diseases	The authors aim to detect outliers (neurodegenerative diseases) from learning a distribution of atrophy patterns from healthy MRI. Healthy Brain MRI from Biobank were processed using Freesurfer, resulting in gray matter volume of 16 subcortical nuclei and 66 neocortical areas (so presumably 82 features per scan). A variational autoencoder was used with a latent space of dimension 10. Additional variables are added as condition (input to encoder and concatenation of latent space). The proposed criteria for detecting outliers is a z score over the latent space.	The paper proposes a normative modeling framework with confounder removal for Alzheimer's disease datasets.	The authors proposed a latent deviation metric and use it to quantify deviations in individual subjects with neurological disorders. They claim model is able to identify these disease cohorts as deviations from the normal brain in such a way that reflect disease severity.	Simple idea with reasonable validation. Interesting finding in Figure 4 that the pattern detected seems consistent with known neurodegenerative patterns associated with AD.	The paper tackles an important problem of normative modeling + confounder removal.	variational autoencoder is a better model compared to autoencoder in terms of parameterizing your data.  I am happy that the authors used this model. However, I am a bit unsure of the main contribution of the paper. Why authors directly started talking about Deviation metrics in page 4 without linking it to Eq. 2? Maybe I am missing something but It would be great to explain Pinaya's method .	It is unclear what the VAE approach add beyond a standard outliers detection of the underlying data, the only comparison provided in figure 3 seems to be with a mean square error of the original data. It is also unclear how the hyperparameters were chosen and influenced the results.	There are major novelty concerns: The confounder removal method (cVAE) is largely outdated. There exist many other options (e.g., domain adversarial networks, conditional normalizing flows, etc.) The proposed deviation metrics are identical to the normative probability map (NPM) by Marquand et al. [16]. In fact, [16] suggests extreme value statistics to perform statistical tests on the NPM (which are essentially z-scores) for subject-wise statistics.	why github code is in abstract? Fig. 4 is kind of needs some work. Table 1, red colors needed to be black. You can use different symbols to emphasize the model.	method is not clear enough to be reproduced, the network architecture is not provided.	Reasonably reproducible.	anonymous git is a good idea, but it should be in main text.	More investigations of the network design could be informative (dimension of latent space, network complexity,....), as well as including a state of the art statistical analysis on the raw data.	The paper tackles an important problem which, if successful, would have a broad impact. However, there are critical concerns about the novelty of the contributions. I would appreciate if the authors could comment on those.	Method needs re-writing. It seems the main contribution seems very minor without properly linking to the baseline.	Clear and simple paper with promising results.	For above reasons, the paper holds little technical and analytical contributions.	Overal, paper is a good one. But, in rebuttal I want to see a clear method part. If we have a clear VAE model in the paper, we need also relevant theory from literature too.
103-Paper2792	Consistency-based Semi-supervised Evidential Active Learning for Diagnostic Radiograph Classification	A Consistency-based Semi-supervised Evidential Active Learning framework (CSEAL) is presented in this submission to mitigate the data annotation problem in supervised learning for radiological image classification, where consistency-based semi-supervised learning is combined with uncertainty-based active learning. The proposed CSEAL is applied to enhance four types of consistency-based semi-supervised learning methods. Evaluation on NIH-14 Chest X-Ray dataset is performed to demonstrate the effectiveness of the proposed method.	The paper proposes a method to leverage both semi-supervised and active learning for the task of X-ray classification. The proposed method, called CSEAL, can be applied to any consistency based semi-supervised learning method - the paper applies it to Pseudo-labelling, Virtual Adversarial Training, Mean Teacher and NoTeacher. Experiments on chest X-ray classification show that combining with active learning does improve performance compared to other semi-supervised + active learning methods.	The authors proposed a new method (CDSEAL) to enhance existing consistency-based semi-supervised learning methods. They compare it with existing semi-supervised active learning algorithm and show that their method improve over the baseline. On top of the semi-supervised learning, authors introduced an uncertainty estimation for active learning. The results show that it improved over random sampling.	A Consistency-based Semi-supervised Evidential Active Learning framework (CSEAL) is presented, where two major components are involved: evidential-based semi-supervised learning, and evidential-based active learning. In the low-range labelling regime of the average test AUROC, the best performing CSEAL method eNoT+AU outperforms two baselines ToD+CoD [10] and VAT+Aug Var [7].	The authors propose a novel combination of SSL + AL that can be applied to a variety of SSL methods. The new method obtains better performance than strong baselines like COD + TOD and VAT + AugVar. The improvements are especially good for rare classes which are harder.	The paper improves the classification performance in the existence of low number of annotated data and abundant unannotated data. The application is indeed appealing in the biomedical domain where the expert annotations are rare and expensive. The way the CDSEAL semi-supervised learning is formulated. In addition, authors have a good grasp of the current state of the art. The paper is well structure and the stream of thought is clear. The selected low-range labeling regime is realistic and well done.	The discussion of the results is not enough to help to understand the results. In particular, Fig. 2 is relatively easy to understand without explanation, but Fig. 3 definitely needs additional discussion. The presentation and organization of this submission needs to be improved. It is recommended to move the figure close to the text where  the figure is referred or discussed (e.g., Fig. 1 is at page 5, but the corresponding text is at page 3). The limitations of the proposed approach and the directions for future research are not discussed in this submission.	Parts of the paper could be re-written for better clarity. The paper uses too many abbreviations which makes it hard to read. Limited experiments: The method is only tested on a single X-ray dataset with a single model (Densenet-121 backbone). This makes it difficult to tell if the results generalize. Further, it is unclear if the same method (eNoT+AU) is superior on every dataset.	While known semi-supervised algorithms are used to implement CDSEAL, they are not benchmarked themselves. For example, ePSU is benchmarked on the dataset. However, PSU (pseudo-label) itself is not benchmarked. Therefore, the question arises that the observed improvement over the supervised learning comes from the PSU or the CDSEAL loss. The authors also used AUCROC for the evaluation of the methods. While AUCROC is a well-known metric, it is flawed and skewed for highly imbalanced datasets. It is suggested to use the area under the precision-recall curve instead.	Code is not submitted for review.	Hyperparameters are specified in the paper, and the model has been clearly described. The dataset used is publicly available.	Since code is not released, it is not clear to which extend one can reproduce the work from the paper itself.	This submission can be improved in the following places: Revise and add discussions for Fig. 3 to provide a clear understanding of the results. Fix the figure location issue by moving Fig. 1 to be closer with the text referred to it. Add discussion of the limitations of the proposed approach and the directions for future research.	Improving the readability of the paper: please expand some of the abbreviations or use descriptive short names. Adding more experiments showing that these methods are broadly applicable is helpful.	While providing a new framework, there is always a question that how the new framework perform on different datasets/modalities. The framework is tested on NIH-14 Chest X-Ray data, which is a challenging dataset. However, it is not clear how it would work on new datasets. Regarding the active learning part, the authors only provided AU for the uncertainty sampling. They have shown that the AU works better than random sampling. However, it is not compared to other active learning algorithms. Therefore, the question comes up that whether AU is the best method for active learning. The figures were all readable. However, the abbreviations made them complicated to follow. For example, for the Fig.2 and Fig.3 (which include the main message of the paper), one has to remember a series of not very well-known abbreviations to be able to understand the plots.	The rating is based on the fact that a new active learning framework is presented in this paper for radiological image classification, which achieves comparable (or better) performance to the state of the art.	The proposed method is novel, and the authors obtain good results on an X-ray dataset. However, with limited experiments, it is difficult to justify a higher rating.	The approach is quite interesting and seems to be useful. Although there are some shortcomings in reporting the results.
104-Paper0744	Consistency-preserving Visual Question Answering in Medical Imaging	* The paper proposes a VQA model with focus on consistency across model replies, to ensure that the model doesn't contradict itself when answering multiple questions over same image.	This paper proposes a method that enforces VQA consistency specifically by regularizing the answers to perception questions and to reasoning questions. The authors demonstrate improvements on model consistency as well as model accuracy in diabetic macular edema staging.	This paper proposes a novel loss function and corresponding training procedure that allows the inclusion of relations between questions into the training process. The proposed method is evaluated on  Diabetic Macular Edema (DME) staging from fundus imaging and showed that proposed method outperforms state-of-the-art baselines, not only by improving model consistency, but also in terms of overall model accuracy.	* The paper is well-written and easy to follow.  * The idea to enforce consistency, by letting the user ask questions about a specific region in the image (by providing a mask) is interesting. * The strength of the paper lies in learning from special dataset that provides presence or segmentation labels of hard-exudates. * This paper is a good example of extending a deep learning model to incorporate domain specific constraints. Though the extended VQA is trained using a special supervised dataset, the training is still compatible with standard dataset without any consistency information.	Solid experimental results on improving both model accuracy and consistency in the diabetic macular edema staging experiment.	Problem addressed in this paper is intresting and an important. The model outperforms a baseline on one dataset	* Gathering special dataset with expert annotations either for marking presence or segmentation of hard-exudates or for marking fovea to compute macula region is expensive.  * To support clinical deployment, the authors should report results on a third dataset that have only DME labels.  The authors can report results in two scenarios, o Consider this dataset as just a deployment dataset for inference time only.  o Consider this dataset for fine-tuning the pre-trained VQA.  Report how VQA helped in building trust by identifying DMW scale and specifying regions with hard exudates.	"Unclear what the difference between the proposed approach and the one in Selvaraju, Ramprasaath R., et al. ""Squinting at vqa models: Introspecting vqa models with sub-questions."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020."	No technical novelty. only one data is used. Generalizibility of method is unproven. Recent VQA methods in medical domain (MedVQA) are not discussed. Recent SOTA methods are not used to compare the performance of the proposed method. Writing is a bit poor; there are a lot of typos and grammar issues.	The paper will be reproducible when the code will be released.	Good.	none	"* The attention vectors can help identify the important regions in image for answering a question. For a given main question, an analysis of when main and sub questions pay attention to same or different regions, will further help understanding the reasoning process of the VQA. * At inference time, for an image with DME scale of 1 or more, one can create multiple sub questions, while gradually questioning the entire image. Aggregating positive replies can create a weak attention map over the image highlighting the location of hard exudates.  * The authors can consider an additional question, ""is this region macula?"" This can help in identifying macula in un-annotated images and then further evaluating question: ""Are there hard exudates in the macula?"" * The current scope of the study is very narrow, with a specific imaging modality highlight a specific disease. The authors should consider evaluating their method on another study to demonstrate the generalizability of their approach."	"Adding a paragraph to discuss related work would be helpful. The authors should be explicit about the contributions / novelty of this paper and consider discussing Wang, Peiqi, et al. ""Image Classification with Consistent Supporting Evidence."" Machine Learning for Health. PMLR, 2021."	-First, this work lacks of novelty. -The experiments are not enough, and it is not convincing only on the single dataset. -This paper lacks a discussion on the experimental results and motivation analysis. -Fig 2. occupied more than half of the page (page 4) - you could have saved this space to better discuss the motivation and results -I suggest the authors could add baselines that recently proposed.	Its a novel approach for a very specific medical problem.	The authors demonstrate improvements on model consistency as well as model accuracy in diabetic macular edema staging. The novelty of the proposed method, however, is not clear.	Lack of novelty and generalizibility of method is unproven. Further, no recent SOTA methods are used to compare.
105-Paper0772	Context-Aware Transformers For Spinal Cancer Detection and Radiological Grading	This paper presents spinal context transformer, a deep learning architecture considering context from multiple sequences and neighboring vertebrae, for spinal cancer detection and radiological grading. In this study, the training labels of the spinal cancer detection task was obtained from free-text radiological reports, avoiding the necessity of annotating the images.	The paper describes a method for vertebrae analysis using an algorithm relying on transformers. The authors focus in particular on vertebrae classification using multiple MRI sequences. Particular attention is drawn to weak supervision, as the labels are extracted from clinical reports. The authors perform the evaluation on the Genodisc dataset and compare the method to some state-of-the-art works. Moreover, some ablation is performed regarding the contribution of the sequences being used.	The paper proposes a Spinal Context Transformer (SCT) for a variety of spine-related tasks in multi-series spinal MR scans. It also proposes strategies to use annotations derived from reports. Experiments on two datasets show improved accuracy.	This study considers context from multiple sequences and neighboring vertebrae for for spinal cancer detection and radiological grading, which is interesting. This study demonstrates the potential of obtaining supervision from free-text radiological reports.	The paper focuses on clinically relevant task and does that in the technically appealing context, that of using transformers, and within weak supervision making it of interest for the medical imaging community overall. The proposed evaluation is sufficiently broad to allow for comprehension and judgement.      Moreover, the paper is nice to read as it is quite clear and well exposed, so the reading flows. The provided illustrations contribute to the understanding of the paper.	The paper proposes to use transformer layers and attention mechanism to fuse features from multiple MR slices, series and vertebrae, which is intuitive and effective. The use of labels extracted from reports is also economical. Comprehensive experiments are done on muliple spinal tasks.	The structure of attention mechanism (in Fig. 1b) was not described in the paper, therefore it is unknown how the attention is achieved. More information of the baseline models has to be provided to evaluate the fairness of comparison. For example, do they operate on multiple sequences? In what aspects that they are good choice of baseline? In Table 3, instead of SpineNet (T2) and Baseline: SCT Encoder (T2), it would be fairer to use SpineNet (T1, T2) and Baseline: SCT Encoder (T1, T2) for comparison in Table 3.	The authors introduce a transformers-based algorithm, which is of raising interest in the community. However, there is few discussion of the role the transformers are playing in the performance increase, that might lead to uniformed conclusions.	The proposed method is only compared with one existing method published in 2017. There should be more comparisons with existing methods, as well as more ablation studies to assess each component in the method. Besides, in Table 3, SCT only improved SpineNet by 0.7% in average accuracy with T2 images. The content is somehow too much for this 8-page paper, so there are little space for more comparison and ablation studies. I think the introduction can be trimmed a bit.	The dataset for spinal cancer detection is private. No experimental codes are provided.	The authors provide some reasonable amount of details on the dataset definition and its preparation. They also provide details on the training setup. Some pseudo-code is provided in the supplementary material. The encoder is introduced as ResNet18.	The description of the method is satisfactory but not perfect, possibly because the algorithm has many parts but the space is limited.	Please describe the structure of attention mechanism and explain how the attention is achieved. Please consider to justify or change the baseline models.	Method In Fig. 1 the authors introduce STIR and FLAIR sequences, while later, in experiments, they talk about T1 and T2. Could the authors revise it for consistency? Experimental results At this stage, it might be useful to remind the encoder being used (ResNen18 as it stands from 3) and the way it was trained. Results: it appears that the SCT (T1, T2) has lower performances than T1 or T2 alone. Could the authors comment on that? Results: In table 2 the authors show the performances compared to the expert and report annotations with slightly different trends (e.g., the baseline outperforms the proposed method in fractures classification). Could the authors provide more details on how the tables should be read and how the results could be interpreted? Discussion I wonder, whether the T1+T2 trained method would require both sequences available at test time which might be a limitation of the method? Could the authors comment on that? Overall: I would suggest revising the format and better use of subsections and paragraphs, as some of the sections appear to be a bit lengthy.	Please clarify if the method needs manual annotation of vertebra levels. What is balanced accuracy in Table 3?	The novelty of this paper is the design of SCT and the use of information from free-text radiological reports as supervision, both of which are interesting. The main problem is the baseline models, which might be unfair when compared with the proposed approach.	The paper is well written and exposes well the proposed approach. The topic could be of interest to the general public as it could raise discussion of the interest of transformers as a promising tool. There might be a few minor improvements that could be done before moving forward (see detailed comments.	Reasonable and novel method with comprehensive experiments.
106-Paper0991	Context-aware Voxel-wise Contrastive Learning for Label Efficient Multi-organ Segmentation	This paper develops a voxel-wise contrastive learning (CL) method to utilize both labeled and unlabeled data in partially labeled dataset to improve the multi-organ segmentation.	1) A novel loss function that can be used to train a multi-organ segmentation network based on both labeled and unlabeled information in partially labeled datasets. 2) A contrastive learning method is proposed to learn better feature representation. 3) Comprehensive study for the proposed method.	The authors proposed a context-aware voxel-wise contrastive learning method to train a network on partially labeled dataset.	- This work aims to solve the partial label multi organ segmentation, which is an important problem in medical image segmentation. - The contrastive learning loss is applied for unlabeled voxels to enhance the learned features.	1) The performance of the proposed method demonstrates superior performance than previous state-of-the-art loss functions [12], [21] in the task of partially labeled segmentation. 2) The motivation to learn the unlabeled information is good.	"The motivation (utilizing partially labeled datasets) is practical. The idea of ""context-aware"" seems interesting. The authors would like the same voxels but from different patches (context) to be the postive pairs."	- The methodology contribution seems to be relative minor. The main contribution is adding the contrastive learning loss to unlabeled voxels. However, the organization and description of this paper is not clear. It is hard to see how authors handle the partial label dataset using CL and why it works. There are also too many math symbols in the main text affecting the readability. - There are several major concerns for the experimental evaluation. (1) The experimental setup description is not clear.  It seems that the baseline nnUnet's results in D1 to D4 is just a directly inference. If that is the case, then, reporting the retrained results of nnUnet on D1 to D4 should also be provided to see the upper limit of training directly on the target single organ dataset. (2) The improvement over the previous work is minor, e.g., there is a 1.7% DIce improvement over [12], this kind of improvement might be brought by the backone of nnUnet, instead of the developed CL method.  In other words, if [12] is also eqquipped with nnUnet, what are the results? (3) The ablation study on the network architecture is weird. When using original UNet as baseline to segment D0, it achieves better performance than nnUNet in both Dice and HD95. I think this cannot be true. There is also no detailed description on the UNet training details.	1) To leverage the unlabeled information, the author proposed a new training approach: extract two overlapping patches and use contrastive loss to minimize the difference between features of the corresponding area. The motivation is that most of previously methods adopted patch/subvolume-based strategy. However, for organ segmentation in CT volume, it is always not necessary to adopt the subvolume-wise patches for training. 2) The work is more like an incremental work. Contrastive learning for enhancing the feature representation is not new in medical imaging area.	"The effectiveness of ""context-aware"", i.e., pulling feature of a voxel from different patches closer, needs to be justified in the ablation studies. What if designing the positive pairs by using same voxels from rotation-augmented patches instead of different cropping areas? The author mentioned a recent work DoDNet (Ref. [13]), which is also designed for partially labeled deep learning segmentation. However, there is lack of comparison to this method."	I did not check the reproducibility.	No code is provided. But the paper provides the details for reimplementing the work.	The authors will make the code available upon acceptance of paper.	Please see my detailed comments above.	"The motivation that introduces unlabled information as complementary information for training is interesting. Sequentially, the proposed method of using contrastive learning for better feature representation also demonstrates helpfulness in the network training.  However, many weaknesses have to be addressed: a) the motivation to introduce the contrasitive learning is weak.  b) Some typos, e.g. ""it still challenging"" -> ""is still challenging"" in Section 1.3. c) Statistical tests are required to demonstrate the performance improvement."	"As mentioned in the weaknesses part, it would be good to add ablation studies on the effectiveness of ""context-aware"", as well as a comparison to DoDNet. Some minor corrections (1) ""a organ"" -> ""an organ""  (2) Introduction section: ""Alternative, one can design"" -> ""Alternatively, one can design""  (3) Introduction section: ""it still challenging"" -> ""it is still challenging"""	Consider that the paper's methodology contribution is relative minor, quite a few major concerns for the experimental evaluation and the clarity of this work is relative low, I recommend weak reject.	1) The motivation to introduce contrastive learning is weak. It's always not necesary to sample two overlapping patches for training in organ segmentation. 2) The innovation is not sufficient. Constrastive learning has been shown to be useful for feature representation learning. The work is an incremental work.	The use of contrastive learning in partially labeled segmentation problem and the improvement of the performance.
107-Paper1211	ConTrans: Improving Transformer with Convolutional Attention for Medical Image Segmentation	The paper introduces the Swin Transformer to medical image segmentation and develops a hybrid network leveraging CNN's local information extraction ability and Transformer's long-range dependencies. This model outperforms both CNN and Transformer's previous SOTA methods.	This manuscript proposes a hybrid architecture termed ConTrans for medical segmentation. It not only exploits CNN's capacity in capturing detailed local information, but also leverages Transformer to build long-range dependencies and global contexts. Extensive experiments on four typical medical segmentation tasks across ten public datasets demonstrate its effectiveness.	The authors propose a model that combines features from Swin Transformer and a well-designed CNN to perform medical image segmentation. A feature integration module named spatial-reduction-cross-attention (SRCA) module are also proposed for effectively fusing the two-style features. Extensive experiments are performed on various datasets to show the effectiveness of the proposed methods.	This paper is well-written, understandable, and obtains impressive results. The paper is insightful. It analyzes the inherent shortcomings of CNN and Transformer and comes up with a well-designed hybrid network to inherit their merits. DAB (Depthwise Attention Block) looks promising. This could not only be applied to medical image segmentation but also other CV-related tasks.	++ This method achieves promising results on three common medical segmentation tasks, including polyp segmentation, skin lesion segmentation, pneumonia lesion segmentation and cell segmentation). ++ This paper has good clarification and organization.	The authors explore and exploit the most advanced progress of vision transformer (Swin Transformer, Cross-attention) in medical segmentation task; Evaluation on multiple datasets and achieve state-of-the-art performance.	"The authors mention the ""inductive biases"" of CNNs several times. However, they did not elaborate on what these biases are. According to Section 2.2, ""they may not perform well on medical datasets due to the lack of inductive biases inherent to CNNs."" and then propose ""Depthwise Attention Block (DAB)"". This DAB module provides depthwise convolution, channel attention, and spatial attention, which are not provided by vanilla-CNNs. The DAB is an enhancement of CNNs. It may be necessary to revise the expression here to show clearly what vanilla-CNNs bring to Transformer and what DAB brings to CNNs. The evaluation metrics are insufficient to validate the performance. Specifically, the authors should include experiments of the critical metrics, i.e., E-measure, Fbw, and S-measure. References to these metrics can be found in the following papers: [1] Liu et al., Visual Saliency Transformer. ICCV 2021 (E-measure) [2] Wei et al., F3Net: Fusion, Feedback and Focus for Salient Object Detection. (E-measure) [3] Su et al., Selectivity or Invariance Boundary-aware Salient Object Detection, ICCV 2019 (Fbw) [4] Zhao et al., Pyramid Feature Attention Network for Saliency Detection, CVPR 2019 (Fbw) [5] Fu et al., Deepside A General Deep Framework for Salient Object Detection, Neurocomputing 2019 (E-measure, Fbw, S-measure)"	"- The authors claim that ""The proposed Depthwise Attention Block (DAB)"". However, this module is commonly-used in the computer vision, which could not become the one of three major contributions of this manuscript. - Thought promising results have been achieved, the whole comparison maybe not fair level. The authors better provide the efficiency comparison in the experimental section, for example, the inference speed and model parameters. - The ablation study is insufficient. How about the performance of basis Swin Transformer backbone? Furthermore, I am interesting whether such improvement comes from the Swin Transformer."	The proposed DAB module is basically the same with CBAM ([28] as cited in the paper]), with the only change of depth-wise conv; however, the reason why depth-wise conv would reduce the local redundancy compared to conv is not justified; While the proposed method achieves SOTA on various dataset in terms of segmentation metrics, the model size and FLOPs are not shown. It is hard to know whether the improvement is due to larger model (as two parallel encoders are used here) or the proposed design; The evaluation datasets are mostly 2D segmentation, it would be more interesting to know the performance on, e.g. 3D dataset such as MRI and CT; As Swin Transformer has already introduced locality, the motivation of using two parallel encoders, one as CNN and another as Swin Transformer, is not well-justified; The design motivation of SRCA module is not justified, e.g. why the transformer feature is used as query and CNN feature is used as key and value?	It is possible to reproduce the paper. The paper presents a straightforward and well-explained model.	I have checked the 'Reproducibility Response' from authors. And I also think it is not difficult to reproduce this model.	Sufficient implementation details are provided in the manuscript.	"Fig.1's notions and texts are too small to be read on print paper. Please consider resizing them. There is an extra ""Q"" in the first ""LN"" of the SRCA module in Fig.1. Probably a miss type. The authors may remove it. To improve Table.1, it would be better to include the categories in which the Methods fall. By adding a column to show whether the methods are Transformer-based or CNN-based."	Please see section 4 and section 5.	"Sec 1. P1: ""To alleviate such issues, considerable efforts are devoted to enlarging the receptive fields by introducing effective sampling strategies [7,14], spatial pyramid enhancement [31] or various attention mechanism [28]"", how do spatial pyramid and attention mechanism in [28] try to enlarge receptive fields? also, it would be better to cite works in the medical domain if possible; In the SRCA module, transformer features are only used as ""query"", that is to say, the transformer features and CNN features are not fused, the SRCA module essentially filter the CNN feature and is equivalent to a spatial attention module of the CNN features; however, the paper claim that SRCA module fuse the two-style features, which is not precise; please consider rephrase the presentation; Ablation study, how do you ablate DAB and SRCA, in other words, what is the baseline version of your model? More detailed analysis of the ablation study is needed; As mentioned in the weakness part, I suggest include the model size and FLOPs as comparison between other methods, as using two parallel encoders may significantly increase the model parameters; also, the design motivation of parallel encoder, DAB module and the SRCA module should be enhanced; Evaluate the methods on 3D medical dataset and compared with, e.g. nnUNet3D."	Refer to weakness and strength section.	Overall, this manuscript have some merits and ideas. However, the introduced attention-based module is not novel enough, which could not become the major contribution. I could not recommend the acceptance of this manuscript at such top-ranking performance.	While the proposed model achieve SOTA on various dataset, whether the improvement is due to proposed design or simply using a larger encoder is unclear; on the other hand, most of the contribution is derivative and based on prior work with few modifications and weak motivation.
108-Paper0616	ContraReg: Contrastive Learning of Multi-modality Unsupervised Deformable Image Registration	The authors propose a new approach for multi-modal image registration that uses patch-wise features to learn a multi-scale multi-modality embedding space for the loss function.	In this work, the author introduces a new contrastive loss for deep learning multimodality registration. Three networks are used : a registration network, a T1 auto-encoder and a T2 autoencoder. The deformed image and the reference image pass through the two auto-encoder and the loss is calculated between their projection. The author also used a hypernetwork to choose the regularisation parameter lambda.	The authors present a new approach for unsupervised Deep Learning-based multimodal image registration. Their approach builds on contrastive learning techniques and is validated on the registration of brain image data from the Human Connectom Project and compared to other current approaches and variants of the proposed method.	The use of an embedded feature-based contrast comparison is a novel idea, and the presented results suggest strong performance for the proposed approach over existing methods. In addition, the paper was well written, and does a fantastic job covering relevant work.	The author deals with the hard problem of multi modality registration and proposed an approach based on contrastive learning. They compared with different classic multimodality losses and obtained better results.	New approach. The novelty of the approach lies in the use of contrastive representation learning to measure the similarity of images with different modalities. The approach is novel in that it does not build on common geometric image similarity measures such as NCC or MI, nor does it explicitly learn a metric. It provides a very interesting and fully data-driven alternative for multimodal registration. The presentation is clear and sound with a balanced high technical level.  The motivation is clear, with a detailed list of works cited (>20 references). Strong evaluation and comparison. The evaluation compares several variants of the approach using state-of-the-art methods, including hyperparameter optimization, to provide a fair and realistic picture of the comparison. The results are good and slightly superior to the compared methods for the presented expalele. This is very interesting as it strongly suggests that such data-driven methods could be a quite attractive and effective approach for multimodal registration.	Discussion/analysis regarding some key parts of the algorithm seems to be missing. Namely, the impact of the autoencoder performance on the alignment result, and the chosen parameterizations for the STN/registration network. The dataset used for the evaluation of this work is disappointing. Outside of the rare cases of lost data, there are very limited situations where one needs to actually perform inter-subject registration between T1w and T2w MRI, since the two contrasts are almost always acquired together. The results would be more convincing if the method was tested on a real-world application.	To the reviewer it is not clear if the proposed method is better than SynthMorph. Indeed, SynthMorph achieves a better Folds and sdLog (B1 Table1), and from the curve Figure 3 (row3), it seems that for the same level of % Folding Voxels, the dice score of the proposed method is much lower than SynthMorph. The author did not discuss the impact of using a supplementary network in term of training time and memory used. If we have a fixed GPU memory, is it better to add two autoencoders for the constrative loss or increase the number of parameters of the registration network ? The clarity of the paper should be improved, and specially the clarity of the methodology .	"The paper contains a lot of information and the presentation is quite ""dense"" due to the page limitation.   Therefore, it is more accessible to a knowledgeable audience."	Potential reproducibility of this work is moderate. The authors do a good job of explaining their method/hyperparameters, but are unwilling to release their software.	It seems to the reviewer that all the parameters are not given, to fully reproduce the paper. For instance, the number of negative and positive pairs to calculate the loss and how these pairs are obtained.  The reviewer highly recommand to provide the code of this paper, if the paper is accepted, as it could explain part of the methodolohy.	The authors do not provide any code. The data used are publicly available. The experiments and methods are clearly described in the paper. Qualitative reproduction of the results should be possible.	It would be helpful if the author can provide some additional discussion regarding the auto-encoder used for feature detection. It seems to be a key part of the network, and plays a big role in determining the reliability of the contrast loss. What happens if the encoder just doesn't work well for a given modality? I would really encourage the authors to test their method on a more real-world  example of multi-modal data alignment. Perhaps intra-operative to pre-operative alignment, as they motivated in their introduction. Or even CT to MR, which would demonstrate the method is robust to large nonlinear differences between the intensity spaces.	The reviewer suggests to the author to improve the clarity of the paper, specially the methodology, and to perform more experiement with published registration methods, to improve the quality of this paper.	In summary, I think this is an excellent piece of work. I am very impressed with the clarity and density of the presentation and think this is close to what can be accommodated in 8 pages. In fact, I have no concerns or further substantive comments. My only minor complaint would be that the font size in the diagrams in Fig. 3 is too small and the legends are difficult to read.	The authors present a novel method with promise results.	The clarity of the paper and the insufficient comparisons with other methods.	The authors present a novel approach to multimodal image registration using deep learning. Excellent concise, theoretically sound paper with clear motivation, presentation of related work, good evaluation and comparison with the state of the art.
109-Paper1057	Contrast-free Liver Tumor Detection using Ternary Knowledge Transferred Teacher-student Deep Reinforcement Learning	A ternary knowledge transferred teacher-student DRL (Ts-DRL) for liver tumor detection that is contrast-free (without the use of chemical injection) which is safe, speedy, and inexpensive technology.	This paper presents contrast-free liver tumor detection using ternary knowledge transferred teacher-student deep reinforcement learning. The test results are also provided.	This paper enhance the previous work WSTS by integrating additive teacher model features into the teacher-student based DRL framework with a novel P-strategy. Experiments show that the proposed method outperforms previous baseline methods.	Paper has good and strong evaluation dataset - 352 patients with data from two MR scanners which helps to prove that the technique proposed has strong test value. The formulation of the technique is simple but good enough to be understood in layman term ; Ts-DRL provides knowledge set that is <A,R,F>  - action, reward, and feature from the DRL's agents of teacher network to guide the student network learning.This ternary knowledges also inform the student network what to do,and innovatively embeds F to teach the student the reason and purpose behind the A,R. The novelty of this method is the why to do of the ternary knowledge framework (A,R,F) .Thus, it improves the effectiveness of the teacher-student DRL framework and the accuracy of detection. It also  possesses a novel progressive hierarchy transferring strategy (P-strategy) to strengthen the transfer of ternary knowledge.	The topci of this paper is interesting. The methodology used in this work is well described.	The method is well motivated to use a P-strategy to stale the training process which is better than other DRL strategy including DQN, DPG, DDPG, etc. The extensive experimental results show that the proposed methods can detect tumor better than previous baseline models.	In testing phase, well trained student network drop the ternary knowledge from the teacher network. Why the student get rid of the knowledge from the teacher network? What method is used? Without the teacher network, the student will not have the previous knowledge plus with the new knowledge. Is this process necessary? Experimental ablation is used in research on animals. Is this experiment necessary for the human liver datasets?	More jobs should be done to better verify the work.	It may not easy to reproduce the results, because some details are missing: how to select hyper-parameter in the reward function, grid searching? for the proposed P-strategy, since P-strategy only introduces one item (4 items in total) from the student network's features that is most similar to the teacher at each step,  how to decide when shoud perform the 4 exchangement to make the training process stable?	Most reproducibility checklist are fulfilled.	The reproducibility of the paper is OK.	It may not easy to reproduce the results due to some missing details and the in house dataset.	Paper has good organization and easy to follow experiments used for getting the right results. Results were explained in a manner where non-scientist person can understand the process of the method and result analysis used to prove the technique easy to follow.	Here, there are some review comments of this reviewer: 1 The computational cost of the proposed approach isn't discussed. The approach should be computationally efficient to be used in practical applications.  2 The proposed method might be sensitive to the values of its main controlling parameter. How did you tune the parameters?  3 Have you considered the effect of noises on the performance of the proposed method? Please discuss how this would impact the results and conclusions of this study. 4 The practicality of the approach should be further discussed.	More experiment details can be provided for the reproducibility. The resolution of Fig 4 can be improved for visual comparision. The regions of interest can be zoom in.	Author was able to justify the method used with good explanation in methodology and structured analysis. Ternary knowledge add another feature F where the student get to know why the action need to be done.	The topci of this paper is interesting. The paper is technialy sound with well presentation.	Although some details are missing, the paper study an important problem and the proposed method show improvement than previous baselines. This work will interest more researchers and clinical doctors if the datasets and source codes will be availabel.
110-Paper0148	Contrastive Functional Connectivity Graph Learning for Population-based fMRI Classification	The paper proposes a method to encode functional connectome features by using contrastive learning to obtain embeddings that are then used as inputs to graph convolutional networks for disease classification	The authors proposed a contrastive learning framework for population-based fMRI classification. Th proposed framework contains two parts: 1) constrastive graph learning and 2) dynamic graph classification. By employing both of the two parts, the proposed framework is able to outperform all the other benchmarks.	In this paper, the authors propose contrastive learning of functional connectivity graph for population-based fMRI classification. In addition, a population graph is constructed for dynamic graph classification. They perform experiments on ADHD datasets. The results show that the proposed method outperforms baseline methods.	The paper proposes a method to encode functional connectome features by using contrastive learning to obtain embeddings that are then used as inputs to graph convolutional networks for disease classification. As brain imaging datasets are scarce, attempts to overcome overfitting problems are needed.	This is a novel application of contrastive learning on the FC graph data. The AUC improvement looks pretty good.	Overall I think the idea of using contrastive learning on functional connectivity graph and Dynamic Graph Classification on Population is novel. The authors compare against a wide range of baseline methods and conducts statistical tests.	There are several issues in the paper. Notations are difficult to follow and I found several errors with mathematical formulations.  For example, First paragraph, section 2.1: Errors start with the notions P is defined as number of number of patients. What is cal_P refers to?  G_i^j is defined in line 1. Line 7, what is G_i referred to? Is P missing?  And so on .... The effects of random partitioning time-series into two sets need to be studied. Though authors generalize their claim on small datasets, methods are tested only one dataset. The details of dynamic graph classification (DGC) is sketchy and from the table 1, I doubt it is ineffective. References to some existing work on unsupervised graph embedding methods for connectome analysis is missing. For example: Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning Ming JinYizhen Zheng[...]Shirui Pan (2021),10.24963/ijcai.2021/204 Though comparison results are provided, how the performance matrices were derived - either from your own fair simulations or from literature has to be mentioned.	The constrastive learning framework proposed is mainly a straightforward application.	The authors only perform experiment on one dataset, they could extend their experiments on more datasets.	Details of the methods are sketchy and would be difficult to reproduce.	If the codes and data are available, the results should be reproducible.	The authors haven't release code, but they provide network architecture in Supplementary material. The ADHD dataset is also publicly available.	The paper proposes a method to encode functional connectome features by using contrastive learning to obtain embeddings that are then used as inputs to graph convolutional networks for disease classification. As brain imaging datasets are scarce, attempts to overcome overfitting problems are needed. There are several issues in the paper. Notations are difficult to follow and I found several errors with mathematical formulations.  For example, First paragraph, section 2.1: Errors start with the notions P is defined as number of number of patients. What is cal_P refers to?  G_i^j is defined in line 1. Line 7, what is G_i referred to? Is P missing?  And so on .... The effects of random partitioning time-series into two sets need to be studies. Though authors generalize their claim on small datasets, methods are tested only one dataset. The details of dynamic graph classification (DGC) is sketchy and from the table 1, I doubt it is ineffective. References to some existing work on unsupervised graph embedding methods for commectome analysis is missing. For example: Multi-Scale Contrastive Siamese Networks for Self-Supervised Graph Representation Learning Ming JinYizhen Zheng[...]Shirui Pan (2021),10.24963/ijcai.2021/204 Though comparison results are provided, how the performance matrices were derived - either from your own fair simulations or from literature has to be mentioned.	"In this paper, the authors proposed a constrastic learning framework for FC graph classification and showed improved performance in patients classification. Overall I found this paper to be pretty clearly written with good evidence of improvement. I only have a few comments Why weren't the PCD features included in the KNN model? If you remove PCD features, will the current method still outperform KNN? I am curious, if you train the network in a supervised-way, e.g., instead of considering two views from the same subject as ""attracted"", consider all the views from the same patient group as ""attracted"", will you get worse or better result?"	"They could have an encoder to extract imaging features from fMRI. Some typos can be fixed: e.g. ""The framework overflow"" -> ""The framework overview""."	Clarity of the methods and missing details of comparisons with state-of-the art methods	The improvement is good and the method is rather straightforward.	I think overall the idea is novel and their experimental evaluation is strong in terms of setting and results.
111-Paper1698	Contrastive learning for echocardiographic view integration	In this work they fuse two Echo views (A2C and A4C) to estimate the LV volume. The main contributions are the intra-subject contrastive loss and inter-subject contrastive loss which are proven to improve the performance of the model.	This paper proposed a contrastive learning method for view fusion to estimate LV volume in echocardiographic examination. Results on CAMUS dataset illustrate the effectiveness of the proposed approach.	This work presents a volume contrastive network used to derive the left ventricle volume (a 3D measurement) from Apical-2-Chamber and Apical-4-Chamber 2D echocardiographic views. The main contributions are tackling the 2D fusion information to generate 3D measuements and introducing intra and inter volume contrastive losses to improve this fusion	Well-written; clear story-line, and reliable results. Rigorous experiments with ablations studis which independently assess the impact of each component and idea. To the extent of my knowledge, the ideas around intra-subject contrastive loss and inter-subject contrastive loss are novel. Experiments are conducted on public datasets and source code will be released, hence, a more prominant impact is expected.	This paper is well-written and easy to follow. The figure is illustrative. The idea of contrastive learning makes sense especially considering the physical characteristic of ED/ES in the two views of A2C/A4C. The ablation experiments are good to illustrate the effectiveness of each proposed loss function.	The authors propose a novel method that uses a volume contrastive network for 2D information fusion. The network itself is not novel but the application is.  Furthermore, the authors propose using intra and inter subject constrastive losses by maximixing/minimizing positive/negative distances between pairs, which shows a deep understanding of the problem and a novel method to improve the solution. The evaluation is very detailed, with ablation studies showing the contribution of each of the features added to the base architecture and statistical analysis shoing the statistical significance of the improvement.	The work seems to have failed to cite other LV volume estimation works, and they are a plenty. In turn, they have not compared their method with such methods. Yet, to be fair, they have compared their method with the ablated versions of their own model which highlgihts the impact of each component.	The experimental results are not solid. Results are performed on only one dataset without comparison to the existing methods. I'm not familiar with the targeted task but I wonder if there's any existing work on this task for the purpose of comparison. The technical implementation of the contrastive loss is trivial. Direct maximizing and minimizing the eculidian distance is not optimal and barely used. In this scenario, I think triplet loss or regular contrastive loss (in self-supervised learning) are better alternatives to try and compare with simple distance-based losses.	The authors claim to be the first to apply volume contrastive networks to solve this problem but the evaluation lacks other methods in the literature that have been used to combine the views. The only comparison provided is the ablation study, which shows very convincing results. It would also be helpful to show some clinical correlation rather than just the MSE. For example, how does this improvement in the volume estimation help diagnosis, is this improvement relevant to the final task for which the volume estimation is used?	Source code will be included; all good.	Good if code is released.	Authors have provided enough detail for the paper to be reproducible and have used a public dataset	"Question: In the ""Inter-subject volume contrstive loss"", it is stated that ""we find a random subject i'from the batch in each training iteration which has a similar volume at phase p to form a positive pair"". How do you find this random subject from the batch? How do you guarantee that such sample is available in the batch for the given positive sample?"	Please see weaknesses.	This is a well written manuscript that clearly describes the problem at hand and the solution presented as well as the logic behind it. It shows a good understanding of the problem and presents some simple additions to a known architecture (contrastive networks) for a novel application.  It would be helpful to do a more in depth comparison of other state of the art methods to solve the LV volume from the 2 views the authors use and show the actual clinical impact of the error reduction the authors are showing.  Nonetheless the results from the ablation study show that each part that has been introduced contributes to the performance of the network and it would be useful to the community to have this published.	The idea, its explanation, and execution of experiments are complete and close to perfect. The paper would add value to the community.	The idea is interesting and the paper is well-written. However, the experimental drawbacks  and the technical implementation remains problematic.	The novelty of the paper is enough to be accepted and the ablation study evaluation provides convincing results that show that the proposed architecture and loss does indeed improve the model performance.
112-Paper1582	Contrastive Masked Transformers for Forecasting Renal Transplant Function	They propose the use of contrastive schemes, generating informative manifolds of DCE MRI exams of patients undergoing renal transplantation. Different self-supervised and weakly-supervised clinical pertinent tasks are explored to generate relevant features using the cosine similarity. They introduce a transformer-based architecture for forecasting of serum creatinine score, while proposing a tailored method to deal with missing data. In particular, Their method is using a key mask tensor that highlights the missing data and does not take them into account for the training of the sequential architecture. Such a design is very robust with respect to the position and number of missing data, while it provides better performance than other popular data imputation strategies.	They have twofolds in thier contributions:- They propose the use of contrastive schemes, generating informative manifolds of DCE MRI exams of patients undergoing renal transplantation. Different self-supervised and weakly-supervised clinical pertinent tasks are explored to generate relevant features using the cosine similarity. They present a transformer-based architecture for forecasting of serum creatinine score, while proposing a tailored method to deal with missing data.	Contrastive Masked Transformers for Forecasting Renal Transplant Function This paper presents a methodology for forecasting renal transplant function based on contrastive masked transfoms. Overall, the problem is interesting. However, the paper needs more work to be presented in the prestigious conference.	According to my knowledge, this study is among the first that propose a novel, robust, and clinically relevant framework for forecasting serum creatinine directly from imaging data. This study proposes a novel transformer based architecture tailored to deal with missing data for the challenging task of serum creatinine prediction 2 years posttransplantation using imaging modalities. First, they show the significant use of contrastive learning schemes for this task. Their trained representations outperform common transfer learning and contrastive approaches. Then, a transformer encoder architecture enables to input the sequential features data per follow-up in order to forecast the renal transplant function, including a custom method to handle missing data. Their strategy performs better than other commonly used data imputation techniques. Those promising results encourage the use of medical imaging through time to assist clinical practice for fast and robust monitoring of kidney transplants.	I think this is a new novel study for forecasting serum creatinine directly from imaging data.They proposed two contrastive learning schemes to explore meaningful data representations. They showed the significant use of contrastive learning schemes for this task. They used the transformers encoder  to input the sequential features data per follow-up to forecast the renal transplant function, including a custom method to handle missing data.	* Interesting research problem * Up-to-date reference list * The method outperforms related works	It's hard to reproduce.	I see the authors make challenging task of serum creatinine prediction 2 years posttransplantation, however, I see only four different follow-up exams ends with (M12) after one year not two years as they discussed. The authors need to discuss what is the limitations of the proposed framework and when the proposed system can fail. I see the authors used ResNet18 to extract a latent representation from the MRI volumes (size =, 512x512x[64-88] voxels), however, the resenet18 uses an input image of size 224 x 224; this mean a lot of information in the MRI images can be dicaseded during the training process. and this can effect on the training result. testing sample size (20) is very limited in this study also. I see the authors used the only 10-fold cross validations, are you tried different cross validations ? and what was the output ?	* Results are incomplete * Missing Discussion	Negative. The authors did not share the source codes and the results are limited.	I can't tell that this method can be reproducible cause they didn't applied the framework on a benchmark dataset	The paper is reproducible	It is difficult to reappear. The authors mentioned a lot method but the results were limited. It was hard to persuade the reviewers. More experiments will be better.	They need to incrase the test set. The authors need to discuss the proposed system limitations in addition when the system can fail. I see the authors can use their own CNN instead of ResNet18 cause this pretrained network loss a lot of information in MRI images The paper didn'y have figures enough to show how the proposed system works on MRI images and can show these images during the prediction stage over two years	Comparison with conference papers is not appropriate. Strong journal paper should be used instead Visual assessment of the results is missing Discussion is missing. How and why the method outperform others. Limitations should be given and why the approach not work well with certain cases	According to my knowledge, this study is among the first that propose a novel, robust, and clinically relevant framework for forecasting serum creatinine directly from imaging data.	Overall, the paper is intersted but have a lot of modifications and some experiments	The methods and results are sufficient. However, it needs more discussions
113-Paper1027	Contrastive Re-localization and History Distillation in Federated CMR Segmentation	The manuscript proposes an approach for the cross-center and cross-sequence cardiac segmentation problem following a federated learning framework. To deal with the distribution shift problem between clients and the server, a contrastive re-localization (CRL) module is applied which is facilitated by a cross-attention transformer. The optimization of the local client models are assisted by a momentum distillation module which stores the its own training history. An ablation study and a comparison with other federated learning approaches are conducted using Dice similarity index as the metric for accuracy.	Authors have proposed a federated learning framework, specifically suited for datasets with imbalanced distributions, where some datasets are more heterogeneous than the rest of the clients. Their method reduces representation bias in the model fusion and overcomes optimization stop in the model replacing. Their method FedCRLD uses a contrastive difference metric for the former purpose and uses a momentum distillation strategy for the latter.	A CRL module that corrects the server bias in federated learning is proposed and supposedly validated.	Working on the distribution shift problem in federated learning is important. The introduction of CRL and MD modules to the solution combines novelty with a realistic problem. Results show substantial improvements compared with standard methods.	1.Clarity in problem introduction and motivation: The problem statement regarding heterogeneous datasets and bias in FL is explained well and the motivation is quite clear. The figures are clear and help in understanding the problem. 2.Novelty of the method: The proposed method using momentum distillation is quite novel and the use of cross-attention transformer for reducing representation bias in this application seems interesting (though attention-based algorithm have been attempted for fairer client selection recently in Chen et al., 2021, ArXiv). 3.Works on cross sequence and validation on multiple datasets: The method is shown to work on cine-CMR and DE-CMR sequences obtained from different scanners/centres and dataset size is also different from each other. 4.Results are shown  to work, not only on heterogeneous scenario but also on homogeneous scenario (within cine-CMR sequence), indicating the adaptability of the model.	Comparison with other federated learning strategies and ablation studies are conducted.	The descriptions on CRL and MD lack clarity. Results could be further refined. The writing could be improved which could help the reproducibility as well.	Limited statistical evaluation: FedCRLD seems to perform better than the prior methods, data sharing and within individual clients, but improvements in certain datasets seem quite marginal. It would be quite interesting to see the statistical significance of the improvement in performance. So authors should perform tests wherever necessary and report the significance to better appreciate the effect of the method (same goes for the ablation study). Also would be good to see the standard deviation in addition to the average. Computational complexity: the method seems quite complex and attention networks are generally computationally heavy - what is the training time? Was any measure taken specifically to reduce computational load? Comparison of such settings could be something to try for the journal article or future work. Repetition of aims and lack of details: The aims have been explained in the intro section and has been repeated in the method section in great amount of details, while that space could have been used for explaining cross-attention transformer better (eg. details such as number of layers and filter sizes are missing).	The quality of writing and the way of presenting the methods are unsatisfactory (see 8 for details). The experiment descriptions are flawed.	The work is conducted on public datasets but the code is not provided. Overall the description could be improved for users to reimplement the method.	In general, the implementation details, dataset processing and training parameters (including Data Augmentation) have been explained quite well. As I said earlier, a bit more information on the model architecture would be helpful.  Also, certain practical information required for reproducibility could be useful - after the model training how was the model parameters aggregated (figure shows averaging but was it same after cross-attention in the subsequent rounds?), how do authors handle differences in the labels (see table 1) between M&Ms and Emidec datasets (any preprocessing done?) and how was the dice metric calculated?	A public dataset is used.  However, the reproducibility is limited due to inadequate description of the methods (e.g., structures of the unet used).	"Distribution shift is one of the major challenges in federated/distributed learning which is even more important when working on medical images obtained from multiple centers using different imaging modalities/sequences. The authors are attempting to solve a meaningful problem. The description on CRL lacks clarity and the authors could have used equations to help describe the loss function etc. Also the terms used in describing the cross-attention transformers are not defined clearly. The parameter alpha was never clearly introduced. Overall the paragraph between page 4 and 5 needs to be rewritten with the assistance of Fig. 3. The MD training procedure in 2.2 could be made much easier to follow if it is turned into pseudo code/algorithm. Overall the authors could follow the following paper for the description of methods: Li, Junnan, Ramprasaath Selvaraju, Akhilesh Gotmare, Shafiq Joty, Caiming Xiong, and Steven Chu Hong Hoi. ""Align before fuse: Vision and language representation learning with momentum distillation."" Advances in Neural Information Processing Systems 34 (2021). The authors did not report structure-wise results and distance-based measures. Both could be put into supplementary materials if there is not enough space. From the ablation study, the mutual information-based loss seems to have the most impact to the performance of the approach. It could be further proved by removing two model features per experiment. The writing of the manuscript could be improved. Multiple statements are made without context or clear purpose, e.g. on page 4 ""In cross-attention transformer, the localized server distribution benefit client"", on page 5 ""The Seg_p is the regularization"". Also there are multiple typos, e.g. ""wit weight"" on page 4 should be ""with weight"". The authors claim that ""For the first time, our FedCRLD enables the cross-center cross-sequence medical image segmentation possible"", which is not 100% correct. For example, the following paper has made an attempt on cross-center and cross-sequence problem, and it was done on a different structure as well:  Dani Kiyasseh, et al., ""Segmentation of Left Atrial MR Images via Self-supervised Semi-supervised Meta-learning."" In International Conference on Medical Image Computing and Computer-Assisted Intervention, pp. 13-24. Springer, Cham, 2021."	In addition to my above remarks, here are my specific comments: Please condense the introduction for the methods section and add more details regarding the model architecture. Please add more details and steps involved in label preparation, data preprocessing and how evaluation metric is calculated. Authors should add statistical test results and standard deviation values for dice (if dice values are calculated for corresponding number of labels as specified in table 1, it is not a fair comparison) A brief comment on the computational complexity of the method would be helpful. For future work, Authors could extend their analysis to see if the proposed method would be effective for CT vs CMR. It would be good to see the upper limit of model capacity (to know the maximum range of heterogeneity the model can handle without affecting the optimization)	I think the manuscript could be improved from the following aspects: A careful proof-reading is necessary to correct the grammatical errors present in the paper. To name a few:  1)makes the segmentation is still a challenging task 2)causes the multi-center multi-sequence CMR has larger heterogeneity than regular studies 3) However, the heterogeneous server model replacing client model in FL directly causes the long-distance clients utilize worse optimization replace original optimization 4) Through minimizing KL divergence wit weight... 5) The data-sharing strategy makes the model are biased towards similar data... Large chunk of texts (or texts expressing the same information) are repeating in the Abstract, Introduction and Methodology sections, while the actual methods are not clearly described (in terms of Fig 2 and Fig 3). The authors could make better use of the space for the essentials (add more detailed description of the methods and the U-Net used for experiments). Qualitative comparisons among the methods are lacking. Besides quantitative comparisons (Table 2), qualitative comparisons (e.g., the output illustrated in Fig. 3) are helpful to visually comprehend the improvements of the proposed methods over previous ones. It is unclear if the 3D U-Net used for comparison with the other methods (Table 2 [12,9,10,3]) are kept the same and the FL strategies are the sole variable for comparison. It is helpful if the authors could provide more detailed descriptions about the experimental settings. Visual results (appendix) are compared with traditional deep learning methods instead of other federated learning strategies.	The manuscript attacks a legitimate problem in federated learning and has introduced innovative ideas. Results demonstrate superior performance compared with established approaches. The major flaw is the description of the method which could be addressed in a proper rebuttal.	Representation bias is quite a huge problem in FL and the method aims to reduce the bias at the same time ensuring the optimisation is continuous and not affected by the heterogeneity in the dataset characteristics. The method is novel and handles multiple functions simultaneously and is interesting to see in this specific application. The comparative analysis is extensive on multiple datasets. The work is clearly presented with good figures.	The quality of writing, and the way of presenting the methods are unsatisfacotry. The experiments that support the claimed contributions are not described clearly and convincingly.
114-Paper0573	Contrastive Transformer-based Multiple Instance Learning for Weakly Supervised Polyp Frame Detection	The paper introduce a novel robust anomaly video classification method to detect polyp frames in colonoscopy videos.	This paper proposes a weakly-supervised framework based on transformers and a contrastive snippet mining approach to identify frames with abnormality (eg. Polyps) from colonoscopy video frames. An imbalanced dataset is collected from publicly available colonoscopy data and used in this work. The method is compared against some SOTA work and the results indicate better performance in abnormality detection.	The authors propose to use I3D to extract features from colonoscopy videos for the convolutional transformer to achieve Polyp Frame Detection. The authors utilize multiple instance learning (MIL) and contrastive snippet mining (CSM) to further improve the performance of the model. Experiment results show that their proposed method can outperform some SOTA methods for Polyp Frame Detection on a new dataset.	The idea is novel, with new proposed hard/easy example mining based on the task. The illustration is clear. The experiments are robust and the results are promissing.	The paper deals with one of the challenging problems in colonoscopy, detecting abnormality, especially the video snippets with flat or small polyps. A dataset including normal, abnoram(polyps) is collected from publicly available data.	The authors combine multiple methods in the literature as their method for Polyp Frame Detection. This combination does have its advantage and outperforms many SOTA methods. From this aspect, the system is well-designed and has some novelty.	See comments.	"A clarification on Cls token is required; ""The Cls token is applied for a video classifier to predict if a video contains anomalies."" Please elaborate on this as all readers are not familiar with transforms. it seems the paper is missing a comparison against another SOTA which is using similar snippet mining approach ""CoLA: Weakly-Supervised Temporal Action Localization with Snippet Contrastive Learning, Zhang et al, 2021"" More clarification about the dataset is required, as one of the claims is that this work has a better performance when small or flat lesions appear. Therefore, a table presenting the percentage of these sorts of lesions and the performance of the framework on them is required."	"(1) The authors did not cite previous work in the Contrastive Snippet Mining section.  Please also fix the words in the contribution section to reflect this. Previous work is here: Zhang, Can, et al. ""Cola: Weakly-supervised temporal action localization with snippet contrastive learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021. It is called Snippet Contrast (SniCo) Loss in the above-mentioned paper. The Polyp Frame Detection is a special use case in the Temporal Action Localization problem category. (2) The authors combined several public colonoscopy datasets and build a new large-scale diverse colonoscopy video dataset to benchmark their method against other methods. However, the authors only provided the training details for their proposed methods. As this is a new dataset, training parameters might be different from the original papers. Please also provide training details for all other methods that are used in the benchmark. It will be also more clear for the readers if the authors provide method differences (For example: Did MIL is used? What Loss is used in training?) in Table 1."	The author has provided a lot of experiment details.	Some information is missing for example how the Cls are calculated, but I assume that is explained inside their code repo.	The authors provide training details for their proposed method in the paper and they will share their code and dataset if the paper is accepted. I do not see a reproducibility issue for their proposed method.	The reason why depth-wise 1D conv works for temporal modeling instead of FC is not fully explained. If I3D is not fine-tuned on any medical dataset, it is recommend at least show what is the (expected) output of I3D. Otherwise, it is pretty confusing why it would work since pretrained dataset has nothing similar. One of the key contribution, as stated in paper, is the selection of hard/easy example. However the description is hard to follow/understand. It is strongly recommended to expand Fig.2 for a much more detailed explanation. For example, how the hard abnormal snippets are formed is not directly shown in the figure.	please refer to section 5	For weakness point (1), please cite previous work or share the reasoning behind why citation is not needed.  For weakness point (2), please provide training details for all methods benchmarked on the new dataset. It will be great to share the reasoning behind why combining multiple open-source datasets instead of applying the proposed method to each dataset and compared it with papers published accordingly.	The paper is well written, except some places are not perfectly organized which may be caused limited space.	This paper proposes a framework to identify abnormalities (frames that include polyps) from colonoscopy video snippets. The authors collected a dataset from publicly available data. This data should be approved by experts before publication, and further experiments are required to validate this method against the one which deployed a similar approach.	"This paper is more of an application paper in my eyes. Although the building blocks of the system come from the literature, the proposed system does have a great performance. It can be a novel application paper from this aspect. This is the reason I vote ""Weak accept"" for the paper."
115-Paper0458	Coronary R-CNN: Vessel-wise Method for Coronary Artery Lesion Detection and Analysis in Coronary CT Angiography	The paper discuss a two stage cascade neural network to detect and classify abnormal regions in a coronary artery. The first neural network is used for detection and the second neural network is used for classification of the abnormal regions into plaque type and degree of stenosis. The authors discuss their results and compare them with other existing methods.	This paper presents a deep learning based method for coronary artery lesion detection and analysis. It incorporates Faster R-CNN and a multi-task network for lesion detection, plaque classification and stenosis degree regression.	(1) This paper proposed a novel vessel-wise detection architecture inspired by Faster R-CNN To the best of our knowledge, it is the first application of this type of method on coronary artery lesion detection. (2) This paper proposed a multi-head analysis module that can predict not only plaque types but also the exact stenosis degree, rather than just significant stenosis classification. (3) This paper achieved an outstanding performance on a dataset consisting of 1031 CCTA images with 7961 vessels.	Main Strengths are: Well written and explained Good and convincing diagrams Comparison with existing methods. Experiments Large dataset	The main strengths of this paper include introducing Faster R-CNN framework for lesion detection. Then a multi-task network is employed for plaque classification and stenosis. In the plaque classification branch, two FC layers are used to determine the presence of calcified and non-calcified plaque respectively. Experiments have been conducted using a large clinical dataset consisting of 1031 CCTA images.	The whole curved planar reformation (CPR) volume along the coronary artery centerline is used for the coronary plaques analysis.  This is an application innovation	The paper presents a good application of the Faster R-CNN method for abnormality detection but in doing so lacks to present any new innovation/method for coronary CTA. The paper also lacks details in centerline extraction and vessel segmentation methods The paper does not provide any details on how straight line MPR views were extracted from cardiac CTA. The authors should have omitted that the data was collected from 6 centers in China in order to not break anonymity.	The first stage of the method seems to be a simple application of Faster RCNN to the lesion detection problem without modifications for medical images or lesion detection target. The second stage of the multitasking network is an intuitive solution to accomplish lesion classification and stenosis regression at the same time. Therefore the method may not be innovative enough. Please see detailed comments below.	This paper is generally biased towards innovation in application, and the method and ideas are borrowed from Faster R-CNN.	I know that the centerline extraction and straight MPR generation tools sometimes comes with the softwares associated with the scanner. But they should be clearly specified in the paper. If the authors are using some other tools for centerline extraction and straight line MPR generation, they should specify that too. If they make the code available that would be very helpful for reproducibility too.	The authors did not release all the details about the proposed two modules, such as network architecture and feature map channels. The dataset used in the experiments are collected clinically and will not be made public.	The dataset is nice and hopefully made public.	I think the paper is well written and explained. It was consistent and diagrams were appropriately explained. The authors have compared their work to previous published works too. However, it is lacking some details as mentioned earlier and I think including those details would make it an interesting paper. Some other work in the literature for example (Performance of a Deep Neural Network Algorithm Based on a Small Medical Image Dataset: Incremental Impact of 3D-to-2D Reformation Combined with Novel Data Augmentation, Photometric Conversion, or Transfer Learning, Automatic stenosis recognition from coronary angiography using convolutional neural networks) are missing. I would encourage the authors to include them as they see fit.	"(1) For vessel lesion detection, using the curved planar reformation image stacked from centerline seems to be a common preprocessing method. However, I think it is necessary to explain why this transformation is used instead of the original 3D volume of patch.  (2) The proposed detection module seems to be a simple application of Faster RCNN to the lesion detection problem without modifications for medical images or lesion detection target. (3) Ablation experiments are needed to demonstrate the superiority of the multi-task network in the second stage, i.e., is it more effective than performing plaque classification and stenosis degree regression separately? (4) Why is it necessary to refine the localization of the region proposals in the second stage? (5) What does the word ""vessel-wise"" in the title mean specifically? (6) More details are needed for the ground-truth of stenosis degree. What is the doctor's annotation? What are the advantages of the proposed regression method between [0,1] compared with performing classification directly? (7) The term ""polar coordinates"" appear for the first time in session ""Ablation Experiments"", but is treated as a major novelty, which needs more explanation in the ""Method""."	The author mentioned arbitrary CPR length is acceptable. Did the experiments in this paper apply different length CPR inputs?	I think the paper is well written, but it lacks any new technique or ideas. The results are comparable to the previous methods and I do not see any statistically significant difference in the accuracy values.	The method may not be innovative enough. The first stage of the method seems to be a simple application of Faster RCNN to the lesion detection problem without modifications for medical images or lesion detection target. More details and explanation need to be added.	This paper tackles detection of plaque-type and stenosis degree simultaneously by inputting the whole curved planar reformation (CPR) volume along the coronary artery centerline instead of slices. A commonly used Faster R-CNN and a  novel two binary digits code loss are applied. The method is verified on a private CCTA dataset and the results demonstrated the efficacy of the approach.
116-Paper2123	CorticalFlow++: Boosting Cortical Surface Reconstruction Accuracy, Regularity, and Interoperability	In this paper, the authors propose an upgraded cortical surface reconstruction pipeline upon CorticalFlow. The efforts exist in three perspectives: a more accurate ODE solver, a sommther template generation routine, and a deformation process from white surface to pial surface.	This paper improves the cortical flow method in the following 3 aspects: Using the Runge-Kutta method to replace the forward Euler method to improve computation efficiency and the approximation accuracy. Using a genus zero smooth mesh templates to replace the convex-hull template in the cortical flow method. Using the predicted white surface to further predict the pial surface, which can build the vertex-wise correspondence between white and pial surface.	The paper presents cortical surface reconstruction using neural nets. The underlying architecture follows what was originally proposed in CorticalFlow. The authors propose the RK4 ODE solver, cortical shape-like template, and pial reconstruction from the WM surfaces. The experimental results show improved performance over the original CorticalFlow and also compare with a single baseline method (PialNN).	This is a technical submission addressing a fundamental but important problem in brain morphometric analysis, cortical reconstruction. The major novelty lies in the improvement of pipeline and practical application scenario. The proposed work competes with several latest main-stream methods, and shows a strong performance gain.	Accurately located the weaknesses of the cortical flow method and improved these weaknesses correspondingly. The paper organization and writing is quite clear, which makes the paper easy to follow.	The proposed method extends [15]. The authors improves each component in [15] for better cortical surface reconstruction. The RK4 ODE solver was used for accurate estimation of the solution to the flow ODE. It is well-known that RK4 can offer a more accurate solution than the Euler forward method. The union of training volumes was used as an initial template. This approach may better capture deep sulci by placing the initial vertices close to the cortical tissue. Pial reconstruction was done by employing the estimated white surface rather than a convex hull template. This idea has been widely adapted in the classic surface reconstruction pipelines and generally offers better CSF/GM boundaries.	(1) Motivation of using fourth-order RK approximation is less discussed. The accuracy is theoretically expected to be better than Euler method, but isn't RK4 an iterative method, too? Is backward Euler or RK2 a reasonable option (esp backward Euler)? Comparing with these popular methods is welcomed. (2) Concerns on the generalization. Is the proposed method sensitive to the inputs? For example,  if model is trained with high-resolution MRI scans, what's its performance on lower-resolution MRI scans (eg, train with ADNI2, test on ADNI1)? Such aspects are expected to be discussed. (3) What about the training efficiency? Testing efficiency is well demonstrated, but the training efficiency is not given. Please provide more information about the training process, such as convergence information, data preprocessing, epochs used to train, etc. As far as I know, RK4 converges slower compared with forward Euler, you have to set a larger time step for RK4, so I'm interested in its actual training efficiency in this task. (4) Not see in the submission if the code will be released to public.	These improvements are incremental. Although the weakness of convex-hull is explained, however, using the proposed smooth templates is dependent on the brain size. And the improvements contributed to this template is not discussed or validated.	See below.	The general idea is expressed clearly, but I didn't see from the submission that the code will be released somewhere.	The reproducibility of the paper is good.	It seems reproducible.	"Besides the weaknesses mentioned in Sec 5, here some other questions: (1) About white-to-pial surface morphing, considering the thickness of the gray matter is itself not a ""big"" number, so a ""small"" deformation might be a large influence. I wonder if any constraints on the thickness are taken into consideration.  (2) Question about registration of scans. FreeSurfer will do the registration, I wonder if your method will set a canonical template and mapping all inputs to this template."	Please justify and discuss whether the size of the brain have influences on the template generation. Also, if the test brain is larger than all the training brain, would the method still work? The current paper mainly validated the Runge-Kutta improvement, but not discussed the influence of the template improvement. The author claims the template improvement is a contribution, so it need some addition discussion to justify.	The motivation is great in the proposed template reconstruction. However, in Fig. 1, the actual template does not make a huge difference from the convex hull template in visual inspection. This is perhaps because the average across all the training samples. Although the authors show some examples in Fig. 1, it is still hard to see which regions have indeed a merit using this template. The white to pial mapping is a good approach, but this idea is already adapted in many classic surface reconstruction pipelines (e.g., FreeSurfer, CIVET, etc.). Please acknowledge this strategy in the manuscript. RK4 generally requires more time and memory requirement. It is a surprise that the computing burden is negligible on switching Euler forward to RK4. In particular, memory consumption presumably remains unchanged from Table 1. Could the authors discuss more about this? One baseline method is insufficient for validation. I understand the dataset is overlap with [15], but Table 2 does not appear in [15]. Why do the inference time and required GPU memory differ from [15]? The current evaluation is performed under the assumption that FreeSurfer is ground-truth. Nevertheless, the surface reconstruction quality can be validated on other metrics such as manual landmark placement, geodesic distance of the landmarks on the reconstructed surfaces, etc. As this is a conference paper, there might be no enough room.	I would like to hear from the authors about my questions. I have concerns on the feasibility of the proposed method.	The paper is well written and the proposed method is easy to follow.	See above.
117-Paper0775	CRISP - Reliable Uncertainty Estimation for Medical Image Segmentation	The paper aims to integrate anatomical prior knowledge into uncertainty estimation via a novel method called CRISP via contrastive learning where the valid segmentations to the corresponding images are defined as positive samples.	The authors introduced the nearest neighborhood ensemble in the latent space as an uncertainty estimation. For that, the authors leveraged contrastive training between image and segmentation. The final uncertainty map is obtained from a weighted sum of error between the predicted mask and the k-nearest ground truth masks from the training data.	The paper proposes a method for uncertainty estimation for image segmentation. The proposed architecture consists of a segmentation (shape) encoder, segmentation decoder, and image encoder. The architecture is trained such that image and corresponding shape representations are mapped to a similar location in the latent space while at the same time segmentation autoencoder is trained to reconstruct shapes. In test time, an image is encoded and M shape representations from the training set closest to the encoded image representation are found. Also, the most similar M images are reconstructed in the shape decoder. Uncertainty map is obtained as the weighted sum of the differences between the predicted segmentation and the reconstructed M shapes. Experiments are performed on 4 different datasets and the results are compared LCE, ConfidNet and MC_Dropout which shows improvement in many cases.	The task tackled by the paper is highly relevant to the community especially for clinical deployments of deep learning methods. It is very important to have anatomical bias in the uncertainty maps to increase the interpretability of the confidence of the neural networks. The paper is well written and easy to follow. The evaluations are very rigorous with an evaluation metric that is proposed by the authors that i believe is very suitable for the task at hand. The qualitative results provided are very impressive an dpromising. Usually uncertainty estimation methods tend to boil down to edge detection as discussed in the paper and the method presented seems it goes fir the first time beyond being an edge detector.	The authors leveraged contrastive learning for joint image-segmentation embedding, which leads to better calibration of predicted confidence and true probability. While the formulation has drawbacks, the idea to have an uncertainty estimator based on an anatomically feasible shape prior has merit.	The idea of mapping images and segmentations to closer in the latent space and exploiting this information for uncertainty estimation is a quite interesting idea. The proposed method is evaluated on multiple dataset and show improvement compared to 3 existing methods.	I would have liked to see more comparisons to generative approaches like using GANs or probabilstic U-Nets posterior distribution modeling to ground truth annotations. Given the complex latent space modeling of the paper more comparisons to simpler approaches for the latent space / similarity heuristics could be experimented.	The authors did not clarify what kind of uncertainty this is. Given that they compared with Monte Carlo dropout, I assume it is epistemic. What guarantees that a perturbation in model weight will land on a similar latent vector in the hypersphere as found from the nearest neighborhood? It also raises questions about how compact is the latent space. There is a major drawback to this formulation. The network produces perfect segmentation, which is identical to the ground truth. However, the nearest neighborhood ensemble will still produce uncertainty. Ideally, there should not be any uncertainty for a perfect prediction. How will one interpret the uncertainty in this scenario? How is the method different from simply computing dice between y* and Y, then finding the k-nearest neighbor and computing the uncertainty? Since shape is a major factor for medical image segmentation, how would the latent space disentangle shape and location information? How else has the location shift been taken care of in this study? What are CRISP-MC and CRISP-LCE? These are not explained clearly.	"I think the only major weakness is the lack of summary of the relevant literature and comparison with them. There are methods that are developed to estimate uncertainty in image segmentation [1, 2, 3]. These methods are directly comparable with the proposed method and I believe the method should be compared to at lease 1-2 of them. [1] Kohl et al. ""A Probabilistic U-Net for Segmentation of Ambiguous Images"", Neurips 2018. [2] Baumgartner et al. ""PHiSeg: Capturing Uncertainty in Medical Image Segmentation"", MICCAI 2019. [3] Monteiro et al. ""Stochastic Segmentation Networks: Modelling Spatially Correlated Aleatoric Uncertainty"", Neurips 2020. Although not directly related, there is another work which was proposed to estimate the segmentation accuracy [4]. I think mentioning this work would be interesting because working principles are very similar. While the proposed method selects M most similar images in the latent space, [4] exploits similarity in the image space after registration. I think the proposed method is more practical and sophisticated, it would be interesting to discuss similarities/differences with [4] in the paper. [4] Valindra et al. ""Reverse Classification Accuracy: Predicting Segmentation Performance in the Absence of Ground Truth"", TMI 2017."	I believe it is reproducible to a large extent.	The authors have provided adequate details to make the study reproducible.	Code is not available. The method is explained clearly in the paper and the experiments are performed on public datasets. So, I think it should be easy to reproduce the results in the paper.	I believe the paper presents a novel idea for a very relevant task nowadays. iven the page limit this might not be possible; however, I believe more comparisons to baselines and different latent space modeling and similarity metrics for contrastive learning for a journal version of the paper would strebgthen the findings of the paper further.	One needs to compare k-nearest neighbor uncertainty based on the mask level similarity like dice. The uncertainty formulation has major flaws, as mentioned in weakness point no. 2. The authors need to fix this. Additionally, they need to explain what kind of uncertainty the ensemble captures. Is it epistemic or aleatoric? Because segmentation from unseen data deviates from the training set, it does not necessarily have to be anatomically not consistent. Authors can try to encode the prior shape in the latent space to derive uncertainty in the predicted shape than just plausible segmentation ensembles. Uncertainty-error overlap was introduced in the metric section but never used later in the experiment.	"As I wrote in the weaknesses section, the only major weakness of the paper is the lack of comparison and discussion of the prior art. I would expect to see a summary of the more recent literature and comparison to some of them. There are some details that are unclear to me. 1) It is stated that ""Edge"" is applied to the predicted segmentation map. By which method were segmentation maps obtained? 2) CRISP is also evaluated on the outputs of MC-Dropout and LCE as stated in the first paragraph of page 6. Which samples of MC-Dropout and LCE were used for uncertainty estimation? Do methods CRISP-MC and CRISP-LCE in Table 1 correspond to these experiments? 3) I didn't quite understand why domain shift is simulated in the test images as mentioned in the 2nd paragraph of page 6. If data augmentation was performed during training which is a standard approach while training networks, the simulated images may become in-distribution since they have been seen during training. What is the aim of applying data augmentation in test time? How would the results change if there was no this step?"	The method presented is novel and the results clearly show that the method is able to provide uncertainty estimates beyon edges. I believe the paper is promising and insightsfull for future research.	While the experimental evidence is on par with existing uncertainty estimation, the proposed method has major flaws, as mentioned in point 2 in weakness. Further, it is unclear how the latent space k-nearest ensemble is different from a mask-level ensemble. Hence I recommend rejection.	I found the idea quite interesting and I am quite lean to suggest accept for the paper. However, lack of comparison and discussion of the relevant literature is a major weakness which prevent me to give a higher score. If the authors can justify why there is no comparison and discussion about the methods I mention, then I would be happy to increase my score.
118-Paper0133	CS2: A Controllable and Simultaneous Synthesizer of Images and Annotations with Minimal Human Intervention	This paper presents a method to synthesize images and labels for medical image segmentation. The method is technically sound. It is compared with three strong baseline methods on both in-house and public datasets and delivers promising results.	The paper describes a novel approach for synthetic CT generation. There are two key ideas. First, it is important to combine both masks and noise vectors into the generation process, that on one hand, allows controllable synthesis, and on the other, can learn a large variability of images. Second, by using modified Hounsfield unit (HU) maps as a replacement for traditional binary/class maps, more structural information is encoded into the model. Experiments demonstrate that the proposed method creates more anatomically correct CTs and demonstrates competitive results with respect to semantic segmentation.	The authors propose a new medical image synthesis method based on AdaIn GAN and average HU value assignment. With their contribution, the authors claim that less number of annotated images are required to produce more realistic synthetic images that can aid final segmentation output.	It is important to explore means to reduce the cost of medical image annotation. The presented method, CS2, is an promising approach. It utilizes 30 labeled images for data synthesize while its baseline counterparts require 1000 labeled images. The presented method is technically sound. Its key components are clearly explained and figures are provided to understand the method. This paper presents good experiments. The proposed method and three popular baseline approaches are compared on both in-house and public datasets. Promising quantitative and qualitative results are reported with detailed discussions.	The key strength of the paper is the clever idea of incorporating HU value maps into an image generation network for CTs. By adding controlled manipulation into the more traditional V2I network, the resulting approach combines the best of both V2I and M2I methods.	Novel idea to use less human-labeled data to generate more synthetic images. Comprehensive literature review of existing synthetic image generation methods.	I don't see any major weakness of this paper.	While I found the presented idea quite innovative, it would be very valuable to expand upon the quantitative experiments and discussion. Please also include a thorough description of limitations and failure cases of the approach.	Weak systematic analysis of the proposed method: It is not clear why the proposed method should be of attention to the readers in the field. Does the method really generalize? If so, why? Does it only apply to certain lung nodules in CT images that have distinctive HU values? Experiments are not enough to justify the proposed method: Lung segmentation is regarded as a relatively easy task that could be even done without any supervised ML method, not to mention the necessity of synthetic images. GGO segmentation too, is limited in demonstrating the novelty of the proposed method. The experiments do not fully show the strength of the method - there is only box plots comparing the proposed method and M2I for lung and GGO segmentation.	The authors promise to release code but not the in-house data. Hyper-parameters for model training are not provided.	Yes, it should be easy to reproduce the approach from the details included in the paper.	The paper seems to be reproducible, checking all the boxes.	The mentioned supplementary file is not provided. Font size in figures is too small to be read. Is data separated in patient-wise? Why only use one image from each CT volume? Why only 10 volumes are used to fine-tune nnUNet while 30 are used by the proposed method?	"First, please explain whether the approach is applicable to all CT acquisition methods, or only certain types of CT where HU values are reliable. It is not clear if the presented approach improves upon the lack of variability issue in M2I methods. It would be helpful to add an experiment or otherwise discuss this. For the downstream segmentation experiment, I would have expected to see results where the model is pre-trained on the synthetic data and fine-tuned on the small amount of real data, as synthetic data is typically used to augment smaller training datasets. Section 2.2:  in the section that states ""MSE between the HU value map and our synthesized CT images"", is the CT thresholded, or otherwise how is it post processed? Some sections of the paper reference details in the supplementary material, which is not included. Please also include some sample failure cases of the model. Is it possible that the HU maps and the generated CT do not correspond? Small detail: please check grammar in the abstract (""in this study"" does not follow sentence flow)."	Try to do more systematic analysis of why the proposed method should work. What is the role of the AdaIn GAN? What features do we need from an existing network? Is the method generalizable? Does it depend on HU values? Do more thorough experiments We need more comprehensive comparison tables showing the strength of the proposed method. Two box groups of box plots comparing the proposed method and M2I for lung and GGO segmentation is not enough. More compelling visual examples Visual examples showing why the proposed method generates more viable synthetic images compared to other existing method would be good - Perhaps in a scenario where the ground-truth annotations are scarce or it's hard to obtain such.	This paper studies important problem, presents technically sound approach, and reports promising results. These are major factors lead to the positive rate.	Certain aspects of the approach were unclear to me, however, the main idea is quite innovative. I hope the authors can address some of the technical questions (please see above) during the rebuttal period.	Experiments and analysis don't back the motivation of the paper: Synthetic image generation is beneficial for rare disease that do not have large number of human annotation readily available, or some pathologies that require multi-modal images to conduct proper analysis and therefore are hard to be annotated even by human experts. Lung segmentation and GGO segmentation are not.
119-Paper0756	Curvature-enhanced Implicit Function Network for High-quality Tooth Model Generation from CBCT Images	The paper presents a method for generating high quality tooth reconstructions from CBCT scans using superresolution ideas based on implicit function networks and a curvature enhancement for the tooth crown surface based on k-NN based features and a CNN regression.	The paper presents a method for 3D tooth model construction from CBCT and intra-oral images with implicit function networks (ICT). The motivation of the paper is to combine the intra-oral scans and CBCT images for better root and crowns due to limited resolution of CBCT images. The method is validated on a dataset of 50 subjects where 20 of them are test data. The method outperforms the compared methods.	key idea is to combine the commonly used CNN-based segmentation network with an implicit function network to generate 3D tooth models with fine-grained geometric details	Obtains a solution for producing quality tooth reconstructions from CBCT images without the need for the intra-oral scan and image registration to obtain an adequate accuracy.	The idea of using both modalities is very interesting. Also, implicit function networks are very popular for 3D shape reconstruction and this method is a new example of using IFN for medical data.  It is applicable to several practical medical reconstruction problems. The comparison of the method with other methods (especially HGMNet) is very useful for showing the effectiveness of using intro-oral data. I think the ablation study shows the effectiveness of the curvature enhancement and surface reconstruction. The figures provide useful information about the method. The curvature enhancement with a separate branch is an original approach.	its potential applicability in clinical practice	The method description is a little unclear because the descriptions of the training phase and that of how the method is used for unseen images have not been separated. Many details on the network architecture are missing, such as the number of filters in each block, the filter size, the number of training epochs, the learning rate, the optimizer used, etc. The evaluation uses a fixed test set of 20 images, which is not as convincing as a 5-fold cross-validation or multiple random splits.	Technical details of the method especially the network parameters (filter sizes, optimization parameters, etc.) are missing. The figures are very useful for presenting the method however their details are not sufficiently provided in the text. Also, more details on the experiment for the baseline models should be added. Another major weakness is dataset size. It is very hard to gather multi-model data, however, instead of using 20train, 10 validation, and 20 test data, cross-validation would be much fairer. Also, if are there any missing teeth, dental implants, braces, etc. they should be given. There is also reproducibility concerns about the work given in the below sections.	The innovation is weakness	Some details are missing (e.g. size and number of filters in conv layers), so the method cannot be exactly reproduced from it description.	I have some concerns about the reproducibility of the paper. The implementation and technical details of the method are not given in the paper. In the checklist, they claim that they will publish the data, training code, and network model. However, this information is not given in the text.	Yes	"In Table 2, since it is an ablation study, a more clear descrition of the ablations would be ""No implicit functions, no Curvature enhacement"" and so on."	"The idea of using two modalities for 3D tooth construction and using IFN has novelty. Cross-validation would be much better for comparison with other methods and validation of the method.  Although there is a section for ""implementation details"", the details of the method (especially network parameters) should be given. The running time of the method should be given. Also, since only the crown information comes from intra-oral data, the results of the construction of the root and crown can be given also individually."	Please list  the contribution or highlight of this paper How about the time complexity about your model compared other methods, because there is another segmentation step. There are lots of the reconstruct 3D shape models, why do you only select these 2 models to compare with your models? The number of SOTA is so less, which can not demonstrate this is a hot topic or Meaningful research work. After segmentation, each tooth is modeled by suface resconstruction, then whether it needs to be integrated into a full tooth image but not only one by one tooth model?	Obtains state of the art results, but has some clarity issues and misses many details about training. Also, experiments could be more convincing using cross-validation.	The work proposes a novel method for 3D tooth model construction. The ideas of using multi-modal data and using IFN with curvature encoding look reasonable and the experimental results show their effectiveness. However, there are certain reproducibility issues, and more detailed explanations would help readers.	application and innovation
120-Paper0539	DA-Net: Dual Branch Transformer and Adaptive Strip Upsampling for Retinal Vessels Segmentation	The authors of this paper propose a novel architecture for retinal vessel segmentation by combining image and patch level information in a joint encoder-decoder model. Their approach combines regional and global features from the encoder and introduce them to a transformer module. The transformer output is then decoded with an adaptive, line like, stip upsampling block. Compared to the existing work, they introduce the transformer module to fuse features from two different scales and they modify the standard decoding part of the existing architectures to take into account more context from the regions along the length of the vessels. The authors validate their approach on 2 widely used public databases. Compared to existing methods, the method outperforms existing work in the literature.	The paper proposes DA-Net for retinal vessel segmentation. The design of the network addresses limitations of image-wise and patch-wise approaches by combining them in a single network with a dual branch transformer. Also, the paper presents an adaptive strip upsampling method to better mimic tubularity of vessels. The proposed method is validated on two fundus images datasets; Drive and CASE-DB1 and shown to outperform previous methods	The paper proposes a patch and non-patch combination retinal vessel segmentation network, including a shared-weights U-net like encoder and decoder for patch and non-patch training and a transformer module for combining them in the latent space.	"1) Methodology: They propose a novel architecture that fuses regional and global information using transformers. To the best of my knowledge, it is the first application and intergration of a transformer module in a encoder-decoder like architecture. Related work that is inspired from U-net and Transformers is the PCAT-Unet model presented by Danny Chen et al. [Ref1] for the retinal vessel segmentation. For the decoding part, they replace the standard square kernel with oriented line detectors that take into account context along the length of the vessels.     2) Reproducibility: The authors provide the necessary tools to the MICCAI community to be able to reproduce the experiments and results. They provide the code implementation with the necessary dependencies, the evaluation of their method, the dataset that was used in the study. Also the previous are supplemented with the trained weights of the proposed model so the community can replicate exactly the algorithm. [Ref1]: Danny Chen et al. ""PCAT-Unet: UNet-like network fused convolution and transformer for retinal vessel segmentation"", PLOS one, 2022"	The paper provides a new way of how to combine patch-wise and image-wise segmentation approaches in a single network for vessel segmentation, with the use of dual transformer, which makes the paper interesting.  Also, adaptive strip upsampling block seems to suit well for the segmentation of vessels, which are elongated structures.	Overall, it is well written and well organized. The idea for tackling small vessels in a patch way is very straightforward, yet the mechanism to combine patch and non-patch is interesting. Additionally, this method seems to obtain decent experimental results. With their codes promised to be released. I have no doubt about the methodology part of the paper.	1) The use of the proposed upsampling block (ASUB) is not justified enough. It is not clear how well the stip upsampling block can capture the regional context in more complex vascular features, like bifurcations, where the vessel deviates from a line like shape. Another case where the use of the ASUB is not justified enough is in the smallest vessels case. The smallest vessels deviate significantly, in terms of shape, from a straight line like structures approximating curvilinear shapes. The pathology also can change their appearance making them more tortuous.  2) The difference in the segmentation performance between the proposed method and the existing work is relatively close (see CHASE-DB1 dabase, Table 1). The standard deviation is not provided. Also, the authors do not provide statistical significance analysis of their results.  3) The authors do not discuss the limitations of the methods on more complex vascular features, like in junctions, bifurcations, in cases of pathology, and they do not give examples of their segmentation of pathological images. For example, bifurcations consist of one parent and two daughter vessels and the strip like upsampling can not capture the organization of this structure. The future research steps in the discussion section are also missing.	No statistical analysis is given for ablation studies to show if the contributions of the proposed modules on method performance is significant, which seems very small. Although using a shared encoder for image and patch wise segmentation seems to helps network parameters to be kept small, I have some concerns about that it can reduce the information can be extracted from a fundus image (which is down-sized by a factor of N=4), which would loss small vessels due to downsampling, in contrast to previous work using full size of fundus images. So, this means the network does not use image-wise approach purely.  There is no ablation experiment provided to show if using dual branch improves performance instead of using only patch-level branch or image-level branch.	Though, the results seem to be excellent, the experment results are not sufficient. Only two datasets are included.	Is a complete submission with the training and evaluation code, the dataset, and the pre-trained weights.	The authors shared their code and weights of their network, which makes the paper reproducible.	providing code, dataset is open access	"1) How does the authors' method compare in terms of methodology and performance with the [Ref1]? Where [Ref1]: Danny Chen et al. ""PCAT-Unet: UNet-like network fused convolution and transformer for retinal vessel segmentation"", PLOS one, 2022 2) Please replace the ""Cytomegalovirus Retinitis [12]"" with a more relevant paper. The refereed work does not deal with this specific pathology. 3) The following sentence in section 1 is not very clear: ""In contrast, patches-level methods make the geometric features ... usually span multiple patches."" Please rephrase it. 4) In Fig. 4 (a) is the visualization of the strip convolutions correct? The authors propose a 4 pixel strip which seems to cover a significant area in the retina. Does each square of the strip cover that large area, or pixels, in the image? 5) In ASUB, the authors propose a straight line structural element to extract the context from the vessels. Could more complex vascular features, for example junctions, be approximated by more complex filters?  6) In Table 1 are the differences in the metrics statistically significant? Also in the results are the performance improvements originating from better small vessel segmentation, or large vessel boundary detection?"	There is no justification given why adaptive strip kernels are used in upsampling but not when feature extraction. They would also improves detection of vessels. An ablation experiment to show if using dual branch improves performance instead of using only patch-level branch or image-level branch would improve the quality of the paper. The authors stated that they resized all fundus images to 640x640 pixels. However, they did not provide any details if they crop images to be square before resizing; not doing it can change representation of vessels in images. Also, there is no information given if patches are cropped in overlapping fashion or not.	"However, I still have a question. Have you tried the STARE dataset [A]? It is also a widely used dataset to prove the effectiveness of the segmentation model. You should vaildate your method on it as well. [A] Adam Hoover, Valentina Kouznetsova, and Michael Goldbaum, ""Locating blood vessels in retinal images by piece-wise threshold probing of a matched filter re- sponse.,"" in Proceedings of the AMIA Symposium. 1998, p. 931, American Medical Informatics Association."	The authors propose a novel architecture with transformers. Their method takes into account global and local information by processing regions and the whole image. Also the approach is supplemented with an additional  methodological novelty that seems to be tailored to the vessel segmentation problem (ASUB block).	The paper presents a novel way of combining image and patch level segmentation, which is shown to outperform previous methods. Also the use of adaptive strip upsampling suits well for segmentation of elongated structures. However, there is not sufficient justification given to appreciate if dual branches outperform single branch.	The paper is well writen and the proposed method is novel. Furthermore, the code will be released.
121-Paper0233	D'ARTAGNAN: Counterfactual Video Generation	The paper introduced an approach to generate counterfactual images. The approach built upon Deep Twin networks and proposed a new architecture. The model has been assed on two datasets.	D'ARTAGNAN (Deep ARtificial Twin-Architecture GeNerAtive Networks) answers the question what would this echocardiogram look like if the patient had a different ejection fraction?	This paper focuses on computing counterfactual queries for echocardiograms. The authors proposed a method called D'ARTGNAN that combines deep neural networks, twin causal networks, and generative adversarial learning. The model is tested on a synthetic dataset and a real-world echocardiogram dataset.	well written interesting problem of generating counterfactual image generation evaluation on two datasets	Practical strategy for the absence of true labels missing for synthetic data. They defined  3 rules that will allow them to broadcast their true labels to the generated videos. Key point here is they want to make counterfactual videos that are visually indistinguishable. Two public datasets used MorphoMNIST and EchoNet databases. They are open to releasing their code. Proposed technique seems to be noval. Claiming that this is the first time this approach has been explored for medical image analysis.	The authors propose a novel model. The experimental results look promising.	no comparison with similar approach applied on general computer vision tasks, e.g. https://github.com/autonomousvision/counterfactual_generative_networks not sure if we need all the theorem in Section 2. The authors might have been able to explain in a simpler language.	Novel technique is mentioned with SSIM score but no comparative analysis with any baseline is presented. Reference to the EchoNet Github repository is missing.  Too many and unnecessary abbreviations such as Ultrasound to US. Related work either discuss simulators which are physical simulators and compute intensive or implementations of deep twin networks. No reference for cases where counterfactual queries are worked upon may be in domains other than medical imaging.  Too much mathematical detail, for Definitions which are difficult to comprehend without much context. Overall paper is difficult to understand. More explanation for the Abduction-Action-Prediction solution (discussed in Preliminaries) may be useful.  Variable names should be described in Fig: 1 for a quick overview. Fig. 1 can be drawn wrt. the application on hand instead of a generic one. Minimal or zero representation of models using figuratively, leads to a very lengthy and confusing description. Poor sectioning and sub sectioning with casual paper writing. Also English needs to be significantly improved.	The detailed descriptions of the model are not clear. The authors do not sufficiently describe how it is related to existing work, and no quantitative comparison is given.	They are going to release the code. So it should be reproducible. otherwise the paper does not contain all the data to reproduce the results.	With the details in the paper it is hard to reproduce but they are planning to release the code.	I am not sure whether the MorphoMNIST dataset and Echonet-Dynamic dataset are publicly available. The given details are insufficient to reconstruct the proposed model. However, the authors promise to provide the code.	please check section 5.	Novel technique is mentioned with SSIM score but no comparative analysis with any baseline is presented. Reference to the EchoNet Github repository is missing.  Too many and unnecessary abbreviations such as Ultrasound to US. Related work either discuss simulators which are physical simulators and compute intensive or implementations of deep twin networks. No reference for cases where counterfactual queries are worked upon may be in domains other than medical imaging.  Too much mathematical detail, for Definitions which are difficult to comprehend without much context. Overall paper is difficult to understand. More explanation for the Abduction-Action-Prediction solution (discussed in Preliminaries) may be useful.  Variable names should be described in Fig: 1 for a quick overview. Fig. 1 can be drawn wrt. the application on hand instead of a generic one. Minimal or zero representation of models using figuratively, leads to a very lengthy and confusing description. Poor sectioning and sub sectioning with casual paper writing. Also English needs to be significantly improved.	The authors propose a novel model for counterfactual queries. However, it is not clear how it is compared to existing models. Is there a video generating model available? What is missing in the existing models for a similar purpose? I have difficulty in understanding the details of the model, and a better clarification might be necessary. In Section 2, what are the relationships between $U$ and $X$, and how $E$ and $Y$ are related to $V$? In Section 3, how is a twin network used to generate the counterfactual samples? What is the objective function? What is the random variable $U_y$ in Fig. 1? How is the DAG utilized in the framework? How is the propensity score defined and used in the model? The experimental results look promising. However, the authors might need to compare the proposed model with a baseline model, such as an existing model or an ablative version of the proposed model. Without comparison, it is not clear whether the metrics shown in Table 1 are good or not.	I think the author could have compared with strong baseline by using methods proposed on general images and videos.	Work seems interesting with a nice application.	The proposed model looks novel, and the experimental results look promising. However, the description of the model is not clear, and the proposed model is not compared with other models in the experiments. The paper might need major revision before it is published.
122-Paper1816	Data-Driven Deep Supervision for Skin Lesion Classification	Authors present a Class Activation map based deep supervision method for training skin image classification model. They highlight that due to absence of object-level labels, the models are deemed to the restricted to image level information and feedback for most classification problems. They propose the use of effective receptive fields as a deep supervision mechanism to solve this problem to some extend and boost the classifier performance. The also present a layerwise effective receptive field determination strategy to make the mode in variant to the size of the object within the field of view.  The resulting  model is tested on several skin image datasets	The paper shows that adding deep supervision to the layer whose effective receptive field size approximately matches the average object size, improves the performance of a skin lesion classification model.	This paper proposed a supervision generation method for skin lesion classification, where the total framework composes a LERF module, morphological object size approximation, and deep supervision. The experimental results on several skin lesion datasets have shown the better performances.	Overall the paperis well writen and the presentation is satisfactory. The lack of object level feedback in classification problems is an important problem to tackle.	The idea is novel, particularly in the area of skin lesion classification. The study is performed on multiple datasets and performance gains are reported on all the datasets	The method can provide accurate lesion localizations through activation mapping.  The obtained quantitative results are competitive or better than baseline methods.	The paper reads like, authors utilized methods from the literature, on a novel application. Presentation lacks of clarity at a few places. The use of CAM from the last conv layer in order to determine the L_{target} in LERF may not be robust. The isotropic nature of the LERF is not well justified. Conclusions section is very short and lacks of discussions.	While the study shows improvements, it is not clear if the improvements are indeed significant (no confidence intervals or statistical tests for example). While the average performance seems to increase, it is not clear if it also introduces more variability in performance- for example, does the model perform significantly worse on cases where the object size is too small or too large (vs competing models)? There is no analysis presented which stratifies the performance by size of the object. Due to 2 above, it is not clear if the method is generalizable to different datasets, particularly if the average size of the object varies across different datasets. Additionally, because of the dependence of object size, it is not clear how the method will work with multiple classes with different sizes. Indeed, for ISIC 2018, the best result reported in the paper with deep supervision (0.701) falls far short of the best result (0.845).	There lacks the comparisons to other activation mapping method such as Grad-CAM.	It is not clear, what type of images do the inhouse dataset contains; dermoscopy, close up clinical, etc... An analysis of situations in which the method failed is not given. (authors said yes to this in the reproducibility statement)	Reproducible	The method can be reproduced.	"The methods are not novel. ERF and LERF methods are adapted from the literature. The paper reads like, authors utilized methods from the literature, on a novel application. Presentation lacks of clarity at a few places. The use of CAM from the last conv layer in order to determine the L_{target} in LERF may not be robust. CAMs are known to be problematic in finding the lesion area (or diagnostically critical area) in many skin lesion classification applications and therefore there is not guarantee that the LERF selection will also be from a diagnosticly critical area. The isotropic nature of the LERF is not well justified. For skin images this assumption may not hold.  Similarly, the unimodel assumption may not hold for skin images. as seen in Figure 3, first and third row, skin lesions may contain multifocal areas of high response (or high diagnostic importance) The inference procedure is not well explained. Do LERF have any significance during inference. Page7, 2nd sentence: "" To neutralize variations..."". This statement is not clear. Please further elaborate on the issue and the proposed solution. Overall, the table captions are too short and does not convey enough information in regards to the table. What does V and R stand for in column C of Table 2 The message that the authors would like to convey via Figure 4 is not clear. Are CAMs after deep supervision better, more precise or ? Conclusions section is very short and lacks of discussions."	I appreciate the fact that the method was tested on multiple different datasets and the method shows consistent improvement in each case. Adding confidence intervals to the presented results will add context to the presented metrics and give us a hint of the variability of the results. Since the key idea is based on object sizes, I suggest adding some analysis by stratifying the test group by sizes of the objects and analyzing the performance dependence on object size. It will be more convincing if instead of training and testing on the same dataset, you could train on one and test on another (for example on the Vitiligo private and public datasets) For the LERF computation, 20 iterations are used. Adding some information on the relative variability across the 20 iterations (maybe in supplementary) is useful as it will show the robustness of the LERF computation. Minor comment: In the introduction, the citation of ISIC 2018 has not been added.	It is better to provide localization maps of other method such Grad-CAM, Score-CAM.	The presented problem is important especially in explainable AI sense. Even if the authors approach has some points that requires further explanation and evaluation, the reviewer thinks that the contribution is valuable.	I think the method is interesting and it shows some improvement, but I am worried that the performance will depend heavily on the size of the objects (compared to standard methods) and the method will not generalize well to cases where the sizes of the objects are variable, or if there are multiple objects of different sizes, etc.	The whole framework is complete and the experiments are extensive. The accurately localized lesions are helpful for assisting clinical diagnosis.
123-Paper1296	Data-driven Multi-Modal Partial Medical Image Preregistration by Template Space Patch Mapping	Presents a method for partial image registration of 3D rotational angiography (3DA) and CT angiography of the head. Their model combines several blocks to predict the location of patches in the template space and a transformation. They demonstrate improvement over 4 baseline method in measuring distance from ground truth anchor points.	The authors propose a deep learning-based method to perform the initial rigid registration of two images (especially when one image has a much smaller field of view) in order to provide an accurate starting point (based on translation only) for a second registration refinement. This effectively gets the second refinement registration out of many local minima. The proposed method utilizes a common template space with multiple sub-patches from the moving image to infer the initial transformation and then utilizes a standard registration approach for refinement. Experiments rigidly registering multi-modal 3D rotational angiography and CT angiography demonstrate the approach.	The paper introduces a method that aims to improve the initialization for other local-search registration in rigid multi-modal partial image registration. Instead of doing a direct matching, the 2 images are matched to a common template space. The method works for images of diverse sizes, and for partial volumes	Strong experimental results Large dataset with gold standard labels Good comparison with baseline methods	* Comparison to both standard initialization techniques using traditional registration methods and to deep-learning methods is rigorous. * The proposed method outperforms the comparison method. * Evaluation on moving images with partial field of views (FOVs) stratified by FOV size is good (Fig. 4). * Cross-validation is rigorous for evaluation. * Ablation experiments demonstrate the importance of removing outliers (using RANSAC) in the inference results.	Alternative deep learning methods typically assume that data images the same whole region, that they are roughly aligned already or even that data has the same size or scale. This initialization step, however, performs well without requiring any of those. The proposed method represent a potentially very interesting addition as initialization for a variety of multimodal registration methods. The experiments section is quite good. It's very detailed, covers a good range of experiments and enough competing methods	Limited methodological novelty: combines several blocks from prior work Unclear training routine: is the entire dataset used in training? Time is not reported Requires labels	* There are some model training hyperparameters that could use additional evaluation and/or analysis, e.g. patch size.	It would be interesting to see also execution time comparisons for the diverse registration methods While the experiments section in general is quite good, there is not enough technical data about the dataset to recreate a similar one There is no time performance indication. The Conclusions section is too short and it doesn't really add much. It would be interesting to merge the Discussion subsection from the Experiments into the Conclusions section	"The paper is challenging to reproduce. Their algorithm uses several blocks from existing work, and the authors modified these blocks without detail. For example, when describing AirLab, the authors list that they ""modify the code as needed"" to process partial volumes. While details of this are not needed in the paper, it would be helpful to release code. Further, the training routine and data split is not well described."	The dataset and imaging information used in this study are well described. The authors describe their training procedure in sufficient detail. Source code will be made publicly available.	Method is not publicly available Dataset is not publicly available, and there is not enough technical data about the dataset to recreate a similar one Method could be implemented from the provided description	This paper presents a method that improves performance on the challenging problem of partial volume registration. However, my concern is with the limited novelty in this work, and incomplete experimental details. Specifically, can the authors: describe why the method is not sensitive to the choice of template shape? Describe the data split used in training. From my understanding, all images were used in training, in which case the network is overfit to the data, with supervised labels. This seems like an unfair comparison, and I don't believe this method would be feasible in practice. How sensitive is the method to the choice of number of image patches selected? Since the method is the combination of several existing works, it is hard to recommend acceptance without methodological novelty.	"Sec. 2.1: There are a few Template-Space Patch Mapping (TSPM) parameters that would be interesting to test. Most interesting would be the patch size. 16^3 is used as input, but larger patches might offer more context that would improve performance. Different patch sizes should be investigated, or at the least provide a discussion of this at the end of the paper. Sec. 3: Regarding your data splitting for the cross-fold validation experiments, please verify that your training/testing splits were stratified by subjects. You want to ensure that there was not data contamination between the two sets, e.g. the training and testing sets both had patches from the same subject. Sec. 3: One of the challenges in this approach is that the small FOV images may come from very different parts of the template volume. I am curious if your dataset had 3DRA images that were uniformly distributed throughout the template volume. Or, did one particular location of the anatomy have more 3DRA images? Some discussion about this would be interesting. This might also be interesting to see if your failure cases were examples that were from locations poorly represented in your training data. Grammatical/typographical: Sec. 2.1: ""The rest space"" -> ""The remaining space"""	In general terms, this is a solid paper. It introduces a novel method that is an interesting and robust alternative for being an initialization for other multimodal registration methods. The introduction and method sections are well detailed. The experiments section is quite good, providing extensive information about the experiments. However, it would be good to provide more technical data regarding the datasets that were utilized in the experiments. It would also be good to see time performance information The conclusions section is quite short, but I feel that most of its contents are actually located under the Discussion subsection from the experiments section. I'd aim to combine Discussion and Conclusions under the Conclusions section. The proposed future work, regarding both the performance improvements, as well as experimenting with deformable registration look like the adequate next steps for this work	The paper lacks methodological novelty. While the results improve on prior work, I am unsure about how the training was done, and seem convinced the model is overfit to the dataset. This would render the model unusable in practice.	This is a well written paper with good experimental setup and validation of a novel  registration initialization method based on deep learning. The authors provide a set of cross validation experiments to compare their proposed initialization strategy to conventional initializations and then demonstrate the effect these have on rigid registration in both standard (non-deep learning) and deep learning registration.	The paper provides a very interesting alternative initialization for multimodal registration methods, outperforming competing methods in a range of categories. I think it's a good contribution to the field. I can't find major faults on the method or the paper itself, which is quite solid
124-Paper2698	DDPNet: A novel dual-domain parallel network for low-dose CT reconstruction	The paper provides a novel dual-domain parallel network for low-dose CT reconstruction. The network is composed of a dual-domain parallel architecture, a unified fusion block and a pair of coupled patch-discriminators. The proposed method achieved good noise reduction and structure maintenance.	The authors proposed a Dual-domain parallel network (DDPNet) for low-dose CT (LDCT) reconstruction. A parallel structure is proposed to make parallel optimization between sinogram and image domains streams to reduce the cumulative error caused by the process of the sinogram domain denoising. The dual-domain information is used complementarily by Interactive Information Flow (IIF) mechanism. Then the authors design a triple-cross attention block to fuse features from two domains.	The authors propose a novel DDPNet for low-dose CT denoising. They design special modules for DDPNet, including dual-domain parallel architecture, a unified fusion block using multi-head attention and coupled patch-discriminators. Extensive experiments prove the effectiveness of each module and the best performance of DDPNet.	A novel dual-domain network, which is parallelly designed and not cascaded as the previous work.	The paper is written clearly, and the motivation is well justified. The proposed method utilizes both image and sinogram domain information through an IIF mechanism to reconstruct high-quality CT images from LDCT images. The cumulative error can be suppressed by parallel optimization between sinogram and image domains streams.	The authors propose a dual-domain parallel solution with interactive information flow to fuse the dual-domain features and eliminate the accumulation error. The unified fusion block use multi-head attention to better fuse dual-domain features. Quantitative results and visual comparisons show the superior performance of DDPNet.	"The logic and readability of the article are relatively poor. a) The introduction to loss functions is ambiguous. The respective effects of the L1, SSIM, and adversarial losses should be properly explained. b) On page 6, the sentence ""With the rich experience granted from the ..."" is lengthy and grammatically incorrect, making it difficult for the reader to understand. c) What does the ""coherent constraint"" refer to? How is it established in dual-domain cross-communication? In Fig. 2, the arrowhead should point at the Unified Fusion Block, but not the image ysin~img. There are many grammar and spelling mistakes. a) In the Abstract, ""The extensive experiment"" should be ""The extensive experiments"". In the same sentence, the period ""."" after ""13.54 HU"" should be dropped. b) In the Abstract, The sentence ""All of these findings reveal our method a great clinical potential in CT imaging"" can be replaced by ""All of these findings suggest that our method has great clinical potential in CT imaging"". c) The word ""readable"" is repeated in the sentence ""how to effectively make LDCT reconstruction in the more readable readable pattern..."" on page 2. d) ""to comprehensively integrates"" should be ""to comprehensively integrate"" on page 4. e) ""more realistic and details"" should be ""more realistic and detailed"" on page 4. f) ""Fig. 3: IIF connect"" should be ""Fig. 3: IIF connects"" on page 5. g) What does the sentence ""IIF extracts the intrinsic noise distribution, and guide it image domain ..."" mean on page 5? Does it mean ""IIF extracts the intrinsic noise distribution and introduces it into the image-domain stream ...""? h) ""the sinogram domain extracted information"" should be ""the extracted information from the sinogram domain"" on page 5. Lack of details, especially in section 3.1"	"This work is quite similar to the following work that explored the interactive dual-domain parallel network. Although slightly different tasks, the motivation is the same. Without citing this related work and discussing the differences, this paper seems to lack of novelty.  Wang, Tao, et al. ""IDOL-Net: An interactive dual-domain parallel network for CT metal artifact reduction."" arXiv preprint arXiv:2104.01405 (2021). Fig. 5 showed that the proposed method has HU shift compared to NDCT images; the obtained images are darker. Such HU shift certainly limit its potential in clinical practice. The authors use the downsampled image and sinogram in the dual-stream framework. But how to upsample the image is not mentioned in this article. The parameters of the FBP algorithm need to be scaled for the downsampled sinogram. If upsampling after reconstruction will bring errors. How can the authors eliminate this error? A triple-cross attention block is designed in this article to combine the features extracted by the two-stream structure and LDCT images. However, the advantages of the block are not clearly stated. If it is feasible to directly use a 1*1 convolution kernel or a fully connected layer for feature fusion? How much will the performance drop? In subsection 2.3, four operators are used to calculate the gradient image. Usually, only the first two operators are needed to calculate the gradient image. What are the functions of the last two operators, and how much does it improve the performance of the final model? How to set the weight parameter ""alpha"" in the loss function is not mentioned in this article. How to combine the adversarial loss of the gradient image and the original image is also not mentioned. The capitalization of symbols is not uniform. In ""Overall performance"" of subsection 3.2, ""db"" is lowercase. But in the following ""dB"" is capitalized."	As authors claim DDPNet has a great potential in CT image, they should try the method on real clinical data. The authors should perform ablation studies on different architectures of dual-domain frameworks to claim DDP can eliminate the accumulation error, like SD-ID, ID-SD.	Average reproducibility. The description of the loss function and the coupled patch-discriminators is rather vague. The paper provided the details of materials and configurations, but did not tell the parameter in the loss function. Therefore, the reproducibility is not satisfactory.	This paper did not provide the detailed network architecture and key parameters such as alpha in the loss function, which make the reproducibility questionable.	The authors use public datsets but do not share the codes.	Please provide more details in section 3.1, such as parameters setting in the network. In Fig.5, the improvement of visual quality of the DDPNet is not obvious. Although the tiny structure is easier to observe compared with other methods, its shape has possibly changed. Please give more explanations. In addition, as seen in the enlarged ROI of DDPNet, the entire ROI is shifted a little bit to the right, which indicates the inaccuracy of DDPNet. Please check if it is due to wrongly selected ROI or the drawback of DDPNet. There are many typos in the paper. For example, 'more readable readable pattern' in Page 2, 'is proposed to makes' and 'grade-based discriminator' in Page 3, arrow direction from the fusion block to y_sin~img in Fig.2, 'signogram-domain stream' in Page 4, and so on. Please read the paper carefully and revise them.	It would be better to clarify the difference between this work and most related works. It would be better to compare the most related works to show the effectiveness of the proposed method. It would be better to investigate the HU shift between the proposed one and the ground-truth as this limits the clinical value. Key details should be provided for reproducibility.	The arrowhead in Fig2 should be pointed from y_{sim~img} to Unified fusion block.	Good novelty, poor clarity, poor rigorousness.	The novelty is limited as the interactive dual domain has been investigated in literature; the authors did not cite and compare them. The detailed network architecture and hyperparameters were not provided. Most importantly, the results have HU shift compared to ground-truth one, which limits its clinical value.	The performance of DDPNet is impressive since it is hard to improve PSNR from 44.90 to 45.29.
125-Paper0688	Decoding Task Sub-type States with Group Deep Bidirectional Recurrent Neural Network	This paper uses a single classification model to simultaneously complete multiple brain function network decoding tasks under different external stimuli. The authors proposed a multi-scale random segment preprocessing and multi-task interaction layer to realize a single model's perception of varying brain functional states.	1.The authors proposed a group-wise brain function state classification network by capturing the temporal-spatial-wise context information.  2.The proposed multi-task interaction layer can help the classification network to learn the temporal-task-wise context information between multiple brain activation states.  3.The proposed method improves the performance of classifying the brain activation states of multiple subtasks.	This manuscript proposed a two layers-bidirectional GRU network to extract time-series features and the inter-class association features. The proposed method improves the classification performance of deep neural networks for functional brain activation states decoding.	The authors propose Group Deep Bidirectional Recurrent Neural Network that decoding the functional brain for group-wise tasks is novel. The proposed Multiple-scale Random Fragment Strategy and the multi-task interaction layer for decoding functional brain states are innovative.	1.The proposed bidirectional GRU-based group-wise brain functional state classification method is innovative. 2.The proposed multi-task interaction strategy for mining temporal-task-wise context information is novel. 3.The experimental results show that the proposed method has good classification performance.	In the manuscript, the authors proposed a novel group-wise Bidirectional Recurrent Neural Network for analysis of the brain function sub-type states. It is innovative that the authors implement the classification task of different subtypes of brain networks using a single model.	The description in the methods section lacks some necessary explanations, e.g. The motivation for data preprocessing is unclear. There is a lack of discussion on the importance of context information from both directions to decode brain network states.	1.The structure of the manuscript needs improvement. The methods section does not correspond closely to each module in Figure 2, e.g. lack of detailed description about MITL.  2.The article lacks a description of the motivation for the proposed method. The motivation of proposing random combinations of multi-sub-type tasking MRI sequences is unclear.	"The introduction of this manuscript is very difficult to follow and the motivation of the study was not well articulated. The article has some grammatical errors, such as: ""Group-wise brain sub-type states convenient our method..."""	The method is clear and reproducible.	The method proposed in this paper has been validated on the HCP dataset and is highly reproducible.	It is easy to follow	"The authors should add some explanations that describe the intrinsic relevance between the different external stimulus tasks and the properties of brain functional networks. Why did the authors use bidirectional GRUs instead of unidirectional ones? It seems to be to learn temporal context information from both directions. In the data preprocessing, the authors divided the fMRI into multiple time slices. The motivation for doing so is unclear, and the authors are advised to add corresponding explanations. The multi-task interaction layer proposed in this manuscript is not introduced in detail. I suggest the authors to add corresponding diagrams to explain the mechanism of the multi-task interaction layer in detail. The descriptions for data propressing are not clear. In the proposed data pre-pressing method, as shown in Fig2, the input T7-T1 refers to 7 different tasks? For the training phase, does the method need to choose 7 sub-tasks from different tasks? Or arbitrarily choose 7 subtasks? In addition, the article lacks a description of the testing process. Does the proposed method need to enter 7 different tasks at one time during the testing phase? The article also includes some errors such as ""Fig. 1. Confusion matrix for classification accuracy of 24 events"" should be fig 3. and ""Fig. 2. Classification comparison chart of 24 events"" should be Fig.4. It is recommended that the author make careful revisions."	From the manuscript, the advantage of the group-wise functional brain states decoding method compared with the traditional method seems to be the improvement of the classification accuracy. I think the group-wise analysis method seems able to discover the internal relations between the sub-type functional brain states caused by different stimulus tasks. It is recommended that the authors add relevant experiments to analyze the ability of a group-wise manner to discover the intrinsic associations of subtype brain states. The experiment results reported in table2 that some combinations of tasks can improve the accuracy. However, the possible reasons are unclear, and the author is advised to increase the discussion about the possible reasons for the improved classification performance caused by the group-wise manner. What is the method of DSRNN in Table3? There is no citation in the manuscript. I suggest the authors add the introduction for the relevant methods and mark the cited references. Also, the comparison methods in Experiment 3.3, such as SVM, MVPA, and SoftMAX, also lack reference citations. The resolution of the pictures in the article is very low, such as Fig2. The color labels of different methods in the figure are not clear. the authors should improve the quality of the figures in the manuscript. The experimental results in Table 3 require further analysis. The classification performance of the proposed method for class Emt and Lng is reduced. I suggest that the authors discuss the reasons for the decline in classification performance.	"Introduction needs to be carefully re-written; it is very difficult to follow and the motivation of the study is not well articulated. What are the intrinsic correlations between brain activity states under different external stimuli? Why is the group-wise brain network state decoding strategy helpful to explore this interconnectedness? Which steps in data pre-processing are referred to as ""Random combination"" and ""Signal extraction"" in Fig.2? Authors are advised to describe in detail in the manuscript. Meanwhile, the multi-task interaction layer mentioned in Figure 2 is not introduced in detail in the article. For the proposed method, The multi-task interaction is more important than GRU, and it is recommended that the author explains it in detail. The authors should check and revise the entire manuscript for grammatical errors and typos. How is the multiscale manifested in the Multi-scale Random Fragment Strategy (MRFS)? The work seems to just randomly splice different fMRI sequences and does not use multi-scale MRI sequences. In the experiment, the authors only reported the classification accuracy of brain network decoding, which was improved by the group-wise strategy. Whether the authors add some visual classification results in the revised manuscript? So that the readers can intuitively discover the interpretability of group-wise brain network decoding."	This manuscript proposed a novel method to identify brain state. It shows great improvement over current SOTA methods. However, it still need to be carefully revised to make the method to be more clear. Therefore, I think this manuscript should be weakly accepted.	This paper proposes a novel group-wise brain function state classification network. Experiments demonstrate the excellent performance over current methods. It is important to this field.	In the manuscript, the authors proposed a novel group-wise Bidirectional Recurrent Neural Network for analysis of the brain function sub-type states. It is important to this field.
126-Paper0406	Decoupling Predictions in Distributed Learning for Multi-Center Left Atrial MRI Segmentation	"In this paper, the authors present a technique for performing image segmentation with federated learning while combining the two diverging tasks of global and local optimisation. Their approach draws inspiration from the probabilistic U-net, and consists of a VAE architecture and a ""DA net"", which in combination allow to tune the network's prediction to the specific local distributions. The approach is evaluated for atrial image segmentation from MRI and seems to reliably outperform the implemented baselines."	"The manuscript proposes a method extended the ideas from [1] for distributed Learning for Multi-Center Left Atrial MRI segmentation.  The authors used the ""conditional VAE [18]"" to model the latent representation of the joint data distribution, and proposed a ""distribution adaptation network"" to generate an adaptation matrices conditioned on the joint data distribution. This is used to decouple the global and local predictions and adapts the prediction to be consistent with local distribution during testing.  The proposed method evaluated on the constructed dataset, collected from three centers, and the results shows substantial performance improvement for global and local tasks over its competitors."		The paper addresses a very relevant and difficult problem, namely combining global and local optima in a federated learning setting. The methodology is novel, sound and interesting. The experimental set-up is (mostly) well-devised and the results are convincing. The paper is generally well-written and easy to follow.	Good clinical feasibility for distributed learning to handle the learning-based analysis method with privacy-sensitive data. Good results (outperform competing methods) in DICE score metrics across global and local tasks.		"Partially problematic experimental procedure: the authors ""generated a test set of generic data, using 30 cases from Utah with no modification to the gold standard labels"". Since these images belong to the same distribution of those used for training one of the nodes, I don't think they constitute a good proxy for generic data. This aspect should be at least discussed in the Discussion section. Lack of details about the network architectures: for instance, the authors state that ""The personalized module was built with five convolution layers and SoftPlus activation"", but no other information is available. This must be fixed by presenting the full architectural details at least in the supplementary material."	Difference from previous work are not clearly described. Result reliability may be affected by the dataset settings during experiments. See detailed comments.		I find the authors' answers do not represent the reality of their submission in several aspects. Specifically, the following are missing in the paper: Description of the study cohort. Information on sensitivity regarding parameter changes. The exact number of training and evaluation runs. Details on how baseline methods were implemented and tuned. An analysis of statistical significance of reported differences in performance between methods. The average runtime for each result, or estimated energy cost. A description of the memory footprint.	some details are not clearly described to reproduce the work.		"Some grammatical errors and unclear sentences: Page 1: ""has shown great potential"", ""the former"", ""the latter"" (no plural), ""the latters target multiple models, of which each for one center (denoted as local data)"" (unclear sentence), ""been proposed to for"". Page 4: ""the segmentation risk"". In page 2, the authors state that ""Although the above methods have solved the problem of privacy and fairness"". I think that neither of these problems have been solved: issues about privacy can still arise given that it has been shown that images from the training dataset can still be allucinated from a trained model. As for fairness, the concept is very broad and should be better defined in this paper to better understand what the authors mean. Please edit accordingly. Page 5: ""As q - 0, it is equivalent to CE loss which emphasizes more on uncertain predictions, and it degrades to MAE loss [3], which equally penalizes on each pixel, as q approaches 0."" This sentence is unclear. Please revise."	"The connection to [1] should be emphasized. Currently, [1] is only simply touched on. Please explain the main ideas of [1] and how they approach the two seemingly contradictory objectives simultaneously. Following this, please clearly explain how the proposed method follows the approaches of [1] and clearly list the proposed extensions / differences with respect to [1]. The method proposed a distribution adaptation network to used learned latent representation to generate an adaptation matrices. But it is unclear how the DA net yield such a high dimensional matrices? How about the computation and memory? It would be better to describe clearly or provide some visualization. The method introduced a regularization term L_TR to minimize the diagonal elements of W_k^I for each pixel. The authors claimed that it is used to forces the matrices to modify the prediction as much as possible. However, the underlying mechanism is not clear. There are some inconsistent descriptions in the paper: Introduction Para.5 ""our method decouples the predictions and labels"" and Sec.2.2 Title ""Decoupling Global and Local Predictions"". Please clarify it. The method is evaluated on datasets constructed from three centers. Morphological operations is used to simulate the settings of Center C/D/generic data (unseen center) for Utah dataset. The method demonstrated significant superiority in Center C and D. But it seems that most of the training data come from the Utah dataset (35 for C,D and 15 for A,B. So this may be the main factor of the obvious improvement of the proposed method for center C and D, but not for Center A and B, as shown in Table 1. This make the experiment results less convincing. Please clarify. Please also describe clarify the different of the propose method and the competitors in the introduction."		The paper is highly interesting. It presents a novel solution to a very relevant problem in federated learning, which could become the state of the art for this task.	The work is interesting and results are good. The novelty of the method and the reliability of the results need to be further clarified.	
127-Paper1750	Deep filter bank regression for super-resolution of anisotropic MR brain images	This paper perfect reconstruction filter banks to perform MR super resolution in the out-of-plane axis. The pipeline is split into 2 stages; stage1 randomly samples 1D row/columns from in-plane slices to train analysis and synthesis filter banks (apart from H_0 that's fixed to the PSF), and stage2 where 2D training pairs of y (output of H_0) and { d_1 to d_(M-1) } are trained with pix2pix.	"The paper does super-resolution (SR) of thick-sliced MR single-modality images by a training-free method (""self-super-resolution""), which casts recovering the unknown high-resolution (HR) image in the context of perfect reconstruction filter banks. They fit the parameters of the filter bank from 1D rows and columns from in-plane slices using gradient descent, and the detail coefficients using a regression CNN on 2D patches extracted from in-plane slices. Finally, the HR image is recovered by encoding the input image with the first set of filters, regressing the detail coefficients from the encoded image, and then decoding the coefficients with the last set of filters. The method is compared to simple resampling, which it outperforms, as well as another self-super-resolution technique where it performs favorably for larger slice thicknesses."	(1) This paper proposed a novel filter bank formwork for super-resolution of anisotropic MR brain images based on filter bank theory.  (2) The structure of the network is interpretable. (3) Network training does not require extra training data.	This paper is novel in the way that the problem is modelled from a Digital Signal Processing (DSP) perspective instead of traditional Deep Learning model design. Authors have compared their method to traditional upsampling methods (i.e. b-spline) as well as a SOTA deep learning method (SMORE) and have shown an improvement in performance.	This is a good paper, both well written and well structured, with a novel methodology and solid validation. I am not very familiar with perfect reconstruction filter banks, but I get the idea of the paper, and the results are promising and convincing. A method that can super-resolve high-quality SR of thick-sliced hospital MRIs - without requiring training data - is valuable, and is what is presented in this work.	(1) This paper proposed a novel formwork which is supported by theory.  (2) Experiments verify the effectiveness of the proposed method.	There aren't any weaknesses from my mindset	My main concern with this work is its applicability in a more realistic scenario - when slice-thickness is not an integer value. This is something that is not covered in the discussion and conclusions section. As this is a very common scenario in routine clinical data I would like an elaboration (or discussion) on this extension of the work in the paper? From looking at the model, my feeling is that this could be a non-trivial task? Furthermore, as the model is fitted on an image-to-image basis, using two first-order optimization steps, I would like details on runtime. This is particularly important if large populations of retrospective hospital MR images are to be processed, in a feasible time. Finally, it is a bit concerning that the results section shows a drop in reconstruction performance in the thick-slice direction, seeing this is the direction of interest. It may very well be that this is because data of this type is missing during training; however, the nature of the problem this paper is tackling is such that this type of data will not be available. Or if it was, could it be included in your model somehow? Please elaborate.	(1) The method is not adequately described, for example, how does the data flow in the model? How does the data flow in the model? How do we manipulate the filter? How do we train generator? (2) Experimental validation cannot support the benefits of the framework described in the Page2-3.  How does the method explicitly synthesize the missing high frequencies? The dynamic capacity for lower-resolution images? The robustness of the method?	The method should be reproducible from the information provided in the paper.	Good. The paper presents their results with statistical significance, which I appreciate. The authors state that the code will not be made publicly available.	This paper has the reproducibility.	The paper is overall well written, and makes mathematical sense. I only have a few questions hoping the authors can clarify. Is there a reason why H1 - H_(M-1) and F_0 - F_(M-1) are modelled as 1D filters? Can they be 2D since, in stage2, y and d training pairs are 2D? NIT: Can the authors clarify in text what is the exact value of M used in experiments? From Fig3, I assume M=5. Similarly with the order of the filters (k, I assume M-1?). How does changing the value of M and k affect performance of the model? NIT: What is the runtime of the model to create a HR image? and how does this compare to B-Spline or SMOTE?	"Fig 1: It would be nice to add the analysis/synthesis terms to this figure, and also the encoded/decoder analogy (as this is something that DL readers will appreciate). Maybe as boxes around the relevant parts? ""neural SR methods"", I am not sure I understand what this means, please explain. Could h_0 be learned from the data, without using SMORE as a pre-processing step, or would this break the theory? Why is a Gaussian used as part of the low-resolution generating process, and not the estimated slice profile? Would not the latter stay truer to the forward generating process? As the method shares similarities with earlier work in SR - which casts the recovery of the HR image as an inverse problem - some of this work could be referenced in the literature review. Fig 4: The Low Resolution scans would be better visualized using nearest neighbor interpolation. Routine clinical scans are often acquired with a number of MR contrasts. It would be nice with a comment in the Discussion section if the model could be extended to this 'multi-channel' scenario."	(1) Please introduction the method in more detail. (2) Please conduct more detailed experiments to verify the advantages of the proposed framework with theoretical guarantee.	"The paper is overall well written, an interesting take of SR using DSP methods (albeit not pure DSP since detail coefficients ""d"" are predicted using pix2pix). Authors have shown the method is comparable with SMORE, a SOTA method."	Methodological novelty with a solid validation, and a well written paper to boot.	The writing is not clear, and the experiment is inadequate, so I do not recommend acceptance.
128-Paper2496	Deep Geometric Supervision Improves Spatial Generalization in Orthopedic Surgery Planning	"The authurs proposed an automatic landmark detection framework to identify an anatomic landmark point inside the distal femoral head called ""the Schoettle Point"". The accurate calculation of this point is of great importance for patellofemoral ligament (MPFL) reconstruction surgery. In the common practice, first relevant anatomical structures, for example, the tangent line to the posterior femur shaft cortex, are detected by optimimising a proxy objective function. Next, the Schoettle Point is calculated based on geometrical properties of the detected structures. This paper proposed a framework called 'Deep Geometric Supervision' to merge these two separate steps into one."	The authors develop a novel method to accurately identify the graft fixation site for MPFL surgery, from both diagnostic and intraop X-rays. Their method combines anatomical landmark extraction together with a model that learns the geometric relationship between landmarks to infer the fixation site.	This study jointly optimized anatomical feature extractor and the planning target in an effort to automate the surgical planning for reconstructive orthopedics.	The proposed formulation is novel and improved the point detection accuracy measured as median error from 2.29 mm [95% CI=1.84 to 2.82] to 1.58 mm [95% CI =1.15 to 2.09].	Appropriate statistical analysis Novel method seems to improve results significantly Validated on two different datasets	Novel method: This is an extension from presumably the authors' previous work where the planning target was calculated from the optimization of so-called proxy objective without optimizing planning target itself. In the current study, the optimization for planning target and proxy objective are jointed performed, achieving significantly improved planning target accuracy. The proxy object was also improved to accommodate joint optimization.	The manuscript is a bit difficult to read for me. I appreciate the effort by authors to explain the framework as generic as possible but this generalisation make the paper difficult to read. Please identify the cost functions in Eq. 1 as it is based T=3. What are MSE and BCE on page 4? what is spatial-to-numerical transform (DSNT) on page 5? Please define them mathematically in separate equations. Figure 1. Please provide explanation in the caption explaining notations. Figures should be understandable on their own without searching in the text. Figure 2, why the frequency of p>=2.5 and p<2.5 does not sum to 100%? Please also explain the vase shape in panel A.	Lack of appropriate discussion of the results and experiment choices Excessive use of math notation makes the paper convoluted and difficult to read in my opinion. Some of the methodology could be better explained	I fully understand it is not easy to put everything into this paper due to the page limit, but the paper suffers from a lack of explanation. The organization could be improved to help readers better understand the problem to solve and developed methodology.  All the papers cited to explain the limitation of the existing methods are from the author's group. Citing other studies would help better assess the outcome this study achieved.	Some mathematical details on objective loss functions are missing. Otherwise, it should be reproducible.	Good details on experiment parameters, but would be better if code was made available to examine.	The reproducibility can be rated as sufficient	"authours may wish to consider automatic landamark detection using deep convolutional neural networks for this task. Liu, Wei, et al. ""Landmarks detection with anatomical constraints for total hip arthroplasty preoperative measurements."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2020."	"Section 1 - RQ1, why the choice to examine activation maps as a research question? This seems unrelated to the planning or surgical goals and does not seem relevant. Seems like this is for supplementary material instead a core research question General - What is the accuracy of directly segmenting the graft fixation site using a FCN like U-net compared to using the proposed method which uses multiple learning models (Proxy + DGS). If this is already published, cite in discussion section for comparison Section 2 - What value does task 3 (semantic segmentation of femur) add to the overall accuracy? Is segmenting the femur necessary for reasonable planning accuracy? Section 2.3 - What is meant by the ""original segmentation task""? Is this directly segmenting the graft fixation point or is it referring to segmenting the femur as in task 3 of the meta learning? Section 2.4 - What exactly did the medical engineer label in the X-rays? The surgeon marked the SP point, did the engineer mark the Blumensaat point and turning points or was this automatically calculated from the femur polygon? Section 3 Fig 3 - Why is there a small increase in error for Shaft/Head ratios greater than 2.4? Figure 4 - The legend on figure 4b seems to be wrong as from this plot it seems configuration A is the best. Section 4 - What is the accuracy when only DGS is minimized? While this might be less interpretable, I would be interested in the accuracy difference as well."	"Among three model variants, the 'proxy' model is an improvement of (or equivalent to) authors' previous work [13]. I'm wondering how much performance improvement was achieved by hard parameter-sharing proposed in this study. It would be great if the authors could more kindly explain the rationale for model variants 'B' Proxy + DGS w/o segmentation. Why did the authors' expect segmentation task may have chance to negatively impact the overall performance? Also, the current way of writing and naming is confusing because model variants 'A' Proxy model, inherently include 'segmentation' as one of three tasks. In my opinion, ""B) Proxy w/o segmentation + DGS and c) Proxy + DGS makes more sense.  Is the multiplicative weighting term (lambda) in 2.2 the same as the risk-weighting (lambda) in 2.4? Keeping consistency in terminology would avoid any unnecessary misunderstanding.  How was the initial (0.99) and decrease internal (0.01) of the risk-weighting determined? Have the authors performed any sensitivity test for it? The results are impressive. What would be the next step to achieve full automation? How far the authors think is left to adopt the developed tool in real clinical setting?"	This is an intersting paper with novel formulation and satisfactory results. However, the method description should be explained better and so I rated the paper as 'accept'.	Well written (although sometimes a little convoluted). Novel methodology with strong evidence of improving planning accuracy.	This study is an important step towards the automized planning, which is robust to various clinical environments. The writing could be improved for better context and clarity.
129-Paper0636	Deep is a Luxury We Don't Have	This paper proposes a high resolution transformer based model HCT, in particular the attention convolution (AC) block.	Transformer has limited application in analyzing large-size medical images due to its huge computation requirements of the self-attention block. This work adopts the recent proposed Performer with reduced computing consumption to power the shallow layer of the neural network, providing a possible way to explore efficient learning for other research.	In this paper, the authors address this complexity of high-pixel medical image recognition by exploiting a linear self-attention approximation. With this approximation, the authors propose an efficient vision model called HCT, which stands for High Resolution Convolutional Transformer. Extensive case studies validate the effectiveness of the proposed HCT.	The proposed HCT is a convolutional transformer for high resolution inputs. With linear attention approximation, HCT seems to improve over GMIC, a benchmark for high resolution mammography.	this work adopts the latest transformer technique Performer (Transformer architectures which can estimate regular full-rank-attention Transformers with provable accuracy, but using only linear space and time complexity, without relying on any priors such as sparsity or low-rankness), and demonstrates the feasibility of applying the Performer on the large-scale medical input.	The problem studied in this paper is important and needs to be solved in Medical Imaging Extensive case studies	This paper only compares with one existing approach, namely GMIC. Also, the proposed method can be seen as a modification to GMIC.	this work fails to clearly explain why the effective attention is suitable the high-resolution input. Meanwhile, the title fails to reflect the entire submission and misleads readers in the architecture design.	Weaknesses: Need more justifications about the novelty claims Need to include more related work that are highly important Need to check for grammatical errors and typos. The evaluation needs to be enhanced in terms of baselines, datasets, settings, etc.	No code is given; the description seems to be clear enough to reproduce the work.	The reproducibility of this paper is good as the author used a public implementation and evaluated their method on the public dataset.	The dataset and code are not disclosed in this paper, so the reproducibility of this paper needs to be further improved.	The proposed method seems to be highly related to GMIC. Adding more comparison with other methods can strengthen the results.	"This work adopts Performer equipped with different settings to detect the tumor region from the high resolution mammograph. However, many basic concepts are wrongly addressed, making the submission inconvincible. The 'High resolution' concept has been disclaimed overall the submission. High resolution refers to the fine unit spatial distance instead of a large number of pixels. The attention mechanism aims at dynamically searching out similar regions from the images, which is invariant for its variants. Then, the concept ""HCT focuses dynamically on regions based on their contents and not their spatial positions"" misleads the readers and should be corrected. Since the authors argued that the AC is the contribution, please clarity the difference between the AC block and attention block in Performer [1]. [1] Choromanski, Krzysztof, et al. ""Rethinking attention with performers."" arXiv preprint arXiv:2009.14794 (2020)."	"Comments: Need to include more related work that are highly important [1] Woo S, Park J, Lee J Y, et al. Cbam: Convolutional block attention module[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 3-19. [2] Kitaev N, Kaiser L, Levskaya A. Reformer: The Efficient Transformer[C]//International Conference on Learning Representations. 2019. The authors need to introduce some related work on the efficient attention mechanism design. Need more justifications about the novelty claims The proposed Attention-Convolutional (AC) block of this paper is too close to the Convolutional Block Attention Module in Ref [2]. Therefore, the authors need to provide more justifications about the novelty claims. This paper lacks a theoretical analysis and a complexity analysis of the proposed efficient attention mechanism. The following references are hoped to be helpful for the authors to further improve the quality of this paper. Although Ref. [3] is only recently published, it is still easy for authors to refer to and follow up. [3] Zheng, Lin, Chong Wang, and Lingpeng Kong. ""Linear Complexity Randomized Self-attention Mechanism."" arXiv preprint arXiv:2204.04667 (2022). The evaluation needs to be enhanced in terms of baselines, datasets, settings, etc. It would be better if the authors could add more benchmark datasets to verify the effectiveness of the proposed model. The authors need to add more advanced baselines such as work based on an efficient attention mechanism (e.g., ref [2]) to further validate the effectiveness of the proposed model. Need to check for grammatical errors and typos The picture in table 1 is blurry, please provide a clearer version. The editorial quality of this paper is largely unsatisfactory. It contains quite a lot of inconsistent/non-precise description, as also reflected in the above comments."	The paper proposes an efficient transformer model to process the high resolution mammograms. The results suggest improvements over previous approaches.	Unlike the transformer limited by the GPU memory, the Performer can be deployed in the shadow layer due to the full-rank computing strategy. Taking advantage of Performer, this work validates the feasibility of Performer in analyzing high-resolution mammography. Despite of the improved performance, this work lacks the dedicated comparison between the deep and shallow network, which is emphasized on the title.	The experimental part of this paper is insufficient to illustrate the superiority of the proposed method in terms of experimental setup, baselines, and datasets. In addition, the novelty of this paper still needs to be improved.
130-Paper1440	Deep Laparoscopic Stereo Matching with Transformers	The paper presents a deep-learning-based method to solve the stereo matching problem involving laparoscopic images. An existing framework (reference [4]) is modified to achieve superior performance for laparoscopic applications. The authors main contributions are the following: Use of Transformer networks in place of Convolutional NN in cost-aggregation. The authors justify this architectural change in terms of loss landscape, learning trajectory and accuracy The proposed architecture is compared to the state-of-the-art using publicly available datasets	This paper targets the task of stereo matching between two rectified images. Authors try to replace CNN with Transformer for learnable components of a cost-volume-based network architecture from the prior work named LEAStereo. Various visualizations have been done for loss landscape upon convergence. Experiments on SceneFlow, SCARED2019, and dVPN datasets have been conducted and the authors claim that the proposed model performs better than some of the prior works. They claim that by replacing the CNN with Transformer for feature extraction, the model results in faster convergence, higher accuracy, and better generalization.	This paper introduces a stereo matching architecture, consisting of a feature extraction network using a transformer and a matching network using a CNN. It performed extensive experiments on ablation, learning behaviours and comparisons to other methods.	The paper presents a deep-learning-based method to solve the stereo matching problem involving laparoscopic images. An existing framework (reference [4]) is modified to achieve superior performance for laparoscopic applications. The authors main contributions are the following: Use of Transformer networks in place of Convolutional NN in cost-aggregation. The authors justify this architectural change in terms of loss landscape, learning trajectory and accuracy The proposed architecture is compared to the state-of-the-art using publicly available datasets	The idea of visualizing the loss landscape to demonstrate the differences among various architectures is interesting, though the way to represent the results could be further improved.	Extensive experiments; Combining feature extraction using a transformer and matching using a CNN.	Nothing as far as MICCAI conference is concerned. For minor suggestions, see detailed comments.	"Limited novelty in terms of method design. The authors simply try replacing the 2D and 3D CNN with the equivalent Transformer architecture that comes from prior works. No special adjustment has been attempted to better exploit the attention mechanism for the task of stereo matching. Loss landscape visualization is not convincing. The authors try to use Figures 4 and 5 to show that the proposed model can achieve a lower local minimum with a flatter region around the optimal point. However, it is hard to appreciate the claim based on these figures. The scales of the figures are not controlled, and it is also difficult to see the value of the optimal point. Additionally, a lot of other factors besides the network architecture can affect the loss landscape upon the convergence. For example, the choice of the optimizer. There are works showing that Transformer can form a steeper loss landscape around the local minimum but with a better optimizer, it can achieve a flatter neighboring region [1]. The experiments are not thorough enough to validate the performance of the proposed method. The authors only use 10 frames in SCARED2019 to evaluate the performance. With such a small sample set, it is difficult to say if there is any statistical significance in the performance of various methods. The methods named ""Supervised"" are also not clearly cited, thus it is difficult to appreciate what prior works the authors have compared to. LEAStereo is also not included in Table 3. Authors claim that the proposed method works better than LEAStereo in the texture-less region. In Fig. 1 of supplementary material, I don't see a significant difference between the methods. More learning-based stereo matching methods need to be evaluated on a larger dataset to validate the claims made in the manuscript. Chen, Xiangning, Cho-Jui Hsieh, and Boqing Gong. 2021. ""When Vision Transformers Outperform ResNets without Pre-Training or Strong Data Augmentations."" arXiv [cs.CV]. arXiv. http://arxiv.org/abs/2106.01548."	Maybe I missed it somewhere but I was wondering how fast the inference is, as it will be quite important for use in practice.	The architecture is clearly described, and references are given where further details can be found. The framework used for implementation is given. The metrics used to compare different methods are adequately described. However, the paper does not contain details about the running time, memory footprint and the computational platform in which the implementation was tested. Moreover, no failure cases have been included and discussed.	Okay reproducibility because datasets are public and they will open-source the code.	it looks reproducible.	The paper is well-written: adequate background to the problem with references to state-of-the-art methods in learning-based stereo matching is given. The rationale behind the use of transformer networks is given and the proposed architecture is studied in terms of loss landscape, learning trajectory and accuracy. Finally, the experiments are adequately described while the results are clearly presented. The proposed method is compared to the state-of-the-art using publicly available datasets. Mean absolute error has been used to compare the performance. I would suggest reporting either the variance together with the mean or report the RMS-error. Other descriptive statistics such as the 95th percentile will be useful to the serious reader. Significant value can be added to the paper by including details on the running time and memory requirements of proposed architecture.	"In Fig. 1, a wrong disparity map is used. Replace it with the one associated with the stereo input. ""Transformer"" instead of ""Tranformer"" in Fig. 1. 4th line in Sec. 2, ""natural the stereo matching task"". In Sec. 2.1, besides the number of layers, you will also need to maintain a relatively similar number of learnable parameters to make a fair comparison. 3rd line in page 4, ""we"". In Fig. 4, it is difficult to see the value of the lowest point. Also, there need to be some quantitative measurements to justify your claim about the shape here. A larger set of hyperparameters should also be tried in order to remove the randomness in the loss landscape caused by a single set of settings. What do the axes stand for? In Fig. 5, the numbers are too small to see. Also, the scales of the axes are different across different methods. It is very difficult to obtain any useful information from these plots. What do the axes stand for? The DPI of Fig. 6 is too low. Also, this figure is pointless if you want to show the accuracy comparison because the models have not converged yet. In Table 1, why not fine-tune the learning-based methods on this dataset to show those results also? The references to these comparison methods need to be made in Table 1. The comparison with STTR is not fair because this method tries to predict an occlusion mask and does not produce valid disparities values in those occluded regions, as can be seen in Fig. 7. You should also try to compare the performance of the non-occluded regions. In Table 3, the direction of the arrow for SSIM seems to be wrong and you did not explain what these arrows stand for. DSSR uses a similar architecture as STTR, and you should explain this in the paper."	I just have a question other than the one above. In Fig 6, why are the errors lower on SCARED2019 than on the in-domain data?	The paper is very well written. It has some novelty in the methods, and the authors provide adequate justification for the proposed architectural changes. The superiority of the method is shown both quantitatively and qualitatively using publicly available datasets. Moreover, the performance of the method is compared to the state-of-the-art demonstrating its competitiveness. The application novelty combined with thorough validation makes this paper acceptable for presentation in MICCAI 2022.	Very limited novelty, not convicing claims, and not thorough and validate experiment results.	As aforementioned, extensive experiments on various aspects were performed, which is very helpful for the readers. Also, the results seem promising.
131-Paper2581	Deep Learning based Modality-Independent Intracranial Aneurysm Detection	The authors present a novel modality agnostic aneurysm detection method	The manuscript proposes a deep learning-based, modality-independent method for intracranial aneurysm detection based on a previous extraction of the vascular surface mesh. The method was demonstrated in cross-modality and mixed-modality experiments based on 500 datasets and achieved 96% sensitivity and 0.81 false positive detections per image.	The primary contribution of this manuscript is a deep learning system to detect intracranial aneurysms, tested in both MRA and CTCA imaging. This approach shows good results compared to the state of the art and does seems robust to changes in training data, at least between MRA and CTA.	well-written paper interesting approach to use surface models and pointNet good evaluation	The main strengths of the manuscript are: modality-independent method for aneurysm detection, which achieves very good results on both, CTA and MRA datasets in a study with 500 datasets, and outperforms the state-of-the-art well-written, concise manuscript which is easy to read	Generalization of machine learning models can be difficult, and moderns trained on a specific dataset may not actually work well in practice. This work has addressed these issues atleast in part by showing that their deep learning method for detecting intracranial aneurysms performs on part or better than manual segmentation, and changes in imaging modality do not particularly affect the performance of their method.	no true baseline method, comparison to other methods is based on literature values that were achieved using different datasets. Dataset is biased towards larger aneurysms, which are easier to detect.	The main weaknesses of the manuscript are: missing related work on automatic intracranial aneurysm detection in both modalities, CTA and MRA by Hentschke et al.: https://ieeexplore.ieee.org/document/6235669 It remains unclear from the data description how well the used data sample reflects data from clinical routine. Has the data been acquired using the same MR and CT scanners by the same trained personell? Were there differences in the MR scanning protocol and sequence parameters? Additional information is necessary to assess whether the approach can be transferred to data from clinical routine. I also miss a brief discussion of data quality/imaging artifacts and their potential impact. The key part making the method modality-agnostic is the vessel extraction. As such, it must be described in more detail. It remains unclear how good the segmentation is, whether it fails in which cases and if manual refinement is necessary. The post-processing of the initial surface, e.g., smoothing, usually requires the setting of parameters. Which parameter values were chosen? In summary, it remains unclear how good the vessel extraction performs and how much the aneurysm detection depends on the quality and the parameter adjustment. Manual aneurysm identification was conducted by one rater. Since this is a difficult task and small aneurysms may remain unnoticed, I suggest for future work to include a second rater and also, determine inter-rater variability.	The PointNet architecture used is somewhat outdated, and there have been significant advances for these types of architectures. Might be worth mentioning this in the discussion. I also wonder if testing on CTA data that has at least 1 aneurysm per patient potentially underestimates the false positive rate, although the sampling performed somewhat addresses this concern.	Challenging to reproduce without access to the data	The reproducability of the vessel extraction part is rather poor.	The methodology used is reasonably clear, primarily relying on two previously published architectures (nnU-Net and PointNet). Preprocessing and parameters required are mentioned in the manuscript.	This is a really well-written and interesting paper. The data should be described in more detail, especially the spatial resolution is needed. It may be the case that the method just works well because the resolution off the CTA and MRA is similar. The CTA in Fig 2 looks odd. The ground truth used to train the vessel segmentation needs to be explained in more detail. Another alternative for a modality agnostic detection method would be to use image-to-image translation techniques. No true baseline comparison	"Background: ""...methods are summarized in Table 1."" -> Table 2 Fig 2. could be enlarged to improve readability, in particular of the heatmap. More details on the imaging parameters must be provided. More details on the vessel extraction and its accuracy must be provided. Impact of the vessel extraction on the detection performance must be investigated."	I would suggest including the processing time for each case, as it would make evaluation of the time savings involved clearer.	The missing comparison to other methods is the major weakness.	The paper presents an interesting approach and achieves very good results but the impact of the vessel extraction on the detection accuracy and the generalizability of the approach with respect to imaging parameters and therefore, clinical routine remain unclear.	The method presented shows good performance, along with robust validation strategy that shows the model can generalize to some degree to different imaging modalities. The problem of intracranial aneurysm detection is fairly important, and it is great that the authors are aware of the common pitfalls regarding generalizability of the machine learning models and attempting to rectify this.
132-Paper0144	Deep Learning-based Facial Appearance Simulation Driven by Surgically Planned Craniomaxillofacial Bony Movement	The paper describes a method for predicting facial appearance after osteotomy.  The method uses the pre-operative bone and soft tissue surfaces of the skull and face, as well as the planned revised bony surfaces as input to a pointnet based network.	A deep Learning-based method was presented to predict the facial appearance following bony movement in this paper. The PointNet++ network was adopted to extract the point-wise features of facial and bony model. And, a novel cross point-set attention module was proposed to explicitly calculate the correspondence matrix between each bony-facial point pair. The calculated matrix was then used to predict the postoperative facial change. The proposed method was evaluated on 40 sets of CT data.	This paper describes Attentive Correspondence assisted Movement Transformation network (ACMT-Net), a method to predict post-operative facial appearance after orthognathic surgery leveraging on movements of corresponding bony segments. This is achieved by exploiting point-to-point correspondences between facial and bony points, which are estimated thanks to a novel module called Cross Point set Attention (CPSA). The proposed method is evaluated on models extracted from real CTs and it achieves comparable performance to related works in terms of accuracy, while outperforming their computational performance.	The paper solves a new clinical problem using a pointnet,  Allowing for quicker and easier computation than labour intensive FEA and without relying on surgical expertise. The authors compile a dataset of pre and post op imaging of facial reconstruction patients that is used to train and evaluate the model The authors present a novel architecture that uses PointNet derviced features, point to point correspondence with convolutional layers to learn displacements to apply to pre-operative geometries Limited data, but still good performance The article is well written The methods presented use a different formulation, relying on points that is less researched than voxel based methods.	The whole manuscript was well prepared and clearly written, the contributions in both technical and clinical perspective were well demonstrated.  The proposed method took both bony and facial surface points to be considered, it is more convincing for real clinical application. The experimental results showed good achievements and efficiency of the proposed method compared with the SOTA FEM-based method.	This paper proposes an innovative approach to address the problem of transferring correspondences between shapes and/or point sets, which is a well-known and open problem not only for the specific application considered in this paper. I believe this work can have a quite broad impact on all those applications that require correspondence mapping between different point sets. In addition to the novelty of the methodology, I congratulate the authors for the clarity and the thoroughness of the paper. The methodology is clearly described with plenty of details. Figures are well-done, self-explanatory and provide complementary and essential information to the text. The conducted evaluation is complete and convincing. It assesses the performance of the method in comparison with alternative approaches both quantitatively and qualitatively, includes statistical analysis and an ablation study. Obtained results are thoroughly discussed. Limitations of the method are mentioned, as well as possible future directions.	The methods could be better explained with more detail. o Figure 1 needs more labelling and explanation in the caption o 2.2 convulational layers are used but it is not clear how many filters are used The context for the results is missing.  How large can errors be for this task?  What is the resolution of the imaging being used?	One concern is that, as demonstrated in experiment part, no significant difference was found of the quantitative experimental results on six different regions. It probably because the different deformity may involve different regions, the author may further refine the task and the dataset in future work to further improve this point and apply the proposed method to real clinical application.	"The only main weakness that I find in the paper deals with related works.  First of all, references to related works should be double-checked. For example, authors cite PointNet papers [9,10] to support the sentence ""Deep learning-based approaches have been recently proposed to automate and accelerate the surgical simulation"", which is not consistent. Moreover, I would suggest authors to include some references to other works which had to cope with the problem of identifying correspondences between shapes/point clouds in surgery (such as https://link.springer.com/chapter/10.1007/978-3-030-87202-1_36 or https://link.springer.com/chapter/10.1007/978-3-030-59719-1_70). This would allow to strengthen the paper by stressing the potential impact of the proposed methodology outside the specific field considered."	Could be reproduced.  The dataset and code are not open, neither seem to be referenced.  The methods are described however some details are lacking and this is noted in other sections of the review.	The demonstration of the method part is clear. The reproducibility is good.	The paper provides many details on their implementation, hence there is no major reason to argue about paper reproducibility. Authors report the different hyperparameters used (both in the data processing phase and network training and evaluation phase). Details on the dimension of each modules of the architecture are also provided. Although authors declared that they shared dataset and code in the reproducibility checklist, I do not see any link to such resources in the paper. I would expect that authors share their implementations upon acceptance.	Introduction The introductory paragraph about the use of FEA should be revised.  The standard of care seems to be primarily physician opinion.  FEA could be used.  Is FEA used clinically with widespread deployment for this indication?  That is not my understanding. The references do not refer to specific uses of modelling for CMF surgery.  Has modelling been specifically used for CMF surgery and shown to be successful?  If so, the authors should reference it.  If not then the paragraph should be largely revised. Most of the second last paragraph can be put in the methods.  This paragraph is repetitive and not clear until having already read the methods. Figure 1 - more detail is needed and more description in the caption.  There are numerous rectangles that are completely unlabeled.  Other than a pointnet++ feature extractor it is completely uclear what the other elements are. Dataset Include the spatial resolution of the imaging used.  Comment on how this relates to the errors observed. How accurate were the registrations? Implimentation What boundary conditions are put on the FEA. How difficult are these to apply? Results The nose predictions appear to be the best.  Is this because there is little change in the nose after osteotomy?  How much displacement is there between the faces before and after surgery? Ablation study o The finding on that CPSA was more effective than closest point is overstated.  The differences are small and the model has more parameters, these parameters may establish point correspondence or may do some other error correction	"The facial deformities are complicated, the author could focus one or two kinds of main deformity to conduct and evaluate their method, as this work is really useful in clinical area. But the high accuracy is required. The qualitative experiments were performed on upper/lower lips, this may be the key attention of the clinicians? On page 5, "".... deep learning-based SkullEngine segmentation tool, SkullEngine segmentation tool [6],..."", repeated ""SkullEngine segmentation tool""."	"The paper is strong and well-written, thus there are no major issues (apart from the ones mentioned above dealing with the related works). Some possible improvements/suggestions are listed below. The computation time required by ACMT-Net to complete a simulation significantly outperforms its FEM-based competitor. It would be interesting to report which is the part of the proposed methodology taking most of the computation time. Authors should consider adding further details about the experienced surgeons who took part in the qualitative evaluation. How many of them? Typos: Faical -> facial ""Skull engine segmentation tool"" is repeated twice in 3.1"	good paper, a useful goal, novel implimentation some issues related to clarity, details and discussion of the findings	The manuscript was well prepared and written, the motivation and contribution are clear. The constructed dataset is reasonable. The demonstration of the method is good. And the author presented the good results and high efficiency of the proposed method. This work may have a potential application in orthognathic surgery.	The paper proposes a novel and interesting methodology to address a well-known and open issue in surgery. The paper is clear, well-organized and detailed. The conducted evaluation is satisfactory and thorough. Obtained results are strong and convincing.
133-Paper1387	Deep learning-based Head and Neck Radiotherapy Planning Dose Prediction via Beam-wise Dose Decomposition	The presented work introduces a new strategy for deep-learning-based radiotherapy dose prediction. The proposed method includes two main innovations: 1) A neural network is used to predict the radiation dose along the paths of the individual radiation beams before averaging the obtained dose maps. Explicitly encoding the prior knowledge of beam-wise radiation delivery has the goal of improving accuracy along the beam paths. 2) The authors introduce two loss functions, value-based DVH loss and criteria-based DVH loss, with the aim of placing more emphasis on the dose prediction in the regions containing the tumor and critical organs. In extensive benchmarking experiments using the publicly available OpenKBP dataset, the authors show that their method outperforms seven other algorithms.	"This paper proposes a deep-learning based dose prediction method. It generates beam mask to represent an irradiation boundary as a prior knowledge, and applies ""disassembling-then-assembling"" strategy for the dose prediction."	This paper presents a framework for head and neck radiotherapy planning dose prediction, including global coarse dose prediction and beam-wise dose prediction based on decomposition and multibeam voting mechanism. Some experimental results on the public OpenKBP challenge dataset are provided.	The newly introduced strategy to include the beam directions as prior knowledge makes intuitive sense. It is elegantly implemented in form of the beam masks, which can be efficiently calculated and processed. The proposed method has been extensively compared to seven relevant baseline methods. Additionally, ablation experiments were conducted to quantify the benefit of the newly introduced components. All benchmarking experiments were based on the publicly available OpenKBP dataset. The manuscript is clearly structured and nicely illustrated, making it easy to understand the main ideas and conducted experiments.	This paper proposes a novel deep-learning based dose prediction method, which generates beam mask to represent an irradiation boundary as a prior knowledge. This paper evaluates the estimation accuracy using a grand challenge dataset and showed the proposed method outperformed the state-of-the-art methods.	"The general idea of ""disassembling-then-assembling"" strategy to consider the dose prediction task seems interesting. The proposed architecture looks workable to this specific task."	"I do not understand the reasoning behind the value-based DVH loss. In order to calculate a dose volume histogram, voxels receiving a similar amount of radiation dose have to be binned together (see Nguyen et al., ""Incorporating human and learned domain knowledge into training deep neural networks: A differentiable dose-volume histogram and adversarial inspired framework for generating Pareto optimal dose distributions in radiation therapy"", Medical Physics 47.3 (2020): 837-849, reference 10). Without such thresholding or binning operations, the inner part of equation 3 simply becomes the average error - the sorting operation does not change this. Radiotherapy dose prediction is one important step for radiotherapy treatment planning and has been arguably the most researched one in the recent deep learning era (Ge and Wu, ""Knowledge-based planning for intensity-modulated radiation therapy: a review of data-driven approaches."" Medical physics 46.6 (2019): 2760-2775). However, in order to generate an actual treatment plan, a set of radiation beam arrangements, shapes and intensities that delivers the desired dose distribution has to be found. While experiments demonstrating the utility of the proposed method as starting point for such an optimization may exceed the scope of this study, the authors should at least discuss how exactly they envision their method to fit into the clinical workflow and acknowledge that the proposed method offers no guarantee to generate a radiation dose distribution that is physically deliverable. One aim of the work was to specifically improve the prediction accuracy increase along the radiation beam paths. However, dedicated experiments that quantify the obtained benefit have not been included. Currently, several baseline methods are not included in the result figures 4 and 5. In several passages of the paper, the authors use the terms ""statistic"" and ""significant"" without having performed any statistical analysis. The authors should either include a statistical test to determine whether the observed improvements are significant or refrain from using these terms altogether. There are a few grammar and wording mistakes throughout the manuscript that should be corrected."	No statistical analysis has been made for evaluation, and then it is unclear that the result can be confirmed. The authors should show the variance of the resultant scores. In addition, statistically significant difference should be discussed. Improvement from the existing studies seems to be small. Each radiotherapy beam might have a machine-specific dose distribution. However, it does not assume it.	Some of the key technical components are introduced without sufficient details. The evaluation and comparison are not sufficient. A fair comparison description should be provided in the experiments. More detailed comments are given in the following Sec. 8.	"The authors have essentially answered ""yes"" to every question in the reproducibility checklist. However, in reality most of the relevant points have not been included in the manuscript. While not all raised questions can be addressed in the manuscript, the checklist should be filled out correctly. I believe the most interesting information to add to the manuscript would be how the proposed method's hyperparameters were tuned (especially the loss function hyperparameters alpha and beta), how the baselines were implemented and tuned and additional discussion of the clinical significance (see comments above)."	This paper describes the experimental conditions for evaluation well.	Seems okay since author promised to provide the data and code in the reproducibility checklist. Otherwise, it is unclear to say since some key network architecture details are not provided in the paper itself.	"I believe the authors should aim to address the main weaknesses of the paper outlined above. In particular, the value-based DVH loss function should be re-evaluated and checked whether it is indeed equal to the average error. In case I have have misunderstood the corresponding paragraph, the authors should consider reworking it to improve its clarity. The experiments and results section should be updated to include all baseline methods. The authors should consider including dedicated experiments that evaluate the quality of the dose map predictions along the beam paths. Finally, I believe the authors should aim to more carefully discuss how their work fits into their vision of an ""AI solution for radiotherapy""."	This paper seems to take a promising approach.	"There are some concerns of this paper: In recent years, many deep learning-based methods have been proposed to handle this dose prediction task, what are the limitations of the previous methods? Authors lack to discuss them in the introduction section. There is no clear information about what network architectures are used in Global Dose Network and Beam-wise Dose Network. It is unclear how they realize the ""disassembling-then-assembling"" strategy at the network design level. The experiment evaluations are a bit insufficient: In Fig. 4, how about other views in the visualization results? It seems that there is no clinical expert involved in the evaluations. All comparison methods should be provided in the visualization of DVH curves (Fig. 5). How did you implement the methods, which do not have the source codes in public, such as Lin et al. [5], Gronberg et al. [3], etc.? More information should be provided for this in order to guarantee a fair comparison."	I believe the core idea of this work - decomposing the dose prediction utilizing beam path masks - is an interesting concept that has been well implemented via the proposed method. However, I fail to see the rationale behind the value-based DVH loss, the other main contributions claimed by the authors. Additionally, I have concerns regarding the incomplete experiments and the lack of discussion how this work fits into the full clinical radiotherapy pipeline.	This study propose a novel dose estimation method and showed experimental results using a grand challenge dataset. The reviewer found that this study is interesting. However, this paper has a room for improvement. Major concerns: No statistical analysis has been made for evaluation, and then it is unclear that the result can be confirmed. The authors should show the variance of the resultant scores. In addition, statistically significant difference should be discussed. Each radiotherapy beam might have a machine-specific dose distribution. However, it does not assume it. Please discuss the validity for the proposed method that does not assume a machine-specific dose distribution. Improvement from the existing studies seems to be small. Minor comments: Table 2 appears earlier than Table 1. Table 2 should be placed later in the paper.	This paper proposes a global-to-beam network to conduct the dose prediction task. Meanwhile, there are some concerns on unclear technical/network description, insufficient comparison and evaluation, esp. to justify that it is really useful and practical in the clinical studies.
134-Paper1226	Deep Motion Network for Freehand 3D Ultrasound Reconstruction	Authors proposed a new deep learning model MoNet for integration of US images. They used data from IMU for training of the MoNet.	Conventional Freehand ultrasound is utilized as tomographic imaging modality by utilizing IMU sensors together with a novel deep motion network (denoted as MoNet by the authors) for almost seamless 3D reconstruction. Thus, no exterior devices are needed anymore. Both key contributions of this paper address the IMU, on the one hand getting the elevation displacement of the planes and further to reduce the sensor drift. So this work addresses the current main challenges of Freehand ultrasound reconstruction, namely elevational displacement estimation and the cumulated drift.	This paper describes a method to reconstruct 3D ultrasound volumes from 2D video clips and IMU signals with a deep learning-based approach. In particular, two ideas related to the usage of the IMU information are investigasted:  (1) the network uses the orientation but also the acceleration signal from the IMU (which is a challenge since it is very noisy) (2) an online refinement of the prediction by trying to match the IMU signal The method is evaluated on two datasets (carotid and arm) and yields better estimate than 3 baselines.	Clear usage of IMU for the first time in this literature MoNet for reconstruction of image sequences Combining ResNet and LSTM for using temporal information in 3D reconstruction Use of 6DOF for reconstruction	Presentation of the methodology is very precise, sound and clear. State of the art and problems of approaches from related work are precisely delineated. So it seems, the paper perfectly addresses the need / missing link of the given solutions. The visualizations (cf. Fig. 5) are very nice, useful and impressive.	The paper addresses a relevant and challenging problem, namely the use of IMU acceleration data for 3D ultrasound reconstruction, which has not been done before. The method outperforms several baselines, and an ablation study is performed to investigate the effect of the two contributions. The references and figures are adequate and generally well-made.	Movement records from IMU looks really noisy and to solve that problem, better IMU sensor and possibly Kalman filter can be used. The scale of location and position is in meter which is not make sense in this study.	Process of calibration mentioned but not explained. Similar it is unclear, how the ultrasound and the IMU sensors get synchronized at highest precision.  Evaluation is performed with SOTA and proposed MoNet approach on two home in-house datasets. It would be nice to have other/external benchmarking datasets in the sense of an away game being in depth tested, too - if possible regarding input data requirements.	I am a bit skeptical about the justification of some parts of the method. The chosen notations make the paper a bit harder to understand (more details in Section 8).	Supplementary data shows that authors made a GUI for their work. I suggest to publish that GUI with your model to this research community.	without the two utilized datasets and the IMU-enriched ultrasound device hard to judge.	"Training/evaluation code, as well as pretrained models and data, are specified as ""available"" but no link has been provided."	Authors claimed that the method is really better than others. And this is clear from quantitative and qualitative data. However, explanation is weak about why the method works better than others. I suggest authors to publish their GUI and also their results for other researchers IMU and details of network implementation should be added to the paper.	"Fig.3 hard to interpret due to noisy sensors. Maybe an additional trend-line or visualization of the local variability can help the reader to understand how much the pose gets stabilized within adjacency of the neighboring frames.  Statement ""All images were scaled down to 0.6 times their original size"" needs some clarification - why? Due to the input network tensor size? Which PyTorch version is used?"	"The main focus of the paper is the usage of the acceleration signal of the IMU, so I will first focus my main comments on this part. While the experiments do show that this is effective, I am a bit skeptical about the justification of some parts of the method. I am not sure to understand Eq.2, and in particular why would removing the average signal would reduce the effect of the noise. Do you assume a noise model with a non-zero average?  Also, what is the point of subtracting the gravity vector, since the output vector is enforced to have a zero-mean? It seems to me that the equation could be simplified into: Ai <- Ai - 1/N \sum_n An. While I do understand the inspiration behind Equation 3 (v_i = v_{i-1} + a_{i_1}), I am not sure it really makes sense for feature maps. Of course the network can be trained with this model, but I am wondering whether this really brings something compared to just concatenating them. About the self-supervised step, are some layers frozen or are they all adapted? why doesn't the network prediction collapses to the IMU raw data, and completely ignore the original image content? It must be noted that this step requires more computational power and is therefore not as easy to deploy (compared to just runnning an inference). It would be interesting to study quantitatively how well the acceleration is estimated (for instance report correlations between accelerations). Unlike what is stated in the paper, the bottom row of Figure 3 does not seem very convincing to me: I don't see any particular trend between the blue and red plots. One key drawback of most current methods is the difficulty to detect ""turning points"" (having the user go back and forth to extend the covered area for instance), due to the implicit ambiguity of the direction of the probe movement. Since this method uses IMU acceleration, it raises some hope that such points could be detected and estimated, yet this is not discussed at all. General remarks The notations are not very standard and make some equations a bit confusing: angles are represented by E (instead of Greek letters like phi, theta, etc.) or sometimes with alpha, inverse transformations with a * (instead of ^{-1}) accelerations sometimes with A, sometimes with a The results are reported in a Table with only the mean and standard deviation, so no information on the distribution is available: we don't know whether one method is more robust or more subject to outliers than another one. Replacing it with boxplots or violin plots would give more information to the reader. Future work is not discussed at all in the conclusion. I think it would be interesting to see how the self-supervised step is able to help the generalization of the network (to new US systems, sonographers, motion speed, anatomies) Besides, being able to detect turning points would be a big deal and would give much more impact to this approach. In my experience, the performance of IMUs differ a lot across models. I think it would be important to report the brand and model used for the study. The authors used a statistical test to prove the significance of their results, which is commendable. However, the t-test that they used assumes the results to follow a Gaussian distribution, which is neither guaranteed nor discussed. A better alternative would have been a Wilcoxon signed rank test, which does not assume a particular distribution. Minor comments The input of the network is not clear - are the pairs of images encoded as 2 channels? Was the ResNet pre-trained (for instance on ImageNet)? ""Based on ResNet, LSTM"" -> LSTM modules are not necessarily based on ResNets It would be interesting to know whether the authors came up with this architecture after a lot of trials (i.e. this is the best position for those LSTM modules), or whether they just added them here because it makes sense and provided better results right away. What are ""loop scans"" exactly? I don't see any such motion in the 6 examples shown. Could you add one? Figure3: the blue line hides the red one and makes the figure difficult to read. Consider adding some transparency (for instance apha=0.75 in Matplotlib) It seems a bit suprising to only use a batch size of 1. Isn't it a problem for the LSTM modules? Figure 5 is nice, but it would be even better with a legend to distinguish between the different colors (more visual than reading the colors in the caption)"	I really enjoyed reading your research report. It is a combination of Sensors (IMU) and deep learning field which is a promising solution for 3D reconstruction of 2D images. I am not a fan of estimating 6DOF using Machine Learning and Statistical methods. Authors exactly answer this issue especially when I see they used EM sensor for comparison section.  Experimental results and also video in supplement data clearly show the effort of authors for improvement of 3D reconstruction problem.	It is very sound, the method is nice and the results are very impressive. Furthermore, relevance for clinical use is high.	This paper deals with a niche but active area of research.  Using the acceleration from an IMU had never been proven to be useful so far, so this paper does contribute to the state of the art.  As I mentioned in the other subsections, the paper would benefit from a polishing and a rewriting of some parts, but overall I recommend acceptance.
135-Paper2703	Deep Multimodal Guidance for Medical Image Classification	The paper proposed a student-teacher method that distills knowledge learned from a better-performing (superior) modality to guide a more-feasible yet under-performing (inferior) modality and steer it towards improved performance in two clinical tasks.	"The authors propose a method to guide a neural network with an ""inferior"" input modality based on a network trained on a superior input. The method is tested on two datasets with two different target tasks and demonstrate, that their method is even able to outperform the model trained on the superior modality."	The paper describes a multimodal / student-teacher learning approach for training classifiers to do a task based on inferior (cheaper, more accessible, more convenient, etc) medical data vs. superior (expensive, exotic, invasive, etc) medical data, and transferring information from the superior to inferior classifier to improve the performance of the inferior classifier.	The paper trained a guidance model to learn the latent representation from inferior modality to superior modality, and the classification performances slightly improved in two different tasks.	The authors present a method with ample novelty and clinical relevance. The experimental design is well motivated and described such that other researchers should be able to reproduce the results. The proposed method is benchmarked against sound baselines and findings supported by multiple well-justified metrics on two datasets centered on two different tasks.	The main strength is the importance of the problem setting, which is very widespread in medicine (i.e., the desire to use inferior modalities to do jobs that are well performed by superior modalities). Another strength is the evaluation, which very carefully teases out what exactly is added to task performance by superior data, inferior data that has been transformed towards superior data, and raw inferior data.  This evaluation is done on two entirely different medical data classification domains- another strength. Clarity of writing is an additional strength.  It is difficult to get confused about how this method works or why it might be thought to work.	The improvement of the method is not significant, which is consistent with the intuitive expectation. In my opinion, the paper is a type of feature-level fusion work, which is common in the domain.	No major weaknesses to report. Comments: You evaluated the performance on multiple splits. Please consider also mentioning the standard deviation between the rungs to allow estimating the robustness of the results (if the available space is too tight, possibly in the supplementary materials alongside the boxplots). Consider highlighting the best performing methods in the tables for easier interpretation Although widely know, consider introducing abbreviations like MRI and MSE at their first use	The novelty is not 100% clear.  We are told that references 11, 16, 20, and 30 do a highly similar thing to this paper... but there is no explicit statement about whether the current paper consists 100% of applying a useful technique from another domain to this domain; or whether there are novel methodological aspects.	the reproducibility is good.	The authors use publicly available datasets and provide details about the techniques used, such that I feel confident I would be able to reproduce the results.	The data sets are publicly available.  The code foundation is publicly available (PyTorch, TensorFlow, etc).  The implementation description is provided in some detail.  I believe reproducibility is high.	to compare the model performance difference with different input features, the statistical analysis should be used. I am not sure whether the displayed difference is significant in statistics. 2.the model performance didn't work better than  direct fusion of two imaging modalities, which made the work not important enough. multi-site external dataset (if exist) should be used to testify the generalization.	The paper is concisely written and nicely guided through the text. The related work is introduced in an appropriate length for such a conference paper. The figure and tables support the findings and contribute to the understanding. Please consider the comments listed under 5.	"""Consequently, it would be advantageous to leverage the inferior modalities in order to alleviate the need for the superior one. However, this is reasonable only when the former can be as informative as the latter."" Luckily for the authors, this assertion is not true.  If it were true, the authors would be in trouble because their own data shows that inferior is not as informative as superior.  The problem with the assertion is that there are many use cases when inferior is not expected to replace superior 100% of the time; let's say both modalities are locally available, but we are trying to reduce usage of the superior modality due to cost, burden, or some other reason.  Using the inferior modality as a cheap screening tool that rules out, say, cancer without the superior modality most of the time (say in 90% of cases), with the other 10% of cases requiring the superior modality, would still make a large positive impact in many application domains.  This kind of cheap screening problem setting is not really considered here. The description of the tradeoffs associated with image-to-image translators is mostly OK.  It's true that they don't deal with image-to-non-image translation very well or differences in dimensionality etc.  But an additional stated weakness is essentially ""what if it doesn't do image-to-image translation very well?""  That's a kind of non-informative critique- you could say ""what if it doesn't work"" about any hypothetical method.  The upside of image-to-image translators- that you can use the translated image for a multitude of different tasks, not just the one you did inferior-to-superior translation for- is not mentioned.  The current method trades off that flexibility with perhaps superior performance on a more limited job (i.e. the classification task). To me, ""guidance"" is confusing.  The term has an active connotation, akin to active learners that are provided novel training examples optimally selected for them by an oracle as ""guidance"" to improve their representations.  Really, using more traditional terminology, the inferior and superior classifiers includ feature selectors that reduce the dimension of the input data in a way that is beneficial for classification; what you are calling ""guidance"" is really feature translation or transformation. Table 1 is very difficult to follow, with notation that is only partially explained in the caption.  Where the rest of the notation is described in the text, it's in code, e.g. we need to translate from ""G(I)+I"" in the text to ""G(R)+R"" in the table.  Make it easier for the reader by writing out the row names in the table in English. The discussion seems to take it as given or non-surprising that the inferior modality adds value on top of the superior modality in both applications.  How do we know that there is really independent information in the inferior modality, as opposed to just poor guidance / feature transformation from inferior to superior?"	the model performance and the methodological novelty	The paper includes both technical novelty and clinical relevance. Together with the careful preparation of the paper and the good reproducibility, I propose to accept this contribution.	The main reasons for the strong accept are the compelling clinical application, seemingly very clear and straightforward approach, and rigorous multi-faceted evaluation.  The question about novelty compared to the prior student-teacher methods is the only factor pulling it down in my view.
136-Paper2262	Deep Regression with Spatial-Frequency Feature Coupling and Image Synthesis for Robot-Assisted Endomicroscopy	X	The first approach is proposed to automatically regress the distance between a pCLE probe and the tissue surface during robotic tissue scanning. The first pCLE regression dataset (PRD) was generated which includes ex-vivo images with corresponding probe-tissue distance. A novel FT module to synthesise pCLE images between fine distance intervals to incorporate image level supervision into the training process.	This paper presents a network, called the spatial-frequency feature coupling network (SFFC-Net), for pCLE probe to tissue surface distance estimation. The proposed network use both the image and frequency information for distance estimation. The author also proposed a feedback training strategy to boost the training. The experimental results show successful application on pCLE,	X	The paper is well-writen and well-organized. The first approach to tackle the pCLE probe and tissue surface with regression. The performance outperformed other methods. Dataset construction is helpful to the community and further experiments.	This is the first application for pCLE probe and tissue surface distance estimation. The authors proposed the SFFC-Net utilized both the image and frequency domain information. The authors proposed feedback training to boost the network training, with augmented data The results looks promising.	X	In the Methodology part, a general picture and mathematical problem formulation are recommended. More details of fig.1 and 2 are preferred. The viusalization of result is recommended.	Equation 3, why use linear interpolation instead of other interpolation? Wouldn't the interpolation create artificial image with artifacts or blurring that confuse the network?	X	Video dataset is constructed. Public Data and code is expected to release.	N/A	X	Too much details are introduced in the first paragraph. More introduction in the figure 1, 2 and its description. More straightforward visualization of result, rather than the Signal direction only.	Please see above	This paper describes the authors approach for feedback control of a probe-based confocal imaging system.  The system is image-based, whereby the distance from probe to sample is optimized by examining the probe output.  The network architecture is interesting, using both spatial and frequency domain learning, and a loss that function that encodes both the (supervised) optimal distance value and image sharpness.  Overall it seems to be a worthwhile paper that solves an important problem.  Evaluation against manually labeled distances shows good performance compared with other architectures.	The organization and clarity are good. The contribution is clear and solid. The Result is compatible while not clear to visualize.	The paper is well-written and easy to follow. The application is new and the method consisting of SFFC-Net and FT seems novel. The experiment results also looks promising.
137-Paper2350	Deep Reinforcement Learning for Detection of Inner Ear Abnormal Anatomy in Computed Tomography	This paper proposes a method for inner ear abnormality detection based on a deep reinforcement learning model for landmark detection trained in normative data only. The method is based on multiple landmark locations in CT scans using communicative and standard multiple agent reinforcement learning (C-MARL and MARL). After landmark location, two different metrics for measuring abnormalities were defined: Variability across agents within a PCA subspace and Q-values.	The authors describe a deep reinforcement learning framework for inner ear abnormality detection that leverages landmark detection. The abnormality detection is expressed through 2 proposed methods. One approach uses a projection of proposed landmark configurations into PCA space and analyses the variability using a norm (Procrustes distance) defined in that space. The other uses the distribution of Q-values of the last ten states before landmarks are located (i.e. measurement of uncertainty of final landmark location) as a measure. They entire approach is based on using normative data and does not require learning representations of the anomalies. They conduct  tests on artificial and clinical data and the results indicate good performance.	The authors presented a framework to detect inner ear abnormality using deep reinforcement learning-based landmark localization. They used a score that combined the PCA shape distance and the Q-value history distribution to detect the abnormality.	Clarity in the exposition and detailed explanation of the problem and state of the art. Great validation with manual annotations, scans with congenital abnormalities (123) and normal anatomies (300). Results show good detection performance.	Their proposed method is training uniquely on normative data for landmark location making it easy to adapt for other anatomies and also circumventing the need to represent the anomalies, nor have for that matter have balanced data of normal vs. abnormals. This is significant for the medical imaging case where such data are difficult to obtain. Both abnormality measures are relatively straightforward to implement and easy to interpret. The first is a typical analysis in geometric morphometry-based anomaly detection. The second method, uses the variability in Q-values for landmark detection with the hypothesis being that normal configuration would result in a uniform distribution while abnormal configurations of landmarks would have a more varied distribution. The authors provide tests on both synthetic and real data with compelling results for both despite a significant drop in the performance in the real dataset. This seems to be the first work of its kind and presents a possible solution to a non trivial problem of anomaly detection in the inner ear.	The idea of combining (or simply separately using) the PCA shape distance and the Q-value history distribution to detect the abnormality is novel. The way to generate a synthetic set is also interesting. The paper is well-written.	"The methods part is not clear and detailed.  1. How do you define D_image? How is it related to d_ji?  2. How do you ""merge"" the Q-values for each specific landmark? it seems you have a vector of Q-values per agent and a set of runs. How do you calculate the standard deviation if those are 10 runs of vectors? Please clarify  3. How does aggregation of variances across agents and Q-values validate the hypothesis of uniform distribution?  4. Describe Fig. 4 in the caption.   5. The definition of the weighting factor looks somewhat arbitrary. How were D_training and U_training trained?   6. The weighting factor defined with the median might dramatically change the variance of wU_image after weighting. Depending on their resulting variances, that may lead to an effective C_image equal to D_image or wU_image. "	The strength indicated above of using normative data only could also be viewed as a weakness. Without knowing the representation of anomalies, the decision boundary for anomaly detection maybe ambiguous (see Xuan et al, 2022, GAN-based anomaly detection. https://doi.org/10.1016/j.neucom.2021.12.093). For example, this approach would be limited approach in differentiating novel anatomies vs real abnormalities. However, as a first attempt this can be considered a step in the right direction. The core deep reinforcement learning presented is largely a straightforward application of Leroy et al (2020). https://doi.org/10.1007/978-3-030-66843-3_18 and Vlontzos et al (2019). https://doi.org/10.1007/978-3-030-32251-9_29. The innovation is therefore limited to the development of the abnormality measures. There is no effort to explain the difference in the two developed abnormality measures' performance particularly for the second data set given that the two approaches are quite different.	Lack of comparisons to other methods. With enough amount of data, I was curious how a classification network would perform on this task.	The reproducibility of the paper is low. Datasets and code are not available. The description of the dataset heterogeneity was not provided.	The models and algorithms are reasonable adequately described although the reader is referred to the literature for the training procedure.  The data is adequately described. No code is provided.	The authors will not release the code.	"I think this is a very promising work that shows good performance in abnormal inner ear anatomy detection. The validation framework is excellent, and the proposal is sound. However, the good quality of this work is reduced by the lack of clarity in the methods part. Please, consider clarification of the methods in the following points:  1. Describe the distance between shapes properly. d_ji is the L2 norm of the difference between shape vectors b_i, b_j?  2. How do you ""normalize"" and ""merge"" the Q-values?  3. How do you calculate the STD from vectors of Q-values?  4. How does aggregation of variances across agents and Q-values validate the hypothesis of uniform distribution?  5. Describe Fig. 4 in the caption. With these clarifications, I would strongly recommend publishing this interesting work."	"For the first measure based on PCA, a key limitation of the covariance matrix in classical PCA is its high sensitivity to anomalies meaning increased false positives. There are more robust PCA variants available even within one of the papers the authors cite: Amor et al. (2017) https://doi.org/10.1109/DISTRA.2017.8167682. This maybe worth mentioning in a discussion section. The two methods presented PCA and Q-value based showed markedly different performance (fig 5e and 5f) for which the authors should try and provide any explanation. Some statements on limitations of the approach and future research direction would be welcome. The training regime is not presented and instead the reader is referred to the literature. This could be briefly outlined so the manuscript is self contained There is no explanation for why 3 agents per landmark are used for the test on Synthetic data whereas 1 agent per landmark is used for the Real abnormality test. An explanation should be provided. It's not clear where the 92 normal anatomy images mentioned on page 5 come from until one reads the results section on page 7 which is odd. The narrative order of mention should be revised. A description for the normalisation of Q-value vectors on page 6 would be helpful. How are the values normalised? The authors may want explain their specific contributions, if any, to CMARL and MARL. The authors may want to improve the readability by breaking up some of the long sentences. Some attention should be paid to grammar throughout the manuscript. A range of voxel resolution on described on page 3 should be provided. There is a typo on the first sentence of the Figure 1 caption, "" Set of landmarks used in small the Synthetic Set""."	Add more comparisons to other methods. I was curious how a classification network would perform on this task. It would be interesting to further explore how to better combine different abnormality scores and make a better prediction. For example, the way to choose the weighting factor, other clues from the model output when the abnormality exists.	The proposal, approach and validation are good. The method's clarity can be improved.	This is interesting work albeit preliminary. This reviewer feels the authors should have focussed on one of the approaches and developed a more complete description and  evaluation than reporting on two different approaches incompletely. In addition, the narrative presentation needs some revision as the long dense sentences sometimes make it difficult to understand what the authors intend to convey.	The paper has a clear clinical motivation and brings some new ideas to detect the inner ear abnormality.
138-Paper2325	Deep Reinforcement Learning for Small Bowel Path Tracking using Different Types of Annotations	The paper introduces a RL based method to extract the path of the small intestine from ct images	A reinforcement learning framework has been setup that utilises ground truth segmentation data and optionally ground truth path data to learn path tracking. The reward is computable even without the ground truth path, thus making this optional during training. Experiments were performed using CT data and ground truth labelled by a human observer. The results show improvements compared to other techniques, however, the lack of statistical tests limits the conclusions that may be drawn.	This paper presents a DRL framework to extract the centerline of the small bowel. The proposed framework can be trained on different annotations, i.e., bowel segmentation and bowel path.	the method appears to be performing well. to the best of my knowledge this is a novel application of RL for this problem set	The manuscript is well-organised and information is easy to find within the respective sections Sufficient details have been provided in order for the work to be reproducible The problem being solved is well-motivated and properly justified using previous work	It is the first DRL based method to track small bowel path, and the idea of enabling the framework to train on different types of annotations is interesting. A customized reward calculation algorithm that incoorperate the wll detection and geodesic distance transform. Comparison to a wide variaty of other methods (graph-based, DRL-based, and supervised learning-based methods)	there is little discussion on related literature - algo 1 could have been better analysed in text - It is not clear why RL is the solution here and not a wall aware graph method or topological method. Can the authors please expand on the motivation of their chosen methodology	"It is not obvious what the task of 'path tracking' means (without reading other cited work) and a complete definition needs to be added in order to make the paper self-contained Some unclear sentences e.g. ""They were done during the portal venous phase"" (better to clarify what 'they' means); ""to include from the diaphragm through the pelvis"" (to 'include' what?) Some informal wordings e.g. ""It would be nice if"" Some undefined terminology e.g. ""One more termination condition of zero movement"" (the condition needs to be explained further; is it when the agent converges to a steady answer?); ""wall detection as input"" (I appreciate the explanation in the following sentences but what is exactly is used as input? Is it a mask of the image?) Some claims made without proper evidence e.g. ""Euclidean distance to the closest point on the GT path, which is used in [20, 9], would be inappropriate for our problem"" (I appreciate the explanation that follows but some preliminary experiments to demonstrate this would have been nice to see) Statistical test results for comparisons help to justify claims such as ""increase the performance"" and provide information about comparisons, however, these are not performed"	The metrics to evaluate the methods are different from those in Ref. [16]. And the reason/motivation of using different metrics is not clear to me.	Paper appears to be at an acceptable level with regards to reproducibility	Sufficient details have been provided in order for the work to be reproducible	The authors will make the code available upon acceptance of paper.	See above	To make the manuscript more complete and to justify the conclusions that have been drawn, the authors should address the comments made. Specifically comments about statistical tests since without these tests drawing any conclusions is difficult. Moreover, some sentences need clarification and some definitions need to be added in order to make the paper self-contained. Please refer to the comments above for further details about the points mentioned here.	Overall, I think it is a good paper. The only concern is the way to evaluate the predicted paths as mentioned in the weakness part.	There are no major errors in the approach - paper appears to be interesting for the community	The major weakness of the paper is in justifying the results using statistical tests, if that is addressed along with some other comments made above, I think this paper would be more complete.	It is a nice DRL paper in the application of path tracking . The authors adapted some related algorithms to make the DRL work well.
139-Paper0232	Deep treatment response assessment and prediction of colorectal cancer liver metastases	The paper proposes a longitudinal approach to treatment response assessment via siamese networks as well as treatment response prediction from baseline image.	Authors have proposed a method to predict and assess treatment response using pre-treatment and post-treatment CT scans and deep learning. The problem is of clinical importance and the method achieved a good performance	This paper proposed a model to assess the treatment response from pre- and post-treatment 3D CT volumes. The same network architecture was also used for predicting treatment response from pre-treatment scans only. The models were trained/tested on a dataset of 102 patients with liver metastasis treated by chemotherapy.	treatment response prediction can help in patient stratification for a given treatment. Ablation study to establish contribution of each component	Clinically relevant problem Well-written Proposed completely automatic system can avoid the variability caused by manual segmentation	Applied deep learning model on longitudinal data to predict chemotherapy response.	Since the dataset is relatively small ... 245 scans ... crossvalidation or montecarlo sampling would have been better to show robustness of the approach. It can currently be biased to the split.	No clinical or demographic information of the cohort is provided. PRODIGE20 dataset is for aged patients (~75 years). It can create a bias in the model. I am wondering how the model will work for the young generation It seems authors have designed a classifier to predict 4 classes. In that case, it is not clear what the ROC curve is providing. Moreover, it will be interesting to see the results for each class. It is not clear why the method is working so well. Which features were extracted using deep learning!!	There's limited innovation in the model design. The feature extraction model is based on ResNeXt architecture with slight change of adding skip connections. The size of the dataset is limited. No cross validation was conducted. The clinical value of treatment response assessment model is unclear.	Reproducible	No issue	The model was trained and tested on a specific dataset PRODIGE20, which was collected in a previously published study. All cases were all colorectal cancer patients with liver metastasis and received specific treatments. So it is difficult for readers to validate the performance of the model using other datasets.	Further clarifying the difference between the Siamese network approach of Jin et. al. and the proposed work would be beneficial. Including a sentence or reference on the validity of the selected augmentations would help justify the choices made. Was only one type of treatment used? How would the performance vary with treatment ... a quantitative evaluation of some sort.	see no. 5	Given the size of the data is limited, please consider k-fold cross validation (or additional testing on independent data). All the data can be fully utilized through cross validation. Also, more evaluation metrics can be obtained to check the stability of the model across different fold. It is not very clear how CT pairs were split into train/validation/test. Each patient can have multiple CT pairs so there might be label leakage if the train and test have images from the same patient. Please clarify how dataset was split. How many pre-treatment CT scans available for each patient? If there's only one, the total number of cases for the prediction model is only 102 (not 400). Please clarify the data difference for the two tasks. How was the liver metastasis segmentation evaluated? Did the segmentation performance impact the outcome assessment and prediction? Did the TRA and TRP models share the parameters in the ResNeXt module? Or they were trained separately? How was the sensitivity/specificity defined for the four-way classification problem? I can see the potential value of the treatment response prediction model, but it is unclear the benefits of the treatment response assessment model for clinical application. If the goal is to alleviate the burden of manual liver metastasis segmentation, the segmentation model has already achieved this goal. As mentioned by the authors in the paper, RECIST is just a unidimensional assessment of the lesion based on human annotation. So some simple measurement of the lesion contours would be enough if the segmentation is good.	Treatment response prediction is an important step to patient selection for treatment planning as well as drug trials. Promising results in this direction. Results are on a small dataset which is typical in medical image problems 3D images are used as input as compared to 2D thus incorporating more local and global info.	The paper is clinically relevant as well as technically novel.	The results presented in the paper were not fully validated because 1) the data size is limited and no cross validation was used; 2) there might be label leakage for the treatment response assessment model if multiple CT pairs were acquired for one patient.
140-Paper0637	DeepCRC: Colorectum and Colorectal Cancer Segmentation in CT Scans via Deep Colorectal Coordinate Transform	The paper presents DeepCRS, a method to segment colorectal cancer (CRS). DeepCRS guides the tumor segmentation using a surrogate task to locate the colorectum, allowing the model to better differentiate the target organ from the rest. The results show that the proposed approach obtains superior performance than nnUNet.	The authors propose an extension of a UNet like method to improve segmentation of the colo-rectum and colorectal tumours. Addition of a colorectal coordinate transform to capture positioning with the addition of a regression loss Addition of attention to the network This accounts for the relatively unusual shape characteristics of the colo-rectum compared to other organs.	This paper studies a segmentation problem of the colorectum (colon and rectum) and CRC in routine abdominal CT scans. The task is interesting and valuable. The idea of adopting topology information to enhance the segmentation performance is rational and novel.	Novel approach to segment CRS: the surrogate task proposed helps the model to better identify the organ and improves the results. This solution tackles a problem inherent to the task (organ with a long shape that is hard to distinguish) in a simple and effective way. Complete experimental validation: the model starts from nnUNet and performs two major modifications: adding self-attention layers at each stage and adding the surrogate regression task to guide segmentation. Both modifications are evaluated independently and together to show their effect in colorectum and tumor segmentation. Results reach human performance: the method obtains similar metrics to the human annotators. Complete related work: the paper correctly places itself with respect to the literature.	Generalisable extension of a Unet like architecture for the colon. Nice adaption of a standard segmentation approach to specific characteristics of a certain problem. In this case the length of the colon. Appears to be a novel deep learning approach to the problem Good comparison to the literature in table 1 that provides a clear reference and differences in the datasets Evaluation - Good comparison to nnUNet and the impact of both proposed extensions as well as a range of performance metrics including detection rate and DSC in Table 2. This provides clear justification of the impact of the additions.	The motivation of the paper is solid and clear; The task of the paper is interesting and important; The solution of the algorithm is novel; The experimental results to some extent validate the effectiveness of the algorithm.	The relevance of some results in unclear: in table 2, the last row shows the results of nnUNet and Swin UNETR in the colon task of the Medical Segmentation Decathlon, but there are no results for DeepCRC. Hence, the reason to include results on that dataset is confusing. In addition, the analysis of those results is confusing because it is combined with the conclusion on the in-house dataset (last paragraph of Quantitative results).	Would have been more valuable to also apply to public colorectal datasets with prepartion e.g. the medical image decathlon dataset so that true comparisons can be made to the state-of-the-art. This would demonstrate the generalisability of the method and a wider comparison to published results. The authors conduct 5-fold validation on the 107 case dataset. While not overfitting during training, there is a risk that algorithm decisions and hyperparameters were tuned to this dataset. It would strengthen the paper to see an additional independent dataset or testing of generalisability on the medical image decathlon dataset No code and models are planned to be released making this approach difficult to verify independently, especially given that a custom in-house dataset was used.	"To make the contribution of the paper clearer and make it to be valuable to a broader range of readers, more discussion about the existing literature on how topology information is exploited is important. More state-of-the-art segmentation algorithms suitable for colon rectum and colorectal cancer Segmentation should be compared. The expression of the paper is clear in most of the circumstance. However, there are still typos and grammar mistakes in the paper. It should be checked carefully. Here is an example: ""We validate our proposed method on a in-house dataset, including 107 CT scans with manual colorectum and CRC annotations."""	Neither code nor the dataset will be made publically available. The description of the experiments is complete and might be sufficient to reproduce the experiments in some extent.	No training, evaluation code or trained models provided make this algorithm potentially difficult to reproduce. Especially given the fairly complex adaptations to the training loss and network to incorporate both the predicted coordinate map and attention.	The dataset is an in-house dataset. This could be a problem of code reproducibility.	MSD is used both to refer to the Medical Segmentation Decathlon and the Mean Surface Distance, making the analysis confusing.	This approach appears to have a lot of potential and thorough validation is performed on a small in-house dataset with cross validation To show more generalisability and further independent testing it would be valuable to show performance on Decathlon-Task08 which is reference for comparison. Even if the dataset does involve preparation -- which should make the task easier. Alternatively, releasing the code would make this easier to verify independently.	Please check part 5 and part 7 for detailed information.	The paper presents a simple and novel approach to solve the task at hand. The experimentation validates all the design choices and the results prove the improvement obtained by the surrogate task intorduced.	Paper has potential but for MICCAI standards it needs evaluation on a wider or alternative dataset, or release of the code to allow reproducibilty and independent testing on other datasets.	The motivation of the paper is solid and clear; The task of the paper is interesting and important; The solution of the algorithm is novel; The experimental results to some extent validate the effectiveness of the algorithm.
141-Paper0970	Deep-learning Based T1 and T2 Quantification from Undersampled Magnetic Resonance Fingerprinting Data to Track Tracer Kinetics in Small Laboratory Animals	A deep learning based MRF parameter mapping method is proposed for preclinical MRF data, achieving 4-fold further acceleration compared to the baseline MRF method.	This paper presents a deep learning method to compute T1 and T2 map by MR fingerprinting in Mice that have been injected with a Mn2+ tracer. The method mostly replicates blocks that have been applied in Human before, but trained on Mouse data. An ablation study was performed.	The manuscripts presents a deep learning-based method to generate T1 and T2 maps from undersampled MRF data acquired in mouse brain pre and post Mn2+ administration. Specifically, the developed pipeline involves sliding-window cascaded modules, a U-Net to infer T1 and T2 maps, a network to generate back MRF from the inferred T1 and T2 maps, and a data-consistency module to suppress estimated errors. The performance was evaluated by means of MAPE, PSNR and SSIM and proven to be higher with the dedicated components. The approach to solve the limitations from MRF is novel and well addressed.	The proposed deep learning based MRF method uses a sliding-window averaging to extract spatial features and combine the use of map loss and recon loss. The results show a 4-fold acceleration and faithful T1 mapping both before and after Mn2+ injection.	Significance: There is a lack of computational methods development for small animals and this paper fills a gap. It convincingly shows that DL-based MRF can be applied to Mouse data and saves acquisition time by allowing a higher undersampling factor. Validation: The impact of the different components of the network is correctly evaluated in an ablation study. Writing: The paper is well written and very easy to follow.	Overall, it is a well written manuscript with supporting validation. The application of the modules with sliding-window and U-Net for MRF reconstruction is appealing. The proposed framework has been well explained. The method achieved an on par precision compared to the baseline method. The ablation study is sound and validates the improvement of each component in the proposed pipeline. The proposed work tackles the clinical need for robust MRF reconstruction from undersampled data.	As only T1 map is needed to detect Mn2+ tracer effect, a T1 mapping only method with much shorter acquisition time can be used instead of using MRF and complicate network and map both T1 and T2 information. In other words, simultaneous T1 and T2 mapping is not mandatory here.	The proposed method is not hugely novel as it mostly builds on architectures that have been proposed for MRF in Humans before. (Method novelty is not necessarily a claim by the authors here, and they correctly cite previous works). Most components have no discernable impact on the reconstruction score. The use of a U-Net over a template-matching approach, and the use of the moving window over a feature extractor (i.e., a single conv layer) are the only choices that seem to clearly improve things. In general, an issue with single dataset studies like these is that it is unknown if the proposed method requires retraining on each new dataset, meaning time and compute, or if the learnt features are general enough to be applied to any new dataset (with the same sequence). The impact of the paper would be much stronger if generalizability was investigated. Another impactful question is: can you design/train the network in such a way that any number of spiral shots or any number of time frames can be used as input, so that the most is made of the available data.	Data augmentation during training: an up-down flipping may barely augment usable data, other simple data augmentation techniques could be more useful. Missing discussion on related works [3,4] and explanation of the lower performance of the cascaded network.	A detailed description of the model and the data used is illustrated.	The checklist states that the dataset is downloadable, but I did not see an (anonymized) link in the manuscript Same for code and models The exact architecture of the UNet (number of features per layer) is not provided.  Not a problem if code is released. Hyper-parameter strategy is not mentioned, but maybe no hyper-parameter search was performed (which would be fine) Time/cost/memory not reported No statistical significance tests	The checklist mentions the code will not be available in a repository as it is still under development, which limits reproducibility.	A deep learning based MRF parameter mapping method is proposed which uses a sliding-window averaging to extract spatial features and combine the use of map loss and recon loss. With the proposed method, a 4-fold further acceleration is achieved compared with the baseline MRF method. Due to CSF flow and large T1 and T2 property, T2 estimation is always not as faithful as parenchymal tissue under MRF parameter mapping framework. Besides, CSF normally is not of interests. Thus I suggest to further analyze on different brain tissue(such as WM, GM and CSF), which should better show how different methods or different acceleration factor data perform.	In general, I think it is a good scientific paper, with sound experiments. My main concern is the low impact it may have, due to limited novelty of the methods and narrow application. It may not be the best-suited conference for this paper. One of the stated objectives of the authors is to track T1 variations induced by Mn2+, and the training set includes images acquired before and after Mn2+ injection. While the authors do show T1 differences between time points, an even stronger experiment would be to train on pre-injection images and test on post-injection images (and vice versa) to see if the inclusion of both conditions in the training set reduces a bias in the fitting process; or conversely if training on control cases only generalizes to Mn2+ cases. It could be clarified how the data was undersampled for training: were the same two spiral arms always used? Or were two spiral arms out of 48 randomly selected? It is a bit surprising that the moving window does that much better than the feature extraction layer. It seems to me that the FE layer has the flexibility to learn a simple moving window. Does it mean that the FE is just more difficult to train? (t would be useful to state the kernel size - input and output features, width and height - of the FE layer) It's not perfectly clear to me if C-2 contains exactly one consistency loop? (i.e. C-1 would not use the consistency module at all) Similarly, does the fully sampled component (FS) correspond to lambda != 0, that is the loss include loss_recon? There are no statistical tests, and it seems to me that no true difference exists between SW+U/SW+U+FS/C-2.	In the abstract, I would suggest mentioning only the approach that yielded better results, to avoid confusion. The implementation details should be described, i.e., training time, test time, etc. Discussing the related works [3, 4, 5] would help to highlight the value of the contribution. SW+U+FS was expected to yield better results, with the current tone of the manuscript, but it did not. Rephrasing or further discussion would clarify this. Data augmentation approach could have been more exploited. The works [3, 4, 5] have not employed data augmentation.	A deep learning based MRF parameter mapping method is proposed for preclinical MRF data, and it uses a sliding-window averaging to extract spatial features and combines the use of map loss and recon loss. It achieves a 4-fold further acceleration compared to the baseline MRF method, which should also be applicable to human MRF for acceleration.	My recommendation is mostly driven by the lack of technical novelty and the narrow application. The science is sound but lacks a broad impact.	The presented work aims to solve the existing problem of MRF acquisitions with a well explained framework achieving promising results.
142-Paper2133	DeepMIF: Deep learning based cell profiling for multispectral immunofluorescence images with graphical user interface	The authors develop a deep learning based framework to identify cell phenotypes for M-IF images. The authors develop a whole slide M-IF viewer to visualize the M-IF images as well as cell detection and phenotytes.	The author proposed DeepMIF, a novel deep learning method for detecting and quantifying cell phenotypes on M-IF images. DeepMIF also includes a GUI, making it accessible to researchers with less programming background. The authors also demonstrated that DeepMIF was able to achieve effective cell classification performance.	This paper proposes a graphical user interface based on deep learning for the profiling of cells in multispectral  immunofluorescence. The paper is well writen and described and a graphical user interface would be attractive for researchers who are not interested in running scripts from the command line. As such it is an interesting contribution.	A deep learning based framework is proposed for the cell detection and phenotyping for M-IF analysis. A GUI is developed for the M-IF visualization.	Building accessible computation tools is essential for better biomedical research. DeepMIF enables effective cell phenotype identification in M-IF images. It also shows great accessibility with a GUI and generalizability to different panels.	The paper is well written, offer a standalone GUI capable of analysing multispectral data (up to 25 channels) and provided a very high output, higher than Resnet, Inception and other architectures. The improvement over VGG is marginal but still it is better, which is good, and at the levels they are working, would be difficult to have a higher improvement.	Cell detection is proposed on M-IF images, though the authors show promising classification performance based on the detection results. While cell segmentation is more desired from the biology point of view. Besides, there are existing deep learning based cell segmentation algorithm (deepcell). With the detected cells, a fixed size (20x20x3 for nuclear, and 28x28x3 for non-nuclear) is used for classify marker positivity. While this size lead to the inaccuracy for the marker positivity evaluation since different cell can have different size.	While the paper is clearly written and the authors provided detailed results to demonstrate the effectiveness of the proposed method, some parts of the evaluation could be further improved. Moreover, as DeepMIF is proposed as an accessible tool, it would be great to see its application on different diseases. This is also mentioned by the authors in the limitations.	My single criticism of the paper is that I could not find that the data is available. The code will be released through GitHub upon acceptance, and that is fine, but there was no mention (or I missed it) of where the data is and if it is available for comparison. The other limitations (small sample size, etc.) are acknowledged by the authors and is understandable at this stage of their work.	Highly reproducible. Also the authors suggest it would be publicly available .	The reproducibility of the paper is good. The authors provided necessary details from the reproducibility checklist.	The software will be made available through github upon acceptance. This is reasonable and is not public yet, perhaps to keep the process double blind.  The data on the other hand is not public or I could not see where this is available.	Although cell detection is one manner for the cell-based analysis, while segmentaion based manner should be the preferred manner. Firstly, there is already model (deepcell) with promising performance, and secondly downstream analysis based on cell segmentation is far more interpreable compared with detection based manner. Secondly, the classification evaluated in this manuscript is confusing to me. Is the NK/T deemed as positive, and macrophage as negative in the setting? Then how about other cell phenotypes?	For Table 1, could the authors also list the positive and negative distributions of each DI? It would be better to also include which markers are nuclear and which are non-nuclear. Since cell detection and classification are two stages in DeepMIF, it would be better to also show the overall combined evaluation metrics to show how effective cell types have been identified from detection to classification. For the external validation dataset, what's the performance for cell detection? Could we perform cell detection for the external validation dataset based on the trained model from the Immune T cell panel? As DeepMIF cell identification is a two staged process, only reporting the classification performance could not provide a comprehensive view of the performance of DeepMIF. As performance between VGG16 and DeepMIF is very close, it might be better to perform cross validation to see if the performance of DeepMIF is statistically significantly better. Moreover, the authors mentioned that model 2 contains much fewer parameters than VGG16. What about the training time between model 2 and VGG16?	This is a good paper bar the availability of the data.	This study has great promising in the future, but the current framework needs further refinement. Cell detection is suggested to be replaced with cell segemntation. Secondly, the classification evaluation need to be further elaborated.	The problem addressed in the paper is quite important as accessible computational tools are essential for more efficient biomedical research. The paper is well-written and the results presented demonstrated the effectiveness of the proposed method.	I think this is the best paper of my stack.
143-Paper1080	DeepPyramid: Enabling Pyramid View and Deformable Pyramid Reception for Semantic Segmentation in Cataract Surgery Videos	The authors proposed a U-Net based model for semantic segmentation in cataract surgery called DeepPyram. The main contribution is the inclusion of two blocks: Pyramid View Fusion and Deformable Pyramid Reception. The former is in charge of recognizing the relative information between the object and its surroundings, whereas the latter performs shape-wise feature extraction. Ablation studies and comparisons to twelve models are also presented.	This paper proposes a semantic segmentation network, which contains three modules. The main novelties of this method is a varying-angle surrounding view, shape-wise feature extraction and multi-scale semantic feature maps.	In this paper, a network, called DeepPyram, was proposed.  DeepPyram can deal with challenges posed by transparency, deformability, scalability, and blunt edges in objects. It has three novelties: a Pyramid View Fusion module, a Deformable Pyramid Reception module and a dedicated Pyramid Loss module. The authors showed that the proposed approach outperforms existing methods without imposing additional parameters.	The paper is well written and clear in general. The use of deformable convolutions gives the model the ability to learn geometric distortions that are common in cataract surgery. The additional blocks appear to improve performance.	The main strengths of this paper are two-fold: 1) it designs a varying-angle surrounding view to extract features for each pixel. 2) It  extracts shape-wise features, which is very useful for segmentation of complex objects.	The structure and the text of the paper is good while a clear, and The paper contributes to the body of knowledge,	"The authors use data from CaDIS dataset, but they do not compare its results to DeepLab v3+ and UPerNet, which are the two networks used in the original CaDIS article [1]. There is no discussion about the impact of the imbalanced class Instruments. Cornea (78:84), Instruments ( 3190:459),  Lens (141:48), and Pupil (141:48). The authors provided an 8-page supplementary material in which they further discussed their proposal and presented additional experiments. However, according to MICCAI guidelines, only images, tables, and proof of equations are allowed and that these materials must not exceed two pages. [1] Grammatikopoulou, M. et al. ""CaDIS: Cataract dataset for surgical RGB-image segmentation,"" Medical Image Analysis, Volume 71, 2021."	The weaknesses of the paper are two-fold: 1) The proposed three modules are all combination of some exiting techniques. The novelties of this paper is very limited.  2) The experiment analysis is not sufficient. The ablation study did not explain some details. For example, the PL loss is removed, which other loss is used?	The paper needs more organization, a detailed description and more experiments.	The authors claim the dataset will be available after acceptance of the paper.	The reproducibility of the paper is ok.	The technical contribution is limited.	"What is the criteria for choosing the networks to be assessed? I missed the authors did not include the best results that have been reported on the CaDis dataset for a fair comparison even though they used an extended dataset. I recommend the authors to show cases where their proposal failed and discuss the reasons for such mis-segmentation. Minor corrections: In Abstract ""Compbined"" In Fig. 1 ""Challenges in semantic segmentation for different in cataract surgery"" ?? In Page 2 the order of citations. ""Several network architectures for cataract surgery semantic segmentation have been proposed or have been used in the recent past [15,13,14,1,21]."" I recommend the authors to sort the numbers for a better presentation ""... have been used in the recent past [1,13,14,21]."" In Section 4 and in Table 1. are PSPNet+ and PSPNet the same model?"	Explain more motivation why some exiting techniques are used and more analysis why they are effective. Provide more details about the experiments.	"1) Abstract: The authors should mention the performance improvement achieved by their model compared to state of the art. 2) The main concern is that the authors need to justify why they include PVF in the decoder network. In most of pyramid view fusion networks used in the end of the encoder network. In a typical autoencoder architecture, the encoder extracts global context information from the input image, including the adjacent and class characteristics of the object. However, transmitting information to shallower layers will weaken the extraction of the context information due to the down sampling processes. Thus, many works proposed to use PVF in order to generate multi-scale features for each split using reduced pyramid pooling module. But here, the authors insert this layer in all layers of the decoder. The authors should explain their architecture in more details. 3) The Deformable Pyramid Reception (DPR) network is needed to more details. The proposed DPR is very similar to the network proposed in ""DefED-Net: Deformable Encoder-Decoder Network for Liver and Liver Tumor Segmentation"". Also, most of papers proposed DPR networks used them in the encoder networks to learn better context information of the input images than the Atrous spatial pyramid pooling. 4) It is very important in any medical method, to show the limitation of the work is something very promising, given that when reading the work we can glimpse a way to get around these limitations. 5) The experimental section is very limited. There is no ablation study to address the effects of each module on the proposed segmentation model. The authors gave an overview of the proposed model, but they did not mention anything related to each submodule's architecture. The article was missed a detailed description. 6) Missed References Nandi, A., Lei, T., Wang, R., Zhang, Y., Wang, Y., & Liu, C. (2021). DefED-Net: Deformable Encoder-Decoder Network for Liver and Liver Tumor Segmentation."	The paper looks technically correct, but there is no reason to include experiments and further discussions in supplementary material.	the novelties of this paper is limited and the experiments are not sufficient.	The proposed model is with limited novelty.
144-Paper0626	DeepRecon: Joint 2D Cardiac Segmentation and 3D Volume Reconstruction via A Structure-Specific Generative Method	The contribution of the paper is a method for synthesizing 2D cardiac MR images by interpolating an optimized latent embedding, which has been obtained by an encoding-generating network architecture. A segmentation network is furthermore trained, using the synthesized MRIs as input. The interpolation technique allows for reconstructing (super-resolving) 3D images and segmentation, as well as transferring information from subjects with longitudinal data, to subjects without such data. The method is validated on the tasks of: segmentation, 3D volume reconstruction, and motion pattern adaptation.	This paper presents an end-to-end latent-space-based framework, DeepRecon, that generates multiple outcomes, including image segmentation, 3D reconstructed volume, and motion adaptation. Some cardiac datasets have been used for evaluation.	In this paper a 2D cardiac segmentation  and a 3D volume reconstruction tool is shown, and after this they obtain the 4D motion pattern shown in volume flow graphics, to probe the correct segmentation and reconstruction.	The idea of reconstructing 3D images and segmentations from an optimized latent embedding, as well as transfering longitudinal information, is interesting and novel. Using the generator of a GAN for decoding the latent embeddings gives a rich model for synthesizing images. The validation is thorough and shows good results on the experiments performed.	The motivation of this work looks interesting and useful, i.e., joint 2D cardiac segmentation and 3D volume reconstruction are fundamental to building statistical cardiac anatomy models and understanding functional mechanisms from motion patterns. The datasets used for evaluation look very large.	Authors show the path that clinicians need to obtain a complete analysis preview of the heart.  And get to obtain graphic volume flow.  Although it is a known problem, they propose a different solution.  The solution shows the workflow necessary to obtain a final product for clinicians. so it could have a immediate application.	The paper is good overall, however the following issues need clarification: Dice on the segmentation produced by the generated images is a good proxy for the quality of the synthesized MRIs; however, I am surprised that no metrics on the image was computed in the experiments (e.g., SSIM, MSE, etc), as this could be a more direct measure of the synthesizing quality? Why was this not performed? Some test for statistical significance (e.g., Wilcoxon) would provide improved interpretability of the results. The loss function contains several balancing hyper-parameters, but the paper does not state how these were chosen (nor their values); please specify. Furthermore, is there also some weighting between the segmentation part of the loss, and the image part?	The novelty in the method and network design is not high. Some technical components need to be clarified. The evaluation and comparison are not sufficient. More detailed comments are given in the following Sec. 8.	The paper does not have great novelty  in the technical area, it makes an ensemble of already existent architectures, to improve a known problem with a different solution.  The solution could be too elaborated, although it has very good results.  It is missing more information about parameters and the type of infrastructure needed.  Some steps are not so clear, though I understand that 8 to 10 pages is not enough space to describe it completely.	Good, the author's answers to the reproducibility checklist correspond to the content of their paper.	Seems okay.	Some parts are not so easy to reproduce specially concerning the methods used, because information is not enough, and the complexity of the architectures used, it may need more information about hyperparameters. The expertise reached by their tool it is based on coupling different architectures for the segmentation and reconstruction tasks previously known.	"""minimal radiation exposure"", MRI has no radiation exposure. The input images (x) were motion corrected by a preprocessing step I assume (see Fig 1)? Please specify how this was done. ""resampling function that align"", it is not clear to me if this step performs some registration, or simply resamples the data? Please clarify. The Settings paragraph of Section 3.1 is a bit cluttered because the five experiments are not in an enumerate{} environment. If space allows, please consider changing this for improved readability. ""3D DICE"", the Dice metric is agnostic to the dimensionality of the inputs. It is surprising to me that linear interpolation seems to perform substantially worse than computing Dice on the original data, I would expect a very similar performance, or slightly better for linear interpolation (often used as a simple baseline in super-resolution work for example). Any ideas why this might be? Fig 4: would be easier to interpret if each plot had a title (e.g., 'normal-to-normal', 'diseased-to-normal.')"	There are some major concerns of this paper: The general pipeline of this method is simple. It includes an autoencoder and U-Net network. There is not much novelty in the method and network design. Furthermore, the two tasks, i.e., image representation/reconstruction and segmentation, work sequentially from their workflow. It is unclear how they work jointly as they proposed? In the proposed framework, it is unclear how it can realize high-/super-resolution 3D image generation (no technical description on it). There is also no evaluation on this task as they proposed. In the 4D motion adaption, how to guarantee the smoothness and accuracy along the time space is unclear. From the supplemental video, we can see that the organ shape results look noisy and jittering. What are the training steps on the comparison methods (e.g., DirectSeg, SemanticGAN, W+SegNet)? We can see that the proposed method has two training step settings. But it is unclear what is the setting for the comparison methods. In general, the experiment and evaluation sections are a bit weak: The evaluation only includes limited segmentation methods for comparison, more state-of-the-art deep learning-based 3D image segmentation methods should be included. The evaluation only includes some basic reconstruction methods (e.g., Linear Interp, CPD) for comparison, but there is no comparison with state-of-the-art deep learning-based 3D image reconstruction methods. The visualization results in Fig. 2 and Fig. 3 are difficult to see the difference between different methods.	The complete system used are a good ensemble of the methods with very good results of the system is very good. Nevertheless, the novelty is not the strong part of the paper.	The methodology is novel and the validation shows encouraging results. The validation could have been more complete.	Overall, this paper proposes a latent-space-based generation method for multiple medical image tasks. However, there are some concerns on the limited novelty, some unclear technical components, and insufficient evaluations.	I
145-Paper1802	Deformer: Towards Displacement Field Learning for Unsupervised Medical Image Registration	This paper proposes a new deformer-based multi-scale registration method (DMR) for medical deformable image registration. The proposed Deformer module leverages the multi-head attention strategy to capture the long-range dependency between high-level features. The multi-scale architectural design and auxiliary loss from different levels further improve the registration performance. The method is evaluated on two public brain MRI datasets (LPBA40 and OASIS). Extensive quantitative and qualitative evaluations demonstrate that the DMR performs favourably against state-of-the-art methods (2 conventional methods, 2 CNN-based methods and 1 CNN+Transformer based method).	The paper propose one deep learning network to perform deformable registration. Instead of convolution operations, transformer module is used in the framework. Experiment is performed over the two public brain MR image datasets.	It proposes a novel Deformer module along with a multi-scale framework for the deformable image registration task. The Deformer module is designed to facilitate the mapping from image representation to spatial transformation by formulating the displacement vector prediction as the weighted summation of several bases. With the multi-scale framework to predict the displacement fields in a coarse-to-fine manner, superior performance can be achieved compared with traditional and learning-based approaches.	This method proposed a multi-scale structure with multi-head attention to capturing the long-range dependencies between the high-level embeddings. The results in tables 2 and 3 demonstrate that the proposed Deformer block outperforms the existing Transformer and CNN blocks by a significant margin. Strong evaluation. This paper comprehensively evaluates the registration accuracy, diffeomorphic properties and smoothness of the deformation field of the proposed method. The paper provides a sufficient ablation study to justify the hyperparameters choice and architectural design of the proposed DMR model. The writing is clear and easy to follow.	The paper is clear to read.	The ablation experiments and comparison with other models show Deformer module really works. It successfully improve performances in registration area.	"The idea of using multi-head attention to capture the long-range dependencies in image registration is not particularly novel. Recent Transformer-based methods [1,2] share a similar design. Model complexity. It is worth noting that the multi-head attention is notoriously computationally intensive, while the runtime and model complexity are ignored in this work. The results will be more convincing if the model complexity, i.e., number of learning parameters and FLOPS, and runtime of each method are reported. The authors argue that the attention mechanism in the DIR task may lead to inferior performance as corresponding voxels should only be found in a limited local range. However, there is no particular solution to this issue in this paper. And the proposed Deformer module also leverages global multi-head attention to model the long-range dependencies of the features, which contradicts their statement. The sample size of the test set is not sufficient. Only 1 atlas and results of 28 (8 + 20) scans are reported. The paper will benefit from including more atlas for evaluation to reduce the statistical bias. References [1] Chen, Junyu, et al. ""ViT-V-Net: Vision Transformer for Unsupervised Volumetric Medical Image Registration."" MIDL2021. [2] Zhang, Yungeng, Yuru Pei, and Hongbin Zha. ""Learning dual transformer network for diffeomorphic registration."" MICCAI2021."	Lack of novelty. Transformer has been widely used into medical image registration, authors should compare with the existed methods. Zhang, Y., Pei, Y. and Zha, H., 2021, September. Learning dual transformer network for diffeomorphic registration. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 129-138). Springer, Cham. Reviewer concerns about the validation of the experiment. In terms of LPBA40 with 56 annotated regions, although it is widely used in registration evaluation, the validation seems not to be well qualified. Like revealed in Wei et al., the annotation of LPBA40 is of low quality. If there is no refinement, it is better to avoid directly using it to evaluate the registration methods. Wei, D., Zhang, L., Wu, Z., Cao, X., Li, G., Shen, D. and Wang, Q., 2020. Deep morphological simplification network (MS-Net) for guided registration of brain magnetic resonance images. Pattern Recognition, 100, p.107171. In terms of the Fig. 2, it is still hard to find the improvement region.	It is still an encoder-decoder architecture which adds several Deformer Module. It would have more novelty with more model design.	Good reproducibility. Many technical details of the proposed method are reported and the authors claimed that the code will release if the work is accepted in the reproducibility checklist.	Should be good.	I think the reproducibility of the paper is good.	"The qualitative results in fig.2 are upside down and stretched. I prefer the qualitative result in the supplementary material instead. Minor: Avoid ""Title Suppressed Due to Excessive Length"". (The running head.)"	Please try to evaluate the methods over broad clinical registration scenarios, in addition to the brian image registration.	I think the author's work is valuable because it outperforms VoxelMorph which is one of the most popular benchmark in medical image registration. One very interesting thing in this paper is that encoder-decoder framework adding Deformer module outperforms both CNN and Transformer model. I think although this model does not have too many design on model, the results are enough provocative to researchers. I encourage the author to make the codebase open source in Github and thus people could build new benchmark in registration area.	Overall, the paper is well-written and the proposed method is adequately evaluated. Although there are some concerns about the model complexity and novelty, this paper provides a comprehensive evaluation and ablation study to demonstrate the effectiveness and the registration performance of the proposed method. I believe this paper is of interest to the MICCAI community.	Lack of novelty.	Registration is sensitive to computation times, so I think the author may add GFLOPs and FPS information of every models.
146-Paper0151	Degradation-invariant Enhancement of Fundus Images via Pyramid Constraint Network	The paper proposed a network architecture and loss functions to enhance the fundus images. It contains three modules, one for augmentation, one for Laplacian pyramid extraction and one for autoencoding.	The authors proposed PCE-Net for the enhancement of fundus images.  Moreover, a Laplacian pyramid is introduced to exploit the retinal structure in PCE-Net.	This manuscript proposes a pyramid constraint to develop a novel model called PCE-Net for fundus image enhancement. The work is of interest. The experimental results verifies the effectiveness of the model. I think it would be a good enhancement approach for fundus image analysis.	the paper written and explained well. the contributions are clear. the experimental results contain well methods, data and metrics the method sounds reasonable	The organization of this paper is good and clearly describes the pipeline of the task. The structure of PCE-Net is technically sound. As shown in table 2, the proposed method outperforms the existing method by a margin in terms of restoration, segmentation, and diagnosis.	The main idea of the work is to form SeqLCs (image sequences) and LPF (Laplacian pyramid features) from degraded images. The PCE-Net is to learn a degradation invariant model. The manuscript proposed a novel loss function to train the PCE-Net.	the main issue if the paper is in the results section and comparison to other methods. Since the proposed method has a augmentation module (SeqLC), it is not fair the competitor methods did not have any data augmentation step. Or similarly, we need to have the performance of the proposed method without the SeqLC. Therefore the contribution of the paper is not well examined. make table 1 easier to read it seems that figure 1 missed the cnn layers. Perhaps the orange arrows need to be denoted as conv/downsampling. same for gray arrows.	"The novelty is limited, for example, the feature pyramid structure is a common strategy [1] and the constraint is contrastive learning conducted on features of each level. Hence, the FPC may not be qualified a contribution to this work. The authors claim that the straight constant on feature maps is too inflexible. However, they do not demonstrate the convergence of FPC. [1] Lin, Tsung-Yi, et al. ""Feature pyramid networks for object detection."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017."	The experimental results focus on the vessel segmentation after enhancement. How about the results on classification tasks after enhancement.	The authors intend to share the source codes. one part of data is publicly available.	The reproducibility is well sound.	The description of the method is clear. I think the proposed method could be reproduced.	see section 5	The FPC shows superior performance as shown in table 2. However, the discussion on it is not sufficient. More experiments on it would make FPC more convincing.	This manuscript proposes a pyramid constraint to develop a novel model called PCE-Net for fundus image enhancement. The main idea of the work is to form SeqLCs and LPF from degraded images. The PCE-Net is to learn a degradation invariant model. The experimental results verifies the effectiveness of the model. Suggestions and questions are as follows: In ablation study, the authors didn't consider the effectiveness of the enhanced images for classification tasks. How about the segmentation results on SE, EX, MA in enhanced fundus images?	comparison to state of the art methods is not fair.	The paper is well written and easy to follow. The proposed PCE-Net and the Feature pyramid constraint are technically sound. Some drawbacks exist such as the inadequate discussion on FPC. considering all things, I recommend the acceptance of this paper.	The idea combines the neural network models and image analysis methods. It is pretty intesting. The organization and writing of the manuscript is satisfactory.
147-Paper0429	Delving into Local Features for Open-Set Domain Adaptation in Fundus Image Analysis	This paper describes a method for unsupervised domain adaptation with unknown/new classes in target domain, i.e. OSDA problem. The paper proposes a collaborative regional clustering approach to identify feature clusters, then a contrastive loss for better cluster boundary shaping.	This paper proposes a collaborative regional clustering and alignment method to explore the open-set domain adaptation (OSDA) issues in the domain of medical images. The experiments show that the proposed model can achieve consistent improvements over the state-of-the-art methods.	This manuscript proposes an open-set domain adaptation (OSDA) method for fundus image classification. The proposed method employs contrastive learning based on clusteredl assigned by k-means to align source and target features, as well as to identify private classes. This paper can be interpreted as a sensible multi-scale version of to Domain Consensus Clustering (DCC, CVPR'21).	overall the paper is clearly written and well organized. it has a clear clinical application it has a good comparison study with existing arts.	1.This paper investigates the OSDA problem in the medical imaging scenario, specically for fundus disease recognition. A novel method is proposed to perform positive transfer more precisely while avoiding negative transfer across domains, through delving into local features. The  proposed method achieves state-of-the-art performance compared to previous methods.	This manuscript investigates a somehow overlooked problem in UDA for medical images: open-set domain adaptation. The design of informative region selection makes sense considering that lesions in these datasets are often localized. It is shown to dramatically boost the performance of the baseline DCC. The proposed method is tested on two pairs of datasets, and is shown to outperform baseline OSDA methods.	computationally expensive, could be infeasible for large data model details need clarification	1.Some technique details are not clear:    a.The paper does not mention whether the training process of the CNN model is end-to-end or not since it contains multi-branch.    b. In the Informative Region Selection part, it depends on the CAM to select patch features, but not mention the size of the patch.    c. The experiments does not mention the input size. People may want to know it because fundus images are probably in higher resolution than natural images. 2.Datasets are not enough: I notice that the path cluster contains hard exudates, which are sign of diabetic retinopathy(DR). Try some DR datasets such as IDRID may make the paper more promising.	The proposed method seem to be relying on relatively strong assumptions: a) lesions (discriminative information) are localized; b) the source model performs reasonably well on the target data (small domain gap) so that initial clusters of the same class between the source and target can stay correctly close; c) there is little class-imbalance issue (otherwise majority classes may overshadow minority ones in the clustering); d) the number of private classes ($\beta$) is kind of known beforehand. The authors are encouraged to comment on whether these assumptions are too strong to be applied in practice? Given that there is a k-means clustering in the method, what is the computational overhead of that, in comparisons with other steps during the training process? There is little information regarding the source model: a) does it use a linear classifier in the end? If so, why are clusters assigned based on l2 distance, while the detection of private classifier are based on thresholding the softmax output of a linear classifier? b) will the source model to be updated during adaptation as well? If so, would it hurt the performance on source-domain data?	The reproducibility of the paper should be good since the authors will provide the code and the datasets are public. Important hyperparameters are specified in the paper as well.	The CNN model is ResNet50 and the datasets are all public, thus the reproducibility of the paper is strong with more detail of experiments' setting, for example, the input size.	There is no significant issue with reproducibility.	Overall I think this paper is well presented. The model description is clear. Here are my comments. Though the paper proposes IRS to reduce the computational cost, it seems to me it is still very expensive to train the model, considering a K-means clustering is extensively run to update the prototypes, which makes it even not feasible for large scale data. Please comment. In addition, I would recommend the paper to show computation time for model training. It may be a minor issue or I misunderstand, in the experiment K^s=5 since there are 5 diseases in the source domain, \beta=1.5, so K^t=7.5, a fractional cluster number? For the update of prototype, the paper proposes to use momentum controlled by \yita. A suggestion would be adding linear scheduling for \yita to make it variable instead of a fixed constant, this may improve convergence speed. Such approach is frequently used in existing models, such as mean-teacher exponential moving average.	"Since you have mentioned that ""they usually have similar global structures with uniformed appearances, such as the optic disks, vessels, and even common lesion patterns appeared in different diseases"", it may be a good idea to make use of the spatial information for OSDA methods on medical images."	The authors are encouraged to comment on the relatively strong assumptions of the proposed method: to which extent can the proposed method to be deployed in practice?	good presentation on the methodology clear evidence in numerical experiment on the advantage compared with existing art.	The paper states that it is the first time to investigate the OSDA problem in fundus images. The performance of the proposed methods is considerable. The expression of the manuscript is good.	The proposed work is built on too-strong assumptions on the data and domains involved. Therefore, it is questionable whether the methodology is applicable in practice.
148-Paper0702	Denoising for Relaxing: Unsupervised Domain Adaptive Fundus Image Segmentation without Source Data	This paper presents a method for unsupervised domain adaptative segmentation via a coarse-to-fine label denoising scheme. With pseudo labeling and uncertainty-rectified label soft self-correction, the model trained on the source domain labeled data can be further finetuned without the access to the source data. Extensive experiments demonstrate its effectiveness.	To address the source free domain adaptation (SFDA) problem, authors present a novel uncertainty-rectified denoising-for-relaxing (U-D4R) framework. Contributions can be summarized into three aspects: C1: Considering the unreliable pseduo labels of target data generated from source model, authors present an adaptive class-dependent threshold strategy as the coarse denoising process to generate the pseudo labels. C2: Authors introduce the uncertainty-rectified label soft correction for fine denoising by taking advantage of estimating the joint distribution matrix between the observed and latent labels. C3: Extensive experiments on cross-domain fundus image segmentation showed that the proposed approach outperforms SOTA SFDA methods and achieves comparable performances with source-dependent methods.	This paper suggests a method for unsupervised domain adaptation for a scenario where  the source data is not available. Pseudo labels are generated on the target domain via an adaptive class-dependent threshold strategy. Then, uncertainty-rectified label soft correction is introduced for fine denoising. They apply their approach on  multi-site retinal fundus images for the optic disc and cup segmentation, and show that they outperform existing models. Various ablation studies are performed.	The authors present an interesting problem to solve for domain adaptation. The paper is well-written and enjoyable to read. Problem is well-formulated and extensive results effectively demonstrate the effectiveness.	S1: The contribution of uncertainty-rectified label soft correction is novel, where author estimate class-conditional label error map through the confident joint matrix and obtain uncertainty map through Monte Carlo dropout. Then, both estimated label error map and uncertainty map are utilized to correct pseudo labels. S2: Although the proposed method aims to solve SFDA problem, it can benefit many self-training based methods to alleviate the noisy pseudo label issues. Moreover, it can also be applied to unsupervised domain adaptation and semi-supervised learning tasks. Hence, the proposed method is of great application value.	Unsupervised domain adaptation in cases where the source domain is not available is of great interest in clinical applications. The motivation of this work is clear and well described in the introduction. Related work is very well embedded throughout the paper. The evaluation of the method and the comparison to other methods is extensive and clear.	The paper builds on the previous UDA methods and the innovation is incremental. The accuracy boosts are also incremental. How clinically significant is the accuracy boost?	W1: This paper lacks related work review in self-training based SFDA methods. Authors only review one self-training based SFDA method [3], and hence the comparison makes the proposed approaches seem to be novel. However, there are many self-training based SFDA methods, such as [1-3]. For example, [*1] also propose an uncertainty-aware self-training method to generate reliable pseudo-labels. It is suggested to compare the proposed method with more self-training based SFDA method, so as to elaborate the contributions of proposed method. [1] Ye M, Zhang J, Ouyang J, et al. Source Data-free Unsupervised Domain Adaptation for Semantic Segmentation[C]//Proceedings of the 29th ACM International Conference on Multimedia. 2021: 2233-2242. [2] Prabhu V, Khare S, Kartik D, et al. S4T: Source-free domain adaptation for semantic segmentation via self-supervised selective self-training[J]. arXiv preprint arXiv:2107.10140, 2021. [*3] Kim Y, Cho D, Han K, et al. Domain adaptation without source data[J]. arXiv preprint arXiv:2007.01524, 2020. W2: As for contribute C1, the novelty of the proposed adaptive class-dependent threshold strategy is limited. It is similar to [*4], and this idea has been widely utilized in self-training based unsupervised domain adaptation methods. [*4] Zou Y, Yu Z, Kumar B V K, et al. Unsupervised domain adaptation for semantic segmentation via class-balanced self-training[C]//Proceedings of the European conference on computer vision (ECCV). 2018: 289-305. W3: As for contribute C2, the definitions of symbols are not clear, making details of the whole process hard to understand. For example, it is not clear what 'the out-of-sample predicted probabilities P_{t}^{hat}'  means, what 'out-of-sample' is and what the shape of P_{t}^{hat} is.	-In section 2.1, not all parameters are described before they are used. For example in equation 6, what does v stand for? Details about the  complexity of the model (e.g., number of model parameters, or training time) are missing. -The split into training and test data on the source domain is unclear to me. Are there 50 images in the training and 51 images in the test set?  -I guess table 1 shows the mean and the standard deviation over all subjects of the test set? If so, this needs to be stated in the caption. The standard deviations are quite high, is this due to the small number of images in the test set? How big is the test set? -No hyperparameters were mentioned. If they are taken from other implementations, please indicate that in the paper. -In Section 2.2 in the last sentence, the mislabeling is only the case for i not equal to j, which is not stated in this section. -For the ablation study, it is not clear exactly what parts are ablated in the cases a) to e). I suggest to refer to the subsections of Section 2 to make clear what part of the pseudo labeling is left out.	The data is from an open challenge and the code will be open-sourced. So the reproducibility is great.	Authors claim that code will be available after review.	The authors state in the abstract that they will make the source code available upon acceptance of the paper. The used datasets are cited and publicly available. The backbone architecture is also cited.	How clinically significant is the accuracy boost?	It is suggested that authors should give specific definition of each utilized symbol. It would be better if that each used symbol is mentioned in the whole framework figure. Authors should compare with other label correction methods in unsupervised domain adaptation task or learning with label noise task, such as [*5], to demonstrate the superiority of the proposed label self-correction method. [*5] Zhang P, Zhang B, Zhang T, et al. Prototypical pseudo label denoising and target structure learning for domain adaptive semantic segmentation[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2021: 12414-12424.	"Please address the points listed under ""weaknesses"". In section 3.1, a better description of the split into training and testing data on the source and the target domain would be helpful. Maybe a table could better visualize the different allocations."	Overall a very good paper to read. It's well written, properly formulated and motivated.	I gave the overall score considering the novelty and application of paper.	The paper is well motivated, clearly written and well organized. The tackled problem is important for real-world applications, where the source data is not available for domain adaptation. The evaluation seems to be complete, as a lot of comparing methods and ablation studies are included. However, there are some explanations missing in the formulas, such as not defined variables.
149-Paper0796	Denoising of 3D MR images using a voxel-wise hybrid residual MLP-CNN model to improve small lesion diagnostic confidence	Authors propose a voxel-wise hybrid residual MLP-CNN model to denoise 3D MR images with small lesions. The proposed method shows a good performance (SSIM and PSNR) compared to some state-of-the-art methods. -The diagnosis confidence for small lesions is confirmed by radiologists.	The authors proposed a method for image denoising of 3D (multislice) MRI using an MLP-CNN architecture. The method processed each image patch with MLPs followed by an encoder-decoder CNN with residual connections. Results show that the proposed method outperforms other methods in terms of recovery of small lesions.	Authors propose a MLP-CNN structure for 3D MRI denoising. Compared with some other denoising methods, the proposed method shows superior results by presenting clear small lesions.	Novelty:  Authors present a deep learning method based on a voxel-wise hybrid residual MLP-CNN model Radiologist confirmation: two experienced radiologists confirmed that the proposed method outperforms other methods in terms of recovery of small lesions and overall image denoising quality.	The paper has a very good structure. The method improve image quality while restoring the details of small lesions in the  image.	The paper is well organized and has presented enough figures and tables to support authors' ideas. A reader study is conducted to evaluate super-resolution results and show the possibility of the using the proposed method in real-world applications.	Information about computational time is not presented. Limited comparison to state-of-the-art:  Only PSNR and SSIM mesures are studied. But, it's enough for a conference paper.  A computational time comparison is missing.	The authoer only evaluate the proposed method on data with simulated noise. Albation study of the proposed MLP-CNN architecture is missing. Relatively week baseline. Recent proposed Transformer-based denoisers are not compared, e.g., SwinIR.	The only suggestion is to consider conducting ablation study to show the contribution of residual MLPs and residual convolutional subnetwork parts for final denoising results.	The code is not available at the moment, but enough details to reproduce it are given. The dataset used by authors is publicly available. The used dataset is described and  cited.	The authors used a public dataset with URL provided.	It is hard to be reproduced as there is no detailed description about the proposed method and no code publicly available.	Some suggestions: It would be interesting that authors gave some information about the computational time required by the proposed method. Summarize the research limitations and future research directions. Extend the Conclusion with details on the method performance when compared to other tested techniques (in terms of PSNR improvement and SSIM).	Ablation studies need to be performed to evaluate the contributions from the MLP and the CNN. How does the patch size affect the performance of the model? Why only the central slice and its neighboring slices were extracted? Do the authors also did this for the test data?	This paper proposes a neural network with multiple residual MLPs and a residual convolutional subnetwork for MRI denoising. Compared with some other denoising methods, the proposed method shows superior results by presenting clear small lesions. The paper is well organized and has presented enough figures and tables to support authors' ideas. The only suggestion is to consider conducting ablation study to show the contribution of residual MLPs and residual convolutional subnetwork parts for final denoising results.	The topic of the paper is relevant and interesting to the MICCAI community. It presents an innovative  idea to noise reduction of 3D MR images: deep learning technique based on a voxel-wise hybrid residual MLP-CNN model The results are clearly presented and the conclusions are supported by the results. Radiologist confirmation: the diagnosis confidence of small lesion is confirmed by radiologists.	The proposed method outperforms baseline in terms of both quantitative metrics and recovery of small lesions. However, the authors only experiment with simulated noise and ablation studies are lacking.	Novelty, experimental design, result presentation.
150-Paper0632	DentalPointNet: Landmark Localization on High-Resolution 3D Digital Dental Models	The paper presents an end-to-end deep learning-based method for the localization of dental landmarks. The method follows a coarse-to-fine strategy that generates proposals with RPN and refines them with DLLNet.  The presented method is an extension of DLLNet that first finds the candidate landmarks with RPN, then refines them with DLLNet. The method is evaluated on a dataset containing 77 patients and outperforms the state-of-the-art methods.	This paper proposes a coarse-to-fine framework to automatically localize landmarks on dental surface models. In the coarse stage, it addresses the issue of foreground/background imbalance problem by a balanced focal loss and uses the curvature as a threshold for filter predictions. In the fine stage, a DLLNet is trained to further improve the results. The proposed method achieves promising performance.	This papers presents a deep learning framework to automatically localize landmarks on dental surface models. The main contributions on the paper are the two sub-networks (Region Proposal Network and Bounding Box Refinement Network) proposed to precisely localize 3D landmarks on high-resolution 3D digital dental models. Other relevant points are the comparison of the proposed methods with other published methods using the same dataset and the possibility of applying the method to edentulous patients.	The coarse-to-fine strategy presented in the paper where landmark point detection is handled as an object detection problem with RPN is quite interesting.  The method is end-to-end which allows making better detection. The experiments including variant-1 and variant 2 are useful for showing the effectiveness of the proposed balanced focal loss and curvature-constraint. I think the results of Table 1 and Table 2 should be given with the whole dataset to show the overall performance of both normal and partially edentulous patients. Also, the number of missing teeth in the dataset should be given.	The paper is well-organized. The improvement seems to be promising. The proposed is application-oriented and could be helpful for clinical use.	Introduction of a new loss function to train the region proposal network. Results of the proposed method were compared with two other methods using 5-fold cross validation. The assessment of method on missing tooth/teeth was conducted using separated anatomical regions. Results have shown significant improvement compared to the other compared methods. The number of figures and tables are appropriate; the figures are well illustrative.	The novelty of the paper is limited.  Although there is an RPN stage and a new loss function, the contribution of improvements of the DLLNet is not clear. In the paper, DLLNet is criticized for not performing well on meshes with missing teeth and being sensitive to the segmentation errors. More experiments are needed for this claim. Is the RPN  only needed to eliminate edentolous patients or has it more advantages. There is also reproducibility concerns about the work. The method is claimed to work well on patients with partial edentulous. However, the experiments are limited because there are only 15 patients with partial edentulous.	"Major weakness: Limited novelty. In section 3.2, the authors indicate the main differences are balanced focal loss and the curvature constraints to prior work.  However, the hyper-parameter in focal loss is not given, i.e., alpha and gamma for both the proposed method and PointRCNN. The proper tuning of these hyper-parameters for the use case of this paper may improve the reported baselines, although I believe the proposed method could still outperform the improved baselines. Besides,  I also suggest giving a complete form of focal loss in the paper. Second, it is unclear how the curvatures are computed and how the threshold (0.65) is defined. Without curvatures (variant-1), the proposed method degenerates to a similar performance as DLLNet in table 1 but is better for edentulous patients. Please consider clarifying this point. Missing results on only coarse stage. The proposed method is further refined by training using DLLNet after coarse stage training. What is the performance of only the coarse stage? Is it similar to PointRCNN? (To me, the coarse stage is PointRCNN + curvature. Correct me if I am wrong.) The training time seems to be doubled or more (PointRCNN training can be time-consuming) than only training DLLNet. Will DLLNet get improved by longer training? Minor issues: It should be ""Adam"" optimizer, right? ""ADMA optimizer"" appears twice. At the beginning of page 5, the shapes of feature matrices should be modified to be consistent. For example, F^4_s should be R^{1x256}, instead of 256x1. It confused me at the first glance."	The proposed method uses parts (same parameter values and sentences) from the DLLNet method published in the MICCAI-2021. The choice of some parameter values used in the paper should be detailed. Tables 1 and 2: Apparently the FP and FP rates are computed for all five assessed regions - it would be more appropriate to present these numbers for each region.	The deep learning model is not directly given and will not be published after acceptance. The input size of the data and some details are missing for reproducibility.	No special issue here.	Based on the information provided in the methodology section, I believe the proposed method could be implemented. However, since the data is not publicly available - it is not possible to reproduce the results presented in the paper.	"A discussion on the improvement of DLLNet to DentalPointNet can be given. Also, demographic information about the patients (age, sex, etc.) and the ratio of missing teeth should be given. The robustness of the method should be discussed because the FP in Table 1 is greater than in Table 2. The term  ""DentalPointNet"" should be given in the abstract. The results before/after data augmentation can be useful to show the effectiveness of data augmentation. Is curvature constraint (>0.65 threshold) held for all landmarks? In Table 1, bold parts are inaccurate for FP and FN."	See weakness.	"** Abstract please remove the RPN abbreviation from the abstract since it has not been used further. ** Introduction page 3 - first sentence: ""..., which significantly reduces false negatives on ...""; This sentence should be modified - first, ""significantly reduces"" is too vague and second, this reduction is compared with what? ** Methods There are a few parameters that were set without explanation - selected 128 proposals, thresholds (0.65, 0.85), lambdas. Landmark annotations from Fig.3 should be included in Figs. 4 and 5. page 6 - first line of section 3.2 - DLLNet reference is number 10, not 9. ** Results Table 1 - PointConv and DLLNet have shown 0% of FP and FN? Is this correct? ** Conclusions Instead of ""significantly outperforms competing state-of-the-art methods"" please provide numbers or percentages to illustrate the performance. ** References Do not use ""et al."" in the reference list. All authors should be included."	The novelty of the paper is limited, however, it addresses partially edentulous patients which is not studied before. It is an improvement on previous work and the improvements are not sufficient for a strong acceptance.	See strength.	The manuscript is well written; the methodology is well presented and provides all required details to allow the implementation of the method. The number of Figures and Tables are appropriate. The contribution of the proposed method is clearly communicated and the results have shown significant improvement compared to the other methods.
151-Paper0408	DeSD: Self-Supervised Learning with Deep Self-Distillation for 3D Medical Image Segmentation	This paper proposes a non-contrastive self-supervised learning method and validates it on different 3D datasets.	This paper proposes a new design for self-supervised learning (SSL) and shows its effectiveness in medical image segmentation. The proposed SSL method is based on DINO (a well-established SSL approach), where the loss is only computed at the last layer. This paper shows that it is beneficial to have supervisions (self-supervised) at intermediate layers.	This manuscript introduced a new self-supervised learning method, referred to as DeSD, by introducing deep supervision into single self-distillation. DeSD was pretrained on the DeepLesion dataset and evaluated on 7 segmentation tasks from 3 datasets. DeSD showed higher or comparable performance compared with 3 SSL methods, including SimSiam, BYOL, and DINO.	The idea of cross-scale comparison sounds interesting.	Simple idea. The core idea is to introduce deep supervision to the DINO approach. This idea is simple and easy to implement. This makes the message from this paper clear and facilitates the adaptation of this idea. Good experiments. The paper conducted a series of experiments to demonstrate its effectiveness. I particularly like the comparison with two variants of DeSD (FC-DeSD and para-DeSD) which shows that the authors have some critical thinking in introducing deep supervision to DINO.	This manuscript is well-written and easy to follow. DeSD showed promising results compared with 3 SSL methods. The ablation study of different deep self-distillation variants is interesting and insightful.	There are several weaknesses: Since self-supervised pre-training has been widely adopted in 3D medical images [1-3],  it is somewhat strange that the authors directly omitted the comparison with these methods in the experiment section. Meanwhile, the authors provided no literature review on the development of self-supervised learning in medical images. From my perspective, it seems that the authors just tried to borrow existing self-supervised methods, which are originally designed for natural images. The authors fail to clarify whether they have used a 3D backbone and how to build a 3D segmentation network in details, which matter a lot in medical image segmentation. In fact, it is confusing that the authors used a 2D ResNet-50 (the authors cited kaiming's paper) and applied 1x1x1 convolution on top of it. So, do you employ a 2D or 3D backbone in practice? Lots of implementation details of baselines are missing. Moreover, they provided no visualization results of most baselines in the supplementary material. In my opinion, I'm not sure about whether the experimental comparisons are fair. [1] Taleb et al. 3D Self-Supervised Methods for Medical Imaging. NeurIPS 2020. [2] Zhou et al. Preservational Learning Improves Self-supervised Medical Image Models by Reconstructing Diverse Contexts. ICCV 2021. [3] Zhou et al. Models Genesis. Medical Image Analysis 2020.	Limited novelty. The major issue with this paper is its novelty. The deep supervision idea is incremental to DINO, and the other parts are almost the same as the original DINO paper. Meanwhile, deep supervision itself is not a novel design. It has long been introduced to train deep neural networks [1]. Unclear practical use of the proposed method. Although this paper demonstrates the effectiveness of introducing deep supervision to SSL, it is unclear how this idea may be used to address medical image segmentation in practice. In particular, the current mainstream segmentation approaches are U-Net or transformer-based, while the segmentation model used in this paper is simply a ResNet followed by several decoder layers. The paper does not show 1) how the proposed SSL method may help the mainstream approaches and/or 2) how the segmentation model in this paper may compete with the mainstream approaches. [1] Lee, C. Y., Xie, S., Gallagher, P., Zhang, Z., & Tu, Z. (2015, February). Deeply-supervised nets. In Artificial intelligence and statistics (pp. 562-570). PMLR.	The novelty of the proposed method is limited. The main framework is based on DINO [1]. And the idea of deep self-distillation is identical to Zhang et al. [2]. [1] Caron, M., Touvron, H., Misra, I., Jegou, H., Mairal, J., Bojanowski, P. and Joulin, A., 2021. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 9650-9660). [2] Zhang, L., Song, J., Gao, A., Chen, J., Bao, C. and Ma, K., 2019. Be your own teacher: Improve the performance of convolutional neural networks via self distillation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 3713-3722).	Some important details are missing, such as how to build a 3D segmentation network based on ResNet-50.	The datasets are publicly available. SSL part is easy to reproduce since it is based on DINO. The paper does not show details on how the four sub-encoders are constructed (but it may be inferred since the encoder is ResNet-50). The downstream segmentation model cannot be easily reproduced. There are some descriptions of how to construct the decoder, but it requires more details to fully reproduce.	This paper has moderate reproducibility. The code is not released, but most training details (e.g., architecture, optimizor, learning rate, batch size, and augmentations) are provided in this manuscript.	Please address my concerns in the weakness part.	Can you show the effectiveness of DeSD on other tasks such as classification? Can you include comparison to other segmentation approaches?	Please refer to my comments in Section 5. The baseline methods compared in this manuscript, which is somewhat limited. All three methods are SSL developed in natural images. I would also suggest the authors include more methods developed for medical images. Particularly, a. SOTA segmentation method developed for medical imaging analysis (e.g., nnU-Net [3]); b. SOTA SSL method developed for medical imaging analysis (e.g., Models Genesis [4,5], Rubik's cube+ [6], or other SSL methods evaluated in [7]) I would suggest the authors release the code and pretrained models to increase reproducibility. [3] Isensee, F., Jaeger, P.F., Kohl, S.A., Petersen, J. and Maier-Hein, K.H., 2021. nnU-Net: a self-configuring method for deep learning-based biomedical image segmentation. Nature methods, 18(2), pp.203-211. [4] Zhou, Z., Sodha, V., Rahman Siddiquee, M.M., Feng, R., Tajbakhsh, N., Gotway, M.B. and Liang, J., 2019, October. Models genesis: Generic autodidactic models for 3d medical image analysis. In International conference on medical image computing and computer-assisted intervention (pp. 384-393). Springer, Cham. [5] Zhou, Z., Sodha, V., Pang, J., Gotway, M.B. and Liang, J., 2021. Models genesis. Medical image analysis, 67, p.101840. [6] Zhu, J., Li, Y., Hu, Y., Ma, K., Zhou, S.K. and Zheng, Y., 2020. Rubik's cube+: A self-supervised feature learning framework for 3d medical image analysis. Medical image analysis, 64, p.101746. [7] Taleb, A., Loetzsch, W., Danz, N., Severin, J., Gaertner, T., Bergner, B. and Lippert, C., 2020. 3d self-supervised methods for medical imaging. Advances in Neural Information Processing Systems, 33, pp.18158-18172.	The authors omitted previous studies on applying self-supervised learning to 3D medical images, and I believe such activity should not be encouraged. Also, important implementation details are missing, which makes the reproducibility questionable.	I find it is hard to position this paper and hence acknowledge its contribution. This paper can be regarded as both an SSL paper and a segmentation paper. If we consider it as an SSL paper, then the current experiment is not enough to show the true SSL effectiveness. The mainstream SSL papers are all evaluated with image classification and standard benchmark datasets such as ImageNet. This paper does not conduct experiments with these settings, and hence it is unclear if it is really better than the mainstream approaches. If we consider it as a segmentation paper, then the current experiment does not show the true segmentation effectiveness. The segmentation model used in this paper is a primitive one. It is unclear if the benefits demonstrated on that model can be readily transferred to more advanced segmentation models.	The proposed method showed promising results. However, the novelty of the proposed method is kind of limited.
152-Paper1127	DeStripe: A Self2Self Spatio-Spectral Graph Neural Network with Unfolded Hessian for Stripe Artifact Removal in Light-sheet Microscopy	The proposed method removes stripe artifacts from LSFM images. It leverages domain specific information including fourier domain behaviors, and uses a graph NN to fill in suspected corrupted datapoints in the fourier domain. It combines this with an optimization in the spatial domain to encourage spatial continuity.	This paper tackles the problem of removing stripe artifacts in light-sheet fluorescence microscope images. The method models stripes as a combination of a  graph neural network on the Fourier coefficients connected in a polar coordinate structure, a spatial domain Hessian based regularization scheme. and an energy minimization scheme to seek the optimal noise reduced image. The method is analyzed on a single noise-free image-volume with repeatedly artificially added stripe noise and compared with several existing algorithms. The results are assessed wrt. peak signal to noise ratio and structure similarity measure and performs better than the existing methods. The method is also applied to a real image with favorable result.	This paper contributes a new method to remove stripe artifacts from light-sheet microscopy data. The approach is based on self-supervised learning with a graph neural network and a Hessian regularization and thus, combines both frequency and spatial domain features.	The motivation (problem to be solved and why) is compelling. The blending of optimization in both spatial and spectral domains is ingenious, and leverages the specifics of the use-case. The proposed method appears to give better results than the baselines used (though there is little visible difference in the output images). Uncertainty measures are provided.	The paper proposes a novel extension of existing works by applying a graph neural network as a model in Fourier space together with the Bregman algorithm to optimize an energy formulation. The paper is well written, and it is compared with a wide range of existing techniques. The method appears to perform as state-of-the-art	The paper introduces a novel method to solve the stripe artifact removal which is an important problem in light-sheet microscopy. The results are very convincing.	Some of the exposition was insufficiently clear (examples given in Detailed Comments). I was unclear why a GNN was desireable, and why a more straight-forward method might not work (see Detailed Comments).	The model is very complex, has several layers, and it is unclear how many parameters, the method has, what the computation resources need to run the method, and the method has only been compared against the state-of-the-art on synthetic images. Since SSIM is linked to human perception, which seems a less obvious choice in this case, and it is unclear how the PSNR between two images has been calculated, and thus the results in Table 1, can be difficult to interpret in light of the noise removal task.	I don't see many weaknesses. If I have to name one, I would say the paper could benefit from a few more explanations to lead the reader to better understand the technical details of the methods.	There is no mention in the main text about code being available.	My mental test is always, whether I believe I would be able to make a student reproduce the results, and in this case, the answer is Yes.	The authors state that the code will be made available for biologists for academic use. Can you clarify how restrictive this is? What about scientists from other fields? Maybe you could comment on why you decide against open-sourcing the code.	"Thank you for providing uncertainty measures - highly useful. p-values are not so useful. See Wasserstein, ASA statement on P-values 2016. ""within a wedge in the Fourier space"": maybe clarify here that stripes are always perpendicular to the edge in LFSM images (not diagonals), assuming this is the case. Fig 4 does not show any difference between filling the wedge and DeStripe - would a differently-zoomed image show a difference? (2.1)  ""survival function"": what is this? ""Rayleigh distribution"": why Rayleigh, and what is its relevance here? ""this concentric annulus"": how thin or thick? Does it matter? Fig 1e: It looks like all coefficients in the wedge are labeled as corrupted. Is this typical, or would the actual mask be more pixelated, with clean and corrupt labels intermingled? Perhaps clarify that at the end of 2.1 where the wedge mask is discussed. If pixelated, perhaps modify the figure to make this clear. (2.3) This section leaves me wondering why a more straightforward method, such as filling in the fourier coefficients with the MAP based on the annulus distribution, would not work. Can you explain why the complex GNN system proposed is necessary? ""also corresponding frequencies into account"": what does ""taking into account"" mean concretely? (2.4) How are lambda_x, etc determined? Presumably they are image-specific, based on the number of edges and steps in the image. (2.5) First term of eqn 7 (ie |Y-X|): Why do you strive to match corrupted sections of Y (the stripes), if these are what you are trying to remove, ie why is the first term used at all? Fig 1, 2,: Sideways lettering is basically impossible to read. If it is important, perhaps the figure can be rearranged so that the lettering is horizontal. Miscellaneous: ""prior one"": what does this mean? Typos/grammar: In general, an additional thorough proofing would improve the text. -holds true -> hold true barely exits -> exists. Also the grammar of the sentence wants correction -LSFM once excites -> leave out 'once' ""B is the Bregman"" -> V (I think; there is no B in eqn 2) -(in 2.5) are X and X_tilda the same thing?"	See weaknesses for useful improvements.	"p.4. ""which fall within the same thin concentric annulus A^r_k"" Maybe you could annotate this annulus (or an example) in Figure 1 to guide the reader. p.4. after equations 2a - c) you mentioned ""B is the Bergman variable"" but there is no ""B"" in the formula. I suppose this is a typo p.5. "" Note that we use complex-valued building blocks [20] for W1 and W2"". Just out of curiosity: What framework did you use? I remember that there were some issues with PyTorch complex numbers implementations. But maybe this has been solved by now. p.5 ""s a result, by explicitly modeling the sample-only spectral response as a weighted combination of its uncorrupted neighbors on a polar coordinate, stripe-only Fourier projection is exclusively re-served as activation M  H(L+1), which can then be subtracted from the input stripe-sample mixture Y for striping removal."" I think I get the point, but I find this hard to understand when reading it. Maybe this sentence should be rewritten. p.6 ""shrink(*) is the scalar shrinkage operator"" Maybe a naive question: I am not sure how standard this operator is but maybe you could add a reference here just in case. p.6 ""Eq. (7) is to prevent the model from learning an identical mapping by quantifying isotropic properties of recovered X, where Pk is the corrupted subset of Ark indicated by corruption mask M, and Qrk = {x  |x   Ark , x  / Prk }."" I think here a short explanation might be nice to help the reader understanding the general idea here."	The method is interesting, creative, and well-motivated by the problem statement. The results appear to be strong.  The writing (clarity, grammar) is somewhat weak, so the content is harder to absorb than would be ideal.  One piece of the architecture (the GNN) is not well-motivated. The paper's rank in my stack could have been higher; I had to pick an order. The paper rating could have been higher; I had to pick a number.	Although modelling the stripes in the Fourier domain is not novel, e.g., [8] in the paper's list of references, the gnn, optimization, and the comparison with existing methods makes it an acceptable paper.	I think it is a strong and interesting method, the results are very convincing.
153-Paper1579	Detecting Aortic Valve Pathology from the 3-Chamber Cine Cardiac MRI View	The authors describe a multi-stage machine learning approach to detect pathologies of the aortic valve from Cine MRI imageing sequences. This is intended to support medical staff in determining which other types of imaging are required for the specific patient, potentially improving the imaging duration and quality.	Overall paper describes an approach for aortic valve disease classification based on cardiac MR. Using deep learning based heat map regression, features are derived from anatomical landmarks & contours (aortic hinges & leaflets) and pathophysiological dynamics (stenotic and regurgitant jets). Based on these features random forests are trained to obtain an estimate of whether or not aortic valve disease is present.	This works presents a multi-stage deep/machine learning strategy for aortic valve abnormalities classification from a single cine CMR view (3-chamber). The authors propose to regress heatmaps with the location of the aortic valve hinge points, followed by regression of curves representing the valve leaflets and pathological blood jets. By applying a ridge detection method on these curves, several handcrafted features are extracted per frame, later aggregated in a per-video feature set, and use for classification using a random forest (RF) classifier. The approach was validated in data from 3 centers, with interesting results.	The presented method is very interesting and relies on a simple, yet obviously powerful, machine learning approach. The authors describe a multi-step network to first detect features of potential aortic valve pathologies and then to classify them. The paper is well written, easy to understand and the methodology is explained properly. I would specifically like to point out the importance of explainability in AI, which the authors have addressed with their multi-level approach.	Disease classification and the combination of machine learning and explainability are generally interesting to the MICCAI community. The evaluation is done on a reasonably sized training and testing sets.	Novelty: the proposed approach presents novel aspects, both in terms of application (first attempt to classify aortic valve abnormalities from a single standard CMR view) and methodology (an ingenious multi-stage hybrid [deep/machine learning] strategy that mimics clinicians reasoning and provides an explainable framework). Explainability: a key aspect for clinical practice integration, which is here accounted for with a tractable machine learning classifier (RF). Applicability: the proposed curve-based heatmap regression (followed by a ridge detection method) seems generic and sufficiently interesting to a panoply of clinical applications.	I do not see any major weaknesses.	"Many details are not actually clear from the manuscript and the presentation is rather confusing at times: 1) What does curve tracking mean and how is this implemented? 2) What quantitative criteria do you use to obtain ""probability of being a pathological curve"" 3) How are features (angles, length, distance to image patch center, probability) computed from the heat maps? 4) ""coronary cusp leaflet"" is a confusing formulation. Does this model only one aortic leaflet? Does it model contours or a landmark (located on the free edge of the leaflet)? 5) The number of frames to be used also depends on cardiac phases, as aortic stenosis is only apparent during systole, and regurgitation only during diastole, which are typically of different duration. Also patients may rarely suffer from both stenosis and regurgitation, more frequently from only one of these conditions. How is this handled?"	Lack of methodological/implementation details: certain modules/aspects of the proposed methodology lack sufficient details to be fully understood and potentially reproduced. These include details regarding heatmap creation, curve tracking algorithm, proposed network, feature extraction, etc. (see specific comments below). Limited results supporting algorithmic decisions: does replacing the conv block with dense blocks effectively improved heatmap regression? Does a three-stage heatmap regression (Fig. 3) outperforms a single- or dual-stage network?	The paper seems to be relatively straight-forward to reproduce. It would be great if the dataset could be made available to the public, since significant effort has to be taken to annotate the data as was done here in this study.	Mainly the open points on the description should be clarified, as right now I would not be able to implement exactly the same as many details are not really described. I realize this may be tricky given the size limit of the paper, maybe it would be possible to replace some text with a more descriptive figure.	"The authors employ a novel multi-center dataset comprised of 1017 patients. However, limited description is given regarding image acquisition parameters, instructions given to annotators (e.g., which rules were used to define a pathological jet and its extent?), methods employed for quality control (any type of consensus?), etc. Moreover, it is not understood if the dataset used for training heatmap regression (80 patients) is a subset of the main dataset (1017 patients) or an independent one. If the former, how was that dealt with when evaluating the accuracy of the classifier? Several methodological/implementation details are lacking, hampering the reproduction of the authors' method/results. See ""Weaknesses"" and associated specific comments below."	I have no major points of criticism. Minor issues: Please add units to Table 1. Also, it would be nice to give the resolution of the Cine MRI sequences to relate pixels to mm. Did you perform an ablation study? It would be interesting to see in which cases classification failed. Could you give an example image, maybe?	"6) What is the sensitivity w.r.t. hinge detection? Practically this is a surrogate for the distance to hinges (since crop window is computed based on these), no? 7) Please provide the image resolution  and relate of the error in mm to the overall size of anatomy and subcomponents. 8) The sentence in Fig 4 ""The green curve shows the predicted an AV-regurgitation curve."" seems incomplete - please correct this, message not clear. 9) The processing is based on 2D slices only where as jets may have 3D shape, please comment on potential sensitivity w.r.t. imaging limitations and variations in acquisition"	"In addition to the comments raised above, some comments follow: The term ""curve"" may be difficult to understand when first reading the manuscript, especially in the abstract. To improve comprehension, consider replacing the term in the abstract (referring to blood jet or similar) and then properly define it in the main text. Please provide further detail regarding your proposed networks. Are the number of levels and filters per level equal to the original U-net? What is the initial localization network's input size? How are the curve-based heatmaps created? Perhaps a Gaussian convolved with a binary mask of the ground-truth curves? Since the heatmap can present insignificantly low values, was a minimum threshold value used to ""stop"" the curve tracking (i.e. ridge detection) algorithm? How is the orientation of the curve defined? A vector connecting the initial and end point (as suggested by Fig. 4), or perhaps a line fitted to all curve points? How is the ""probability of being a pathological curve"" defined? From the text after eq. (2), it seems to be the estimated mean probability of the regressed map over the curve, rather than a true ""probability"" of being pathological (vs. healthy). No reference to ""random forest"" is given in the ""Method"" section. Please add it there (including certain implementation details found in section 3). Was a patient-disjoint split use to separate annotated frames into training, validation, and test sets? Consider decreasing the length of the abstract (the first sentences are unessential). Given the variable pixel spacing of CMR images (and potential resampling used during your pipeline), consider reporting the localization accuracy in mm. Please correct English grammar/spelling mistakes/typos. A few examples follow: p.1: where it reads ""leaflet hinge points"" should read ""leaflets' hinge points"". p.2: remove ""of"" from ""assess of blood flow"". p.5: remove ""an"" from ""predicted an AV-regurgitant curve"" in Fig. 4 caption. p.7: where it reads ""utlises"" should read ""utilises""."	The paper tackles an important medical imaging topic, the topic is addressed in a convincing way. The algorithmic details are adequate and the paper is well-written and explains everything in detail.	Overall quite interesting, nonetheless very confusing on many parts, overall the lacks clarity, which could be solved with a better presentation (e.g. using graphical rather than textual description)	I found the manuscript well written and methodologically sound. Despite simple, the proposed pipeline is ingenious (with certain modules potentially applicable to other clinical tasks), provides interpretable results and shown a good overall performance. Despite the few weaknesses described above, these seem feasible to be corrected in the rebuttal phase (lack of methodological details limiting the reproducibility, limited discussion on the results, etc.), which could result in an interesting proceeding paper.
154-Paper1426	DEUE: Delta Ensemble Uncertainty Estimation for a More Robust Estimation of Ejection Fraction	This work makes several interesting contributions in form of an epistemic uncertainty estimation method for deep learning regression tasks applied to cardiac echo,  including agnostic to neural  network architectures, real-time and deterministic; memory-efficient, scalable and computationally fast for large models, and provides high-quality uncertainty estimation.	The authors present a method for estimating the uncertainty in a Deep Learning estimate of a real valued prediction. The method computes the empirical variance of the parameters over 5 trained versions of the same network model. The authors explain how such an estimate can be used as an approximation to the network covariance which would be used as part of a Taylor expansion of the expected squared predictive error. The approach is applied to the estimation of ejection fraction from apical 4-chamber ultrasound views from a publicly available database.	In this paper, the author mainly proposes an epistemic uncertainty estimation method for Deep regression networks in the context of EF learning. Compared to other methods, this method only requires one forward-pass at the inference time.	well written and motivated novel use (as far as I can tell) of Delta Method with model ensembles for uncertainty estimation use of public dataset EchoNet-Dynamic comparison of ResNet-18 and its use for Ensemble of Methods (EM)  and Deep Bayesian Neural Networks, and a single deterministic deep regression model (DDR)	The benefits of this approach over prior work are relative computation tractability especially at inference time since only one pass of inference is required and there is no special architecture required of the Deep Learning network. The exposition is very clear. The approach is one others could easily adopt and evaluate.	The main strength of this paper is that it proposes a memory/time-efficient estimation of epistemic uncertainty with a single feed-forward pass through the network.  The method was based on the Taylor's expansion of the expected squared error. The model parameter and the covariance matrix were determined by multiple feed forward propagation. Then during test time, on one forward computation is required to obtain the uncertainty. The method can identify samples with poor quality.	no novel DL model per se, but method to be used on top of trained models mainly a benchmarking paper where it is not clear what the alternative uncertainty estimation adds and how would be helpful for in diagnostics, ie a further downstream task is missing	My main criticism is that it is unrealistic to expect the computation of variance over 5 samples to be a good estimate of the actual variance of the network parameters even when the expected variance of the parameters is low (reference [4] in the paper). Nevertheless, the authors are able to show that the estimates are in line with other approaches to estimating the variance. These other methods also have issues with sampling but it is encouraging that the results across methods seem consistent. Also, the few examples of data producing high and low uncertainty estimates are very reasonable. But I would like to see more trained networks be used for sampling the parameter variance despite the computational challenge. The authors assume that the network model weights are independent. But I would be curious to see how large some of the non-diagonal terms of the covariance matrix are. Is the independence assumption reasonable for their network? This is a minor issue for the publications of this conference paper but this is something I would expect to see in a journal paper.	The definition of epistemic uncertainty in Eq. 2 and 7 needs further clarify. Experiments of this paper is inadequate.  See detailed comments below.	"Everything is ticked ""yes"" but I don't see own code or results being made available? Otherwise: Detailed model/parameter description use of state-of-art methods for comparison public dataset"	The reproducibility of this paper is very high. The data is publicly available and the implementation is straighforward.	relatively easy to reproduce. Not all details provided.	This is interesting work which will add to existing uncertainty estimation methods, while providing a fast, scalable, single forward-pass solution, so should be of some interest. The application of regressing ejection fraction for cardiac echo is an important one, and the method is not limited to this - moreover it can run on pretrained models.  However the conclusions are nor clear to me as the compared methods all seem to be in the same ballpark (Figure 3 - predicted RMS vs % of eliminated data, all showing similar quality of levels of uncertainty (apart from DDR) and a similar test error and need for calibration, so it is not clear what extra value DEUE is then bringing in in clinical practice.	The references are good. There is so much work in uncertainty estimation that not all references could possibly be given but the paper has a reasonable selection.	The definition of epistemic uncertainty in Eq. 2 and 7. From the definition in the paper, the epistemic uncertainty in equation 2 is the expected squared error. However, the prediction error and the epistemic uncertainty can not be viewed equivalent to each other.  A prediction can be of high error and low uncertainty in the meantime, or low error with high uncertainty. Please clarify. Ignorance of the prediction error \epsilon_x in Equation 7. While W* is the optimal model parameter, when the network converges, W will be very close to W, which makes the (W-W) equal to 0. Then U_L(x) will be equal to prediction error \epsilon_x Is there any difference between U_L(x) and E(\epsilon_x^2). During training stage, the network is initialized from different points, thus leading to multiple local optimal estimation of W*. How to obtain these different initializations? I am wondering the results of Eq. 7 maybe very high if they are extremely far from each other. Besides, how about computing the diagonal entries of the covariance matrix \Sigma directly from these initializations? Will this be similar to or quite different from the diagonal entries of \Sigma_M that computed from the converged Ws? Experiments of this paper is inadequate. From figure 3, the proposed method shows inferior performance compared to its competitors. And there are no quantitative results to demonstration the advantage of the proposed method. Table 3 is not convincing without any quantitative results of the memory and running time. It will be helpful to test the real memory cost and time cost during inference.	Interesting method for uncertainty estimation in trained-up models, that could be of interest to wider applications.	Although a great deal of work has appeared in the last couple years on uncertainty, this is a new approach that other researches could easily evaluate on useful problems that require a fast run-time and limited computational resources during inference.	The task is interesting and the method of uncertainty estimation is efficient in time and memory.
155-Paper0164	DGMIL: Distribution Guided Multiple Instance Learning for Whole Slide Image Classification	This paper proposed a feature distribution guided deep MIL framework for WSI classification and positive patch localization. Specifically, The authors proposed a cluster-conditioned feature distribution modeling method and a pseudo label-based iterative feature space refinement strategy. This framework achieves a new SOTA on the CAMELYON16 dataset.	This paper focuses on improving the learning of feature extractor for multiple instance learning for WSI classification. The authors initialize the feature extractor via self-supervised learning (i.e., MAE) and perform clustering, instance selection, and refinement iteratively. Validations and ablation studies on Camelyon16 show its effectiveness.	This paper proposes a distribution guided multiple instance learning framework for whole slide image classification. The proposed method refines the instance representation in the latent space, by using a cluster-conditioned feature distribution modeling method and a pseudo label-based iterative feature space refinement strategy. It outperformance some state-of-the-art methods on CAMELYON16 dataset.	This paper models the multi-instance problem from the perspective of the data distribution of pathological slides, which doesn't need to design complex discriminative networks. This paper is well-written and logically clear. This paper achieves a new SOTA on the CAMELYON16 dataset, verifying the effectiveness and superiority of the method.	The paper is well-organized and easy to follow. The motivation and method are clearly demonstrated. The experiment is basically complete. The proposed method achieves performance gain on Camelyon16.	+The proposed cluster-conditioned feature distribution modeling method and a pseudo label-based iterative feature space refinement strategy is interesting and can be beneficial to the community on better utilizing the MIL for medical images. The paper is well organized and easy to follow. The performance improvement against the state-of-the-art is significance on CAMELYON16 dataset.	This paper only uses AUC for evaluation on one dataset (CAMELYON16) and does not visualize slides with high positive scores.	Lack of explanation for some settings: e.g., the reason for using Mahalanobis distance instead of others like cosine similarity, the reason for using feature after projection head instead of backbone, and the definition of refinement convergence, etc. The reported results in table.1(a) are significantly lower than that in DSMIL, e.g., the Slide AUC of Ab-MIL is 0.6612 in this paper but is 0.8653 in DSMIL. I notice that the settings of patch size (512 vs. 224) and magnifications (5x vs. 20x) are different. It would be more convincing to run the proposed method in the same settings as DSMIL instead of reproducing other works to avoid unfair comparison.	-About the proposed method. Rigid MIL formulation requires the permutation invariance of the MIL aggregation function (e.g., from the instance space to the bag space). However, the authors do not discuss this, and whether or not the cluster-conditioned feature distribution modeling method and a pseudo label-based iterative feature space refinement strategy can grantee the permutational invariance is unclear in the current form of the manuscript. -The authors missed multiple latest works of deep MIL and its application in the medical image. For example, Deep MIL: [1] A multiple-instance densely-connected ConvNet for aerial scene classification. IEEE Transaction on Image Processing 29, 4911-4926 (2020) Deep MIL & its application in medical image: [2] Multi-instance multi-scale CNN for medical image classification. International Conference on Medical Image Computing and Computer Assisted Intervention, (2019) [3] Local-global dual perception based deep multiple instance learning for retinal disease classification. International Conference on Medical Image Computing and Computer Assisted Intervention, (2021) The authors are suggested to enrich the related work, and if necessary make more comparison. -More validation on additional dataset is highly preferred to fully demonstrate the effectiveness of the proposed method.	The authors have clarified that the code will be available in the reproducibility checklist.	Seems good as almost all [YES] are chosen.	The dataset is publicly available. The proposed method is clear enough for coding and realization. However, I don't know if it can be confirmed for reproducibility.	This paper analyzes the binary classification problem as an example. For a multi-classification problem, how to choose negative slides to cluster at the beginning of each iteration? The author divided each WSI into 512x512 patches without overlapping under 5x magnification. Is the performance of DGMIL affected at other magnifications? Does the clustering step take up a lot of training time if the patch is split under high magnification? Why are the AUC results in Table 1(a) different from those published in the DSMIL article? How to determine if a patch is positive or negative when calculating the metric of patch AUC? what is the threshold of positive score for determining a patch is positive or negative?	"More explanation of some settings:  (a) It would be more convincing to either illustrate the advantage of using Mahalanobis distance or show the proposed method is agnostic to the choice of distance. (b) Authors are encouraged to conduct the experiment of using the feature of backbone for refinement. (c) It would help to detail the refinement training setting, e.g., epochs, learning rate, etc, especially the definition of refinement convergence. More explanation of the performance gap: the reported results in table.1(a) are significantly lower than that in DSMIL, e.g., the Slide AUC of Ab-MIL is 0.6612 in this paper but is 0.8653 in DSMIL. Authors use different settings of patch size (512 vs. 224) and magnifications (5x vs. 20x). It would be more convincing to run the proposed method in the same settings as DSMIL instead of reproducing other works to avoid unfair comparison. As is figured out in introduction, many key-instance based methods may give wrong pseudo labels or select insufficient instances. The authors could further validate that the propose method solve this problem by more detailed ablation study or visualization. Reference mistake: (a) Mistakes:The reference of Loss-based-MIL and CHiknotew-MIL are mistaken. (b) Missing discussion of [1,2]. [1] Xu, Yan, et al. ""Multiple clustered instance learning for histopathology cancer image classification, segmentation and clustering."" CVPR, 2012. [2]Sharma, Yash, et al. ""Cluster-to-conquer: A framework for end-to-end multi-instance learning for whole slide image classification."" Medical Imaging with Deep Learning. PMLR, 2021."	Please refer to the weakness part in Sec.5.  -Justification of the permutation invariance in the latent space is necessary.  -More related works need to be covered, and if necessary need to be compared.  -Validation on more datasets is highly preferred. -Also, some typos need to be corrected before publication. For example, in Eq1, what's 'iff'?	This paper uses multi-instance learning to solve the problem of pathological image classification from the perspective of data distribution, and the perspective is novel. However, this paper only conducts experiments on one dataset and lacks visualization results.	The paper is well-motivated and easy to follow, but it lacks details and explanations. Especially, the reported results are a bit unconvincing as are much lower than that in previous work.	Both strength and weakness are quite obvious and this manuscript is clearly a borderline paper. Currently I am willing to rate a weak accept due to its interesting idea and significance performance gain, which can be beneficial for the community.
156-Paper0682	Did You Get What You Paid For? Rethinking Annotation Cost of Deep Learning Based Computer Aided Detection in Chest Radiographs	A large-scale analysis with 120K chest X-rays is conducted to analyze how annotation cost impacts CAD systems for classification and segmentation. Useful conclusions include: 1. bounding box annotations are as useful as the accurate contours when provided as additional supervision to the classification model. 2. Relatively small improvements to the label extracting algorithms lead to gains in classification performance. 3. We can achieve strong segmentation performance by mixing image-level labels with only small amounts of pixel-level contour labels.	This is quite a good paper that analyze the cost of annotations for two tasks of chest X-ray analysis: classification and segmentation, from three dimensions: annotation granularity, annotation quality, and annotation quantity. The experiments are extensive and convincing with several interesting findings that could inspire further studies on the same field.	This paper investigates the role of quantity vs. quality of annotations for medical image classification. The authors have collected an impressive private dataset of Chest X-rays which is close in size to CheXPert (120K vs. 224K images), but is manually annotated by radiologists. In addition, pixel-level annotations are provided. Specifically, different architectures are trained for chest X-ray classification and segmentation with different ground truths: expert labels (gold standard), expert labels + random noise, expert labels + segmentation, expert labels + bounding boxes. Their key take-away points are that i) noisy labels affect performance especially when training on smaller dataset, ii) providing lesion-level annotations improves performance at all scales and iii) mixing a small number of gold standard annotations with a larger dataset of imprecise annotations can improve.	The findings are useful for the community to improve accuracy with minimum annotation cost. The study is conducted on a large dataset with manual labels and comprehensive results are reported. The quantity, quality, and granularity of annotations are analyzed for both classification and segmentation. I enjoy reading the paper. It is clearly written.	The paper extensively analyze (two tasks, four different kinds of annotations, dozens of different settings) the effects of different annotations for chest X-ray analysis with interesting findings, which is a practical evaluation. All models are run three times with mean and std reported. I quite appreciate this point that makes the results quite convincing.	The paper addresses one of the most important aspects of training deep learning models in the medical domain: how to properly budget for the cost of annotation The paper investigates not only the issues of noisy labels, but also the added benefit of providing lesion-level annotations. Previous studies in literature, such as the mammography Dream challenge, stressed that  training from image-level labels alone results in lower performance. The present paper, however, explores the topic in a more quantitative and systematic fashion The paper is overall well written and the experiments comprehensive	The way to leverage bounding boxes in this paper can be improved. The boxes are directly converted to noisy pixel annotations to train the segmentation head, which is not optimal. There are many weakly-supervised segmentation algorithms that can better utilize the boxes. Thus, the comparision of image-wise labels and box labels in the segmentation task may be debatable.  c.f. Tajbakhsh, N., Jeyaseelan, L., Li, Q., Chiang, J., Wu, Z., & Ding, X. (2020). Embracing Imperfect Datasets: A Review of Deep Learning Solutions for Medical Image Segmentation. Medical Image Analysis. http://arxiv.org/abs/1908.10454 Tang, Y., Cai, J., Yan, K., Huang, L., Xie, G., Xiao, J., Lu, J., Lin, G., & Lu, L. (2021). Weakly-Supervised Universal Lesion Segmentation with Regional Level Set Loss. MICCAI. http://arxiv.org/abs/2105.01218	When comparing the segmentation models, is the ground truth all contours? If that is the case, and if the authors also using the same dice + bce loss during training for all models, then using Bbox can also be regarded as using noisy labels for the segmentation task, as the authors have also mentioned in the paper. I think the comparison in Table 4 is mainly on the impact of quality, not granularity. Some details and discussion could be made to improve the paper.	The main weakness is the noise model: the authors generates error randomly at a given F1 score, whereas it is plausible that the errors in NLP labelers, such as that used in CheXpert, are not randomly distributed across classes. For instance in CheXPert the percentage of uncertain labels is not evenly distributed. The chosen F1 score is also relatively low (0.8), which appears to be low compared to the F1 scores declared in the CheXpert paper Statistical analysis is not conducted to verify whether differences are statistically significant Some parts of the methodology are not clearly described - in particular the annotation process and some aspects of the training methods. Related works are not discussed.	The results are convincing. However, the paper will be more impactful if the authors can release the dataset.	Would be good if the authors release the code as they claimed in the list.	The dataset is unfortunately private, but the methodology is clear and key hyper-parameters are reported. The authors will release the code.	For tables 2~4, the results will be easier to interprete if they are shown in line charts or bar plots.	"Fig.1 contains many symbols, and it's hard to read separately from the main text. It's recommended that the authors add necessary explanations to all the symbols in the figure for better readability. In the Learning Objective, L_{cls} and L_{seg} using the same symbols for prediction and GT. Please distinguish the different output and GT. Please add a brief explanation for L_{cls} in Eq.2. Table 2: why the authors said the models trained with less than 12K training samples in total all performed similarly? There are clear differences between the models' performance as shown in the table under different dataset size. Plus, it's recommended that the authors replace ""Dataset Size"" to ""Training Dataset Size"" in the Table. In ""Improving Performance by Mixing Levels of Granularity"", the authors mainly describe the observations without further explanation. It would be appreciated if more discussion or explanation of the observations are provided. Fig 2: it's recommended that the authors also put results of using pure Bboxes in this plot, and discuss why the performance is the worst. There are few related works to this paper. I would appreciate it if the authors could discuss the differences in findings between the following works and theirs: [a] Zlateski, et al. ""On the importance of label quality for semantic segmentation."" Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition. 2018. [b] Luo, et al. ""Rethinking annotation granularity for overcoming deep shortcut learning: A retrospective study on chest radiographs."" arXiv preprint arXiv:2104.10553 (2021). If there would be an extension of this paper, I would appreciate it to see: (a) a quantitative analysis of the cost of annotations under different scenarios; (b) what's the best strategy when the annotation cost is limited. The authors can also refer to: [b] Ren, Zhongzheng, et al. ""UFO2: A Unified Framework Towards Omni-supervised Object Detection."" European Conference on Computer Vision. Springer, Cham, 2020."	In this paper, the authors investigate the role played by granularity, quantity and quality of the annotations and conduct a series of experiments to identify the best trade-off between accuracy and reducing the annotation costs. A common trend in literature is to exploit NLP to automatically extract labels at scale: this strategy is current exploited by most state-of-the-art public datasets in chest x-ray classification (CheXpert, Chest X-ray14). It is cost-effective but inevitably leads to a certain level of noise, despite advances in the NLP labelers.  The results are, to the best of my knowledge, in qualitative agreement with previous literature in the medical and general CV literature.  The most interesting contribution, in my opinion, is how to mix image-level and pixel-level annotations for both segmentation and classification, proving that lesion-level annotations can substantially improve performance regardless of the training set size (at least within the range considered in the paper, from 1.2K to 121K) The impact of noise is, in my view, less relevant, since the noise model considered may be too simple. The practical importance of the findings greatly improved if the noise model was backed up by an analysis of the actual errors made by an NLP such as the one used in CheXPert (https://github.com/stanfordmlgroup/chexpert-labeler). Non-random, structured  or feature-dependent noise is likely to have a far greater impact than postulated (https://arxiv.org/pdf/2003.10471.pdf).  Previous studies postulate that, in CheXpert, the 'No finding' class is the noisiest one (https://arxiv.org/pdf/2103.04053.pdf).  This limitation is appropriately addressed by the authors in their conclusions.  A few aspects should be further clarified: Are the networks used in the experiments pre-trained on ImageNet? The notation used in Section 2.1 - learning objective is not very clear, as y^n is used to indicate both the class level and pixel level predictions. It would be clearer to differentiate the notations How is the segmentation network trained from image-level annotations only, especially in the experiments depicted in Fig.2 in which different images had different label granularity? How were the types of abnormalities selected for annotation? Less categories are used than CheXpert or other benchmarks: is there a particular reason? In Table 1, it is interesting to note that the distribution of different types of findings is largely different from CheXpert - for instance consolidation appears in 35% of the images, whereas in CheXpert it appears only in <7%. I wonder whether these differences due to the population or to the labelling method. In Table 1, I would report also the number of cases with no findings In Table 2, the authors report the average of three independent run. I would specify whether data subsampling was repeated for each round. Given the level of data imbalance it is likely that randomly selecting a small subset of the training set will result in an insufficient number of samples for the less frequent classes.	The paper is clearly written. The conclusions are useful and convincing. However, the weakly-supervised segmentation algorithm can be improved.	Overall the experiments are extensive, and the results are convincing and insteresting.	The paper addresses a fundamental practical problem and, while the results are empirical, they provide several pointers for practitioners to reduce annotation costs. The proposed strategies could be easily replicated for other pathologies, although the same performance benefit may not be replicated.
157-Paper0467	Diffusion Deformable Model for 4D Temporal Medical Image Generation	This paper proposed a novel 4D image generation framework by adapting the denoising diffusion probabilistic model (DDPM) to the deformable registration model. The proposed method learns the distribution of the source and target and estimates the latent code to generate deformed images along the continuous trajectory. Experimental results on 4D cardiac MR image generation verify that the proposed method produces dynamic deformations from the end diastolic to systolic phase volumes, and outperforms the existing registration-based models.	This paper proposed a novel deep learning based model to generate the intermediate temporal 3D+t cardiac MRI image. This model is a combination of diffusion probabilistic module and deformation module which both are implemented with 3D U-Net. Their implementation is adopted from a PyTorch version of these two modules from [10] and [3].	This paper proposes a method for 4D cardiac image interpolation. The method uses a generative model based on denoising diffusion probabilistic models. The loss function combines a loss based on the diffusion model with a loss based on the deformable model. The model learns a code in the latent space that can provide the interpolation path between the source and the target images by a scaling sampling of interval [0,1]. The method has been evaluated in th ACDC dataset. The evaluation shows that the method provides plausible interpolation between the diastolic and systolic phases.	generated realistic deformed images along the continuous trajectory in a 4D cardiac MRI generation; proposed a diffusion deformable model for 4D medical image generation, which employs the denoising diffusion model to estimate the latent code; by simply scaling the latent code, the proposed model provides non-rigid continuous deformation of the source image toward the target.	The method utilizes a diffusion deformable model adapted from the denoising diffusion probabilistic model and can learn the spatial deformation along a geodesic path. As I know, this is the first time to use the denoising diffusion probabilistic model in this 4D data generation with a trick of deformation module. The experimental results on ACDC dataset verified that the performance of the proposed DDM outperforms the registration based method. These experiments covered most parts for evaluation of the registration performance.	The proposal of a diffusion probabilistic model for deformable image generation is a very interesting idea. The experiments show that the proposed method provides an interpolation that may be plausible between the diastolic and systolic phases of the cardiac cycle.	This paper proposed a diffusion deformable model (DDM), which can generate images of continuous trajectories with latent code. The result is comparable with state-of-the-art image registration tool voxelmorph in terms of PSNR and DICE score. However, no down streaming tasks are carried out to evaluate the effectiveness of the proposed images, so we cannot really draw conclusions about the quality of the proposed images.	"The reference style should be consistent. Typos like ""deiffusion"" in ""Then, for the reverse deiffusion, DDPM learns the following parameterized Gaussian transformations"" It is very hard to visually check the difference between the proposed method and VM. From the Fig.4. and Table 1, their difference is still very tiny. The statistic significance from paired t-test is not enough, I would like to have a test of Wilcoxon signed rank test. The difference of the performance on training data and testing data is not given in the experiments. The PyTorch code provided is quite difficult to follow and verify some details, like x0 in Fig. 2 is not described in the code and I could not find theire implementation."	The accuracy of the interpolation method is difficult to validate. The authors have shown the superiority of the proposed method with respect to VoxelMorph, however, this is probably not the closest competing method methodologically speaking and/or competitive enough for this application.	The authors provided code of the experiment in an anonymous github account.	Their code is ok, but it is a little difficult to follow some details.	The datasets used for evaluation are publicly available, although the exact images used for training, validation, and testing are not provided. The source code is available in a github repo. I did not check how does it run. The parameter values were provided. For the evaluation, the authors provided a clear description of metrics. Statistical significance is  stated.	As mentioned above, it would be interesting to do at least one down streaming task after generating 4D images, such as analyzing the cycle consistency of the  cardiac volume, to show that the advantages of generated images compared to registration. In table 1, it is hard to tell if the proposed method outperform the other two types of voxelmorph or not. So including a statistical test and a significance score would be appreciated here. In Figure 5 the authors compared the results of different /lambda values in the loss function, and picked an optimal one. It would be interesting to include two extreme cases - when there is only the diffusion loss or only the deformation loss in the model, and see the functionality of each individual loss.	See the above comments.	I found the idea of using diffusion models as a generative model for the interpolation of the cardiac cycle really interesting. The authors provided promising results. The proposed method could be used for interpolation in datasets with missing images or in applications where a more continuous temporal variation of the images are needed. I have just two comments I would be happy if the authors consider to address in the manuscript. 1) My first question is, did the authors implemented also a generative model using GANs? The authors claim in the introduction that GANs may generate artificial features, but do they have experience for the application? What about other ways of obtaining a latent space or generative models not based on deep-learning? 2) My second question is on the selection of VoxelMorph as a baseline for the evaluation. Are there any evidence that this could be a competitive method for the application? What about alternative methods based on traditional image registration such as geodesic regression or EPDiff based time-dependent interpolation based on LDDMM?	I think it is very novel to adapt DDPM to image generation, especially generating continuous 4D data for cardiac cycle analysis.	This paper is sound and well-written.	My recommendation is accept because I believe that the proposed method is a nice methodological approach to solve the problem. My main concern is on the evaluation of the generated interpolation. The method is very difficult to evaluate due to the lack of ground truth. I believe that the authors performed reasonable experiments, although the selection of VoxelMorph as baseline should be further justified. I personally think that other methods could be more interesting and help for a better assessment of the proposed method.
158-Paper0704	Diffusion Models for Medical Anomaly Detection	This paper introduces a a novel weakly supervised anomaly detection method based on denoising diffusion implicit models. Experiments are conducted on BRATS2020 dataset for brain tumor detection and the CheXpert dataset for detecting pleural effusions.	The paper proposed an anomaly detection method using Denoising Diffusion Implicit Models (DDIM). The key idea is to iteratively add noise and learn to subtract it from input images using DDIM. During the anomaly detection stage, the anomaly image is treated as the noisy image and the learned network is used to generate a healthy image. Finally, their difference is used to calculate the anomaly map.	The paper proposes to use diffusion models trained on healthy patient scans to restore a 'pseudo-healthy' scan for scans with anomalies and use the residual between the original image and the 'pseudo-healthy' image as anomaly localization.	The paper is mostly well-written and clear. The method achieves good performance for anomaly segmentation. Most of the aspects are described in sufficient detail to enable the reproduction of results. Denoising Diffusion Implicit Models seems a good adaptation to anomaly detection in medical images	The key strength of the paper is the idea to use diffusion processes in medical image denoising. In this case, denoising is treated as an image synthesis problem (generate a healthy image), where diffusion models have shown to preserve more details and generate higher-quality images as compared to GANs, which are traditionally used in this domain. In addition, the experimental validation of the paper is thorough and demonstrates well the capabilities of the proposed method.	Great and novel Idea! (I personally feel like this idea was overdue already, I was thinking about trying that for some years now as well) Experiments on multiple datasets. Nice analysis of the effect of the hyperparameters of the method.	Denoising Diffusion Implicit Models seems not novel enough given that the authors did not provide many modifications to the previous method [25]. The experiment lacks quantitative comparisons. It would be better if the author can compare with some of the SOTA unsupervised anomaly detection methods and semi/weakly-supervised classification/AD methods. Right now, one cannot justify whether the proposed method is promising or not. The motivation of why denoising diffusion implicit models works better than the traditional Generative methods is not clearly discussed.	The paper seems to be mainly an application of an existing technique from computer vision to medical imaging.	"The performance of the method is not entirely convincing (in some hyperparamter settings it seems to surpass the presented baseline). The VAE baseline appears to have been trained very poorly. Hardly any quantitative comparison. Not sure using BRATS2020 as dataset for anomaly detection is the best choice (prevalence of tumors already gives a different data distribution between normal/healthy and abnormal/diseased slices (even when discarding the lowest and uppermost slices), furthermore it is hard to tell if a tumor has deformed some unlabeled part of the brain ) A discussion on using the ""reconstruction"" error / residual as anomaly localization score would be appropriate since it has some received some critics in the recent years (see Meissen paper)"	The authors have produced the anonymous codes.	The model is very clearly explained and the code is included. Evaluation is performed on public datasets. I commend the authors on releasing very clear instructions in the code repository.	Looks good.	The paper is well written and clear to understand. However, I have some concerns about the experiments and the novelty. As discussed in above, it would be better if the author can compared with some baselines . Moreover, the novelty can be considered as insufficient given that it was proposed in previous paper [25].	"Overall, I understood the main ideas of the paper and found it very clear. There were some specific points I did not understand: How is the binary classifier used and why is it necessary? How does the iterative noising process induce specific anatomy details (Section 2, page 4)? It seems like noising would add random, non-anatomical noise. For results in section 3, are the anomalies found indicative of certain clinical findings or annotations? How does the proposed approach compare to non-synthesis based anomaly detection methods, e.g., density estimation or feature modelling? Please see ""Visual Anomaly Detection for Images: A Survey"" by Yang et al. It would be helpful to add to the related work section and discuss."	I think some points could give great benefits and more credibility to the paper: Please check you VAE implementation and training, looks like something went wrong there (put at least as much effort in your baselines as in your own method). Another dataset with a predetermined hyperparamter and training scheme setting and a realistic evaluation with quantitative numbers would give a better estimate of the performance of the method (e.g. a similar setting to the MOOD challenge).	Overall, the technical novelty of the proposed method is not enough and lack some vital experiential analysis. Hence, I recommend a weak reject for the paper right now.	While the paper is mainly an application, it is an important extension of a well-known computer vision technique to medical images. As such, I believe it would be a valuable contribution to the community.	Overall while some aspects of the paper are lacking I think the idea is interesting and works reasonably well and as such could be accepted (no one expects a first interesting idea to work perfectly from the get-go).
159-Paper1952	Digestive Organ Recognition in Video Capsule Endoscopy based on Temporal Segmentation Network	This paper proposes an automated organ recognition method in VCE based on a temporal segmentation network. MS-TCN++ model is used for temporal locating and classifying organs including the stomach, small bowel, and colon in long untrimmed videos	The authors propose to solve the important issue of automatically identifying anatomic transition points in video capsule endoscopy (VCE). To solve this, the authors propose to combine features from a timeSformer and I3D model combined in a MS-TCN++ model to automatically identify organs.	This paper proposes a temporal segmentation network to to recognise three different digestive organs throughout a capsule endoscopy video. The method is based on the concatenation of two feature extractors (TimeSformer, I3D) that are fed into a temporal model (MS-TCN). Given the low number, and long duration of the 3 classes, a smoothing term is included to remove any noisy predictions. The method is trained and tested on a large dataset of 200 videos that includes both normal and abnormal cases. An ablation is performed comparing different feature extractor options.	The main strength of this paper is to propose a method that can possible to longitudinally segment VCE images.  Whole video can be segmented by the proposed method.	The paper is well motivated. Anatomical transition annotation for VCE is a much needed technology to help cut down on what type of lesions and typical locations should be looked for. There are very few other works published for this specific problem, so there might be a publication justification from this point of view. The ablations are appreciated.	The clinical motivation is very well set, and the application use case is very clear. The utilised dataset is very large, it would certainly have impact if released. The inclusion of segmental metrics in the validation is well apreciated. The exclusive focus on frame-based metrics has been a longstanding issue in surgical temporal segmentation literature, and the study of other temporal event metrics is needed in the field.	The weakness of this paper is combinations of the existing methods. MS-TCN++,  timeSformer and  I3D. However, novel combination of existing methods can be evaluated.	The authors take two off the shelf feature extractors and use them in an off the shelf temporal organ classification model. There is virtually no technical novelty in the proposed approach. Although the application use case is an understudied problem with the previous state of the art being a simple 2D CNN. There is nothing in the proposed method which enforces temporal consistency. In Figure 3 we can see several areas where the proposed approach bounces around from stomach to small bowel, back to stomach, to colon, back to small bowel, and back to colon. While the final proposed approach doesn't show these in the illustrated example (only in the ablated parts) there is nothing to enforce temporal consistency to identify these transition points and ensure they're localized to a single location (one case of this is shown in the supplemental materials of the 40 cases). There is no comparison with any prior works (e.g. [11]) or even some reasonable baselines. But there are no true technical contributions to this work, so any baseline would just be an off the shelf method, which their work already is. How the data was split into training, validation, and testing, and any attempts to ensure no bias was introduced in this splitting decision (e.g. purposely putting easy examples in test) was also not discussed.	Video temporal segmentation is a widely studied topic in MICCAI, with a large number of recent methods published (TeCNO, TransSVNet, TMRNet, etc). Without any direct comparisons, it is hard to understand if the proposed method has any advantage over these (see detailed comments). When compared to these other methods, the one being proposed is very complex, with two spatiotemporal feature extractors and an MS-TCN with a very large number of stages. I would suspect this method is computationally more expensive by a significant margin, even though such details are not currently provided in the paper (happy to be proven wrong).	ok	This was not filled out at all, just no for everything.	Code not available. The method description is clear, as well as the training details and parameters. Data is not available (it would be very interesting if authors are planning to release, though). It is plausible that the general architecturte can be reproduced and tested on a different dataset than the one presented in this paper (there are numerous public datasets)	This paper is written well to read and data design is also well explained. However, methodological novelty is limited. The paper shows how the existing methods are utilized to make high performance. It is better to emphasize methodological novelty as a MICCAI paper.	See weaknesses and justification for improvements.	"On the comparison with other methods: There is a significant amount of literature on surgical workflow segmentation. While the application context is different, the problem formulation of supervised temporal segmentation is exactly the same. The most recent have publicly available code and can off-the-shelf be applied to this problem. Two examples: Gao, Xiaojie, et al. ""Trans-svnet: accurate phase recognition from surgical videos via hybrid embedding aggregation transformer."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021. Czempiel, Tobias, et al. ""Tecno: Surgical phase recognition with multi-stage temporal convolutional networks."" International conference on medical image computing and computer-assisted intervention. Springer, Cham, 2020. No comparison with any of these widely available alternatives is a weak point in the paper. There are a few other details that the authors could clarify: The data videos are classified into ""normal"" and ""abnormal"". Can the authors provide more details about what type of abnormal videos are included? From figure 3, I understand that ""vascular"", and ""bleeding"" are two sub-classes within abnormal, but this is not sufficient to fully understand what type of data is included. On the same note, it would be interesting to understand if the proposed method has different accuracy depending on whether the case is normal or abnormal. At present only global accuracy metrics are provided. Can the authors expand on this? ""We use segmental edit distance (i.e., edit) and the segmental F1 score at intersection over union (IoU) ratio thresholds of 50%, 75%, and 90% (F1@{50, 75, 90})."" - Please provide references here that fully describe how to compute segmental edit and segmental F1-Score. These metrics are not widely utilised in surgical action recognition, and a reference would be useful to the reader."	I made my decision based on the fact this paper is combination of existing methods.	Overall I am a bit torn and lean toward weak reject. If there were no issues with this paper at all (i.e. comparison with what limited previous works that exist (e.g. [11]), cleaned up explanation of how the data was split, discussion if the code and data will be released to the public, etc.) I might lean towards a very very weak accept simply because there are so few works published for this problem and the application novelty might be enough to carry this work for publication despite having no real technical novelty, but as the paper sits I cannot justify giving an acceptance rating.	This is a very borderline rating, very weak reject. I recognise the paper has very interesting elements. However, it is at the moment unclear if the proposed algorithm is superior to the widely available literature on the topic, given that no comparisons are made, and no other justification is provided for why such methods do not need to be considered. Open to review my assessment after the rebuttal discussion, with more information available.
160-Paper1447	Discrepancy and Gradient-guided Multi-Modal Knowledge Distillation for Pathological Glioma Grading	This paper address the clinically relevant problem where paired pathology-genomic data are available during training, while only pathology slides are accessible for inference. To ensure effective knowledge transfer, a DC-Distill module is proposed to allow the teacher to provide knowledge via reliable contrastive samples with teacher-student discrepancy. A GK-Refine scheme is proposed to allow the student to selectively absorb the beneficial knowledge according to the gradient-based agreement.	The authors propose a novel multimodal model using genomic+histopathology information for glioma grading. The model jointly uses three components: 1) State-of-the-art knowledge distillation techniques 2) Contrastive loss over teacher-student model features and 3) gradient-guided knowledge refinement. The model is compared with current multimodal models and outperforms them using the image modality, reaching   92.35% of AUC.	Authors propose a two-stage knowledge distillation framework for pathological glioma grading. At stage I, a multi-modal network is trained with both histopathological images and genomic data as inputs. At stage II, the privileged knowledge of the trained multi-model network is distilled to a unimodal model which only takes histopathological images as inputs. Authors further propose the discrepancy-induced contrastive distillation and the gradient-guided knowledge refinement to improve the performance of knowledge distillation. According to experimental results, the performance of the learned unimodal model can be very close to the multi-modal upper bound.	1, The proposed problem is clinically relevant, where one data modality is present and another is missing. 2, The problem is clearly motivated and formulated, and each component is well explained. 3, The proposed methods properly addressed the challenges of missing modality in testing time.	The multimodal framework is highly novel and includes recent advances in computer vision in a comprehensive manner. The results clearly show that the use of genomic information in the multimodal approaches outperform the unimodal unimodal pathology approaches. The proposed DC- Distill is more effective at transferring the knowledge than state-of-the-art knowledge distillation methods. While the model has many components, each one with its own technical properties and rationale, the authors explained it clearly and with the necessary formal support. The paper is very well written.	The main strength goes to the novel idea of transferring multi-modal knowledge to unimodal model via knowledge distillation. After training, the performance of the learned unimodal model can be very close to that of the pathology-genomic model and only require histopathological images as inputs. Besides, authors also propose discrepancy-induced contrastive distillation and the gradient-guided knowledge refinement to improve the performance of knowledge distillation.	The improvement to the accuracy seems incremental. Please clearly state the relationship between Eq. 1,2 and Eq. 3. How is g_{ens} in Sec. 2.2?	Nor, p_m (the labels ground truth) or the fusion F of genomic and CNN features are properly defined. There are no qualitative results showing meaningful genomic expression + morphology in image regions. There is no statistical significance test for the difference of the methods results.	The description of stage I training is insufficient. The explanation of DC-Distill loss is insufficient. The description of data preprocessing is too brief.	Data is publicly available. Code and split will be made available after review. So it's reproducible.	The dataset is open, and the authors mentioned that the source code will be released. Therefore, I'm confident the results could be easily reproduced.	Authors claim they will release the code. If so, the study could be reproduced.	Fixing the minor issues would make the paper better.	Figure 1 is great, but I think it is a little bit cluttered. Numbering each loss function component is a good idea but should have been ideal if you also reference and explain them properly in the same order in the text, currently only L_{DCD}^{m} does it. In the first stage of the Training of the teacher the fusion of the genomic features vector and CNN features is denoted by a F, what is F? Concatenation? Kronecker or Hadamard products? Addition? I guess the 80 dimensional genomic vector gets transformed to an embedding dimension equal to the multimodal (and unimodal) embedding vector before classification. Anyway this should be clearly explained (since it is the multimodal part of the method!), but currently it is not. This might be a great contribution to computational pathology if the code to reproduce the experiments is released with proper documentation and explaining how to extend it for other genomic+pathology paired datasets. Extending this framework and showing the potential for different tasks would be a great contribution for the community and for a prominent journal in the field. Qualitative results? It would have been great to see few heatmaps (for the different approaches) with paired gene expression data and compare with what is known for Glioma prognosis. Is the AUC the average one versus rest AUC of binary classification problems?	Authors should give more details on stage I training and explain the motivation of using mean-teacher simultaneously. More explanation about DC-Distill loss should be given to facilitate understanding. Authors should introduce the data preprocessing carefully. How to choose ROI? Working on which magnification?	Except several small issues, the paper is well-written. Two major contributions properly addressed the presented challenges.	Clearly written paper. Solid mathematical insights and knowledge of how to include state-of-the-art advances in ML in a clinically relevant problem. Open access datasets and (not yet released) code. Awareness of relevant works in the area/task and properly comparing to them.	Good performance, inspiring idea of transferring multi-modal knowledge to unimodal model via knowledge distillation
161-Paper0659	Discrepancy-based Active Learning for Weakly Supervised Bleeding Segmentation in Wireless Capsule Endoscopy Images	This paper presents an active learning method to reduce annotation cost for medical image segmentation. The method uses novel criterions for image selection. On a public dataset, it outperforms many baseline approaches from the literature.	This paper proposes an activate learning framework for weakly supervised bleeding segmentation. It proposes a new scheme to select pseudo labels or ground truth for training images effectively based on the reliability of the generated labels.	This paper propose an active learning pipeline that incorporates CAM-based weakly-supervised method and pseudo-label-based semi-supervised method. Experiments demonstrate that the proposed method has considerable advantages over prior works in the 10% data regime.	It is important to explore means to reduce the cost of medical image annotation. The presented method, DEAL, is an promising approach. It can use only 10% label budget to achieve comparable performance to fully supervised method. The presented method is technically sound. Its key components are clearly explained with clear formulations. This paper presents good experiments with promising performance and informative ablation study. The proposed method and eight baseline approaches are compared on a public datasets. Promising quantitative results are reported in comparison with the baseline approaches.	The paper proposes a new activate learning framework which selects pseudo labels according to the proposed CAMPUS criterion considering model divergence and CAM divergence. The proposed way to measure the divergence based on different threshold values is simple and efficient. The quality of the generated annotations is a critical problem in weakly supervised learning. The proposed method considers both the model divergence and CAM divergence.	A novel active learning pipeline (DEAL) that borrows idea from CAMs and pseudo labels and a carefully designed label selection criterion that fits the pipeline ; CAM maps and decoder with multiple heads/predictions to tackle the noisy predictions cause by the weakly supervision;	The presented method design is mostly empirical, lacking theoretical analysis. For example, no discussion provided to illustrate convergency of the proposed training procedure.	The proposed method is based on the assumption that large probability values represent high reliability. The assumption is natural in general, but how valid is the assumption? It should be better to add more discussion or experiments about this point. The methods replaces CAM labels with generated labels when the samples have small model divergence and large CAM divergence. And the rank is represented by the multiple of model divergence and CAM divergence. Do the model divergence and CAM divergence have the same weight influence in the final result?  Is there any better way to measure the influence? It will be nice to evaluate on more than one dataset in experimental part. It will be better to introduce more details about the selected baseline methods. Why choose these methods? What's the SOTA performance on the CAD-CAP WCE dataset? Only one fully supervised result is report, no SOTA result is mentioned and compared.	The design of multiple propensities (coarse, standard, fine) lack details and justification; The training procedure, especially the design of loss function, lack intuition; The improvement is mostly on 10% data regime, while in 20% and 30% data regime the improvement is marginal; also, the evaluation is only done on one dataset;	This paper provides source code and uses a public dataset. It should has good reproducibility.	Meet the requirement.	The authors have mentioned that code would be public.	"This paper can be improved with analysis on training procedure convergency. It can be further improved if the authors can show the current formulations of model divergence and CAM divergence are theoretically optimal. Fig.1 can be improved to be more clear. For example, rearrange sub-figures into a more clear logic flow. minor issues, a) Abbreviation GI needs to be declared before using; b) typo ""Tabel"" in section 3.2."	See the weaknesses.	"As discussed in the weakness part, the design of multiple propensities needs further justification, i.e. using multiple sub-decoders in active learning, which could be viewed as model ensemble, has been shown to improve performance in prior works (e.g. [1]). What would be the performance if we simply use three identical propensities? Please also cite the relevant papers. [1] Beluch, W.H., Genewein, T., Nurnberger, A. and Kohler, J.M., 2018. The power of ensembles for active learning in image classification. In Proceedings of the IEEE conference on computer vision and pattern recognition (pp. 9368-9377). The paper only mention that the three-propensity CAMs are generated by multi-threshold, how the threshold is determined? Equation 4 maximizes the L1 distance between D_c and D_f, what is the intuition between this? Should D_f be a sub-set of D_c? Maximizing the L1 distance would hurt the model performance on the overlapping region between them; Equation 5, the paper mentions that the goal is ""making the boundary of the discrepancy decoders always surround ...standard decoder"", how is it achieved? Training steps (2) and (3) seems contradictory, on (2), D_c and D_f is supervised to be different, while in (3), they are supervised to be the same, what is the intuition behind and why this would not cause the model failure? The writing needs to be improved and be more precise, e.g. the paper claim that the improvement is significant and other methods are ""far inferior to ours"". However, comparing the performance in table 1, the improvement is mostly on 10% data regime while for the others the improvements are marginal."	This paper studies important problem, presents technically sound approach, and reports promising results. These are major factors lead to the positive rate.	The paper brings an interesting method for active learning based weakly supervised segmentation.	The paper explore and exploit ideas from semi/weakly-supervised and propose a novel active learning pipeline. Experiments demonstrate the superiority of the method over prior works. On the other hand, some design needs clearer insight and in-depth justification.
162-Paper0353	Disentangle then Calibrate: Selective Treasure Sharing for Generalized Rare Disease Diagnosis	The paper proposes a generalized network for rare disease diagnosis, which simultaneously diagnose common and rare diseases. The network includes a gradient-based disentanglement (GID) module that separates common-disease features into disease-shared channels and disease specific channels. Also, it includes a distribution-targeted calibration (DTC) module that uses disease-shared channels to enrich rare-disease features via distribution calibration.	The paper proposes two modules to i) disentangle common disease features into disease-specific and disease-agnostic channels based upon gradient agreement, ii) use disease-shared channels to enrich rare disease features via distributon calibration.  With a WideResNet backbone, the authors compare the method against six recent approaches, producing improved results on two medical image classification tasks.	The paper approaches the problem of computer-aided-diagnosis by framing it into a few-shot-learning problem. The most occurring classes are used to learn shared and class specific features. The distinction between those categories relies on the gradient consistency across classes. Rare diseases  produce  increments to the shared features that are class specific. To avoid biases induces by the class  imbalance and  normal/lesion areas, the authors  learn to calibrate the distributions via an attention mechanism.	Network design is novel Comparison with other existing methods Performing Ablation study The paper is well written	Clear and easy to read, with good clinical motivation. The method is well described, interesting and properly motivated. Comparison with six recent approaches.	The major strength of the paper is the formulation of the classification task into a few shot learning task. The concept of splitting feature maps into shared and specific features based on the gradient agreement addresses the feature disentanglement in an elegant way by circumventing the interpretability issue. From an empiric point of view, the reported numbers show the superiority of the introduced methods compared to the state of the art.	details of network architecture are not clear	The MICCAI writing template was clearly violated (removing spaces between text and table, wrapping text around table). No statistical test to compare proposed methods in table 1.  This is particularly important as the method is compared against multiple baselines. Questions: Was the WideResNet backbone also used for the baseline experiments?	The authors could have also thought of framing the problem into a continual few-shot-learning problem with a single incremental epoch. A comparison with such a method like AIM (Lee et al. Few-Shot and Continual Learning with Attentive Independent Mechanisms ICCV 2021) would have made the paper stronger The authors argue that learning new rare classes leads to a decrease in performance for the major classes. Although this is an intuitive behavior, several methods from the incremental learning literature have been adopted in the few-shot learning research and help alleviating the catastrophic forgetting issue.(Minor) This paper might present a serious fairness issue since it compares different methods that prerequisite different and partly contradictory setups in order to work well. For instance, the FCICL method which relies on contrastive learning expects larger batch sizes than what the authors report in sec3.Implementation. From the text, it is not clear whether the hyperparameters have been optimized from each method separately (or at least taken from the respective papers). If not, then we are in a setup that favors the proposed method over the other SOTA methods. The description of the DTC deserves better formulation to make easier to understand and reimplement. (minor)	The authors provide most of the details, but the network architecture needs to be clear	Generally good.  However no 'analysis of statistical significance of reported differences in performance between methods', 'average runtime for each result, or estimated energy cost', 'description of the memory footprint'  is provided. As stated in section 5, the lack of statistical comparison is a negative.	The authors give a good overview of the adopted hyper-parameters but some details are still missing (for instance number of iterations and augmentation pipeline). Also, given the ambiguity of the DTC module description, a faithful reproduction of the method might be difficult.	Eq. (1): please revise it. The magnitude of g* is not included. Also, use magnitude for gi. For the GID module, how is sort implemented. Also, sort operation is not differentiable, so how does this affect the backpropagation? Please clarify. For the DTC module, how is the mask generated? Please clarify. Also, provide visual example if possible. The details of the network architecture are not clear. Please consider adding some details on the figure or add them in text.	Don't risk desk rejection by modifying the writing template, read https://conferences.miccai.org/2022/en/PAPER-SUBMISSION-AND-REBUTTAL-GUIDELINES.html  carefully. Use the supplementary materials for space. Use a statistical test, e.g. Wilcoxon signed-rank test for table 1, taking into account multiple comparisons. Some of the phrases in the paper were strange e.g. 'identifying rare diseases based on scarce amountof data is seen as inevitable.' 'studying automated rare disease diagnosis using an extraordinarily scarce amount of medical images is of far-reaching significance' 'selective treasure sharing (STS)' what does 'treasure' refer to here?	In order to make the paper stronger, I kindly ask the authors to reformulate the DTC module description (for example how to crop the lesion region and the normal region). Also clarifying the experimental setup of each of the used methods might help avoiding any fairness issues and let the reviewers and future readers have a less biased feeling of the empirical potential of the method. As I mentioned in the weaknesses section, a comparison with a continual few-shot-learning method would have been nice to get the full picture. This is definitely not required for accepting the paper but in case of rejection, I strongly recommend to add this comparison.	experiments and results are strong and comprehensive paper is well written	Clearly written. Interesting method applied to a relevant clinical problem. Comparison with six recent papers.	The formulation of the problem as a few-shot-learning problem and the nice formulation of the GID module are the most interesting parts of the paper. The distribution calibration is per se not a new concept but the authors implement it in a slightly different manner. The final rating would  depend on the fairness of the comparisons. In case of an appropriate setup, I am willing to  upvote the paper.
163-Paper2193	DisQ: Disentangling Quantitative MRI Mapping of the Heart	This work proposed a novel disentanglement framework for learning latent space of cardiac Quantitative MRI (qMRI) for contrast and anatomy separately, to benefit the downstream image registration and quantitative mapping.	The manuscript presents an unsupervised pair-wise registration pipeline with disentangled latent space of contrast and anatomy for a sequence of MOLLI T1-weighted images with inherent varied contrast and residual motion. Specifically, the developed neural network architecture decomposes the input pair into synthetic representations with same contrast, preserving anatomy, and same anatomy, preserving contrast, using U-Nets, for an easier pair-wise registration with Voxelmorph. The performance was evaluated by means of the reconstructed T1 fitting error map in the myocardial region and proven to be higher than the vanilla Voxelmorph approach. The application of disentangled representations for motion correction in T1 maps is novel.	This paper addresses the issue of improving cardiac quantitative MRI (qMRI) such as T1 mapping by proposing an image disentanglement method, called DisQ (Disentangling Quantitative MRI), to discompose cardiac qMRI images into their anatomical representation and contrast representation in the latent space.	(1) The proposed contrast-anatomy disentanglement framework suit well for cardiac qMRI problems as this paper promoted. Cardiac MRIs often has different object motions and contrasts across sequences, causing difficulties of quantitative mapping. Thus a disentangled representation of contrast and anatomy can help unify the baseline images, as demonstrated in experiments using T1 mapping. (2) The writing of this paper is very good and easy to follow. (3) Both qualitative results for disentanglement and quantitative results for baseline comparison and ablation studies showed that the proposed method is promising.	Overall, it is a well written manuscript with supporting validation. The application of disentangled representation for straight unsupervised registration in T1-weighted images is appealing. The proposed framework has been well explained. The method achieved an improved precision compared to the vanilla Voxelmorph. The ablation study is sound and validates the improvement of each component in the proposed pipeline. The proposed work tackles the clinical need for robust qMRI with a different, useful approach (disentanglement framework) contributing to already existing deep learning-based methods for motion correction in T1 mapping.	A novel formulation of cardiac qMRI image disentanglement A novel network architecture for cardiac qMRI	"Major concerns: The double-encoders approach is a common practice for disentanglement, however, it often requires additional contrast loss or adversarial loss for different modules to prevent them from learning the same representations. (One recent paper I remember is [1].)  In this paper's case, for example, the contrast module need to be forced not to learn anatomy representation and vice versa, such that they are disentangled. I am a little supervised that the cross-reconstruction alone can achieve disentangled contrast and anatomy. For bootstrapping disentanglement, this work proposed a similarity constraint for anatomy (Eqn (4)) and an information bottleneck constraint for contrast (Eqn(5)). However, it is not clear to me how this can prevent one module from learning representation from another? Maybe the way of combining anatomy and contrast as in Eqn(6)? It would be better that this work can discuss more on how the disentanglement is promoted and achieved. Some metrics of dismantlement will also be helpful besides figure (3). Minor concerns: It would be better to be more specific on the drawbacks of ""Groupwise"" baseline for readers to have a sense on the tradeoff between performance and computational cost. Instead of saying Groupwise ""demanded lengthy optimization"", maybe consider pointing out how roughly the computational time or resources increased? [1] Harada, Shota, et al. ""Order-Guided Disentangled Representation Learning for Ulcerative Colitis Classification with Limited Labels."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021."	"Myocardial motion correction in T1 mapping has already been addressed [[1, 2]], but the disentanglement representation in this field is novel. The employed models did not present execution time. Besides showing a higher precision made by ""Groupwise"", no further limitation has been discussed. Although the paper properly validated the proof of concept, the amount of data is highly limited, not well described, and conclusions may be limited by the generalisability constraints."	Not appropriately justify the medical motivation of the work Unclear presentation of the results	I didn't find obvious problems of reproducibility. Most of hyper-parameters are mentioned, especially for the loss function. Network architecture seems missing but this work intends to release its code thus it should not be a big problem.	The manuscript mentions the code will be available in a repository and the checklist supports this statement. There are some little details not explained in the paper, but it may be out of the scope of a conference paper. Once uploaded, it will guarantee reproducibility.	No elements on paper reproducibility could be found in the paper.	It would be better to add some discussion or metrics to the concerns of disentangelement described above.	"In the abstract, I would suggest mentioning the proposed method is a pair-wise registration method. In the abstract and introduction, the authors affirm that modern learning-based registration methods also fail on sequences with highly varied contrast, whereas the tested vanilla Voxelmorph did reduce the observed precision and existing work on deep learning-based methods for motion correction in T1 mapping [[1, 2]] also presented promising results. Unless proven otherwise in the study, I would suggest changing the tone on the limitations of the learning-based registration methods. The units of the plot 1(b) should be shown. The literature review did mention the classic motion correction method [22] and a robust PCA-based method [7, 21], but the cited learning-based methods [1, 16, 17, 18] did not address the issue of motion artefacts in myocardial T1 mapping as in [[1, 2]]. I would suggest opting for a more conservative tone as this issue has already been addressed. The useful contribution of the paper is in the disentanglement framework. The population of the dataset of MOLLI acquisitions should be explained at least in terms of number of subjects, centres, cardiovascular conditions, number of acquisitions with pre and post-contrast, and the ethical clearance. The implementation details should include the amount of computing time at least in a GPU. The selection of t = 5 is not properly justified, the results with another selection are not presented in the paper, only mentioned that it was not sensitive. In case the selection of t where the T1-weighted has the lowest contrast still achieved the same performance, it would be a missed benefit of the model that is worth describing. In Results, it should be specified that these are the results from the test set. Table 1 may have a typo in ""Ori"", which was previously described as ""Org"". Table 1 may be improved by including the execution time in each method. This may prove the efficiency of the proposed method compared to ""Groupwise"". Further brief clarification on the implementation of ""Morph"" may be needed. I assumed it was the same Voxelmorph with the input pair as is. In Results, I would update ""significantly"" by ""substantially"", as statistical significan was never quantified. The presented results showed an improved performance in terms of precision, measured as the fitting parameters variability, but not necessarily accuracy, which may be validated in phantom experiments, as stated by the authors. In Qualitative analysis, it should be mentioned it is a case study as one case may not me representative for all the test set. In conclusions, to support the statement that the method is generic, further evaluation may be required, i.e., evaluating the performance in pre and post-contrast separately, in an independent dataset, etc. It may be worth mentioning future studies will involve development or validation in extended datasets. I would suggest ordering the references by their appearance. [[1]] Arava, D., Masarwy, M., Khawaled, S. and Freiman, M., 2021, November. Deep-Learning based Motion Correction for Myocardial T1 Mapping. In 2021 IEEE International Conference on Microwaves, Antennas, Communications and Electronic Systems (COMCAS) (pp. 55-59). IEEE. [[2]] Gonzales, R.A., Zhang, Q., Papiez, B.W., Werys, K., Lukaschuk, E., Popescu, I.A., Burrage, M.K., Shanmuganathan, M., Ferreira, V.M. and Piechnik, S.K., 2021. MOCOnet: Robust Motion Correction of Cardiovascular Magnetic Resonance T1 Mapping Using Convolutional Neural Networks. Frontiers in Cardiovascular Medicine, 8."	"This paper addresses the issue of improving cardiac quantitative MRI (qMRI) such as T1 mapping by proposing an image disentanglement method, called DisQ (Disentangling Quantitative MRI), to discompose cardiac qMRI images into their anatomical representation and contrast representation in the latent space. The idea is new, but the medical motivation is not convincing. Throughout the paper, the authors used the term ""cardiac qMRI"", but the shown images contain not only the heart but also other organs or tissues, so that it was very difficult to precisely assess the improvement of cardiac T1 map. For ex., it is difficult to assess real interest brought by the proposed method with respect to the original T1 map (Fig. 4) on the myocardium. From a medical-imaging point of view, the ""contrast"" meant by the authors is not really a contrast problem. Instead, it is a signal loss problem. In this sense, such understanding of the problem may call into question the basis of the proposed method. Another important issue in this kind of methods is the sensitivity to noise. This point was not addressed in the paper. Some typos: by substituted Lanatomy with: substituting one common factors: factor will be present: presented"	Generally this paper is well-written, solving important and specific problem with novel solution and valid experiment results.	The presented work aims to ease the registration complexity in a real clinical need, disentangling both contrast and shape, which is quite novel in myocardial motion correction. My comments will hopefully improve the quality of the paper and clarify relevant aspects.	The medical motivation of the work was not appropriately justified The presentation of the results was confusing and unclear
164-Paper0323	Distilling Knowledge from Topological Representations for Pathological Complete Response Prediction	The authors use topological information and a student/teacher network to improve pathological Complete Response (pCR) for breat cancer diagnosis based on MRI.	This paper proposes a deep learning method with topological priors for pCR predictions. The authors use DenseNet as the backbone prediction network, and extract Betti curves as topological priors. They incorporate the extracted topological features into a linear layer and distill them into DenseNet. Compared with previous method (TopoTxR), the key difference is 1) the distillation makes it possible to avoid computing topological features at inference time; 2) the usage of Betti curve is different from persistent homology. Betti curve is a weaker feature (less expressive) than persistent homology. But it seems sufficient for this task.	In this paper, the authors study the neoadjuvant chemotherapy treatment response in patients with breast cancer named pathological Complete Response (pCR). Their main contribution is this paper is 1) Extracting persistent homology-based features from breast DCE-MRI images using Betti-curves which is allegedly less time-consuming. 2) Using a response-based knowledge distillation approach, they designed a network to fuse features from a DenseNet network and topological based feature extraction block based on a teacher-student model 3) Outperforming the state-of-the-art by 5% margin, thanks to the topological feature fusion.	The fusion of topology and the student/teacher network makes for  compelling reading and the improvement in performance appears to be considerable.	Using topological features for breast cancer imaging is novel and can have a big impact. I really think this is a very good research direction and is underexplored. The key advantage of the proposed method is the possibility to use topological information without computation at inference stage. TopoTxR can be very expensive as it not only computes persistence diagrams, but also computes representation cycles. I think this is a quite nice motivation and should be better stressed and supported empirically. Empirical results reasonably demonstrated the performance gain. But I have some issues with the baselines. Will explain below.	1 - The main strength of the paper is the incorporation of the topological features with a CNN model to outperform the state-of-the-art results in the literature. This is the first time a medical imaging study incorporates both CNN features and topological features.  2 - Authors exhaustively report the results based on the state-of-the-art as a benchmark and they did use different strategies for knowledge-based distillation as an ablation study.	There is no statistical analyses of the results in Table 1. The paper is difficult to follow and lacks clarity on several points.	The motivation of using Betti curves as topological features is a bit weak. As mentioned in the paper, it is more efficient in terms of computation. However, the topological features in the distillation method are only extracted once, and are involved only in training. It does not seem very necessary to use the faster yet less expressive feature (Betti curves) instead of persistent homology. To strengthen the motivation, empirical evidence should be provided (e.g., computational speed of Betti curves over persistent homology). Ideally, the comparison should be in the same resolution (right now the processing reduces the input image resolution). The empirical comparison needs improvement. The authors cited the baseline results of TopoTxR from [Wang et al. 2021]. However, the backbone in the original paper is using a much weaker backbone than DenseNet. Without using the same backbone, it is not clear whether the performance boost is due to the proposed contributions or the more advanced backbone. The authors used baselines DenseNet-CONCATE, DenseNet-MSE and DenseNet-KD to show the distillation is necessary. The main issue is that these baselines are not well designed. The authors used the logit output of a DenseNet to concatenate with / map to / compare distribution with topological features. But the logits (only 2 dimensional if I understood correctly) are already losing too much information. These baselines should be done using the last layer representation (e.g., the embedding before the last FC/soft-max), not the logits. In general, I think the real benefit of the proposed method is the efficiency not performance gain. Even if the baselines, including TopoTxR with DenseNet backbone, DenseNet-CONCATE, and DenseNet-KD are slightly better than the distillation method, it is OK. The key is the time saved during the inference time. 42 pCR and 116 non-pCR do not add up to 162, the reported total number.	"1 - While the approach in this paper is novel there are some ambiguities in this paper. Authors should clearly justify the term ""quicker or less-time consuming"" in Betti-curve compotation as opposed to the previous persistent homology-based approaches.  2 - The cubical complex filtration example and its definition are not very clear to the reader. While other types of simplicial complex filtration are based on the appearance and disappearance of homology groups, like graphs, which are intuitive, their cubical complex counterpart is not. Also, the term ""structure"" in appearance and disappearance is not a good choice for homology groups. 3 - There are some state-of-the-art approaches to deep learning-based persistent homology presented in top machine learning venues like ""PLLay: efficient topological layer based on persistent landscapes"" and ""PersLay: A simple and versatile neural network layer for persistence diagrams"" and ""Deep learning with topological signatures"". All these approaches circumvent the non-Hilbert Space nature of persistent diagrams by appropriate kernelization while in this study it is not clear how Betti curves are used as mere features for concatenating with CNN features. 4 - Although the authors claim the previous approaches do need feature engineering, in this study, the normalization and adaptation of two very different sets of features can also be the same engineering (clearly Betti curves need feature engineering to be used as features). In terms of pre-processing, however, the authors correctly address the minimum effort to do so."	Low. The used data is public but recreating this network and being able to compute the topological priors is an advanced task for anyone. If the code is made available, then this will improve. But as the code would only cover the network, there would still be some serious barriers to other users.	The reproducibility seems OK. The method is clearly explained. Ideally the code should be released upon acceptance along with hyperparameters.	Good.	"""distill the topological priors (i.e., Betti curve)"" Are these really priors? I do not understand why the authors need to write out Eqn. 2. It says L_{RKD} = L_{R}, couldn't that have been easierly achieved with just the statement that ""L_{RKD} is the cross entropy loss between the softmax functions logits of the teacher and student model."" or something similar? What is ""we use the middle half slices""? I will assume it is slices 15-45 of the 60 available slices, is this correct? The authors should clarify in the text. Figure 3, currently, confuses me. Subfigure (a) shows, I think, the 3D 0-cycle, 1-cycle, and 2-cycle Betti curves. If this is the case then these images should be captioned or be annotated (a1), (a2) and (a3), with the captioning explaining things. Now subfigures (b)-(d) show different filtration levels for the Betti curves. But I do not know if this is just for two subjects one pCR and one non-pCR? Or multiple subjects? It bothers me that the scales of the x & y axes are completely different. I think the x-scale should be the same in all three, even if it only goes up to 50 filtration steps. I think the y-axis might be better as a percentage rather than a total count. Just an opinion. What is absolutely driving me crazy, and I only can only complain about this because the authors provide the total counts, is the totals for the 0-cycles and 1-cycles. If I have N 0-cycles, I can have at most ((N * N-1) / 2) 1-cycles, because math. From Fig. 3(b) at filtration step 15, there appears to be less than 100 0-cycles, maybe below 50 0-cycles. From Fig. 3(c) there are north of 20,000 1-cycles. (This is the case for both pCRand non-pCR.) I cannot square these two numbers. Can the authors explain the discrepency? Or am I on the wrong wavelength. Grammar ""which has been shown high"" -> ""which has shown high"" OR ""which has been shown to have high"" ""implemented by the PyTorch"" -> ""implemented within the PyTorch"" OR ""implemented using the PyTorch"""	See the weakness section.	As mentioned in section 5, the authors should clearly determine the time complexity of the Betti curve over the previous ones. Also, they should address why they did not include the mentioned deep learning-based approaches using PH in their work. Those methods are more deep learning-friendly. Betti curves are not inherently feature-ready to use for classification/regression purposes. It is strongly recommended to better explain and illustrate the cubical complex filtration and show the appearance-disappearance of homology groups (when a node, cube, or square appears and dies).	It is very interesting work.	Overall, I like this paper. I think it is well motivated. But I think the authors are a bit distracted from the main benefit (efficiency). And the empirical evaluation is not sufficiently convincing currently. I am happy to increase my scores as long as my concerns are addressed.	Although there are some ambiguities that are addressed in section 5, the authors' approach to studying the very vital medical treatment prediction, pCR, is novel by incorporating topological features and CNN ones. Topological features can be a great informative feature as a complementary counterpart with other ubiquitous image-based features.  The outperformance of the proposed model with a clear benchmark comparison is another reason for the reviewer to recommend this paper.
165-Paper1029	Domain Adaptive Mitochondria Segmentation via Enforcing Inter-Section Consistency	The authors propose to conduct domain adaptive mitochondria segmentation by  enforcing inter-section consistency. They  align both segmentation results and intersection residuals predicted from source and target volumes via adversarial learning. The validations show that their method outperforms leading methods on several datasets.	The authors present a method for successfully applying a model trained on one EM dataset to another EM dataset. This domain transfer allows the reuse of existing ground truth annotations. The authors compare their results with previously published methods for the same task and other UDA methods.	"A new method is proposed for unsupervised domain adaptation (UDA) of 3D mitochondria segmentation in EM images.  The inter-domain alignment design leverages adversarial training to align segmentation output space. The authors designed residual decoder and discriminator, where ""residual"" refers to the differences between the prediction maps from two sections of the same volume, which enforces simultaneous alignment between the prediction maps of two sections from the same 3D volume as well as the residual prediction maps from the target domain with the corresponding masks from paired source domain sections. For intra-domain alignment, they designed inter-section consistency loss for the target domain to penalize differences between the prediction map of each section and their corresponding pseudo-label generated by subtracting the residual prediction map from the prediction map of the paired section of the target same volume. The method is validated on four mitochondria datasets."	The exploration of  inter-section consistency for this task stands for its novelty; The work presents an extensive experimental section. The paper is clear and well written.	The authors evaluated their method on multiple datasets and compare with an extensive set of methods that exceed methods developed for this task The authors show that their method outperforms existing methods The paper is written clearly The authors include ablation studies	The design of inter-section consistency leverages intra-domain information, which is intuitive and effective. Although enforcing the prediction consistency in general is a common idea for 3D image segmentation, it seems has not been leveraged for UDA of 3D images yet. Both the extensive experiments on four datasets as well as the ablation experiment seem solid and convincing.	1) It seems that the performance of the Oracle and NoAdapt for adaptation from MitoEM-R to MitoEM-H in Table 2  is  low.  2) It seem less informative to conduct ablation results onadaptation from VNC III to Lucchi (Subset1).	"The authors developed a 2D model for a 3D task. 3D models have been used for organelle (synapse, mitos, ...) segmentation and detection since at least 2017 and vastly outperform 2D models. The method presented by the authors relies on comparing predictions on 2D slices; however, it is not clear whether this would work for the more appropriate 3D task. Further, one could argue that by having access to a ""larger field of view"" in the third dimension, their comparison to existing methods is not precise. It is not clear, whether the access to adjacent slices alone would have created the observed performance increase The presented datasets are not representative for current EM datasets. Two of the datasets are < 20um^3 in volume."	UDA network design needs more justification and clarification. Please see comment section Point 2 for details. Descriptions about UDA in the introduction and method section are a bit confusing and inaccurate.	"The authors listed ""yes"" for both code and pre-trained models. In this case, it can be an easy task for both training and testing. If the reproduction was only based on the descriptions in the paper, it could be somewhat difficult."	The method appear reproducible.	"Overall it seems feasible to reproduce the results. Some information selected as ""yes"" in the reproducibility report is missing: mean, variance, statistical significance and failure case analysis. Another question is whether the authors will publish their code."	1)Please check the performance of the  NoAdapt for adaptation from MitoEM-R to MitoEM-H in Table 2. 2)It seem less informative to conduct ablation results onadaptation from VNC III toLucchi (Subset1).  3) Discussions about the limitation of the study and future work are recomended. In general, the authors should emphasise more the real benefits of the methods found and give some general points and suggestions to authors taking these fields into account.	"Please avoid exaggerated language (""for the first time"", ""as a resuce"") the term ""gaps"" is not clear at first. Please provide a better explanation early in the paper. The evaluation would be better done object-wise and not segmentation-wise as has been done by Kreshuk et al, 2014; Dorkenwald et al, 2017 and others. Object-wise evaluations better reflect the performance as it relates to the downstream use-case Please use a different color combination in Fig 2 to allow red-green blind readers to parse this figure. The dataset pairings in the evaluation minimizes differences between paired datasets reducing the information that can be cleaned from it. In the interest of better evaluations please consider different pairings such as MitoEM-R + VNC III. The work by Januzweski et al, 2018 (CycleGANs) addresses a similar problem for neuron segmentations and should be at least mentioned."	"It seems to me that there are a few inaccurate descriptions about UDA: 1.1. In the introduction section on Page 2 it is ambiguous and misleading to describe some of the unsupervised domain adaptation (UDA) approaches as ""... aligning segmentation results ... to make the prediction of mitochondria on the target volume similar with that on the source volume"". However, with UDA it is not the exact prediction of the image volumes from the two domains that are made ""similar"", but the distribution of features (or selected summary statistics of the features) extracted from the two domains with the domain adapted model. Though this alignment can be achieved via various methods, including aligning the output space, in the end we just need the adapted segmentation model that extracts aligned features for both domains. Could the authors rephrase and clarify the description?  1.2 Related to 1.1, on Page 4 Methodology section under ""Prediction Consistency"": ""... we enforce the target predictions to be similar with the source predictions ..."" This description is confusing. I would suggest the authors rephrase it. For example, the described approach can be described as ""enforcing the distribution of target layout to be similar with the distribution of source layout"".  1.3 In the ""related works"" section on Page 2, the description of UDA literature for EM applications is not accurate: The ""pseudo label-based"" approaches also seek to align the features of two domains in order to learn domain-invariant features, despite via entropy-based self-supervised training. Thus such definition overlaps with the authors' definition of ""domain alignment-based"" UDA approaches.  It is therefore confusing to categorize UDA approaches into ""pseudo label-based"" and ""domain alignment-based"". Looking at Supplementary Fig 1, the authors designed two separate segmentation encoder branches for two image sections from the same volume with a certain z-step, respectively. Separate decoder branches are also depicted in this image. 2.1.  It is misleading in the main manuscript that these two separate segmentation decoders (including the classifier layer for segmentation) are labeled with the same color and denoted the same as ""Seg. Decoder"". A related concern is that the segmentation discriminators for each of these two decoders are also labeled the same in Fig1 and denoted the same in the loss term (Equation 2).  2.2. There is no justification of why adopt two separate decoder braches. Since the layout of the segmentation masks between the two sections should be from the same distribution, it seems that one shared decoder (as well as a shared discriminator) can serve the purpose and the additional decoder complicates the model. I would suggest that the authors add their justification for such a design.  2.3.Which exact network modules are used for inference? Can any of the encoder and decoder branch be used for inference? ""inter-section gap"" belongs to intra-domain gap, which has been studied in multiple reports (such as the baseline used in this manuscript DA-VSN and Pan et al ""Unsupervised intra-domain adaptation for semantic segmentation through self-supervision"" CPVR 2020). Considering the framework includes both inter-domain alignment and intra-domain alignment, I would suggest that the authors provide such elaboration/clarification to make the design easier for readers to relate to domain adaptation. UDA have great promises for the MICCAI community, but one open question is how to avoid the heavy dependency of the target domain annotations as validation set for UDA model selection. I would encourage authors to provide insights on this regard in their future work and help the community seek a way to make UDA more feasible/useful in practice, e.g. clinical applications. Minor: 5.1. Source domain and segmentation map are both denoted as ""s"". Although the former was denoted by upper case and the latter by lower case, it can be confusing to the readers. Could another letter be used to denote segmentation maps? 5.2. In Table 2, ""AdaptSegNet[4]"" -> ""AdaptSegNet[21]""; DANN reference is not [21] and is missing?"	The authors address domain adaptive mitochondria segmentation by  enforcing inter-section consistency. The exploration of  inter-section consistency for this task stands for its novelty. The work presents an extensive experimental section. Moreover, the paper is clear and well written.  There also some issues in this study. For example, the performance of the  NoAdapt for adaptation from MitoEM-R to MitoEM-H in Table 2  is  exceptionally low,which may mislead the readers.	The method is based on a 2D model. 3D models are the state-of-the-art for this task for several years and should be used to show the effects of the introduced methods on actual applications. Given the comparison to other 2D models, it could still be argued that this is not a sole reason for rejection. However, the fact that there is ample reason to believe that a 3D model would not work with the proposed inter-section consistency means that this work will have limited impact.	Developing models with the constraint of data scarsity is of great interest to the community. To this end, the authors propose a simple, elegant yet effective approach for improving UDA for 3D EM mitochondria segmentation. Such an approach potentially can be extended to other 3D imaging modalities and applications. The experimental validation is solid and extensive.
166-Paper1242	Domain Adaptive Nuclei Instance Segmentation and Classification via Category-aware Feature Alignment and Pseudo-labelling	In this paper, the authors propose a class-aware feature alignment method in domain adaptation for nuclei segmentation and classification. They also propose to select a certain branch to generate pesudo labels to reduce the negative effects of incorrect prediction in pseudo labels. Experiments show the effectiveness of the method.	Proposes a new unsupervised domain adaptation approach for nuclei instance segmentation and classification. The approach uses class-level feature alignment with class-specific adversarial discriminators and self-supervised learning from pseudo-labels predicted by the model. The proposed approach outperforms the baseline methods on segmentation and classification.	Revise the class-awre feature alignment compared with Ref.12	It is a simple but effective way to use the class-aware feature alignment for domain adaptation to handle different types of nuclei, because the appearance and distributions among different nuclei are large. The ablation studies clearly present the effectivenss of the proposed methods.	The approach is well-motivated. Although class-aware domain adaptation has been done in the machine learning communities, it is claimed to be the first time for nuclei instance segmentation and classification. Empirical results and ablation studies show that the proposed approach outperforms other baselines.	This paper proposed a category-aware prototype pseudo-labelling architecture for unsupervised domain adaptive nuclear instance segmentation and classification	1). The details of the method part are not clear enough. The losses are not explained either in the main paper or the supplementary materials. 2). It could be better to provide other methods' results in the Dpath -> GlaS task as those in the Dpath -> CRAG task	Some details of the proposed methodology is missing. For the class-aware discriminator, it is unclear how the classes are assigned for the target domain due to the lack of labeled data. How to handle prediction errors that could interfere with the discriminator. The paper doesn't adequately discuss the limitations of the proposed approach. Extensive use of pseudo-labels (in both the class-aware discriminator and in self-supervised learning) could result in model deterioration and poor calibration due to enforcing the model to optimize towards what it has already know. What are the motivations and practical considerations for using pseudo-labels? Is there any issue encountered/resolved due to mistakes in the pseudo-labels?	What's the main difference between the proposed method and Ref.12, so what's the highlight of your paper. There are a lot of papers about Zero-shot or one-shot image segmentation or classification, why do not you directly use these techniques and select the UDA, which more complexity than others? How about the failures? There are some grammatical and sentence expression errors in this manuscript, pls revise them carefully.	The authors do not provide the description of results with central tendency (e.g. mean) & variation in the paper, which they answered Yes in the checklist. It would be a plus to add mean and variance to the results.	Not sure how easy/difficult to reproduce this paper.	I think  it can be reproduced	(1) There are details missing in the method section, which affects the overall quality of the paper because the readers have no idea how the method works. It is better to make sure that all necessary infomation is provided, either by clear description in the paper or adding references. (2) About the pseudo labels, I am wondering if the authors generate pseudo labels for all images/patches no matter what the probablity scores are. It may be better to only use the pseudo labels that have high probability scores (i.e., close to 1 or 0), which are more trustable. (3) It is interesting to see how the method works in the reverse direction, e.g. CRAG -> Dpath.	In the experiments, why Dpath is always used as the source domain in both the main paper and the supplementary materials? It would be interesting to know the adaptation performance from the other direction, i.g., CRAG -> Dpath. Not clear what is the difference between Baseline+CA+PL and the proposal. It is typically not trivial to learn the loss weighting parameter in Eq. 2. Would it be possible to share what is the weighting parameter after training and if any special optimization technique is used.	What's the main difference between the proposed method and Ref.12, so what's the highlight of your paper. There are a lot of papers about Zero-shot or one-shot image segmentation or classification, why do not you directly use these techniques and select the UDA, which more complexity than others? How about the failures? There are some grammatical and sentence expression errors in this manuscript, pls revise them carefully.	It is an overall good paper but the method part is not clear enough. Thus I tend to give weak accept.	The paper presents an interesting approach using feature alignment and self-training with competitive results over the baselines. The main limitation of this paper is that it doesn't adequately discuss the limitations of the proposed approach, e.g., it is well-known that the type of self-supervised learning in this paper could result in model deterioration and poor calibration.	reproduced
167-Paper0127	Domain Specific Convolution and High Frequency Reconstruction based Unsupervised Domain Adaptation for Medical Image Segmentation	This paper presents an unsupervised domain adaptation method for medical image segmentation, introducing a novel domain-specific convolution (DSC) module to dynamically extract domain invariant features, and an auxiliary high frequency reconstruction (HFR) branch for filtering out task-irrelevant low-frequency features. The effectiveness of each module has been validated in the ablation study. Comparison study with several competitive unsupervised domain adaptation methods on the multi-domain RIGA dataset verifies the superiority of the proposed method. Yet, I have concerns regarding the applicability of the HFR component for noisy image segmentation. See below.	The authors have considered an important problem of domain adaptation in medical image segmentation. They have proposed domain specific convolution module to get appropriate features and an additional high frequency task to guide the domain adaptation process. They have has evaluated the method on relevant datasets.	This work presents a multi-source-domain UDA method called Domain specific Convolution and high frequency Reconstruction (DoCR) for medical image segmentation, where an auxiliary high frequency reconstruction (HFR) task is proposed to facilitate UDA and the domain specific convolution (DSC) module is constructed to boost the segmentation model's ability to domain-invariant features extraction. The experimental evaluation on a benchmark fundus image dataset demonstrated the superior performance of the proposed DoCR over other UDA methods in multi-domain joint optic cup and optic disc segmentation.	The paper is well-written and easy to follow. The two core components are novel. The comparison study and ablations study are somewhat thorough. The idea of DSC with a domain-specific controller to characterise domain-specific convolutional block is interesting and effective. The advantage  with such a dynamic convolution kernel design compared to shared convolution kernel with different sets of domain specific batch normalisation layers have been discussed and validated in the experiments.	Paper is very well written and easy to follow. The usage of resnet-18 to get the encoding of the input image for domain specifc controller is interesting. The additional branch to compensate for the differences in the low frequency data is an apprecibale solution (more like multi-task learning). The design of experiments is reasonable and the results are also promising.	The design of HFR is based on the assumption that the style information is embedded in low-frequency components and structural information is embedded in high-frequency components. Therefore, the authors propose the HFR to filter out the low-frequency components where most style information locates and hence preserve the structure information for UDA, which is interesting. The authors conduct extensive comparative experiments with eight methods, including baseline, data-level, feature-level and decision-level, to evaluate the performance of their method, which is a plus. Fig.1 is clear and helps to understand and this paper is well-written and nicely organized.	How did the authors select the beta, the window size for filtering out the low-frequency component? The sensitivity analysis of model performance against different choices of beta is missing. Any other data augmentation did the author used in this paper, except for the Fourier style augmentation based on frequency spectrum replacement? I would like to know if training images have been augmented with high-frequency noises or itself contain some high-frequency artefacts, will the HFR bias the model to keep these non-robust features in f_img? In that case, can the segmentation branch (which takes f_image as input) still produce reasonable segmentation? Have the trained segmentation model been tested on noisy test images?	What would happen if we use the one-hot encoding instead of providing the output of resnet? A better commentary on Table 2 in discussion would improve the paper further. For instance DoCR is better than Intra-Domain, a simple explanation on these aspects.	The description of DSC module doesn't seem very clear. For example:  (1) The light-weighted DSC head is missing in Fig.1.  (2) What the domain code is designed for?  (3) How x^a is generated? What augmentation method is used? (4) Why the DSC module can extract domain-insensitive features? In Table 2, the results produced by w/o DA are better than the results of some methods with domain adaptation. It confuses me while necessary analysis seems to be missing.	Reproducible.	Authors have provided enough information for reproduciblity of the paper.	The source code of this work is provided, which is a plus. I believe this work is reproducible.	Have the authors evaluated the accuracy of the domain predictor on other unseen test data? The predictor was only trained on a limited unaugmented data. Can the segmentation network still produces reasonable segmentation result when the predictor fails to predict the domain id on some OOD data? I am also interested about the source domain segmentation performance as the DSC by design can support multi-domain segmentation.	Check weakness section	The authors should discuss the issues I mentioned. Comparing the results between w/o DA and Intra-Domain, the domain shift is not very severe. So I encourage the authors to evaluate their proposed method on other tasks with more severe domain shift. I recommend 5-fold cross-validation to verify the effectiveness of the proposed method. I encourage the authors to visualize the reconstructed image at the inference stage to prove the effectiveness of the proposed HFR module.	novelty and effectivess.	I enjoyed reading the paper because of its novel extension and also the experiments design.	The description of DSC module and some experimental results make me confused, as I mentioned before.
168-Paper1315	Domain-Adaptive 3D Medical Image Synthesis: An Efficient Unsupervised Approach	This paper proposed an unsupervised domain adaptation method based on 2D VAE approximating 3D distributions. The proposed domain adaptation approach is applied to image synthesis problem and the authors demonstrated the effectiveness of 2D VAE method.	This paper presents an unsupervised domain adaptation strategy for image translation/synthesis. A VAE is pre-trained on output domain of the paired training set and used to approximate its distribution. The synthesis network is trained on the paired training domain (S) (in a supervised fashion) and on the unpaired shifted domain (T) (KL between the output domain of S and output domain of T, under the VAE). In order to work with small training set, a 2D VAE - rather than a 3D one - is used. The latent code of a 3D volume is obtained by concatenating the latent code of all its 2D slices.	The authors explore a new topic of unsupervised domain adaptation (UDA) for image synthesis. The key difference from the previously well-researched UDA classification and segmentation tasks is the discrepancy between objectives. Here, the authors suggest an approach based on two existing ideas: image synthesis, and domain distributions (generated by VAE) matching. Besides, the ideas are combined in a novel way and used in a new setup.	The idea of considering 3D volume as the stacked 2D slices and applying them in a mini-batch instead of different channels is very interesting. In this way, the model size can be dramatically reduced compared to 3D based model.	Novelty: Does not try to transform the inputs from one domain to the other, but directly trains the network on both domains. Significance: Unsupervised and easy to implement, and can potentially be used on a number of tasks (e.g. semi-supervised training on datasets where only a small proportion of the data is paired)	The authors suggest a novel problem formulation that potentially enhances the development of DA and domain generalization methods. The motivation, structure, experimental setup, and analysis (including the study of different factors) are properly detailed, creating a complete picture of the problem and approach to solve it.	The evaluation could be performed in a cross-validation manner. Only mean values were reported in the quantitative results. It's hard to tell the improvement is statistically significant. Please add standard deviations or test statistically to show the significance of the proposed method.	"Performance: The qualitative results in Fig. 3 are underwhelming. It's difficult to gauge without an example of what a ""good"" synthesis (in the absence of domain shift) would look like. Writing: The paper lacks clarity and is difficult to follow at times. Evaluation: Given that the adaptation strategy is quite generic, the paper could have more impact if the strategy was evaluated on multiple tasks. For example, it seems that the exact same architecture could be used on a segmentation task, where the VAE would be trained on one-hot labels."	"The clinical applicability of study is limited. Among publicly available datasets, I could not find the one that contain some of the modalities only partially. And the authors do not explicitly describe the application scenarios. [More details are given in Sec. ""Detailed comments"".] The metrics SSIM and PSNR do not explicitly measure the quality of some algorithm. Furthermore, one of the frequent metrics in image synthesis is the quantitative assessment of the impact of generated images on the downstream task. More specifically, the authors could add Dice score of the glioma segmentation task, using the original modalities, as an upper-bound, and present Dice score of the same model using the generated modality instead of the original one (or, alternatively, trained from scratch using generated modality). This difference in Dice score quantitatively assesses the image synthesis algorithm also with the clear motivation in terms of further applicability."	Lack of explanation on the hyper-parameters selection, average run time	The code is not provided, but maybe it will be made available upon acceptance? No statistical tests nor measures of variance are provided, making it difficult to evaluate the significance of the quantitative results.	I would like to suggest the authors to make their code publicly available in an anonymous form (e.g., by creating an anonymous account on github) along with submission of the paper.	Please use the term 'data augmentation' instead of 'data argumentation'. Please double check the notations and equations in page 4. Some are not consistent or not fully explained.	"In Fig 3 I would suggest adding a column with one of the supervised DA results, which I expect look much better. Currently, it is difficult to evaluate how much is ""lost"" by going unsupervised. I would train/test a network without domain shift (e.g. train and test on TCIA) in order to get a feel of the best possible synthesis. This would provide an additional upper (upper) bound. Fig 3. shows that even after DA, the synthesized images can be quite unusable. The authors may want to acknowledge this and comment on it: what's missing to get networks that are truly domain agnostic? In the 2D VAE, I don't see why the fact that batch order follows the slice order should have an impact. I imagine that the VAE could be trained on a subset of shuffled slices (even from different subjects). What matters is that the concatenated code for a full volume (in a single batch, or split over minibatches) is computed during fine-tuning. I did not understand the paragraph about ""impact of the amount of volumes"" at all. What does ""the first continuing training batch in the UDA process contributes more to the results"" mean? The authors stop short of claiming that they provide a solution to unsupervised domain adaptation, and merely claim to ""explore domain adaptation for medical image-to-image synthesis models"". I completely agree that some problems are hard and stay unsolved, but in this case, a better paper would evaluate different unsupervised approach and discuss the factors that make the problem still unsolved. Is the VAE just not a good distribution encoder? Are there not enough training examples for the VAE? Are there more than intensity differences between the two domains (e.g., different pathologies, different anatomies) that makes matching distributions not a good surrogate form of supervision?"	"On major weaknesses: M.W. 1 (a) From my practice, it is a typical approach to filter out such cases with missing modalities when creating a dataset, and in practice such cases are not rare. But the authors need to properly describe that point (maybe the description could be find in literature). (b) Moreover, I have seen few works, that address the problem of missing modalities and use a synthetic experimental setup on BraTS dataset, stochasticaly removing modalities. Such work could enhance the authors message and method motivation. Unfortunately, I could not recover the papers due to limited time, but I believe this hit would encourage the authors to find these works and strengthen their message further. Other comments: It seems like contribution (1) is enough, and (2) and (3) are the implementation and evaluation details, respectively. The authors could formulate their contribution as a plain paragraph with one strong message (1) and supporting details (2) and (3). Fig. 2 floats, no reference from the text is given. Besides, Fig. 2 is self-explanatory, but it still would be better to link it with text. What is the floating ""n"" in the caption? major comment Why do not the authors train 3D VAE in the same fashion on small patches? As far as I understand, the only reason to discard 3D case is diminishing difference in distribution with the increase in size (e.g., equalizing anatomical structure). But reducing 3D images to small patches also solves the problem of equalizing distributions. Moreover, the same procedure of learning structured (along one of the axis) latent representation for 2D images, as in [16], could be applied to learn a structured representation for 3D images, switching from 1-axis structuring to 3-axis structuring. In my opinion, the authors should develop 3D method in depth as well as they do for 2D. It might underperform due to the lack of finetuning, as 2D case has. Work [3] has been published as [https://dl.acm.org/doi/abs/10.5555/3304415.3304514], thus its citation should be replaced with the appropriate form. In Sec. 2, par. 2D s-VAE for modeling 3D distribution, it would be more clear to use word ""call"" instead of ""nickname"". How do the authors use the order of slices in their method? It seems like the order does not impact the training procedure... Why do we need regularization of distribution to N(0, 1) with KL-divergence loss (Eq. 1)? It seems like the task does not require the exact form of distribution. N(0, 1) is a multi-dimensional distribution, so the authors should replace 0 and 1 with the zero vector and ""identity"" matrix, respectively. Why the authors use L2 loss to train a VAE (Eq. 1), while training a CNN for synthesis with L1 loss? In both cases, the task is the same -- image generation -- thus, this choice is unmotivated. Also, super resolution reviews (e.g., [https://arxiv.org/pdf/1902.06068.pdf]) indicate that L1 is perceptually a better choice. The authors might use it to motivate their decision. In Tab. 1, the authors might indicate the unavailability of labels (e.g., in row 2, col 2) with a specific symbol (e.g., *) to enhance readability. The authors should name their figures in Supplementary Materials starting from 5. Providing the explicit links to them from the text (e.g., backbone in Fig. 5) would also increase the readability. Hyperlinks between two files would be unclickable, but it still a visual improvement. The authors could additionally report SSIM and PSNR for the ground truth (using the original modalities) as the upper-bound. In Sec. 4.2, the authors could describe the procedure of [16] in few lines, so the paper becomes self-containing. Clarification ""sampling infinite number of 2D slices"" seems to be misleading. Describing the results in Fig. 4 (b), the authors should specify the domain (target or source) of volumes that they vary. (Fig. 4 (b) caption indicates the target one.) The use of term ""batch"" in Sec. 4.2 diverge from term ""iteration"" in Sec. 3."	The proposed method sounds interesting and novel, but it's hard to decide whether the improvement is statistically significant or not. Also, there's no comparison with the other competing methods.	The concept makes sense to me. It does beat the baselines, but results are not completely convincing.	The authors address a novel topic, clearly formulating their contribution. Their setup and evaluation are properly designed and, from my perspective, do not contain explicit mistakes. Most of my comments are directed on the minor improvements of the paper that could be addressed at the proof-reading stage.
169-Paper0992	Domain-Prior-Induced Structural MRI Adaptation for Clinical Progression Prediction of Subjective Cognitive Decline	This paper introduces a domain adaptation method for MRI-based Subjective Cognitive Decline (SCD) classification. Specifically, they devised a two-path framework and jointly trained them with activation-averaged attention and maximum mean discrepancy loss. Taking a relatively large ADNI dataset as a source domain, the proposed network was trained to reduce distribution gap to a target AAA dataset. The experimental results showed improved performance with their method.	This paper modifies the transfer learning methods using feature adaptation, and propose a domain-prior-induced structural MRI adaptation (DSMA) method for automated SCD progression prediction. Experimental results on 795 subjects from the public ADNI dataset and a small-scale SCD dataset demonstrate the superiority of the proposed DSMA method.	In this work, the authors propose a method to predict the progression of Subjective Cognitive Decline (SCD) by accounting for the distribution gap between Alzheimer's Disease and SCD datasets.	This work tries to solve a vital small-sample problem in medicine by means of domain adaptation. The paper is well organized, making their contribution clear.	(1) Feature adaptation is important for transfer learning, which is the study of this paper. (2) The paper is well organized, and the idea is easy to understand. (3) The performance is improved by the proposed method.	Very well organized Technically thorough Great experiment design - appreciate the authors investigating different source dataset; performing ablation studies and hyperparameter analysis and benchmarking with other methods Strong evaluation of results Neat figures	The technical contribution is moderate because the proposed mechanisms are existing and not nobel. The proposed method is not persuasive. The experiments should be more rigorous and thorough by comparing with the state-of-the-art methods.	(1) The proposed DSMA model is similar to the Inductive transfer learning (ITL) model except for the feature adaptation module, so the innovation of this paper is limited. (2) The paper employ the MMD based feature adaptation module to alleviate the inter-domain discrepancy, but still ignores the intra-domain data distribution gaps that may be caused by different imaging scanners and scanning protocols. (3) The references are not adequate and up to date.	-	NA	It is difficult to judge the reproducibility of the paper as the parameter settings are unclear.	Does not seem reproducible only because the 'AAA' dataset (as referenced in this work) is not publicly available.	The authors argue that this work could be among the first attempts employing domain adaptation for SCD progression prediction. However, many existing domain adaptation methods can also apply to the same task. In this regard, it needs to thoroughly compare the existing domain adaptation methods in the literature. In Eq. (2), the denominator should be $m\times n\times k$. It is unclear why is the tensor $A$ interpreted as an attention map? It is just a rescaled activation map with the averaged value of the small volumes. Note that large activation values don't necessarily contribute to the task-related important features. What is the rationale for using entropy loss $L_{E}$ over the bottom branch with unlabeled target data? According to the description, it doesn't even participate in the back-propagation. In the test phase, the top brain model is used. This case assumes that after training, the model learned to map the samples of sSCD and pSCD to CN and AD, respectively. However, to this reviewer's understanding, no mechanism makes such a relation in the proposed framework.	(1) The authors should give details of parameter settings of the proposed model and the competing models. (2) The author should clearly show how the experimental data is divided into training, validation and test sets. (3) Comparison with more transfer learning methods should be given.	Recommend the authors to compare attention maps obtained from the implemented attention module with other explainable AI method such as Grad-CAM - would be very interesting to see those results!	Lack of technical novelty Non-persuasive framework to solve the problem Lack of comparison with SOTA methods, especially, domain adaptation	The motivation of this paper is clear, the innovation is limited, and the experiments are complete	Excellent submission - well-written; technically sound; thoroughly evaluated; high-quality figures; extensive investigations such as ablation studies, hyperparameter anlaysis
170-Paper1693	DOMINO: Domain-aware Model Calibration in Medical Image Segmentation	This paper proposes that deep-learning models calibrated with domain aware model calibration (DOMINO), are more accurate than models not tuned with DOMINO. To test this medical image segmentation algorithms were chosen for this analysis. Specifically, algorithms that perform head segmentation. The DOMINO framework that uses the confusion matrix (UNETR-CM) outperforms both hierarchical class-based (UNETR-HC) and non-calibrated UNETR (UNETR-Base), in most instances, however both UNETR-CM and UNETR-HC outperform UNETR-Base. Calibrated models also outperform Headreco on all tissue classes except for gray matter and csf.	The authors put forward a sensible domain-aware loss function which leverages class similarity and hierarchy.		This is a novel approach to tuning deep learning algorithms that can help increase deep-learning model accuracy. Very well organized paper that was very detailed, enough to where the methods could be reproduced. This method could be used across multiple deep-learning models, even outside of segmentation. DOMINO appears to be useful across multiple deep-learning algorithm domains. The paper also displays a good understanding of the problem paired with a novel solution. The authors were able to demonstrate the validity of the DOMINO framework through extensive quantitative testing measures.	The paper is very well written, very elegant, well structured Experiments are convincing Motivation is quite clear		N/A	My main concern about this work is that the loss function now depends more on the training data. I wonder whether the performance on another dataset would then be compromised. The authors need to include experiments on other datasets acquired with distinct imaging considerations to show whether this is the case or not. Performance of network wrt to headreco: Fig 5 shows headreco can segment most regions nicely (except air). I wonder then what would a model trained on headreco with the domain-aware loss would have to offer?		The methods are very detailed. The derivations for the approach are shown, which makes this framework reproducible. Images are not publically available, however, all parameters used, such as repetition time (TR), echo time (TE), and field of view (FOV), are all listed, which means the study is also reproducible. The authors do say they will release DOMINO in the future.	Ok		Experimental and Results The 'Ground Truth' section, it is mentioned that 11 tissue types are being used. These 11 tissue types should be listed here as well. SOme were listed, however, all were not. All 11 do appear in the figures, but they should all also appear in the text. In the same section as above, the semi-automated segmentation routine used by the trained staff member should also be mentioned here. This helps to add to reproducibility of the pipeline, especially for training. In the 'Evaluation Metrics' section, when the author says 'brain segmentation', I think the authors meant to write 'head segmentation'	"Minor points Introduction: ""interpretability"" is not really addressed, consider removing this term. The way to compute the hierarchical class-based W matrix is unclear. The authors mention "" following this formula"", but I could not find any. Why is S set to 3? What is the effect of changing this parameter? Section 3.3. Unclear what the authors mean by ""due to its high variability between individuals"". All tissues would vary between individuals, wouldn't they?"		The authors introduce a novel approach to tuning deep learning algorithms. This approach could help improve model accuracy. The paper is very detailed and gives a step by step of the approach, the reason behind the approach, and does a detailed quantitative analysis on the output of models tuned, untuned, and against a widely used algorithm. This is solid work that can be expanded and used across many different deep learning algorithms solving different problems, other than segmentation.	"This work is very interesting, the proposal makes sense, and the manuscript very well written. There are a few items (""marketing"" and generalizability) that I think the authors should consider."	
171-Paper0021	Double-Uncertainty Guided Spatial and Temporal Consistency Regularization Weighting for Learning-based Abdominal Registration	The authors investigated deformation regularization weighting in context of Voxelmorph based non-rigid abdominal CT-MRI registration. They proposed to use a mean teacher approach to adjust dynamically the weights of the spatial and temporal consistency regularization based on the transformation uncertainty and appearance uncertainty. Experiments involving 10 intra-patient test CT-MRI scans showed that the proposed approach seemingly outperformed state-of-the-art SyN, Deeds and DIF-VM methods according to organ-wise Dice, average surface distance, deformation Jacobian evaluation metrics.	This paper presented a double-uncertainty guided spatial and temporal consistency regularization weighting strategy for the Mean-Teacher (MT) based registration framework, avoiding the grid searching for the optimal regularization weight.	This paper proposes a student-teacher model for image registration, a typically ill-posed problem.  Separate student and teacher models are implemented; the former is a typical registration network which features spatial regularization to improve the ill-posed nature of the model.  The novelty is in the teacher model (and its interaction with the student model), which imposes temporal regularization for consistency with the student model, and allows tuning of weights for spatial regularization in the student model.  Their method is evaluated on CT-MRI images of the abdomen from a partner hospital.	novel formulation of dynamic regularization weighting, exploiting temporal information across the training steps reported the state-of-the-art registration results for the 10 abdominal CT-MRI pairs well written and clearly organized manuscript	This work exploited the self-ensembling teacher model with the transformation and appearance uncertainty, avoiding grid searching for the optimal fixed regularization weight in the deep registration model.	The authors do effectively describe the method, which is able to produce uncertainty maps in the image registration process.  Adaptive weighting allows images which are more difficult to register to receive more attention in subsequent steps of the iterative algorithm.  Method outperforms other image registration methods in various metrics.	"the ablation study does not seem to have a clear conclusion the statistical significance of obtained ""VM (AS+ATC)"" results should be verified wrt. methods in comparison in order to make enable clear conclusions it should be investigated how the initial regularization weight impact the obtained final result (i.e. is hyperparameter tuning still required with the proposed approach?)"	The discussion on the mean field or images is required to justify the uncertainty computation. There lack some descriptions of the threshold selection and the loss function.	Description of the implementation was somewhat hard to follow, especially as it appears that it requires manual specification of numerous parameters (e.g., thresholds).  How might these things change for a different dataset?  It also seems like uncertainty results for registration are mainly based on 6 forward passes, which would appear fairly small.	PyTorch version not reported. Data description is scarce (no scanner info,  instructions to annotators, their degree and level of experience not reported). Review board approval is claimed to have been obtained, but is not referenced (id number). Placeholder for code link not included in the manuscript, but authors claim code will be made available upon acceptance. Relevant statistical significance tests not performed.	This paper has provided details about the models, datasets, and evaluation.	Data is not currently publicly available (awaiting IRB approval), no discussion of code availability and this seems like it would be fairly difficult to implement from scratch.	This is a well written manuscript that introduces a novel formulation of dynamic regularization weighting, exploiting temporal information across the training steps and reports the state-of-the-art registration results for the 10 abdominal CT-MRI pairs. The main drawback is that the evaluation is missing an analysis of statistical significance of reported differences in performance between methods, which would allow to make firm conclusions.	The average deformation fields and deformed images of N stochastic forward passes on the teacher model with random dropout were used to define the uncertainty. How to guarantee the mean field or the deformed image consistent with the ground truth field or image in the deformable registration? It would be helpful to discuss the mean field or images to justify the uncertainty computation. The thresholds \tau_1 and \tau_2 are important to update the regularization weight and the appearance consistency terms. We noticed that \tau_1 is ten times of \tau_2 in experiments. Whether the thresholds were set empirically? In Fig. 3, \lambda_{\phi} and \lambda _c seem to converge in the training process. How about the backbone network, such as the VM, used the converged weights? Variant II used fixed weights in the ablation study, while the selected values are not consistent with Fig. 3 (b, d). It would be helpful to describe the loss functions, such as the temporal consistency regularization L_c.	Some of the parameter settings seem fairly arbitrary, for instance thresholds for focusing on the most uncertain predictions, and would necessitate further study; are results robust to these choices?  I also don't know what the baseline runtime is for these procedures to compare the newly proposed methods to, just that they are faster.	novel training scheme for learning based registration with state-of-the-art results on 10 abdominal CT-MRI pairs well written manuscript	This work provided a double-uncertainty-guided spatial and temporal consistency regularization weighting strategy to relieve the weight tuning. Experiments on abdominal CT-MRI registration have shown promising advances by the proposed strategy.	This contribution could be deemed innovative, but perhaps requires more detailed study of various choices and model implementation.  It was not easy to follow details of the model itself.
172-Paper1456	DRGen: Domain Generalization in Diabetic Retinopathy Classification	The paper explores domain generalisation for the task of classifying grading (0 to 4) of diabetic retinopathy from retinal fundus scan. The proposed method is to average model weights identified at particular iterations of the training. An additional loss is added to reduce the covariance of the gradient across datasets. 4 datasets are used, with a leave one dataset out for testing.	In this paper, the authors address the problem of domain generalization in Diabetic Retinopathy (DR) classification. The baseline for comparison is set as joint training on different datasets, followed by testing on each dataset individually. The authors therefore introduce a method that encourages seeking a flatter minima during training while imposing a regularization.	The authors address the problem of domain generalization applied to retinopathy classification. The proposed method is build on Fishr regularization and the generalization capability is shown using 4 dataset with different dimensions. The averaged results show an improvement vs. SOTA of ~1%. The authors plan to share the github repository containing the source code for reproducibility.	The experimental design is sound (leave one dataset out for testing), the method is explained well and the paper is clear. The hyperparameters are reasonably well adjusted. Honest results showing heterogeneous improvement across dataset.	The proposed method is easy to implement.	The paper is clear and well structured. The description in the implementation details is helpful to understand the precise settings of the experiments. The manuscript addresses a precise problem and evaluate the method with appropriate comparison to baselines and the closest method, Fishr. It is the first time that this method has been applied on the chosen dataset.	No major weakness, however the results are a bit underwhelming. Performance on 3 datasets is clearly improved, whereas the performance on the 4th one, is markedly decrease. As a results the overall performance is only slightly above baseline. This is noted in the discussion but no explanation of why this is happening is provided. It would be worth investigating a few hypothesis.	The motivation behind the proposal is unclear.	I see 2 points as main weaknesses: it is not clear to me the contribution of this work with respect to the method. It is clear that the existing method was not yet applied to the chosen dataset, but what is exactly the difference vs. Fishr? is it the usage of stochastic weighted averaging? if it is, then this is not a new method, rather a variation of Fishr. the standard deviation measure is misleading. In this case the standard deviation quantifies the difference in performance for different testing datasets, however here it is not clear to me why a successful method should have less variability, as the different datasets vary in distribution and size. A standard deviation measure should be given to any result by re-running the experiments with different pseudo-random numbers, in order to quantify the improvement vs. variability; not quantifying the variability of one experiment across different settings.	Enough details are provided for reproducibility.	Good.	The implementation details together with the source code used to run the experiments, which the authors plan to share, address reproducibility. Assuming that the source code is going to contain a readme that explains how to run the code to get the same results of the experiments, reproducibility is fulfilled.	It would be great to at least speculate why is the performance so uneven across the dataset. Even better to conduct experiment to investigate this further. For example the dataset with degraded performance is the smallest one (with no grade 4). It should be easy to check whether the relative size of a dataset  or its imbalance are linked to their performance improvement, or lack thereof.	The motivation to develop such a regularization term should be described in detail. The theoretical guarantee of the influence of regularization term on the model training process should be presented. The comparison experiments are not enough to prove the superiority of the proposed method.	"Table2: would be good to keep absolute numbers only in the total images column and express the remaining columns as percentage. In this way the comparison of class distribution is easier for the reader. page5: coefficient gamma is not precisely defined, a precise definition would help the reader to avoid continuing comparison with the related work that present Fishr regularization. page5: ""We adapted Fishr [24] loss to enforce invariance based on the difference in co- variance matrices as represented in equation 2"" I find the same equation in Fishr paper (eq. 4) what has been adapted? Table5: Table 3 and 5 should use the same labels, i.e. instead of column Dataset Name, Testing Dataset Table4: as above, the column Accuracy is misleading, should be average accuracy. page8: in the discussion no possible explanation is given to the difference in performance Fishr - proposed method for Messindor. why has the proposed method for Messindor the lowest accuracy, if it was for Fishr the dataset with larger gains? page8: in the discussion it should be given more importance to give possible explanations on why the improvement vs. Fishr is measured. I can only find ""This is can be attributed to seeking a flatter minima empirically"", this should be explained in more detail in my opinion to make this work useful to the reader."	Interesting paper but analysis could have been more informative.	Theoretical analysis and experimental results are both insufficient.	Even if this work is not about a completely novel method in domain generalization, it shows to improve an existing method and it is the first time it is applied to the chosen dataset. Therefore, I believe that this work is interesting for the community, provided the comments above are addressed in the manuscript.
173-Paper1510	DS3-Net: Difficulty-perceived Common-to-T1ce Semi-Supervised Multimodal MRI Synthesis Network	This paper presents a Difficulty-perceived common-to-T1ce Semi-Supervised multimodal MRI Synthesis network (DS3-Net), which promotes the synthesis task by predicting a difficulty map. The predicted difficulty map can guide a pixelwise constraint and a patchwise contrastive constrain. Experiments on BraTS2020 dataset have been performed to investigate the effectiveness of DS3-Net.	1) To the best of our knowledge, this is the first semi-supervised framework applied to multimodal MRI synthesis for gliomas. 2) In light of the teacher-student network, we make full use of unpaired multimodal MRI data through maintaining consistency in spaces of both high and low dimensions. 3)They innovatively estimate a difficulty-perceived map and adopt it to dynamically weigh both pixelwise and patchwise constraints according to the difficulty of model learning, and thus the model can assign the difficult-to-learn parts (such as the glioma region) more attention. 4) Extensive comparison experiments are conducted, both quantitatively and qualitatively.	This paper introduce an multimodal MRI synthesis network to generate T1ce based on given modalities. By introducing an attention map, the proposed method can flexibly focus on the important area in the generation, and through a distillation procedure, the method incorporate more unpaired data to help this synthesis task.	There are two major strengths in this work: Multi-modal MRI synthesis is able to make full use of multimodal information for more accurate MRI synthesis, which therefore improves the performance over the traditional single-modal MRI synthesis. The paper is well organized and easy to follow.	They proposed a novel difficulty-perceived semi-supervised multimodal MRI synthesis pipeline to generate the difficult-to-obtain modality T1ce from three common easy-to-obtain MRI modalities including T1, T2, and FLAIR. Difficulty-perceived maps are adopted to guide the synthesization process of important regions, and dual-level distillation enables the model to train a well-performimg network with limited paired data.	The strengths of the paper are summarized as follows: 1) The authors introduce a semi-supervised framework to make full use of unpaired multimodal MRI data for T1ce generation based on the easy-to-acquire modalities, which demonstrates efficiency to save the cost of the paired data significantly. 2) The authors designs a specific attentional mechanism to relax the generation process so that the model focused on the important area from both pixel-wise and patch-wise.	The technical contribution of this work is not very clear. Some key parameter settings seem adhoc. The experimental results are not well explained.	The authors only tested the proposed method on the BraTS2020 dataset. More tests are required. Otherwise, the quantitive evaluations for the proposed method on Table 1 are not over all for other methods. The authors should apply the CUTGAN, pGAN, MedGAN and Pix2pix methods on the different Paired percentages. But, the proposed method are interesting for the clinical applications.	The main concern of this paper is their ablation studies is not enough, which is summarized as follows. 1) Regarding the attention map defined in Eq. (1), a small constant is set to 0.2 at x_i^T=0 lack of sufficient clear explanations e.g., some empirical validation results. 2) It seems that the authors do not show the performance of the plain DS^3-net by only preserving the GAN loss and the distillation loss in Eq.(4) and Eq.(7), i.e., LAGAN with distillation. 3) The hyperparameter is very intuitive in Eq.(4) and Eq. (7) where the authors do not shows how they motivated to be set. 4) It should be useful to show the gap of the model over previous baselines by also conducting their experiments with only 5% paired data in Table 1.	"The authors should provide more detailed explanations on the differences between their work and the existing semi-supervised learning frameworks, especially those based on the teacher-student strategy. Currently, it is hard to understand the true technical contribution of this work. The proposed framework seems a straightforward application of existing semi-supervised learning work in multi-modal MRI synthesis. Page 5: ""Here we empirically set l id , l fd , l pad and l GAN as 100, 1, 1 and 1."" How to determine these parameters? What is the influence once any of them is changed? Table 1: The results for the case of 100% paired percentage look odd. The second best method, Pix2pix, provides the best image quality with highest PSNR and SSIM, however, the corresponding segmentation results are not as good as DS3-Net. Why are two sets of results inconsistent? What is the underlying reason? Following the last comment, another strange thing is that the case of 50% paired percentage outperforms 100% case in terms of TC Dice, which is inconsistent with the conclusions observed in other results, i.e., larger paired percentage better TC Dice score. Please clarify. It is helpful to provide the model size (e.g., number of parameters) of different networks. Some failure cases should be provided in the manuscript. The authors should discuss the potential reasons for the failure of their model, which is helpful for the further improvement of the proposed model. Finally, the authors can consider to provide some results regarding the computational time of different methods."	Positive for the reproducibility of the paper.  It's easy to be reproduced, because they have shared the source codes on GitHub. But, the authors should describe more details about the network.	The training schedule including the learning rate and optimizers etc are not well clarified.	Good.	It's easy to be reproduced and this is very nice work. But the modality T1ce is not popular in the general diagnose. More details of T1ce imaging should be introduced for authors. Otherwise, the authors only gave some values of lamda parameters in the loss function. More tests of different values of lamda parameters should be discussed for other readers and users.	The authors should add more ablation study to well characterize the merits of the proposed method. Especially, the constant in Eq.(2) that is set to 0.2 is not well explained, since it actually affects the weighting in the remaining loss terms. Besides, the hyperparameters for most of loss terms are set by 100, 1, 1, which is also empirically due to the large value range. Addressing these concerns about DS^3-Net could further improve the  submission. I would like to raise the score if these critical concerns could be well solved.	The paper focuses on the multi-modal MRI synthesis task, which is interesting and has a number of practical applications on both research and clinical sides. The proposed method has some novelty and provides satisfactory performance. However, the experiments should be strengthened and more efforts should be made to clarify the contributions of this work. Considering these factors, I would like to give this paper this credit.	They proposed a Difficulty-perceived common-to-T1ce Semi-Supervised multimodal MRI Synthesis network (DS3-Net), involving both paired and unpaired data together with dual-level knowledge distillation to address issues related to acquire T1ce.	Overall the performance of DS^3-net with only 5% paired data and 95% unpaired data is appealing compared with the baselines with all paired data. The advices above could make this draft easy-to-follow and easy-to-generalize in other image synthesis tasks.
174-Paper0950	DSP-Net: Deeply-Supervised Pseudo-Siamese Network for Dynamic Angiographic Image Matching	The authors proposed a novel framework DSP-Net to automatically match the intra-operative X-ray fluoroscopic images to the dynamic angiographic images, which can provide doctors with dynamic reference images in PCI.	To solve a series of problems caused by cardiac motion during PCI treatment: DSP-Net realizes automatic matching of dynamic angiography images; The PSAD block is designed to successfully distinguish subtle frames, overcome the noise background, and enhance generality.	the first automatic approach exploring the task of dynamic angiographic image matching problem DSP-Net processes X-ray fluoroscopic and angiographic images parallelly in a proposed dataset.	The proposed DSP-Net processes X-ray fluoroscopic and angiographic images parallelly, achieving state-of-the-art performance on their medical image datasets. The designed PSAD block successfully distinguishes nuanced frames by learning the representative features and efficiently overcomes the noisy background, showing great generality to images from different sequences.	The method proposed in this paper is novel: Use the transmissing information and the compensating details to connect the two inputs, extract the significant features, and improve the accuracy; The comparative experiments of the methods proposed in this paper are sufficient: there are many similar methods compared, and the comparative results are more detailed.	The dataset is constructed, in which the performance is competitive compared to other methods. The depiction is clear with loss. Well written and well oeganized.	This study mainly focused on searching the matched image of the X-ray fluoroscopic image from the angiographic image gallery, and lacks the evaluation of time efficiency.	Incorrectly written order of references; Excessively long sentence structure; Lack of theoretical support for some of the descriptions; The experimental test data accounts for less;	More comparison with other SOTA Siamense structure is recommended. Registration methods literature review.	Most of the implementation details have been addressd.	The network architecture is clear and the algorithm logic is rigorous. The experimental part uses comparative and ablation experiments, which are rich in type. And the comparison test is full. But, the experimental datasets are not clinical data and the effectiveness of clinical use is debatable.	Easy to follow Dataset is expected to release. The method is clear.	The authors proposed a novel framework DSP-Net to automatically match the intra-operative X-ray fluoroscopic images to the dynamic angiographic images, which can provide doctors with dynamic reference images in PCI. The writting and organization of this paper is good. I suggest the authors to add the time cost of the proposed method for this matching task.	P1 : References throughout the paper are marked in a confusing order in the text. P2 line4-8: The sentences are too long and not easy for the reader to read and understand. P2 line3 from last: Lack of theoretical basis. Can you add a recent paper with similar content for illustration? P3 Fig.2.: Does 'gallery' have any special meaning? Can it be replaced by 'datasets'? P3 line2: Can you give a simple explanation for this? P3 line5: Can you add a relevant comparative test to confirm this benefit? P3 2.1 line7: Can you give specific comparative tests or add a paper with corresponding conclusions? P6 3.1 line6 74: The proportion of test sets is relatively small.	As the structure outputs a binary number, the authors are encouraged compare the intuition compared with GAN. More methods compared with non-rigid registration methids.	The authors has proposed an effective method to automatically match the intra-operative X-ray fluoroscopic images to the dynamic angiographic images. However, the segmentation of the guidewire and vessels is achived by an existing method, the projection method or the registration method has not been introduced clearly, and the time cost evaluation is not available.	The paper is clearly structured and logically sound. It has some bright spots, the experiments are adequate, and the experimental results have been improved compared to other methods. But, as the datasets are not from the clinic, the clinical use effect is uncertain, and there are also problems such as unclear description of some details and lack of supporting materials.	Clear contributions and organization. Solid work and contributions. Extensive comaparison.
175-Paper1389	DSR: Direct Simultaneous Registration for Multiple 3D Images	A method for simultaneous mono-modal rigid registration of multiple volumes is proposed, based on a bundle adjustment mathematical formulation. The properties of the presented algorithm are demonstrated on simulated and real TEE ultrasound data from six patients, and compared against a number of other related algorithms.	The paper proposes a novel simultaneous registration of 3D image data without requiring a reference image or features, specifically for Transesophageal Echocardiography. A predefined panoramic image is used to optimise the global poses of local image frames. The method shows promising results on both simulated and in-vivo datasets.	DSR (Direct Simultaneous Registration) approach is able to match 3D images without using the definition of keypoints. It is adapted to object (here organs) that have no easy keypoints to detect. The framework of the direct bundle adjustment is used. The first contribution DBA (Direct Bundle Adjustment) consists in redefining BA that jointly optimizes the poses of local frames and the intensities of the panoramic image (instead of 3D point positions in BA) and the second contribution is the proof that the optimization of poses is independent of the image intensities.	The simultaneous registration approach is expressed as a bundle-adjustment problem that allows for a Gauss-Newton solver for least-squares problems to be utilized. It's mathematical properties are thoroughly explained. This is an innovative idea that could be of interest to other researchers.	The paper is easy to follow Clinically very valuable Methodology has all essential mathematical details	Well Written Well Structured mathematically well-founded Rigorous evaluation Code available if the paper is accepted	There seems to be very little practical use for this method. For one, the improvement over other methods is marginal. Then, only the most basic portion of a product-grade registration algorithm is addressed, namely a straight-forward SSD cost function, without any overlap normalization, artifact considerations (portions of ultrasound volume will be occluded that are visible in others, which violates the SSD assumption of the intensity relationship); and most important some real-time motion handling, which provides for some of the most interesting research problems. Hence one could argue that the authors are solving a non-problem, despite the elegant mathematical formulation. No implementation details, runtime, and other computational resource information whatsoever is provided. So I will have to suspect that in light of the highly parallelized GPU implementations that are state of the art today, this method will fare unfavorably, by require dealing with huge sparse matrices and throwing a BLAS library at it (which might be hard to parallelize).	Paper needs additional clinical motivation w.r.t 3D Transesophageal echocardiography registration Few simulated cases and in-vivo cases	Lack of illustrations for the method	There is an algorithm diagram in the supplementary material which helps (it should be moved to the main manuscript), but most of the other implementation details such as numerical libraries, programming language etc are missing.	The authors should include more details on dataset. It would be great if they can release the dataset together with the code for reproducibility of the work conducted.	The code will be available after acceptation.	The mathematical proof in 2.2. looks elegant; however the outcome could be put in context better. To me it sounds intuitive that there is no dependency on the panorama intensities, since they are also reconstructed with a least-squares assumption, hence they will amount to the average value of all overlapping input volumes. Having a gradient iteration that is using the grid of that panorama image but not the intensities is nice - and the key idea of this paper it seems. Please provide implementation details and run-times such that readers know whether this is only a mathematical feat or something useful for their medical image analysis problem. The provided presentation video is very nice!	How are these panoramic image predefined even though the solution does not depend on this? Other 3D CT or MRI cases would be very relevant examples to see if this can be applied to other imaging Are the chosen transformations of +/-15 pixels and +/- 12 degrees realistic range. How did the authors come to these values? Could authors include some samples of panoramic images (predefined) that they have taken and how these look after registration? What is the estimated time for the proposed SE(3) pose optimisation? Multimodality version or changing pixel intensities during simulation can be interesting to include especially to validate that the method is not dependant on intensity variation as claimed.	The state of the art is presented simply and briefly: from direct pairwise registration (based on features or intensities) to multiple 3D volume registration in order to emphasize the main contribution in the field of bundle adjustment in registering an image collection. The contribution is conducted in two steps as explained in section 3: DBA based on panoramic images, which consists of an original way to solve the BA, and then the proof that this DBA, thanks to Gauss-Newton optimization, is independent of the image intensities. The contribution is straightforward but clearly presented and the demonstration of the independence to intensity is convincing enough.  The experiments were realized on synthetic data (5 simulated sequences of 3D grey heart images) and real data (46 TEE, Transesophageal Echocardiography images). The data amount tested is quite limited, however, regarding the real data, this is probably due to the difficulty of collecting this kind of data.  The proposed method is compared to four methods: the pairwise method, the Lie normalization method, the sequential method and the APE method.  This comparison seems relatively fair.	Nice mathematical idea, but (in light of missing computation times, and similar performance than existing methods) limit practical use.	The paper hold huge clinical benefit, however, the authors need to brush up their clinical motivation of applying this to 3D TEE image. The results looks promising for both simulated and in-vivo studies.	The work is interesting for a reduced community.
176-Paper0333	Dual-Branch Squeeze-Fusion-Excitation Module for Cross-Modality Registration of Cardiac SPECT and CT	The paper proposed a rigid multi-modality DL-based registration algorithm with application to SPECT and CT images. The SE block (Squeeze Excitation) is utilized in between the feature extraction for each of the SPECT and CT feature extraction pipeline. The dataset includes in-house 450 aligned pairs, which have been artificially deformed with a random rigid transformation for the evaluation. The proposed method is compared with a conventional method and several multi-modality DL-based registrations.	The paper proposes a potentially novel, dual-branch squeeze-fusion-excitation (DuSFE) attention module for feature fusion between cardiac SPECT and CT. The proposed attention module aims to explicitly model the feature fusion process between the two modalities. The proposed framework aims to directly utilize the spatial and channel re-weighting property of the squeeze-and-excitation networks (SENets) to better fuse multimodal information for image registration. The authors motivate the need for such a task specific feature fusion module due to a lack of cross-modal feature integration models.	This paper is well-organized, clearly written, and easy to follow. The novelty of increasing cross-modal registration accuracy by squeeze-fuse-exci both the channel and spatial dimensions is intuitive and interesting. The main motivation is well-supported by strong experimental evaluations.	1) Comprehensive evaluation but limited to artificial deformations. 2) The proposed method improves the performance for this rigid SPECT/CT registration task. 3) The paper is well written.	The proposed method claims to propose a completely novel attention module for early feature fusion between two different medical imaging modalities before registering them using a deep multilayered module. DuSFE module is takes two inputs from a parallel convolutional backbone catering to each modality. This layer then fuses and recalibrates the features into two stages - channel-wise squeeze-fusion-excitation (cSFE) followed by spatial squeeze-fusion-excitation (sSFE). This construction offers a potentially novel way of fusing multimodal features across spatial as well as channel dimensions.  Model performance was validated in an appropriately sized cohort of 450 individuals and the proposed scheme was compared with four different iterative and deep learning-based image registration methods.	This paper is well-organized, clearly written, and easy to follow. The novelty of increasing cross-modal registration accuracy by squeezing-fusing-excitating both the channel and spatial dimensions are intuitive and interesting. The main motivation is well-supported by strong experimental evaluations.	1) Novelty is limited 2) Lack of evaluation of real data.	The proposed approach simulates mis-registrations by manually introducing translations and rotations. It is unclear as to what mis-registrations were inherent to the dataset. How bad was the alignment between two images before adding translations and rotations? This leads to two cases: * Case - I: If there were mis-registrations before adding rotations or translations, why did the authors introduce them?  * Case - II: On the contrary, if the images were in reasonable spatial agreement, why did the authors simulate mis-registrations?  If the image were in good agreement, then mis-registering the images could possibly lead to a simpler task of learning rotations and translations? Moreover, if the analysis cohort does not exhibit naturally occurring mis-registrations, would it be useful to evaluate the model over a cohort that has some natural mis-registrations?	The detail illustrations of figures and tables are severely insufficient. i.e., the captions of figures and tables are too simple and need to be enriched. The difference in the visualization results are not clearly to see. Please update the figures with zoom-in view highlight.	All looks good. It would be nice to make several cases in the dataset publicly available.	This could be assessed by looking at the source code repository which is currently anonymous.	It is easy to reproduce since the authors will provide the code upon acceptance and the description is clear in the paper.	1) It is not mentioned how the ground truth transformations are generated. Are the SPECT and u-maps aligned manually? How are the ground truth transformations validated? 2) It would be helpful to add the original size and voxel spacing of CT images. 3) It would be nice to clarify (a few sentences) how the proposed method can be applied to a real case scenario, especially when CT and SPECT images are taken with different size and voxel spacing. 4) Is there any intensity augmentation used during the training? If not, it would be nice to mention that explicitly. 5) The squeeze excitation module has been used in several DL-based registration publications but not in the context of multi-modality registration. It would be relevant to cite a few of those. (not necessary for this conference paper)	It would be great to clearly state that the paper focuses on rigid image registration and not deformable image registrations? Could there be potential deformations as well in case of SPECT vs. CT that might needed to be modeled. Also, authors should compare their method with at least one approach that specifically focuses on feature fusion.	This is a very good submission with enough technical contributions and strong experimental validations. However, there are some minor problems need to be solved: -- It is better to put subsection 2.1. Dataset and Preprocessing and subsection 2.4 Implementation Details to section 3. Results. It will be more concise to describe only the method design in the section 2. Methods. -- Please enrich the caption of the Fig.1. and 2. -- The difference in visualisation results in Fig.3. is not straightforward to see. Please use some signs or zoom-in window to highlight the different regions. Also enrich the caption would help to understand this figure too. -- There is a third kind of registration encoder, which use a weight-shared encoder (means one encoder) to extract CNN from the moving and fixed images. I would suggest to discuss this third kind in the literature review part too. [1]. Liu, L., and et al.: Contrastive registration for unsupervised medical image segmentation.	The proposed method achieved the best result for the rigid SPECT/CT registration task over an in-house dataset  compared to current multi-modal approaches. Although no evaluation has been performed on real data, the artificial transformations look sufficient and promising for this conference paper.	It would be great to clearly state that the paper focuses on rigid image registration and not deformable image registrations? Could there be potential deformations as well in case of SPECT vs. CT that might needed to be modeled. Also, authors should compare their method with at least one approach that specifically focuses on feature fusion.	This is a very good submission with clear organization, interesting novelty, and strong experimental validation. I would recommend acceptance of this paper.
177-Paper0838	Dual-Distribution Discrepancy for Anomaly Detection in Chest X-Rays	In this manuscript, the author proposed a dual-distribution discrepancy for anomaly detection. This paper is the first work that includes the unlabeled normal and abnormal images in training to improve abnormal detection. The abnormality is evaluated by two evaluation metrics: intra- and inter-discrepancy. The experiments on two benchmarks show state-of-the-art results and observed the increasing AUC by including more abnormal data in training.	This paper proposes a new strategy method for anomaly detection based on labeled and unlabeled data, which is a novel idea. Experiments show the effectiveness of the method. This will change the traditional way of thinking about anomaly detection.	Previous anomaly detection papers often consider the problem as one-class classification using only normal images. The authors in this work propose to leverage both normal and unlabelled images containing anomalies for training to perform more accurate anomaly detections.	This is the first work to include data with unlabeled abnormal and normal images. A dual-distribution method is proposed to learn the inter-and intro- discrepancy between the two reconstructed images: from the network trained with unlabeled image datasets and normal image datasets. The proposed method achieves state-of-the-art results demonstrating the effectiveness of involving abnormal data in uncertainty training.	This paper can use unlabeled data and labeled data for anomaly detection, which will greatly improve the efficiency of anomaly detection, and is also very consistent with the actual situation of clinical application data. The theoretical formula expression is also relatively clear, and the comparison experiment and ablation experiment both demonstrate the effectiveness of the method.	The paper is mostly well-written and clear. The method achieves good performance for anomaly detection in chest x-rays. Most of the aspects are described in sufficient detail to enable the reproduction of results. Authors claim this is the first time that utilizes unlabeled images to improve the performance of anomaly detection.	"The word 'label' is not defined, maybe refer to the label of normal and abnormal? It can be confused because the label can also be defined as the lesion annotation label. Similarly, in the introduction, ""To the best of our knowledge, it is the first time that unlabeled images are utilized to improve the performance of anomaly detection"". As all the unsupervised methods use the unlabeled data, 'unlabeled images' may confuse. In training, if model A trained with a pure normal image, will the input of the abnormal image be failed with both normal images generated? If model A trained with all abnormal images, will the normal image be failed to be detected as abnormal images with high inter-discrepancy as one abnormal and the normal image generated? To compare with the SOTA methods, why the AS intra outperforms the AS inter with such large margin, but opposite scores observed for RSNA dataset? By training both the normal and abnormal cases, will the robustness be affected by the limited abnormal data the network has seen? Minor: The fonts in Fig. 3 are too small to read."	The full writing and abbreviations in the thesis should be consistent with each other. There are abbreviations in the front and try to use abbreviations in the back. The full writing and abbreviations should not be repeated. How K was chosen, please explain. There are some spelling mistakes, please note, such as fisrt time. Regarding the selection of training data sets, when comparing with other optimal methods, use data sets that are common to other methods for comparison. Otherwise, the conclusion drawn is very likely to be overfitting. Please find the datasets used by the other best methods listed in the article, and do experiments to compare the results. The code used in this paper needs to be open-sourced to demonstrate the reproducibility of the method, or to provide relevant proofs.	"The proposed setup is similar to noisy-label learning, it would be better if authors can explore some of the baselines from noisy label in this problem. In practice, collecting a large number of normal images can still be time-consuming, it would be better if the authors can show the performance with fewer normal training images (e.g., hundreds instead of thousands) In figure 2, the comparison is a bit unfair given that the proposed model contains much more learnable parameters (K AEs for module A and K AEs for module B) than the baseline autoencoder. It would be better if the authors can compare with some recent SOTA anomaly detectors from 2021. Some of the references are missing [1,2,3,4] Reference: Tian, Yu, et al. ""Constrained contrastive distribution learning for unsupervised anomaly detection and localisation in medical images."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021. Dey, Raunak, and Yi Hong. ""Asc-net: Adversarial-based selective network for unsupervised anomaly segmentation."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021. Marimont, Sergio Naval, and Giacomo Tarroni. ""Implicit field learning for unsupervised anomaly detection in medical images.""  International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021. Chen, Yuanhong, et al. ""Deep one-class classification via interpolated gaussian descriptor."" AAAI 2022."	The author claims to release the source code, and the paper provides enough details to reproduce the paper.	Given the uncertainty in the choice of datasets, more material is needed to demonstrate the reproducibility of the method.	Authors claim to provide codes based on the reproducibility list.	The detailed suggestions are integrated to the limitation session: 1) the definition of the unlabeled data 2) the question about the training cases in model A 3) the detailed explanation of the experiments 3) concern about the robustness of this model.	The full writing and abbreviations in the thesis should be consistent with each other. There are abbreviations in the front and try to use abbreviations in the back. The full writing and abbreviations should not be repeated. How K was chosen, please explain. There are some spelling mistakes, please note, such as fisrt time. Regarding the selection of training data sets, when comparing with other optimal methods, use data sets that are common to other methods for comparison. Otherwise, the conclusion drawn is very likely to be overfitting. Please find the datasets used by the other best methods listed in the article, and do experiments to compare the results. The code used in this paper needs to be open-sourced to demonstrate the reproducibility of the method, or to provide relevant proofs.	The paper is well written and novelty can be deemed as sufficient. There are a few issues can be addressed during rebuttal.	The paper proposed novel uncertainty estimation methods by utilizing the unlabeled data for uncertainty and normal cases. This is the first work to do so and provide an alternative way to explore the uncertainty estimation for medical analysis in detection. The proposed methods reach state-of-the-art performance, which is promising and the ablation study shows the performance increased while including more diverse data.  A concern about the robustness of this model as the abnormal data is applied for training.	This paper is generally good, and has a new perspective on this field. But it's better to be able to answer my related questions and be accepted for publication.	The paper is well written and novelty can be deemed as sufficient. Some of the weakness can be addressed. (See above for details)
178-Paper1476	Dual-graph Learning Convolutional Networks for Interpretable Alzheimer's Disease Diagnosis	This paper proposed a dual-graph interpretable GCN to classify AD-NC and related MCI. And further, the proposed method could identify AD-related biomarkers.	In this paper, dual-graph learning framework is proposed in the GCN context which has mainly three components  including graph construction, dual-graph learning and graph fusion. Proposed framework is being utilized for the early AD diagnosis	The article proposes to derive interpretable deep learning classifier (for Alzheimer's disease) by jointly investigating subject and feature (structural ROIs) diversity. The model relies on jointly performing subject graph learning and feature graph learning within a graph convolutional network. The proposed method is more accurate than several baselines when applied to the cortical scores extracted from MRIs acquired by the Alzheimer's disease neuroimaging initiative (ADNI) datasets.	This paper makes a clear description of the dGLCN, which is a novel method to fuse the dual graphs from different group participants. The paper makes a detailed ablation analysis that make the method valid.	Problem is being posed in the context of dual representation learning. Graphs are being constructed at both the subjects and features level.	While the idea of accounting for noise in the labelling of subjects and brain ROI measurements is not new (see for example Adeli et al PAMI 2020 https://ieeexplore.ieee.org/document/8653343), doing so within a deep learning framework is intriguing. The authors nicely develop and carefully explain the model. The experiments are thorough especially for a conference paper. They consist of t-tests performed on the accuracy scores derived from repeated cross-validation applied to several classical machine learning approaches and more advanced deep learning models, and contain an ablation and parameter sensitivity study. Also the selection of ROI measures based on multiple trials is nice.	Fig.2 is small. The version of tensorflow and GPU should be provided.	"Correlation is the metric being used for construction of subject graph and the feature graph.  Is it optimal metric as inherent normality assumption is always there in this metric? Moreover, how to justify this statement statistically ""correct correlations among the data are captured""?"	The corresponding critics is minor in nature. Given that the authors include a sparse learning approach in their comparison, they should consider including the approach by Adeli et al. The description of the data set is too terse - it should at least include sex. Given that the topic is interpretability, the authors should at least discuss how confounders, such as sex, could influence their findings.	The authors provided the code that make it is reproducible.	Code is being provided for reproducibility	manuscript should specify which ADNI Data release was used for analysis	This paper proposed a new method to classify AD-related groups and outperformed than other methods. It's a good idea to provide more neuroimage information to make the interpretable method more substantial in Fig. 1.	Please define abbreviations everywhere in the text wherever they are being used first. What is n and sigma in equation 1. Similarly, at others places variables are being used without defining them. How the kNN being used for graph generation? More detail is need in this context. Graphs are being updated after each and every layer. What does this signify? Practically, graph is being formed on an initial data which is than carried forward to all the layers in GCN. Context of equ. 4 is not clear. Please explain. How the lambda_1 and lambda_2 parameters are being tuned? How the size of architecture is being defined? Will A' and S' actually correspond to identifying interoperability? Did the model convergence? It seems very less samples being considered in each class. Comparison on other public dataset with large samples is required in order to check generalization of framework.	see above	This paper is well organized and give a new interpretable method, which is interesting and novel.	Overall the paper is well written and easy to follow. My one major concern is lack of enough samples while training and testing and use of conventional correlation metric for generating graphs.	see above
179-Paper1430	Dual-HINet: Dual Hierarchical Integration Network of Multigraphs for Connectional Brain Template Learning	In this study, a dual graph convolutional network architecture is proposed to learn connectional brain templates of brain multigraphs, which learns multigraph representations at node-level and module-level simultaneously.	In this paper, the authors propose a novel method, namely Dual-HINet, for dealing with brain multi-graphs while considering the hierarchical structure of neural interactions. The framework consists of several major steps, from which they seem to firstly construct node-level and cluster-level embeddings simultaneously, which are fused together for final analysis. They also use 4 large-scale connectomic datasets (ABIDE I) to validate the performance.	This paper proposed a Dual Hierarchical Integration Network (Dual-HINet) to simultaneously learn the node-level and hierarchical cluster-level integrations for the connectional brain template (CBT). Through the proposed dual GCN block, the proposed method can group the nodes through hierarchical layers based on their multi-edge interactions. The subject-specific CBT is derived from the concatenated node-level and cluster-level embeddings. Finally, the population CBT is generated by taking the median of all the subject-specific CBTs in the training set.	Node-level embeddings and hierarchical module-level embeddings are fused to generate multigraph representations.	The motivations for constructing brain multigraph in a hierarchical way are sound, and the methodology introduced in this paper is valid, and with sufficient novelty. Experiments also show the validity of the proposed method.	It is very challenging to simultaneously learn the node-level and hierarchical cluster-level integrations to produce the  subject-specific CBT.	The proposed method was only evaluated on brain images of single modality (structural), instead of evaluation on different types of brain connnectivity from multi-modality data, such as fMRI and sMRI. The generated CBT was only evaluated in terms of representativeness and topological soundness. It is also important to evaluate if the generated CBT can better capture inter-subject differences and the associations between brain connectivity and phenotypes.	The organization of the paper needs improving, as there are many repetitive parts in the paper.	It is unclear what is the 'hierarchical structure of neural interactions'. It is recommended to give a specific definition to avoid confusion. The 'hierarchical structure' is obtained through the block C in Fig. 1. It is recommended to give more clinical/medical intuitive how and why this learned clustering-level hierarchical structure helps building a better CBT. It is unclear how GCN works in the proposed method. Typically, GCN requires a fixed adjacent matrix to perform the graph convolution. However, the proposed method inputs multiple graphs and there are multiple GCNs modules in the proposed method. It is recommended to give more technical details about how each GCN works to make the proposed method more convincing. There is a 'predefined number of clusters in the 'C) Hierarchical multigraph clustering'. It is recommended to give a detailed discussion about how this predefined hyperparameter impacts the model performance. The proposed method is very complex, with multiple GCN and clustering modules. However, the training set is relatively small compared with the proposed method's complexity. It is recommended to give a more detailed discussion about how and why the proposed method can be trained on such a small dataset without overfitting.	Good.	The main idea and the major steps are provided and described, current contents seem to be OK for reproducing, but it cannot be fully confirmed unless we really dig into it. Publishing source code will be much welcomed which can greatly help in understanding the proposed method.	As mentioned in the weaknesses, a lot of technical details are missing making the reproducibility less convincing.	For the hierarchical multigraph clustering, it will be helpful to visualize the assignment matrix to see if the learned clusters are biological meaningful. How does the clustering loss L_d affect the performance? It is better to provide an ablation comparison regarding this. Both sMRI and fMRI are available in ABIDE dataset, not sure why only sMRI was used. Morphological connectivity is usually computed as region-wise correlation of multiple morphological features, instead of using one single feature seperately. Learn CBT from multi-modal data will be more promising. Further evaluation are needed to see if the generated CBT can better capture inter-subject differences and the associations between brain connectivity and phenotypes.	"Contents in the last paragraph of Section 1, the start of Section 2, and the rest parts are repetitive. It is like the authors have introduced their framework based on Fig.1 by three times, and some contents are overlapped with each other. It is suggested that you provide only a brief introduction of your method that focuses on the main idea in Section 1, while in Section 2 focuses on the detailed descriptions of each major step. Similar contents in the three parts of the paper should be reduced.  In Fig.1, it is suggested to exchange the location of subgraphs E and F, so that the order of these subgraphs can be organized in a more natural way. In the second line of the second paragraph in Section 3, what does that ""AD"" stand for? Should that be ""ASD"" instead? It is also suggested to provide table to describe the contents in Fig.2 instead of figure, which can provide more accurate information between different configurations and the DGN. It is also unknown why they choose ABIDE I for validate their performance, since it is specifically collected for ASD disease. As this method is more like a general brain network analysis method, using only one dataset might not be sufficient to fully validate it."	Please refer to the weaknesses for details.	The proposed method to learn CBT from brain multigraphs is interesting, but further evaluation is needed to demonstrate its effectiveness.	The novelty and the methodology is good, but the organization of the paper needs improving.	The proposed method is very complex with multiple GCN modules and clustering modules. However, the current writing of this paper did not provide enough technical details making this paper less convincing.
180-Paper1066	DuDoCAF: Dual-Domain Cross-Attention Fusion with Recurrent Transformer for Fast Multi-contrast MR Imaging	Towards better multi-contrast MRI reconstruction, this manuscript proposes a dual-domain cross-attention fusion mechanism to make full use of a reference image, and a recurrent transformer to remove the non-local aliasing artifacts.	The authors present a novel dual-domain cross-attention fusion network with recurrent transformer for fast multi-contrast MR imaging. The cross-attention fusion scheme enables deep and effective fusion of features extracted from two modalities. The dual-domain recurrent learning allows the proposed model to restore signals in both k-space and image domains by removing the artifacts effectively. The recurrent transformers can capture long-range dependencies from the fused feature maps for improving reconstruction performance.	This paper proposes a dual domain deep learning framework for MRI multi-contrast super-resolution. According to the results, the proposed method can generate superior results when compared with some other methods.	Manuscript is well organized and easy to follow. Motivation is clear and persuasive. Methods are well formulated and presented. Experiments are comprehensive and informative. The proposed method is compared to various baselines and is evaluated on different sampling strategies and accelerating ratios.	1) The cross-attention fusion mechanism fuses the features extracted from two contrast images in a bidirectional way to harness complementary information of these two contrasts. 2) The residual-reconstruction transformer to model the long-range dependencies based on the fused feature maps in both domains to counteract aliasing artifacts and faithfully reconstruct the target images. 3) The recurrent dual-domain learning makes the reconstruction results more interpretative.	The paper is well organized and has presented enough figures and tables to support authors' ideas. It is novel of the proposed dual domain network.	Experiments are conducted on real-valued single-coil MRI dataset (i.e. simulated dataset), which it is far from real scenarios where reconstruction algorithms deal with complexed-valued multi-coil MRI. Also, it is not clear how this proposed method can be extended to multi-coil MRI. This should be clarified. The proposed method contains a so-called Swin Transformer Layers, but from my point of view, the key ideas of swin transformer (patch merging and W-MSA/SW-MSA) are not mentioned in the main text. If I misunderstand, patch merging and W-MSA/SW-MSA	1)The authors need to analyze the computational complexity of the proposed method in comparison with baseline methods. 2)The authors need to discuss the impact of the differences between MR images from healthy subjects and those from patients on the performance of the proposed method. 3)The authors need to provide in-depth discussion on the consistence of multi-contrast image synthesis obtained by the proposed method.	In k-space CAF, feature maps are cropped into smaller patches and then embedded and input into RGT and TGR. Mathematically, each data point in k-space will contribute to every pixel in the image domain after Fourier transform. Please example the reason for cropping k-space. It is very interesting to know the improvement of each recurrent block. Please consider replacing Fig, 3 to show results from each recurrent block. In the ablation study, please provide more details on the network structure design for each experiment show in Table 2 as the statement of w/o CAF, w/o DD in the paper is confusing.	Although some details are missing, it is still reproduceable, considering that the proposed methods is well presented, and authors promised to release code after acceptance.	The results of this paper may be reproducible.	Reproducibility is good.	It is not clear to me how to extend the proposed method to complex-valued and multi-coil MRI, which is more practical in real MRI reconstruction scenarios. (c.f. main weaknesses of this paper.) Also, authors claim the proposed method is evaluated on fastMRI dataset, which is somehow misleading. Usually the fastMRI dataset is reference  to the raw complexed-valued multi-coil data, while actually only simulated data are used in this manuscript. Extending the description of  residual swin transformer block (RSTB) would make this method more clear. For example, details like strategy patch merging and W-MSA/SW-MSA  in Section 2.3. Is the TGR sub-module necessary in the proposed CAF module? It seems that the target of multi-contrast MRI reconstruction is to borrow information from reference image to target image, why the proposed method needs a target guided reference (TGR) sub-module? I wonder why authors still need CNN layers instead of powerful non-local Transformer layers at the end of RRT and CAF modules in Fig. 1? In the context of Transformer, element wise addition and linear layers are preferred for image information fusion and feature extraction. Minor Comments Inconsistent under-sampling mask ($k_u$, Cartesian mask) and artifacts (Radial mask) in intermediate reconstruction result ($\tilde{x}_{u_1}$) in Fig. 1. Content and contribution of section 2.3 is too weak for a subsection. This subsection should be further extended or simply merged/scattered into other sections.	"1)The authors should provide the analysis on the computational complexity of the proposed method. 2)The authors need to explain whether the proposed method has consistent performance for different modalities.  3)The authors need to give clear explanations on whether Individual health status affects the performance of the proposed method.  4)Font size of Fig. 1 is too small. 5)There are a few typos and grammar errors, such as ""the attention outputs is introduced by using one contrast image "", ""The TGR runs in the same way and thus form a"", "". In i-th image domain restoration block"", ""groups in out network"" and ""which means that learn the interaction between two different contrasts step by step is optimal""."	This paper proposes a dual domain deep learning framework for MRI multi-contrast super-resolution. According to the results, the proposed method can generate superior results when compared with some other methods. The paper is well organized and has presented enough figures and tables to support authors' ideas. However, there are some major concerns. In k-space CAF, feature maps are cropped into smaller patches and then embedded and input into RGT and TGR. Mathematically, each data point in k-space will contribute to every pixel in the image domain after Fourier transform. Please example the reason for cropping k-space. It is very interesting to know the improvement of each recurrent block. Please consider replacing Fig, 3 to show results from each recurrent block. In the ablation study, please provide more details on the network structure design for each experiment show in Table 2 as the statement of w/o CAF, w/o DD in the paper is confusing.	The organization, motivation, novelty look good to me. For experiments, while selection of dataset has some defects, other parts, the baselines and under-sampling strategies are still reasonable and comprehensive.	The authors need to provide enough experimental data for method validation.	Novelty, experimental design, result presentation.
181-Paper0434	Dynamic Bank Learning for Semi-supervised Federated Image Diagnosis with Class Imbalance	This paper aims to address the challenging problem of class imbalanced semi-supervised FL. It proposes a  dynamic bank learning scheme consisting of the dynamic bank construction and the sub-bank classification. Expensive experiments on two benchmark datasets demonstrate the superior performance of the proposed method.	This work proposes a dynamic bank learning scheme to address the class imbalance in the semi-supervised FL. This scheme consists of two parts at the client side, including the dynamic bank construction to distill various class proportions for each client, and the sub-bank classification to guide the local model to learn different class proportions. On two public datasets, the method achieves remarkable improvements.	This paper aims to achieve effective class-imbalanced semi-supervised federated learning, where the server has labeled data and the clients have only unlabeled data. To this end, this paper proposes a dynamic bank learning method to leverage the class proportion information. The dynamic bank construction distill class proportions for each client, and a sub-bank classification task is used for local training. The proposed methods are evaluated on two datasets and show improved performance over existing works.	The idea is relatively novel. The overflow of the proposed method is clear. This paper is well written. The results of the proposed are very good.	This work aims to address the class imbalance in the semi-supervised FL, which is an interesting task. Moreover, they consider the server with a small amount of labeled data and all clients only have unlabeled data, which is a relatively new setting in medical imaging. Significant performance improvements. The work is well written.	The idea of using bank learning as a proxy task on clients is novel and interesting. The performance improvement is effective compared with other methods.	Some figures and equations are not well explained,like: In Fig.1, it is not clear what is the \pi_1\cdots \pi_k. Additionally, it is better to simply  explain the overflow in the caption. In Eq. 2, it is better to explain the function 1(\cdot).	The technical improvement is limited to the local training at the client side. In the imbalanced case, accuracy is not suitable as a primary analytic metric. The experimental results in Fig. 2 (d) are somewhat strange.	The labeled data on the server is a subset of the client-collected data, which can raise privacy concerns.	This paper has shown the implementation details. Additionally, this paper has provided the codes in the supplemental material, and it will release the codes. So I think its reproducibility is very good.	Hyper-parameters are complete. Authors are going to release the code. The details of thresholds h_m/h_c may be improved.	The description of the experimental setup and the hyperparameters are clear, which is sufficient to reproduce the reported results.	"Some figures and equations are not well explained,like: 1). In Fig.1, it is not clear what is the \pi_1\cdots \pi_k. Additionally, it is better to simply  explain the overflow in the caption. 2). In Eq. 2, it is better to explain the function 1(\cdot). It is better to simply analyze the reasons that why the proposed method has superior performance over the comparative methods in the experimental section. The paper might miss some related works as follows: [1] Laine, Samuli, and Timo Aila. ""Temporal ensembling for semi-supervised learning."" ICLR  (2017).  [2] Shi, Xiaoshuang, et al. ""Graph temporal ensembling based semi-supervised convolutional neural network with noisy labels for histopathology image analysis."" Medical image analysis 60 (2020): 101624."	"According to the S_k of the experimental setting, the server side has a small number of class-balanced samples, which is a huge advantage compared to other semi-supervised FL algorithms in comparison. However, the proposed method only improves the training on the client side, without making full use of the server-side knowledge provided by the setup. Compared to the upper bound of performance, there is still a lot of room for improvement. Author should introduce the analysis in Section 3.3 is performed on which dataset. Furthermore, accuracy is not suitable as a primary analytic metric due to the severe effects of imbalance, such as the large difference in numerical values between accuracy and F1. The experimental results shown in Fig. 2 (d) are not intuitive. (1) When S_k=15, the standard deviation is reduced significantly compared to the adjacent values. What makes the method significantly improve the robustness at the selected S_k? (2) In addition, a larger S_k (i.e., more class-balanced samples on the server side) should not hurt the performance of FL, why is the performance apparently lower than 10 and 15 when S_k=20? It seems the method requires the assumption of IID for data at server and clients. In other words, the method does not consider the data heterogeneity (shift on p(x)) among participants. In practice, due to the small number of samples on the server side, data heterogeneity will inevitably be involved. Since the Dirichlet distribution is used to construct the imbalanced clients, authors should state the work is verified as a simulation of imbalanced scene to avoid potential ambiguity with ""real-world use"". Does this work use the same threshold (h_m/h_c) for clients with different class distributions? If so, this may not be reasonable."	When building the memory bank by Eq. 2, the threshold for the majority and minority classes need to be different. However, the class distribution may be unavailable and which class is the minority class may also be unknown. When estimating the class prior for each sub-bank, it is assumed the class distribution of each sub-bank are not exactly the same. However, the sub-banks are split from the bank by random splitting, which may have the same class distribution. If the data are distributed among client by a i.i.d. distribution, does the proposed method still outperform other methods? The proposed methods require training on the server, but the baseline methods such as FedAvg do not. Could you please add this server-side training to FedAvg-FM and compare the results? In Fig. 2 (e), it is interesting to see that with the increasing number of unlabeled clients, the accuracy of the proposed methods increase. Intuitively, with more clients, each client will have less number of samples and degrade the accuracy. Could you please explain more on this result? Some related works on self-supervised federated learning [1][2] need to be discussed. [1] Federated Contrastive Learning for Decentralized Unlabeled Medical Images, MICCAI 2021 [2] Federated Contrastive Learning for Volumetric Medical Image Segmentation, MICCAI 2021	Well written. The idea is relatively novel. Good results.	The interesting task and setting for medical FL. Significant performance improvements. Improper primary metric for evaluation.	The overall quality of this paper is good. The only concern is that the server needs data for training, which does not follow standard FL protocol and raise privacy concerns. Besides, in the experiments, the labeled data on the server seem to be a subset of the client-side data, which may cause privacy leakage.
182-Paper2304	EchoCoTr: Estimation of the Left Ventricular Ejection Fraction from Spatiotemporal Echocardiography	This paper aims to estimate the left ventricular ejection fraction (LVEF) from 2D echo sequences. To do so, the authors adapt the existing UniFormer architecture with the objective of overcoming the limitations of CNNs and vision transformers for this type of task, therefore leading to a convolutional transformer. They demonstrate their methods on 10.000+ sequences from the EchoNet public database, focusing on 4CH views. Extensive comparisons with state-of-the-art methods and architectural choices are performed, demonstrating improved performance in terms of MAE and correlation.	In this article, a-convolutional transformer (EchoCoTr) is proposed as a method that combinies vision and CNN transformers to analyze echocardiogram video sequences and generate LVEF prediction. Deep learning networks require a fixed number of video samples, to obtain them they are taken at uniform frequencies, and authors proposed to use images from the end of systole and diastole images. The EchoCoTr architecture learns local features without avoiding redundancy in adjacent images while capturing global information through video. The results show that EchoCoTr can train with little information and give better or comparable results to other models such as EchoNet-Dynamic, BERT, DistilBERT and ViT although they also show that the model results are affected by the way the samples are taken in the model. video.	The authors present an application of the UniFormer network to the task of LVEF prediction. The results outperform existing approaches by a very small margin.	Relevant architecture overcoming the limitations of two types of state-of-the-art methods Use of a known and publicly available large dataset Extensive evaluation against state-of-the-art methods and architectural choices	There is a novel formulation in the use of transformers to obtain ejection fraction of the heart's left ventricle.  Authors compare their new proposal with known ones and in all give quantittative results, comparing each other and demonstrate the advantages of using transformers.  They clarify the clinical background importance.	The results of the paper outperform the state-of-the-art approach, which was using CNNs. The architecture used here, while not new, uses a combination of transformer and convolutional blocks. It beats the existing transformer-based approaches by a large margin. The results are supported by a solid ablation study and compared to the relevant literature. The network architecture used by the author had never been tested for this specific task before.	Strong reserve on the applicative impact of this work, given the current performance of LV segmentation including for echocardiography. Limited methodological originality, although the authors perform extensive evaluation.	Although it has a first use of the transformers to calculate the ejection fraction, is  looks the method still  does not possess such convincing results when compared to the other methods. And the theoretical part is not so strong to demonstrate the reasons of the different experiments.	The novelty of the work is limited. The authors took the existing UniFormer [1] architecture and adapted it to their task. It is unclear how efficient this approach is compared to the other state of the art. [1] Li, K., Wang, Y., Gao, P., Song, G., Liu, Y., Li, H., Qiao, Y.: Uniformer: Unified transformer for efficient spatiotemporal representation learning (2022)	Use of a large, well-known, and public dataset (EchoNet). Code will be available if the paper is accepted.	If authors get to publish their code, it can be reproduced. This paper it is strong in this aspect, database is publicly available also. And details on language and platform is given.	The data used by the authors is public and the code will be made available upon acceptance. The higher-level parameters used by the authors are listed in the paper. It is mentioned that the UniFormer model was adapted, but it is not clear what changes were actually performed. The paragraph detailing this (end of page 4) is not clear enough and may be not complete. Overall, the reproducibility should be excellent once the github is made public.	"As said in the weaknesses, the applicative impact is in my opinion limited, given the performance of current segmentation and tracking methods. I also wonder if better impact would be reached by focusing on assessing the dynamics along the sequences, instead of trying to (slightly) improve the performance on a rather classic problem (estimating LVEF). The network is supposed to select the most representative frames to estimate LVEF. I wonder how it behaves on cases with abnormal motion, and in particular little motion. Writing could be revised on several aspects: The Title and in particular ""spatiotemporal echocardiographic assessment"" may be revised to better fit what is actually proposed. Abstract: the sentence ""However, according ..."" is rather vague and could be revised. I have similar remarks for other parts of the paper: beginning of the Introduction, section on LVEF in the center of p.2 p.2: ""adopted"" should be ""adapted"""	The database has been extensively used and now authors provide a new method and a good amount  of experiments,  that makes the paper strong and illustrative. I believe it can be relatively easy to reproduce and help for others to have access to your code and be able to compare the new ones. Also, the contribution mentioned in the introduction is well written and gives a very good idea of the paper.  Perhaps it would be stronger if authors go deeper on the reasons why their method is better compared to the others and talk about specific disadvantages.	It would be welcome to have a comparison of the different models' efficiency in Table 1. This may highlight one of the limitation of EchoCoTr or reinforce its usability in a clinical setup, where compute resources are limited. Efficiency could be estimated by looking at how much time it takes for each model to analyze all the videos from the Validation or Test set, or by using the standard FLOPs metric. The data sampling (page 3, section 3.1)  looks well described, but EchoNet dynamic contains videos of arbitrary length. In the case where the video is longer than the clip covered by the sampling, how is the clip starting point selected ? And how do the authors handle the case where the source video is too short for the selected sampling method ? The changes made to the UniFormer model are not very clear. The model is partially described at the end of page 4, but it would be welcome to know what differs from the UniFormer and what was taken as-is. If all the mentioned parameters are different from the ones used in the UniFormer paper, please clarify it. Authors mention that their model can predict the LVEF with a good accuracy when it is given just one end-systolic and one end-diastolic frame. This is very interesting, but it should be stated that this is an important bias, as the model usually has to determine the position of these key frames.	Mainly motivated by the limited methodogical originality and applicative impact of this work, as mentioned above.	Authors explain their paper, show all the experiments and work done, and compare and quantify themselves against other existent methods. Also, they show the clinical importance of their method.	The paper beats the state of the art on the EchoNet dynamic dataset, which is the new standard for the LVEF prediction on 2D echocardiograms, but there is not much novelty.
183-Paper1732	EchoGNN: Explainable Ejection Fraction Estimation with Graph Neural Networks	The authors describe a computational framework to automate EF calculation using ultrasound cines. This is achieved by mainly 3 steps - a video encoder, an attention encoder and a regressor, with embedded neural networks at different stages. For testing and training purposes, a large sample of echo cines is used (10,000+) containing 'ground truth' data. Comparison and validation with other methods is also provided, showing relatively good agreement with ground truth for some EF sub-cohorts, and with computational complexity superiority.	In this work, the authors integrate an attention encoder, which learns the adjacency matrix describing the relationship between video frames, and a GNN after that, which leverages the learned attention matrix, to predict ejection fraction (EF), from AP4/AP2 Ultrasound images. The particular formulation of the attention encoder and the GNN is perhaps unique and interesting as it offers some explainability - which the authors claim is lacking in many works in this domain. They offer very good results and explainability as well, which is good to see.	The authors propose a Graph Neural Network for explainable ejection fraction (EF) estimation from cardiac US imaging (echo) which they call EchoGNN. The weakly-supervised training pipeline does not directly rely on ES/ED ground truth annotations and benefits from a low number of parameters, hence reducing computational requirements. EchoGNN consists of three main components which are explained in detail: a video encoder, an attention encoder and a graph regressor. The authors show that their framework is able to accurately predict EF while also correctly identify end systole and end diastole. They tested their method on a large data set of AP4 echo cines.	I found the paper to be strong and clinically relevant. Methodology, experiments and size of datasets are sound and sufficient. Particularly enjoyable is the apparent simplicity of the method, at least from an architectural point-of-view. In a way, the more difficult aspects of the methodology are hidden in plain sight, facilitating clear objective functions and measures. I think the combination of tools is very clever, and yields simple and effective results. - The paper is well written and organised. The used statistical methods are simple as a first cut, and also well presented. Supplementary material is helpful.	This work is novel in its formulation of the GNN, and using it to generate weights for each frame of the echo video. The adjacency matrix is used for the graph convolution. These frame weights seem to align well with the cardiac cycle, in situations where the videos are of good quality. They don't seem to align well, when the video quality isn't good. So, these weights can be used to guide a clinical user when to trust the result and when not to. Another advantage is the flexibility this model provides where a single video can be used or multiple if necessary. They test their algorithm on a nice standard set (EchoNet), which makes it easy for comparison. The model is also pretty light-weight, making it practically usable in POCUS settings and other resource constrained settings.	The authors succeed in highlighting three advantages over competing models: 1) EchoGNN produces explainable indicators as to why a model fails or succeeded, which is why the authors claim that their model can indicate whether human intervention is required; 2) the framework does not rely on accurate ground truth ED and ES labels; 3) the model has a lower number of parameters to reduce computational time.  They achieved this by implementing a Graph Neural Network, which they claim was the first time such a network was applied to echo cines in the context of EF estimation. Quantitative results appear convincing in terms of MAE, R2 and F1. The paper is well written, and each component explained with detail. The supplementary material and the illustrations are helpful to the reader.	Some of the limitations of the method could be explicitly summarised at the end. There is half a page available. Some may argue that the improvement overall is incremental, but I agree with the authors that the lower complexity of the problem described is important for clinical translation. Clinical aspects could be further analysed. The authors have a wealth of data available, many more experiments are possible. Please note that echo is not the only modality where EF is assessed. This is also routinely done with CMRI, and other imaging techniques. Might add a comment. What are the clinical implications of having an unbalanced training set? Could you tease this out in the Discussion? How could you 'correct' this in future?	While the model does offer some explainability, its not clear how impactful it can be to clinicians. It is unlikely that clinicians will take the time to look at adjacency matrices, frame weight curves and make assessment. It may be a good idea to use these more complex information to synthesize a single number, or metric to be used a proxy for quality and/or confidence. Because in terms of clinical workflow, its much more convenient to report a number or two and keep track of those numbers, as opposed to having qualitative description of quality, etc. While the possibility of using multiple echo videos is interesting, it may not be a good idea clinically. Reason being, sometimes videos are acquired at slightly different angles, imaging parameters, etc. Your model may have to learn, not only to be responsive to these variations, but also the interplay between different variations when multiple videos are used together. The frame sampling may also not work too well when there are too many frames from too many videos.	Generally, there are no major limitations.  The paper would, however, benefit from a more critical discussion on results that were presented in the supplementary material.  A few comments are detailed in (8) but these can be addressed during the rebuttal period.	Please address Ethics / IRB approval - you can use a placeholder for now. Otherwise I have no comments, seems sufficient.	The authors describe their methods, architecture, etc. reasonably well, but some more details would have been desirable (although some more is included in the supplementary material). They provide anonymized link to the code in github. One thing that reduces reproducibility is the use of custom architecture blocks. For example, the video encoder could be replaced with a generic frame based image encoder. Perhaps, the performance suffers a little bit? Would be good to know why they made this choice.	Authors have taken great care to make their work reproducible. In the reproducibility disclosure, authors state they will make code available. The methods are described in great detail in the paper. The EchoNet Dynamic dataset is publicly available.	"Minor style issues. For example, I would not start a sentence with a reference such as ""[18] did this and that"". Typo: ""EF error and increase THE model's ability"" How could this method be used in longitudinal studies? e.g. progressive heart disease. Table 1. Would it be possible to include running time? Justify T_fixed = 64. Rephrase ""necessitate the need"" (abstract) What about other sources of bias / noise in the data, e.g. image protocol, intra-observer bias, etc. The selection of the thresholds (unstated weights value and block width of 55) for ED/ES frame approximation is not explained (supp. Fig 3), but this selection will directly impact the aFDs. Can you please comment."	Continuing my point from above. SInce the temporal relationship between the frames are learned using the Attention encoder + GNN, why do we still need the temporal relationship handled using 3D convolution in the video encoder? It would be conceptually cleaner/simpler if those are separated out. Would also increase the modularity/reproducibility. Defining the edge/node relationships, why don't you write the equation for the last one (Ws) since that seems important? One thing I was curious about was, for GCN, could you not leverage the sequential nature of the data and establish stronger edge relationships? Essentially bake in a stronger inductive bias. Perhaps you're already doing that? Your model does have less parameters. But what about training time because of the 3d convolutions? For table 1, what's the hypothesis on good performance of aFD on ED and not good on ES? Another question I have is, EF is calculated using AP2 view US images as well. Do you think this work will translate well if used there as well?	In the supplementary material (Fig 2) it appears that coefficients for EF are low on the diagonal for 30%<EF<40% and 40%<EF<50%. Patients below 40% require medical care, and this data suggest that about a third would be misclassified as above 40%. There should be a discussion on this point in the main text.  Generally, the Conclusion section is missing some detailed discussion on limitations and would benefit from a more specific outlook on future work.  I would appreciate if the author could discuss as to why they think EchoGNN struggles with ES aFD in comparison to other methods. Also, are aFDs of 3+ frames not considered a poor outcome? Coming from a background of segmenting MR images, I view the temporal resolution of echo imaging superior and thought that the identification of ED and ES should be more accurate (especially when considering the closure and opening of the valves). Small editing comments: first line in Section 2: ConvoLutional Neural Networks	Many strengths and minor weaknesses, as detailed above. The paper reads easily, is elegant and has the benefit of using well executed and combined past contributions from the field. A MICCAI exemplar.	This is a good work overall with added bonus of explainability and good results. However, as I've written above the explaination provided by the model is perhaps not very interesting clinically. Some technical portion could also use more clarification and consideration.	A well written submission that clearly describes the network and its advantages over previous work. The clinical application is highly relevant, as non-expert users of echo (point-of-care) will benefit from reliable detection of EF and insight on explainability. The paper could be more convincing if more emphasis were put on the discussion of the results in a clinical context.
184-Paper0300	Edge-oriented Point-cloud Transformer for 3D Intracranial Aneurysm Segmentation	The authors present a novel point-based 3D aneurysm segmentation using transformer. The proposed method consists of three major components: 1) dual stream transformer for both semantic segmentation and edge classification, 2) edge context dissimilation achieved by graph convolution and 3) hard sample mining of edge points by constructive learning. The proposed method is evaluated on a public dataset. Experiments show that the presented method provides promising results.	This paper propose a new framework to segment intracranial aneurysm from point clouds containing both aneurysm and blood vessels, and emphasis is placed on how to segment the aneurysm edge accurately. To this end, the proposed framework consists of three parts, a dual transformer to segment the aneurysm and the edge separately, and a graph convolution part and a contrastive learning part to further enhance edge segmentation. Experiments are done on a public intracranial vessel and aneurysm dataset consisting of 116 annotated aneurysm. Better performance than baselines are achieved on aneurysm segmentation .	Point cloud processing and analysis have been a popular topic in the community of 3D computer vision, however, medical point cloud studies are still in demand towards clinical applications of practical significance, such as clipping surgery and Intra identification. This is an interesting topic, which has not been well-explored yet. It is thus encouraging to see studies proposed to tackle geometric processing problems for medical usage.	The strengths of the paper: 1: The authors present a novel method for intracranial aneurysm segmentation by edge-oriented processes. These processes include edge point classification by point-based transformer, edge point refinement by graph convolution network and hard sample mining around target boundary. 2: The authors evaluate the method on a public dataset compared to previously reported results. Effectiveness of proposed components are also studied as well. 3: The paper is well organized and the presentation of the paper is great.	The proposed aneurysm segmentation framework is new and the motivation of each part is reasonable. Better performance on aneurysm segmentation is achieved than baselines. Evaluation is performed on public dataset. The paper is well organized and easy to follow.	The paper proposed a three-stage paradigm to achieve 3D Intra Segmentation with clearer edge boundary. The proposed method outperforms current SOTA by a considerable margin. The paper is well-written with meaningful figures.	Overall of presentation of the proposed method is great. I have a minor question about the paper. The authors provide three edge-oriented processes to incorporate boundary information of the target. Although the overall results and some ablation studies are provided in the experiments. The details of effectiveness of each component is not given in the paper. It would be better to give more analysis about the presented components.	The clinical significance of this work is doubtful. If aneurysm segmentation is critical for clinical decision making, it is better to do the segmentation carefully by the doctors and it is not difficult to segment an aneurysm manually. The proposed model is rather complex while the dataset used for evaluation is rather small, in which there are only 116 aneurysm. The proposed method is based on point cloud, while generating the point clouds from original images may introduce some errors, which further increase the uncertainty of this method in clinical practice. Ablation study is not complete, since the results of applying any two of the three components are desirable.	Qualitative comparisons with SOTAs on the segmentation near edge boundaries are necessary but missing in the paper. The paper claims that they proposed a three-stage paradigm to focus on distinguishing the edge boundary segmentation (which thus improves the overall segmentation performance), however, this claim is unjustified in the manuscript. It would be insightful if we could see these improvements near edge boundaries qualitatively. The ablation discussion looks lack of insight. However, considering the page limitation, I may find it personally acceptable if the aforementioned additional qualitative comparisons can be added to reveal the insights. The authors achieved better results, however, it's probably due to their usage of edge labels, which are unavailable for other SOTAs. Given a standard setting (where edge labels are unavailable to model training), it's unsure whether the proposed method can still outperform others by a considerable margin. It's likely their overall proposed pipeline might not even work when these edge labels are unavailable, as the proposed modules look rely heavily on the edge annotations.	Although some details of the experimental protocols are missing in the paper, I would say that it is not that difficult to reproduce the experiments.	"Reproducibility is OK though parameter tuning is not provided. ""An analysis of statistical significance of reported differences in performance between methods."" is not provided though the answer to this question is YES."	The paper is well-written, with clear specification. However, the authors did not promise to release code, thus I might have some concerns in its reproducibility.	The overall presentation of the paper is great. I have some suggestions. 1: It would be great to provide the experimental results of the effectiveness of each proposed component. Some experimental details are missing in the paper. For example, the details parameters of construction of the edge graph.	"Consider improving on the main weakness. The last sentence of 3rd paragraph of Introduction says ""3DMedPT can still not perform well around the edge between vessels and aneurysms due to the less supervision and ambiguous features, where is extremely harmful for the clipping surgery process."" However, visualization of 3DMedPT results are not provided and its performance in segmenting edges are not clear. The first sentence of section 2.2 ""Due to the ambiguous features generated from similar contexts, points around the edge are easily misclassified, which is harmful for the surgery process."" It is not enough to only say ""is harmful for the surgery process"" and it should be explained how it harms the surgery process."	Failure to address my concerns listed above might reduce my impression and marks. I am giving this result, considering the limited works in this domain and based on the expectation if these concerns could be satisfactorily addressed. Open source is also preferred by the community.	The authors present a novel point-based method for 3D intracranial aneurysm segmentation. The method aims to pay more attention to the target boundary. The edge oriented segmentation method can be applied to other segmentation tasks in the 3D medical image domain.	Clinical relevance is doubtful and experimental dataset is small.	See my comments above.
185-Paper1159	Effective Opportunistic Esophageal Cancer Screening using Noncontrast CT Imaging	In this work, authors proposed a novel, non-invasive, low-cost, and highly accurate tool for opportunistic screening of esophageal cancer based on nonconstract CT scan, including esophageal tumor detection and classification (cancer or benign) task. The model achieves a sensitivity of 93.0% and specificity of 97.5% for the detection of esophageal tumors on a holdout testing set with 180 patients, which outperforms the mean doctors by absolute margins of 17%, 31%, and 14% for cancer, benign tumor, and normal, respectively. It is even more sensitive for early-stage cancer and benign tumor, compared with established state-of-the-art esophageal cancer screening methods, e.g., blood testing and endoscopy AI system.	This paper presents a deep learning method to classify esophageal tumors  from non-contrast CT. The deep learning method is based on a baseline nnUNet model (ref 8 in the paper) but incorporates position-sensitive full-attention layers.  The authors claim improved performance of their method compared with doctors' reading of non-contrast CT (which is not the gold-standard screening method for esophageal cancer), and a performance comparable to established state-of-the-art esophageal cancer screening methods.	The work provided a self-attention-based nnUnet model for screening esophageal cancer, as a non-invasive, low-cost, ready-to-distribute, and highly accurate tool, which showed strong performance compared with doctors and other AI tools.	They present a deep learning method to detect and classify esophageal tumors from noncontrast CT, a novel, non-invasive, low-cost, ready-to-distribute, and highly accurate tool, for screening esophageal cancer. They proposed the position-sensitive full-attention layer to better use the positional information and long-range dependencies in 3D noncontrast CT, which could improve the performance over a strong baseline nnUNet model. Compared with doctors' reading of noncontrast CT, they automated method shows substantially higher accuracy in both detection and classification. Compared with established state-of-the-art esophageal cancer screening methods, e.g., blood testing [11] and endoscopy AI system [14], they screening tool has comparable performance and is even more sensitive for early-stage cancer and benign tumor.	The large dataset from 4 institutions is an important strength. The manuscript is well-written and well-organized.	The main strengths of the paper: (1) One novel effective opportunistic esophageal cancer screening model. The position-sensitive full-attention layer could improve the nnUNet performance.  (2) Very detailed experiments and discussions.  A. The authors implemented the two-class classification and three-class classification experiments respectively (see Table 1). B. The detailed experiments comparison with other algorithms and readers (see Fig.3, Table2, and Table 3). (3) The strong evaluation (see Table 1-3 and Fig.3). (4) The big dataset and strong annotation would ensure the study's quality.	The authors seem to not describe clearly the main methods including how to train the segmentation model and what is the loss function,  the operation details of the position-sensitive full-attention layer which would be the main distribution in this study, such as, whose position is the position o=(i,j,k), possible position p et al, as well the classification steps. These will influence the readability and reproducibility of the proposed model.	The statistics are lacking in this paper: a) Whenever a value for a performance metric is reported (such as AUC, sensitivity, specificity) it should be accompanied by an error estimate (preferably a 95% confidence interval), b) No statistical tests were performed to support claims of superior performance. Just because one number appears to be higher than another, does not mean that this improvement is statistically significant. In spite of the large dataset (and relatively large hold-out test set which is presumably completely independent from training/validation and only used once as a test set), there are only 80 normals within the test set. The authors note that the prevalences are different from screening, which is the intended applicationI (it is normal practice to do this to increase statistical power), but the authors don't focus much on the performance for these normal cases. Of course you want to detect all cancers but the cost of false-positives is important, especially when extrapolating to a screening setting. Without statistical proof or error estimate, the baseline nnUNet (Figure 3) seems to have no false-positives for normal scans while the proposed method finds one more cancer at the cost of 2 false-positives. Is this benefit worth the cost (assuming statistical proof can be made)? The 'reader study' is not really a reader study but a comparison (without supporting statistics) of performance of the deep learning to that of physicians. What is of more interest is how the physicians perform with and without aid. The model appears to be an incremental change from the baseline model (ref 8) Because of the class imbalance of the test dataset 80:20:80, the authors should consider adjunct weighted versions of 'accuracy' as performance metric since 'accuracy' is influenced by class prevalence.	The main weaknesses: (1) The overall novelty and innovation of the work were limited. The self-attention module was not new tech, and nnUNet was also the classic segmentation method. (2) The experimental demonstration and discussion of the research work were sufficient and complete, but the research methods and ideas were relatively simple.	It is interesting and a good idea to improve the tumor segmentation performance by proposing the position-sensitive self-attention layer for each encoding layer in the segmentation network nnUnet. However, detailed method and trained procedure were not introduced clearly, which influence the reproducibility of this work.	Neither code nor data seems to become publicly available. The reproducibility statement is unclear to me in that under #4 the authors filled in N/A on a few occasions whereas I think that for any AI method the exploration of hyper-parameters and sensitivity are applicable.	According to the Reproducibility Response, I think the reproducibility of the paper was good. Providing the common dataset and code can improve repeatability.	1) Clear description on the operation of global self-attention layer should given for better readability and reproducibility.  2) The classification method should be described in details. 3) How to train the segmentation network should be described clearly.	Minor comments: It is unclear what the last 3 columns in Table 1 are; are these sensitivities? It seems Table 1 and Figure 3 are conveying the same information? It would be good to have Table 1 directly above Figure 3 in the manuscript, rather than be interspersed with Figure 2 which shows example output. ROC curves should be plotted on square axes. Figure 3 (ROC curve) needs error bars for the operating points (and preferably a shaded 95% confidence interval for entire curve) It is unclear how the AI operating point was determined. This should have been done based on the training/validation results without taking into account the test set at all, i.e., a threshold for the output score should have been determined for the training/validation set and then applied to the test set. Please clarify. The authors talk about detection and classification, but the performance evaluation is framed entirely as a classification problem (ROC, sensitivity, specificity) without localization or FROC (free-response ROC). Localization is implicitly included through a cutoff for the Dice score (with the reference standard), but subsequent performance evaluation assigns a single label (cancer, benign, normal) to a scan. Looking at the example output of the method (Figure 2), it would be possible, e.g., to have more than one false-positive region in a scan marked as false-positive.   In true detection problems, FROC analysis is important to evaluate performance, especially the number of, and types of, false-positive marks for normal scans. The overlap of 0.1 for the Dice score that determines a true-positive 'hit' seems to be very low; why so low? The sub-analysis by cancer stage is interesting but lacks statistical power (and needs error estimates). The dataset needs to be better described in terms of stage and lesion sizes in the 'Datasets' section	I think it was a good study, with relatively complete, sufficient experiments and discussions. Further refinement of the language and supplementary charts will help this study to be published in a good journal.	The authors proposed a good idea to improve the tumor segmentation performance by proposing the position-sensitive self-attention layer for each encoding layer in the segmentation network nnUnet. This a well novelty. However, its method seems to not be described clearly, which influence the reproducibility.	This seems to be a pretty strong paper, but the lack of statistical analysis is a major weakness.	The experiments were very complete and meticulous.
186-Paper2172	Efficient Bayesian Uncertainty Estimation for nnU-Net	This paper proposes a method to estimate the nnU-Net uncertainty for medical image segmentation.	This paper leads to several nice contributions: * it provides a novel VI approximation method, * it provides an uncertainty estimation scheme for nnU-net architecture * it boosts this same architecture in the context of biomedical image segmentation	This paper presents an uncertainty estimation method that employs the posterior sampling of weight space and validates it in two public datasets under the nnU-Net framework. The uncertainty is estimated by ensembling multiple snapshots (checkpoints) during one model training but under a cyclic learning rate schedule. The obtained results outperform three commonly used baseline methods.	Comprehensive literature review; Novel with utilization of network checkpoints at various training epochs; Comprehensive evaluations;	The main strength of this paper is that it fills a gap in DL community by providing an uncertainty estimation method for the nnU-net architecture.	The paper is well-organized and easy to follow. The idea is neat and it can have border impact once code is available, as it is integrated into the popular nnU-Net framework. The improvement in the ECE metric is significant, although the gain in the segmentation performance is relatively marginal compared to Deep Ensemble.	"""Efficient"" in the title is not well supported by the results. Only 3D data is evaluated. 2D data segmentation should be included."	I do not see any particular weakness in this paper.	"Too limited novelty. This method proposed in this paper is essentially similar to ""Snapshot ensembles"" (ICLR 2017), cited as [13] in the paper. However, this paper does not discuss it in related work and only mentions [13] in the cyclical learning rate setting (section 2.3). To me, the single-modal posterior sampling is similar to NoCycle Snapshot Ensemble, and the multi-modal posterior sampling is similar to Snapshot Ensemble.  In my reviewing process, I was waiting for the EXPLICIT discussion and comparison of the difference between the proposed method and Snapshot ensembles but got rather disappointed. The differences are small, e.g., the use case  (medical images), and the cyclical schedule (from cosine lr to a proposed one). Besides, there is no ablation experiment on the proposed cyclical schedule. If the authors would clarify the difference between the proposed method and Snapshot ensembles, I would consider adjusting my rating. Minor issue: The name ""multi-modal"" can be confusing in the medical domain. Snapshot ensembles: Huang, G., Li, Y., Pleiss, G., Liu, Z., Hopcroft, J.E., Weinberger, K.Q.: Snapshot ensembles: Train 1, get m for free. ICLR 2017."	Easy to reproduce.	This paper is reproducible.	No issue here.	"""Efficient"" in the title is not well supported by the results. nnUnet has been powerful on both 3D and 2D data. However, only 3D data is evaluated here. 2D data segmentation should be included."	No special recommandation to make.	A paper with limited novelty is fine, as long as it honestly claims its contributions. Do not overclaim. An extensive ablation study or study on important factors can improve the contributions of this paper. For example, why cosine lr fails for nnU-Net training? How do the hyper-parameters like the number of checkpoints, the gamma parameter, or training epochs affect the quality of estimated uncertainty?	The novelty is good and the field is interesting.	This paper seems to be a very important contribution in the domain of biomedical image segmentation, since it provides an uncertainty scheme to the state-of-the-Art architecture that nnU-net is, which was not obvious to develop (as explained in the paper). Furthermore, its performance is boosted, leading to a new state-of-the-Art segmentation method.	I'm okay with a paper with limited novelty but with extensive experiments and ablation studies to distill the key factors that can contribute to the community. The authors cite Snapshot ensemble [13] in their paper but present no discussion/comparison between the proposed method and Snapshot ensemble. In my opinion, it is more like purposeful behavior. That's why I downgrade my rating. I would raise my score if the authors could address my concerns.
187-Paper0368	Efficient Biomedical Instance Segmentation via Knowledge Distillation	This paper introduces a knowledge distillation method for biomedical instance segmentation.Specifically, the authors proposed two schemes: graph distillation and pixel affinity distillation to transfer the knowledge.	Submission 368 proposes a novel knowledge distillation approach which is suitable to distill networks trained to do very difficult tasks, i.e. networks where one would expect a large number of parameters is necessary. Through the use of both instance- and pixel-level consistency between teacher and student networks, the authors achieve excellent distillation results, as evaluated on 3D EM segmentation data (CREMI challenge) and 2D natural image data (CVPPP challenge).	This paper presents a knowledge distillation method targeting on medical image segmentation task. This method can transfer the knowledge learned in a large teacher network to a lightweight student network using two distillation schemes, i.e. instance graph distillation and pixel affinity distillation. The experiments show the potential of improvement on student models trained with the proposed distillation method.	1.Clear explanations. The authors present their ideas clearly, making the paper easy to follow 2.Thorough experiments. The authors compare the proposed method with several typical segmentation models.	Novel idea to enforce both instance-graph consistency and pixel affinity consistency evaluation on difficult datasets clearly shows the method superiority the paper is very well written and illustrated	The ideas of instance graph and pixel affinity distillation is novel, since it explores a different aspect of knowledge distillation where high-level representations of feature maps are used to transfer knowledge from teacher model to student model.	Contribution is somewhat limited. Applying knowledge distillation on the affinity maps is not very novel. Please refer to 'Adaptive Affinity Fields for Semantic Segmentation' .	nothing significant	The complexity of the distillation method was not discussed, especially, instance graph distillation. Since constructing graph can be costly and time consuming, it would be better to include such discussion in the paper. Some details are missing from the paper, for example, L_{PAD} in Eq. 4 is not defined anywhere in the paper and other reproducibility mentioned in the section 7.	Easy to reproduce	the data is public, the code will hopefully be made available	Some details are missing. The authors seems just check everything 'yes' in the checklist. Missing one of the loss function definition. Missing software framework and version. No new dataset proposed. Missing training time for the proposed distillation method.	It would be more convincing to validate the proposed method on some additional datasets	I'd be curious to see if the student network can be trained without the affinity loss L_aff. This would enable distillation of a large network pre-trained on private data without access to groundtruth.	The authors should emphasize on how significant the performance gap between the student and teacher networks. The authors do include a table in supplementary materials, however, no further discussion is provided.	The writing structure of this article is clear and the approach is reasonable	Novel ideas for an important problem, detailed evaluation shows superiority compared to state-of-the-art	The paper is well written and easy to follow. The proposed distillation method is novel and the experimental results demonstrate its advantage. Although there are some missing details, they are not the major issues. Therefore, my initial rating is accept.
188-Paper2377	Efficient population based hyperparameter scheduling for medical image segmentation	Authors propose a hyperparameter optimization method that performs local search by using best performing checkpoints from an initial training run and then, in parallel, retraining the model from the checkpoint using  multiple hyperparameter sets sampled using Tree-Parzen Estimators. The method does not require expensive retraining (from scratch)  every time a different hyperparameter setting is explored. The method was tested using several datasets from the MSD challenge.	This paper presents a hyperparameter optimization technique, based on population based training (PBT), that reduces the training cost of original PBT, to make it feasible for large 3D medical images. They key idea is to start from a set of default parameters (chosen due to prior knowledge) instead of random parameters, which reduces the number of workers needed for PBT to converge. The method is evaluated on 4 tasks of the medical segmentation decathlon, and on 2 network architectures. It shows slight improvement in Dice Score (1-3%).	This paper utilizes Population based training (PBT) for hyperparameter tuning for medical image segmentation models. From a default setting, the tuning is able to achieve performance improvements and save 90%~97% computation cost of training from scratch compared to the original PBT.	the method to use multiple checkpoints, and retraining models to populate the configuration space for  TPE appears to be a novel application for hyperparameter optimization	the claimed reduction of computation cost to 3%-10% of original PBT is impressive	The paper uses an existing technique to solve a practical hyperparameter tuning problem. There is potential value to applying it in many other applications and accelerating the development of new deep learning models in the future.	authors have not convincingly explained why results are worse than the default setting, when the number of workers were small (W-9) the authors are making an assumption that a good set of hyperparameters exists for a given model/problem which may not be the case. If this default set of hyperparameters used is not good enough, I am not sur eif this method would work well.  If authors can show that this is not the case, that will make the method more convincing. For example, given a task and a model, if the authors were to use a random set of hyperparameters to initially train the model, and apply their hyper parameter optimization technique, will the method still yield a good enough solution compared to when a good set of initial hyper parameters were used to train the model the first time  ? As such the method seems to be useful to find incremental improvements to a model, rather than help find a set of optimal hyperparameters as it appears to be a more localized search than a global search in the hyperparameter space.	the main novelty of this method is to start PBT from default hyperparameters, which already achieve good performance. This restricts the use of the method to settings where  'good' hyperparameters are already known and can be used as a starting point. The authors acknowledge that the performance improvement using their method can be quite small, it might be mainly useful to tune hyperparameters to compete in challenges a performance comparison of the proposed method, original PBT and default training is missing.	The main weakness of the paper is the limited validation and novelty. Although it is a good idea to incorporate PBT into medical imaging using prior knowledge and can obtain consistent improvements, the main value of the method is quite practical and lacks methodological contribution. That makes the current version of the paper less interesting. However, this can be a good paper if more solid validation is provided, e.g. improves nnUnet in more medical imaging challenges.	No code or implementation has been shared. Public dataset used. Reproducibility status unknown.	the authors plan to publish their code. Data used is public	The authors would like to make all codes publicly available, which ensures good reproducibility.	"it seems that from the set of hyperparameters that were used in Table 1, there are none that are related to the architecture of the models - for example the activation functions or number of layers, or filters. Could you explain why these specific hyperparameters were chosen ? explain why when the number of workers are low, that you do not obtain better results than the default setting . could you provide test results on a held-out test set for the optimized models vs default model. It seems you have only reported validation results and not test results. I'm interested to see how much of a test performance gain can be seen if the model was retrained from scratch using the ""optimized"" set of hyper parameters found using this technqiue"	since the search space of the hyperparameters is restricted by a range around the default values, I wonder how this method compares to simple grid search around the default values. This could be discussed in the paper, by e.g. comparing to grid search. the authors mention an average performance increase of 1-3%, but the actual performance on the 4 datasets is not mentioned in the paper. Figure 3 gives some hints of performance on the validation set, but it would be helpful for the reader to see the performance e.g. in a tabular form. Also a comparison with original PBT in tabular form would be desired. Figure 3 is mentioned before Figure 2 in the text on page 3 the authors claim that their method reduces the training cost to 3%-10% of original PBT. Could the authors elaborate how they estimated this amount of reduction?	*Why there is no pbt best worker from scratch (W=27) in UNet Lung in Figure 2? *Add significance test to the results. Repeat the experiments to remove the randomness of the parameter searching. *Evaluate it in more challenges.	the utility of this method seems limited to cases where models with good initial hyper-paramaters are already known. We cannot assume this is the case for all problems.	The paper proposes a modified PBT, with restricted hyperparameter search space and starting from a set of default parameters. This does reduce the computational overhead of PBT - which makes it feasible to train for 3D medical networks, but it also comes with the drawback of the restricted search space.  The performance improvement over the default values is small, and the authors do not compare with other simple optimization approaches like simple grid search.	This paper presents a good hyperparameter tuning strategy based on PBT. However, the current version needs more solid validation to prove its efficiency.
189-Paper2746	Electron Microscope Image Registration using Laplacian Sharpening Transformer U-Net	This paper puts together a Cascaded Laplacian-sharpening Swin Transformer U-Net (LST-UNet) for deformable image registration of data slices from the MICCAI Challenge on (neural) Circuit Reconstruction from Electron Microscopy Images (CREMI). As these are preregistered data, synthetic deformations are applied for later recovery of motion.	The authors propose a Swin Trasnformer UNet with Laplacian sharpening in the skip connections to register TEM images by infering the displacement map rather than the registered image. The sharpening filters seem appropriate for connectomics in TEM to preserve the semantic and structural information in the image. They also propose applying the model inference two times, one after the other one (cascade processing). They compare their approach with other existing methodologies in the field, showing a better performance of their proposed architecture and the cascade approach.	The authors investigate the use of a Swin Transformer based U-Net (Swin-UNet) for registration of serial electromicroscopy (EM) image slices. They further incorporate ideas from Sharp U-Net and cascaded registration approaches (see for example Quicksilver by Yang et al. 2017 and Recursive Cascaded Networks by Zhao et al. 2019) to further improve the registration. In addition, a similarity term consisting of two loss terms is utilized.	interesting use of Laplacian sharpening of skip connections interesting use of Transformer-based Swin-UNet for capturing more long-range semantic information with the Transformer modules. comparison to SOTA (designed for different purposes though)	Integration of an adequate filter (Laplacian sharpening) for the nature of the images to analyse in the skip connections. The authors introduce the idea of fine tuning the results by processing the output result with the trained architecture (cascade processing) The authors benchmark their proposal with existing state of the art methods.	Clear motivation and approach integrating three orthogonal ideas from related work. Ablation study demonstrating improvements due to each architectural choice.	"synthetic deformations are suboptimal, as these only test the robustness and precision, but not the accuracy unclear why (simulated by motion) adjacent slices in EM require registration - what is the application/question? EM is suffering from intrinsic motion artefacts (line shifts) which are not dealt with here unclear why this methods is ""specifically designed for ME registration"" - it appears to me to be quite generic in set-up for other 2D registration tasks?"	The novelty of the propose method is questionable: The authors choose a CNN architecture already published in the literature to show it's applicability in a different microscopy image processing task. Additionally, it is not clear from the paper whether their method works in 3D or in 2D. In the original Swin U-Net, they used 2D slices and propose to work on 3D as a future work. An important contribution would be for example, upgrading this method to 3D, which is not clear whether the authors did it already. The authors speak about catching the global information of TEM images to improve the registration. Indeed they claim that this work is specifically designed for serial-section. Although they are using transformers, I think that if they are not analysing the global 3D information, such statement can be confusing. The main reason is because in connectomics, when speaking about serial-sections or global information, it is related to the 3D information contained along all the EM slices, and not only the one that is observed in pairs of slices. If the authors are indeed analysing 3D information, please, make a clear statement in the text and indicate the number of slices entering the training batch.	The paper proposes a methodologically simple idea of combining related work. No standard deviations nor significance tests are reported for the results. Use of both correlation coefficient and structural similarity is not motivated well. No ablation study with regards to this choice of loss is included.	use of public challenge data unclear if code is to be made available no significance testing (not claimed in checklist but unclear why not carried out)	Please, correct me if I'm wrong: In the reproducibility statements, authors declare to provide the links to the code, however I did not find any or any statement in the paper indicating the the code will be openly available.	Publicly available dataset and simplicity of approach based on related work with available open source implementations should facilitate reproducibility.	This is a nice methodological framework, tested on EM challenge data and generally compared well against SOTA methods, which however were designed for different purposes. Results seem moderately, but not significantly better. Deformations were first simulated then recovered, which is suboptimal. I would have thought that serial or cine registration applications would have been more interesting to investigate.	"Typos & proposed corrections to be made: Page 1: Lv et al. [12] proposed (Is it correct?) Page 3: STN, please indicate that it means ""spatial transformer network"" and cite it. Also, At the end of page 2, the authors indicate that they add a Laplacian sharpening skip connection but they do not justify why until the description of feature enhancements. To ease the reading, I think it would be nice to motivate why you integrate such accesorial step in the architecture. Figure 2: could you elaborate a bit more on the Caption. For a matter of completeness it is recommended to explain the meaning of the parameters in the propose architecture (i.e., W, H, C and so on) and how did you set them up. Did you train the STN? How? What is the size of the kernel used for SSIM? Comments: Cascade approach: Is there any theoretical reason to not train the cascade approach end-to-end? Additionally, there will always be an error and a bias implicit to the trained architectures. How does it propagate in this cascade approach? Is there any reason to apply the cascade iteration only once? Would it make sense to apply it iteratively until there is no significant change in the output? The authors use images of size 448x448 to train the model. How does this relate to the receptive field of the network? Additionally, images are resized before the training. Is there any specific reason to do so? Were the images also downsampled before analysing them with the proposed stare-of-the-art methods? A comment on this might be important as in Figure 2, it looks like the proposed network is able to better resolve fine details, compared with the UNet. This might be first, because the resolution of the images was different on each approach, or because the proposed method is able to fine tune the results. Is this a consequence of using the cascade approach? How does the proposed method compare in terms of hyperparameters and memory consumption?"	The authors don't specify how many of the 125 images were used for training, validation, and testing. A reference to related work which also used a cascade of registration networks would be appropriate. Appropriate would be Quicksilver by Yang et al. 2017 and Recursive Cascaded Networks by Zhao et al. 2019, though both differ from the simpler cascaded approach utilized in this work. Personally, I think Recursive Cascaded Networks with the difference that weights are not being shared and learning is not end-to-end (which is fine!) seems an appropriate reference. Given that after an initial alignment, the motivation for long-range dependencies is less profound, would it be more plausible to use a standard CNN registration model for the second stage of the cascade?	See above - nice methodological framework, questionable experiments and moderately convincing results.	I think the authors did a great job describing the method and comparing it with state of the art soltuons, but I think the contribution they are making lacks of technical novelty.	The authors integrate ideas from related work and apply them to particular application. Novelty is limited, but results of combination of Swin-UNet with Sharp UNet for image registration have not been reported before to the best of my knowledge.
190-Paper0420	Embedding Gradient-based Optimization in Image Registration Networks	In this paper, the author deals with the problem of medical image registration. They introduced a two step scheme to connect tradionnal approaches and network based approaches. To do so they have two levels of optimisation : a deep learning based optimisation (looking for the best networks parameters) and a iterative optimisation using gradient descent (looking for the best transformations). The proposed scheme is independant from the network architecture or the registration formulation. Authors compared with deep learning and non deep learning approaches using two datasets with cardiac and brain MRI.	The authors present a combination of deep-learning-based and conventional registration.	In this paper, the authors present a deep learning based method for image registration. It is based on the interleaving of a learned gradient based forward step during a multiresolution based learned optimization. The authors present the individual building blocks of the algorithm and show the applicability of the method on 2d and 3d medical data, combined with a comparison to other published methods.	The main strenghts of the paper is the original way to combine iterative registration and deep learning based registration. The proposed formulation have better performance on out of domain distribution or with a decrease of the training dataset  than competing methods.	The authors propose a method to combine deep-learning and conventional image registration to further improve the results obtained by the DL-Reg The experiments show promising results. The paper is well written. A reasonable selection of metrics was made to evaluate the quality of registration (Dice, HD, detJ, std(detJ), runtime the authors perform further evaluations on data efficieny and domain robustness.	The method presented by the authors is motivated by classical image registration techniques. In the work, they extend conventional deep learning  ased image registration methods with an update step motivated from image dissimilarity. This approach appears promising. The methodological foundations of image registration are explained in detail, thus motivating the newly added step. The method section takes enough space for motivating the individual building blocks. Another point worth highlighting is the use of clinical data to validate the new method. 2D and 3D data representative of current clinical issues (cardio and neuro) are used. These data are not only used to show the own strengths, but a comparison with current methods (classical and AI) is performed. Another strength is the experiments on data efficiency and domain robustness.	"The reviewer does not understand exactly the way the registration problem is solved and why the lower optimisation problem is called ""the forward pass"" by the authors. A pseudo algorithm of the proposed methodology would have clarify it. According to the reviewer, it is necessary to compare the proposed formulation with other network than Voxe"	"-The description of the method is not sufficiently clear. Is this a deep-learning registration, which is used to generate an initial solution for a conventional registration? (Meaning DL-Reg + Instance Optimisation like e.g. Mok et al. did for task 2 of the learn2Reg challenge) It seems to me that this is because the network parameters are not changed in the second optimisation (1b) This would mean that the optimisation from 1b does not have to be carried out during the training of the network, as these have no influence on the network parameters.  However, the title as well as other wording in the text suggests that this is an interrelated optimisation/network. Why else should the image dissimilarity in the section ""Image Dissimilarity Gradient"" be differentiable twice? For the evaluation, automatic segmentations are used on both data sets. It is not clear how good these are and accordingly how great the influence of errors in the automatic segmentation is on the evaluation of the registration. For the 3D dataset, GraDIRN is with a Dice of 0.799 significant better then RC-VM with a Dice of 0.794. Is that really significant better taking into account the segmentation error? Even without this, the difference seems very small to me for significantly better results. It is not clear to me, why the authors have chosen to evaluate their method on this datasets. They clearly propose a new method and therefore, it would be appropriate to evaluate the method on a publicly accessible data set with manual annotations. (e.g. the Learn2Reg datasets) the discussion and contribution section does not contain any discussion. Please discuss your results!"	The formulation of the approach in a bi-level optimization view is not conclusive and probably rather misleading, since the optimization problem formulated in this way is not solved. The role and the design of the regularizer in the lower-level step is not clear (\nabla R_{theta_t}). Which form of regularization is used here should be made clearer. The motivation why the structure of the procedure described in Figure 1 fits the formulation of the optimization problem is not obvious. In the experiments part, the newly presented method is compared with other already published methods. Unfortunately, it remains unclear why the methods used in the paper were chosen. Here, a short justification of this choice would be nice. In the experiments it is motivated by Tab. 2 that the explicit gradient is necessary for the success of the method, here a more detailed consideration would be useful.	According to the reviewer, the paper is not clear enough to be perfectly reproducible. However the authors provided a github repository with the code.	The given answers seem to be correct. I appreciate that the authors honestly answered these questions (no one else did in my paper stack) An analysis of situations in which the method failed. [No]  Discussion of clinical significance. [No]	The procedure itself is not made available for download etc. The data used are freely available. The procedures used for comparisons have already been published and some of them are available as freely accessible code. It is unclear how the presented procedure has to be parameterized in detail. This could be discussed in more detail in the experiments section.	The methodology and particularly the training process with two optimisation level requires more explanations and clarity.	"In Table 1, it is not clear if the reported values are average or median or whatever values.  In Table 1 and 2, what is meant with "" J < 0% ? The number of voxels with a Jacobian determinant smaller than 0% ? Or the percentage of those so "" J < 0  in % "" ?"	The methods section should be revised to clarify the reasons that led to the formulation of the procedure as presented. The role of the regularizer in the lower-level optimization should be better emphasized. What form does it take after learning, what task does it have in the overall registration, what is its contribution to the results. It is unclear what segmentations are used for dice determination - no segmentations are mentioned in the 3d case. The role of lower-level optimization, specifically focused on in Tab 2, should be prepared and at least motivated in the methods section. Minor changes: The abbreviations for the segmentations in the heart are not used again in the following and can be removed (ie LV, Myo,...).	The methodology and the advantage of having two optimisation process is not clear enough, well explained and discuss.	I see in this paper an interesting idea that has potential and should be presented to the scientific community. I hope the authors will revise their manuscript a bit more so that it is a bit clearer.	Minor rearrangements and extensions in the methods section and the addition of explanations in the experiments section can raise the paper to a good level very quickly. The weaknesses presented can be easily elliminated.
191-Paper1872	Embedding Human Brain Function via Transformer	They adoptted a transformer-based framework that encoded the human brain function measured by fMRI data into vectors of latent layer in transformer.Then they  evaluated the proposed framework in brain state prediction downstream task, and found that the embedding vectors are relevant to the response of task stimulus.	In this submission, the authors propose a transformer-based learning model for analyzing human brain functions, specifically, they focus on learning a canonical embedding for better predicting human brain states through fMRI inputs. They address the issue of regularity and variability of different instances, and choose transformer as the backbone to solve this issue.	DL method is proposed based on transformers for building a compact representation of human brain functions from fMRI. 3D volume of fMRI data can be embedded as a dense vector which profiles the functional brain activities at the corresponding time point. Regularity and variability of brain functions at different time points and across individual brains can be measured by the distance in the embedding space. The method is evaluated on the Human Connectome Project task fMRI dataset for brain state prediction. A comparison with various baselines is reported, demonstrating the increase in performance of the proposed method, with a neglectable computational cost increase as compared to a standard auto-encoder.	The work provides an approach on representing regularity and variability of human brain function in a  latent space.	A good implementation of introducing cutting-edge computer vision and natural language processing technology to medical image analysis. This submission provides an option of analyzing medical time series data via transformer.	The paper is well written, easy to follow and the method and task are well motivated. The method is simple and sound. The results demonstrate the benefit of the proposed approach over a simple auto-encoder, for a limited computational cost increase.	The authors stated that the embeding  vectors represent regularity and variability of human brain function, maybe they should show the variability part for different  brains and time points.	One major drawback of the submission is the experimental design. If I remember correctly, HCP contains 7 tasks, but only the results of two are given in the paper. Regarding to measure the prediction performance, more statistical metrics are expected to use, but I only see accuracy and pcc. Besides VAE-based model, I see no other comparison methods. Such an experimental design is not convincing to me. From the perspective of methodology, the proposed method did not well address the issue they mentioned in Introduction. How does the proposed method encode the regularity and variability of different brains is not clearly answered.	The evaluation is limited to a single dataset/task, which may however be sufficient for a conference paper.	The reproducibility of the paper is good.	No code is provided	Method evaluated on a publicly available dataset. The code is not shared and minor implementation details are missing for reproducibility.	If the embedding size was 64,  all fMRI signals of whole brain were represented the 64 vectors, or the fMRI signal of one voxel were represented the 64 vectors? or others? When predicting ADHD, how to divide the training, validation and test data,  I mean that if author use some brains as training data, and the others as test data, or any other manner?	(1) Transformer shows a more flexible and expressive ability compared with CNN, so I'd like to see its application in medical image analysis. This work utilizes transformer in a basic way: switch CNN modules with transformer, which is not impressive. But applying transformer to fMRI is a good topic. LSTM-based or RNN-based methods show a good prediction results on HCP tasks, the authors could explore them and add some of them as comparison methods. (2) Test on all tasks, evaluate with not only accuracy but also precision, recall, f1 score. Statistical results could be tricky, so always provide as much information as possible. (3) Fig2 is redundant, provide some results from comparison methods.	"Some more comments are provided in the following. Have the authors tried to train simultaneously the embedding and the specific task (despite the stated motivation of not being task-specific) ? In 3.2. Since the methods is in two stages (train the embedding, then train the brain state prediction), it is not clear what some parameters refer to in the implementation details. Details should be given for both trainings. Also, is there no early-stopping on the validation set? Figure 1, keep t either columns or rows in both the 2D signal matrixx and the learned embedding. In 3.3. ""But the performance gain is significant"". It reads as if a statistical test was performed but I think it is not. In 3.4. Interesting analysis of the correlation of individual digits with individual stimuli. It would also be good to report the specificity of these digits, as some may just respond to any stimulus, while others may be more specific? I would mention the limitations of the ""interpretation"", it does not interpret the internal behavior of the transformer, it is just a correlation analysis between extracted features and stimuli. ""general, comparable, and stereotyped space"" is repeated in introduction. In 2.3. ""We fix the weights of pre-trained model"" -> the pre-trained model In 3.2. ""the size of two FC"" -> of the two"	The work provides an approach on representing regularity and variability of human brain function in a latent space of Transformer, and uses the embeding vectors to identify the brain state, experimental results demonstrate it effective.	I give this score mainly based on the soundness of the methodology and the experimental design.	I think this simple application of transformer to fMRI is valuable to the MICCAI community. It is well written, an interesting approach with good results.
192-Paper1262	End-to-End cell recognition by point annotation	This paper proposes  an end-to-end framework that applies direct regression and classification for preset anchor points. The pyramidal features aggregation module provides low-level and high-level features for multi-task learning framework.	This paper proposes a neural network structure that predicts the anchor point for each cell without generating the density map. Two main components introduced in this framework are the pyramidal features aggregation and the optimized cost function.	The authors proposed a novel framework for dense cell recognition by using multi-task learning and one-to-one matching strategy. The experiment results demonstrate the effectiveness of proposed modules.	1 The proposed methdod achieves competitive results on PD-L1 IHC stained images of tumor tissue.	This paper is easy to follow and the cost function is explained in detail. Then contribution is sufficient. It deploys the pyramidal features aggregation and the cost function integrate the detection and regression and classification loss. The experiments are sufficient and clearly shows the improvements.	The authors proposed a novel multi-stage cell recognition method, using the point labels. The framework consists of several key improvements, Pyramidal Feature Aggregation, Multi-task Learning and Proposal Matching. Experiments suggest the proposed method can provide better performance.	"1 the novelty of this paper is quite limited. For pyramidal feature aggregation, please refer to ""Feature Pyramid Networks for Object Detection"" and  ""Richer Convolutional Features for Edge Detection"" . 2 Inadequate experimentation. Experiments were performed on only one dataset. The generalizability of the method cannot be well assessed. The division of the test set will have a certain impact on the experimental results.The paper does not conduct multiple experiments for statistical description. 3 The formulas are confusing."	Some notation is confusing.	The motivation for the proposed loss functions needs to be highlighted. The selection of hyper-parameters  is unclear , such as lambda. Some typos make this paper hard to follow and less convincing.	Can be reproduced based on some related knowledge.	All the hyper-parameters are given. It is possible to reproduce the paper.	The code is not provided. All methods used for the proposed framework are depicted clearly and noted with appropriate references.	1 Please conduct experiments on multiple datasets to verify the effectiveness of the proposed method. 2 Please conduct multiple trials on the same dataset to get statistical description of the results.	Overall it is a good paper, but some definitions are confusing. What is the definition of the preset anchor point is confusing. Several variables, e.x. x, y, y* are not explained. How does the network get the center point (x,y)?	"The dataset is only divided into training and test sets. Does the method employ the early stop training? Some cells in IHC slides were not fully membrane stained. How to define whether such cells are PD-L1 positive or negative? What is the standard? example: 50% membrane stained as PD-L1 positive. ""Regression models"" mean which models? It should be clearly described in this section. ""7times7"" is a latex error? ""The overall framework of the proposed model is depicted in Fig. 3"". I suppose it should be Fig .1. Does Time/s refer to the average time per sample? Seconds or minutes? What does 'IC' mean in Table 2? The various abbreviations in the paper need to be explained clearly."	novelty and experiments.	The contribution is sufficient. There are few things can be improved. The introduction of the point-based problem. What is the preset anchor point. How to get the center point, etc.	The method is novel and effective. The data and experiments section are somewhat vague.
193-Paper0791	End-to-End Evidential-Efficient Net for Radiomics Analysis of Brain MRI to Predict Oncogene Expression and Overall Survival	This study attempts to predict the oncogene status of MGMT in glioblastoma patients using multiparametric MRI scans. This prediction has been previously investigated with mixed results. Using data from a prior challenge, the authors show that their evidential deep learning (EDL)-based approach achieve highest performance compared to other state-of-the-art.	Evidential deep learning for classification and regression is employed to predict the methylation status of MGMT and overall survival (OS) of GBM patients.	This paper proposed a deep learning-based framework for the tasks of Overall Survival (OS) prediction as well as prediction of methylation status of  MGMT for patients diagnosed with Glioblastoma from multimodal MR images. The status of  MGMT that is of vital importance for both diagnosis and prognosis purposes and its accurate prediction is of great interest for the communities. The main contributions of this work can be summarized as: Uncertainty information was integrated to the final prediction through Evidential-Regression approach, and Integration of a Gaussian distribution to characterize the final predicted values. The proposed pipeline was based on a tiny version of EfficientNet followed by some modifications. This model was tested on two standard challenge dataset that include multiomodal MR scans of subjects diagnosed with Glioblastoma. Performance of the model for both tasks of MGMT status prediction and OS prediction shows outstanding accuracy.	The manuscript has multiple strengths: (1) tackling a clinically meaningful problem that can potentially alter management of glioblastoma patients; (2) a novel formulation and application of the EDL model to fuse high-dimensional features from multiparametric MRI scans; and (3) superior performance of the model compared to other state-of-the-art.	(1) The idea of this paper is easy to understand. (2) The end-to-end evidence-efficient net is proposed to simultaneously classify the methylation status of MGMT and predict OS.	The proposed method to integrate the uncertainty to the final prediction score is the main strength of this paper. This method was well developed and described. It was tested on comprehensive datasets and the results were well analyzed. External comparison against several advanced models were done that show the superiority of the model.	Few weaknesses are noted: (1) only a single dataset was used to train and test the model, making the generalizability of the model uncertain, and (2) it is not clear why the authors chose to discretize the overall survival prediction into three bins rather than perform a survival analysis (e.g., using a Kaplan Meier curve).	"(1) The author claims that the motivation for introducing evidential deep learning is to improve the prediction accuracy, which is untenable; Evidential deep learning is usually employed to address the ""know unknown"" flaws, rather than to improve the prediction performance of traditional models; (2) The description logic of the paper is confusing. MGMT methyl status prediction is a classification problem, while OS prediction is a regression problem, which should be solved by using evidence classification and evidence regression, respectively. However, the authors incorrectly used evidence regression to address the MGMT methyl status prediction problem; (3) Figure 2 illustrates the architecture of the proposed model, which obviously confuses the Dirichlet distribution and the evidence regression."	More details and descriptions are necessary to avoid the confusion by the readers regarding the data preparation, preprocessing and training. More importantly, the main contribution of the model is to perform the classification without needing for the segmentation labels. It is very important to verify if the model really learn from the target tumoral region to conduct the final classification.	The reproducibility of the work is adequate. The conceptual idea behind EDL is provided, Figure 2 summarizes the network architecture, and model parameters are provided in Table 1 / section 3.1. The code will not be publicly shared.	It is hard to decide the reproducibility of the paper.	As the authors were not shared some important details of the model development and training, it would be hard to reproduce the exact same experiments.	Please provide additional details about the patient population used and whether any cases were excluded and for what reason(s). Also, clarify whether all MRI scans were acquired pre-surgery. Please also consider redoing the overall survival prediction task as a Kaplan Meier analysis.	(1) The paper should be written carefully; (2) The proposed algorithm should be introduced seriously; (3) The experiments should be improved.	"Page2: it was mentioned that some studies recently highlighted the limitations of the studied models. It will be very interesting to briefly state some of this limitation and related such limitations to the reported results by other studies. For example, while some studies reported the prediction accuracy above 90 percent, the best performance of the challenge is less than 65 percent. The authors can connect such a significant difference between the performance with the limitation/challenges of the task. Section 2.2: please add a reference for the ""aleatoric uncertainty"" Section3.1: I assume the cross validation was done a subject-wise as there are several slices extracted for each subject. Please state this in the text to avoid the confusions for the readers. Section3.1: The description of the preprocessing steps seems to be valid only for the MGMT dataset in which 16 slices for each subject's scan from Axial view are extracted. While the BraTS dataset was fed to the feature extractor with 4 channel volumetric images. Please specify the preprocessing steps for these two dataset and clarify whether the EffNetV2-T backbone is a 3D or 2D model. An important question is about the 16 number of extracted slices. How these slices were extracted? Please also clarify how the final classification metrics were clalculated for each subject if it was based on 16 extracted slices? In general, this confusion over 2D or 3D data classification should be addressed to also avoid the inconsistency when compared against other external references. As an important comment, when performing the classification tasks over the whole image instead of the segmented area, there is always the risk of feature learning from irrelevant texture/spatial information. As there have been quite many methods introduced to somehow visualize the learned features over the image space, it would of great interest to include this analyses and investigate whether the model was successful in capturing the features from the target regions or irrelevant regions?"	This work presents an innovative use of the evidential deep learning framework in classifying MGMT status from multiparametric MRI scans. The study utilizes a large publicly available dataset and outperforms many of the existing state-of-the-art.	Both the motivation and architecture of the proposed model are incorrect.	The paper introduced a novel method and tested it on comprehensive standard dataset. The results were analyzed carefully and external comparisons have been performed. Model justification and result discussion were relatively well done.
194-Paper0506	End-to-end Learning for Image-based Detection of Molecular Alterations in Digital Pathology	This paper describes a method that uses k-siamese networks with an EfficientNet backbone to solve digital pathology tasks using one model instead of a typical two-staged approach (stage 1 being some method to identify which regions are important, either manual or using deep learning, and stage 2 being the actual classification task. The k-siamese network samples k tiles randomly from the original image, encodes them and then combines them to make a prediction.	The paper introduces an end-to-end trainable k-siamese network with random tile selection for predicting molecular alterations. The method is shown to be better than the two stage pipeline which requires auxiliary annotations for region-of-interest in the first stage and dense tessellation and aggregation in the second to make the slide-level prediction.	A CNN is proposed by combining k number of well known Siamese CNNs to predict molecular alterations for a number of different use-cases, such as microsatellite instability in colorectal tumors and specific mutations for colon, lung, and breast cancer.	The authors have indeed identified an important challenge for digital pathology tasks, identifying which areas in an image are important for classification. In addition, they use the EfficentNet model a backbone model that has significantly less parameters than other comparable models.	Interesting results using k-siamese networks	Proposing a variant of Siamese CNN to make a decision based on k samples instead of two samples which is generally done by a normal Siamese network.	What if the task is related to only a very small area of the original image? Would this approach still work compared to e.g. a manual approach that ROIs the area that is relevant? E.g. can k be learned based on the complexity of the task?	"No comparisons with MIL and other end-to-end pipelines (cited in the intro). Even though these are technically complex, comparisons with these methods are needed to justify the proposed pipeline. Does these methods give comparable accuracy or better? Recent papers such as ""Benchmarking artificial intelligence methods for end-to-end computational pathology"" (https://www.biorxiv.org/content/10.1101/2021.08.09.455633v1.full.pdf) provides deep insights into the problem with extensive comparisons against end-to-end deep learning models (including MIL and Vision Transformers along with the classic [12] paper that was compared in this manuscript) for tumor subtyping as well as predicting molecular alterations on publicly available datasets (which have been preprocessed in a consistent way). This paper comes with a well put together github that makes it easy to run these end-to-end pipelines: https://github.com/KatherLab/HIA."	Even though the authors show that making a decision for k tiles by a k-Siamese CNN is better than combining k decisions by a regular CNN, however, they failed to show that an end-to-end approach (where segmentation is not done) is better than the two-stage approach (where segmentation is done before the final classification). Because the Seg-Siam approach is a two-stage approach and the AUC of the Seg-Siam approach is slightly higher than the k-Siam approach, an end-to-end approach. It is possible that using a k-Siamese CNN over a regular CNN is the reason for getting higher AUC for the k-Siam approach than for the Two-Stage approach.	the methods are detailed, so someone should be able to reproduce their work.  The first data set seems proprietary though, not sure if they are able to share it, this could be added to methods if IRB does not allow to share this.	The explanations seems good enough to reproduce the paper but can't be sure since the code itself won't be released it seems.	Datasets and experimental setup are clearly mentioned.	Table 2: it's not clear what the percentage is in the first column for each result in this table, it is not defined.	See above. Comparisons with other end-to-end pipeline is important to justify the proposed approach. Specifically, the following GitHub makes it easy to run these end-to-end pipelines: https://github.com/KatherLab/HIA.	Authors can emphasize more on the strength of k-Siamese CNN instead of on an end-to-end approach over a two-stage approach, since experimental results did not support their claim (k-Siam vs Seg-Siam).	This is a reasonable approach for an important problem. The paper is well written and results are compelling.	No comparisons with end-to-end pipelines especially when these are widely available  via GitHub and easy to run.	Authors failed to show that an end-to-end approach is better than a two stage approach.
195-Paper2010	End-to-end Multi-Slice-to-Volume Concurrent Registration and Multimodal Generation	A multi-modal deformable registration algorithm is proposed, based on a combination of modern deep learning techniques, including both modality synthesis with CycleGANs, and DL-based deformable registration. It is evaluated on a large dataset of radiation therapy cases, with intra-session MRI with large slice spacing being registered to pre-operative CT scans.	The work presents a contribution to image generation by modality transfer accompanied by slice-to-volume deformable registration. Authors introduce end-to-end DL-based approach including CycleGAN-based synthesis, 2-D to 3-D registration and improvement of the generation by MIND-based supervision using the registration output.	The paper proposes MSV-RegSyn-Net, a novel, unsupervised, concurrent framework for modality transfer synthesis and MSV mapping in an end-to-end pipeline. The method is evaluated on two clinical datasets. The evaluation shows the mutual benefits triggered by the joint architecture, leading to better performance than state-of-the-art methods. The model is a methodological concept that is theoretically applicable to images of different modalities and quality, but also of different slice spacing, slice thickness or orientation. This has not yet been evaluated by the authors.	Very well written paper, thorough introduction of related work & own contribution, method description, detailed and systematic evaluation on a large data set.	Problem important in medical practice	Novel end-to-end approach for adversarial method coupled with registration, similar adversarial methods were previously used as an intermediate step to generate synthetic data. The large difference in texture resolution between the two datasets used demonstrates the potential for generalization of the method. The parameterization of the basic methods was optimized for fair comparison.	This is more of a system paper, describing a well working overall system; as expected, the inherent mathematical novelty in the method is limited - however many powerful techniques are combined in a smart way and thoroughly evaluated.	Unclear usefulness in the real multi-slice to volume registration References not related to the problem being solved Results are not reproducible	Lack of discussion of the limitations of the proposed method. The influence of the slice ratio on the performance of the method was not evaluated. The influence of the coverage of the field of view by the 3D CT and 2D SCT slices is not discussed. The main innovation is the combination of state-of-the-art building blocks in one method.	Very good; it starts from the related papers that contain source code and points out the algorithmic differences and additions.	The paper is not reproducible, as stated in the reproducibility list.	The datasets used are private. The code is not available, but is based on publicly available earlier methods. The method is generally well described and could be re-implemented to some extent, but the results are unlikely to be reproducible.	I have only very minor remarks: You mention X-Ray 2D-3D registration in the same context as slice-to-volume registration, however it is something fundamentally different, due to the accumulated data along a projection geometry. You refer to the supplementary material several times I believe; make sure this is really optional to understand the manuscript (possibly rearranging figures & text accordingly).	I have several major comments: It was mentioned that all volumes were resampled to given shape. Is it connected also to the MR volumes? What about the interpolation artifacts when the resolution of MR (or USG/Angio in other applications) is much lower than the related CT volume? It seems that the proposed method is dedicated more to low-quality 3-D to higher-quality 3-D instead of slice-to-volume registration. The reported registration time is about 2s. For registering 3-D volumes the reported time is understandable. However, the motivation behind the article is to allow real-time processing during the multi-slice to volume registration. Such a registration should be much faster to be useful during real-time applications. The main contribution of the article is the concept of using the generated image to perform the unimodal registration instead of directly performing the multimodal registration. The concept is interesting, however, was already explored in other works related to 2-D or 3-D registration. I suggest to more carefully review the literature and cite appropriate references. Even though the concept is interesting, the article is relatively chaotic and the method description is confusing. There are some minor language mistakes that should be corrected (basic grammar mistakes, can be captured by automatic text screening tools). The source code is not referenced or prepared to be referenced. The paper is not reproducible.	"In Fig. 1, the naming of the intermediate step as ""CT-to-MR translation"" is somewhat misleading. Probably, ""MR-to-CT translation"" is more intuitive with respect to the proposed approach. Fig. 2 shows a specific case where the proposed method fails. Please discuss why the method fails and how often it fails. Eq. 2: ""i; f _i \neq 0"" should be "" i: f_i \neq 0"". Section 2.2 begins with a description of the creation of a 3D volume for the 2D sCT layers. The information that the intervening layers are filled with 0s and that in fact all layers (and not only those for which 2D sCTs exist) are fed into the pipeline is given at the end. It would be helpful if this information was at the beginning of the paragraph. Related work is a bit dated and not state of the art, e.g. p. 2 ""Traditional Deformable Image Registration (DIR) methods like SyN [1], Demons [23,24]..."" with reference to methods from the late 90s is probably a bit too traditional ;) . The paper should be self-contained, definitions of losses and measures should be part of the paper and not in the appendix."	Well written paper, describing a well working method that demonstrates in a credible manner that the state of the art is improved upon.	Overall, the paper is interesting, however, chaotic and hard to follow. The concepts are not presented in clear way and the manuscript requires several passes to fully understand the presented concept.	I think this is a good paper and it should be accepted. I particularly liked the novel end-to-end approach of coupling the adversarial method with registration, showing that this offers significant advantages over separate sequential methods. However, the building blocks themselves are not new, and the novelty is limited to combining them in an end-to-end approach. In addition, some relevant aspects are not discussed in full detail, and the evaluation is limited to some extent, so the paper is more at the proof-of-concept level. It is difficult to foresee how generalizable the method will be. In summary, therefore, I believe that this is a fair paper that should be accepted.
196-Paper1271	End-to-End Segmentation of Medical Images via Patch-wise Polygons Prediction	The paper proposes using patch wise polygons and neural renderers as the decoding branch of encoder-decoder deep architectures for segmentation purposes. This allows using an arbitrary resolution before rendering segmentation masks on the initial input size. The approach is evaluated on several medical and non-medical benchmark datasets and achieves top results.	This paper presents an image segmentation method in which the object edges are modeled as a polygon with vertices. The method obtains multiple stage-of-the-art results in several public datasets.	The main contribution of the paper is the novel way to model the edge of segmentation boundaries using polygons with multiple vertices. These polygons will then generate a raster image via neural renderer.	* Using neural renderers in a segmentation task is novel. * The method is evaluated on several challenging benchmark datasets and achieves top performance.	1.This paper adopt a neural renderer to translate the polygons to binary raster-graphics masks for optimization purposes of current activte contour methods. 2.The method achieves considerable results on both medical image datasets and a non-medical dataset.	The proposed method for medical image segmentation is novel in the way that it is not directly output pixel value segmentation but a geometry representation instead.  To make this formulation trainable, the author use neural renderer to convert the representation into pixel map where cross entropy and dice loss can be applied.	* The paper needs fundamental improvements in terms of structure and writing (see detailed and constructive comments). The current form makes it very difficult to understand the rationale on why the proposed approach should provide better segmentation performance when compared to classical approaches.  In particular, the authors states that a second type of output representation is provided, which seems not correct the final output is based on a rastered image, the latter being similar to classical approaches. Therefore, it is remains unclear why a classical approach could not learn an equivalent mapping. The related work section contains a enumeration of segmentation approaches and lacks conclusions on what is lacking in the literature and how the proposed approach could solve current issues. Fig. 1 uses many concepts that are introduced much later in the paper, making it difficult to understand. * The performance reported in Section 4 is impressive, but it is not clear how internal parameters (in particular s and k) were optimized for the various datasets. Only one parameter sensitivity analysis is reported in Fig. 6 of the appendix, suggesting high performance variation (well above inter-algorithm variations in Table 4) and overfit. The mIoU reported in Table 4 correspond to cherry picking the top performance among all parameters tested in Fig 6 of the appendix (i.e. 90.92 for k=5 and patch size=2^3).	1.According to the training script in the code,the proposed method is a single-class segmentation method. There is no such claim in the paper, which will make readers confused.  2.There is no figure to present the CNN model structure. Although it is not a hard requirement in the paper, it would largely help the reader understand the design of the techniques. 3.The references are limited.	The description of the network architecture is a bit hard to follow. It is better to have an image illustrating the overall structure and flow of the network.	The paper uses publicly accessible datasources and provide their code as supplementary material (not tested). The reproducibility is very good.	1.Please specify the single-class segmentation method in the paper. 2.It would be better when you show the figure for the CNN model. 3.There are several excellent papers you may cite to enchance the references:         AFP-Mask: Anchor-free Polyp Instance Segmentation in Colonoscopy         Colorectal polyp segmentation by u-net with dilation convolution	Some details are missing. The authors seems just check everything 'yes' in the checklist. Missing one of the loss function definition, i.e. L_{BCE}. Missing software framework and version. No new dataset proposed. Missing training time for the proposed network.	"* Provide a section on k, s parameter selection for each benchmark dataset * Report average and standard deviations of the performance in every table over parameter choices if no clear strategy is available to fix k and s beforehand. * Improve the structure of the paper with the following:     1. The first paragraph of the introduction states that a second type of output representation is provided, which seems not correct the final output is based on a rastered image, the latter being similar to classical approaches. Therefore, it must be clarified why a classical approach could not learn an equivalent mapping.     2. The introduction mixes related work and methods, without really introducing the problem that is addressed. The sentence ""Active contour methods"" is unclear and seems misplaced.     3. The caption of Fig. 1 is using lots of elements that are only introduced on page 4. Please reformulate.     4. Section 2: ""In another contribution, an attention maps to each feature map in the encoder-decoder block..."" This sentence is incorrect.     5. Section 2 contains an enumeration of segmentation approaches and lacks conclusions on what is lacking in the literature and how the proposed approach could solve current issues.     6. In section 3, the output domain of f_1 is \mathbb{R}^{C x H/32 x W/32}, please justify why ""32"".     7. Section 3 states that ""The decoder f_2 contains two upsampling blocks to obtain an output resolution of s = 8"", but s is a free parameter with various values used in Experiments. Please clarify. * Typos are present, please verify the entire document (e.g. ""patche"")"	The multi-label segmention is common in medical images. It will be a more efficient technique to extend the method presented in the paper to multi-class tasks. The contour methods would be effective if the segmentation of the object can be represented by one closed polyline. What if there are more than one polyline, for example,  a tire which should be represented by two circles.	The representation of a polygon with k vertices is not clearly described. The output is 2k + 1 for each polygon, then what does 2k and 1 means? Does it also mean all the polygon have k vertices? What is the k value then? Where does the map M come from? How to choose suitable size for map M? There is a discussion on the advantage of polygons that it allows one to be rasterized at any resolution. Could you further explain this point? why is it important for image segmentation task?	* Novel approach for segmentation using neural renderers. * Unsatisfactory paper organization. * Unclear optimization of internal parameters and risk of overestimation of the performance.	The technique is well designed. The results are considerable. The clarity and organization of the manuscript is poor.	In general, the presentation is lack of illustration for readers to follow. The proposed segmentation method is novel and the experimental results demonstrate its advantage. My main concern is the presentation of the paper only slight above average. Therefore, my initial rating is weak accept.
197-Paper2651	Enforcing connectivity of 3D linear structures using their 2D projections	The paper proposes a novel method to improve continuity in the segmentation of 3D elongated structures by minimizing a 2D connectivity loss in multiple 2D projections. The 2D connectivity loss itself was recently proposed, but has not been used in medical imaging (not extensively or not at all) and its application for 3D segmentation is a neat idea.	better segmentation of 3D linear structures (e.g. vessels) by using 2D topological aware loss on projections ground truth can be annotated on projections for higher efficiency	The authors used an existing 2D loss function on three 2D projections of 3D datasets. In addition, they have performed experiments on multiple 3D datasets, which showed improved topology-aware scores.	Interesting, novel approach to ensure connectivity in elongated structure segmentation Smart way of incorporating a recent, successful 2D region separation loss in 3D segmentation Good validation showing convincing improvements compared  to logical baselines as well as to previous methods proposed to improve topological correctness	effective translation of technology from 2D satellite image analysis (with its focus on connectivity for street network analysis) to 3D medical image analysis elegant solution to prevent gaps in the segmentation of linear structures	The authors have done a detailed evaluation of three datasets and achieved an improved topology-aware score over existing topology-aware loss functions. The method can offer 3D segmentation from 2D annotation.	I missed information on the datasets, especially the number of images in each set and train/test splits. Parameters were selected based on their performance on one of the test sets, whereas for previous approaches Perc and PHomo parameters as suggested in the papers were used. While there was no extensive tuning for the proposed method and the same parameters were successfully used in multiple datasets, this makes the improvements with respect to the previous topology-aware approaches less convincing.	reproducibility (see below for details)	The authors mentioned that occlusion is an example of this formulation not guaranteeing connectivity preservation. However, the dataset used in this example has occlusion very often. This is a huge contradiction. How would the authors still argue in favor of their approach? The technical contribution of this paper is heavily sacrificed by the plug-and-play application of an existing loss [23] in 3 canonical projections of 3D space. The literature review is incomplete. The authors failed to cite the following papers on topology-aware loss function (Byrne et al. STACOM 2020, Shit et al. CVPR 2021), including methods that work directly on 3D data. While APLS and TLTS were proposed for 2D scenarios, e.g., road networks, I am not convinced that it is still a good topology metric from 2D projection because of the abundance of occlusion. On the other hand, Betti error would be a better alternative to topological metrics.	Reproducibility seems OK, public data and largely based on a publicly available code.	Apparently, the authors have not understood how to fill out the checklist: for many questions answered with yes, the information is actually missing in the paper (e.g. no tests for statistical significance, memory footprint, analysis of situations in which the method failed, etc.). Since no code will be published, these details will remain unknown.	Details about the baseline models are missing, e.g. whether trained on 2D or 3D.	In the introduction, perhaps clarify that only in some cases indeed 2D annotations can be used to train 3D models - in many other applications, 2D projections of 3D volumes would not give sufficiently reliable information to enable annotations, but the proposed 2D connectivity loss would still work. Is Fig 3 the brain dataset only? please clarify in the caption. Would it be possible to include results for Perc and PHomo with parameters selected in a similar manner as for the proposed method?	While the paper generally features a good description of training parameters,some details remain unanswered, e.g. what the window size for L_TOPO calculation is. Fig 3: which use case does this describe? Table 1: does bold mean statistical significance? I hope yes, but please clarify in description.	"Do Perc and PHomo also work on the projection of 3D output in combination with the MSE loss? Otherwise, it will not be fair to compare. Authors should provide more details on the baseline models for reproducibility. On page 3, the authors claim that ""continuity of 3D structures implies continuity of their 2D projections."" However, that is irrelevant. One should look for how discontinuity in 3D can be captured in optimal projections in 2D. It may require having more than three canonical projections. What is b? It is not explained anywhere in the text. Why does MSE in 3D perform worse than MSE in 2D for the Brain and neurons dataset? Since the data is highly sparse and most of the time, the error is a false negative, did the author apply any weighting for the cross-entropy? Why didn't they consider Dice as a baseline since it handles mild class imbalance pretty well? Why has no 3D metric been reported? Since the main task is in 3D, authors should consider evaluation in 3D."	Original idea and convincing results (even taking into account that parameters of compared methods may not be optimal). Best paper in my stack.	The main idea is really nice and weights more for me than problems with reproducibility, I believe MICCAI audience will like this work.	Limited technical novelty, and weak and unexplained assumptions heavily undermine the empirical evidence of performance improvement. Hence I recommend rejection.
198-Paper1940	Enhancing model generalization for substantia nigra segmentation using a test-time normalization-based method	The authors use two 3D U-Net vanilla models in a sequential manner to segment substantia nigra in MRI. This model represents a coarse-to-fine cascaded network. The first network generates a ROI that is used by the subsequent network to segment substantia nigra. Furthermore, during inference stage, the authors propose a test-time normalization to boost segmentation accuracy. The final segmentation is the average probability among the input images from the time-test normalization. Validation is performed using an atlas-based metric.	This paper introduces substantia nigra segmentation from T2-weighted imaging since these scans tend to be more readily available in large open-source datasets. The authors show that using a test-time normalization (TTN) method can help increase segmentation of substantia nigra (SN) accuracy. The same model was used on several different combinations of preprocessed data, such as using histogram matching and an asymmetric loss (ASL) function on the training set, to test which procedures worked best for getting the best SN segmentation. Ultimately, TTN with ASL, was shown to be the best method.	This paper presents a test time normalization method using an affine registration and histogram matching to improve the model generalization of substantial nigra segmentation during the inference time. It is said to be resulting in increased segmentation accuracy and the estimation of model uncertainty. Proposed results tend to perform better than the SOTA in terms of mean Dice score and in unseen datasets.	The authors proposed a novel way to use test-time normalization. An affine transformation and histogram matching are used to normalize the input query. 84 cases were used for training, 52 for testing, and 20 for validation.	This paper is extremely detailed in writing every step taken to create the model, run the model, and even the values for each hyper-parameter used and tuned. This makes the methods highly reproducible. Although the model was trained on in-house data, which is not accessible to others outside the lab, anyone trying to reproduce the method could use the 2 open-source databases of images the paper mentions, to train and try out the model. This is a novel approach to getting accurate segmentations of the substantia nigra without using an atlas or without using imaging such as a neuromelanin scan. T2w, as mentioned by the authors, is not typically an imaging modality used for this kind of segmentation, however, this is an image that is typically acquired, clinically, next to the T1, and therefore is more readily available in open-source datasets. This definitely has clinical relevance, as being able to segment the substantia nigra and potentially perform some quantitative metrics such as size or any other relevant clinical feature metrics can be of great value. For the most part, every step of the pipeline is explained really well. The reasoning behind including steps, such as histogram matching, is also explained really well in the 'Discussion' section.	A novel TTN method based on spatial and intensity attributes for accurate SN segmentation and better model generalization. Prior atlas-based likelihood estimation to examine the segmentation output on unlabeled datasets. Fair comparison of the proposed work against the research being done in this application.	The contribution is merely incremental. 3D U-Net, sequential networks, and time-test normalization are well known techniques. There is a lack of description of the in-house dataset. How many experts labeled the in-house dataset? Experimental section must be expanded. Average Dice Index is not enough to assess the performance of the proposal. Specially when the difference is < 2% (Table 1).	N/A	"Some of the areas of the paper are unclear and need to be clarified. For eg. post-processing re-threshold to maximize H^, How is N=10 for support set chosen? Instead can this be set based on some pre-defined Dice threshold? In the Qualitative evaluation, authors claim TTN helps the model to identify SN regions better without showing the GT. To this end, labeled sample could have chosen. Further, it is unclear from Fig 3 that ""the estimated uncertainty maps indicated larger oscillations in the boundaries of SN"". I see these oscillations throughout the segmented regions. Also, a color bar could be useful here."	Two datasets are publicity available. Experimental part sounds technically correct, but short.	In-house data was used to train the model. Open-sourced data sets were used for testing generalization of the methods The authors give very detailed descriptions of the methods used which therefore makes the methods highly reproducible. This is mentioned in strengths as well, even though in-house images are used for training the model, the other 2 publicly available datasets that the authors used for generalization of the method can be used to reproduce the results. Also, speaking to the data, the authors were able to generalize their model to 2 publicly available datasets	Authors have done a good job in providing experimental details and the employed datasets to help reproduce their work.	Authors need to improve and extend Experiments and compare their proposal with SOTA methods. Authors need to improve Conclusions. They only summarized the paper. Minor corrections: In Page 2. deep learning technique have achieved -> deep learning techniques have achieved In Page 2. Inspired by the work of Alice -> Inspired by the work of Le Berre et al.  In Fig. 1 prposed -> proposed In Section Methods squentially -> sequentially In Section 4 dice -> Dice	This was a solid paper, there weren't too many things to comment on, except for a few suggestions... It may be helpful, as far as reproducibility, to include metrics such as repetition time and echo time for the T2-weighted images collected in-house. At the end of the methods section, the authors mention proposing a post-processing re-threshold procedure, this should be explained. Maybe give a few sentences on what exactly was done regarding re-thresholding, post processing.	"Qualitative evaluation section can be improved by choosing examples to compare against the gold standard. Some sentences can be phrased better specifically starting sentences of a Section for eg. Section 2.1, ""To further boost the SN segmentation..."". Similarly Section 2.2 Writing this in active voice will help the reader understand the essence of that section better. Please explain how a large computation time can be handled during the inference stage moving forward."	Marginal contribution. No comparison of the proposal against SOTA methods. No conclusions.	This was a solid paper that gave a very detailed description of the method. The authors considered previous methods and implemented methods they thought could increase accuracy of their model. They even generalized to publicly available data. They used an atlas in this case because publicly those datasets did not provide annotations. The novelty is not only in the approach but in the fact that they used T2-weighted images to do the segmentation, knowing that this is an image that is typically available. Other groups have tried to solve this problem using the T2-weighted image, however, this does not take away from the novelty of this method.	Although this work could be potentially beneficial towards diagnosing one of the neurodegenerative disorders, the novelty is not substantial enough to be acceptable considering MICCAI standards. Further, the qualitatively results do not clearly showcase the essence of this approach in segmenting the desired structures.
199-Paper1749	Ensembled Prediction of Rheumatic Heart Disease from Ungated Doppler Echocardiography Acquired in Low-Resource Settings	The paper describes a deep learning pipeline for automatic processing of colour Doppler images for the purpose of detecting and grading rheumatic heart disease. The pipeline works with non-ECG-gated data and is intended for use in low resource settings.	This work presents and evaluates a deep learning method for diagnosing rheumatic heart disease (RHD) from Doppler echocardiography. It starts with data homogenization to identify two specific echo acquisition planes and identifies the left atrium during ventricular systole. Then an ensemble model is used to predict RHD. The ensemble includes a 3D multi-view CNN that analyzes all frames during systole as volume data, and a multi-view Transformer that evaluates the images frame-by-frame. The results demonstrate the benefit of combining these networks into an ensemble.	The authors propose a pipeline for RHD diagnosis with color doppler echocardiograms (CDE) in a low-cost setting. The authors preprocess the ungated CDEs with deep-network-based view selection, frames of interest selection, and left atrium (ROI) localization to generate video clips in ROI with A4CC and PLAXC views. Then the authors use a 2-view 3DCNN network and a 2-view 2DCNN+Transformer network to predict the RHD probability simultaneously, and use a maximum voting result of these two probabilities as the final prediction. The authors validate their method on a 591-patient dataset, and achieve a better result than a previous RHD prediction method. This study focuses on CDE obtained by hand-held ultrasound devices, which are low priced and easy to deploy in low- and middle-income countries.	I like the focus of the work on the low resource setting, which is an issue that is often overlooked in the research literature. The paper is generally well written and easy to understand. The authors have put together a pipeline that seems to be robust and performs similarly to human experts.	The paper is very well written. The clinical motivation to use ungated Doppler to diagnose RHD in low-resource settings is compelling and significant. The deep learning stragegy is logically motivated. Evaluation is on a relatively large point-of-care dataset. Performance of automated RHD diagnosis is on par with expert clinical assessment.	The problem is relatively new, interesting, and has not received much attention, but it would have great potential and impact.  There are few works on automatic RHD diagnosis with CDE; the most related one is [Ref. 19] in the paper. This study focuses on CDE by hand-held ultrasound devices, which are low priced and easy to deploy in low- and middle-income countries. This work is very practical and can benefit people, especially children, who live in low-income conditions. Our community probably would pay more attention to this kind of study.	Some details of the training/validation procedure were unclear. Details of statistical testing were not presented clearly.	The deep learning strategy is an integration of existing methods.	Organization: In Section 3.1, the authors use too much space on implementation details of preprocessing networks, which causes not enough room for the experiment section to include comparisons with other methods. Lack of technical novelty a. All models in this paper are from existing methods. Both view classification and frame selection use ResNet, and ROI localization use VGG based LinkNet [Ref. 26]. The first prediction models are a 3DCNN model and a DenseNet+transformer model, which are widely used in the community. The straightforward maximum voting is used to aggregate two RHD scores by prediction models. b. The whole framework is complicated. The pipeline is long, resulting in minor errors at the beginning of the pipeline and would become large at the end with propagation. Unsound evaluation a. The authors only show their experimental results with their proposed method. There is no comparison with other methods, for example, other echocardiograms classification methods, ultrasound classification methods, or even video action recognition methods for natural videos.  b. The authors only show their method with the data after processing. Showing the experiments on the data with less preprocessing (view selection, view classification+frame selection, and view selection+ROI localization) would be helpful for readers to understand how challenging the original task is. These can be a part of the ablation studies.	"Overall reasonably good. The model was clearly described although code has not been made available. One point of concern though - in the checklist, the authors answered ""Yes"" to ""The range of hyper-parameters considered, method to select the best hyper-parameter configuration, and specification of all hyper-parameters used to generate results."" I agree that they specified final hyperparameter values but I did not see any description of the range of values tested or how the hyperparameters were optimised (see detailed comments below)."	The paper provides a substantial methodological description.	The authors filled out the reproducibility checklists, but will not release their codes and dataset in the future. Then implementation details of all models in the paper are well described. It is easy to follow the paper and reproduce their work.	"I enjoyed reading the paper and found it to be mostly well-written and easy to follow. But I was left slightly frustrated by an occasional lack of detail, especially with regard to the training/validation procedure. For example, in Section 2, the authors give details of their dataset of 2136 Doppler videos. They also mention that 95 of these were annotated with view information, systole frames and left atrium segmentations. Later (Section 4), it is stated that the training/validation/test sets were 5108/1277/1510 images. Why switch from talking about videos to images in this way? I found this slightly confusing. Also, the numbers given in Section 4 are for the rheumatic heart disease (RHD) detection task, but no mention is made of the training/validation of the preprocessing steps (for which only 95 videos were available). What training/validation split was used here? And was there any overlap between these 95 and the data used for training/testing the RHD task? I.e. is it possible that some of the RHD test set had been ""seen"" before when training the preprocessing steps? Regarding hyperparameter optimisation, as noted above the authors stated their final values and (at least for the RHD detection task) which data were used to optimise them but did not mention the ranges of values tested or the strategy used for optimisation. For the model description, I also found the text slightly unclear. The first model uses 3D CNNs but the input data are 4D (64x64x3x16) so presumably one dimension was handled by defining multiple input channels? Which one? Later, for the transformer model it is stated that 3 colour channels are used so I presume the same approach is adopted for the 3D CNN model but please state this explicitly. And does every video clip have 16 frames, all of which are of the same view? This should be mentioned in Section 2 if so. Finally, the results of the 3D CNN and transformer models are combined using a ""maximum voting strategy"". I presume this strategy only makes sense when there are multiple A4CC/PLAXC videos for a subject? E.g. if there is just one of each and they disagree how does maximum voting help? What do you do if the votes are split equally? The results of the statistical testing are also not clearly presented. What was being compared to what? In Table 2, there are two symbols (*, **) indicating p-values of 0.03 and 0.04 respectively. The second column from the right has both of these symbols - so how can a statistical test have two different p-values? I'm sure I have misunderstood something but this is because the results have not been clearly presented in my opinion. Finally, there are no asterisks in the rightmost column - does this mean no statistically significant difference was found on the test set? Other more minor comments: The Introduction mentioned that RHD is often associated with mitral regurgitation (MR) and cited some prior work on detecting MR. But then the rest of the paper is all about RHD. Is there a clinical need to detect RHD? Or MR? Or both? In the data used in this paper, what proportion of the subjects had MR and were there subjects who were either MR +ve/RHD -ve or MR -ve/RHD +ve? The last sentence of the Introduction seems like it should have come earlier, not as the closing sentence of this section. Also in this sentence: ""life even"" should be ""life and even"". In the introductory text of Section 3.2 I would recommend the authors explain why they are introducing two different models (3D CNN and transformer based) as I was confused by this at first. They could mention here that they will be used as an ensemble later. In Section 4, the authors state that they use 5-fold cross validation for hyperparameter optimisation. But then which model was used for final testing? Were the best hyperparameters used to retrain using the entire training set? The last paragraph of the Discussion highlights a very important point in my opinion. The authors might want to comment on what level of expertise is required to acquire images that are of good enough quality to be processed robustly by their pipeline. What evidence do they have that ""minimal training"" will be enough to acquire such images?"	Please refer to the comments on strengths and weaknesses.	Suggestion: If possible, move all implementation details in Section 3.1 and experiments of preprocessing to supplementary, which saves lots of room for more comparison experiments. If possible, run ablation studies for preprocessing. If possible, run comparison experiments with other methods. If possible, provide inter-observer annotation results. Future works: It would be helpful to develop a single- or two-stage model, which would highly speed up the training and inference. It would be helpful to develop a model with fewer parameters, which can be deployed on mobile devices, such as smartphones, and laptops without GPU.  With this setting, handheld devices can be connected to mobile devices and directly predict the probability after imaging. It would be helpful if the authors could release the dataset in the feature, which would attract more researchers to contribute to this problem.	I liked the work done by the authors and the paper was written well. But there were too many details left out or presented with a lack of clarity for this to be of a standard to be accepted at the MICCAI main conference.	Excellent presentation, compelling clinical problem, logical methodology, good dataset and evaluation.	The major factors are: a new and exciting task impact on the community and society The paper brings a good problem and application to the community, which leads the rate to a positive side. However, some unignorable weaknesses lead the score to a 'weak accept'.
200-Paper0439	Estimating Model Performance under Domain Shifts with Class-Specific Confidence Scores	The manuscript describes a method for DL model output probability calibration, which is to add parameters for class-specific tunings.	This paper addresses the problem of estimating the predictive performance of a machine learning model on unseen data, where no ground truth labels are available. The authors adopt the concept of average confidence scores and state that the confidence is especially miscalibrated on imbalanced datasets. The main contributions are class-wise confidence calibration methods that greatly improve the performance estimation.	Various methods have been developed to estimate how well a trained model will perform on out-of-distribution data. These methods do not account for class imbalances. The authors propose new class-aware modifications to such methods to take rare classes into account when estimating a model's confidence. They carry out a thorough evaluation for both image classification and segmentation on different distribution shifts.	The description of the method is clear.	Performance estimation on new, unlabeled data is a highly relevant problem for implementing trained machine learning models into clinical routine. The miscalibration of overconfident classifiers is another problem on its own. The authors tackle the first problem by solving the latter, which is an interesting approach. Existing methods are extended by class-wise confidence calibration, which helps to improve performance estimation. The proposed method is extensively evaluated on different classification and segmentation datasets with introduced domain shift. The authors clearly show the benefit of global vs. class-specific calibration in Tab. 1.	The evaluation is very thorough and for multiple use cases (classification and segmentation). The data corpus includes many publicly available datasets. The results show a clear improvement after taking advantage of the proposed modification. The paper follows a clear structure and maintains good writing quality. The authors clearly state how their method is different from existing ones and the math is rigorous. Figure 2 also provides a nice summary. Only very limited prior knowledge on the topic is required to understand the paper. The authors also explain how they adapt their methods to image segmentation.	The method described is not innovative. Probability calibration is a well-studied field. There are many well established methods that needs to be compared to (read On Calibration of Modern Neural Networks by Guo et al.). In addition, class-specific calibration has been discovered and widely adopted to address issues caused by training data imbalance (read Improving class probability estimates for imbalanced data by Wallace et al.). This is more than a domain adaptation problem but a general ML problem. To tie it to DA is restricting. Result. The Authors should also compare to unseen test set performance within the same domain to evaluate how calibration works without domain shift. Result is unreliable. Fig 3 shows that the predicted accuracy values does not distribute well from 0 to 1, making the linear fitting unreliable. The manuscript also does not explain well how probability calibration is an important topic for medical imaging problems.	The main weakness of this paper is that it states class-wise calibration, especially with temperature scaling, as own contribution without acknowledging prior work. E.g., Guo et al. (2017) already suggested vector scaling as class-specific extension to temperature scaling. Kull et al. (2019) and Nixon et al. (2919) further discusses these topics in the context of calibration error metrics. The actual contribution of this paper seems to be the use of class-wise calibration in the context of performance estimation. This should clearly be stated as such. Given my first point, the actual novelty of this paper seems quite low. Calibrated performance estimation has already been intensively investigated by Guillory et al. (2021) and class-wise calibration is also not novel. Luckily, the authors clearly show the benefit of the combination of the two approaches (see strengths). I expect Tab. 1 to at least include MAE +/- std dev - or even better, show box plots instead. Instead of using bold font to highlight the best MAE, proper statistical tests to show statistically significant improvements are much appreciated. Kull, M., Perello Nieto, M., Kangsepp, M., Silva Filho, T., Song, H., & Flach, P. (2019). Beyond temperature scaling: Obtaining well-calibrated multi-class probabilities with dirichlet calibration. Advances in neural information processing systems, 32. Nixon, J., Dusenberry, M. W., Zhang, L., Jerfel, G., & Tran, D. (2019, June). Measuring Calibration in Deep Learning. In CVPR Workshops (Vol. 2, No. 7).	The authors state that their method differs from class-distribution-aware TS for long-tail problems and list a few reasons. Yet I have trouble grasping how significant these differences are. It would have been interesting to get a comparison on long-tailed problems of the authors' method with the other existing methods. I find it unusual how the background class is handled for segmentation task. Similar methods such as [29] also require a separate handling for the background compared to other classes. It might be interesting to verify how relevant the Dice for the background is for the problem at had, especially considering that the background is most often the majority class in segmentation problems and that this Dice is rarely reported.	The method should be simple to implement.	With the additional information in the supplemental material, it should be able to reproduce the results of the paper. The use of public datasets further helps with reproducibility.	Sufficient reproducibility is ensured.	Probability calibration, especially in neural networks, is an interesting and important topic. The Authors should take a look at the current SOTA probability calibration work (many review papers) and go from there.	I think this paper would be a good fit for MICCAI. However, the contribution statement has to be fixed prior to acceptance and the results have to be presented as described above (see weaknesses).	"Writing: Since the authors aim for American English, the correct spelling for ""labelled"" is ""labeled"" Please check all the indices in Fig 2 for DoC, ATC, and TS-ATC Eq (4): ""d_y'"" - ""d_j"" Some other suggestions: Abstract: ""be deployed or its performance"" - ""be deployed or if its performance"" Section 2 Method: ""achieve the goal"" - ""achieve this goal"" Section 2 Method ""all the training pairs for the case z of totally n pixels"": ""of totally n pixels"" should be rephrased. Figure 1: there is too much content for a quite small figure. At least fill the entire page width, but I would recommend removing some details. Table 1: Even though the best results are in bold, they are not easily noticeable. Supplementary material: Section C appears empty because the table appears on the next page. Maybe rearrange the section placement."	Low innovation level of the work. Unconvincing experiment & results.	The novelty is low, but the addressed problem is important and the proposed method seems effective. I therefore suggest a weak accept.	The quality of this paper is above average. The content is clear, the authors clearly explain the novelty of their methods and the results are good.
201-Paper1563	Evidence fusion with contextual discounting for multi-modality medical image segmentation	The paper proposes a method to process multi-MRI image separately and merge the segmentation results using the formalism of Dempster-Shafer theory. The merging part is learnable.	Authors propose  a DST and deep learning-based multi modal evidence fusion framework with contextual discounting. the method was evaluated on the BraTs 2021 dataset. In particular, they claim their method is able to take into account the uncertainty of the different sources of information compared to probabilistic approaches.	The authors propose a novel evidence fusion framework with contextual discounting for multi-modality brain tumor medical image segmentation. For the first time, the authors proposed  an evidence discounting mechanism. The experiment demonstrates that their method outperforms the best previously published results for this task.	The paper estimates the importance of each input (different MRI modality), without treating them equal (common practice), which is to help with the final accuracy.	The authors proposes a multi-modal deep learning and DST based framework or brain tumor segmentation. They investigated the contribution of each module component in their framework. Quantitative and qualitative evaluation was provided.	The paper designs an evidential segmentation module based on Dempster-Shafer theory and an evidence discounting mechanism take into account the ability of each modality.	the method lacks substantial algorithmic novelty, and has weak evaluation	The limitation of the study is not discussed. Discussion on qualitative results on the scans with less dice score will be helpful. The results shown in Table 1 does not show significant improvement. Memory footprint is not discussed, the complexity of the network is increased in this method while not giving significant improvement (around 1%).	The descriptions about evidence discounting mechanism is very vague. The writing is confused for readers. A large number of errors exist.	the paper is well written, and should be reproducable	The authors agree to provide the code, so the results can be reproduced. they demonstrate the performance of their method on widely-used publicly available dataset.	The description of the proposed method is not clear and the paper may be not reproducible.	1) conceptually the idea is simple - train 4 independent encode-decoder networks (with slight modifications) , and learn the fusion (merging) of segmentations at the end. The paper frames it from the  Dempster-Shafer theory angle, but essentially presents a simple trainable merging approach.  2) Evaluation/comparison is lacking modern sota approaches  (including nnUnet). Instead the method is compared to many U-net like methods of 2018 or older, which is not necessary.  3) Since the brats2021 dataset is chosen for evaluation - why not report  the results on brats2021 official evaluation hidden test set (their server accepts submissions)	It will be helpful if the authors can discuss the limitation of the proposed method. Some examples where the dice score is less can be visualized with possible explanation. In results section, few sentences can be added to explain why the proposed method is useful if it is not showing significant improvement in dice score. Few lines on how it can be used together with any state-of-art method? Minor comments: Typo in the first line of Page 5.	"The number of citation is disorder, such as number of the first citation is [23]. English spelling problems :""the to uncertainty""should be replaced by""to the uncertainty"". ""can be extend""should be replaced by ""can be extended"" The two important compared articles (""Peiris, H., Hayat, M., Chen, Z., Egan, G., Harandi, M.: A volumetric transformer for accurate 3d tumor segmentation. arXiv preprint arXiv:2111.13300 (2021)"" and ""Zhou, H.Y., Guo, J., Zhang, Y., Yu, L., Wang, L., Yu, Y.: nnformer: Interleaved transformer for volumetric segmentation. arXiv preprint arXiv:2109.03201 (2021)"") are all published in arxiv. More comparison with published papers with peer-review should be given. ""2.1 Evidential segmentation module"" is same with subtitle ""Evidential segmentation module"". Dempster-Shafer theory be combined into segmentation module. However, the Dempster-Shafer equation is not differentiable for optimization, how to deal with the model optimization? equation 9a uses different fonts for Typesetting."	the paper explores the segmentation and fusion of multi model MRI from a Dempster-Shafer theory, but essential trains  4 encoder-decoder networks with a fusion module, which lack a substantial novelty. the evaluation lacks sota method comparisons.	The paper has good potential. It is well written. Thorough experiments and ablation study are done.	The proposed method is novel, but the description and writing is relatively poor.
202-Paper0960	Evolutionary Multi-objective Architecture Search Framework: Application to COVID-19 3D CT Classification	The author pays attention to the problem of searching the neural architecture problem. By proposing an objective, which is called potential, the author balances the exploitation and exploration operation on finding out promising models and reducing the search space during weight training.	"The paper proposes a neural architecture search based on ""potential"", a regression parameter fitted to the history of model accuracy. Higher values of ""potential"" indicate more promising architectures (subnets of a supernet) which are then more likely to be further exploited by a evolutionary algorithm. The method is evaluated on three separate COVID classification tasks (all 3D CT chest images) and shows promise of previous methods."	This paper proposes a more stable neural architecture search (NAS) approach for three COVID-19 prediction from 3D CT data. Based on the observation that weight-sharing (WS) strategy in NAS   incurs search instability, the author proposed a new objective called potential, which is used to explore more 'potential' or promising sub-nets.  Combined with accuracy, the proposed method can get more compact model with better performance on three public COVID-19 3D CT datasets.	The author proposes a new objective which can be optimized during training, namely potential. By mutating and cross-overing operations, the author balances the exploitation and exploration during weight training step. Numeric experiment shows the effectiveness of the method.	"The use of ""potential"" for evulation-based neural architecture search seems interesting and novel, at least in the context of medical image classification. The paper is well-written and interesting. The application is relevant to the medical imaging community."	This paper tackle an important problem as NAS instability is a big issue for neural architecture search. The experiments show a improvement over three public COVID-19 3D datasets.	Details in preliminary is controversial. The author mentions the advantages of mini batch but chooses size 1 as the best size. These two opinions are opposite. In section 3.1, the author mentions that E is a vector recording the epochs, then it should be a list of numbers of epoch count. How can the transpose of such a vector multiply itself in formula 3? What is the meaning of (E^TE)^(-1) and E^T F? The number of specific item influences the results if formula 3 is right. Results of experiments should be put more clearly. For example, figure2 is very hard for comprehend a conclusion.	The evaluation of this approach is quite narrowly focused on COVID chest CT classification. The architecture search algorithm itself seems more general and it would have been interesting to see the performance on several other classification tasks, including for example 2D chest x-ray classification. Furthermore, the baseline comparisons are based on methods proposed for the same sets of data (also both by the same group of authors). It could have been interesting to see how the proposed method compares other popular neural architecture search techniques, e.g. DARTS.	Search 3D neural architecture is not new[1], and it seems not hard to transfer such method to 3D CT data. The authors could highlight the difference caused by the domain knowledge. More important, the Regression Methods[2], like ordinary least squares (OLS), random forests (RF), Bayesian linear regression (BLR) are commonly used in NAS for performance evaluation and NAS acceleration. From this perspective, it seems the novelty of this paper is limited. The authors should compare the proposed potential objective to such methods to prove its advantages. [1] Video Action Recognition Via Neural Architecture Searching, 2019. [2] Accelerating Neural Architecture Search using Performance Prediction, 2018.	The author provides code and data, but not a requirement for acceptance.	Very good. All results are based on public datasets and the authors promise to make code avaialbe.	The authors claimed to release the code once accepted.	The author should describe the potential object in a clearer way, especially the formula 3. The author should rewrite some paragraph and fix the typos, such as 8.57 hours instead of 8.57 ours. The author should explain the experiment results in a clearer way. In fig 2, the axis title is missing and the conclusion drawn from three figures is missing in the title.	"The method seems relatively general and therefore I would have liked to see it being evaluated on a more varied set tasks, e.g. both 2D and 3D medical image classification. I realize that the change to segmentation tasks might require more changes to the pipeline and should be reserved for future work (as mentioned by the authors). Typo: Fig. 2a) ""ours"" -> ""hours"""	As I mentioned in the weakness part, please highlight the difference between the proposed method and the other NAS acceleration methods, like [2]. Seems the authors did not provided the searched architectures for different datasets. Are the architectures are very similar or very different from each other? In addition, what kind of insights we can get from the resulted neural architectures? In the abstract, the authors mentioned that: Recent works show that no model generalizes well across CT datasets from different countries. Basically, for my understanding, the authors searched three architectures for three COVID-19 datasets from three different countries. That would be great if the authors could search for one unified architecture which provide superiority over all datasets. Minor issues: I suggest to make all 'i.e.,','e.g.,', and 'et al.' in italic. Method [16] in table 1 should have a name. Typos: section3.3, relu6 should be relu. Please also check other parts.	From the perspective of models, The author proposes a new object for optimize, namely potential. The author should put more clearly on the definition. In formula 3, the calculation of potential is confusing. What is the meaning of multiply a vector of number of epochs and its inverse? Why is the higher the potential, the more promising the model? The one-sample mini-batch is controversial. I think such a trick cannot be included in mini-batch range. What is the border line between exploitation and exploration? Under what circumstances we choose exploitation? From the perspective of experiments, In the right bottom picture of fig 1, the author points out three kinds of points. But what do the x-axis and y-axis mean? The title is missing. The fig 2 and fig 3 is hard to track. It is better to affiliate the conclusion with the title. From the perspective of writings and organization, There are many typos that need to be fixed. In fig 1, the one hot vector is wrong for MBConv	While I find the evaluation a bit narrow based on only one task (3D chest CT classification), the paper overall is well written and results are convincingly presented and are promising. Therefore, I would tend to accept this work.	As mentioned before, the novelty of the method.
203-Paper1856	Explainable Contrastive Multiview Graph Representation of Brain, Mind, and Behavior	This paper proposed a model to coupling the structure and function activity of brain on graph representation.	This paper used GCN for contrastive learning of structural and functional multimodal data and used the model results to analyze the strength of the structure-function coupling patterns between functional connectivity, structural connectivity and behavioral performance.	The paper mainly proposed a novel heterogeneous contrast subgraphs representation learning based method to exploit the coupling of structural and functional connectivity from different brain modality.	A lot of fancy features in the model: dynamic, heterogeneous, explainable, causal model...	In general, this paper is comprehensive and technical sound that will surely inspire future research along similar lines. The main message is brought across fully supported by the presented results.	the fusion manner of multimodal imaging in the paper is novel, effective and interpretable, which shows great potential and extensibility in many related clinical tasks.	Too much details are missing for the whole model. e.g.  rare hyper parameters were provided; how edge of graph was defined? no time-window used for dynamic FC? why distillation is necessary?  And for sex classification result, it's better to compare to other published methods, since a lot methods have been proposed for this problem.	However, I still have a few major concerns before possible publishment. (1) In the introduction, the authors mention several reasons why the previous method is not applicable, and in the article authors should emphasize how the method proposed in this paper solves these problems and why it has advantages over the previous methods. (2) Authors should add the parameters of the methods. It would be better to add some necessary arguments for Equations to make them easier to understand. The overall schematic illustration needs to be clear and easy to understand and highlight the innovative points of the model. (3) The first experiment used the causal explanation model to obtain the regions that play an important role in the classification, and the authors should further show and analyze them. (4) In the second experiment, three different FC data were used to assess whether the analytical approach of the role of SC is the innovative approach of this paper. Please add some details of this experimental approach or relevant literature.	there are not enough validation tasks for the proposed framework, e.g. to use external dataset or different combinations of imaging modality.	Too much details are missing for the whole model, so it will be hard to reproduce this method only from paper.	N/A	The reproducibility is good for the clear elaboration in method detail.	"Much more details should be provided. e.g.  hyper parameters, how edge of graph was defined? Was time-window used for dynamic FC? why distillation is necessary? I don't think the model can find ""cause"" instead of correlation. And granger causality is hard to applied in resting fmri with large number of node. 22 major ROIs for fmri is not enough. sex classification is not that good compared to current FC based result. where the ""structural connectivity"" come from? ablation study is necessary for this model."	This paper used GCN for contrastive learning of structural and functional multimodal data and used the model results to analyze the strength of the structure-function coupling patterns between functional connectivity, structural connectivity and behavioral performance. In general, this paper is comprehensive and technical sound that will surely inspire future research along similar lines. The main message is brought across fully supported by the presented results. However, I still have a few major concerns before possible publishment. (1) In the introduction, the authors mention several reasons why the previous method is not applicable, and in the article authors should emphasize how the method proposed in this paper solves these problems and why it has advantages over the previous methods. (2) Authors should add the parameters of the methods. It would be better to add some necessary arguments for Equations to make them easier to understand. The overall schematic illustration needs to be clear and easy to understand and highlight the innovative points of the model. (3) The first experiment used the causal explanation model to obtain the regions that play an important role in the classification, and the authors should further show and analyze them. (4) In the second experiment, three different FC data were used to assess whether the analytical approach of the role of SC is the innovative approach of this paper. Please add some details of this experimental approach or relevant literature.	Several minor limitations are as flollowing: the authors should testify the fusion of other modalities in HCP dataset for the sex discrimination to demonstrate its generalization of the proposed framework, such as fmri and dti, or smri and fmri. there is no colorbar in Fig 4, and it is not clear how to compare them. the used parcellation atlas is not detailed enough for fMRI.	Too much details are missing for the whole model. e.g.  rare hyper parameters were provided; how edge of graph was defined? no time-window used for dynamic FC? why distillation is necessary?  And for sex classification result, it's better to compare to other published methods, since a lot methods have been proposed for this problem with better results.	Interesting topic and novel idea.	the fusion manner of multimodal imaging is impressive
204-Paper2558	Explaining Chest X-ray Pathologies in Natural Language	The paper proposes an approach for extracting natural language explanations for conclusions from radiology reports.  The approach is used to generate and publish a new dataset, MIMIC-NLE.  The paper establishes performance baselines on this dataset.	This paper introduce the task of generating natural language explanations (NLEs) to justify predictions made on medical images. As a first step, authors created MIMIC-NLE, the first, large-scale, medical imaging dataset with radiological NLEs and contains over 38,000 NLEs, which explain the presence of various thoracic pathologies and chest X-ray findings. In addition, authors proposed a general approach to solve the task and evaluate several architectures on this dataset, including via clinician assessment.	The paper introduced the first dataset of natural language explanations (NLEs) to justify predictions made on medical images. The authors validated a novel approach to generate NLEs for multi-label classification. It automatically distilled NLEs from radiology reports from the MIMIC-CXR dataset and created a new dataset called MIMIC-NLE. The paper also proposed self-explaining models that learn to detect lung conditions and explain their reasoning in natural language.	The paper proposes an interesting extension to the MIMIC CXR dataset and a general method for producing such extensions.  Both numerical and clinician evaluations are performed.	- a new dataset is presented and will be useful for community - presented an approach to provide NLEs for multi-label classification - evaluation is validated by a clinician.	The paper is interesting. It proposed to fully capture how evidence in a scan relates to a diagnosis. Specifically, the predictive model aims to detect lung conditions and explain their reasoning in natural language. The new dataset MIMIC-NLE (38,000 high-quality NLEs from the over 200,000 radiology reports) can be served as an important resource that provides chest x-rays with diagnoses and evidence labels and NLEs for the diagnoses. The paper proposes the first general approach to provide NLEs for multi-label classification, that encourages new research on  NLE for chest X-ray interpretation.	"There is a strong focus on explanations for positive findings, however radiographic exams are often used to rule out hypothesis.  Natural language explanations for negative findings would be relevant as well. Some aspects of the work are simply asserted, without proof or reference. E.g. ""we observe that [...]  a small selection of phrases, ... are very accurate identifiers"" How accurate?  How did the authors avoid confirmation bias here? E.g. The evidence graph was constructed using prior radiologist knowledge and by empirically validating the coocurrences.  How many radiologists?  Authors or non-authors, and could the empirical validation be provided? The example discussed of how ""a model that generates generic NLEs that make reference to Lung Opacity"" will yield a good score"" may indicate a more serious problem than the authors suggest. The baseline results provided are not entirely convincing."	- Contribution is limited. Presented a new data collected from already public dataset. - I could not find any explaining factor in the paper. Traditionally, explaining means that authors will explain how their results/method they used in not a black box but an explainable. (Title of a paper is misleading) - Previous methods are used to set the baselines results. Authors claimed they proposed a new method.  - all baselines are from previous studies including DPT is inspired from previous SOTA method. DPT leverages a DenseNet and GPT-2. - This paper lacks a discussion on the experimental results and motivation analysis. - - The results lacked visualization or statistical analysis.	It is difficult to evaluate NLEs with automatic NLG metrics. The GT NLEs obtain an absolute rating score of 3.2/5 is quite low. It's not clear to me that an absolute rating score of 3.2/5 can be explained by a generic inter-annotator disagreement between the clinician and the author of the reports or it comes from another source. The author can verify this assumption via an additional evaluation. The clinical evaluation should be performed by a consensus of clinicians.	The reproducibility statement claims code, and script to reproduce the results will be released, but I see no placeholder for this in the text.	It is OK, and the authors will release the source code.	The authors claim that their dataset and code will be made publicly available upon acceptance.	Spelling error p 4 p 4 radiolographic	- Contribution is limited. Presented a new data collected from already public dataset. - I could not find any explaining factor in the paper. Traditionally, explaining means that authors will explain how their results/method they used in not a black box but an explainable. (Title of a paper is misleading) - Previous methods are used to set the baselines results. Authors claimed they proposed a new method.  - all baselines are from previous studies including DPT is inspired from previous SOTA method. DPT leverages a DenseNet and GPT-2. - This paper lacks a discussion on the experimental results and motivation analysis. - - The results lacked visualization or statistical analysis.	The clinical evaluation should be performed by a consensus of clinicians.	Natural language explanations are relevant but underexplored.  This paper contributes a new dataset and approach.  Unfortunately, several aspects seem to be asserted without strong enough basis, and the limitations in scope of the resulting dataset are not sufficiently discussed.	Altough the new data is useful but paper lacks discussion. The motivation and detailed discussion is missing. Title of a paper is misleading, explaining refers to show that method is explainable with visualization. Paper lacks the visualization.	New large-scale, annotated dataset and new evaluation framework for chest X-ray analysis with natural language explanations.
205-Paper0081	Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation	This paper proposes a modified semi-supervised method for image segmentation. They design two losses to separately promote model regularization and inter-class separation. They also conduct comprehensive experiments to show the effectiveness of the proposed method.	This paper proposes the SS-Net for semi-supervised medical image segmentation via exploring the pixel-level Smoothness and inter-class Separation at the same time. The pixel-level smoothness forces the model to generate invariant results under adversarial perturbations. Meanwhile, the inter-class separation constrains individual class features should approach their corresponding high-quality prototypes, in order to make each class distribution compact and separate different classes. The methods are evaluated on two public benchmarks.	The authors proposed a deep learning framework for semi-supervised medical image segmentation. Pixel-level smoothness and inter-class separation are explored in the proposed framework. In the experiment, two public datasets are used for evaluation.	The paper is very well organized. The motivation and ideas of the two losses are clear and straightforward. The proposed methods are helpful as verified in the experiments.	I found this paper well-written and easy to follow. Especially, it is good to use a toy exemplar data to help readers understand their two key insights and motivations. The combination of VAT-based strong perturbation with class separation is well linked to their two insights. The experiments are well designed with rigorous ablation study. The method will not introduce obvious computational complexity.	Two different datasets are used for evaluation. The proposed method has been compared to five other semi-supervised segmentation methods.	The main concern is that the proposed two methods are both existing semi-supervised solutions. The authors adopt adversarial noises from VAT [13] with a small modification of discrepancy measurement and the inter-class separation strategy is the same as [1]. Most of the equations are very similar to the original papers. The authors should clearly explain the novelty of their method compared to [13] and [1]. The performance gains are limited.	"Since the paper is highly related to two directions, i.e., strong perturbation and class prototype learning, related work should be more comprehensive, with more relevant methods regarding to their insights. For strong perturbations, authors can discuss more relevant papers about it besides the VAT they borrowed. For example, French, Geoff, et al. ""Semi-supervised semantic segmentation needs strong, varied perturbations."" BMVC (2020). Also, for the class prototype-based methods to push feature separation and compactness, authors can also discuss relevant papers, including but not limited to: Xu, Z., et al, All-Around Real Label Supervision: Cyclic Prototype Consistency Learning for Semi-supervised Medical Image Segmentation. IEEE Journal of Biomedical and Health Informatics (2022). The method introduces some other parameters compared with previous methods, like in their adversarial perturbation and the tradeoff. It will be good to see some sensitivity analysis to help readers better understand the method and tune for their applications. The supervised results are very low with your lowest portion of labeled data. It is sometimes due to the batchnorm module will have negative effect trained with very limited data. Like, if you train MT model but turning off the L(unlabeled), it can also serve as the supervised method, and the results will be normal. I suggest using InstanceNorm here to avoid this problem."	The contributions are a little bit over-claimed. The technical novelty of this work is limited. Detailed comments please see below.	As the authors claim that they will release their codes, the reviewer has no other concerns.	The authors promise to release their codes.	The supplementary material provides detailed algorithms. No code is provided. Not sure about reproducibility.	Overall, I think this paper has merit on the proposed method and adequate experiments. However, the paper can be improved in a few ways. Some details about Inter-class Separation are missing in Sec 2.2. How to select the subset of F_l according to the predictions' confidence? Is there any threshold? What is the detailed structure of the attention modules? What is the starting value of lambda and the specific warming-up function? Backbone selection. Why do the authors use different backbones for LA and ACDC? In addition, the authors should demonstrate whether all the comparisons were conducted with the same backbone. It seems from both Table 1 and Table 2, the performance increase is very limited under 8 labeled settings. It's difficult to assess whether the model is significantly better than the baselines. It would be helpful to provide the variance or testing models under more labeled/unlabeled settings. A few other minor comments: What do the red arrow and orange block represent in Fig.2? Some of the notations in Fig.2 do not correspond to notations in the paper, e.g. X_{L} and X_{l}, Y and y. Some descriptions are missing, e.g. P_L, P_X. What do 'y' in Eq(2) and 'y_c' in Eq(3) represent? The reviewer thinks they might be typos. Please double-check all the notations and equations.	More comprehensive and relevant literature should be discussed, i.e., other strong perturbation SSL and prototype-based SSL. Sensitivity analysis is needed since there are many hyper-parameters.	"Regarding to the first contribution, ""fewer labels"" and ""blurred targets"" are two common challenges in medical image segmentation task and existing semi-supervised methods always aim to address these challenges. In addition, the idea of using ""constrain the pixel-level smoothness and inter-class separation"" to address these challenges is not new, as the authors mentioned in the Introduction section (paragraph 2) that existing methods did the same thing. For the second contribution, using adversarial perturbations for medical image segmentation has appeared in previous works (e.g., [A], [B]), which is not the first in this paper as claimed. [A] Adversarial Perturbation on MRI Modalities in Brain Tumor Segmentation, IEEE Access, 2020 [B] Towards Robust General Medical Image Segmentation, MICCAI, 2021 The proposed framework is more like a combination of two existing works and has limited technical novelty. For the two parts of the proposed method, the ""Pixel-level Smoothness"" part is basically the work from [13] and the ""Inter-class Separation"" part is very similar to [1]."	The paper applies previously proposed methods to a new scenario, medical imaging. From a theoretical perspective, the novelty is very limited. However, considering the motivation is clear and inspiring, I recommend a weak accept based on the understanding that the authors will release their codes.	I appreciate the insights and good use of figures in this paper to support their claims. Therefore, I lean to acceptance.	The contribution is a little bit over-claimed and this work has limited technical novelty.
206-Paper2742	Extended Electrophysiological Source Imaging with Spatial Graph Filters	This paper addresses the ill-posed problem of source reconstruction in EEG or MEG. The idea of the method is to be less sensitive to spatial high frequency activation. The method proposes a law rank representation to estimate the different parameters on a projected subspace spanned by a low-frequency graph basis.	The authors provide a novel method for estimation of both source location and extents . They provide a new that exploits the graph structure defined in the 3D mesh of the brain by separating the graph signal into different frequency subspaces, where they project the signal.	This paper investigates extended electrophysiological source imaging with spatial graph filters. The simulation tests have been carried out in detail.	The strength of this paper is to project the problem in a subspace with lower frequency, and ask for temporal and spatial regularisation, that is feasible thanks to the representation of the problem with graphs.	The method proposed is novel and the  results presented are very convincing for the need of the method. The rational - starting point of the method used makes absolute sense and overall is a well written- well structured paper.	In general, this paper is technically sound and the topic is interesting.	There is too much supplementary material, better write a journal paper to be more comfortable, this would allow you to give more details in the method, or limit your paper to synthetic data analysis. As it is, it is frustrating to have only partial information.	The main weakness of the paper is the lack of information about the tuning of the parameters and their optimal selection. In methods were multiple regularizers are employed a critical point is the difficulty of the selection of the regularizers parameters (a,b,g etc). Especially when comparison among such methods is made it needs to be sure that all the parameters are optimized (for fairness).	Here, there are some comments of this reviewer: 1 In the introduction section, the literature review must be strengthened. Avoid lumping references as in [2,21,24,32], [30, 22, 5, 4, 1] and all others. Instead summarize the main contribution of each referenced paper in a separate sentence 2 In this work, how to guarantee of the convergence of the ADMM used in the final algorithm? 3 The proposed method might be sensitive to the values of its main controlling parameters. How did you determine these parameters? Please elaborate on that. 4 In practical applications, noises may be non-Gaussian noises. Have you considered such non-Gaussian noises? Please discuss how this would impact the results and conclusions of this study.	No code, no data available, nor algorithm given in the paper.  However, the paper gives enough details to re-implement the method. The parameter tuning is not mentioned, that provide from reproducing results.	The authors have done a very nice work in terms of reproducibility and the supplementary text helps a lot towards this direction, especially if the code will be released. One minor note, the authors answered positive in the question: The average runtime for each result, or estimated energy cost. But the average runtime is not mentioned	I think the reproducibility of the paper is good.	Did you try other metrics to evaluate the performance of the method? To me, when looking at Fig2 (3rd row), the MNE method gives a more suitable result than the proposed method that eventually covers a quarter of the hemisphere. I do not understand why on real data, the proposed method gives such a sparse solution compared to other. This is counter-intuitive when regarding at the results on synthetic data. On real data you mention the highly diffuse activation of other method, but on synthetic data your method is the most diffuse. There is too much supplementary material, better write a journal paper to be more comfortable, this would allow you to give more details in the method, or limit your paper to synthetic data analysis. As it is, it is frustrating to have only partial information.	"Will start for more details about the main weakness of the paper. The proposed method includes a,b, and l . The authors they do not mention a lot how the tuning of this parameters affects the results and how susceptible to error is based on a suboptimal selection of the parameters. Furthermore when coming to comparison between other methods with less parameters (e.g. I think sloreta has only 1 regularizer) is even more important to include the influence and easiness of tuning them. Other comments of less importance Fig. S.5.1 I n the case of 10 db. The performance difference between the proposed method and all the others seem significant while from table 1 this is not the case (From auc the method is not even optimal). Could you elaborate on this, might be an indication that the metrics are not optimal and some metric of mutual information - or correlation might be better? ""We set the length of EEG to 1 second"" - Not very clear to me you mean the whole EEG was 1 second? Why such a small choice is it much more difficult the the graph setting will be used in bigger segments? In case where you have two distinct areas of activation (simultaneous) e.g. left and right temporal lobes, instead of 1. How would this affect the performance of the method? Especially the ""Forcing"" of neighboring signal (which helps in the case of single area of activation, as well as the low rankness when you have multiple ""areas of activation""  (e.g. temporoparietal network) Define tr  (is trace ok but you need to define it)"	Please specify details of the computing platform, programming language and parameter settings used in this study.	see comments above	The paper is of good quality and of interest for publication the idea behind the use of the method has ground and the results presented are very good since outperform methods that are being used for year as sota. There is a need for clarifications on the comparison between those methods especially for the tuning of the hyperparameters.	In general, this paper is well written. The major factors are as follows: 1 The contributions are clearly demonstrated. 2 The paper is technially sound as the verifications of this work is sufficient. 3 The presentation is acceptable.
207-Paper2316	FairPrune: Achieving Fairness Through Pruning for Dermatological Disease Diagnosis	The paper introduces a method for modifying a pre-trained classifier to achieve fairness with respect to certain sensitive attributes. The method is based on identifying network parameters (nodes) with high saliency difference between different demographic groups and then pruning those nodes. This is a novel way of pursuing fairness compared to the traditional adversarial-training-based strategies.	The paper proposed a pruning approach to remove the bias of models to sensitive features, specifically skin tone and gender. The approach operates on a pre-trained model and has the added benefit of reducing the model size. Each parameter of the model has different importance for different groups' accuracy scores, therefore pruning the parameters based on their importance removes the effect of the sensitive parameter and bridges the accuracy gap between the two groups.	The paper describes a method for increasing the fairness of machine learning models whilst minimising the drop-off in accuracy for protected groups. The method is based on the idea of 'pruning', which is an approach normally used for reducing model complexity. The authors propose a novel metric of parameter saliency that enables the pruning operation to act to address lack of fairness in the model.	This is a new way of debiasing a model that is conceivably more stable and efficient than the adversarial strategies. It does not require re-training the model. The results are relatively comprehensive on two large datasets.	The paper is well-written and easy to follow. The method is intuitive and has clear motivation. Bias in machine learning datasets is a critical issue, thus there is wide clinical applicability for the approach, specially since it has the additional benefit of smaller models after pruning. The related work section gives a good overview and taxonomy of the existing methods for the task of overcoming dataset bias. There were many comparative baselines from different de-biasing method categories.	As far as I know, the proposed idea of using pruning to improve fairness is novel The paper is well-written and easy to follow Experiments are extensive and include analysis of the effects of key hyperparameters	The proposal is confined to binary senstive attributes. Future work should extend to categorical (race) and continuous variables (age).	The attribute examined in the second dataset as sensitive data is the gender. First, it should be mentioned whether there is a clinical difference among genders regarding skin conditions. Moreover, the difference between the two groups even for the vanilla method was marginal, thus it is critical to justify the clinical need for fairness in this case. The groups are named 'privileged' and 'unprivileged'. I would replace these terms since they are not descriptive of the particular situation. 'Privileged' is a group that achieves higher performance on the vanilla model, however, that collides with the common use of the words privileged or unprivileged that are used for majority and minority groups that enjoy different rights and face different societal issues. It is crucial to mention how many samples were included in each group of 'light' vs 'dark' skin and 'male' vs 'female' and whether there was class imbalance. Why is one group achieving higher performance over the other? Could it be attributed to more training samples originating from that group? It would be interesting to show that the proposed pruning approach works for more complex architectures like ResNet-50 and is not only performing well on simpler architectures like VGG-11. The experiments were not repeated or cross-validated, thus no standard deviation was reported.	The literature review on fairness in medical imaging applications is slightly limited	Reproducibility should be good if the authors can release their code.	The datasets used were publicly available. There was no mention that the code would become public upon acceptance.	Overall good. I would like to see code made available if the paper is accepted. The authors do state that a grid search using the validation set was used to set the beta and pruning ratio hyperparameters. I would also like to see a statement of how other hyperparameters were set, e.g. batch size (for pre-training and for saliency calculations), learning rate, number of minibatches used for saliency calculation.	I generally enjoy reading the manuscript. Although there seems to be some intuition given in the text, I still feel that the proposed approach ends up being a series of engineering steps without clearly showing why the objective of Eq. 4 is equivalent to fairness. It seems that the authors refer to fairness as the accuracy difference between demographic groups. If so, please explicity define because there is broader definition of fairnes which I don't believe this artical is pursuing. Some technical questions in case I missed something. 1. Why \Delta E (change in objective function) has to be identical to accuracy drop? 2. Why can we ignore the third-order terms in Eq.1 if \theta is not near zero? Minor: bold the accuracy columns in Table 1	The word 'significantly' is widely used in the results and discussion sections. Since no statistical tests were performed I would replace it with 'substantially'. It would be interesting, maybe as future work, to show how the method would generalize to a multi-class setting, for example if we performed the classification for each skin tone class separately. There is a minor error, in Table 1 for DomainIndep the calculated differences Diff are wrong, they must have been copied and pasted from AdvRev and not replaced with the actual values.	"I enjoyed reading the paper and found the central idea of using pruning for fairness to be very interesting. The idea is simple (as many of the best ideas are) but as far as I know this has not been proposed before so I believe this paper has a high degree of novelty compared to other MICCAI submissions. I believe the paper would be a good addition to the MICCAI program. The comments below are aimed at improving it still further. The paper is generally well-written, although see below for minor comments/suggestions. The introduction sets the methodological context quite well, and there are a good number of relevant papers cited from the computer vision literature. However, I thought that the review of papers on fairness in medical imaging was slightly limited. The authors cite Larrazabal et al [13] which is certainly relevant, but there are other equally relevant papers that were not mentioned. In particular I would highlight Abbasi-Sureshjani et al (https://doi.org/10.1007/978-3-030-61166-8_20), Seyyed-Kalantari et al (https://doi.org/10.1038/s41591-021-01595-0) and Puyol-Anton et al (https://doi.org/10.1007/978-3-030-87199-4_39). They could even distinguish between papers that assessed bias (Larrazabal et al, Seyyed-Kalantari et al) and those that also applied mitigation strategies (Abbasi-Sureshjani et al, Puyol-Anton et al) to make the discussion more relevant to this paper. Also, although the specific idea of using pruning to promote fairness is novel, a few papers have analysed the impact of pruning on fairness and so these could also be mentioned (https://doi.org/10.48550/arXiv.2009.09936, https://doi.org/10.48550/arXiv.2201.01709). In addition, as noted above I think it would be useful for the authors to state the approach they used to set hyperparameters for their model (& comparative approaches?) and the data used. Other minor suggested edits: * Section 1, para 1, line 5: ""rthe"" -> ""the"" * Section 1, para 1, line 8: ""turns to perform"" -> ""performs"" * Section 1, para 1, lines 10-11: Put dataset details in brackets. Also, I think ""ISIC 2018"" should be ""ISIC 2019""? * Section 1, para 1, line 16: ""with different"" -> ""from certain"" * Section 1, para 1, line 18: ""biased"" -> ""bias"" * Section 1, para 2, line 9: ""proxy of"" -> ""proxy for"" * Section 1, para 3, line 13: ""Besides"" -> ""In addition"" * Section 2, para 3, line 3: ""regularizing"" -> ""regularize"" * Section 2, para 3, line 4: ""sensitive related"" -> ""sensitive attribute related"" * Section 3.1, para 1, line 1: ""Given"" -> ""We define"" * Section 3.2, para 1, line 6: ""row i and column i of second"" -> ""row i and column i of the second"" * Section 3.2, para 2, line 6: ""and is biased"" -> ""which is biased"". Also, I think it should be ""biased against"" not ""biased for""? * Section 3.2, para 2, line 7: ""In the coordinate"" -> ""In the bottom illustration"" * Section 4, Baselines section, line 6: italicise ""DomainIndep""? * Section 4, Ablation study section, line 9: ""consistent"" -> ""consistently"""	The proposal is novel and the results are sufficient.	The topic is very interesting and the method is intuitive and well-explained. The experiments were repeated only once, the use of the wording privileged/unprivileged groups was confusing and there was little explanation to why gender in the case of skin disease classification is considered sensitive data. However, the discussed topic is interesting and the method could be widely applicable in such scenarios.	The points I raised above are relatively minor. The paper has novelty, is well-written and the experiments are extensive. The subject of fairness in AI for medical imaging is topical and one which I expect to grow in years to come, hence I believe there will be significant interest in this paper.
208-Paper2313	Fast Automatic Liver Tumor Radiofrequency Ablation Planning via Learned Physics Model	This paper proposes a surrogate model for liver tumour radiofrequency ablation using a non-autoregressive operator learning	Previous studies on automatic planning for Radiofrequency ablation commonly neglected the underlying physiology and used simplified spherical or ellipsoidal ablation estimates. The main contribution of this work was speeding-up biophysical simulations by using non-autoregressive operator learning, so that the proposed planning method could consider the biophysical effects of thermal ablation.	This work proposes the use of non-autoregressive operator learning approach for a very fast automatic RF liver ablation planning method.	Combining biophysical models and machine learning is a recent and developing area of research and the authors have made a good work in connection to planning of RFA Predictions were accurate and matching closely those using spherical models while taking into account patient-specific data Inference time similar to those of spherical models	"(1) This work focused on an important challenge on the automatic planning of RFA, i.e., how to estimate ablation zone quickly during planning with considering biophysical effects. (2) The authors proposed a noval method to enable fast, biophysics-based RFA planning. The authors considered the simulaiton as an ""operator"" and they used an operator learning approach. This is a new formulation for heat transfer simulaiton, which is interesting for MICCAI community, especially for computer assisted thermal ablation. (3) The results of the proposed method were constent with the simulation using finite difference solver, which showed that the proposed method has the potential for fast estimation of theraml ablation."	The work is well justified and solid, with thorough evaluation. The authors address an important and interesting problem in the domain of treatment planning. The approach, which is much faster, has potential to significantly improve planning for RFA and treatment efficacy. .	Ground truth taken from simulation rather than real data. And to that point, it is unclear how the threshold set for generating the ground truth was selected. Validation dataset is small on only 10 patients Simulation parameters taken in general rather than from a distribution from possible patient-specific values	(1) The method part was not clear (some important information was not provided) and it is not easy for readers to follow this work. (2) The results of the proposed method mainly depend on the reference solutions. However, there were two problems on the reference solutions: (1) formula 1 was not consistent with the formula in ref [2] and seems not right. (2) the simualtion resolution is 4mm, which seems too big for accurate simulaiton. The authors should clarify the two issues.	The authors need to further clarify which portions of their work is their main contribution. For example in the introduction they state that the work was inspired by DeepONet. How much of this work is novel compared to prior work? This work was compared against one reference ([15]). But it is not clear from only that comparison where the current work stands in the literature.	There are some details missing to be able to reproduce it fully but most of the architecture is described. However, datasets used for validation are not open.	It is not easy for readers to follow this work since the method part is not clear and no codes are provided.	Enough details are provided for repeatability in terms of parameters used. It is not clear if the code will be shared or the data would be available.	It is unclear how much of the architecture is different from that of DeepONet Please clarify how inputs of the branch network are put together in relation to the trunk network, is it only the (elapsed) time used as input? if not, is that the set of queries starting from 0s? please clarify. Overall, it would be great to validate this approach on a phantom model with known material properties, geometry and heat sinks. Are there 14 (text) or 15 (figure) convolution networks?	(1) The formula 1 seems not right as the advection term (a_vp_bc_bvgradT) and R(T_b-T) should be in seperated equations (see eq(2) and (3) in ref[2]). Please check it and make sure the reference solutions are right, otherwise the presented results may be wrong. (2) The authors used 4mm grid for simulation, which seems not reasonable -4mm grid will make the geometry of liver and vessel distorted, thus the simulation was affected. For example, Fig 3 shows that the vessels are even not continuous, how did the authors simulate the blood velocity and make it right? (3)  The method part is not easy for readers to follow and the following information should be clarified: a. it is not clear weather the input images are 2D or 3D. I guess it was 3D but the  figure 1 showed the input images were 2D. b. For the trucnk network, it is not clear weather the input was a time point or  multiple time points. c. What is the spherical source and what's physical meaning of the radius of the spherical source? minors: typo->section 2.1 : for a(an) advenction-diffusion-reaction problem.	The 4mm grid seems quite coarse. How would this affect results?  A figure to illustrate the details in section 2.2. would be very helpful. It is difficult to visualize the approach through text only. Please further clarify what electrode position means in this application. Is it the depth of the needle? Coordinates of the tip of the needle? From Table 1 the superiority of the proposed method over the Sphere method is not clear. The latter does not account for the heat sink effect of blood vessels which I believe is the benefit of the proposed method over the Sphere method. Would there be a measure/metric to demonstrate that? Is AE sufficient as a comparison metric? Is it possible to understand in what situations the Sphere method is producing better AE % compared to the proposed method (in Table 1)	Very interesting paper backed up with novelty	The idea of this paper is quite insteresting, and the results are good. However, the method part is not described clear and not easy for readers to follow.	Further clarification (see questions above) on novelty and results would greatly strengthen the paper.
209-Paper2730	Fast FF-to-FFPE Whole Slide Image Translation via Laplacian Pyramid and Contrastive Learning	The paper presents a GAN based model to synthetize FFPE image from FF samples. It proposes to use Laplacian Pyramids to increase computational performance.	The authors proposed a GAN-based model using Laplacian Pyramid frequency decomposition and Contrastive Learning via a memory bank to translate low-resolution FF into high-resolution FFPE-style slides.	This manuscripts describes a methodology that translates  one type of histological staining process (frozen, FF) to another (Parafin FFPE) as the frozen is much quicker to acquire but has lower resolutions and quality that the FFPE. The rationale behind this is good as the FF can be acquired quickly and then translated to a higher quality for further investigations.	The paper is well written and it is easy to follow. The method is clearly explained The idea of using Laplacian Pyramids even though already used in natural images, has not been exploited in Digital Pathology. *Good ablation study	The motivation for the work looks interesting. Collecting FFPE takes tremendous time when comparing it with acquiring FF. The work focused on providing a framework with efficient computation resource usage, i.e., training time and memory usage.	The paper is generally well written, with a clear rationale and proposes a methodology that revisits the oldie-but-goodie technique of the Laplacian Pyramid. The methodology allows the use of larger patches (from 512x to 1024x and 2048X) which indicates a more compact use of memory. The processing is also much quicker than alternatives.	*The paper focuses on increase in computational performance, and it does not devote any time to analyze the network. It would be interesting to see what the intermediate results of the Generator look like , for example the masks, etc *It was not clear in the MSI task prediction if the gain comes from the combination of FF->FFPE + data augmentation or only from data augmentation.	Using the unpaired dataset for training is totally making sense. However, for evaluation, since the motivation of this work is to transfer FF to FFPE, I strongly recommend the data be paired or at least ask a domain expert to grade the synthesis result, i.e., if the synthesized image is useful in clinical. The result shown in Table 1 does not convince me to use the fastFF2FFPE  (vs. AI-FFPE) even though the training time, memory usage, and inference throughput are better than the other two baselines because of the FID performance. Authors might need a sensitivity analysis to show results with different hyper-parameters.	Whilst the paper has many merits, there are some weaknesses. since the methodology is a naturally off-line process in which the FF is post-processed, the speed of processing is less important than the quality. After all, it is possible to process the tissue with FFPE and get good images anyway. Thus the focus should have been the quality. The quality obtained does not seem to be that good!  Best case on the GBM data was 46.85 but that is not comparable with other methodologies. At the same resolution the results of the proposed method are 49.67 against 46.89. For the LUSC set, the best of the proposed is 43.64 whilst the alternative was 34.81.  *The paper is fairly well written but at times is confusing, more details to follow.	The method is well described using public data, and the authors engage to share the code upon acceptance. The reproducibility seems good.	The author claimed that the data, code, and models would be released if accepted. The paper should be reproducible.	Seems to follow all the requirements on reproducibility.	The paper is well written and the idea is interesting because it has not been used in digital pathology before. The quality of the paper would increase if a deeper analysis of the LP method is included e.g. show what the internal representations in the generator look like, the masks, etc. If space is an issue, this can go in the Supplemental material.	T_m should be defined similarly to c_h. In the memory bank section, how is FF patches' size decided? 65535. In the dataset section, what is the relationship between images at 512x512, 1024x1024, and 2048x2048? Downsampling or independent?  If the images in 1024x1024 are downsampled from 2048x2048, why are there only 4K images in 2048? Please clarify. What is the batch size when training/testing fastFF2FFPE, AIFFPE, and vFFPE? Fig2. could add a zoom-in view to show how fastFF2FFPE overcame the artifacts. I assume the results are validated by the statistical tests. Please clarify.	"The paper makes several assumptions from the reader, who may or not be familiar with these. As such, it would be good that the details were clarified and not left to educated guesses or having to search for these. Examples ""steps, e.g., dehydrated, saturated with formalin and stained with dyes, whereas FF slides are produced in ultra- low temperature freezers with liquid nitrogen. ...""  There are well defined protocols to obtain FF and FFPE, please add references for these. ""However, there are many artifacts in FF slides and variations between FF and FFPE slides (see Fig. 1.a)."" Which artifacts exactly? Please add a list and if these are visible, add arrows to illustrate these in the figure. In Figure 1a What is the green line around the samples? I make an educated guess that the background has been removed, but readers should not be making educated guesses. Fig 1b uses FF patches as input and Isynth as output, but Fig 1c uses Iin and Isynth, there should be consistency or if the terminology is correct, the caption should explain the differences. Fig 1b ises Lrec, Lcl and Ladv, which are explain many pages afterwards. Same with M,N. Fig 2 The visualisation of the results is rather useless if I do not know what I am looking for. I do not even know if the proposed method is supposed to be better, why not add the accuracy metric for each case so that the reader see if the proposed method is better."	The paper is well written and it is easy to follow. The method however seems like a small incremental modification of the now commonly used CycleGAN.	The testing data and evaluation part is confusing.	This paper is interesting but the quality and not the speed should be the focus
210-Paper2428	Fast Spherical Mapping of Cortical Surface Meshes using Deep Unsupervised Learning	This paper proposes a spherical mapping of cortical surfaces using spherical convolution [29]. The proposed method infers a deformation field to adjust triangles on the unit sphere to minimize the mapping distortions. Three distortion metrics are used for computing deformation fields, and L2 regularity is introduced for a smooth velocity field. The experimental results show that the proposed method achieves a fast spherical mapping compared to the baseline methods.	Proposes a very efficient method to obtain deformation field to map between spherical mesh and brain mesh. The method achieves results with less distortion. The method works in an end-to-end manner in an unsupervised way.	In this paper, the authors proposed a novel framework based on Spherical U-Net for spherical mapping of cortical surface meshes. Compared with FreeSurfer, which is the most popular tools for brain images, the proposed method have fewer distortions and achieve a speedup for more than 200 folds.	A spherical mapping is key in many surface-based analysis for cortical surface registration, parcellation, etc. In this paper, a fast spherical mapping was proposed that significantly reduces the computing time over the classic approach such as FreeSurfer. The approach is non-supervised, which is also an advantage of the proposed method.	Efficiency: proposed framework runs fast. Clear writing: the paper is well written.	In this paper, the authors proposed a novel framework based on Spherical U-Net for spherical mapping of cortical surface meshes. Compared with FreeSurfer, which is the most popular tools for brain images, the proposed method have fewer distortions and achieve a speedup for more than 200 folds.	See the comments below.	Lack of experiment with ground truth.	The only weakness of this work is that Spherical U-Net has been previously proposed. This paper utilized it for spherical mapping.	Seems reproducible.	Probably reproducible.	The paper meets the requirements of the checklist.	The proposed method utilizes existing spherical CNNs [29]. The central idea of spherical deformation is based on Spherical Demons that employs diffeomorphic deformation over a stationary vector field using the scaling and squaring approach. This is also used in [24] for spherical CNNs-based surface registration. Although a combination of several metric losses is incorporated, this paper technically reads an application of what was proposed in [24]. A hierarchical optimization has been widely adapted in classic surface registration. I think the paper needs some acknowledgement of the previous studies. In terms of their methodological descriptions. Local charts of the velocity field are presumably predefined. To learn deformation fields, spherical CNNs need to support rotation-equivariance as initial mappings are not necessarily aligned. As such is absent in [29], rigid alignment and possibly data augmentation may be required for the proposed framework. In particular, [29] defines a spatial filter on an icosaherdral mesh with a specific order of neighborhoods, where the performance may depend on the alignment of initial mappings. The authors claim that the IAP is used due to its popularity, but the proposed method perhaps works because the initial mappings are already roughly well aligned? Please provide more in-depth rationale of the average convexity for this work. The average convexity roughly captures overall sulcal patterns but offers a rougher representation than other fine geometry such as mean curvature. Fine features might work better to capture local deformation. As the authors pointed, FreeSurfer has an extra step after spherical mapping that unfolds self-intersection. This is another bottleneck in FreeSurfer. Even if the proposed approach models diffeomorphism, the resulting deformation can practically have self-intersection since the re-tessellation introduces sampling errors and the deformation field is defined on an icosahedral mesh. The experimental results also need to be interpreted carefully. For example, if some method uses a conformal mapping, large distortion in distance or area is somewhat expected. Indeed, an isometric mapping is not possible, so any mappings have their pros. and cons. depending on which metric they optimize over. Since spherical mappings have several types (conformal, area preserving, etc.), I think a more fair comparison in this work would be to show performance of individual component (loss). This also lies along what the authors claim in the introduction. What was the step size for the scale and squaring approach? What is the memory requirement for the proposed method? It looks like the reported runtime of the proposed method exclude that of that of the IAP. This is misleading as the IAP is preprocessing in this work. This paper cites about a third of papers from a specific group, and some of them seem not strongly related to this work. Please focus rather on relevant studies. There are yet other studies for spherical mappings. For instance, Choi et al., FLASH: Fast Landmark Aligned Spherical Harmonic Parameterization for Genus-0 Closed Brain Surfaces Nadeem et al., Spherical Parameterization Balancing Angle and Area Distortions	"How is the coarse-to-fine mesh exactly generated? ""reparameterize it again using an icosahedron with higher resolution"" may not be sufficient and perhaps a reader would like to know the differences between the coarse and fine meshes after reprarmeterization. What properties are maintained after the transform? I am wondering if the authors have done any toy experiment with ground truth, although not reported in this paper. I understand that this method operates without supervision; but such an analysis would have been helpful. How much of a change is expected at each iteration? Is there a way to control how much to update per iteration (e.g., learning rate) so as to transform the mesh faster or slower?"	More than 800 cortical surfaces were processed by the method. Is there any failed case? If so, what is the reason for the failed cases.	See my comments above.	Clear novelty and outstanding performance.	FreeSurfer plays a significant role in medical imaging. The proposed work has fewer distortions and is 200-times faster than FreeSurfer.
211-Paper1680	Fast Unsupervised Brain Anomaly Detection and Segmentation with Diffusion Models	This papers introduce latent DDPM (Denoising Diffusion Probabilistic Model) into medical image analysis and proposes a new method for unsupervised brain anomaly detection and segmentation. DDPM as a new generative model has the potential to model the data distribution with high image quality. Based on the observation that if the input image is from a healthy subject, the reverse process will only remove the added Gaussian noise,  while if the image contains an anomaly, the reverse process removes part of the signal of the original anomalous regions, the authors proposed to compute a mask from the sampling, thus detecting and performing segmentation from brain imaging in an unsupervised fashion.	This paper proposed a novel unsupervised pipeline, based on vector quantized variational autoencoder (VQ-VAE) and denoising diffusion probabilistic models (DDPM), for brain anomaly detection and image healing. It achieves comparable performance with state-of-the-art algorithms while having the advantage of fast inference time, which may increase its value in clinical applications.	"The paper proposes a novel method combining a variational autoencoder with a codebook to encode images into a latent space with a denoising diffusion model to ""heal"" pathological images. This unsupervised model is then used to restore anomalies without the need for manual annotations and then use the residual between the restored and original image to segment anomalies."	The method using latent DDPM for unsupervised medical image analysis is new as there are limited works in this field. The proposed method is simple and seems reasonable. The proposed methods are evaluated on four public MRI and CT datasets. The experiment results show an improvement for some cases in an efficient way. The organization of the paper is also good.	The proposed pipeline is methodologically sound and very suited to the application. It is interesting that the introduction of DDPM significantly improves the performance of VAE-based methods. The authors performed evaluations on three different synthetic and real datasets, which is comprehensive. The detailed supplementary material and the nice video add to the thoroughness of the paper. The evaluation of the inference time demonstrates its clinical feasibility. This paper is well-written and well-organized.	The authors propose a clever solution to segment anomalies in an unsupervised manner combining two state-of-the-art techniques for image synthesis. The idea of compressing the original image into a latent space and then correcting that representation by making it closer to the previously learned healthy distribution is an interesting one. The model is tested with different public datasets with different types of anomalies (lesions) ranging from small lesions to tumors. Furthermore, execution times for a time critical approach are also provide to further validate the proposed method.	My main concern is the effectiveness of the proposed method. Basically, this architecture is based on latent DDPM [16, 19], and the unsupervised strategy is based on an observation. At least there should be some theoretical supports or visualizations for this observation. The performance of the unsupervised segmentation is worse than previous methods.	Lacks statistical analysis to demonstrate whether the improvements are significant. Also, it would be better if the authors can report the standard deviation of the Dice scores. To better understand the upper bound, it would be valuable if the authors could report the segmentation accuracy of supervised methods (maybe U-Net) in each experiment.	"While the paper is in general well-written (especially the detailed introduction), the writing starts to fall apart when it comes to the methodology. The abuse of notation only obfuscates the point the authors are trying to make. For example: why say that training samples follow the distribution of the training dataset with notation x0~q(x0)? This fact never comes again and seems superfluous. The same goes for most of the formulation for VQ-VAE and DDPM. There is a contrast between the way each concept and definition is presented mathematically in the original papers and the rough summary presented here for the same ideas. In fact, most equations are mostly taken as is with slight modifications (and some typos). A short summary of the most relevant ideas and a link to the original papers for the interested reader would have made the methodology clearer and easier to read. Why is there a need to combine two different synthesis methods for anomaly detection. Why is VQ-VAE not enough? Why are diffusion models applied on the latent vectors from VQ-VAE necessary? While I can see from the results that the combination is better than VQ-VAE alone I would have preferred a better intuition for the need of combining both. What happens if the diffusion model is applied on the original data and not the latent representation? Why is the upsampled mask from the latent space so important for the final result? What happens if the mask is used as is? Hypothesis or discussions on these questions would strengthen the paper. The results on the synthetic dataset are misleading. First of all, a dataset of 64x64 images seems like a less than ideal benchmark for segmentation where image detail is one of the most important things. Furthermore, the way the experiment is setup is unrealistic when compared to lesions in pathological brains (by randomly masking pixels). As a consequence, the impressive results obtained in Table 1 are far lower than those presented on real imaging datasets (Tables 2 and 3). In fact, the ensemble model obtains the highest results in Table 2. While this is compensated by the time comparison in Table 3, for segmentation purposes the results seem fairly low for all the methods (especially for small lesions). Finally, the metrics are either not defined or their definition is relegated to the captions. What does ""theoretically best possible Dice score mean""? How is that upper bound calculated? Why is AUPRC never defined (I assume it means area under the probabilistic ROC curve)? Furthermore, some other concepts are also poorly defined. I understand that the original image has HxW dimensions, while the latent representation has hxw, which implies a smaller size. However, this is never clearly stated. While it is implied by the fact that masks obtained on the latent space are upsampled at the end, clearly stating the relationship between the dimensions would help."	Not mentioned	None	While the code is not publicly available (and the reproducibility checklist says so), I believe the authors might have used the original repositories for VQ-VAE and DDPM which are publicly available. Even though implementation details are not given, it might be possible to reproduce the method partially. Furthermore, the results are presented using public datasets.	One of the main disadvantage of DDPM is its slow sampling speed. But the authors claimed in abstract that DDPM can achieve log-likelihoods that are competitive with transformers while having fast inference times. I really donnot undertand why a transformer[16] need about 10 mins for the inference in one 2d image. If so, the training process will be extremely crazy. Can you give me more explanations about why? More convincing Visualizations for both the observation (as mentioned before) and segmentation. At least, from the supplementary material I get limited information from the visualization. What do you want to emphasize in Fig. 1? For BRATS, there are three tumor labels, which one do the authors use? which region of the reported DICE score is for? Minor: What is the 'f' in tables 1 and 2?  The m in page 5 is referred before definition.	"In addition to the above major comments, please find below some minor suggestions: The authors mention that the proposed approach may be useful in 3D neuroimaging applications in the introduction, but all the experiments are done in 2D. It may be helpful to provide a discussion about this at the end of the paper. Although a video is provided in the supplementary material, it would still be helpful to the readers to show some example images together with their segmentations in the main manuscript. Small typo: ""using at diffusion model"" in the introduction."	"One thing that I think could paint the results in a better light, would be to present detection results instead of segmentation. In critical cases where detecting possible anomalies in time is more important than getting a good delineation, a good detection metric paired with a small execution time would be required. For example, a set of anomalous and normal images could be gathered and then the detection rate of anomalous regions could be used instead of Dice. That would help recontextualize Tables 2 and 3. Another aspect that I think could be improved is the methodology section. There is an abuse of math notation that is mostly lifted from the original papers but removing some key details and definitions. As a consequence, the summarized explanations are hard to follow and confusing. I would have preferred a high level explanation (which is partially given) with a limited use of notation complemented by the reference to the original papers for the interested readers. One example of this is the explanation of Lcodebook as ""We used the exponential moving average updates for the codebook loss"". Without checking the original paper and only reading the explanations on the manuscript it is impossible to understand what it is referring to. Finally, this is more of a personal opinion, but I think that it would be interesting to use classical unsupervised segmentation approaches to compare where possible. They are simpler, before the deep learning revolution they were extensively used, they are fast (especially if implemented in GPU) and they can also give good results. That would further help contextualize the idea of unsupervised lesion segmentation."	As mentioned before	This well-written and well-organized paper is methodologically sound, and the evaluation is comprehensive. I believe it will be interested to the community.	While I think the idea is interesting and it is great to see new unsupervised applications for deep neural networks that heavily rely on supervised training, there are some concerns that make me hesitate about the paper. The methodology seems to be unnecessarily obfuscated. I think it would have been much more valuable to give a general intuition about why these two methods need to be combined. Finally, the contrast between the synthetic results and real ones makes me feel weary about the contribution. According to the results, it is really hard to say that the proposal can actually address segmentation in real scenarios.
212-Paper2777	Feature Re-calibration based Multiple Instance Learning for Whole Slide Image Classification	The authors present a method for WSI classification using Multiple instance learning. They exploit the assumption that the features from positive instances have larger magnitude, and by recalibrating with the predicted highest magnitude instance (critical) they produce better separable groups.	This paper proposes a method built using the positional encoding (PEM) [26] followed by a single pooling multi-head self-attention block (PMSA) [19] modules for WSI classification. The main contribution is exploring the effectiveness of the feature re-calibration idea (which is used in few-shot learning) to produce balanced bags of +/- instances and, as a result, improve WSI classification. Two ideas are used for feature re-calibration: 1- max-critical instance embedding 2-  feature magnitude loss	The authors worked on multiple instance learning for whole slide image classification. The authors re-calibrated the distribution of a WSI bag (instances) by using the statistics of the max-instance (critical) feature.  The authors of this manuscript also proposed a balanced-batch sampling method to effectively use the feature loss and a position encoding module to model spatial/morphological information and perform pooling by multi-head self-attention with a Transformer encoder.	The paper is well written and clearly states the objectives of the work. The idea is simple but effective.	The problem, idea and proposed solution are clear. Specifically, the application of feature re-calibration for producing balanced bags is interesting. The organization of the paper is good.	The study objective is interesting. The authors used multiple instance learning for image classification and to solve ground truth labeling issues. The authors worked on the benchmark datasets. The authors reported that their approach outperforms the existing methods on CM16 and COLON-MSI datasets in terms of accuracy and AUC. The density plots between normal vs tumor and MSS vs MSI seem significant.  The authors checked the performance of their algorithm by varying some loss functions.	The evaluation on only two datasets, one of them is a bit outdated and the other one is in house.	The technical novelty is limited. The whole architecture is built on top of PEM and PMSA modules. The main contribution is exploring feature recalibration for increasing separation between +/- samples which has very limited application as it only works on the binary classification problem. I think a simple contrastive loss (or loss with weighted samples) probably makes this improvement while it is not limited to binary classification. The experiments are not also enough. Note the CAMELYON16 is a fairly old dataset. Further, there is no ablation study on gi hyperparameters.	The authors didn't include qualitative results. Hence, it is hard to review the classification performance. Multiple instance learning is not a new approach. Hence, technical novelty is limited. The authors didn't share their source codes.	The paper should be reproducible for the CAMELYON experiments, but it is not clear whether the in-house dataset will ever be released.	Implementation settings are available	The authors didn't share their source codes.	I would have liked to see a more in depth analysis on further datasets but I understand the limitations of space.	As mentioned before, I guess a contrastive loss (or loss with weighted samples)  provide with us the current improvement and still is not limited to binary classification. So, I suggest authors to explore if this statement is correct. PEM and  PMSA abbreviations have been introduced many times.	It will be better if the authors include qualitative results. The authors should also revise the result validation section as much information is not available. The results should be validated by pathologists as well. The authors should include stepwise results of figure 2. The quantitative analysis part also needs to improve. Include ROC instead of tabular format.	The paper is valuable and is well written. Addresses an interesting problem.	Please see weaknesses section.	Details are available in the main strengths of the paper.
213-Paper2407	Feature robustness and sex differences in medical imaging: a case study in MRI-based Alzheimer's disease detection	The author proposed to evaluate the robustness of automatic feature extraction method based on a 3D CNN compared to a logistic regression method for Alzheimer's disease classification. Authors showed that	In this paper, the authors analyze the robustness of two different MRI volume-based classifiers to distribution shifts. Both classifiers are trained to detect Alzheimer's disease, based on different feature representations. The first is a logistic regression model that uses manually selected volumetric features as inputs, which are obtained using FreeSurfer and SPM. The second is a CNN using the full 3D MRI volumes as inputs. They analyze the effect of differing training dataset sex compositions on the performance for male and female test subjects.	In this work the authors evaluate the effects of sex imbalances on the performance of two classifiers (logistic regression and CNN) in the context of Alzheimer's disease diagnosis/prognosis. They trained the two classifiers with several training sets more or less imbalanced and showed that, in contrast with other domains such as lung disease diagnosis from x-ray, sex differences do not seem to affect the results.	The paper is well-written Hyptheses are clearly stated and tested Findings show no sex-dependency	Analysis by gender in the classification of subjects with AD/HC, pMCI/sMCI.	The problematic is important as it has been shown in other contexts that sex differences can bias classifiers and many works focus on Alzheimer's disease computer-aided diagnosis/prognosis. The experimental setup seems sound and well designed to answer the proposed question. The results seem solid and well explained. The paper is clear and easy to follow.	The paper could provides a more comprehensive analysis of possible pit-fall for deep-learning methods.	The volume of brain substructures can vary subject to subject, it is not considered a good feature. During CNN training, the dimensions are normalized, which can compromise the shape of the brain. They do not mention anything about age, it is important in the study of dementia CNN and logistic regression are already widely used algorithms.	The work focuses on ADNI, a database that has been extensively used and includes peculiarities, such as the fact that there are more men than women while the prevalence of AD is generally higher for women than men. Extending the proposed work to other data sets would strengthen its impact. The discussion/conclusion could remind that the training task is limited to AD vs CN and that the conclusion reached might not hold when the training task is pMCI vs sMCI (for example because women tend to progress faster than men).	The paper provides enough details to enable reproducibility of the experiments.	The database is known http://www.adni-info.org/ , they added implementation details and the code is available on github.	The answers given by the authors seem consistent with what is present in the paper. Code will be made available on GitHub, and the methods and results are well described for an 8-page paper.	Why did the authors restrict their analysis to sex dependency only? It would be interesting to see how age-dependency plays a role in such a framework. The paper would benefit for a more exhaustive analysis. The author claims that deep-learning reach lower performance compared to the  hand-crafter feature and logistic regression method. However, the method used have been proposed in a pre-print submitted recently (February 15, 2022) - Without saying that given the date this probably goes against the double-blinding submission process. The performance of this network is below current state-of-the-art for AD classification, therefore all the discussion might be not adapted for more advanced deep-learning methods.	-I suggest adding the main quantitative results to the abstract Denote MRI and HC before table 1 Review the format of table 1 The validation and final test sets are not clear, I suggest adding a table with the subsets used in each validation instead of figure 2. I recommend considering the age of the study subjects as a covariate. Brain morphology can change according to age. One of the challenges in the classification of AD or MCI is the automatic classification between MCI/HC, justifying why this test was not performed.	The authors could mention why they mix FreeSurfer and SPM to obtain regional volumes normalised to ICV (I assume this is related to the better consistency in ICV estimation with SPM?). Could the authors explain why the classification performance is higher for women than men, even when no women were part of the training set? Even though nothing is significant, it seems that the CNN is more affected by sex differences than the logistic regression, could you comment?	Experiments and results detailed in this paper are not enough to guarant presentation at MICCAI.	CNN and logistic regression are already widely used algorithms. There is no contribution regarding the method. Age is not considered, which is important. One of the main challenges is the classification between MCI and HC, which was not carried out.	Well designed and described study with an important message for a community that extensively works on the computer-aided diagnosis/progonosis of AD with ML/DL.
214-Paper2425	Federated Medical Image Analysis with Virtual Sample Synthesis	This paper aims to solve a realistic problem that the local data from different clients are likely non-i.i.d distributed. To deal with this problem, authors propose a method named FedVSS, which uses Virtual Adversarial Training (VAT) for data generation/synthesis to align the local models with the global model.	In this paper, the authors propose to utilize virtual adversarial sample synthesis to improve the performance of federated medical image analysis on heterogeneous data. On one hand, the local synthesized samples can smooth the local model, on the other hand, the global synthesized sample can help align local models. The combination of the two types of samples reduces the negative effect of heterogenous data in local sites.	This paper proposes the virtual samples synthesis to help tackle the data heterogeneity issue. The proposed method is validated on five public datasets with significant performance improvements and additional study is performed.	The main strengths are summarized as follows: Easy to follow. VAT introduced to federated learning for data synthesis is interesting. Comprehensive comparison experiments.	(1) The paper is well written and easy to follow. (2) The idea is novel. It is simple but effective. The synthesis of local and global samples can be done locally thus does not introduce extra communication cost. (3) The evaluation is good to prove the effectiveness of the method.	This paper aims to tackle the important problem of data heterogeneity in federated learning. The idea of using the local model and the global model to synthesize virtual training samples is novel and interesting. The motivation for improving the client-side training by using the virtual samples is well demonstrated. The paper is well organized and the writing logic is clear. The method is validated on multiple datasets and achieves large performance improvement on the Camelyon dataset.	The main weaknesses are summarized as follows: Using such MNIST-like datasets for evaluation is not quite convincing. Relatively limited performance improvements. Experiment details are missing especially on simulating domain shift (i.e. non-i.i.d.).	There is a concern in the initial training stage. The global virtual sample synthesis assumes that the global model has roughly learned the gloabl distribution, which is not true in the begining of training. Could it be better to add the global loss in eqn. (2) after a certain iterations of training?	The clarity of the sample synthesis process can be improved. It is not clear how the direction r is obtained. The choice comparing 10 and 20 communication rounds are confusing, it seems the model has not converged. No visualization of virtual samples to demonstrate the results by using the VSS. Besides ACC and F1, better use more evaluation metrics (e.g., sensitivity, AUC) to make the comparison strong.	The code and dataset are accessible. The VAT approach may slightly affect the training process.	The authors provide a lot of details such as training details, code, etc. I think it is easy to repoduce the results reported in the paper.	All questions in the reproducibility checklist are positive and implementation details are given in the paper.	Major comments are as follows: More details on experiment settings should be provided, especially on non.i.i.d. distributions. More realistic datasets shall be used for evaluation. MNIST-like datasets are quite different from clinical data, which makes the experimental results less convincing. It will be interesting to show the training curves when using VAT. Because data synthesis via VAT between local and global models can affect the convergence.	Besides the concern in the weaknesse, it is interesting to see how this method works on imbanlanced data.	Fig. 1 can be improved, the middle distribution part does not deliver intuitive and direction information. It is not clear how to calculate the direction r_l and r_g, better give a clear formulation. Better perform repeated experiments with mean and std. Better add the visualization of virtual samples or draw the distributions with and without virtual samples to demonstrate the effectiveness of using the VSS. Better add the training curve, given current results, the model seems has not converged. Besides performance comparison, current additional studies are weak.	VAT is firstly introduced to federated learning, though the evaluation section may not be sufficient.	It is a good paper with clear presentation, novel idea and sound experimental results. The concern mentioned in weaknesses is minor.	This paper proposes an interesting and novel idea of synthesizing virtual samples to help tackle the non-iid issue in FL. The proposed method is validated on six public datasets with significant performance improvements. However, I still have some concerns regarding the calculation or the term r, the model convergence, and the insufficient additional analytical studies.
215-Paper0107	Federated Stain Normalization for Computational Pathology	The paper proposes a method to account for stain heterogeneity across different sites whilst training using Federated Learning.  The method includes a generative model, known as BottleGAN,	This paper proposed BottleGAN for stain normalization in an unsupervised way, and further integrated BottleGAN into WA-based FL. The experiments outperform on conventional FL algorithms.	This paper presents a BottleGAN generative model for computational alignment of staining styles of many laboratories. The purpose is to apply deep federated learning in computational pathology for creation of datasets that reflect diversities of many laboratories. That is expected to provide a vast amount of training data for deep networks, and that is prerequisite for computer-aided diagnosis, prognostication and assessment.	The proposed method only includes 1x1 convolutions, which appears to be well-suited to the task of stain normalization. This is an interesting adaptation of more generic architectures to this problem, with benefits of fewer parameters to be shared. The Many to one to Many formulation of the GAN seems to be a novel approach that offers benefits over other approaches that do not scale as efficiently with more stain varieties. The displayed results certainly seem to be compelling when compared to simpler FedAvgM approaches.	this work introduced a GAN-based architecture for staining style transfer, and combining Federated learning for across laboratory training in a privacy-preserving manner	The idea to apply federal learning paradigm to computational pathology can really be of great practical importance. In this regard, the authors offered solution for the major obstacle: how to solve problem created by different privacy-protected staining-styles protocols in laboratories supposed to cooperate in creating large datasets necessary for deep networks training. The main novelty is architecture of BottleGAN following Many-One-Many paradigm.  That allows staining style transfers between clients using only two generators and two discriminators. That is in a strong contrast with the existing solutions such as Stain-GAN and Star-GAN-based. For K clients, they respectively require K and K^2 generator-discriminator pairs. Regarding generator architecture, it is based on 1x1 CNN without pooling and skip connections. Since there is no long-distance correlation modeling between the pixels, the architecture is independent on the size of the input image. Two discriminators have different roles. First one to decide whether an image is destained or reference stained, and second one on a particular staining style. The main strength of proposed concept is capability to implement federated learning paradigm in computational pathology by solving problem where different laboratories (clients) have their private (non-shareable) staining styles.	"Despite the claims there have been papers that have shown decent results for FL in non IID settings (e.g. arXiv:2009.01871v3) . There are different types of non IID that could have different effects on FL training and this is not really explored in the paper. Little detail is provided on the actual training task. Labels and annotations are mentioned but it is not clear what role they play in the training. ""...architectures like U-Net probably process a pixel differently depending on its position within a crop"" - U-Net often ingesting larger tiles than they predict to get around this. The fact that the styles seem to be imposed artificially on the client images undermines the credibility of the value of this - especially with small numbers of WSIs at each client site. Very little detail on the actual training (e.g. local epochs, batch sizes, tools used, loss functions, optimizers etc.)  Some comparison with FixMatch and FedAvG but there are lot of factors at play in this setup and it would have been much stronger to have done more rigorous ablation studies.  Not sure about the need for the central server to have its own public dataset. What would happen if each of the client's simply used this too?  Having both a local and a central training cycle could leave all the client hardware underutilised for long periods."	1: Since the proposed BottleGAN network can explicitly transfer staining style, what is the advantage of integration of BottleGAN into WA-based FL?  2: In this paper, it said that BottleGAN network to learn staining style transfer with linear growth, it is suggested to add a training time for comparison with other GAN network.  3: In 3.2, author said the proposed architecture is entirely independent of the size of the input image, how to make it, it needs to make clarification? In the experiments, this paper is to solve stanning style normalization, why choose IOU evaluation criteria for performance evaluation?There are no statements in the manuscript.	The main strength of proposed FL BottleGAN concept seems to be also its main weakness (in a relative sense). The concept requires a shareable public dataset of reference stained or destained whole slide images owned by a server. It is not certain whether this assumption can fulfilled in some real-world privacy concerned scenario.	They say that the code will be made available, but from the paper itself it would difficult to reproduce anything. The dataset used is already public.	The authors has clarified the reproducibility,for all code related to this work that they will release if this work is accepted	The authors promised availability of the code. Details related to the network architecture are provided in a supplement associated with the paper. Proposed concept is evaluated on the public  PESO dataset of prostate specimens. It contains 102 hematoxylin and eosin stained whole slide images with corresponding segmentation masks.	I think that the original motivation for the paper is reasonable, but much of the evidence provided to back this up is somewhat convenient/selective. There are many stain-normalisation techniques and data augmentation techniques out there which are orthogonal to Federated Learning and it is not clear what inadequacies these might have that BottleGAN does not (and why).  This is always the risk when combining techniques (GAN, SSL, FL) - that you obfuscate where the unique benefit is coming from. What is unfortunate is that there might be something useful and interesting in this work, but it has not been sufficiently teased out and rigorously tested.	1: Since the proposed BottleGAN network can explicitly transfer staining style, what is the advantage of integration of BottleGAN into WA-based FL?  2: In this paper, it said that BottleGAN network to learn staining style transfer with linear growth, it is suggested to add a training time for comparison with other GAN network.  3: In 3.2, author said the proposed architecture is entirely independent of the size of the input image, how to make it, it needs to make clarification? In the experiments, this paper is to solve stanning style normalization, why choose IOU evaluation criteria for performance evaluation?There are no statements in the manuscript.	The idea to apply federal learning paradigm to computational pathology can really be of great practical importance. In this regard, the authors offered solution for the major obstacle: how to solve problem created by different privacy-protected staining-styles protocols in laboratories supposed to cooperate in creating large datasets necessary for deep networks training. The new original architecture of the Bottle-GAN staining-destaining network is also novel and creative contribution. Its demonstration for different staining styles transformations to a reference staining style and, afterward, destaining is impressive. Thus, to make proposed concept closer to the application in practice, authors should try to evaluate it on more datasets. In particular, in my view, it is important to verify whether assumption on availability of a shareable public dataset of reference stained or destained whole slide images (WSIs) owned by a server is met in practice. It is not certain whether this assumption can fulfilled in some real-world privacy concerned scenario. It is also important, since it is not totally clear to me, whether publicly shareable datset have to be composed of destained WSIs or reference stained WSIs or both?	A combination of too many missing pieces and weak assertions. Although the paper makes for a reasonable read, it does not seem to meet the standards I have come to expect of MICCAI publications	novelty of idea and experiments	This paper presents a BottleGAN generative model for computational alignment of staining styles of many laboratories. The purpose is to apply deep federated learning in computational pathology for creation of datasets that reflect diversities of many laboratories. This can really be of great practical importance. In this regard, the authors offered solution for the major obstacles: how to solve problem created by different privacy-protected staining-styles protocols in laboratories supposed to cooperate in creating large datasets necessary for deep networks training.
216-Paper1639	FedHarmony: Unlearning Scanner Bias with Distributed Data	The work presented in this paper is aimed at simultaneously handling issues related to both scanner/ acquisition differences and data privacy concerns when combining datasets across different site to form large, integrated medical imaging databases. The authors propose a strategy termed based on federated learning to address these issues and experimentally evaluate it using multi-site data from the ABIDE resting state fMRI publicly available database. Ultimately they show that only the mean and standard deviation of learned features need to be shared to address the scanner-specific effects.	The paper describes a federated learning approach that requires minimal information sharing, specifically just the mean and standard deviation of each feature from each site. Experiments show the method outperforms baselines on a single multi-site data set.	The paper proposes a federated learning approach for multisite MR image harmonization. Experiments on age prediction using the ABIDE dataset show promising performance of the proposed FedHarmony on removing scanner-specific information.	The paper is readable and the main themes readily understood. Also, the topic is current and interesting. Privacy-preserving data integration from images is helped by this approach in terms of unlearning scanner bias. The testing performed on the ABIDE public dataset is reasonable, making use of the the T1 MR structural datasets. The 5-fold cross validation performed is appropriate and useful results are reported in the Tables. Using age prediction from T1 MRI as the task, the mean absolute error (MAE) shows incremental improvement in accuracy over alternative strategies (e.g. FedProx or FedAvg) in both the fully supervised and semi-supervised cases, but the key new result is that the new approach (FedHarmony) reduces the ability to identify the scanner site where the individual data were acquired to close to random chance. The PCA diagram in Figure 3 helps to further illustrate that FedHarmony moves to limit the ability to identify different scanner sites.	Very timely work as federated learning is a key enabler of large scale medical imaging studies. Simple method with high practical value. Results are promising in the example shown.	The paper is clearly written. Experiments are thorough. MR images from four sites are used in evaluation, and ablation studies are conducted.	While there is some innovation here in terms of the image analysis/ machine learning methodology that is used, overall it is relatively minor as the approach (including the individual  loss function components and the VGG-based architecture) are mostly taken from the already-published literature. There is some original insight in terms of how the experiments were performed and the observations made about scanner classification accuracy, however. The age-prediction accuracy achieved by FedHarmony does not appear to be particularly improved over other techniques as noted above, although this may be reasonable given the potential scanner identification improvements (reducing it to close to chance). The testing done appears to be performed on T1 MR structural ABIDE data alone, whereas the  ABIDE dataset is much richer, and includes resting state functional MRI data. Indeed others have looked Federated Learning using these rs-fMRI data (e.g. see X. Li, et al,  in Medical Image Analysis , 2021). Perhaps this should be mentioned/ referenced.	"Only demonstrated in one scenario. Although longer term it will be important to reinforce the results with more examples, I believe the single example shown is sufficient for this first publication of the idea. Wording in the abstract ""We show that to remove the scanner- specific effects, we only need to share the mean and standard deviation of the learned features"" is too strong. The single experiment shows that in one specific scenario, this minimal amount of information can still produce decent results. The statement is not true in general. While I can believe that in most practical scenarios the proposed strategy will perform well, one can certainly construct scenarios where it won't. Bounding the conditions under which the strategy performs well would be a good focus for further work on this topic and some preliminary discussion of that in this paper would be useful. Further to the above, the simulation experiments are valuable but limited. It would be nice (not necessarily needed for this submission but for future work) to explore a much wider range of scenarios to identify conditions under which the strategy might fail, e.g. large imbalance of data from different sites, only one or two cases at some sites, large numbers of outliers at some sites, etc."	The application for evaluation is not very representative--the authors explored age prediction as an application scenario of their method. However, age prediction is not a representative task for neuroimage analyses. Experimenting on other tasks like segmentation, disease prediction, or image-to-image translation could provide more insights of the proposed method. Figure 2 shows that the age of all four sites is similarly distributed. In other words, the variables of interest (in this case is age related features) should not be site-dependent. This requirement could limit the application of the method: aging subjects usually have anatomy related changes (e.g., brain atrophy). If one site have generally more aging subjects, this might compound the harmonization.	The reproducibility effort is decent here, but further insight into the particular hyperparameters chosen and experiments related to their settings would be welcome. These appear to be critical in the overall design in order to make the approach work properly.	Seems fine.	Good. Public datasets, implementation details are provided. Code is promised to be publicly available.	The discussion about privacy-preservation and scanner bias is interesting and this particular domain-adaptation approach is interesting. As noted above, it might improve the paper to go into some more detail regarding hyperparameter settings as well as to more highlight the methodological novelty of the approach. Finally, some discussion as to how the approach could be used with other image data types (e.g. rsfMRI) and abnormality outcome prediction (beyond just predicting age) would be helpful.	Not much to add. My main thoughts are in the strengths and weaknesses boxes. This is not a topic I am deeply familiar with, although I have a good understanding. The main feedback would be to run more extensive simulations to identify the boundaries between where the strategy is and isn't effective.	The authors should consider extending their evaluations to broader neuroimage analysis tasks, such as segmentation, disease prediction, or image-to-image translation.	There are some interesting insights presented regarding Federated Learning with respect to the problem of not being able to recognize image scanner acquisition sites. The most interesting ideas from the paper have to do with the experimental results found when combining these 3 particular loss terms (proximal loss, domain prediction loss and confusion loss), and the idea that obtain reasonable age prediction results while reducing the ability to identify the scanner/ site where the data came from.	As above, timely, practical, nicely demonstrated. Longer term needs more validation, but great as a MICCAI presentation.	The paper is generally interesting. Although the evaluation is limited to age prediction, which is not a commonly explored task in neuroimage analyses, the general idea and the ablations make the paper an interesting topic for discussion at MICCAI.
217-Paper0905	Few-shot Generation of Personalized Neural Surrogates for Cardiac Simulation via Bayesian Meta-Learning	This paper presents a new concept to achieve personalized neural surrogate in an end-to-end meta-learning framework, the proposed method shows a good performance on cardiac simulation.	The paper uses Bayesian meta-learning for neural cardiac simulation. Specifically, the paper uses graph CNN to model the 3D geometry of heart, and uses Gated Recurrent Units for temporal modeling, which are optimized through amortized variational inference.	In this paper, a Byesian Meta-Learning based method is proposed for few-shot cardiac simulation is proposed. The generative GCNN model is conditioned on specific input data and the output is personalized cardiac simulation results. GRU model is used for temporal modeling.	The proposed learning framework is new as it learns the process of learning a personalized neural surrogate from a small number of available data of a subject. The proposed method is evaluated in synthetic and real-data experiments	The paper targets cardiac simulation, an important problem in electrophysiology, and sophisticatedly designs advanced machine learning algorithms, which perform better than the baselines.	The meta learning framework demonstrate better adaptation performance when the support set is small.	The compared methods are not explained clear or cited. The conclusion does not reflect the contribution clearly.	This paper is not self-contained and extremely hard to follow. For example, on page 3, what are the dimensions of s, theta, and x? And how are they corresponding to notations in Fig. 1? On page 4, what is the meaning of context observations D_c? In Table 1, 1.1 is less than 4.4, but why the latter is marked bold?	"1) Some naming is a little confusing. The common terminology for few shot learning and meta-leaning are ""meta-train"", ""meta-validate"", ""meta-testing"", query and support set, etc. It is better to review the methodology and make it more clear.  2) It is not clear why using Bayesian meta-learning here. There are other popular meta-learning frameworks, like model-agnostic meta-learning, which is used in few shot learning tasks like classification, segmentation, and also cardiac modeling(motion estimation). reference: Finn, Chelsea, Pieter Abbeel, and Sergey Levine. ""Model-agnostic meta-learning for fast adaptation of deep networks."" International conference on machine learning. PMLR, 2017. 3) Table results are not very clear. The meaning of numbers in Table 1 is not clear to me. Seems PNS has a lower MSE in target set compared with meta-PNS"	It would be helpful if the code and the data would be released, otherwise it might be impossible to reproduce the paper	In the Reproducibility Response, it seems the authors will release the corresponding code. However, the reviewer did not find the description of code in the main text.	Looks like the dataset is not public available and there is no plan to release the code.	The compared method should be discussed and analysed to emphase the contribution of this work. the contribution should summarize the contribution of the the work effectively.	Fig. 1 is confusing: Are the four cardiac surfaces D_c or D_x? On the right, \hat{D_c} and \hat{D_x} are not explained anywhere in the paper. The KL notations in the middle are not correct either. On page 4, what is the graph CNN model used here? The description of this GNN is very vague and more detail is needed. In Table 1, comparing metaPNS and PNS, why metaPNS typically has much better CC/DC but much worse MSE? There is a typo in the title.	Refer to weakness. 1) It is better to review the terminology and make it more consistent with those widely used in meta-learning. 2) Add some discussion on motivations of why use Bayesian meta-learning over other methods like MAML.	Overall, this paper contributes a novel idea and shows impressive results, it would be great if it is presented better.	The proposed meta-model shows significantly worse MSE, according to Table 1, so the proposed meta-learning strategy has not been sufficiently corroborated.	The major factors are the unclear motivations and the confusing results.
218-Paper0176	Few-shot Medical Image Segmentation Regularized with Self-reference and Contrastive Learning	Authors propose a regularization method to improve prototype-based few-shot segmentation model for abdominal organs. Their contributions consist of self-reference and contrastive learning. Self-reference regularizes a class prototype to be representative of entire organ in support image. Contrastive learning helps learning similarity between foreground and background features.	The paper proposes a novel method for few-shot semantic segmentation using class prototypes. These prototypes are locally created. The authors incorporate self-reference regularization, contrastive learning, and self supervision in order to train their models. To evaluate, they use two public available abdominal datasets.	This work applied self-supervision and contrastive learning to semantic segmentation, where the support image is used as a query image and the model is asked to segment on the support image itself. The proposed methods outperformed the SOTAs on two datasets of 30 and 20 3D images.	The main idea of this paper is clear and easy to understand. Authors properly incorporate contrastive learning techniques into existing few-shot segmentation method and successfully improve performance. And they show relevant quantitative and qualititative results.	The main strength of the paper is the use of contrastive learning in order to generate more discriminative prototypes. Also, they use this technique simultaneously with other techniques such as novel self-reference loss, and recently proposed ones such as local prototyping and self-supervision. The use of contrastive learning paired with prototypes for segmentation seems to be a fully novel idea that yielded consistently better results than baselines.	The proposed methods are simple and effective on the two tested datasets. The self-supervision method is simple and straightforward as it can be easily applied to any other datasets and network architectures. The paper is well written overall.	Existing comparison methods are primitive, and they need to be supplemented. After they were published, several advanced models have been proposed. Please check below references [1, 2, 3].  Novelty of this paper is limited because their model seems like a simple combination of existing methods. For example, idea of self-reference is already introduced in few-shot segmentation of natural image and cross-reference models are also proposed [4, 5]. The authors should compare their model with them and clarify contributions.  In addition, contrastive learning is applied in several few-shot segmentation papers [6, 7]. Considering improvements in few-shot segmentation models for natural images, improvement of their proposed method is not that surprising. [1] Tang, H., Liu, X., Sun, S., Yan, X., & Xie, X. (2021). Recurrent mask refinement for few-shot medical image segmentation. In Proceedings of the IEEE/CVF International Conference on Computer Vision (pp. 3918-3928). [2] Sun, L., Li, C., Ding, X., Huang, Y., Wang, G., & Yu, Y. (2020). Few-shot Medical Image Segmentation using a Global Correlation Network with Discriminative Embedding. arXiv preprint arXiv:2012.05440. [3] Kim, S., An, S., Chikontwe, P., & Park, S. H. (2021, May). Bidirectional RNN-based Few Shot Learning for 3D Medical Image Segmentation. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 3, pp. 1808-1816). [4] Zhang, B., Xiao, J., & Qin, T. (2021). Self-guided and cross-guided learning for few-shot segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8312-8321). [5] Liu, W., Zhang, C., Lin, G., & Liu, F. (2020). Crnet: Cross-reference networks for few-shot segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 4165-4173). [6] Liu, W., Wu, Z., Ding, H., Liu, F., Lin, J., & Lin, G. (2021). Few-shot segmentation with global and local contrastive learning. arXiv preprint arXiv:2108.05293. [7] Liu, C., Fu, Y., Xu, C., Yang, S., Li, J., Wang, C., & Zhang, L. (2021, May). Learning a few-shot embedding model with contrastive learning. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 10, pp. 8635-8643).	One could argue about the novelty of the self-reference used, since it is similar to the one proposed by the PANet model1. However, the PANet models use the query features to compute prototypes while the authors use the support set features, and this suffices to claim novelty. Additionally, the manuscript would be greatly improved if authors included proper statistical significance testing (hypothesis tests or confidence intervals) in Tables 1 and 2. It would also be very informative to include 5- and 10-shot experiments on the same datasets, as well as experiments on other areas of the body that have abundant public CT/MRI datasets, such as head/neck, thorax and pelvis. 1 Wang, K., Liew, J.H., Zou, Y., Zhou, D., Feng, J.: Panet: Few-shot image semantic segmentation with prototype alignment. In: Proceedings of the IEEE/CVF International Conference on Computer Vision. (2019) 9197-9206	Potentially limited method novelty as there are multiple works (Liu et al. 2021, Learning a Few-shot Embedding Model with Contrastive Learning; Liu et al. 2021, Few-Shot Segmentation with Global and Local Contrastive Learning) that have applied contrastive learning to image semantic segmentation tasks.	"Authors checked ""Yes"" for most questions on the reproducibility of the paper"	The authors describe concisely the implementation details and dataset preprocessing. Code and pretrained models will also be made public. These are enough information to reproduce the experiments with little effort.	The authors did not mention whether the code will be released. However, considering the methods are straightforward and the datasets are public, it is possible to reproduce the results.	I do not understand why performance of PANet and SENet is much lower than the proposed method. As far as I understand, performance of PANet at least be similar with their model without any regularization. Evaluation setting is limited. Authors uses only big organs which are relatively easy to segment. In addition, they use 2D slices from support and query volume after being carefully matched. To make a general framework applicable to various organs, entire process need to be done with 3D volume. Then other organs can be also used for evaluation.	"The paper is well written and organized, however, the heavy notation used in Section 2 could be improved to ease the reading. For example, there are too many indexing terms. In Sections 2.2, 2.3, and 2.4 the indexes k and j are not used in any of the equations but are still present in many of the variables, omitting these indexes could reduce clutter. Also, the equations for the background class and target class c are the same. The authors could present only one of the equations and just treat the background as another class. ""Specifically, our method achieved the best segmentation performance on each abdominal organ which were significantly better than the second-best method (SSL-ALPNet) (77.65% vs. 73.02% in terms of average DSC)"" No statistical significance tests are provided to support this claim. The reviewer strongly suggests the addition of some marker (i.e. \dagger) alongside results that are significantly better than the baselines in Table 1. Authors could also discuss the application of the proposed method in interactive image segmentation as a future work. This could be a great addition to the conclusion of the paper."	In table 2, it would be better to provide another ablation study where the contrastive learning method is used but without self-reference. Regarding the contrastive learning, it would be better to cite some other works using contrastive learning to compare with the proposed method. Especially if the proposed contrastive learning method is different from existing works. This would be a good paper, if the authors can demonstrate that: This paper proposes the novel self-reference setting. As a self-supervised method, it outperformed the setting that uses contrastive learning without self-reference (the required addition ablation study). Moreover, by combining the self-supervised method and contrastive learning, better performance can be achieved. Otherwise, if using contrastive learning alone can achieve a better performance than using self-reference, considering the limited novelty of the contrastive learning approach and limited added value of the self-refrence, the paper would be weak as a conference paper. Given the additional ablation study, I would be willing adjust the score to accept.	Though their proposed regularization improve performance of the model, novelty and the comparison are limited.	The paper presents a novel solution to the few-shot semantic segmentation that relies on the use of several techniques to improve on existing methods. Using only two datasets of the same body region hinders the generalization of the proposed method and consequently slightly diminishes the impact of the work.	The method includes two contributions. They are both simple and effective. But the contrastive learning part is not novel. The self-reference is novel as there seems to be no paper did the exactly the same modification for few shot semantic segmentation task.
219-Paper0530	FFCNet: Fourier Transform-Based Frequency Learning and Complex Convolutional Network for Colon Disease Classification	This paper presents a Fourier based method to classify colonoscopy images, the proposed framework dices input images and calculates the DFT for each dice and then applies complex conv/rel/bn on them to perform the classification. Images are divided into four classes normal, polyps, adenomas, and cancers.  The results show superior performances in comparison to SOTA works in the colonoscopy domain.	This paper presents a frequency learning method for automatic colon disease classification, featured the Patching Shuffling Module and complex network.	The authors propose a frequency-domain complex number CNN for colon disease classification in colonoscopy images. By splitting the data into real and complex parts, the proposed approach can alleviating the effects of uneven brightness by decoupling image content and brightness.	-A complex network is applied to colonoscopy images for classification, which suppose to deal with brightness/speculation in images.  -Images are sliced so that local information can be obtained for better classification through the network.	The motivation of utilising frequency learning for colon diseases classification is well stated and interesting. The patching before DFT may alleviates the lack of local features in frequency learning and generates smaller numerical distribution in frequency domain, which may improve training stability. They carry out extensive experiments to justify the improvement of the proposed method over baselines and the contributions of critical designs.	Overall the approach is rather interesting from a technical novelty point of view. It is exciting to see a method pushing into other areas beyond just CNNs and Transformers, even if the performance isn't particularly spectacular (similar to some simple CNNs). The experiments and ablations are thorough and can be helpful in understanding this fairly novelty approach when trying to apply similar techniques to other domains.	Using complex CNN is not novel enough, in addition, the main claim of the paper is the ability of this network to deal with brightness imbalance challenges within colonoscopy images, yet no visual results are presented to determine that the network could learn to ignore those areas while doing the classification.	There has been many works treating complex data as double channel input to network, so as to mine information from both real and imaginary parts, for example, in fast MRI reconstruction task[1]. As these works are not compared nor referred, it's unclear the difference or improvement brought by the complex network design in this paper. [1] Image reconstruction by domain-transform manifold learning The random shuffling operation in PSM is somehow confusing. If the patches are randomly arranged, i.e. channel index of input is irrelevant to position, how would the network model the relationship between different patches of input? From my perspective,the kernel for each channel would be eventually equivalent in this way. And if the experiment results are correct, then it means this task requires only local feature within a patch.	"The baselines are a bit weak and no related method for colonoscopy classification are provided. There are a number of approaches which perform pixel and patch shuffles (e.g. ""PatchShuffle Regularization"") and none of them are cited. This claimed novelty is a stretch at best and these other works should be cited. I would really like to see why the proposed approach is different or better than these similar shuffling methods. The authors claim in the conclusion that they introduce complex convolutions. These have been known in the literature for years (e.g. ""On Complex Valued Convolutional Neural Networks"" and many works since then). Be careful not to overstate your novelty. You are not introducing complex networks for the first time. But you are one of the first to apply complex CNNs in the way your are to this type of problem are it is interesting."	The method is explained well and is reproducible	Easy to reproduce.	Good, code will be released.	Dealing with colonoscopy images is very challenging and the method proposed in this work can be useful,  -yet regarding the image classification; (i) it is better to consider a sequence of frames, (ii) flat lesions are among those polyps that need to receive more attention, -There should be a section to demonstrate the effectiveness of this network through methods such as GradCam -The method should be compared against more advanced network such as transformers which has the FFT as an embedded feature.	Illustrate the advantages of the proposed complex network over previous methods that are utilized to mine complex information. Also, supplement some ablation study on it would be even better. Add some visualization results to justify the improvement of the proposed method. For example, what kind of hard samples will be better recognized in the proposed method.	See weaknesses.	The paper is well written, but I am not convinced that it performs better in comparison to more advanced networks such as transformers or CoAtNet.	The paper is well-motivated, propose a method for automatic colon disease classification and demonstrate good performance. However, the methodology part is not clearly illustrated and the technical innovation is not convincing.	Overall I am happy to see an interesting and new approach to a decently well studied problem. Even if the results are not particularly groundbreaking and there are some issues with overclaimed novelty, I would still favor a very weak acceptance just to get a conversation going in the community about possible uses of complex neural networks through a Fourier decoupling procedure such as this.
220-Paper1280	Fine-grained Correlation Loss for Regression	1.This paper proposes two correlation-based loss functions for medical image regression tasks, where  two complementary correlation indexes are explored as learnable losses.  2.The experimental results show that the simple network equipped with our proposed loss functions are effective on various medical image regression tasks.	The authors proposed training networks for regression tasks using two loss functions based on Pearson linear correlation (PLC) and Spearman rank correlation (SRC). Since using a pure PLC is highly sensitive to outliers, the defined loss function splits normal samples and outliers and calculates PLC only on normal ones while calculating the L2 norm on outliers. In addition, they introduced a Coarse-to-Fine optimization strategy to ease the rank learning using SRC. The proposed method has been evaluated on image quality assessment and bio-metric measurement tasks using ultrasound images.	The authors propose a method using their fine-grained correlation loss for regression tasks. The method contains two parts: Pearson linear correlation (PLC) training and ranking order training. In the first part, different from using L2 loss in ordinary regression, the authors use PLC, mean, variance as loss for normal samples, and L2 loss for auto-identified outliers. In the second part, the authors propose a ranking constraint on the similarity of features. The authors use the ratio of regression label as supervision information: 1) force the similarity close to the ratio,  2) force the difference of the similarity close to the difference of the ratio. The method is validated on image quality assessment (IQA) and bio-metric measurement(BMM) tasks on ultrasound images, achieving promising results.	1,This paper proposed two novel correlation losses, which are crucial for various image regression tasks. The presentation is acceptable and the readers can follow this work easily.	* The study is well-motivated, and the manuscript is well-written. * The method and experiments are explained clearly. * The idea of using PLC as the loss function and making it robust to outliers, as well as providing a smoother cost function by introducing the Coarse-to-Fine optimization strategy seems interesting.	The organization of the paper is very good. Technical novelty: a. In the first part, the regression branch, the authors use PLC between prediction and target, mean, variance as loss for normal samples. This idea is straightforward since the PLCC is an evaluation metric for regression tasks, but using L2 loss for outliers can avoid misleading, which is a simple and effective idea. b. In the second part, the similarity rank branch, the authors combine contrastive learning methods and ranking order constraints to give a coarse-to-fine learning strategy.  This strategy forces the features also in ranking order as the prediction. Instead of positive and negative sampling in classification, the authors use the ratio of regression label as supervision information, and use the difference of the ratio as an adaptive margin. The visualization in the supplementary explains how the method works intuitively.	1.The novelty is limited since two correlation losses have been presented before. 2.The experimental setting is insufficient and more comparison experiments and ablation studies should be designed to demonstrate the proposed method. Although the presentation is accept, it still should be improved further.	I do not see any major weaknesses as a conference paper; however, I do have some questions that I asked in the comments section.	"Improper wording: a. In Section 2.2, both the coarse and fine losses are not related to the SRC, so it is not proper to name it ""SRC loss"". The coarse loss is an L2 loss on similarity; the fine loss is a margin-based L1 loss on the difference of similarities and the difference of ratios.  b. The fine loss is similar to the margin-based loss, but there are no positive or negative examples. So it is improper to name them 'L_pos' and 'L_neg'.  They stand for ascent and descent constraints in the ordered tuple.  c. In the equation P(xi, xj) = [R(xi)/ R(xj)], the notation R(*) probably confuses readers. In eq. (2), the y_i is used to denote the target. They should be consistent. Unsound experiments: a. There are no experiments on public datasets and other image modalities. If the proposed method works, it should work on any dataset. The authors only evaluate their method on the private dataset and ultrasound images, which seems that the authors are not confident in their methods. b. The ablations for Lpos and Lneg are unnecessary. If R(xi) > R(xj) > R(xk), then S(xi, xj) > S(xi, xk) and S(xj , xk) > S(xi, xk) should both be satisfied. Using Lpos or Lneg separately cannot guarantee xi, xj, xk in ranking order. So it is unnecessary to run the ablation for the loss. c. There are no experiments for the hyperparameter a in Eq. (3). Readers may be curious about how this value influences performance. d. There are no ablations for adaptive margin in the fine loss.  Similarly, readers probably would like to know how the adaptive margin works compared with different fixed margins."	I didn't check it	In the reproducibility checklist, the authors mentioned that the code has been made available or will release if this work is accepted. I think it is great if they release the code if this work is accepted. Unfortunately, there are no details regarding how the authors acquired data. I assume that the dataset was dedicated to this study since there are no citations or download links. If it is the case, it can be mentioned in the manuscript explicitly. The validations seem well-documented except for concern regarding the reported batch size, which has been explained in detail in the comments.	According to the checklist, the authors will opensource their codes in the future. In the paper, the authors also provide enough details for reproduction.	1.more comparison experiments and ablation studies should be designed to demonstrate the proposed method. The authors  should  improve the level of the presentation.	"The samples that cause outlier predictions probably were outliers in the training set too. Could the authors comment on this assumption and explain what will happen if we simply exclude the outliers from the training set and then train the network using a correlation-based loss? And how the results will be different compared to the proposed method that splits normal and outlier predictions before calculating PLC? Although excluding some samples leads to a smaller training set, probably those samples were not so informative. The network was trained using a NIVIDIA 2080 Ti GPU, which has 11GB of memory. Since the input size was 320x320, I was wondering whether a batch size of 160 could fit in the memory. Is there any chance that 160 is a typo, where the correct batch size is 16? Could the authors please comment on why in Table 1, the NIN and SoDeep methods yield much worse AE and RE while their performance is comparable to the other methods for the other cases? It is mentioned that ""to the best of our knowledge, these medical regression studies mainly focus on learning the mapping among input and output for individual samples, but ignore the learning of the structured relationships over the dataset and among the samples."" However, we know that during training phase, the network looks at the whole training set at each epoch, meaning that the trained model considers all samples together. Could the authors comment on how a model trained using regular loss functions completely ignores the relationships among the samples? I appreciate the way that the authors introduce the correlation-based loss functions only when the network passes epoch #30 (ignoring the first few epochs). Probably because they pick the top 10% of samples with the largest difference between prediction and ground truth as outliers in each iteration. Therefore, in the first few epochs, it seems the differences are too noisy to let us select the correct top 10% since the network is not stable yet, and even non-outlier predictions may show large differences with ground truth. ""reduce the distribution discrepancy at at global level."" -> Please remove the extra ""at"""	Suggestion: Use accurate words to describe the method. Evaluate the proposed method with public datasets and other modalities. Run experiments for hyperparameter a. Run experiments with different margins.	1.The novelty is limited since two correlation losses have been presented before. The experimental setting is insufficient and more comparison experiments and ablation studies should be designed	The idea is interesting, and the method and validations are well-documented.	Despite the novelty and interpretability of the work, the unsound experiments and improper wording lead to a score of 4.
221-Paper0464	Flat-aware Cross-stage Distilled Framework for Imbalanced Medical Image Classification	In this paper, the authors propose a two-stage framework for imbalanced medical image classification. The proposed method sequentially learns the representative and discriminating features from the imbalanced data distribution and re-balancing strategies, respectively. In details, the method flattens the local minima to form a common optimal region and further employs the cross-stage distillation to facilitate the network optimization within this region.	In this paper, the authors address imbalanced medical image classification with a flat-aware cross-stage distilled framework (FCD). FCD combines two-stage learning, flatten local minima and cross-stage distillation to re-balance classes while avoiding knowledge forgetting. FCD achieved good performance in two large imbalanced medical image calssification datasets.	This paper proposed to adopt a flat-aware cross-stage distilled framework for imbalanced medical images classification. Extensive experiments have been done on two widely-used pubic datasets. The results demonstrate the effectiveness of the proposed methods.	Data imbalance is an important problem in medical image analysis. The proposed method is reasonable and novel. The paper is well written. The proposed method is evaluated on two public imbalanced datasets. The comparison experiments are comprehensive.	The proposed methods are well motivated and constrcuted Good performance on two large datasets with well-designed experiments / ablations.	Good clarity and formulations. The proposed FLM technique is interesting and obtains significant improvments through the ablation study results. The proposed methods consist several main components, which can be well plugged into many other state-of-the-art methods with further improvments on the performance. Extensive experiments on two public datasets. The comparison study covers most of popular methods in long-tailed learning and imbalanced classification benchmarks.	The illustration of Fig. 1 could be improved.	There is still no guarantee that local optimum of the second stage can be found in the flatten local minimum of the first stage.	"The technique novelty is limited, and most components exist in the literature and some have been widely used in many existing works. This paper claimed that most of existing methods improve the predictive performance for the minority class but at the cost of decreasing the performance for the majority. However, there is no related metrics or discussion (only in Figure 1) for this. For example, for the long-tailed dataset, you can use three groups ""many"", ""medium"", ""few"" to give a complete illustration on how your proposed methods can well improve the performance both on majority classes and minority classes, especially for a long-tailed distribution with more than 20 classes. Lacking related results may make the paper seem to be over-claimed. For the fundus dataset, DR grading is a common challenge, but I also expected to see how different methods affect the performance. Moreover, to the best of my knowledge, the data imbalance issue does not do damage to the performance as we imagined, i.e., the performance of minority classes may exceed that of major classes. For instance, NPDRIII always shows good performance even if there is a few samples available since it show obvious pathological features. I believe DR grading is not a good task. I recommend the following references for some related information. [1] Relational Subsets Knowledge Distillation for Long-tailed Retinal Diseases Recognition. [2] Automatic detection of rare pathologies in fundus photographs using few-shot learning. This paper studied and tested their methods on imbalanced dataset and its specicial condition - long-tailed. However, I suggest to evaluate it on more long-tailed benchmarks due to the reasons I mentioned above. Some other comments: Compared with RS, the two-stage methods (which contains RS component) such as cRT and DiVE did not obtain good results. To the best of my knowledge, two-stage methods show good robustness in many long-tailed benchmarks. Can you explain that?"	None	Good. Experiments were performed on public datasets and the authors plan to release code after peer-review according to the checklist.	The authors promised to release their codes and all the experiments are conducted on public datasets. I believe the results could be reproduced.	See above	Major: Flattening local minima is a clever idea yet there is still no guarantee that local optimum of the second stage can be found in the flatten local minimum of the first stage. It will definitely help to have at least one of the following (1) more theoretical analysis to prove this point  (2) Visualization of the loss landscape in different stages with the proposed methods (3) experiments on more datasets with more variety of data distributions. It is also not clear how rho (radius of neibouring area) is selected and what's the effect of rho. Minor: There are some typos in the last few pages of the manuscript. For example (1) Page 7, row 3, Ours(RS) and Ours(RS) should be Ours(RW); (2) Page 8, Table 3 should be Fig. 3.	See 5.	interesting paper where merits slightly weigh over weakness	Manuscript is well written and ideas and methodology well structured and presented. Experiments are solid and the proposed framework outperforms state-of-the-art methods.	Due to the reasons above, I recommend a weak accept for this paper.
222-Paper0166	Flexible Sampling for Long-tailed Skin Lesion Classification	The authors present a cirriculum learning based sampling strategy to improve the performance in classification of dermoscopy images. They initially train a embedding representation model and determine anchor samples within each class. These anchor samples are then used to train a classification model which will then be used to select the next set of samples that can be added the training set for further training. Authors show that iteratively training a model with such curriculum can help in boosting the classification performance.	Authors proposed a class re-sampling method for learning from the long-tailed distribution. The idea is to first use SSL to learn balanced representations, and then filter out an anchor dataset with balanced difficulty. Finally, using this anchor dataset to initialize a model, the evaluated difficulty/uncertainty samples of the model is sampled to train it in a recurrent manner. Experiments are conducted on skin lesion classification comparing with a wide range of long-tailed distrubution learning methods.	"This paper presents a curriculum sampling-based method to improve automatic skin lesion classification on imbalanced datasets. The paper introduces a strategy to mitigate class imbalance in several stages, such as pre-training a CNN backbone with a self-supervised loss, sampling anchor points to highlight the ""key"" elements on the dataset per class, and curriculum sampling to incorporate unsampled elements on the fly in the rest of the training."	The paper is well written and the ideas are clearly presented The curriculum idea in sample selection overall makes sense in general Authors support their idea with extensive experimental results	the representation and orgnization of the article are nice, figures are clear. the topic of learning from long-tailed distribution is hot and important. Long-tailed data is a common problem in medical imaging. authors provide a novel way toward long-tailed distribution learning. It establishs an anchor set based on balanced representation learned from SSL. This anchor set is key for the method to outperform the others. Such a sub-set helps to fairly evaluate the difficulty/uncertainty of the samples, and thus facilitate the learning from imbalanced data. the experiments comparing with SOTA methods are comprehensive.	Overall, the paper is well written, the experiments are easy to follow, and the results are presented clearly and concisely. The paper is technically sound, and all the equations reflect the key elements to understanding the proposed method. The comparative study is well executed and contains adequate baselines to compare thoroughly with the state-of-the-art.	The anchor sample selection model assumes that the embedding representation show a unimodal gaussian distribution. However, this assumption was not supported theoretical or experimental data. A few minor details of the model are missing.	Authors include too much techniques in one paper. The relation between the modules and the effectiveness of each module are not well-explored.	Although the author's method does show improved results, they are relatively close to some methods in the comparative study. e.g., RW and ELF. The latter also lies on the curriculum learning-based category, making it significantly close to the proposed method. The paper is in general well written. However, some parts of the manuscript regarding the description of the method (section 2) are not entirely clear. For instance some symbols and mathematical variables are used in equation without introduction.	-	yes	Section 3 of the paper includes enough information to reproduce the method and architecture presented in the manuscript. The authors explain in detail what datasets they used, the network's hyper-parameters, the architecture, and the training recipe.	Overall the paper is very well written and the presentation is clear. Extensive experiments and comparisons are conducted to validate the presented method. The only major issue that requires more in depth analysis is the selection of the anchor points. The paper reads like authors assume that the initial embedding representation of the samples are distributed around a mean anchor point (or in more general sense unimodal gaussian) However, no evidence is presented if this assumption holds. Did the authors conduct any analysis on this issue? In the text, it is not clear, how do the authors calculate the entropy for further selection of the samples. Figure 2 makes things more clear but the reviewer thinks that authors could have explained this more clear in the text. It would have been good if the authors can show how many new samples are introduced at every step of the curriculum for an example run.	focus on one main technique and give detailed analysis/motivation/discussion OR give the detailed ablation study to verify the effectiveness of each module, and discuss/analysis the relation between them. If the pages are not enough, I think SOTA comparison can be shortened, comparing with SOTA resampling methods is enough.	My main comments are: 1) The results of some of the methods in the comparative study are relatively close to the proposed method (RW and ELF). However, these results do not seem to be explained or discussed in the paper. If the authors have additional experiments or different metrics to show the improvement of their method with respect to the baselines, I would recommend including them as well. See section 3.3. 2) Table 2  only highlights when the proposed method performance of the proposed method when it does better than the rest. I suggest properly highlighting the best results to show the advantages and weaknesses of the proposed method properly. 3) Section 2.1 is slightly challenging to read. Although it becomes clear later in the paper what x, v represent and the purpose of equation 1, I recommend rewriting that subsection to improve the overall clarity of the manuscript.	The paper is well written and the ideas are well presented The idea makes a lot of sense  The experimental results are extensive	The proposed method is intriguing, but further analysis/discuss/evaluation is needed.	The paper's method does shows improvements on skin lesion classification through a well executed experimental setup.
223-Paper1037	fMRI Neurofeedback Learning Patterns are Predictive of Personal and Clinical Traits	This paper claims that neurofeedback data can also predict individual behavioral and clinical traits, in addition to instead existing generic resting-state data.	This interesting piece of work describes a self-modulation task guided by fMRI. Classifications are based on the fMRI signal and set to various personal and clinical indications. The individual learning pattern based in the amygdala can be used as a diagnostic tool perhaps.	the authors developed a method for extraction of information from rs-fMRI data, which better predict personal traits	This writing is easy to understand and the organization is good.	This has novelty and the research is creative The application to mental disorders is no too far away, since the amygdala is a clear structure for fear/anxiety related symptoms. Neurofeedback is a strong candidate for self learning	The way of predicting data in Amygdala from non-Amygdala regions and using the prediction error as one of the traits predictor is novel and interesting	Neurofeedback is a major innovation of this paper, however, neurofeedback data is not easy to obtain, the authors need to further clarify how these neurofeedback data (e.g. fMRI data, Clinical data) for the experiments were obtained. The design of the method section needs to be further clarified, e.g., why K-means clustering operations are performed. It's not clear that why the authors use the error of prediction as the signature, what is the motivation?	Selection of the datasets is unclear, while it is part of a bigger dataset. The fMRI processing method has not been described. Was there any quality control (movement or artifacts in the data)? How was the fMRI acquisition performed?	Not sure how to properly interpret the MSE results. They seem to be arbitrarily scaled.	The source code has been made public and has a good reproduction base.	The described method seems reproducible. Code has been put up on Github.	no concern	The motivation for the methods section needs to be further clarified. NF data needs to be further compared and discussed with the performance of resting-state data that are commonly used.	Great piece of work! The focus on the amygdala is clear and well defined. This opens opportunities for further research into fear/anxiety disorders. The method is neuromodulatory and uses fMRI. Though details on the acquisition and data quality control are missing.	Not sure how to properly interpret the MSE results. They seem to be arbitrarily scaled. What's the importance of the results regarding to traits prediction. Are they clinically useful? The description of the method does not seem to be very clear, maybe because the pipeline is fairly complicated and the space for this paper is somehow limited	The organization of the paper is good, but the motivation is poor and the method is not clear description.	There is potential for clinical applications.	Overall seems to be a reasonable approach but with some major concerns
224-Paper0656	Free Lunch for Surgical Video Understanding by Distilling Self-Supervisions	The work presented in this paper considers the task of surgical phase recognition on videos and tackles specifically one of its challenges: the lack of annotated data. In response, the paper proposes a two-fold strategy: a self-supervision approach (teacher/student models) and the use of publicly available models trained on large generic datasets to train the teacher model. To do so, the presented method has three main characteristics. (1) it follows a contrastive learning approach, as training an encoder for a dictionary look-up task. (2) it enables to preserve semantics extracted from the model trained with large datasets, by fixing its backbone and updating its head projection. (3) it operates a self-training of the student model with a specific distillation strategy where the similarity matrices of the teacher and student models are constrained to resemble. Experiments with cholecystectomy videos reveal that (2) and (3) are effective strategies, leading to a general improvement.	The paper investigated use of self-supervised training for surgical video analysis tasks. This is an important topic for the community given scarce amount of labeled datasets available. The paper proposed methodology to leverage large-scale publicly available datasets to enhance performance on surgical video analysis tasks. Semantic-preserving training (via contrastive learning) for teacher network and a distillation objective function for student network are interesting and impactful techniques. Authors have done experiments on two publicly available datasets and compared their method with a few other self-supervised approaches.	"This manuscript presents an approach to adapt a self-supervised method (MoCo-V2) from a general computer vision domain to a surgical domain. They propose a two-stage self-supervised training approach: in the first stage, they propose ""semantic-preserving training scheme"" by training just the projection-head of the self-supervised model on the surgical video; in the second stage, the trained model from the first stage, called teacher mode, is used to guide a student model using distillation and contrastive loss. They show the improved results using their training approach on the Cholec80 and MI2CAI16 datasets."	"Well-organized and well written The idea of using knowledge from larger dataset to improve task solving where the dataset is small is relevant in general and especially for surgery videos, where the annotation is particularly time-consuming and definitively require expertise. The underlying assumption (""using the same backbone and self-supervised learning method, the model trained with ImageNet data can yield a comparable performance for surgical phase recognition with that trained with surgical video data"") is tested and verified numerically. There is no new component, but the combination is new and its application as well. The strategy of semantic-preservation is experimentally verified by the ablation study and makes intuitively sense. The experimental section is well furnished: 2 datasets and several appropriate metrics."	This is a well-written paper with clear motivation, description of methods and experimental validation on publicly available datasets. The paper proposes a novel training technique to leverage large-scale publicly available datasets to enhance performance in surgical video segmentation tasks. The method is well described and compared with other SOTA approaches. Experimental validation and ablation studies are reasonable and reproducible on publicly available datasets.	The paper presents the first study using the state-of-the-art self-supervised approaches in the surgical domain. The proposed two-stage self-supervised training approach provides improved results.	"Major weakness: Potential overstatement and incomplete state-of-the-art Note that I am not expert on this particular topic. P3 ""To best of our knowledge, we are the first to investigate the use of self-supervised training on surgical videos."" This claim may be removed considering the following publication: ""Learning from a Tiny Dataset of Manual Annotations: a Teacher/Student Approach For Surgical Phase Recognition"", Tong Yu et al., IPCAI 2019 After looking at the literature, I found that this paper only reports the existing works on self-supervised learning for images, while there are some works on videos. Note that I could not find any paper that shares important similarities with this work and then harms the novelty of the submitted work. They consider other problem configurations and do not have the same assumptions. Here are two papers I found: ""Learning from a Tiny Dataset of Manual Annotations: a Teacher/Student Approach For Surgical Phase Recognition"", Tong Yu et al., IPCAI 2019 ""Teaching Yourself: A Self-Knowledge Distillation Approach to Action Recognition"", Vu et al., in IEEE Access, vol. 9, pp. 105711-105723, 2021, doi: 10.1109/ACCESS.2021.3099856. - Why is this group of works not mentioned in literature review? Minor weakness: lack of qualitative results and impact of MS-TCN One of the main difference with MoCo v2, from which this work is inspired, is that the data used in the submitted work has a temporal component. This work handles this using a multi-stage temporal convolution (MS-TCN). Even if this choice is supported by three references, there is no display of surgical phase sequences, which makes very hard the appreciation of this aspect. Considering that there is no left space in the current version of the paper, at least an appendix would have been great. Minor weakness: Lack of clarity Fig 3 is not clear about the meaning of the x-axis: does a label fraction of X% mean that X% of the data is labeled? Also, it is hard to relate the results of Fig 3 and Table 1."	In recent years, computer vision community has shifted to 3D conv-net style models for video action recognition. Models like I3D, SlowFast, TimSformer and Swin has been consistently beating ResNet+LSTM/GRU/TCN style models. The proposed methods apply to the older approaches and not the newer models more applicable to this problem. Would be good to apply the same methods to Kinetics for pre-training a clip-based model and then apply to a 3D conv-net method for testing. Would be good to add ablation studies on the effect of parameters Tau and Lambda. mAP is metric often used in computer vision community for evaluation of action recognition models on long videos. Might be good to add this metric to the paper for completeness.	The results show some counter-intitutive analysis where the results obtained with 20% annotated labels are better than the 100% labels.	Not all parameters for the training are given, such as the mini-batch size and the number of epochs. The hardware could have been mentioned. The datasets are clearly identified. The authors did not mention that they will release the code. However, this code should share some similarities with the MoCo v2 code, which is available.	The paper is reproducible. Methods are clearly described and tested on publicly available datasets.	Datasets and code: the authors used public dataset for their methods. The authors have neither provided nor mention the availability of the models, training/evaluation code upon acceptance. Experimental results: No result on the different hyperparameters setting or on the sensitivity of hyperparameters on the results. The authors used fixed hyperparameters.	"** General ** The list of key contributions is a bit redundant with the previous paragraph. Some space would be saved then for the display of some sequences of surgical phases. I am not requesting this for the paper acceptance. Sometimes ""et al."" is written in italic and sometimes not. Can you make it consistent? Section 2.1. L6: what is the meaning of the index ""i""?  L6: what is the meaning of the index ""+"" under ""k""? P4, Figure 2: the ""x"" at the bottom of the figure should a \nu instead. Otherwise, you need to change  Eq 1 and the first line of the first paragraph in Section 2.1. I would appreciate a small motivation of the use of moment-updated encoder. This motivation is written in [10], but I think that the paper will gain in clarity. Section 3.1 Can you provide the same type of information than Cholec80? e.g., the type of surgeries and image size. Also, reporting the average length of the videos can give an idea of the dataset size. P6, Section 3.1, L3: ""We sample the videos into 5 fps"". I think that the explanation should be mentioned, even if it is for computational reason. P6, Section 3.2, L8: ""by self-supervised learning approaches"": you can add a reference to the next section 3.3, to make clear what are these approaches. Table 1: Usually, the best value is in bold. However, this common practice is not followed for: Recall on Cholec80: Ours - MoCo v2 Precision on M2CAI16: Ours - MoCo v2 Table 3: similarly for Recall, where the third configuration of the ablation study, and not the second, gives the highest recall. Tables 1, 2 and 3: can you center the metric names? Tables 2 and 3: Can you add in the legend that the results are given for Cholec80? ** Typos ** P1, Paragraph 1, L1: require - requires  P2, Paragraph 1, L6: corrupted images reconstruction - reconstruction of corrupted images P3, 2nd bullet point: dataset to improve - dataset improves P3, Section 2, Paragraph 1, L2: Section. 2.1. -Section 2.1. P3, Section 2, Paragraph 1, L3: illustrated - presented P4, Legend of Figure 2, L4: model ,i.e., - model, i.e., P4, Last Paragraph, L5: formulate - formulated P5, Section 2.3, Paragraph 1, L7: i.e.sim - i.e., sim P5, Section 2.3, Paragraph 2, L4: the the -the P6, Section 3.2, Paragraph 1, L1: frames - frame P6, Section 3.3, Paragraph 1, L5: the performance of self-supervised training on ImageNet outperforms that trained from scratch-  to reformulate (""performance...outperforms"") P6, Section 3.3, Paragraph 1, L7: motivate - motivates P6, Section 3.3, Paragraph 1, L9: outperform - outperforms P7, Section 3.4, Paragraph 2, L1: We conduct ablation study - We conduct an ablation study P7, Section 3.4, Paragraph 2, L4: model ,i.e., - model, i.e., P7, Section 3.4, Paragraph 2, L4: can not - cannot P8, Paragraph 1, L2: fine-tuning - fine-tune P8, Paragraph 1, L3: approaches - approach P8, Figure 3, Legend, L3: scratch, - scratch. P8, Figure 3, Legend, L4: MoCo v2.- MoCo v2, respectively. ** Future works ** It would have been interesting to have an idea of the performance of the proposed method on videos from other surgeries, such as cataract surgeries. Cataract-101 dataset: ""Cataract-101 - Video dataset of 101 cataract surgeries"", K. Schoeffmann et al., Proceedings of the 9th ACM Multimedia Systems Conference, MMSys 2018, pp. 421-425, 2018."	Would be good to add ablation studies on the effect of hyper-parameters Tau and Lambda. mAP is metric often used in computer vision community for evaluation of action recognition models on long videos. Might be good to add this metric to the paper for completeness. The method should be applied to a more updated approach for video action recognition, i.e. 3D convnets which are outperforming 2D CNN based models.	The paper is well written and presented. It proposes to distill the abundant knowledge in the general computer vision datasets learned using the current self-supervised approaches to the surgical domain. The proposed two-stage training approach to preserve the semantics in the first stage and distill the knowledge in the second stage helps it obtain improved results. However, Fig. 3 gives some counter-intuitive results where the results using less percentage of labels are better than using all the labels. For example, results using 20% labels achieve more than 90% accuracy on the Cholec80 dataset, whereas they reach 87% accuracy from the 100% labels. The authors should thoroughly check the correctness of the results in Fig. 3. Moreover, it also provides an insight into the results (if we assume that the results are off by some margin) that the underlying task is relatively straightforward. It shows that the methods reach more than 80% accuracy on the Cholec80 dataset using as few as 5% of labels. The authors should discuss these points or apply their approach to some more complex surgical data science tasks to evaluate their approach.	The challenge tackled by this paper and the proposed strategy are relevant. This strategy includes three components already published, but remains novel since this combination has never been evaluated and applied to surgical video understanding. The missing existing works do not impact the novelty of this work. This paper appears to me quite promising, encouraging a significant number of additional experiments to see how far we can reach with using already trained models for the problems of surgery video understanding. Because it opens a new door for this problem, I think that the paper can be accepted in MICCAI.	Clear motivation and clinical relevance, novel method with clear description, complete experiments to show the impact.	The authors proposed an effective self-supervised approach for surgical workflow recognition and obtained better results, specifically in the less labeled data regime. The authors should consider addressing points discussed above.
225-Paper1550	Frequency-Aware Inverse-Consistent Deep Learning for OCT-Angiogram Super-Resolution	This paper proposes an inverse-consistent deep learning based method to enhance the unpaired OCTA images. To enhance the OCTA image, Fast Fourier Transformation, Gaussian filters and Discrete Wavelet Transform are used to decompose frequency information. The proposed method outperforms the compared methods.	This paper proposes inverse-consistent generative adversarial networks (GAN) for unpaired OCTA image restoration and degradation. In the proposed framework, a frequency domain decomposition module is introduced to enhance high-frequency information at the frequency domain level.	The authors proposed an inverse-consistent generative adversarial network with frequency awareness for unpaired image enhancement of low-resolution optical coherence tomography angiography. In this work, the authors regarded the enhancement task as the mapping from low resolution domain to high resolution domain. Furthermore, frequency domain information including low- and high-frequency components was integrated into the proposed network. Qualitive and quantitative results demonstrate that their approach outperforms other methods, especially in terms of the balance between low- and high-frequency information.	The novelty is clear by integrate spatial and frequency domain information in the GAN. This paper has several Architecture and illustration figures to enhance readability	The paper proposes an unpaired OCTA image enhancement framework to enhance 6x6-mm image quality by learning high-frequency information from 3x3-mm images. The paper constructs super-resolution networks to enhance OCTA image quality from the perspective of frequency domain decomposition.	It is interesting that this work decomposes and processes the data and thus optimize the network from the perspective of frequency domain.	More experiments need to add in section 3. (1) The OCTA image enhancement algorithms [4,5] should be compared. (2) The ablation experiments of total loss should be added based on formula (7). (2) Some public OCTA datasets are available and can be used in section 3, such as: [1] Ma Y ,  Hao H ,  Fu H , et al. ROSE: A Retinal OCT-Angiography Vessel Segmentation Dataset and New Model[J].  2020. [2]Mingchao Li, Yerui Chen, Zexuan Ji, Keren Xie, Songtao Yuan, Qiang Chen, Shuo Li. Image projection network: 3D to 2D image segmentation in OCTA images. IEEE Transactions on Medical Imaging, 39(11): 3343-3354, 2020 The authors should give more details of the related works, especially the work [4,5].	The image evaluation methods are not reasonable. SSIM and PSNR are used only for paired data, and there are obvious structural and domain differences in the low/high-resolution images presented in the experiments. As can be seen from the experimental results, the reconstructed image quality is not significantly improved compared with the original image. In the Introduction, the authors do not  present the clinical significance and value of 6x6-mm OCTA enhancement. The authors used unpaired generative networks to enhance 6x6-mm OCTA images, which may lead to missing structural features or generating pseudo-vessels. The method lacks effective supervision of the vascular coherence loss function.	The experiments seem a little inadequate and have not verified the clinical value of resolution enhancement on optical coherence tomography angiography.	The network could be reproduced only based on the manuscript without major difficulties.	The network details of the paper are not described clearly enough, leading to difficulties in reproducing the method.	The paper includes information that would make it possible to reproduce the methods and experiments.	In page 6, the authors set a parameter a = 0.7. However, I could not find this parameter in any formula of this paper. The evaluation metrics PSNR and SSIM need the ground truth of OCTA image. Please give the way of getting the ground truth.	This paper is essentially a super-resolution task, and unpaired super-resolution methods should be added to the experiment for comparison. The paper may consider proposing indirect image quality evaluation metrics. The paired metrics used in the current paper are not convincing. The resulting image of the non-rigid alignment is missing in the paper. The purpose of the paper is to enhance the whole 6x6-mm image, but the comparison result image of the whole 6x6-mm image is missing in the experiment.	"I think the architecture presented in Fig. 2 could hardly reveal inverse-consistency and may lead to readers' misunderstanding. I get confused about the cross-entropy adopted in feature distribution loss and f() in Eq. 5. In addition, the value settings of b1 and b2 are also puzzled me. Some mini comments:  (a) clinis in the second sentence of the introduction should be written as clinics.  (b) In the sentence ""Weights of each components are set as a = 0.7, ..."", what does a represent?  (c) UnpairedSR using pseudo pairs in the first paragraph of Section 3.3 should cite the reference [14]. Compared methods seem somewhat limited and all of them are unpaired learning frameworks. I think the authors could supply some supervised learning methods due to have paired images. In ablation studies, does the method wo HFB indicates that authors replace HFB with mere high-frequency components? Please clarify it. Only adopting PSNR and SSIM for evaluation is limited. It is of great clinical interest if authors prove that their approach could benifit subsequent vessel analysis or diagnosis tasks."	The novelty is clear but needs more experiments to support the novelty.	The innovation of the paper and the rationality of the evaluation index	The integration of frequency components into unpaired learning frameworks for resolution enhancement of optical coherence tomography angiography seem interesting. However, this work should be further improved in terms of method description and experiments.
226-Paper2446	From Images to Probabilistic Anatomical Shapes: A Deep Variational Bottleneck Approach	A novel framework was proposed for predicting probabilistic anatomical shapes from 3D images based on the variational information bottleneck theory.	This paper proposes to use the variational network to parameterize the statistical shape modeling. It leverages the information bottle neck to enable the learning framework.	This paper proposes a new method for generating probabilistic shapes from 3D image inputs using a variational information bottleneck (VIB) approach. The claimed advantages are higher accuracy and better uncertainty estimation compared to previous DeepSSM methods. Experiments were performed using a large MRI dataset (1001 scans), with paired ground-truth shape annotations with point correspondence.	The main strength of this study lies on the novelty of the proposed methodology, which relaxes the linear dependence assumption in existing PCA-based methods. Better calibrated aleatoric uncertainty quantification was achieved without comprising the shape modeling quality. This study is an advance in statistical shape modeling (SSM) and has a great potential to be used as a research tool for SSM analysis.	The proposed framework is novel and the difference to the related works are also clearly explained. The validation experiments are well designed.	(1) Thorough explanations for proposed vs previous methods (2) Good shape-predictive performance and uncertainty estimation of the proposed method (3) Simple architecture / Loss (Conv/MLP and VAE ELBO loss with a \beta term for KL divergence)	No major weakness was found.	The paper mentions the latent representation is learned, however, it still needs several samples to get the expectations. This part might be over claimed. Some details of the method is not fully exposed.	(1) There seem to be two main differences for VIB-DeepSSM vs other methods - (a) loss and (b) MLP decoder. It's unclear which is contributing more significantly to the improved shape prediction and uncertainty estimation. (2) Unclear whether the dataset is public. If not, may be worthwhile to include more info about acquired images. (3) No mention of Figure 4 in the main text - the x-axis seems to be Training size, but PPCA-Offset-DeepSSM model doesn't change values at all throughout different training sizes. Why is this the case?	The dataset used for developing and validating the proposed method is not available. The code and model will be made available, and a similar public dataset will be provided for testing the code and model. Results are likely producible.	The reproducibility of the paper is good.	Reproducibility seems good.	It would be good to validate the proposed method on an additional dataset, preferably a public dataset. Section 3.1 will need further clarification - it is not clear how the ground truth meshes were generated, by who? radiologists or people without training? how many? Figure 4 is too small, almost unreadable.	In the method part, it would be more clear that the last layer of the encoder is explained in details. The paper mentions the proposed method is self-regularized. It is not clear to the reviewer why it is self-regularized and this part is not discussed. Also, why the comparison work is not self-regularized is also not discussed. Some discussion can make this more clear.	(1) To address weakness #1, it may have been a good idea to perform ablation studies for the two different contributions separately. (2) It would be good to clarify whether the dataset is public/private and to include more descriptions if it is private. (3) It would be good to explain figure 4 (i.e. shape accuracy) a bit in the main text.	The problem this work aims to address is clearly defined - capture the data non-linearity in shape modeling and improve uncertainty quantification. The proposed methodology is novel showing improved performance in terms of both uncertainty quantification and shape modeling compared to the state-of-the-art methods.	The proposed framework seems to be novel, and the validation is convincing.	Overall, the paper was well-written and included relevant explanations and experiments that demonstrated the uniqueness and advantages of the proposed method, respectively. I have some reservations about whether or not the proposed method is novel enough to be considered a new method (conv + MLP and beta-VAE loss), but I think in conjunction with the shape modeling task and performed experiments, it may still be of interest to the MICCAI community.
227-Paper2489	FSE Compensated Motion Correction for MRI Using Data Driven Methods	The paper presented a method for simulating synthetic motion artifacts accounting for k-space acquisition ordering and T2-signal decay in FSE acquisition and showed improved motion correction	In this work, the authors propose a FSE motion-corrupted data generation method that takes into account the intra-echo signal decay.	This paper proposes a new method for simulating motion corrupt data in FSE sequence, which can be used for training deep learning model for retrospective motion correction. The method considers the key effects of FSE sequence such as signal decay and sample ordering and can simulate more realistic motion corrupted data. Experiment results showed that by training with data simulated by the proposed method, a significant improvement can be achieved in motion correction results.	Motion corrupted FSE images were simulated by accounting for MRI acquisition, which improves the realistic representation of training data. The proposed approach was compared with the models trained by naive motion corrupted FSE image, showing its superiorities.	Well-structured Clearly motivated Neat figures	The paper pointed out a critical problem in existing motion simulation methods in MRI, and proposed a reasonable way to solve it. The motivation is clear and sound. The idea of taking into account the effects of FSE when simulating motion is insightful and interesting. The proposed method is simple yet effective, achieving  superior performance for motion correction compared to conventional method.	It wasn't clear why FSE images were generated by multi-contrast brain MRI data. My biggest concern is both training and testing data were only based on simulated motion/FSE images. Testing the model onto true FSE images with actual motion corruption will provide a better understanding of how the data-driven approach would improve the motion compensation. Motion artifacts were simulated by k-space ordering and signal decay, which will be highly dependent on echo train length and TR, but it seemed like one set of ETL and TR was considered. How many testing datasets were used?	Additional training details are not listed in the supplementary material Reason for training a cGAN as opposed to other available models/architectures are not stated	The experiments are lacking. Experiments were only carried out on simulated dataset. The method was not evaluated on actually acquired MRI data with real motion.	No issues.	Seems reproducible since the authors have committed to sharing the code upon publication.	Good.	FSE is often used as a multi-slice 2D FSE, and motion corruption can happen in both in-plane and through-plane directions. Any methods considered in both directions would be more practically useful.	Strongly recommend the authors to bolster their work by performing experiments on prospectively motion-corrupted FSE data.	Evaluate the method on real MRI data with real motion. Add PSNR in Table 1.	The study showed importance of using realistic motion artifacts for training for motion correction approaches.	Clearly motivated and well implemented; well written with neat figures; commitment to sharing code	Novel, interesting and insightful idea. Simple yet elegant method design. Superior performance in experimental results. However, lacks results on real data.
228-Paper2619	Fusing Modalities by Multiplexed Graph Neural Networks for Outcome Prediction in Tuberculosis	This paper presents a framework for multi-modal fusion of medical data based on multiplexed graph neural networks for multi-class classification for the prediction of clinical outcomes in tuberculosis. The authors applied the framework to a large cohort of TB patients with imaging (CT), genomic, treatment, demographic, and clinical data modalities. Comparisons are made to the class fusion schemes (early, intermediate, and late fusion), as well as several methods based on graph convolutional networks.	In this paper, authors adapt Multiplex-GNN theory for the purpose of multimodal/multi-omics latent fusion. They demonstrate their method on a multimodal dataset for Tuberculosis (TB) treatment outcome prediction. The dataset features 5 input modalities (Chect CT, genomics, demographics, clinical data, regimen data). The proposed Multiplex-GNN method is compared against various baselines and ablations, and has the highest and most consistent AUC values across the 5 output classes.	This paper proposes a multiplex graph based representation for fusion of 5 clinically relevant types of data (drug regimen, chest x-ray, demographic, clinical, genomic) and a Graph Neural Network based learning algorithm performing multi-class outcome prediction for Tuberculosis disease	This paper is well written and contains clear descriptions, explanations, and illustrations throughout. The use of multiplexed graph neural networks for multi-modal fusion appears to be novel, and the rationale for its use is intuitive and sound. The clinical task, data types, and sample size are appropriate for testing multi-modal fusion for classification. The results demonstrate that the proposed framework was consistently the best performer across the 5 different outcomes.	"Demonstration of the method was done on a complex dataset which is ""truly"" multimodal, with 5 modalities: CT/Genomic/Regimen/Demographic/Clinical data. It would be great if authors, along with their method (which apparently will be open-sourced), also published their pre-processing pipeline which extracts latent representations from the individual models (e.g. genomic vector representation from genomic sequence raw data). This in itself is a complex task wihch requires domain knowledge from several experts. It would also short-cut the entry of new teams into this dataset and multi-omics fusion. If the data was already present in tabular form, and pre-processing only constituted in modality-wise d-AEs, please ignore this suggestion. Authors propose to use Multiplexed-GNNs as a novel approach for latent fusion method. As far as I understand, its strength comes from the capability of the model to find salient correspondence across sub-dimensional feature spaces of the input modalities. Multiplexed-GNNs are theoretically well founded, a foundational book is cited in the paper for further study. Strong baseline analysis, comparing 7 latent fusion methods. Two methods serve as ablation study - one model replacing  Multiplex-GNN with Relational-GCN, the other removing the latent encoder. Statistical robustness of results by comparing AU-ROC values by a DeLong test and reporting p-values. Convincing results on the Tuberculosis dataset, with consistent performance of Multiplex-GNN on all 5 classes (and weighted avg)."	-A large number of baseline models (including one using a monoplex graph representation)  are considered and experimented on -The proposed model shows statistically significant improvements over the baseline models for the majority of the comparisons -Dataset is relatively large with 5 types of clinically relevant data for patients	The literature review is very light, and only mentions the classic fusion schemes. Some modalities of data were only available for subsets of the patients, and it is unclear how this impacted the models' performance. The procedure for forming edges between feature nodes is somewhat heuristic (e.g., saliency threshold), but this is fine for a proof-of-concept.	"Demonstration of results on only 1 dataset (though page limit wouldn't allow to explore more datasets, see 8.) The method does not seem to have a built-in mechanism to deal with missing modalities (as compared to e.g. XgBoost), which is a common problem in clinical multi-omics. Instead imputation needs to be performed as pre-processing (in this case simple mean-imputation from the training set). Could be an aspect for further study. The paper is very complex and hard to understand, especially for readers not familiar with Multiplex-GNN theory. Difficult to improve, given the page limit, but still somewhat of a weakness of the paper. Several unclarities remain: 1) Are the d-AEs and c-AEs pre-trained and then frozen during multiplexed-graph training, or are they learned end-to-end with multi-target learning? 2) If there are 5 modalities, why is the ""multiplexed graph [only used for fusion of 3 modalities, i.e.] for multimodal fusion of imaging, genomic and clinical data for outcome prediction in TB"" (cf. bottom of page 2)? 3) In Fig 1, c-AE yields a 1D latent vector, so why are 3 latents illustrated, and why does this lead to 3 multiplex planes? And does this mean that K=3 in the figure, but the experiment actually uses K=32 concepts? I think it would be helpful to tie notation to figure contents, e.g. by indicating the values of k, K, P, i etc in the figure wherever possible."	-There is no explicit discussion on possible ways to incorporate interpretability/explainability of the model predictions	The data is publicly available through the Tuberculosis Data Exploration Portal, but some of the genomic data received additional processing to generate functional domains, and this process was only briefly described and may be difficult to reproduce. However, most of the data will be available, as will the code, so reproducibility is likely feasible.	The dataset preparation is complex. It is well explained in 3.1., but for reproducibility (and ease of entry for other research teams), I again recommend the co-publishing of the data pre-processing code, from Tuberculosis Data Exploration Portal to the level of input into the d-AEs. The dataset is publicly available. According to the reproducibility statement, the method is planned to be published open-source (license model would be interesting to mention in the paper). This is not mentioned in the paper itself, however. Importantly, otherwise the method may be difficult to reimplement purely from the paper, unless very familar with multiplexed-GNNs. Hardware requirements for reproducibility: Authors only mention the CPU. Was a GPU used for training? If not, was there a reason (e.g. multiplexed-GNN training cannot be easily accelerated by GPU)? If GPU, how much VRAM would be necessary to reproduce, or at least how much VRAM did the training GPU have?	The authors are using the Tuberculosis Data Exploration Portal dataset, and they specify that training and evaluation code will be available for download	Other than the basic fusion techniques, perhaps a high-level summary of multi-modal learning approaches (e.g., for text and images) would be useful to provide more context. To understand the data better, please provide the prediction class frequencies for each data modality. If possible, it would be informative to see which relationships between modalities are most important for each outcome.	"Authors demonstrate the performance of Multiplex-GNN on only 1 dataset (Tuberculosis). This is perfectly sufficient for MICCAI, would probably be hard to fit more experiments into the page limit. But for a journal extension, I would really be curious to see the performance on more datasets. Maybe also strong baseline methods from ""conventional"" ML on the d-AE joint latents, especially Gradient Boosted Classifiers (e.g. XgBoost). The presentation of the multiplexed-graph framework is extremely condensed in this work. This may be hard to avoid within the page limit of MICCAI, but for a journal extension, I would appreciate a much clearer introduction (almost tutorial-style) into the theory I believe citations 3&4 refer to the same book. Probably good to check&merge."	Could you please comment on possible ways to incorporate interpretability/explainability into your model? Since this is a clinically relevant problem, this aspect would also be important for the practitioners	This paper presents an apparently novel but intuitive solution to an important problem in machine learning analysis of multi-modal data. The data used for testing is appropriate, and the results are promising.	"Interesting and novel approach for multimodal fusion via multiplexed-GNN. Demonstration on a ""truly multimodal"" dataset with 5 modalities (internally even 6, considering categorical and numeric clinical features as separate modalities). Convincing results, including an interesting ablation study and comparison to several baselines."	The problem is clinically relevant and it proposes to use a large variety (drug regimen, chest x-ray, demographic, clinical, genomic)   of types of data for outcome prediction. The experiments are adequate and clearly described. Statistical significance is shown to prove improvements over a large number of baselines including a monoplex graph based representation
229-Paper0029	FUSSNet: Fusing Two Sources of Uncertainty for Semi-Supervised Medical Image Segmentation	"The paper describes a framework for semi-supervised deep learning-based segmentation that incorporates both aleatoric and epistemic uncertainty. Epistemic uncertainty is used to divide the image into ""certain"" and ""uncertain"" areas, which are handled differently in the semi-supervised learning approach. The aleatoric uncertainty is used in the supervised part of the loss."	This paper is built on a joint training regime of epistemic and aleatoric uncertainty. Particularly, the authors computed the epistemic uncertainty from four different classifiers on top of a shared embedding and used the thresholded uncertainty as a mask for computing loss between student and mean-teacher networks. They have shown two dataset experiments that showed marginal performance gain over previous methods.	"The paper proposes an iterative multi-step training strategy called ""FUSSNet"" for medical image segmentation tasks consisting of unsupervised and supervised learning aspects. In different steps, epistemic and aleatoric uncertainty is exploited to boost segmentation performance according to multiple metrics (pixel-level and global-level). The authors claim superiority to other recent semi-supervised methods on two challenging segmentation tasks (on MRI and CT) in the small data regime (only 12 or 16 labeled datasets used per task)."	I like the main idea of using both aleatoric and epistemic uncertainty in the semi-supervised learning framework The comparative validation and ablation study are generally good (but see concerns below)	The evaluation is detailed on two different datasets showing slight improvement over previous methods, which has clinical relevance. The novelty of this paper is limited, as explained below. However, the ablation regarding PL and CR is interesting and can be helpful to other student-teacher unsupervised learning settings.	This paper addresses uncertainty at various levels, which contributes to an important trend that emerged over recent years in the MICCAI community. The way uncertainty is exploited here in the context of anatomical segmentation appears meaningful to allow learning useful features from both labeled and unlabeled data, in particular in challenging regions of the image (organ borders, noisy regions, imaging artifacts, etc.). In the future, this framework may even be worth extending to potentially include an active learning component to allow for a dynamic increase of meaningful labeled data to further boost its performance.	Lack of clear statement of contributions in context of most closely related work Details of hyperparameter optimisation are unclear There is no statistical testing for significance of results	The technical contributions are a bit weak. This paper combines the existing idea of an uncertainty-based mask [14] with an extra aleatoric loss function [11] which is a trivial extension. Enforcing the consistency loss for the EU regions of the image between teacher and student prediction seems empirical and not well supported. One can argue that teacher and student networks can have the same EU, and thus consistency loss will not be helpful in such a scenario. Inclusion of aleatoric uncertainty with epistemic uncertainty did not improve much over epistemic one, and there is no statistical test reported on the contribution coming from aleatoric uncertainty. This raises the question: how much weight does one need to provide between them in the final loss function? Is this something dataset-specific?	Evaluation is not fully convincing, especially given the sometimes small margins for various metrics between competing methods. The main reason is that the paper uses small datasets only (~100 volumes per task, of which only 12 or 16 labeled ones were used) and at the same time reports only the results of a single training run per task/method. To minimize the risk of random lucky results, the authors should re-run the same experiments multiple times with different random network weights initialization, and more importantly, different random splits of training datasets (at least shuffle the labeled and unlabeled training datasets randomly). Then the distribution of the resulting metrics should be reported and compared to state of the art (e.g. using mean and standard deviation, if applicable). Is FUSSNet still coming out on top for both tasks? Furthermore, were other tasks tested and are the authors aware of any limitations of their approach?	Overall good, code has been made available. But details of how optimal hyperparameters were arrived at is lacking (see below).	The details about the experimental setup are thorough. And the authors have promised to release the code upon acceptance which is good for reproducibility.	Authors use public datasets for benchmarking and employ a widely used network architecture, etc., all aiding reproduciblity. Moreover, the code will be made available. However, it is not clear if the code will also provide information about the data split (train / validatiation and train with vs. without label for the different steps). This would not be an issue, if the authors reported results based on multiple random splits of the same data instead of single data points (e.g. reporting mean +- std.dev. of reported metrics over e.g. n=5 training runs of the full pipeline).	"There were a number of aspects of the proposed framework that I liked. It is interesting to investigate the effects of using both aleatoric and epistemic uncertainty. I also like the way that epistemic certain/uncertain regions are treated differently by the semi-supervised learning. I think there is a novel methodological contribution in this paper, but I don't think it came across that clearly in the authors' summary of their contributions at the end of the Introduction. E.g. they highlight the different treatment of certain/uncertain regions as a contribution, but in Section 2.3 they mention that [14,17,20] have already done something similar. It would be useful to clearly state the authors' contributions in the context of the most closely related work. Currently it is difficult to do this in the Introduction because a lot of the most relevant work is only discussed/cited in the Methods (i.e. Sections 2.3/2.4). I think the paper could be organised better by having a Related Work section to more thoroughly review the literature, then the authors' contributions could be more clearly stated in this context. I also have some concerns about hyperparameter optimisation. There are a number of hyperparameters in the proposed framework but there is no information about how they were chosen, what data were used for optimisation and (sometimes) even what the final values were (e.g. what was the value of lambda in Eq 1?). This is all important information to help the reader interpret the results. E.g. for the left atrium (LA) dataset there is mention of training and validation sets but no test set. Does this mean that the validation set was used for hyperparameter optimisation, and that these are the results reported? Or did they use the 54 cases of the challenge test set at http://atriaseg2018.cardiacatlas.org/? This was not mentioned in the paper. Finally, what hyperparameter optimisation was performed for the comparative validation models and the ablation study experiments? Some of the differences in performance, especially on the LA dataset, seem to be quite small, and I believe that the test (or validation) set was only 20. Could some statistical significance testing be included? Also, I presume the results presented are for a single run of all models? What would happen if the authors simply reran all their experiments with a different random seed - would they still see the same differences? Other minor comments: Can the text in Fig 1b be made bigger? It is quite hard to read at the moment. P4: ""Thus, we do not enforce consistency constraint in certain areas ..."" I know what the authors are trying to say but I think this choice of words is misleading - it sounds like they are saying that they do not treat different areas differently, which is directly contradicted by the following Eq 1. I would suggest something like: ""Thus, we do not enforce the consistency constraint in areas considered to have low epistemic uncertainty ..."" It would be interesting to see some examples of the epistemic certain/uncertain areas. E.g. is this just highlighting boundary regions as uncertain? I understand that the authors are constrained by the page limit but some insight into this would be useful, even if it were just a sentence in the Discussion."	"On page 4, the authors said, ""The CE loss and focal loss focus more on pixel-level features, while Dice loss and IoU loss care more about shape information."" This is not true; Dice and IoU do not care more about shape; they are still a pixel-level loss. This and the next sentences need correction. Eq. (3) the softmax would be on the channel dimension, right? If so, the index j should be out of the softmax function. softmax(\eta_i^(t))_j. Unnecessary acronyms and symbols can be avoided for better readability, e.g., MIS, EMA. Also, please introduce ASD and HD before using those acronyms."	"Abstract: ""... outperforms the state-of-the-art ... by large margins"" -> consider a more quantitative statement such as ""by up to XX% DICE"" or similar. Fig 1: some text is too small to read, especially some boxes on Fig 1b; please enlarge Fig 1a and text indicate an iterative nature of the framework. The paper is lacking convergence criteria and discussion about the added complexity (runtime, memory) for training, especially in light of the comments regarding other methods computational cost issues (MC sampling) Fig 1a: not clear what pretraining entails (first box in Fig 1a): what is the training objective, what data is used, etc. I suppose it is a fully-supervised training of the V-Net directly on the segmentation task using the selected training datasets that are also used later in the training? Please clarify. page 4: ""four classifiers share the encoder but differ in loss functions"": what about the decoder? Why is only one forward pass required (or only one forward pass for encoder, but four for different decoders (?))? This part needs clarification page 5, eq. 4:  What is the value of \omega? And right below: ""weighted sum of ...: how are L_sup and L_unsup weighted against each other? Are these hyperparameters difficult to tune, and where the same used for both tasks? The paper highlights several times that AU is modeled in logit space, but it is not well motivated why and if it helps More details on experiment section FUSSNet appears relatively complex to train. Therefore it would be great to compare not only final performance metrics, but also time until convergence to those results for each method it would be interesting to see the impact of increasing the number of labeled datasets in training (or changing the ratio of labeled vs. unlabeled training datasets) page 5: Unlabeled data not truly unlabeled, because data is preprocessed (crop) based on labels. page 6: sliding window patches are very small (16x16x4 pixels) -> can the authors motivate why? memory limitation or does a small window iprovide benefit in any way? Same setting used for comparsion methods? Maybe this very small context explains the many disconnected regions in some thumbnails in Fig 2. ""theoretical upper bound"" (used to describe performance with a standard training strategy using all training data) -> this term is not adequate here. Please use ""fully-supervised upper bound"", ""fully-supervised reference result"", or similar."	My main two concerns were the lack of a clear statement of contributions, and the lack of clarity about aspects of the validation, especially hyperparameter optimisation.	The paper lacks technical contribution, however has potential room for discussion since this simple idea has produced good results. Hence, I recommend weak acceptance in the initial round.	Focus on uncertainty is one of the strongest points of the paper, however the method is relatively complex, while gains over other methods may not be overly significant. Also, one of the main claims/novelties, the one regarding aleatoric uncertainty, turns out to provide only minimal benefit. My main criticism is that the evaluation is not 100% convincing given that only single experiment results are being reported (instead of mean+-std.dev. over multiple randomized runs) using very small number of training datasets. Especially in this low data regime, random selection of training datasets can have a huge impact. But since the results of FUSSNet are somewhat outperforming all other methods in both experiments, assuming the authors have not hand-picked results, I am eager to learn more details at about their work at MICCAI.
230-Paper0398	GaitForeMer: Self-Supervised Pre-Training of Transformers via Human Motion Forecasting for Few-Shot Gait Impairment Severity Estimation	This paper proposed a novel method GaitForeMer that forecasts motion and gait (pretext task) while estimating impairment severity (downstream task). By pre-training on NTU dataset, it can improve performance of early diagnosis of Parkinson's disease (PD) on a small dataset.	This paper presents a method to predict Movement Disorder Society Unified Parkinson's Disease Rating Scale (MDS-UPDRS) scores. To this end, the authors propose Gait Forecasting and impairment estimation transformer (GaitForeMer), a transformer model using motion forecasting as a self-supervised pre-training task. The proposed system achieves an F-1 of 0.76, which is 0.18 higher than OF-DDNet [1]. [1] Lu,M.,Poston,K.,Pfefferbaum,A.,Sullivan,E.V.,Fei-Fei,L.,Pohl,K.M.,Niebles, J.C., Adeli, E.: Vision-based estimation of mds-updrs gait scores for assessing parkinson's disease motor severity. In: Medical Image Computing and Computer Assisted Intervention (2020)	It is an interesting paper. The paper develops models to predict MDS-UPDRS gait impairment severity and these models are first pre-trained on public datasets to forecast gait movements.	This paper proposed a novel method. It is the first one to used motion data from normal people to pretrain the model and then fineturn on the skeleton-based motor impairment estimation task, which can well solve the problem of small sample size. With a strong and reasonable motivation, they designed a reasonable and feasible transformer-based model and achieved remarkable results. I think it deserves to be promoted in this field.	1) The author presents an original idea of self-supervised learning of a given model by predicting the latter half of the video. 2) Detailed experimental results are presented (e.g., comparing various existing methods, showing results while changing the fine-tuning strategy, and showing performance changes as the amount of training data changes).	the use of pretraining has enhanced the performance	"Some important details are missing. For example, in Sec 2, despite citing previous work, it is not clear enough for the model architect, and in Fig. 1, some details are not shown, such as positional embeddings, self- and encoder-decoder attention in decoder, add operation in residual connection and so on. And the classification result after linear should be ""Activity when pretrain and MDS-UPDRS Score when fineturn"". All these confuse me when I read the paper."	1) The reproducibility of this paper is low.  2) The dataset used is not disclosed, but an accurate explanation is still lacking. 3) Since only simple explanations are listed for the experimental results, it is difficult to grasp the advantages and disadvantages of the proposed method based on the results.	Discussion on possible misclassification missing. Also novel contributions in the work are unclear. The use of pretraining using public datasets is fine but this is general idea that pretraining improves performance which is already established.	I think this paper has a strong reproducibility with a complete description of the training process, as well as its reasonable model design.	A lot of parameters and design details are missing. It seems that it cannot be reproduced only by reading the paper.	reproducible	"It is a good job that shows us the potential of pre-training models in this field. It could be better if the paper can describe the proposed model in more detail. Besides, Fig. 1 is somewhat simple and is very similar to the picture in the referenced previous work POTR. Redesign the picture in a different way could be better. And I suggest that ""z0"" should not be put in the same box as ""z1,z2,...,zT"" for it will not serve as input to the decoder."	In contrast to good ideas, the explanation of the proposed design is not thorough. Compared to the cited paper [1], the novelty of this paper is incremental. [1] Lu,M.,Poston,K.,Pfefferbaum,A.,Sullivan,E.V.,Fei-Fei,L.,Pohl,K.M.,Niebles, J.C., Adeli, E.: Vision-based estimation of mds-updrs gait scores for assessing parkinson's disease motor severity. In: Medical Image Computing and Computer Assisted Intervention (2020)	It is an interesting paper. The paper develops models to predict MDS-UPDRS gait impairment severity and these models are first pre-trained on public datasets to forecast gait movements. Comments are below: Illustrate and discuss the misclassifications from the proposed method? And also suggest possible ways to make it better? Any discussion to make the model more interpretable would add value. Report areas under roc as well and areas under the pr curve for all methods.  What is the threshold used for all classification methods? How was it decided? Is that threshold optimal? Provide more description on the subjects in the study such as their age, gender and severity of Gait Impairment etc.	A novel method that first use pre-training models to deal with small sample size problem in this field.	The manuscript is clear and easy to follow, and the results are plausible. But the novelty is incremental.	further evaluation needed in terms of performance metrices such as area under roc and area under precision recall curve
231-Paper2241	GazeRadar: A Gaze and Radiomics-guided Disease Localization Framework	The paper combines basic radiomic features and visual attention from gaze maps to localize disease.	A novel architecture to fuse radiomics and visual attention is proposed for classification and localization tasks. A novel loss is proposed to calculate the distance between the student block attention distribution, and the joint representation. Experiments demonstrate the effectivenesss of the proposed method.	The authors present GazeRadar, a novel global-focal student-teacher architecture for disease localization based on radiomics information and visual attention features. The representation is used to train a student block for downstream classification and localization tasks. The authors develop novel Radiomics Attention Fusion and Gaze Attention Fusion strategies to fuse radiomics features and gaze features.	Combining radiomics and gaze information. Ablation study of the contribution of each component	1, The proposed problem is clinically relevant, as model explanation is an important task. 2, The proposed method is extensively evaluated on several benchmarks, demonstrating its effecitveness. 3, The problem is clearly motivated and formulated, and each component is well explained. 4, The proposed methods properly addressed the gaze difference problems of the ragiologists.	The authors came up with a novel loss function to transfer the joint radiomics-visual knowledge from the teacher block to the student block. The authors claim that the proposed work is the first work that incorporates radiomics and radiologists' search patterns into a decision-making pipeline. The authors provide ablation study for their GAF-RAF strategies.	Using only a few radiomic features The metrics used for comparison are not typically used for localization/detection tasks.	The performance boost seems rather negligible. It would be ideal if the authors can demonstrate that the proposed method performance increase is significant.	There is a need to work on the clarity of the text. Many abbreviations and incomprehensible references within the text make confusion and make it difficult to understand.	Reproducible	The authors are willing to share the training and testing code. Thus, it's reproducible.	The paper seems to be reproducible since the authors intend to provide the code and open-source datasets were used for evaluation (RSNA Pneumonia Detection Challenge Dataset, SIIM-FISABIO-RSNA COVID-19 Detection Dataset, NIH Chest X-rays Dataset, and VinBigData Chest X-ray Abnormalities Detection Dataset).	The methods section can be restructured for clarity. It is currently confusing as to how the GAF is pretrained and what was the input data for pretraining. Standard metrics such as IoU, mAP can be used for comparison. As a baseline, a simple model that does a basic multiplication of the radiomic image with the gaze map and then thresholded for localization can be used to show the benefit of a deep learning model.	It would be ideal if the authors can demonstrate that the proposed method performance increase is significant.	The authors present the global-focal student-teacher architecture for disease localization based on radiomics information and visual attention features. The authors explore an interesting topic related to eye-tracking. The developed model can be of practical importance, as it improves the results of disease localization using easily collected eye-tracking data. The authors provide a solid ablation study but the paper lacks clarity.	Combining radiomics and attention without the segmentation aspect. Unclear methods description. insufficient metrics for comparison.	Clinically relevant The network is novel Properly evaluated on several benchmarks and the results demonstrate the effectivness.	Technical novelty, reproducibility, and results achieved.
232-Paper2084	Geometric Constraints for Self-supervised Monocular Depth Estimation on Laparoscopic Images with Dual-task Consistency	A self-supervised monocular depth estimation framework is presented for surgical video datasets. This framework has included a scene coordinate prediction branch in addition to depth and pose estimation branches. The pose estimation branch has also been added with an additional Siamese optimization process. A weighting mask is used based on the dual-consistency test to reduce the effect of unreliable predictions.	A self-supervised approach to monocular depth estimation with a promising results Depth prediction and pixel 3d coordinate prediction are handled as two distinct tasks whose results support each other via a consistency loss	This work proposed a self-supervised framework for laparoscopic depth estimation. The authors used the multi-task training strategy, adding scene coordinate prediction to train the network with dual-task consistency. The confident mask is also computed from the scene coordinate prediction. The authors also updated the pose estimation with the siamese process, which improved the pose rotation prediction.	This paper is overall well-written. The flow of the paper is easy for readers to follow. The Introduction section has included a good review of the literature. The technical contribution of this paper is adequate, given the inclusion of scene coordinates and consistency-based weighting mask. A good comparison study was provided by authors and several state-of-the-art approaches have been added into the comparison. The results have shown the proposed framework demonstrated better performance.	The monocular depth estimation is treated slightly different from previous approaches, which enhances the results. The system can be applied to real data relatively easily, since no ground truth is required for training (only a camera calibration matrix) and it works on monocular data, which is easy to acquire Evaluation on real data A thorough comparison against many other methods is performed, as well as an ablation study.	Writing is clear and easy to follow. Adding scene coordinate prediction task as an auxiliary is a novel idea for monocular depth estimation. Averaging the forward/backward poses in training seems to be a useful trick to boost the performance a bit. The performances were compared to the state-of-the-art in general depth estimation areas and in the medical domains, which makes the merit of this work more convincing.	The paper is missing some explanations and insights of methodology in the technical sections. There are also some mistakes in Fig 1 and notations which have made it harder for the readers to capture the gist of the proposed architecture.	"The prediction of the scene coordinates S and the prediction of the depth D are very similar tasks (almost the same except for the representation?). I miss a small discussion of how they are different and why they would complement each other. Would it be possible that training multiple depth-prediction networks (and use them for the dual-consistency check) would have similar results? Section 2.2 is difficult to grasp without reading it multiple times (see details below) Were the runs for the ablation study performed multiple times? Some of the values are so close to each other I feel it may be a coincidence that the ""full"" model performed best. Maybe report the average over N runs? Maybe I missed it, but: Monocular depth estimation is underdefined, i.e. there's no way to know the actual distance (unless objects of known size are in the scene). How do you handle this unknown scale factor? Does this mean that you would have to train on a per-patient basis?"	The intuition behind using siamese poses is unclear. The author mentioned the complex rotation in laparoscopic application makes the problem harder than the autonomous driving cases, but averaging forward and backward poses has no direct relation to dealing with this issue. If the purpose of this operation was to better optimize the pose, additional a loss function on it could be an alternative, such as optimizing the T^t'_t and (T^t_t')^-1 to be the same. Better explanation and more analysis of alternative solutions would be helpful. Regarding camera pose evaluation, the authors only reported the rotation results but missed the translation results. Since both translation and rotation determine the camera pose quality, camera translation evaluation is necessary; at least a demonstration that the proposed methods would not sabotage this aspect is needed.	The authors have confirmed in the checklist that they will share the code to the public. The dataset used in the work is from a public challenge.	The authors mention code being made available, which is great, but please remember to place a link in the paper once it's no longer anonymous.	Implementation details were provided. The evaluation dataset was cited.	"In the first paragraph of Introduction, the authors stated that ARAMIS and RAMIS have become the preferred approach for laparoscopic surgery. This reviewer agrees that RAMIS has become preferred, however, ARAMIS is still questionable nowadays. The provided references for this statement do not provide enough clinical evidence that ARAMIS has become the preferred approach yet. Therefore, it is better for the authors rephrase this statement. There is probably a mistake in Fig 1. Given the only input of Source Image I_{t'}, can the Scene Coordinate Network generate both Scene Coordinate of I_{t} and I_{t'}? Can the authors double check this figure and make corresponding corrections as needed? Also, in the paragraph below the Fig 1, please change ""I_{t} and I_{s} are the input of pose estimation"" to ""I_{t} and I_{t'} ..."". Please describe what is D(p_t) function below Equation (1). Furthermore, regarding Equation (1), how the empty pixels (those ones do not projection) are addressed? Given that Scene Coordinates can be generated from the predicted depth of the image, can the authors provide a detailed explanation why a separate Scene Coordinate Network has to be explicitly trained? I understand that an Ablation Study has been provided, but an insightful discussion on this matter is currently missing, e.g., why this additional branch will benefit on the smooth surfaces? In addition, are Scene Coordinates ranged? Are they normalized? How the performance of Scene Coordinates get impacted when a different camera (with diff. focal lengths, etc.) is used? What are the unit of the rotation errors in Table 2? In radians or degrees? Can the authors describe how the errors were computed? Minor comments: Please use ""Siamese"" instead of ""siamese"", and there are a few places where ""Siamese"" got mis-written (e.g., Abstract, etc.).  In Page 3, change ""To overcome the challenging"" to ""To overcome the challenge"". There are also other places with noticeable grammar mistakes and typos, please perform proof-reading on the paper."	"I don't understand Equation (3). Are you really searching the t' for which E becomes minimal? Does that mean you're selecting the neighboring frame with the lower loss? Or is that a typo? Either way, I think this equation should be explained or removed. Fig. 1 implies that only Source image T_t' is given to the Scene Coordinate Network, however the network predicts both ^c'S_t' AND ^cS_t - is there an arrow for the second input missing? I think this network would require both images? I find section 2.2 relatively hard to follow. I had to read it a few times before I (think I) understood. I understand this is likely because of limited space to describe a complex topic but here are a few things I stumbled accross: The term ""world coordinate system"" is used, however as far as I can tell the system never really uses any consistent world coordinate system? Everything is done in (local) camera coordinates. If I understood this correctly, then I suggest removing the term ""world coordinate system"". ""from the camera coordinate system c to camera coordinate system t'"" should this be c' instead of t'? ""the camera system coordinate c"" -> the camera coordinate system c (?) I think ^cP (introduced for Fig. 3 and in the text on page 5) represents the same as ^cS_t(p_t), correct? As the latter representation is used in Equation 5, maybe replace ^cP by ^cS_t(p_t) (and similarly for ^c'P)? I think you could maybe drop the superscripts c, c' and c'->c? As far as I can tell they're always consistent with the subscripts t, t' and t'->t and this would be one less thing to have to understand/interpret. I think it's clear from the text that camera coordinate system c is where the camera is at at time t and c' for t'. Minor: ""and scene coordinate prediction with novel consistency loss functions under a camera coordinate system."" The ""under a camera coordinate system"" wasn't clear to me at this point - maybe remove here, or explain? Fig. 2 referenced before Fig. 1 ""I t and I s are the input of pose estimation to estimate the transformation matrix"" -> I_s should probably be I_t'? If Fig. 4 represents absolute values, could you show the range instead of ""low"" and ""high""? Since you show rotational errors in Table 2, I assume that SCARED includes ground truth poses? If so, I would mention this in 3.1 where SCARED is introduced (since the ground truth depth is mentioned). Language (suggestions only, I'm not a native english speaker): ""...the smooth surface causes the photometric error to reduce even the corresponding positions between adjacent frames are inaccurate."" Words missing? add ""for"" and remove ""are inaccurate""? ""We predict the scene coordinate prediction as an auxiliary task."" predict... prediction (maybe ""estimate the scene coordinates"" instead?) ""an unified"" -> ""a unified""? ""siamses"" -> siamese""? (2x) ""depth estimation had become"" -> ""depth estimation has become"" ""To overcome the challenging of pose estimation"" -> the challenge of ""Then the minimized photometric error can be deformed as"" -> can be formulated as (?) ""in the previous researches"" -> in previous research? ""two consistency"" -> two consistencies? Is this the same as ""dual consistency""? If so, maybe replace with the same term everywhere? ""an weight mask"" -> a weight mask ""ours w/o scene coords represent s the proposed method was evaluated with the siamese optimized pose process."" -> ? ""on test datasets overall the whole scenes."" -> on test datasets over all scenes. (?)"	"Only the encoders' structures were described. It would be better to also provide details of decoders. ""siamese"" was spelled wrong several times. Page 3, wrong notation ""I_s"". Ambiguous sentence on the top of Page 8."	The addressed application is very relevant to the MICCAI topics. The technical novelty of this paper is adequate and validation has been conducted rigorously which also includes an ablation study. The authors have compared their approach to several state-of-the-art approaches and the results have demonstrated that their approach performs better.	The method seems sound, it is interesting that the two presented tasks seem to complement each other even though they are very similar. As mentioned above, the only larger thing I'm missing is a small discussion why this works (or at least references explaining it). The explanation is mostly clear and can likely be made clearer by making minor changes to the variable names etc. There are multiple language errors, but nothing serious.	This paper proposed a new framework to deal with the special challenges in laparoscopic depth estimation. Although some aspects need more clarification, this work can provide insight for the area, especially its novelty of using dual-task consistency from scene coordinate prediction to improve the depth quality.
233-Paper1173	Gigapixel Whole-Slide Images Classification using Locally Supervised Learning	In this work, the idea of training a network block by block (locally supervised networks) has been adopted to feed Whole Slide Images to the network. Random feature reconstruction has been proposed to improve the last layer feature quality rather than the whole slide reconstruction. Results have been discussed in three microscopic WSI datasets.	This paper proposes to employ the locally supervised learning scheme [24] for bypassing the memory bottleneck that exists in end-to-end WSI representation learning. To further deal with the memory issue, the authors replace the reconstruction loss in the original locally supervised learning scheme [24] with a random feature reconstruction unit.	To overcome the spatial relations loss of conventional multiple instances learning for WSIs classification, this paper proposed locally supervised learning by splitting deep network into multiple gradient-isolated modules.	The idea is novel in pathology and seems applicable; however, comparisons are not comprehensive. The main strengths of the paper: 1- Feeding WSI to the network is beneficial as it preserves spatial information. However, 5X is not what pathologists look at in many cases.  2- Recunstractiong a part of WSI is a good idea to overcome the size difficulties; however, ten patches 128x128 seems intuitive without further investigations.  3- Apply Experiments in 3 public datasets is a good strategy.	Given the memory bottleneck for WSI representation learning, the idea of using the locally supervised learning scheme is interesting and reasonable. The motivations and solutions are clear.	The entire whole slide images can be trained on GPU. they evaluated their method on three public datasets and achieved satisfactory results. This work is interesting.	"Experiments are not comprehensive at all.  1- As you were able to feed bigger images in K=4, why other experiments like 10x and 20x are not reported. This is really interesting if we can see even low numbers with higher magnification in some datasets.  2- Selecting batch size one does not make sense to me. One image and class is one, then the next image class two. Even if you pad images or crop images and feed at least 4 was much more meaningful.  3- Why just compare with MIL methods? To me, this is not even a MIL strategy. MIL networks use a part of patch information, and this is not a fair comparison to compare with just MIL methods. 4- I believe SOTA is ignored. You may report the original paper number  (SOS for LSK) at least and then discuss the advantage of your method. Also, readers want to know the comparison with many other states of the art WSI classifications. It may be good to compare results with results of already published papers with the same experimental setup (at least for lung, I know 4-5 papers with more than 95% ACC) 4- big question mark here. ""only. Our method was able to fine-tune the ImageNet pre-trained weights to adapt to the medical image domain, while other methods directly used the ImageNet pre-trained features."" What does that mean? Other models were not able to fine-tune!"	The technical novelty is limited. The main difference between this work and [24] is the random sampling step added to the reconstruction unit. Some results have not been reported. For the LKS dataset, Please report the result of the original paper, which is the SOS method [18]. There is no explanation why  10 locations are used for the RFR model. There is no hyperparameter to control preference of classification and reconstruction losses. An ablation study is needed. The batch size is 1. There is no discussion around this. Doesn't this affect the stability of training? The equations have not been referenced. This is not clear what do TCGA-NSCLC  and  TCGA-RCC  stands for	In 2.1, what is necessity to use feature reconstruction unit? In RFR, the detailed parameters are suggested to give. At the same position, how does random ensure that most positions are ergodic? The corresponding proof was not found in the following text. And, the reason choosing cropped size is memory? In LKS dataset, why the magnification was 4x, which different from other two dataset. Meanwhile, suggest to explain the reasons for choosing 4-5 magnification. How are the number and location of instances determined? The difference between different number of Module block is minor, however, no specific analysis and verification were given. For results, it is suggested to add statistical analysis of significance, or cross-validated.	Seems OK.	Implementation details are available.	For all code related to this work that they will release if this work is accepted	""" You must edit the paper at least with Gramerly. ""the golden standard for many"" Gold standard ""WSIs is challenging due to their enormous image sizes."" I in WSI stands for Image; then image is repetitive.  WSIs are ""which processes the entire slide exploring the entire local and global information that contains"" think about rewriting the sentence, specially contain ""Duan et al. [9, 8] proposed to train each module by minimizing intra-class and maximizing inter-class distances to improve the data separability."" 8 is a survey and the main contribution of 9 is much wider than Fisher idea. ""LK = Lcls(H(FK(xK-1)), y)"" consider semicolon "","" after equations that continue by where"	Include the results of the SOS method for the LKS dataset. Add discussions around batch size, number of locations (10) you use for the RFR model Reference your equations Introduce the abbreviations Improve the writing	In 2.1, what is necessity to use feature reconstruction unit? In RFR, the detailed parameters are suggested to give. At the same position, how does random ensure that most positions are ergodic? The corresponding proof was not found in the following text. And, the reason choosing cropped size is memory? In LKS dataset, why the magnification was 4x, which different from other two dataset. Meanwhile, suggest to explain the reasons for choosing 4-5 magnification. How are the number and location of instances determined? The difference between different number of Module block is minor, however, no specific analysis and verification were given. For results, it is suggested to add statistical analysis of significance, or cross-validated.	Good innovation with questionable experments. I just accepted to give the paper chance to survive.	The idea of using locally supervised learning for WSI representation learning is interesting. However, the paper may not be ready for publication in its current form. Please refer to the weaknesses section for more detailed comments.	Innovation of method and strong evaluation
234-Paper1622	Global Multi-modal 2D/3D Registration via Local Descriptors Learning	"The authors propose a rigid-registration neural network based on descriptors (local features) extracted from 2D US and 3D MR images. The method is adapted from LoFTR (Local Feature matching with Transformers). Each image is processed with separate U-net like neural networks to produce feature maps. A similarity matrix is filled with the dot products of pairs of descriptors, then filtered with a ""double softmax"" to isolate significant ""matching"" features. This matrix is the basis of the loss function of an end-to-end registration network. The pose is finally estimated from the matches with RANSAC. The method is evaluated on MR + US liver images of 16 patients. Several versions of the method are compared, and overperform a baseline method from ImFusion with statistical significance. While the registration error remains relatively large (on a difficult task), the proposed method is interesting at least as an initialization for more accurate methods which require a close initial pose."	This paper presents an automated 2D/3D registration between US and MR images for ultrasound-guided interventional applications. The algorithm follows a classical feature/landmark matching approach but with features learnt from 2 Unet architectures for US and MR images.	The main contribution is the adaptation of the LoFTR algorithm to multi-modal data and by considering imprecise ground truth.	the method is concise and well described it is adaptable to different image dimension and modality no prior segmentation nor pose is needed, although this information could be taken into account if available. the results are very interesting, especially with a far initial pose nice mitigation of the US images FOV	The paper is very nicely written. The motivation are well explained which connects well with the method and results section. The authors clearly had a good understanding of the clinical problem and the proposed method can well addresses the problem.	Well written Consistent bibliography Clear contribution	"processing successive images in a sweep is a plus, but the ""Multiple frames"" section is really brief the generation of your ground truth pose is unclear, which could impact both the training and results. You use uniformly distributed keypoints to limit the avoid the need for annotated landmarks, but are these points reflecting a matching aven with your ""softer loss"" (page 3)? The estimation of the ground truth pose by manual registration is also questionnable. Was it done and validated by clinicians? the maximum errors (Fig.2) remain very large, which is a strong limit for any clinical use. The failure cases should at least be discussed (image properties, far initially pose, ... ?) it would be interesting to evaluate your method on a dataset with annotations available, to better characterize your results on TRE. You should find several registration challenges with appropriate data (e.g. Learn2Reg or CuRIOUS). while your study is a very good first step, rigid registration is always limited to estimate non-rigid deformation... First evaluate on a more adequate task?"	The method used is not entirely novel since it's largely based on LoFTF algorithm. On the other hand, it has been well adapted to solving the problem for medical images. The results are encouraging but still not reaches a very high accuracy which is needed for most image-guided intervention.	Validation with only 16 patients Incremental improvement of existing work (it is an improvement of LoFTR algorithm)	The method is very well described and could easily be reproduced. The evaluation dataset is not public, though.	The steps involved in the experiment design are clearly explained but lack a bit of details. The code base seems to be unavailable.	The description is quite short and probably difficult to reproduce, see for example section 2.	"""Despite all these advantages, to the best of our knowledge, these approaches have been applied the medical domain only to a limited extend. .."" -> An example of MR/UR registration based on SIFT descriptors could be: https://doi.org/10.1007/s11548-018-1786-7. There are others. ""Multi-modality and multi-dimensionality Different modalities exhibit different visual appearances and also emphasize different structures. We overcome this issue by jointly training two distinct networks (a 2D model for US and a 3D one for MR) to produce cross-modality descriptors."" -> the two networks will provide descriptors for each modality, but are the extracted features the same? Probably not, at least due to the different physics behind each modality. This could be discussed."	It would be nice to see how the algorithm can be generalized to different ultrasound scanning setting, e.g. different depth, frequency, etc. The dataset sample size is a bit small, which is why the authors used cross-validation. But it would be nice to have a larger set for a proper validation.	"The goal is to improve multi-modal registration. The work focuses on  US and MRI, of abdominal images. It is challenging due to the noisy data acquired with US and also because the abdominal part is highly deformable.  The proposed method is based on keypoints extraction and matching. The main contribution is the adaptation of the LoFTR algorithm to multimodal data and with imprecise ground truth.  First of all, two models are trained (U-Net) : one for 2D images and one for 3D data for estimating cross-modality descriptors (matching score are computed by scalar product between features extracted with the 2 differant models). Then, problems about the imprecision of the ground truth by considering data augmentation. The approach is validated on 16 patients with ablation studies. Questions : ""In order to not consider the ultrasound geometry but rather the structural features, we further augment the ultrasound data by cropping it in a random polygon shape"" : it is unclear what is really done and more important why."	The method seems sound, and results are promising on a challenging task.	Although the method is not particularly novel and results still needs a bit improvement, the paper is very clearly written and has a very good clinical relevance.	The novelty of the approach is a little bit limited.
235-Paper1165	GradMix for nuclei segmentation and classification in imbalanced pathology image datasets	The authors propose a data augmentation technqiue, termed as GradMix, to improve nuclei segmentation and classification performance, particularly for imbalanced pathology image datasets.	The paper describes a synthetic data generation method to increase training dataset size and address class imbalance to train more robust and accurate nucleus segmentation and classification models.	The paper introduces a data augmentation technique (GradMix) for nuclei segmentation and classification when cells between classes are imbalanced. The proposed method generates patches showing both major-class nuclei and rare-class nuclei. The proposed method is tested on two public datasets and the results show an improvement in classifying rare-class nuclei.	A new way of data augmentation techinique is proposed to improve the nuclei segmetnation and classification tasks. Compared with GAN model, the proposed method doesn't need the supervised datasets.	The paper proposes a method for synthetically creating nuclei of rare class types in a given imbalanced training dataset to improve class balance and achieve a more balanced diversity in training data. The experimental evaluation with two datasets and one deep learning network shows performance improvements compared with using real training data only.	The authors propose a method to augment rare-class nuclei with major-class nuclei to overcome class imbalance. This is a common problem because nuclear classes are indeed imbalanced. The results show an improvement in classifying rare-class nuclei. Specifically, Table 3 shows results of nuclei classification are improved. F1-scores of miscellaneous nuclei (a rare class) in both datasets increased more than other nuclei types when using GradMix. For the first dataset, F^M increased by 0.035 while F^E and F^L changed less than 0.01. For CoNSeP, F^M increased by 0.011 while F^E increased by 0.016 and F^I and F^S changed less than 0.01. Showing more improvements in miscellaneous nuclei would meet the main purpose of GradMix.	The proposed nuclei augumention is not very impressing in improving the nuceli segmentation and classification performance. On the public CoNSeP dataset, the improvement to segmentation is quite limited, and to classification, the improvement on the Miscellaneous nuclei is with the performance sacrifice on the inflammatory nuclei, which actually is also no the major-class. The experimental results of CutMix is really disappointing, which is nearly worse on all evaluated metrics even compared to without using any data augmentation. Though CutMix is the compared method, but the authors shall compare to a more competitive method.	The authors use their own deep learning network to evaluate the performance impact of the synthetic data generation method. This makes it hard to evaluate if performance gains shown in the experimental evaluation are due to the specifics of the deep learning network and whether the synthetic data generation method will lead to similar performance gains with other networks. For example, the nucleus segmentation performance numbers for the CoNSeP dataset (Table 2) with the proposed method are lower than or equal to those achieved by Hover-Net [2] using real data only (Table III in [2]; e.g., 0.853 DICE with Hover-Net vs 0.846 DICE with the proposed method).	This work would be more appreciated if the authors compare between CutMix and GradMix in terms of the methodology. CutMix was not described in the introduction, so without understanding CutMix, it was difficult to follow the result when comparing CutMix and GradMix.	The description of GradMix is quite clear and shall be reproducible.	The proposed method, the dataset, and the deep learning method used in the paper are clearly described. If the codes and the datasets were to be provided, I believe the results could be reproduced.	The authors use two publicly available datasets and the authors plan to make their code available.	The authors comapre to CutMix, which shows quite disappointing performance. The authors shall explain why CutMix behave worse here, otherwise I'd assume the way the authors apply CutMix is problematic. In the evaluation of nuclei classification, besides the F1-score for each type of nuclei, the overall performance is also suggested. I'd also suggest to split the data into train/test multiple times, and report the mean/std. Then the results would be more convincing.	"This work targets an important problem; it is difficult to generate large, representative training data for nucleus segmentation. It proposes a synthetic data generation pipeline to increase the number of nuclei of rare class types in imbalanced datasets. The experimental evaluation shows performance improvements. The experimental evaluation is limited because it uses a single deep learning method (developed by the authors). While there are performance improvements, they are relatively modest (around a few percent on average; Tables 2 and 3). The authors should include another network (e.g., Hover-Net) to show if the performance improvements from the proposed method are due to the specific deep learning network used in the paper or other networks can benefit as well. For example, nucleus segmentation performance numbers from the Hover-Net paper [2] (Table III in [2]) are generally higher or equal to the performance numbers obtained by the proposed method (Table 2 in the paper). The Hover-net paper uses real data only. The authors should also provide a better comparison of their approach to other approaches ([a] and [b]):  [a] Gong, Xuan, et al. ""Style consistent image generation for nuclei instance segmentation."" Proceedings of the IEEE/CVF Winter Conference on Applications of Computer Vision. 2021. [b] Hou, Le, et al. ""Robust histopathology image analysis: To label or to synthesize?."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019. Both [a] and [b] use in-painting and nucleus generation via simple segmentation or deformation of real nuclei as part of synthetic data generation steps. Both works also implement variations of an approach in which an end-to-end training approach is used that integrates training of a task specific network and the synthetic data generation process. The goal is to guide the synthetic data generation and model training to generate training data that can better optimize the task specific model. That is different than the approach used in this paper which separate synthetic data generation and training of the segmentation model. Readers would benefit from a comparison to those approaches."	"This work would be more appreciated if the authors compare between CutMix and GradMix in terms of the methodology. CutMix was not described in the introduction, so without understanding CutMix, it was difficult to follow the result when comparing CutMix and GradMix. In Figure 1, I recommend to avoid using blue color for rare-class. It was difficult to see rare-class nuclei because hematoxylin-stained nuclei are also blue. What is the definition of ""major"" and ""rare""? Figure 3 should show more instances of rare-class nuclei, because the novelty of this work is to classify rare-class nuclei when imbalanced. I only see one instance of miscellaneous."	The authors propose a new nuclei augmentation method to boost the rare nuclei segmentation and classification. While based on the experiments, the improvement is  limited and not convincing enough.	The paper targets an important problem. The proposed method shows performance improvements on two datasets.	Class imbalance in nuclei is a common and realistic problem for nuclei segmentation and classification in pathology. With GradMix which is the proposed data augmentation technique to overcome class imbalance, Table 3 shows results of nuclei classification are improved. Specifically, F1-scores of miscellaneous nuclei (a rare class) in both datasets increased more than other nuclei types when using GradMix. For the first dataset, F^M increased by 0.035 while F^E and F^L changed less than 0.01. For CoNSeP, F^M increased by 0.011 while F^E increased by 0.016 and F^I and F^S changed less than 0.01. Showing more improvements in miscellaneous nuclei would meet the main purpose of GradMix.
236-Paper0770	Graph convolutional network with probabilistic spatial regression: application to craniofacial landmark detection from 3D photogrammetry	A graph NN based approach to locating landmarks on photogrammetric images.  Takes account of specific nature of data and includes a regression method to deal with landmarks which don't coincide with a mesh node.	The authors describe a novel method for automated landmark placement on 3D point clouds of faces that leverages a graph convolutional neural network architecture. The clinical motivation for automated landmark placement is for enabling automatic and reproducible analysis for 3D photogrammetric data. In and of itself, the spectral model approach used here is an adaptation of the well known Chebnet (Defferrard, M., Bresson, X., & Vandergheynst, P. (2016)).  The authors demonstrate the landmark placement accuracy of their framework through an analysis of a large dataset (982) of 3D paediatric faces and they compare their approach, plus variants thereof, to Pointnet++. The results quantitatively indicate the improvement gains in landmark placement accuracy of their approach.	This paper introduces a graph-based convolutional neural network to pediatric craniofacial landmarks from 3D photographs (i.e., surface meshes). Three strategies are adopted in the proposed method: 1) Multi-resolution spatial features at every vertex are extracted with Chebyshev polynomials; 2) A novel weighting scheme dependent on the local data density at every surface location to aggregate the spatial features; 3) A new probabilistic regression framework that uses the aggregated spatial features to calculate landmark locations. The authors evaluated the proposed method by detecting 13 landmarks from a set of patients' 3D craniofacial surfaces. Generally, the three strategies are actually not new, similar ideas are commonly applied by the related methods in the field of geometrical/point-cloud deep learning. In addition, the landmark detection accuracy of the proposed method is too low to meet clinical requirements.	Elegant approach giving SOTA performance (compared to various well known methods).	In and of itself, the spectral model approach used here is an adaptation of the well known Chebnet (Defferrard, M., Bresson, X., & Vandergheynst, P. (2016)). The extension and therefore technical contributions, include a multi-resolution feature quantification approach leveraging different orders of Chebyshev polynomials; a mechanism for weighted spatial feature aggregation where the weighting is cognisant of the local density of vertex data; and a probabilistic regression framework for regressing landmark coordinates as a combination of graph node coordinates. The method caters for both connected and unconnected nodes which makes their approach more applicable generally for automating point-based data analysis.  Their probabilistic framework to regress landmark coordinates as a combination of graph node coordinates provides a general improvement in landmark placement accuracy that  improves not only their approach but also Pointnet++. The use of multi-resolution spatial feature calculation and aggregation schemes makes the proposed approach somewhat robust to local data density issues. Although this reviewer feels that perhaps this issue could be managed more cheaply with some mesh preprocessing (see below in weaknesses)	The geometric deep learning was applied for craniofacial landmarks detection from 3D surfaces.	What is the inter- and intra-rater repeatability for the manual annotation?  How does your method compare to that?	A weakness to the study, in this reviewer's opinion, is the use of a single expert's landmark placement as ground truth. Typically in morphometry studies, more than one expert's landmarks are used to remove subjective bias and this maybe more important when the landmarks are to be used for training an algorithm. It seems to this reviewer that isotropic vertex placement of the raw image data would reduce the need for the proposed multi resolution approach? Isotropic vertex placement regularises the distance between neighbouring nodes regardless of spatial location thus possibly enabling single order polynomial use. Given that the authors are already doing some preprocessing of the raw image data anyway (section 2.2), could this note gave been done within that step? Please note this is not a major limitation, rather an honest question. As indicated above, technically, the proposed framework seems to be an extension of Chebnet, albeit with some important contributions to handle the peculiarities of automated 3D landmark location.	1) The literature review is limited. There are a number of point-cloud deep learning methods [1], which are originally designed for the object detection from 3D point cloud, should can be directly applied for the task of this paper. For instance, VoxelNet [2] and PointRCNN [3] should be mentioned in the Introduction and compared in the Experiments. 2) The detection accuracy of this method is relatively low. The landmark localization error can be more than 10mm as reported in the Experiments section, such large errors should be not clinically satisfied.  [1] Guo Y, et al. Deep learning for 3D point clouds: A survey. IEEE TPAMI. 2020, 43(12):4338-64. [2] Zhou Y, et al. VoxelNet: End-to-end learning for point cloud based 3D object detection. CVPR, 2018, pp. 4490-4499. [3] Shi S, et al. PointRCNN: 3D object proposal generation and detection from point cloud. CVPR, 2019, pp. 770-779.	Algorithm described in enough detail to reproduce it with some thought (and checking the references).  No mention of a public version of the code/data in main paper.	There is a full description of the methods and data used indicating good attention to reproducibility.	The reproducibility of this paper is unclear without the released data and code.	"Overall a sensible approach to an interesting problem, with evidence that it gives good performance. What is the required accuracy of the placement for clinical application, and does the system achieve that? Minor points: Section 2.2 Unnecessary precision. Sufficient to say the average number of nodes was 8000+/-4100 ""unitary surface normal"" -> ""unit surface normal"""	"There is no justification for the orders of polynomials used (k=3 and 7). Where these decided on empirically? The authors may want to comment on the large standard deviations in their results on average landmark detection error. Section 5 conclusion. There is a typo in the first sentence. ""We presented a novel graph convolutional neural network to for the automatic identification..,"""	1) More related works should be reviewed and compared. 2) The detection results should be assessed clinical experts to confirm the clinical feasibility of the proposed method. 3) Quantitative evaluation results should be provided.	Proposes a sensible solution to an interesting problem, and demonstrates that it works.	The paper indicated novel extensions of GCN to make it applicable for landmark detection in 3D facial photogrammetry. The paper is very well written, scores highly on clinical significance, has important contributions to the state of the art, and presents a balanced reporting of results.	1) The literature review is limited. There are a number of point-cloud deep learning methods [1], which are originally designed for the object detection from 3D point cloud, should can be directly applied for the task of this paper. For instance, VoxelNet [2] and PointRCNN [3] should be mentioned in the Introduction and compared in the Experiments. 2) The detection accuracy of this method is relatively low. The landmark localization error can be more than 10mm as reported in the Experiments section, such large errors should be not clinically satisfied.  [1] Guo Y, et al. Deep learning for 3D point clouds: A survey. IEEE TPAMI. 2020, 43(12):4338-64. [2] Zhou Y, et al. VoxelNet: End-to-end learning for point cloud based 3D object detection. CVPR, 2018, pp. 4490-4499. [3] Shi S, et al. PointRCNN: 3D object proposal generation and detection from point cloud. CVPR, 2019, pp. 770-779.
237-Paper0826	Graph Emotion Decoding from Visually Evoked Neural Responses	This paper introduces a novel method, Graph Emotion Decoding (GED), to decode emotions by integrating emotion scores of the video stimulus and brain responses from brain regions. By stacking layers of the GED, the model uses the relationships of the brain regions and neighboring emotions to decode. The model showed reasonable performance by comparing with other models and gradually improved by staking the layer of the model.	The authors proposed a neural network to predict scores of individual emotions for presented videos from fMRI recordings. The network architecture is simple but the results are better than some of the state-of-the-arts methods. Overall this paper is interesting and clearly written.	The Neural Decoding framework is proposed to find the association between Emotion and brain regions via a bipartite graph structure.	This study showed a novel method integrating emotional responses and brain responses of fMRI. Even using the GED, the model can decode using emotion and using the connections of the neighboring emotions. Additionally, this paper showed reasonable performance by interpreting and comparing with the other state-of-the-art model. GED uses more integrated information, making the performance better and even gradually improving performance by stacking layers.	The authors proposed a relatively simple network architecture to achieve a relatively difficult problem, and proved the effectiveness of their method.	It seems an interesting idea to perform embedding propagation. t-SNE Visualization generates interesting visualization, similar emotions tend to be closer. It shows significant improvement above the state-of-the-art brain-network-based models.	Although this paper showed interesting results and a novel method, the number of fMRI subjects is small (n=5), which may cause reproducibility issues when applied to other datasets. Also, since this paper didn't use the emotion scores from the individuals that performed fMRI sessions, which means that the emotion decoding they did was the emotions that people mostly feel from videos, not the emotions of the individual itself This paper doesn't have a conclusion section. It would be worth showing the conclusions by summarizing the results and the interpretation to emphasize the main point of this study. Lastly, it would be better for this paper to show the relevance of this method by interpreting more of the embeddings from emotions and the brain regions, such as interpreting which of the ROIs showed meaningful results to interpret the corresponding emotions.	The experimental design is not clearly stated in article. Also authors should compare their results to some of the newest models.	I don't get a clear understanding of how the evaluation is done. If there are five subjects, are we doing leave one subject out validation? What are 10 folds here? How this 10 fold is formed? Or one question did BrainGNN apply a similar evaluation setup? or it's run by the authors with this idea.	As mentioned in the weakness section, this paper only used five subject data from fMRI, it would be better to use more data to show the reproducibility.	acceptable.	It seems reproducible. Data is available.	It would be better if they could provide the conclusions by summarizing their works and their interpretation. Also, to convince the model interpretation of which emotion scores and the regions in the brain are important for decoding emotions would be needed. Additionally, since the number of the subject is small it would be better to perform an analysis on the other dataset to show the reproducibility.	Overall this paper is interesting and the authors sort to increase the accuracy of a difficult prediction problem using a simple way. The proposed emotion-brain region architecture is novel, and very straightforward to understand, i.e. the biological interpretation of the model is very clear and understandable. However I do have some questions and concerns about the experimental design and the results. 1, it is not clear what value were authors trying to predict, emotional score for audience like me is quite vague, and subjective, authors should clearly indicate how the emotional values were measured. 2, how the input vector was generated was not clear, emotional stimuli (from vedios) and fMRI signals are both time series, did authors segment the time series into pieces? How? and what is the length for each time window? 3, is the output (the emotional score) a vector or a scaler? If it was a vector, then how the accuray was measured? If it was a scaler, what does it represent in a time series window? 4, the prediction is based on a fMRI voxel signals but not a functional brain network or graph. However two methods authors compared (GCN and BrainNetCNN) are both graph based, how authors ran the two models using the same input is unknown. More information is needed. 5, I don't think the models authors compared are most state-of-the-arts methods, authors should compare their methods with more recent methods.	Please refer weakness	The data they used for the fMRI was obtained from a small number of participants to represent the objective emotions. The results may be affected by the subjective emotions of the participants. The reviewer recommends performing an analysis using more fMRI data to show the novelty and its reproducibility. Additionally, as this method uses embedding of the emotions and brain regions, it would be better to interpret the results from the brain regions. Such as explaining the meaningful regions for each corresponding emotion.	The prediction results are better than some state-of-the-arts methods, but the network architecture are much simpler than the methods they are comparing.	The idea seems interesting to exploit relationships between emotions and brain regions. The embedding propagation approach is interesting.
238-Paper1499	Graph-based Compression of Incomplete 3D Photoacoustic Data	In this paper, a graph-based compression scheme is proposed for incomplete 3D photoacoustic data. Both objective and subjective evaluations are provided to demonstrate the compression quality.	The work proposes a tailored compression approach for photoacoustic data. The compression mode is locally adaptive, depending on present features and in the advanced mode uses graphs to encode features.	The paper approaches the problem of incomplete 3D PA in a graph-based encoding scheme. A reliability-aware rate-distortion optimization (RDO) is proposed to enable adaptive compression PA observations based on different reliability levels.	(1) As claimed by the authors, it is the first work to apply graph signal processing in incomplete PA compression. (2) The proposed method allow acceleration of data acquisition and small data size. (3) Medical doctors are invited to assess the proposed method.	The compression shows good performance compared to classic compression. Compression performance has been tested with clinical data and experts rated that there is no clinical value lost with the compressed data.	The topic is interesting and clinically significant. The paper is well organized and easy to follow. A series of experiments in real clinical data demonstrated the superiority of the proposed method over DCT.	(1) The benefit and significance of compressing PA data is not elaborated in the paper, and I think it is a major issue to support the value of this paper. (2) Experimental results need more explanation. For example, I noticed the authors claimed a larger compression ratio K brings more advantages of the proposed method. But the results shown in Supplementary Material seem not very convincing. (3) The clarity and accuracy of the writing language needs improvement.	Some technical details are unclear to me, such as how the image data is turned into the graph structure is not entirely clear. I believe it would not be straight forward to reproduce the results. Nevertheless, the authors have indicated that code will be released, so this would help. The premise of compression of undersampled data in the reconstruction is not clear. The undersampling would appear in the PA measurements and the image domain is usually fully sampled. This needs more detail or motivation I would have hope also for more computational details to be included: needed memory in absolute (MB) and not relative. Computation times etc.	The proposed reliability-aware rate-distortion optimization (RDO) need try all three coding modes and then calculate the coding cost, which significantly increase the overhead of compression. Comparison is limited to Graph-based methods. While authors point out several disadvantages of popular image/video compression standards, there are not experiments to support it. The detail of decoding is missing in method part.	Although related codes are not publicized, I believe this method is not difficult to reproduce with introduced technical details.	Codes will be released. Otherwise reproducibility would be difficult.	Authors do not mention the availability of source code.	"(1) The significance of the proposed method should be stressed by providing more explanation on PA data compression. Specifically, what is the data size you used in the article? If the (W,H,D) in Section 2.1 are not large, so is such compression valuable? (2) I noticed a conventional CS method (BP algorithm) is used here. I wonder if there is other state-of-art CS can be used here? Because the sampling ratio is not very low here, meaning regular CS methods are sufficient to recover the signals confidently. (3) Words and presentations should be chose more carefully. For example, in Section 2.1, is the equation ""sigma X_=P_inc"" an accurate formulation? Besides, the authors claimed ""As medical data tends to be sparse, ..."", general medical data is sparse in what sense? (4) In Equation (3), only the largest $\lambda$ is used, what if we keep top N largest values? (5) In Figure 3, I observe the margin between ""DCT-only"" and ""GBC-only"" is narrower compared with the one beween ""Prop-full"" and ""GBC-only""? If combined with the concern about the rationale of PA data compression. This point further determines the merit of this paper."	My primary concern is how the undersampling and reconstruction is performed. Usually in PA and other tomographic applications, the undersampling to save measurement time is performed in the measurement space (here the PA signal, which is a time-series). Reconstructions are then performed in the image domain, there is no undersampling done here in the reconstructions. If I understand the paper correctly, the authors state that an undersampling is performed in the reconstructed PA image (section 2.1). This does not make sense to me. The undersampling needs to be performed in the measurement space.  Additionally, the undersampling would not be random and follows a pattern that pixels on the 2D scanning domain are omitted, while for all measured points the full time series is available. This needs more clarification and motivation. Maybe I did not understand it and the undersampling is just done to save memory of storing the image? But then the whole discussion on compressed sensing (which concerns measurement space) is misleading. For the construction of the graph, it is not entirely clear how we go from the voxel grid to the graph. This possibly assumes the random omission of voxels? These would be then excluded in the graph? Maybe a figure could help to illustrate this. In equation (1), could you explain the role of \sigma? I would ask for some computational details. How long does the encoding part in the compression take and also the decoding? Could you add some absolute numbers, MB of image before and after compression?	"Including more comparisons with popular image/video compression standards and other learning-based methods will make this approach more convincing. For example, AVC [27], HEVC [20], ref A, and ref. B. Ref A: Pengfei Guo, Dawei Li, and Xingde Li, ""Deep OCT image compression with convolutional neural networks,"" Biomed. Opt. Express 11, 3543-3554 (2020) Ref B: Zhihao Hu, Guo Lu, Dong Xu; ""FVC: A New Framework Towards Deep Video Compression in Feature Space"" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2021, pp. 1502-1511 Authors may clarify the relation between Q and compression ratio (CR)? What's the definition of the compression ratio? From Fig.3, under same Q, the proposed method always achieves a smaller compression ratio."	Based on the current presentation of the significance of this paper and the experimental results, I give my recommendation.	I believe the premise for the undersampling in image domain is either incorrect or not well explained.	The rating reflects innovation of the approach for 3D PA compression.
239-Paper1645	Greedy Optimization of Electrode Arrangement for Epiretinal Prostheses	This paper presents a greedy optimization approach to selection of optimal electrode sites for a retinal prosthesis design. The approach uses a phosphene shape model to optimize the arrangement of electrodes to maximize predicted coverage of the visual field, as predicted by the model.	This paper is in a very specialized field of electrode placement for epiretinal prothesis. The paper defines the problem and proposes a disctionary learning based optimization for arrangement of such electrode for getting the desired outcome.	This paper describes a novel approach to determine the distribution and number of retinal implant electrodes to maximize / optimize the resulting visual perception.	The paper is well written and organized. To my knowledge, this is a novel approach for optimizing retinal prosthesis design, which is an important clinical area.  This approach could be broadly applicable to the design of other neuromodulators. A validation study shows potential performances advantages over state of the art design.	This paper's strenght is in modeling of a clincal problem and proposing a solution in form of adapting existing methodological approaches and evaluating the results in terms of simulated visual subfield coverege.	Nice paper that describes, by using a formulation of dictionary optimization, how to achieve an optimized electrophysiological simulation with, e.g. the Argus II implant. The methodological foundations are quite well exposed and is complemented by an analysis of the algorithm's performance. This approach suggests that a rectilinear array of electrodes not necessarily be the optimal one when compared to the results of this approach.	The application does not involve the use of medical images like most MICCAI submissions, so it may have less broad interest across the conference. But I think the work still fits well under the computer aided intervention umbrella and is a nice example of model-guided design. As written, the validation study lacks data showing the optimized utility function will translate to optimized image quality.	This paper is way to specialized for  being presented at MICCAI conference. The mathematical and algorithmic approaches are known but the modeling of the problem at hand and its implementation in this filed is new, however this paper will not be appreciated in MICCAI community.	While the approach per se is novel, it is not clear why the authors reproduced (?) a figure of Ref. 2, or why the did never mention that Fig. 2 was calculated with the Python library in use for this study. It seems that the author only refer to one single other work. If so, this should deserve mentioning. If not, the section on related work is incomplete. While this work is exploring a rather exotic field of neurostimulation it is quite surprising that the introductory part is extremely short. It would be highly recommendable to introduce the readers to the field with its challenges and problems. Concerning the Phosphene model, some more information on this would be readily welcome. In the section H and W are used but never defined. This has to be fixed, as later on these two values pop up again; comprehension of eq. 3 suffersb dramatically. In addition, F as defined to be [0,HW] is not clear. Furhtermore, eq. 3 is not motivated or referenced clearly. In view of these shortcomings it is hard to judge the real novelty of the approach. Concerning Fig 2 an improved visualization of the values for the precentual coverage is suggested.	Methods are clear and reproducible.	The work here is reproducible in terms of simulation and computation.	na	"I think the work could benefit from some examples of image ""reconstructions,"" i.e., it would be interesting to see examples of how the optimized electrode positions might be leveraged to create better quality image perception."	If accepted and published at MICCAI, I think that the authors will not receive the feedback and citations they would get in case they submit and publish this work in a more relevant community. As citations in this paper clearly shows, no prior work has been published in MICCAI, IPMI, IPCAI, Medical Image Analysis or TMI, which are the main conferences and journals relevant to MICCAI. I think that this work will have much more impact if published in a more specialized community.	please see above.	It's a well-written, interesting study with moderate weakness as described above.	While the paper is well written, the methods are reasonable and the numarical results are valuable, I strongly beleive that MICCAI is not the right community for publishing this work at.	The work itself seems to be good and well done, but it suffers from inconsistencies, missing reference to other basic work, negligencies and cold be improved in the visual presentation of results.
240-Paper1177	Hand Hygiene Quality Assessment using Image-to-Image Translation	The authors propose an AI system to evaluate and document the effect of hand hygiene procedures, using fluorescent hand disinfectants and Ultraviolet (UV) images of the hand. Aim of the proposed method is to generate standardized hand template images which display the amount and location of the hand disinfectants.	This paper describes a deep learning-based segmentation and image-to-image translation approach to standardize hand hygiene documentation from images of hands with different amounts of skin coverage (with desinfectans) taken under standardized UV lighting conditions. The models in the paper are based on a U-net architecture augmented with attention gates in the generator path. The authors use a self-generated dataset to train and validate their models.	The paper describes a deep learning approach for hand hygiene analysis. After applying a fluorescent hand disinfectant, pictures of hands are analyzed using a segmentation network and mapped onto a common template. Results are reported on synthetic and real images.	To my knowledge, this is a novel application of AI methods to establish a better processing, monitoring and documenting of hand hygiene. Using hand templates to document the outcome of UV test for hand hygiene were previously proposed, although templates were generated manually. The proposed method has the potential to measure, document and compare effects of different hand hygiene procedures with considerable more participants, in multiple centers and in different settings.	The paper describes and interesting application / problem domain. The paper is well written in principle.	The key strength of the paper is the proposal to use a neural network (NN) for hand hygiene quality. NN overcome the difficulties of manual labelling or hand-crafted ML approaches on this task. Automatically analyzing fluoresence images and mapping them to a common template provides a relatively inexpensive and rapid way to analyze hand hygiene.	The proposed framework comprises of two main step: Segmentation and shape transformation. Although the segmentation step was extensively tested, evaluation of the shape transformation step was only performed on simulated data	I think the paper lacks convincing motivations and explanations why the chosen methods are appropriate (or necessary). The authors have to make sufficiently clear why a deep learning-based pipeline is needed at all, if the reference annotations were actually created with more traditional (non-learning) image processing methods (Maybe I've missed something but that's what I took away from it). I do not understand, what the benefit would be to translate the results in a standardized template space using a U-net (or any other learning-based method). Wouldn't a spatial registration to a template (that could be created on-the-fly from the available data) be more robust and standardized then an image-to-image translation model? I think this has the be clearly explained and it would also nice to see a comparison with a registration-based method. I am also not convinced with the validation of the methods, in particular for the hand translation model. It is not obvious to me that the synthetic data are actually a good proxy for the real skin-coverage patterns. Also: when would you consider a result good or robust enough for medical documentation? Is variability of the estimates (and their quality) an issue? This part would have to be clarified.	The approach is analyzed using a single model (U-Net), and it would be important to compare to more recent segmentation model (e.g., U-Net++, DeepLabV3, Transformer). Also, the details and limitations of the approach with respect to generalization to: the type of fluorescent used, the demographics of the dataset, and any other confounding factors should be clearly discussed.	The manuscript is well written and contains all information required to reproduce the work. The authors will also make the pre-trained models public, although not the dataset.	The authors do not explicitly state if they make the code or the dataset available.	All the details with respect to implementing the model have been clearly discussed. I encourage the authors to consider releasing the dataset, to facilitate future research in the area.	"The manuscript is well written and easy to read. I have two minor remarks for improvement of the manuscript: On page 5, please add a reference (website, or publication) regarding the MediaPipe and finger-web. In the second last paragraph of the Introduction, there is an extra ""used"".  Please see below two more general comments/suggestion Have you considered to compare your method with using 2D statistical shape models for hand contours? Such shape models might also be useful to generate a test dataset for the hand translation model. I believe the strength of the proposed method is in the improvement of methods introduced by Kampf et al to document, analyze and compare data from much larger populations and multiple settings. Therefore, I would have enjoyed seeing one or two of the statistical evaluation graphs as proposed by Kampf et al. For example, in your reference [5], Figure 1 or 2 show the frequency of untreated skin using a color map. Having a similar image (even just for one of your task categories) in your manuscript would show a potential application of your proposed method."	"p.4 Is the mapping function R^s -> R^t to signify a mapping from s-dimensional real space into t-dimensional real space? I found this a bit confusing p.4/5 The loss function is not well explained. Why is the chosed formulation more robust? This would be nice to briefly explain. p. 5 The experiments with the standardized template space are not well explain and not motivated from my perspective. Why is a registration-based approach not a good option here? This should be explained I think. What was the idea behind the triangle/trapezoid mapping approach? p. 6 Evaluation metrics: Why are U-Net and U-Net++ the baselines to compare against? Of course it depends what you want to compare. So here it seems you would like to argue that the Attention Gates are an important addition to model. However, isn't the more important question if a deep learning-based approach is better than a baseline method? Is there maybe a more traditional method that you could compare against? p.7 Results: Are the difference in Dice and IOU between the models practically relevant ? Can you comment on this? p.7 ""Thus, to avoid overfitting, we chose to use the model trained with eight epochs for translating the lab study data to hand templates"" This is a bit of an unclear argument to me. Why eight epochs? Did you inspect the reasons for overfitting?"	Please address the issues discussed in the main weaknesses section.	I think the work described a novel application and would contributes an interesting topic of research to the conference.	I think it could be quite interesting paper from the application point of view, but the description and validation of the methods is rather weak I would say.	The paper proposes an innovative application of NN to a medical analysis task. However, the analysis of the computational models and discussion of the biases and ethical considerations with respect to the dataset is very limited, given the proposed task.
241-Paper2141	Harnessing Deep Bladder Tumor Segmentation with Logical Clinical Knowledge	1.This paper proposes a novel bladder tumor segmentation method by fusing clinical logic rules of bladder tumor and wall,the rules can guide the DCNN to produce precise segmentation results 2.This paper validate that fusing the logical clinical knowledge is helpful to reduce the data dependency of DCNNs for image segmentation.	The authors propose a novel bladder tumor segmentation method in which the logic rules of clinical knowledge are incorporated into DCNNs.	The main idea of this paper is to merge the clinical logic rules with Deep Convolutional Neural Networks (DCNN) to enhance the segmentation of the bladder tumor. The clinical logic rules depend on the appearance view of a bladder tumor must rely on the bladder wall. While the standard U-net is used for DCNN to create semantic segmentation.	The motivation of Integrating clinical rules into bladder tumor segmentation methods is suggestive. 2.The neural network is very cleverly designed, and the figures in this paper are well-drawn. 3.The transformation of graph neural network is described very carefully, and the definition of loss function is innovative.	The method is novel and provides an original way to use clinical data. The paper and the figures are clear. The authors provide comparison with other state-of-the-art methods.	the paper is well-written and not organized.  -There is a novelty, not big,  for fusing DCNN with clinical logic rules.	I think the main weakness of this paper is, lack of literature review and comparative experiments on clinical logic rules fusing method.	The authors should provide more information about some parameters. In particular they should explain how the parameter beta was obtained. Was the value obtained empirically? (The narrow range 0.9-1 should be justified. The value 0.01 in the Lsegment formula is not justified.  Were the experiments (groundtruth) validated with clinicians? There is a lack of information regarding to that issue.	"-please mention the results in the abstract. the author should write precisely about the bladder tumor. What challenges did you face when segmenting the bladder tumor in the introduction? The introduction sections contain a limited number of the related work for bladder segmentation, only three. However, there are many publications for bladder segmentation in the top journals. Besides, it would help to write at least a paragraph about related work for merging the DCNN with clinical logic rules, not necessarily with bladder segmentation but with any other applications. You wrote in your manuscript, ""To the best of our knowledge, the related works of involving logic rules into DCNNs for medical image segmentation are very limited."" You did indicate any current model in your manuscript. What is the difference between your model and the current one? How did you estimate the value of the variable b? -construction logic rules for the bladder tumor are not clear enough for me. Would you please rewrite or reconstruct the figure for it, Fig. 2? many aspects of the method without references, which means you created everything equations, ideas!! -The author compared the segmentation results with only the general techniques, not current state-of-the-art approaches for bladder tumor segmentation. They compared their results with only one reference with number 14 related to bladder segmentation. However, many recent works are published in the top tier."	The overall method is explained clearly, but the details of clinical knowledge integration into the network are not clear enough	The code is not available but the authors guarantee that they will provide it if the paper is accepted. In that case I do not have other way to proof the reproducibility. The method described looks clear and it looks as reproducible, but the results cannot be checked without the code and the dataset.	NA	"The paper should be compared with more diagnosis rules embedded methods, for example ""Integrating Diagnosis Rules into Deep Neural Networks for Bladder Cancer Staging"". 2.Describe the details of the methods by which clinical knowledge is integrated into the network."	The method is novel and provides an original way to use clinical data. The paper and the figures are clear. The authors provide comparison with other state-of-the-art methods.  The authors should provide more information about some parameters. In particular they should explain how the parameter beta was obtained. Was the value obtained empirically? (The narrow range 0.9-1 should be justified. The value 0.01 in the Lsegment formula is not justified.  Were the experiments (groundtruth) validated with clinicians? There is a lack of information regarding to that issue.	If the author is the first one who applies this idea, it will be a good seed with more analysis for the top journal paper. -May be adding the wight for equation one be adding a before Lsegment and tuning the two available a and b will enhance the segmentation accuracy. The data for bladder segmentation is imbalance. I think you should select a loss function that can face this challenge, such as Tversky loss, which will be better than cross-entropy.	It is instructive to integrate clinical logic rules into segmentation tasks using GNN,so I vote to accept.	The method is novel and provides an original way to use clinical data. The paper and the figures are clear. The authors provide comparison with other state-of-the-art methods.	I found a novelty if he is the first to create it with perfect organization.
242-Paper0695	Hierarchical Brain Networks Decomposition via Prior Knowledge Guided Deep Belief Network	This work proposed a novel method to incorporate prior knowledge to DBN for hierarchical brain network decomposition. Sufficient experiments suggest the proposed method can converge faster than the traditional DBN models. In addition, the proposed method can identify better task and intrinsic functional brain networks compared with the traditional DBN models. This work contributes a novel idea on how to introduce prior knowledge to unsupervised deep models for fMRI data analysis.	The authors proposed a prior knowledge-guided version of deep belief network to infer brain networks with the task labels used as prior information. The inferred networks are more aligned with the ground truth resting-state network	In this paper, the authors propose a novel prior knowledge guided DBN (PKG-DBN) model hierarchical brain network decomposition. By incorporating such constraints in the learning process, the proposed method can simultaneously leverage the advantages of data-driven approaches and the prior knowledge of task design.	This work proposed a novel algorithm to incorporate prior knowledge to DBN for fMRI data analysis. The idea of this work is clearly conveyed. The algorithm is clearly formulated. It is an interesting topic how to incorporate prior knowledge into deep models. This work contributes a novel idea on how to introduce prior knowledge to unsupervised deep models for fMRI data analysis.	The results are interesting as providing task labels will help form more reliable brain networks.	This work contributes a novel idea on how to introduce prior knowledge to unsupervised deep models for fMRI data analysis. This is the first supervised hierarchical FBN identification method as far as I know.	The writing needs improvement and a few details are not clear. For instance, how the overlap rate is calculated is not clear. For Table I, in addition to the average overlap rate between DBN inferred RSNs and their corresponding RSN templates across subjects, the standard deviation should also be reported.	The aim of the proposed framework is not very clear. In my understanding the previous DBN approaches are used as unsupervised method due to the lack of labelling. However, in this proposed framework, as we are aware of the task labels already, why can't we switch entirely to the supervised method?	The writing needs improvement and more details should be clear.	They will release the code after the paper is accepted.	It is reproducible with codes and data becoming available	The paper decribed the method clearly, but did not release the code currently.	In this paper, authors proposed a novel prior knowledge guided DBN (PKG-DBN) model to identify the hierarchical and complex task functional brain networks (FBNs). Specifically, they enforce part of the time courses learnt from DBN to be task-related and the rest to be linear combinations of task-related components. By incorporating such constraints in the learning process, the PKG-DBN model can simultaneously leverage the advantages of data-driven approaches and the prior knowledge of task design. Extensive experiments on both HCP Gambling and Language task fMRI data have well supported the proposed ideas. In general, this is a novel and interesting work and the paper is well structured. There are only a few writing problems need to be improved. For instance, how the overlap rate is calculated is not clear. For Table I, in addition to the average overlap rate between DBN inferred RSNs and their corresponding RSN templates across subjects, the standard deviation should also be reported. The figure resolution need to be improved.	In this paper, the authors propose a prior knowledge guided version of DBN for brain network discovery. By imposing task paradigm information, the discovered brain networks are able to match better with the RSN templates. However, there are a couple of comments I would like to make. It is a bit confusing to show fig 2 as the authors mentioned that DBN has lower loss but the authors mentioned that the model can be overfitted. Did this happen to all the subjects? How did the author infer that the model were overfitted? In my understanding, the authors concluded the overfit by observing an early turning point of the mismatch loss in PKG-DBN. However, could the authors plot similar task paradigm mismatch plot for the DBN model? How were the hyperparameters chosen in the experiment? Similar as I wrote above in the weakness. As we already have access to the task labels, why don't we switch to the supervised method entirely to estimate the brain networks?	In this paper, authors propose a novel prior knowledge guided DBN (PKG-DBN) model hierarchical brain network decomposition. Specifically, they enforce part of the time courses learnt from DBN to be task-related (in either positive or negative way) and the rest to be linear combinations of task-related components. By incorporating such constraints in the learning process, our method can simultaneously leverage the advantages of data-driven approaches and the prior knowledge of task design. In general, it is novel and interesting. The proposed points are well supported by the results. I only have a few concerns/comments. 1) A few Figure resolution is relatively low, such as Fig.2. 2) The model parameters should be clearer. 3) The language need to be improved with native English speaker. There are a few language errors in current version.	This work proposed a novel method to incorporate prior knowledge to DBN for hierarchical brain network decomposition. Sufficient experiments suggest the proposed method can converge faster than the traditional DBN models. It is a novel method for fMRI analysis field. However, there are still a few unclear descriptions and typo errors which need to be further improved.	The advantage of the framework is not very clear to me.	This work propose a novel prior knowledge guided DBN (PKG-DBN) model hierarchical brain network decomposition. This is the first supervised hierarchical FBN identification method as far as I know. It is important to fMRI analysis field.
243-Paper2461	Histogram-based unsupervised domain adaptation for medical image classification	This paper describes a domain adaptation method base on histograms rather than images and applies it to disease detection from chest x-ray images.	In this paper, the authors work on the hypothesis that most domain shifts in medical images are variations of global intensity changes which can be captured by transforming histograms along with individual pixel intensities.	This paper propses a simple and straightforward yet effective for the unsupervised domian adaptation for medical images, which only operate on the high level of the input images via learnable gama transformation. The proposed method also achieves the best results compared with the most poplar image translation based methods.	The most important aspect is that the results show an improvement over baseline only for the proposed methods. Additionally, the paper is well written; it explains the underlying idea well and explains how it is implemented.	The method may work.	The proposed method is very straightforward and effective, which can be utilized as a general method for another medical image applications. The motivation and the method part is well explained. The experiments are sufficient to show the robustness and the effectiveness of the proposed method.	Table 1 is very confusing. That needs major work. For example, the training error is shown in the first few lines for each experiment, but that isn't explicitly explained. Also, what exactly is the baseline method for each experiment? Finally, please add p-values for each row. Re-do Table 1 and add a better caption that explains what exactly is being classified with the AUC.	Experiments are insufficient and the motivation is unclear.	No obvious weakness.	The paper appears to be reproducible.  The most important component will be code in pytorch and tensorflow code for the histogram-layer. The paper doesn't tell which code libraries are used, so it is hard to determine what the implementation will be without code.	Difficult to reproduce.	No abvious issue of the reproducibility as the author claimed.	This paper is a good paper but I would try to improve Table 1 a lot and also the grammar could use some more editing. Otherwise good job.	Experiments are insufficient and the motivation is unclear. The convergence of the model should be demonstrated to help analyze performance. Experimental details should also be supplemented.	Motivation and intuition is clear. The paper writing is good.	Great idea and results to back it up.	Many details are lost.	The simple, effective and general method for medical image domain adaptation are the key facts.
244-Paper2095	How Much to Aggregate: Learning Adaptive Node-wise Scales on Graphs for Brain Networks	A novel dynamic aggregation mechanism for graph convolutional networks, exploiting Heat Kernel equation.	In this paper, the authors proposes a flexible GCN model that learns adaptive scales of neighborhood for individual nodes of a graph to incorporate broader information from appropriate range. Extensive experiments on various datatsets show that the proposed method outperforms the state-of-the-arts methods.	This paper proposes a flexible model that learns adaptive scales of the neighborhood for individual nodes of a graph to incorporate broader information from the appropriate range. The authors derive a parameterized formulation to perform gradient-based learning on the local receptive field of nodes using a diffusion kernel.	The comparison with already existing solution is very meticulous and the idea is original, based on a very good literature review. The story of heat kernels and GNN is really reported in a compelling manner.	(1) The paper focuses on how to aggregate the information for the brain networks adaptively. In this paper, the authors proposes a flexible GCN model that learns adaptive scales of neighborhood for individual nodes of a graph to incorporate broader information from appropriate range. The paper is well written and the problem is clearly motivated. (2) The authors also derive a gradient-based learning on local receptive field of nodes using a diffusion kernel. Extensive experiments on various datatsets show that the proposed method outperforms the state-of-the-arts methods.	This paper proposes the adaptive range of individual nodes. This idea is consistent with the property of the brain network that each ROI has different biological topological properties, thus different local receptive fields should be provided to understand the subnetwork structure. The training on the scale is well derived and the paper is well organized. Some interpretations such as key ROIs are provided with visualization.	Writing can be improved The proposed method is interesting though it seems just an incremental improvement on the GrapHeat (Xu et al.  2019)	(1) In the paper, the authors emphasize the proposed model can achieve the aggregate the information adaptively as a contribution. How does it reflect the adaptively of the proposed method in the paper?  What does the s in Eq. (6) mean?  It seems that in the paper the only contribution is to make the scalars become trainable. My concern is that this operation can be whether achieve the  Node-wise Adaptive Scales. (2) I do not understand the reason why you give the derivative of the loss? As far as I am concerned, the loss in the paper can achieve by back propagation. The Eq.10 to Eq.15 are the formulation for the back propagation. Do exist some constraints that make it difficult to perform the backpropagation, such as the discrete domain optimization? I do not find the constraints.  Compared with the [1], the only difference for the paper is to make the scale s in Eq6. become learnable. [1]. Xu, B., Shen, H., Cao, Q., Cen, K., Cheng, X.: Graph convolutional networks using heat kernel for semi-supervised learning. In: International Joint Conference on Artificial Intelligence. Macao, China (2019)	No need to provide detailed formulas of graph convolution or GNNs in the preliminaries. This paper considers only one dataset. More extensive datasets that involve different modalities should be added into consideration.	The dataset is a well known public dataset, the data selection and use is clear. Crucial details on the preprocessing are missing. Please clarify the PET normalization step.	According to the reproducibility checklist and information from the paper, the results should be reproducible after the potential acceptance.	This paper does not provide code. The only experimented dataset ADNI is restrictively available.	"The major concern is the novelty. Despite the contribution is elaborated and interesting, it seems a small improvement of the graph-heat method. Please be specific which Amyloid tracer (there are several in the ADNI dataset), also which ADNI group? All? The results using the cortical thickness are a bit puzzling. From Fig.3 it seems you only used AD patients at the baseline (correct?) , yet the results in Table 2 are relatively good for the models even if not good when using other features. This is surprising as at baseline in the ADNI dataset, AD/Control/MCI are not so different except for the cingulum, hippocampus and some areas. Old works on voxel-based morphometry and the ADNI dataset had a lot of difficulties in this. It is quite impressive that works so well even with SVM. So, is the cortical thickness discriminant or not? The normalization with the cerebellum for amyloid-PET is known, though depending how it is done it makes a lot of difference. Subtracting the mean value? the max value? Z-score between healthy subjects and each AD subjects? Moreover, it is surprising that degree as features performs so badly with SVM. There are other studies even using t-test and simple features as degree, centrality, etc which says the opposite (e.g. Elsheikh et al. Front. Hum. Neurosci. 2021). Can you please check how you use the degree? As an average degree? Kept as local features? Minor: The style of the paper is sometimes clumsy and presents many typos, majorly due to hurry or distraction, or long sentences (e.g. page 2 ""...Graph Diffusion Convolution (GDC) [14], however,...""  .  Furthermore, the figure 2 has an imprecision in the left image (scale s1 is not as wide as s2, despite the caption tells the opposite). Some symbols are used for different things (I.e index I used both for the convolution layer in eq. 8 and the sample Y in the eq. 7). The performance listed in table 2 is not particularly sharp (in some case the SVM is even better, as in ""All Imaging Features"" section, or the ""GDC"" generally is very close), etc"	Please refer to the strengths and weaknesses of the paper.	Some as the weakness.	The novelty of the idea is acceptable but not outstanding, the technical realization of the work is scrupulous but the written elaborate is not made with the same attention. The final results are tepid (they might be some tuning or not really significantly different from GraphHeat). It might be a more interesting case for generative problems (e.g. Graph-GANs rather than just classification).	The method part. I will raise the score if the author can address my concerns.	The novelty of the adaptive scales for GNN aggregation. Also, this idea is generic and not restricted to brain networks.
245-Paper0053	Hybrid Graph Transformer for Tissue Microstructure Estimation with Undersampled Diffusion MRI Data	This paper introduced a hybrid graph transformer (HGT) method to estimate tissue microstructure measures by using a graph neural network (GNN) for q-space modeling and a residual dense transformer (RDT) for spatial modeling. The proposed method was compared with several state-of-the-art methods and showed better performance for estimating NODDI parameters with reduced samples. Moreover, an extensive ablation study was performed to examine the effectiveness of the learning modules.	Deep learning techniques allow prediction of high-quality diffusion microstructural indices from sparsely sampled data. Existing methods are either agnostic to the data geometry in the q-space or limited to leveraging information from only local neighborhoods in the spatial domain. This paper proposes a hybrid graph transformer (HGT) that combines a graph neural network (GNN) and a novel residual dense transformer (RDT), so that the q-space geometric structure and spatial information are fully exploited. Experiments were performed on the HCP dataset for evaluation, where the proposed method has achieved promising results. Overall, this is an interesting paper with proper validation.	This paper proposed a deep learning approach for estimation of tissue microstructure parameters from sparse diffusion MRI data.  They specifically investigate a hybrid graph transformer network that uses both q-space and spatial information.	1) The HGT method includes modules for both q-space and x-space learning that is very suitable for modeling diffusion MRI and different from previous methods. 2) This work is claimed to be the first one on leveraging transformers for tissue microstructure estimation. 3) Extensive comparison with several SOTA methods and the ablation study make the results convincing	1) The idea of jointly exploiting the data structure in the q-space and the nonlocal information in the spatial domain is novel. Previous works have not considered the joint use of these different types of information. 2) The selection of competing methods is quite comprehensive. 3) An ablation study was performed to justify the use of residual learning and dense connections.	The paper is well written and looks at an important topic.  The technical aspects of the method are novel and clearly defined.  The experiments are expansive in their evaluation across different approaches.	None.	The threshold of theta appears a bit arbitrary, and it is not clear how sensitive the proposed method is to the threshold.	The experiments focus exclusively on NODDI parameters, which is perhaps something to note earlier.  It is not clear how this might generalize to more complex models, e.g. axon diameter or soma density mapping.	This work showed	The data used in this work is publicly available, but the authors have not indicated whether their method will be made publicly available.	The experiments are described in sufficient detail to be reproduced.	1) The presentation in 2.3 mainly focuses on the mathematical aspect of the RDT module. Adding more explanations about the difference between RDT and standard Unet and the meaning of the notations in terms of imaging parameters will be helpful. 2) More details about the definition of \theta is needed. It is not clear how symmetric q-vectors with respect to the origin and the norm of the q-vector are considered in the definition.	The authors may better clarify what the new contributions in their network design are. Is it the new combination of existing building blocks or are these building blocks also novel? The authors have not mentioned how the threshold of theta was determined. Information about standard deviations can be added in Tables 1 and 2. The authors may consider application of the proposed method to other datasets in the future, especially those with patients.	One thing to consider, when evaluating accuracy of multi-compartment models, you may want to consider the underlying tissue/csf volume fraction.  When a voxel is mostly CSF, the ICVF value has little effect of the model fit, so in my opinion, it's not quite fair to penalize that.  You can see in Fig. 2 row 3 that the largest error values are in the ventricles, where ICVF is not very meaningful.  One thought is to use a weighted average with weighting by (1 - Fiso) when summarizing error across the image.	The proposed work is claimed to be the first one on using transformers for tissue microstructure estimation using diffusion MRI and shows better results than several state-of-the-art methods.	I believe that this work proposes a novel method, and its effectiveness has been properly demonstrated. Although there are some minor issues, I recommend acceptance.	The paper and underlying work are skillfully executed and a useful addition to the literature.
246-Paper1120	Hybrid Spatio-Temporal Transformer Network for Predicting Ischemic Stroke Lesion Outcomes from 4D CT Perfusion Imaging	In this paper, the authors propose to predict acute ischemic stroke (AIS) lesions 2-7 days after the stroke onset from Spatio-temporal Computed Tomography Perfusion (CTP). Instead of using estimated perfusion maps, the authors utilize the raw temporal CTP acquisitions, by proposing a new hybrid Convolutional Neural Network (CNN) and Transformer model. CNN encoders extract features from each time step, then the Transformer learns the temporal relations, and finally, a CNN decoder estimates the final lesion. The method is evaluated in an in-house dataset and improves over perfusion maps-based CNN and CNN-based temporal methods.	The authors proposed to use spatio-temporal transformer to predict stroke lesion outcomes directly using 4D CTP images as input.	This paper proposes to segment the ischemic stroke lesions directly from 4D CT perfusion images, rather than from perfusion parameter maps. They used transformer for fusing the temporal information and use a CNN to extract spatial features.	The text is well-written and easy to follow. In particular, this Reviewer appreciated the Introduction, with a literature overview and motivations. Hybrid CNN-Transformer models [1] were previously proposed, but, to the best of the knowledge of the Reviewer, this particular model and its application to CTP is novel. The use of Transformers for this purpose, as proposed by the authors, seems capable of leveraging the best of these models, i.e., learning temporal relationships in CTP. The results are positive, improving over CNN-based naive baselines with perfusion maps, or even considering the temporal component.	The usage of transformer on this specific problem can be considered novel. The manuscript is well written.	1, Using 3D CT perfusion images for segmentation has not been well studied in the field. This topic is some kind of new for the community.  2, The paper is smoothly written and easy to understand.  3, The authors successfully showed that the proposed method outperformed segmentation from perfusion parameter maps.	The authors propose to have a CNN-based encoder to process each of the input time steps of the CTP. This comes with a computational load increase that is linear with the number of steps. Therefore, it poses limitations in terms of application, but also on the capacity of the CNN encoder. A major weakness is the in-house dataset used to evaluate the proposed method. While the authors will release the code, it is hard for other methods to compare against the proposed work. Evaluation raises some concerns. 1) while using 10-fold cross-validation can be good, it leads to the assumption that results are averaged from the fold used for validation, which may be overly optimistic. In other words, a separate test set is missing. 2) the authors crop the images such that only the brain hemisphere with the stroke is maintained, which may lead to overly optimistic results (artificially reducing False Positives), and being impractical in a real setting, where the annotations are not available to determine the hemisphere.	I found no major weaknesses.	"1, Method in the novelty seems to be limited. The structure of the encoder and decoder are based existing works, and the authors also directly used the transformer for the outputs  of the encoder. This makes me feel that the method is a simple combination of these existing methods.  2, The experiments are not sufficient. The authors only compared their method with TCN and U-Net.  3, Some important details of the method are not provided. The authors claimed that ""full details of the proposed architecture can be found in the supplementary material"". However, I only see comparisons of model sizes and runtime in the supplementary material."	The method is well described, and the authors promise to release the code. Therefore, implementation-wise, the proposed work should be reproducible. The main concern is related to the dataset, which is in-house. In this sense, it is not possible to reproduce the final results. The Reviewer wonders if it is possible to release at least a small set of images, together with results and manual annotations. In summary, it should be possible to implement the proposed method adequately, but the final results cannot be reproduced.	The authors state that code will be released. The in-house data is unfortunately not available; it might be difficult to reproduce the presented results.	The authors missed some important details of the method, which hinders the reproducibility of the paper. However, they promised to release the code.	"General In this paper, the authors tackle AIS lesion prediction directly from CTP raw images, i.e., without the conventional way of estimating perfusion maps. This is an important topic since such maps can discard information that may be learnable from a data-driven approach. The novelty of the work lies in the methodological approach, by using CNN and Transformers to extract and combine features from the multiple time-steps, and its application to CTP and AIS lesion prediction. The Reviewer considers the method novel, but there are concerns related to the evaluation. Please, check the detailed comments below. Comments/questions to the authors (not in order of importance) 1) How many transformer blocks are used in the model? Could the authors clarify it, please? Also, it would be interesting to see ablation studies where this number is changed. That would also inform about how much we can gain from further temporal context aggregation. 2) The proposed method is evaluated in an in-house dataset, which makes it hard for future work to be built upon or to compare results. There are public datasets that could be used, for example, ISLES 2017 [2] provides MRI perfusion data with the raw temporal acquisitions, and it could be used to compare with SotA. While it is not CTP, the proposed method should be possible to be applied in ISLES 2017 without modifications. Could the authors comment on why such a dataset was not used, please?  a) Moreover, ISLES 2017 has a hidden test set, which would allow mitigating some of the evaluation concerns mentioned next.  b) The authors state ""sets a new state-of-the-art for lesion outcome prediction from raw 4D CTP data"". But, since the data is not publicly available, such a claim cannot be challenged or claimed so strongly. 3) The proposed work raises some concerns about the evaluation procedure.  a) The authors use 10-fold cross-validation for evaluating the model. This is a valid practice, given the size of the dataset. However, the Reviewer assumes that the reported metrics are the average of the validation fold of each run. In this case, results may be overly optimistic, since there may exist the risk of overfitting. Again, this is a valid method to find hyper-parameters, but having an external test set would be desirable.  b) The authors crop the images such that only the brain hemisphere with stroke is kept, which poses 2 main issues. First, results may be overly optimistic because some potential regions of False Positive detections are removed before analysis. Second, it does not represent a fully automatic real-world scenario, where the affected hemisphere of the brain is not known since manual annotations do not exist.  c) The authors detect a strong statistical difference in the results in terms of DSC when comparing with baselines. But, none to small difference exists when we consider volume. This is of course possible, but it would be interesting to see more discussion on why this is the case. 4) The authors cite [3], but do not compare with. This work was proposed in the context of MRI perfusion but seems possible to be used for CTP, or at least partially. One of the contributions of [3] is to model the temporal aspects of the raw perfusion images as channels of CNN. This would be a simple, yet, valid CNN-only model to compare against. Further comments (suggestions/extra comments on future work) - NOT intended to be addressed during rebuttal The method considers 2D slices of the CTP, which neglects the 3D nature of the images. It is understandable, however, that it is computationally demanding. Indeed, the authors will try to address it in future work. Besides the proposed future direction, it may be worth taking a look at methods that decrease the computational load of the 3D CNN, too, such as [4]. This is more of a detail. The authors wrote, ""paired t-test with p<0.05"". The p-value is indeed used to check the statistical significance, but such a threshold is referred to as significance level, usually defined as alpha. The Reviewer believes that the proposed method is interesting. Still, it would be good to see an extended version of the work that considers more validation, more datasets (including public ones), and a comparison with more SotA methods. References [1] Carion, Nicolas, et al. ""End-to-end object detection with transformers."" European conference on computer vision. Springer, Cham, 2020. [2] http://www.isles-challenge.org/ISLES2017/ [3] Pinto, Adriano, et al. ""Enhancing clinical MRI perfusion maps with data-driven maps of complementary nature for lesion outcome prediction."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2018. [4] He, Junjun, et al. ""Group Shift Pointwise Convolution for Volumetric Medical Image Segmentation."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021."	For fair comparison between methods, I believe all methods should chose their own optimal training settings, e.g., epochs, learning rate, batch size, rather than fixed for all methods as presented in this manuscript. When it comes to spatio-temporal learning, commonly used approaches include the well-known LSTM/GRU, nowadays ConvLSTM and ConvGRU. I wonder why the authors opted for transformer over ConvLSTM/ConvGRU in the first place? As the transform is the main technical contribution, I am curious how this would compare with ConvLSTM/ConvGRU, which is not presented in this paper.	1, Some details should be provided: after the CNN encoder, what is the feature map size for each slice and how are they sent to the transformer and decoder? Let's say there are T slices (at T time points) that lead to T feature maps, the output of the transformer will lead to T feature maps as well. How to combine the T feature maps obtained by the transformer with the decoder to obtain a single segmentation? 2, I think the comparison of the model sizes and average runtime do not take much space and they can be put in to the manuscript.  3, They authors should consider to compare the fusion strategy with some other methods. The transformer here is proposed for fusion the information from the T slices. Is there any alternatives for this purpose? Some comparisons are necessary.  4,In figure 3, it seems that the images had been skull-striped and cropped. However, such preprocessing was not described in the manuscript.  5, Does it really necessary to use the raw 4D CT perfusion images as input? What happens if using transformer-based network for the perfusion parameter maps?	The proposed work has technical novelty. The hybrid CNN-Transformer and application to AIS from CTP raw images are novelties. However, the evaluation raises concerns: 1) the data is not public, 2) results may be overly optimistic due to 10-fold cross-validation, 3) the authors crop the brain hemisphere of interest before analysis. Still, the MICCAI community could benefit from this work.	The usage of transformer in this application is somewhat novel with outperforming results, although I still suggest to justify the choice of transformer over ConvLSTM and ConvGRU.	I would like to reject this paper due to its limited novelty of the method and insufficient experiments.
247-Paper1527	Ideal Midsagittal Plane Detection using Deep Hough Plane Network for Brain Surgical Planning	This paper deals with automated detection of the midsagittal plane (MSP) for brain surgical planning. The MSP bisects the human brain into two cerebral hemispheres. The detection of optimal MSP is based on a deep Hough plane network which combines the idea of Hough Transform based object detection with deep convolutional neural networks.	In the sagittal plane localization task of 3DCT human brain image: Make full use of image space and Hough space features to locate the sagittal plane. The use of DHT increases feature sparsity and reduces computational cost.	Following are the contribution of the paper: Application of deep hough transform (DHT) and inverse deep hough transform (IDHT) which simplifies the plane detection problem into image space. Combination of Hough transform along with CNN features to detect the MSP properly.	The main strengths of the paper is the interesting methological approach combining hough transform and deep learning. In this way, a nice framework is being created which could also be of interest for other applications. Another strength is the evaluation on a decent number of 519 cases providing first indication of clinical usibility.	The method proposed in this paper is novel: combining two kinds of spatial characteristic information into training can accomplish tasks more efficiently and accurately. This paper has high practical value: based on real clinical data sets, it has good training performance.	Following are the strength of paper: Introduction of sparse DHT, which increases the sparsity of features, which further increases the computation speed. Introduction of hough pyramid attention network to extract non local features, which increases the detection. Introduction of dual space supervision, which integrates the training loss from both the spaces (image and hough).	Reproducibility is unfortunately rather moderate (if at all). Details on runtime and memory consumption are missing although the method is being claimed to be beneficial from that perspective. For the comparison against other approaches, it remains unclear if those are based on available implementation or re-implementation. Of course, in the latter it is uncertain if alternative methods are implemented correctly and optimally tweaked. In general, more justification should be provided why proposed method significantly outperforms SOA.	There is no reasonable explanation for the selection of some parameters. The rationality of the method description is slightly inadequate. The number of comparative tests is insufficient, and the comparison method selected is not new enough. The types of experiments are not rich enough, lacking some ablation experiments. Insufficient references. The summary part is less.	The work is interesting. I have no doubt on the novelty of the algorithm, but I am concerned about the application of the work. Detection of MSP looks good, the strength of the paper would increase if the midline shift could be calculated. In the Figure 1, the axial brain CT slices, the ideal midline or ideal MSP is detected, it would be better if the deformed or the midline offset is also detected. After detection of ideal and offset midline, the midline shift could be calculated, which is one of the important parameter in predicting the severity of the brain.  It would be better if midline shift is calculated, and a better validation would be the error in midline shift estimation.	Unfortunately, implementation of the method is not given. From that perspective, reimplementation would be needed. Provided level of detail is however limited and consequently reproducibility is only moderate.	The network architecture is clear and the algorithm logic is rigorous. Only in the experimental part is not obvious, the comparison test is not full; Clinical needs studies based on real data sets; It has relatively good reproducibility.	The paper is reproducible.	"In general the paper is nicely written and easy to follow. It would be great to share code and allow others to reproduce results and apply method to other use cases. The comparison to state of the art would require more justification. From me, the conclusion can often not be fully understood, e.g., ""Consequently, they commonly have the disadvantages of low efficiency, poor accuracy, [...], leading to unsatisfactory and suboptimal results."" However, some of the approaches give for their application promising results and not so sure if I agree with such conclusion. Another example is ""In addition, insufficient number of landmarks will also make the plane fitting unstable, which ultimately leads to the poor robustness and accuracy of the plane detection based on the location of landmarks."" Again, I can not fully follow the argumentation as a plane fit does not require a lot landmarks and I would argue that usually enough 3D landmarks can be detected. At the end, ground truths is extracted from annotated landmarks. Figure 4 deserves further explanation. How have the cases been selected? What can be observed in detail? How can the difference be explained? Very nice to split the data in different levels of pathology (>5mm and < (=?) 5mm)"	"P1 Abstract: For Abstract, data types and forms are not mentioned, which can be appropriately supplemented. Let the reader have a more comprehensive understanding of the work of the paper. P2 line1-7: It's not that the explanation for ""manually delineated guideline"" isn't clear. The specific way of manual labeling leads to bad clinical effects. I think this part of the explanation can be modified to give readers a more intuitive feeling. P2 Fig.1. (a): What is the specific impact of this deviation on the operation. P5 Fig.3. line2-4: There is no specific explanation for this part. Can you use a small amount of space or formula to show it in the text? P5 2.2 (2): Such voting process can greatly improve the training efficiency, but will it affect the training effect to some extent if all pixels with a space median value of 0 are erased? Is it possible to add comparative tests to verify that the training efficiency can be improved while ensuring the improvement of recognition accuracy? P6 2.4 line1-2: Can you give a relatively specific explanation, or add paper citations with corresponding explanations? P6 2.4 l: Can you explain the rationality of setting 0.1? Or add experiments to illustrate. P7 line8-10: The relationship between GT and Landmark is not clearly expressed. P7 line11 3:1:1: The proportion of test sets is relatively small. P7 3.2 Table1: The comparative tests are a little sparse. The latest comparison is 2019. Is there an updated method to compare with the method proposed in this chapter? P9 References: There are relatively few references."	The paper is well written and understood. It would be better that if the authors would detect the deformed beizer curve and estimate the midline shift. The midline shift is an improtant predictor for predicting the sevearity of the brain. The application of hough transform is interesting to see, but tracing the deformed curve is equally important.	Certainly interesting paper with nice methodological contribution that could be relevant for other applications. Unfortunately, reproducibility is rather moderate. It is less transparent why improved results over state of the art are achieved.	The paper is logical and precise, and the research content has certain clinical value. It has some bright spots, and really improves the effectiveness of the work in this task area. But there are also some problems such as unclear description, lack of explanation and inadequate experiment.	I would recommend it weak accept as the midline shift estimate is not done.  With the estimation of midline shift I would recommend it strong accept.
248-Paper1790	Identification of vascular cognitive impairment in adult moyamoya disease via integrated graph convolutional network	In this paper, the authors intend to propose a novel graph-based method of diagnosing VCI using two modalities (fMRI and DTI). The main contribution in methodology is two-fold: one is the dual-modal GCN to firstly process the two-modality images independently (graph construction and convolution) and fuses them together for the diagnosis, the other is the node-based normalization and constrain mechanism to resolve the over-smoothing issue in GCN and incorporation of the non-imaging information. Experiments such as ablation studies, SOTA comparisons and biomarker interpretations also demonstrate the effectiveness of their works.	This paper proposed a dual-modal GCN framwork to integrate imaging and nonimaging information for VCI identification in adult MMDs.	This paper designs different ways to extract complementary information from rs-fMRI and DTI when constructing graphs, which maximizes the utilization of characteristics of different modalities. Node-based normalization and similarity constraint item are proposed to improve performance by solving the problem of over-smoothing and integrating non-imaging information, respectively.  3) Some salient biomarkers for VCI identification are selected by introducing self-attention pooling mechanism which combines node features and graph topology, showing the clinical value of the proposed model.	The methodology of dual-modal GCN is reasonable. The motivation for designing the node-based normalization and constraint is sound, although the design of such mechanisms is not quite clear to me	This paper design different ways to extract complementary information from rs-fMRI and DTI when constructing graphs. The personal biomarker interpretation is interesting.	This paper tackles the recognition of VCI in MMD by an integrated GCN. By constructing graphs that extract complementary information from two modalities that contain the corresponding important information, node-based normalization and similarity constraint item are combined to improve performance. The results from the private dataset demonstrate this method can highlight some salient brain regions related to VCI in adult MMD while achieving high accuracy.	The proposed method is quite a general graph-based tool, which is not specifically designed for VCI diagnosis, and to resolve the potential issues in this research topic.  The implementation of the node-based constraint item is confusing and should be further explained.  Experiments were not conducted in cross-validation, so the validity of the results is questionable, and training number is also limited to construct the proposed model in a proper way.	The process of the graph construction is not clear in this paper. The author should use the cross-validation method and repeat the experiment some times to finally obtain the generalization ability of the proposed method.	The dataset is not sufficiently described.	It seems that the authors have provided the necessary information to reproduce the methods, but it is still strongly suggested to publish the source code online if available, which can help the readers to fully understand its mechanism.	The reproducibility of the paper is limited.	The dataset is not sufficiently described.	The major issue in the methodology section, is its descriptions of the node-based constraint item, especially the similarity constraint mechanism. The motivation to design the fusion of imaging and non-imaging features in such manner (using imaging feature as node and the similarity between two non-imaging features as edge) is not clear to me, as you can just simply concatenate them together for constructing the diagnosis model, which can also be effective and can fully utilize the two features. The computation of loss2 is also missing, which makes it difficult for readers to reproduce this work. Table 1 shows the ablation study, where the last line seems to be the results of the proposed method, but it is unclear what the meaning is for that (1) and (2), which is also not clearly stated in the methodology and experimental sections. Therefore it remains confusing to me why the last line can achieve such a high performance compared with the others. Also, in some lines the performances from the testing set seem to be better than those from the validation set, which is also against my expectation, and it is suggested that the authors explain more about this. The authors claim that they use 156 subjects for training, which might be sufficient for some conventional machine learning methods such as SVM, but definitely not for GCN. Limited training data will result in overfitting of the model training, which can hinder the credibility of the proposed method. To solve this problem, the usual way is data augmentation, from which there are already some ways available in this field. I am wondering if the authors realized such issues and if they have any solutions to them. In Section 3.3, the authors provide the biomarker interpretations, from which they found some ROIs which are highlighted for VCI diagnosis, but it remains unknown if such findings are valid when there are no related references included in this paper to support them.	1)At the beginning of the abstract part, it is suggested that the author briefly describe in one sentence what the existing methods for VCI research are usually and what are the shortcomings, and then lead to the research of this paper. 2)In the second paragraph of the introduction, the research on VCI using rs-fMRI and DTI and their shortcomings are introduced respectively. What is the traditional research on multi-modality data fusion, how to fuse multi-modality data, and what are the similarities and differences between the research in this paper and the previous multi-modal fusion research? The author can briefly explain. 3) In the third paragraph of the introduction, what is the traditional multi-modality method of the graph, and what are the advantages of the model proposed in this paper. The author can briefly explain. 4) In Section 2.2, graph construction based on rs-fMRI and DTI, the authors should indicate whether a fully connected graph or a sparse graph is used. In addition, whether fully connected graph or sparse graph should be reflected in Fig. 1 and Fig. 2. Besides, it should be stated whether a uniform graph structure was used for all subjects, or different graph structures were used between subjects. The authors can briefly explain how the FN matrix is calculated. 5) In section 2.3, how the input and output dimensions of each layer change, the author should give a brief description. 6) In the experimental parts of 3.1 and 3.2, because the sample size is small, the experimental results obtained by only dividing the training set, the validation set, and the test set cannot explain the performance of the model. And please clarify the usage of validation set in this work. More importantly, the author should use the cross-validation method and repeat the experiment some times to finally obtain the generalization ability of the proposed method. 7) In Section 3.3, please supplement the explanation of how the self-attention pooling mechanism preserves the important brain regions of the subjects.	The description of the method section is not clear.	There are some unclear parts in the methodology section, and the explanations to the experimental results are also missing. The rest of the paper seem to be OK to me, there are some contribution and novelties in this paper but are not quite significant to me.	The theoretical novelty of the paper is limited. The experiment strategy is not rigorous.	The paper is novel in method, clearly organized and full of results.
249-Paper1279	Identify Consistent Imaging Genomic Biomarkers for Characterizing the Survival-associated Interactions between Tumor-infiltrating Lymphocytes and Tumors	The paper presents a multi-modal framework that fuses the interactions between TILs and tumors with a graph attention network and the genomic data by concrete autoencoders for prognosis predictions of breast cancer.	This paper analyses the role of immune cells, lymphocytes, in their role in survival of cases with tumours	Fusing imaging features with the genomic features has been used for the prognosis prediction of breast cancer in existing previous works. The contribution of this work is to use an attention mechanism for fusing imaging features with the genomic features.	The graph attention network can characterize the interaction between TILs and tumors by weighting graph edges. The experimental design is comprehensive, especially integrating several scenarios using imaging only and using genomic data only. The correlation between particular genes from the concrete autoencoders and TIL and tumors is demonstrated.	The paper proposes a new methodology that outperforms 9 other methodologies, this is a good performance.	The proposed approach is well motivated. Experiments are carefully done. Multiple neural networks such as U++, graph attention network, concrete autoencoder, and co-expression network, are used to process and combine imaging and genomic data. For that it became an end-to-end deep learning based framework. Clear analysis of the results are done. Comparisons with existing approaches are done.	Why is clinical or demographic info not integrated into the framework? Please check the rest of the minor comments in section 8.	The paper could be much better written. See comments below.	Nothing	Yes.	Public data and adequate description of the methodology	Datasets and experimental setup are clearly mentioned.	What is the magnification of the WSI? The meaning of gamma, lambda, t, and beta in ImQCM should be explained. Were there any duplicate patches across the two categories for the selected 200 patches with the largest tumor or TILs ratio? The red/blue number labels in Fig. 2(b) patient A seem to be inverted. The black bounding box in Fig. 2(b) Patient B appears off.	"The paper is fairly well-written and the few suggestions that I have a on style and clarity: 1) The font in the figures is TINY! I suggest that the authors print one of their figures in Letter or A4 and try to read the figures. I have a 27 in monitor and had to maximise to be able to read. In some cases (fig 3) just making the figure larger would help, but it would be better to increase the font when the figures are produced. In Fig 1, the jet colormap for the 2 step is not good, better to change to greyscale so that the contrast between classes is clear. Again, try to print in a printer that is not colour to notice this would not print well. 2) The captions are just too short. Every caption should help to make the figure self-explanatory so that a reader can turn to the figure and understand it without having to refer back to the text, i.e. which are groups 1 and 2 in Fig 3? Add some insight in the caption, e.g., ""it should be noticed that ...""  3) With the tables, it is easy to get lost in the numbers. Please use bold to highlight the best results. Also use the caption to indicate which is the proposed method, this is normally added as the last one, especially since in the text all other methods are numbered 1-9, but in the table these appear in positions 2-10 4) The majority of the cases are censored, thus very few (10%) are used. What is the implication of discarding so many? What is the criteria to censor? 5) there are many typos and bad English (""patients, 46 of the them are non-censored patients..."")"	Two minor Comments: put space after commas and before '('. [e.g., in page7, methods(i.e.,OSCCA, DGM2FS,SALMON)] make labels in Fig. 2 and Fig. 3 clearer by bigger fonts.	It seems to be the first method that combines TILs information with genomic data for breast cancer using WSIs.	Good and interesting paper	Fusing imaging features with the genomic features by an attention mechanism is a novel part of this work. The proposed approach is well motivated. Experiments are carefully done. Multiple neural networks such as U++, graph attention network, concrete autoencoder, and co-expression network, are used to process and combine imaging and genomic data. For that it became an end-to-end deep learning based framework. Analysis of the results are clearly mentioned. Comparisons with existing approaches are done.
250-Paper1422	Identifying and Combating Bias in Segmentation Networks by leveraging multiple resolutions	This paper described the analysis for resolution-bias in the segmentation network and explored the way to reduce the bias. In order to analyze the problem of the limitation of the segmentation network performance due to the resolution of the training data, a comparison experiment was performed using four approaches. This paper shows that the single resolution network fails to generalize across resolutions, but scale augmentation and network with resolution independence structure are helpful to reduce the bias.	This work illustrates how the resolution bias in the data distribution propagates to the output prediction. Their compare how different strategies such as input resampling, scaling augmentation perform in comparison with resolution-aware architectures, and demonstrate that the later approaches reduce such bias more effectively.	The authors present an analysis of volumetric bias from networks trained on image data acquired at one resolution influencing segmentation results on data acquired at a second resolution. To measure the volume bias, a normalized volume difference is computed between the ground truth segmentation and the predicted segmentation mask. For models and data, the authors test Unets, and voxel size independent neural networks (VNNs) trained with a variety of resampling on MRI images acquired from adults and children to provide a resolution challenge, and hippocampal volume segmentation. Volume bias is examined by looking at the distribution of volumes segmented, Dice similarity, and the volume bias of the predictions. Overall, the authors found that building scale-invariant networks, or using resolution augmentation can reduce volume biases.	Experiments and analysis have been done well on the direction of the problem and solution caused by the resolution bias. The contents of the experiment and the analysis also seem appropriate. This work is expected to provide useful information in the field of deep learning-based segmentation research.	Strengths This paper explores a very relevant problem of image resolution bias in DL segmentation Usage of publicly available datasets and assessment on two different tasks (cortical segmentation in adults and children and hippocampus segmentation)	The major strength of this paper is using the volume bias to check for the generalizability of trained models from data acquired at one resolution to data acquired at another. This metric provides a better method of investigating differences resulting from scale variance, and can highlight which augmentation and network setup leads to the least bias.	For further analysis, two additional approaches (drop Group H, and downscale Group H) could be helpful to examine the systemic bias due to image resolution.	Unclear value of the down-stream task classification Statistical significance of the results 2.5D approach	The methods presented are not novel and have been used before. The models used are all presented in other papers cited by the authors. The use of a volumetric bias in adolescent and hippocampal brain segmentation evaluation has been performed before by Herten et al. in their 2018 paper accuracy and bias of automatic hippocampal segmentation in children and adolescents.	Not applicable	The statement of the authors for reproducibility does not correspond with the paper. There is no mention of code availability, nor information of time/cost for training. Neither hyper parameter optimisation setting. It does not seem that this study can be reproduced due to all this lack of details. Methods are very vaguely described.	The author's results should be easily reproducible as their models are taken from other papers, and their datasets are accessible elsewhere. To improve reproducibility the authors should make the images in each of their splits available.	Additional experiments (drop Group H, and downscale Group H) could be helpful to examine the systemic bias due to image resolution.	The authors illustrate the problem of image resolution bias in two different tasks. I've found the paper extremely well written, and portrays an important message for the MIC community. The paper though lack of details for methods and imaging setting used. My concert regards this work relates to the first experiment. Which is the rationale of mixing adult and children's data for cortical segmentation? It is not specificized which image contrast is used for that task. Children of below 9 months may have quite different MR contrast (T1 and T2 are swapped). Why not explore the issue on either children or adults only? I think there is an error in the 80 scans split (adults children) as 50/10/30 is 90? On the top of scale augmentations, which other augmentations were used in the scenario? Gaussian noise? Which was the exact interpolation used for method c) ? Would super-resolution techniques instead of interpolation do a better job? I acknowledge the value of adding an additional downstream task. But is not clear to me the rationale for the task in classifying adult vs children based on cortical GM. Were those measures normalized from intra-craneal volume? It would have been more interesting to explore this task on patients vs healthy controls population for instance (AD vs controls). I did not understood why adults overall seems to not be affected by the traiing/network strategy in the results, this resolution bias is then just and issue according to the targeted volume to be segmented? I do not understand how in the classification task Table 1, first column, suddently classification improves up to 0.89 at the lowest resolution. But overall I've found this task not so pertinent to explore how bias propagates further.	When training your models, I would have implemented early stopping to finish training rather than the fixed number of epochs the authors used. This could lead to either underfitting or overfitting. This is especially important as the authors used different architectures for their models. Would be beneficial to include visual results of the model performance (and bias on low-res data) It would be nice to assess the correlations between Dice, Hausdorff etc with volume bias There are several available open-source (SOTA) CNN-based hippocampal segmentation models, the paper and the field in general would strongly benefit from comparing them to your augmentation or VINN models. The figure labels are a bit crowded	Experiments and analysis have been done well.	Overall, i've found the paper and the topic very interesting and i think deserves to be discussed during the conference. However the lack of information on methodology and most importantly the choice of some experiments could be improved. I understand though the limited space.	The main reason for recommending this paper is that this application of existing bias measurement methods to scale-invariant models, and models trained with scale augmentation provides an interesting way of looking at systemic errors. Using volumetric bias to evaluate generalizability and awareness of systemic errors will vastly improve confidence in how generalizable a given model is.
251-Paper2056	Identifying Phenotypic Concepts Discriminating Molecular Breast Cancer Sub-Types	The authors propose a method to predict the molecular sub- types of breast cancer, and to identify the associated shared and discriminative visual traits from simultaneously acquired MRI and PET data.	This paper performed image classification on molecular sub-type classification for breast cancer patients. The proposed pipeline consisted of the classification using ResNet-18 pre-trained on ImageNet, the latent features were then extracted from the intermediate layers and clustered using k-means. Phenotypic Concepts were formed and weighting on these was calculated for each sub-type category.	This paper proposes an application of testing with Concept Activation Vectors (CAV) by Ghorbani et al., on breast cancer imaging, showing that the proposed methods provide interesting insights on visual features characterising breast cancer sub-types. The main contribution added to the state of the art method is the identification of patient-specific concepts in the latent space, rather than concepts that generalise to the entire pool of data.	The authors apply the TCAV method on a multi-modal training image dataset. This application is an important contribution towards interpretable identification of breast cancer sub-types. Evaluation of the classification accuracy is competitive with recent state of the art approaches, while using a smaller dataset, and takes advantage of the multi-parametric multi-modal data.	Strength of this paper lies in the pioneering application of concept calculation on medical images for breast cancer patients, which is one step closer to find the co-relation between phenotypic traits and microscopic categories.	The paper poses a relevant question in the field: can we identify phenotypes with non-invasive imaging and what features in the image may be associated to each phenotype? This is a relevant and timely question. The paper is well written, clearly exposed and the results present interesting insights. The formulation of patient-specific concepts is rather interesting, as it allows to describe each patient by a set of measurable imaging features and their respective values.	The related work section has limited reference to other literature in latent space clustering and TCAV and DTCAV implementations in medical imaging, e.g. Gamble et al; Clough et al.2019; Janik et al. 2021. It is not clear how the proposed methods differ from existing work.	Weakness of this paper is really minor and some typos can be corrected during the proofreading process.	There are a few limitations: I fear the novelty in the paper is only incremental, with little innovation proposed to further advance the work done by Ghorbani et al. I think the related work is not sufficiently up to date. The authors missed important works that are relevant and related to their analyses and mentioning these works would have made the paper more complete. More attention and more insights should have been given about the concept formulation, particularly about the contribution proposed in Eq. 1. This is the most important contribution of this work and I think that it was not sufficiently discussed. How do the concept footprints compare across patients? Are there some concepts that are shared among patients?	Yes, the methods are reproducible. The authors reference the ResNet-18 classification model that they train with a modified classification head to fit the task of 4-class classification. Details of the private dataset are provided although there is no reference to ethical approval.	Data from 102 patients was not made public nor is the coding.	Very good.	The authors apply the TCAV method on a multi-modal training image dataset. This application is an important contribution towards interpretable identification of breast cancer sub-types. Figure 3 is a particularly good illustration. However, the font size on the radial diagrams is too small.	This paper performed image classification on molecular sub-type classification for breast cancer patients. The proposed pipeline consisted of the classification using ResNet-18 pre-trained on ImageNet, the latent features were then extracted from the intermediate layers and clustered using k-means. Phenotypic Concepts were formed and weighting on these was calculated for each sub-type category. Strength of this paper lies in the pioneering application of concept calculation on medical images for breast cancer patients, which is one step closer to find the co-relation between phenotypic traits and microscopic categories. Weakness of this paper is really minor and some typos can be corrected during the proofreading process.	"Below, I comment on the paper in more detail: I think important references to previous works are missing to justify the contributions and methods. For example, the paper [a] (details below) is an extremely relevant reference that should not miss in concept-based interpretability works. In this paper the authors explain the limitations of learning concepts directly in a model's latent space, without applying any transformation to make the space centered and orthogonal with respect to concepts. They demonstrate that an important limitation of CAVs is that two very different concepts may have very similar vectors (in terms of their cosine distance) simply because of how the latent space is organised (e.g. skewed and far from the center). [a] Chen, Z., Bei, Y. & Rudin, C. Concept whitening for interpretable image recognition. Nat Mach Intell 2, 772-782 (2020). https://doi.org/10.1038/s42256-020-00265-z Similarly, I think a discussion of related works based on concept attribution would have made the related work richer and more interesting. The work in [b], for example, shows how the binary concepts in CAV can be quantified and transformed into continuous-valued concepts. I think that knowing this work may have even further helped the authors to find ways to improve the current state-of-the-art methods and produce more insights about their work. The bidirectional scores proposed in [b] can illustrate if a given concept is responsible for an increase or a decrease of the probability assigned to each class, hence the authors would have been able to clarify not only what concepts contribute to the identification of phenotypes, but also in what way. [b] M G, V A, S MM, H M. Concept attribution: Explaining CNN decisions to physicians. Comput Biol Med. 2020 Aug;123:103865. doi: 10.1016/j.compbiomed.2020.103865. Epub 2020 Jun 17. PMID: 32658785. In addition to [a] and [b], the work in [c] proposed alternative scores then the TCAV score to overcome some of the limitations of TCAV. [c] Yeche, Hugo, Justin Harrison, and Tess Berthier. ""UBS: A dimension-agnostic metric for concept vector interpretability applied to radiomics."" Interpretability of Machine Intelligence in Medical Image Computing and Multimodal Learning for Clinical Decision Support. Springer, Cham, 2019. 12-20. As a cascade effect, I think that by not including the reference in [a] the authors missed to addressed an important limitation of CAVs, namely that distant concepts may be pointed at by similar vectors. The authors could have addressed this limitation by adding, for example, an analysis of the cosine similarity between the vectors for each concept. I think that more in depth discussions about the concept footprints would have been useful. What do the vectors look like for patients with similar footprints? How far/close are they in the latent space? Can it be that similar patient share similar footprints? Is it possible to identify concepts that generalise to the entire set of patients? The quality of the figures is to improve. The text is too small to read."	The key contribution is the extraction of phenotypic concepts shared across multiple examples. This enables the investigation of associations between top- scoring concepts and shared or discriminating features among molecular sub- types. Therefore the proposed method can identify clinically relevant features in imaging data.	The major factor that backs up my decision of acceptance, is the efforts of trying to set up connection between phenotypic and molecular both visually and heuristically, which might agree with human experts in the future work.	I thank the authors for this work, as it is interesting and well presented. What prevents me from accepting is the limited novelty, that seems mostly incremental.
252-Paper0735	Implicit Neural Representations for Generative Modeling of Living Cell Shapes	The paper proposes a new cell shape representation method. The paper proposes a deep learning framework for spatio-temporal cell representation and synthesis. The proposed method allows for shape synthesis with virtually unlimited spatial and temporal resolution.	The paper addresses the use-case of generating synthetic datasets of cell shapes. It trains a NN to generate sequences of cell shapes from a signed distance function.	This paper proposes a approach to segmentation of living cells in 3d+t using a generative neural architecture. The paper is based on a level-set represented with a MLP that learns an implicit shape model from annotated images and is able to generate accurate boundary models of cells in fluorescence images.	the work proposes a novel and effective cell shape representation method which is able to generate sequences of cell shapes in 3D+time	This paper is creative and interesting.  It is well-written and a pleasure to read. I regret that due to the hour of the day I will not do it justice in these comments.	The method is simple and appears to be quite efficient with quite modest memory requirements. The integration of time as equivalent to the three spatial dimensions is important. The paper is written extremely well. The model seems to be able to cope well with anisotropic resolution (ec C.elegans dataset).	The references are cited randomly in a disturbed order. The best results in Table 1 are not annotated. Section 2 only describe the proposed method.	No non-trivial weaknesses evident to me.	There are no comparisons with other methods, therefore making it difficult to assess performance. The integration of time by normalisation of all coordinates onto [-1,1] is practical, but seems a little unsatisfactory as it does not fundamentally address the different scales in space and time (eg whether the processes are fast or slow). The same applies to the spatial dimensions. Some of the design choices are not well explained, eg section 2.2 why are the code vectors inserted into laters 1,5,8. I did not find the synthetic textures in figure 4 to be very convincing because they treat the segmentation mask as a hard boundary and the behaviour near to the edge does not appear to be very realistic.	The work could be reproduced	The paper indicates that source code will be available.	Sufficient information has been provided to allow the paper to be reproduced. The data appears to already be in the public domain and the methods are sufficiently well-described to allow me to believe that I could implement this method from the information provided.	The references are cited randomly in a disturbed order. The best results in Table 1 are not annotated. Section 2 only describe the proposed method.	"Discussion: it would be good to hear about the dynamics of new protrusions in the cancer cell case. This looks like a bifurcation or other non-linear state change. How does the model generate this effect? Tables: uncertainty measures are good, thank you. I wonder if in the cancer cell case there is a better measure than sphericity, one that quantifies the existence and characteristics of protrusions. (2.1): Re sigma in L_code: is this sigma related to the covariance matrix of the multivariate gaussian? If yes, how is it extracted from the covariance matrix? If not, a different notation would be clearer, since sigma routinely refers to std dev. Does it correspond to the std dev of the initializations? (2.2): Did the sine activation work any better than ReLU? Is the ""low frequency bias"" of ReLU an issue in this context? (2.3): ""70%"": Is 100% preferable? Was 70% chosen solely based on particular RAM restrictions? Does 3 GB max out the GPU's memory?"	It is unlikely that it will be possible for a more thorough evaluation to be completed against other methods during the rebuttal period but I would encourage the authors to consider this for any further journal paper. I would suggest considering at minimum whether this approach performs well against a classical level set approach, and how suitable the generated samples are for downstream tasks. Further discussion of how architectural choices were made should also be provided (section 2.2). What process was followed to arrive at this architecture?	The proposed method is novel and effective in spatio-temporal cell representation ad synthesis.	The paper is well-written, the method is creative, and the results are compelling.	The paper makes a relatively modest technical contribution and the evaluation could be stronger. These factors are balanced against the inclusion of time as a dimension, which is an interesting contribution.
253-Paper1642	Implicit Neural Representations for Medical Imaging Segmentation	This paper adopts a continuous implicit neural representations for medical image segmentation.	This paper propose a new segmentation method via using implicit neural representation.	This paper proposed a novel implicit organ segmentation network (IOSNet) that utilizes continuous implicit neural representations (INRs) to achieve memory-efficient and high-resolution medical image segmentation. Benefiting from the introduction of INRs, the presented IOSNet showed significantly smaller memory footprints and much faster convergence speed than the conventional fully convolutional networks. Experimental results on a clinical head and neck CT dataset demonstrated the superior performance and effectiveness of the proposed method in dealing with the 3D organ segmentation task, especially for small targets.	The training speed is accelerated. The proposed algorithm is suitable to solve problems which are computationally heavy, like super resolution image processing.	The paper is well organized and it is easy to follow. Combine different resolution features  to make segmentation prediction.	This study is well-motivated by a practical but longstanding problem, i.e., the trade-off between the image resolution and the memory footprint of the deep networks for image segmentation. The application of the INRs in the medical image segmentation task is novel and interesting and has been demonstrated well-suited for solving the above problem.	The contribution of the paper is not clear since the literature review lacks the comparison with other similar algorithms like the mentioned reference [2, 14]. Are there any other algorithms? What is the difference between the classic discrete segmentation algorithms with a sampling mechanism and the proposed algorithm? More related algorithms should be compared to illustrate the effectiveness of the proposed algorithm. 4, What is the shortcoming of the proposed algorithm?	For segmentation task, the method is not novel. UNet has similar characteristic.	An analysis of the inference efficiency of the proposed method is required but missed in the paper. According to the introduction of the proposed method, its inference efficiency is highly correlated with the resolution of the output image. The trained model may need to perform a large number of forwarding passes through the decoder to get the segmentation labels for each output pixel, which could be a time-consuming procedure. However, the author did not provide any analysis on this issue. The size of the testing set is too small (10 cases) to make the experimental result convincing. The author used a large training set (261 cases) to train the proposed network but evaluated it using 10 cases, which makes the experimental results less convincing. The author is suggested to rebalance the size of the training set and the testing set.	The effectiveness of the influence of sampling hyper-parameters is not analyzed, it could affect the reproducibility of the algorithm.	Yeah, it looks reproducible	The source code is available according to the author's answer to the reproducibility checklist.	Please check part 5 for more detailed information.	The motivation looks like not reasonable. Compared with typical UNet, the method also still uses different resolution features without decoding layer-by-layer. It also needs big memory resource. Moreover, use some FC layers to incur more parameters. The baselines are not medical segmentation related methods. IF-Net has big advantages in reconstructing and smoothing. The work follows it, but doesn't  show the advantages in segmentation task.	"Section 2.1, ""we learn a continuous segmentation (occupancy) function si for each organ i \in {0, 1, . . . ,R} where R is the number of OARs."": Do we need to learn a continuous segmentation function for the background? If not, the value of i should start from one, not zero. Section 2.1, Equation 3: Is the Dice loss calculated inside the sigma symbol? If so, the loss value could be very large. Do we need to normalize it to a fixed range such as [0, 1]? Section 2.1, ""s_i are the predicted organ occupancies and ground truth segmentations"": ""ground truth"" -> ""ground-truth"" Section 2.2, ""The data sampling parameters are K=4000, L=4, and M=5000."": What's the unit of L? It is better to use another variable to indicate the padding size since L has been already used for the loss function in Equation 3. Table 1 (left): Why do the upper and lower parts of the table list different sampling strategies? Why not evaluate all the four sampling strategies in both the upper and lower parts of the table? And the inconsistent number of sampling points of different sampling strategies may also make the comparison unfair. Section 3.4, ""Although boundary sampling segments well around organ boundaries (see Table 1 (left)) ..."": This conclusion is hard to see from the number reported in Table 1."	The contribution of the paper is not clear. The important reference discussion is missing.	The novelty is limited. The motivation isn't sufficient and reasonable.	Overall, this paper is well-written and easy to follow. The study is well-motivated by a practical problem and the proposed solution is also novel and effective. However, I still have some concerns regarding the inference efficiency of the proposed method and the small size of the testing set (see main weakness list), which should be well addressed before I can suggest the acceptance of this paper.
254-Paper0867	Improved Domain Generalization for Cell Detection in Histopathology Images via Test-Time Stain Augmentation	This paper proposes a test-time stain augmentation (TTSA) method for cell detection in histopathology images, which transforms the test image based on stain mix-up, and then the detection results from different augmented images are fused to produce the final output. The main contribution of this paper is the fusing parts, but an existing method (stain mix-up) [4] is used for stain augmentation. The fusing method is mostly based on the existing test-time ensemble method [3]. Therefore, the contribution is minor.	The authors propose a test-time stain augmentation method for cell detection under stain-varying conditions between source and target. The method uses conventional decomposition in the OD domain to decompose RGB images into the stain color matrix and stain density map. Multiple augmented test images are generated by mixing their stain color with the source domain through different weighting factors. The method is validated for cell detection on a publicly available dataset on which it outperforms the existing similar approaches	This paper is concerned with development of stain normalization method aimed for improving performance of cell counting in histopathology images. Thereby, the emphasis is to achieve robustness to domain variation (training set and target set do not come from the same staining protocol). In particular, the authors adopt test-time stain augmentation approach to domain generalization problem. The concept is based on generating mixtures of stain colors of source domain and target domain images. Given detection models fuses mixed images with the property to be less likely affected by the improperly mixed stains. Detection model is not required to be retrained, because it is applied during a test time only. The concept is validated on public dataset related to mitosis detection. In principle, proposed method improved counting performance in terms of A_50 metric (as well as F1 score).	*The test-time augmentation and fusing these results are simple and easy to implement. *In experimental results, the proposed method improved the detection performance in several methods.	Simplicity: The approach is simple and based on the OD space decomposition and generating multiple test augmented images with different weighting factors. Flexibility: The method is a generic test-time augmentation approach and can be combined with any trained method during testing. Hence, it can be utilized with different training strategies. The method is also generic because it can be tilted towards source or target with the selection of weighting factors. Performance: When used during inference, the approach improves the performance of different training methods.	The main strength of the proposed method is that the stain normalization model is not required to be trained, i.e. the generation of source-target domain mixtures is executed at a test time only.  Furthermore, source domain images can be pre-selected including the case with one source image only. That also can resolve the privacy issue if it exists. Another important strength is that proposed test-time concept is agnostic on detection model as long as it  produces bounded  boxes and confidence.	The technical contribution is minor. Half of the methodological section is explaining the existing work (mix-staining) [4]. The difference from the existing work is using stain mix-up in the test time and fusing the results. However, the reason is unclear, why this mix-up augmentation is good for test images without using training (the reviewer understands the effectiveness of stain mix-up in training, the reason is the same with the mix-up augmentation for semi-supervised general object classification). In test time, it is not always right that the test image is properly transferred to the color distribution of source images. Please clarify it. In the experimental results in table 2, in most cases, \alpha=0 is the best (this is related to the above comments). The reviewer considers that the performance improvement mainly comes from an ensemble of several detection results. It is unclear whether the mix-up augmentation is suitable for the ensemble of cell detection. Even if we used other types of experts (augmentation, network, and data sampling), the performance may be improved. To show the effectiveness of the combination of the stain mix-up and the test-time ensemble approach, it is better to compare another ensemble method (there are many methods for test-time augmentation). *In addition, the existing test-time ensemble method for object detection [3] is used for fusing the results. *There is no discussion about the existing test-time ensemble methods as related works.	Limited Novelty: The work is an extension of [4], whose modified version is used during inference instead of training. For modifications, the authors take a representative of the stain color matrix from the source instead of using a random matrix. However, an approach for combining the multiple augmented test image predictions for cell detection is also proposed. Limited Application: The method is specifically designed for cell detection and hence is limited from this perspective. Limited Analysis: The rationale behind some steps is not clear. The applicability of equations (4) and (5) is not clear. The performance is not reported directly using a random image from the source in (3) similar to [4]. The reason for using Mahalanobis distance is also not justified. It can be replaced with other distance metrics as well.	The main weakness of the proposed method is that the whole concept relies on a conjecture that model's predictions based on various source-target domain mixtures are likely to be suppressed if the mixture ratio is not right. That is based on an assumption that improper mixtures will have poor confidence scores. But that number (set on 3 in the problem considered in the paper) is selected on ad-hoc basis. It is unclear whether the same setting would work for different scenario (dataset). The authors were supposed to perform at least sensitivity analysis with respect to this parameter on the same dataset.	The proposed method basically uses the existing codes and the different part was described, and thus, the reproducibility is fine.	The reproducibility response are followed in the paper.	The source code implementing the presented concept is not provided. Implementation details provided in the paper in principle suffice that experienced programmer could implement the concept. The hyperparameter of proposed concept is the number of detected cells with high enough confidence scores to be fused together. It set to 3 in the only experiment conducted in the paper. But no justification based on sensitivity analysis is provided.	*This ensemble method may work well when each detection result tends to under estimation (there are false-negatives but few false positives). However, when each detection result tends to over estimation (has false positives), the final results also contain false positives, and thus it is not always right to improve the detection performance. How to control this? *TTSA is defined in the caption of Fig.1. It is common that an abbreviation is defined in the main text when it first appears.	I would recommend the following for future work: The method should be explored for other applications apart from cell detection. The significance of (4) and (5) should be proved by comparing it with the direct usage of random source image. If (4) and (5) are necessary, different types of distance metrics should be explored to select the optimal one.	The central part of this concept is fusion strategy that operates on the conjecture that model's predictions based on various source-target domain mixtures are likely to be suppressed if the mixture ratio is not right. Towards that, certain number of detected cells per group are fused based on their confidence scores. That is based on an assumption that improper mixtures will have poor confidence scores. The hyperparameter of proposed concept is the number of detected cells with high enough confidence scores to be fused together. It set to 3 in the only experiment conducted in the paper. But no justification based on sensitivity analysis is provided. Many sentences are extremely long what occasionally makes paper hard to follow.	As described in the weakness, the idea of test-time augmentation is not novel (the method is mostly same) and the effectiveness of the combination of the stain mix-up and the test-time ensemble was not well evaluated. Therefore, my rating tends to 'reject'.	"The strong and weak factors for this decision are: Simplicity, flexibility and performance as discussed in the ""strengths"" The limited application, and limited analysis. Certain aspects are not clear due to the limited analysis (see ""weaknesses"")"	This paper is concerned with development of stain normalization method aimed for improving performance of cell counting in histopathology images, whereat tr training set and target set do not come from the same staining protocol. The authors adopt test-time stain augmentation approach to domain generalization problem. Thus, proposed  stain normalization method is not required to be trained, i.e. the generation of source-target domain mixtures is executed at a test time only. Furthermore, source domain images can be preselected including the case with one source image only. That also can resolve the privacy issue if it exists. Moreover, another important strength is that proposed test-time concept is agnostic on detection model as long as it  produces bounded boxes and confidence. The outlined arguments outweight the potential weakness of proposed method related to a priori (arbitrarily ?) defined hyperparameter of number of detected cells with high confidence ratio.
255-Paper2493	Improving Trustworthiness of AI Disease Severity Rating in Medical Imaging with Ordinal Conformal Prediction Sets	This paper propose a distribution-free method of estimating uncertainty of ordinal predictions, with a simple approximate algorithm.An experiment is carried out on an stenosis grading task from MRI. The result is high-performing in comparison to the competing method, and the uncertainty score is able to select problematic examples as revealed by manual radiology review.	Th paper proposes an ordinal prediction set method that is guaranteed to contain the reported severity with a chosen probability in the context of automatic disease severity rating. It also shows a method to quantify the uncertainty in this setting.	Authors evaluate the usage of conformal prediction sets for uncertainty prediction in clinical settings.	Clinician trust in AI prediction is an important topic. Mathematical guarantee such as presented could have significant impact on adoption of AI technologies. The algorithms proposed is simple, and relatively intuitive. The paper is well-written and easy to follow. The symbols are mostly clearly defined, and the explanation is intuitive.	A novel ordinal prediction set algorithm that was shown to perform similarly to a non-ordinal algorithm in output set size and coverage. A method for quantifying uncertainty in this setting is provided.	The motivation for the work is sound, uncertainty estimation is known to be a serious problem for deep learning methods deployed in clinical settings. Overall, the paper is well structured with no immediate flaws.	The performance of the proposed method seems largely on par with LAC in the evaluation presented. It is unclear what the benefit of the current method is. Does it suffice to simply apply LAC to the current problem to get an uncertainty estimates? While the authors have demonstrated that high uncertainty cases tend to contain problems, is this true on the other end of spectrum as well? That is, do low uncertainty examples also contain less problems? The proof of theorem and proposition is left out completely from the main text. Some high-level idea should at least be included for the reader to follow along. In the supplement itself, the proof is also largely left to the cited article. It can be helpful to reproduce those proof so that it is easier to understand for readers who may not be as familiar with this literature. Some follow up questions/comments: why do higher severity prediction have higher uncertainty scores? Are the problem becomes harder to recognize as the severity increase? There are some runaway text between Fig 4 and 5.	The provided evaluation of uncertainty estimation is biased. The radiologist checked only the cases with high uncertainty for potential issues, but the cases with lower uncertainty might have issues as well. Clarity issues in the algorithm description and few other occasions See details in the comments section.	"The motivation for the paper is that deep learning methods suffer from certain shortcomings and that radiologists have hard time trusting these models when these models fail in unexpected ways. The authors even cite a survey which shows that radiologists have indeed hard time trusting such models. Given that the aforementioned fact is the main motivation of the paper, I expected to see a language that is more digestible by radiologists whereas the language employed in the paper is not straightforward to consume by a person who is not familiar with the underlying math. Herein, lies a contradiction:  a) if the target audience of the paper is computer scientists, the experimental results on a single dataset with a single model fail at showing convincing results. The IID assumption can easily be challenged in the context of medical imaging, which opens long discussions and puts the correctness of Theorem~1 at stake. b) if the target audience is radiologists, the language employed is not appropriate. Given the evidence in the introduction, I believe authors envisioned the paper to target radiologists, rather than computer scientists. With this assumption, I suggest the authors to be more explicit with the definitions, theorems and propositions to allow digestion by a larger audience. This paper must be understood by clinicians who are familiar with the concept of deep learning but not so math-savvy. I understand that number of pages is a limiting factor, my suggestion is to 1) Move Algorithm~1 to supplementary and use that space to enhance the text (algorithm pseudo code also has a limited value since the authors will share the code anyway).  2) Put images in Fig~4 next to each other instead of two rows. Also, experimental details need enhancing, what model has been used to obtain scores? How was it trained? How was the data split? Many details are missing. Other than that, there are number of typos in the paper: 1) (Page 2) set Y is defined to be within {0,...,K-1} but later used in \arg\max as y \in {1, ..., K}, is this the same K? If so, pick one range, if not, use another letter. 2) (Page 2) D_test = {-}^{n} , is it n or n_{test}? since train is defined as n_{train} 3) (Page 4) ""sequence of nested sets that includes Y Using ..."" , missing full stop?  4) (Page 4) at the end of Proposition 2, ""... satisfies 3"". 3 what? (3)? Equation (3)?  5) (Page 6) ""majority class (---). observed in Figure 3"" full stop by mistake? 6) (Page 7) Please put Fig~4 and Fig~5 right after each other with no text in between. Reading a single line between those figures makes it awkward. 7) (Page 8) Full stop missing after \textbf{Uncertainty Quantification for Critical Applications}. The language employed makes or breaks the paper for me, since the current drag for widespread AI deployment is mainly due to the resistance from radiologists. As such, papers like this one which show convincing results on these points of stress need to be digestible by clinicians."	Very good. The algorithm is clearly described and code is promised to be released.	The authors stated that they will make the codes publicly available	Some parts are reproducible but overall, many details about the model employed is missing.	See my weakness section.	"The evaluation of uncertainty estimation is biased. The radiologist checked only the cases with high uncertainty for potential issues, but the cases with lower uncertainty might have issues as well. Cases from low uncertainty regimes should be sampled as well and presented to the radiologist, and he should not know whether the presented cases were indicated as having ""high uncertainty"" or ""low uncertainty"" by the algorithm. Ordinal APS algorithm, which is the proposed method, is only described in pseudo code without additional explanations. It will be helpful to describe the algorithm also in a paragraph text. The following was unclear in the pseudo-code: Initialization: looks like in the beginning S is an empty set. It is therefore not clear how y' is selected. How the pseudo-code makes sure the same y' is not selected twice? Table 1 in appendix: Inconsistent ""grading class"" order. E.g. ""Ordinal CDF"" appears first in the ""coverage"" row but last in the ""Size"" row. This makes results interpretation confusing. The rows description of ""Coverage"" is too large visually and goes beyond the described rows. It is not clear what the ""count"" row means in the stratification by Set size. The title talks about stratification by true stenosis grading label, but looks like there is also a separate stratification by Set size Figure 5: Inconsistency in the chosen cases for radiologist's inspection - there are 5 stars in each category in the plot but in the description looks like only 4 points were taken from each category. It is stated that ""LAC provably has the smallest average set size but achieves this by entirely ignoring conditional coverage, and does not consider ordinal information"". However, results did not showcase any limitation of the LAC algorithm and it was shown to perform similarly to the proposed method. Minor grammar issues:     - Page 4: the sentence ""In practice, therefore approximate Ordinal APS..."" The last sentence on page 6 is cut.                The first sentence in ""conclusion"" section: ""into"" should be removed"	See weaknesses section for the detailed comments.	I think this is a well-written and solid experimentation over all.	The authors stated that the training and evaluation code will be made available.	Mainly, inappropriate language and missing experimental details
256-Paper2533	Incorporating intratumoral heterogeneity into weakly-supervised deep learning models via variance pooling	The authors propose an attention variance pooling method to aggregate patch-level features into a WSI-level representation, which explicitly considers intratumoral heterogeneity. Experimental results show that adding this pooling method into existing multi-instance learning frameworks improves survival prediction in five WSI datasets of different cancer types.	In this paper, the authors propose an attention variance pooling module, that helps multiple instance learning (MIL) frameworks learn  intratumoral heterogeneity (ITH) information from encoded patch features of whole slide images (WSIs). This module can be incorporated into existing MIL frameworks for survival prediction tasks. Authors also provide two metric scores for interoperability. Experiments on five datasets from The Cancer Genome Atlas (TCGA) demonstrate the effectiveness of the proposed method.	Intratumoral heterogeneity in WSIs have been extensively studied in the past and ongoing efforts are focused towards characterizing the structural/appearance differences across different diagnostic subtypes. This paper addresses a key challenge of modeling complex fetaures of the tumor microenvironment. The authors have developed a novel variance pooling architecture that enables an MIL model to empower the inherent intratumoral heterogeneity into deep learning models. The authors also provide two interpretability tools to investigate the biological signals captured by the models.	A novel patch-level features pooling method taking tumor heterogeneity into account.	The paper is well written and easy to follow. The idea of capturing the intratumoral heterogeneity information is novel and interesting in pathology image analysis. The motivation and the implementation method are well explained. The proposed module can be easily incorporated into existing MIL frameworks and improve the performance. Visualizations of patches (selected via attention scores, or ordered variance projection scores) show a good interpretability of the proposed method.	The authors have developed a framework, based on variance pooling which aims to capture intratumoral heterogeneity by quantifying ITH as variance along a collection of low-rank projections of patch features. The authors have designed variance projection contrast visualization to provide model interpretability, which is clinically relevant. Further, the ability to obtain the biological signals captured by the deep learning architectures further promotes trustworthiness of the system upon expert's approval (in this case a pathologist). The mathematical formulation of the proposed strategy is well described. The implementation details are well described. The choice of parameters, loss functions, parameter initializations are justified. An exhaustive set of results from different experiments with standard measures is provided. Visual illustration of the proposed interpretability tool highlights diagnostically useful regions which can be useful for assisting pathologists in decision making.	None	"The MIL architectures in the experiments are not well introduced. ""Deep Sets"" architecture is never mentioned before the experiment section. Lack of implementation details. The process of incorporating variance pooling components to ""Deep Sets"" (without attention mechanism) and ""Attn MeanPool"" (with attention mean pool modules) should be different, but no details are provided. The selection of some experimental settings are not explained. For example, variance pooling projections K=10 and nonlinearity=log()."	The authors describe two interpretability approaches. However, it is unclear how this interpretability approach would address some of the routine diagnostic workflows. Could the authors describe some of the structural alterations as the spectrum progresses from negative and positive SAsqr values? Did any discussions with the pathologists happen prior to planning this study for providing model interpretations? Pathologist guided studies will help in developing models that could help in clinical adoption.	Very good	All five datasets are publicly accessible. The authors stated that code will be publicly available.	The authors have provided sufficient details for reproducibility of the paper.	It is not clear why the authors chose the specific five cancer types from TCGA. They said the choice of these types was based on the number of patients. However, there are some other cancer types in TCGA with more patients than some of the selected cancer types. The sentence below is difficult to understand. Please consider rephrasing them: - Since the number of patches is different across WSIs, we cap - by random subsampling- the number of instances in each bag at the 75% quantile for that cancer type. The 75% quantile of what? According to the results in Table 1, the increase of c-index is only 5.3% for the Deep Sets method on the COADREAD dataset, instead of 10.4% mentioned in the text. Please double check. 4 In conclusion section, no need to provide the full form of MIL again. The number of patches (i.e., 75% quantile) used for training and the number of projection vectors are two key parameters. Sensitivity analysis of them should be performed. Not clear how to incorporate VarPool into DeepGraphConv.	"Related work. I suggest the authors add more introductions about the selected three MIL architectures in the related work. Implementation details. I suggest the authors provide more implementation details on incorporating proposed modules to different architectures. Experimental settings. I suggest the authors do more ablation studies and explain their choice of some hyperparameters or network configurations. Cross validation. Instead of randomly splitting the data 10 times, a more straightforward way to show the stability and generalizability might be 10-fold cross-validation. Baseline experiments. For MIL architectures without attention mechanisms, it is not clarified whether the mean pool module is used or not. If yes, the performance gain from ""MeanPool"" to ""MeanPool + VarPool"" is more important to prove the effectiveness of the proposed variance pool module. If not, I suggest the authors add another baseline with only ""MeanPool""."	Could the authors expand on the ideas of interpretability as mentioned above?	The manuscript is well written and organized and contains novel elements.	The idea of capturing the intratumoral heterogeneity information is novel and interesting in pathology image analysis. Experiments with open-sourced code (authors stated they will make code publicly available) and public datasets are reproducible. The experimental results demonstrate that incorporating the proposed module to MIL frameworks improves survival prediction performance.	The paper has some novelty, however it needs to address some questions described above.
257-Paper0284	INSightR-Net: Interpretable Neural Network for Regression using Similarity-based Comparisons to Prototypical Examples	In this work, the authors propose an inherently interpretable CNN for regression using similarity-based comparisons (INSightR-Net). Experiments were performed using a dataset of diabetic retinopathy grading. The proposed network is able to achieve performances in line with the baseline while being inherently interpretable.	This work proposes propose an inherently interpretable CNN for regression tasks applied in the field of medical imaging, which utilizes the information of similarity-based comparisons. Specifically, the authors incorporate a prototype layer into the model architecture to visualize the areas in the image that are most similar to learned prototypes. The final prediction is then modeled as a mean of prototype labels. Extensive experiments conducted on the task of diabetic retinopathy grading demonstrating the effectiveness of the proposed method.	In this paper, the authors propose a INSightR-Net deep neural network for retinopathy grading. It relies on a CNN architecture with a prototype layer. Such protypes help for better explanation while achieving a good accuracy.	The paper is clear and well-written. The work is novel. The novelty of the work mainly arises from the extension of ProtoPNet (Chen et al. [https://arxiv.org/pdf/1806.10574.pdf]) to regression tasks. New similarity function and additional loss components led to better explanations (measured in terms of sparsity and diversity).	Interpret CNN for regression with similarity-based comparisons is interesting and intuitive. The studied problem is clear and well formalized. The presentation throughout the paper is clear and even reader-friendly.	The authors propose a nice validation scheme with out of sample evaluation and prototype analysis that corroborates retinopathy gradation.	The choice of the clinical problem and dataset is dubious, as it is originally an ordinal classification problem and not a regression problem. As the authors did not consider the problem as an ordinal one, they also do not use any ordinal metric.	Inadequate comparative experiments for the the main claim. Detailed ablation experiments were done in this work, however, the authors only compared the methods with the ResNet-based baselines and did not compare the other interpretable work. Unreasonable choice of dataset, which makes the main claim less convincing. Considering that the method proposed by the authors is for the regression problem, experiments on the dataset of the regression task will make the method more convincing compared to transforming the discrete labels into a continuous distribution.	The contribution does not bring much novelty wrt to the original paper (Chen 2019), apart from the medical application. Also, the contribution lacks comparison wrt baseline methods for regression.	The work is reproducible.	No code available.	Implementation details are given, code will be available on github once the anonymity will be removed.	The authors could provide an additional dataset, where regression is indeed the task to solve. Additionally, it could make sense to adapt their approach to ordinal classification. Both suggestions would make sense for an extended journal version of the work.	Compare the proposed method with appropriate baselines. Choose a reasonable data set that can demonstrate the validity of the method. The authors should also clarify how the hypermparameters were chosen.	It would be interesting to include 'simple' methods such as linear regression in order to better highlight the benefits of the proposed approach.	The work presented has considerable novelty, and is worth discussing at MICCAI, as it addresses one of the major challenges for the adoption of computer-aided diagnosis in the clinics (i.e., interpretability).	The topic and idea is interesting. However, the choice of baselines and datasets needs more consideration.	Prototype is an intersting concept that can be extended to other medical application. Although a deeper investingation could be conducted to better highlight the benefits of such method.
258-Paper0807	InsMix: Towards Realistic Generative Data Augmentation for Nuclei Instance Segmentation	Submission 807 proposes a new method for generating data augmentations to aid the task of instance segmentation of nuclei in histology images. The method contains 3 major components, each of which contributes into the final improvement. Extensive evaluation is made available, including a comparison to state-of-the-art. The proposed method clearly aids the segmentation network in all datasets shown.	The paper describes a novel image augmentation method, that is similar to Mix-based methods.	This paper proposed InsMix which is a realistic data augmentation approach follows a Copy-Smooth-Paste principle. Compared with previous Copy-Paste methods, the InsMix mainly has three different components: 1. Morphology constraints (scale, shape, distance) to maintain nuclei's morphology characteristics. 2. Background perturbation is used for exploit effective use of the background information. 3. Smooth-GAN with triplet loss is designed for generate realistic augmented nuclei images. The experiments were done in two public datasets. Ablation studies on each component were also provided.	Strengths: a difficult and important problem is addressed the method is well motivated, the strategy of the authors is clear several novel ideas are composed into a single pipeline the evaluation includes quantitative and qualitative results on multiple datasets the method clearly outperforms other state-of-the-art augmentation strategies	image augmentation specifically for nuclei instance segmentation great idea with applied morphology constraints (SSD)	The InsMix is based on Copy-Paste augmentation method, with some unique improvements which were especially designed for nuclei segmentation problem, such as SSD (scale, shape, distance) constraints. All the three components presented a complete pipeline and show performance increasements. The overall novelty of this paper is good. The experiments were done in two widely used public datasets, also with the detailed ablation studies. The evaluation is strong. The paper is well organized and easy to follow.	not critical: the discussion is missing an outlook to other applications of this strategy. Can it be used for other histology segmentation tasks?	some details needs further explanation very little improvement in quantitative evaluation background perturbation in not proven enough to have positive impact on the results	"The actual values of some hyper-parameters such as , r, d and g which occurred in SSD constraints are not mentioned in the experiments section. The text said ""determined by cross-validation."", maybe it's better to give the actual values of these hyper-parameters in experiments. In experiments, why all the model only trained for 300 epochs? Does all the model convergent? In Table 4, the performance of Cowout drops evidently, what's the possible causes? Although this method is not proposed by the paper, I encourage the authors to analysis and discuss it."	the code will be made available, the data is public	the idea seems reproducible - the authors claim to make the code publicly available	Good. Datasets are public and code will be public.	Suggestions: I did not fully understand how the background perturbation works, i.e. what do the authors mean by shuffling with a ration of alpha? Could you elaborate or provide a formula?	"Comments: (p.1) ""some studies [10,12,21] proposed to regress the distance map [...]"" there are more interesting and novel approaches than referenced [21] I would suggest substituting it with https://doi.org/10.1186/s13640-020-00514-6 and https://doi.org/10.1109/ICCVW54120.2021.00081 the text about the steps of method on p.3 is redundant (p.4) ""The parameters [...] are determined by cross-validation"" - please provide further explanation par2.2 (p.4) the background perturbation needs further explanation and/or reference to be properly proven (especially the last sentence in that paragraph) The results in table 2&3 would be more reliable if they were cross-validated it is unclear if the results presented in Table 2 were obtained with TAFE or Hovernet, please provide this detail The results in Table4: the claimed increase in AJI value should be confirmed with proper statistical analysis, for now the increase is questionable background perturbation in not proven enough to have positive impact on the results"	Please see the weakness section.	The method is clearly superior to prior art, interesting new ideas are proposed	Overall, well structured and well presented idea, providing novel insight to image augmentation.	An augmentation method based on Copy-Paste with some unique improvements which were especially designed for nuclei segmentation problem.
259-Paper0939	Instrument-tissue Interaction Quintuple Detection in Surgery Videos	This paper proposes a neural network architecture to jointly localize and classify the instruments, tissues interacting with the instruments, and classify the action type. To enhance instrument and tissue detection performances, the anthers have employed joint spatio-temporal information via a spatiotemporal attention layer (STAL). A graph convolutional network is adopted to boost quintuple detection via reasoning relations between the instruments and tissues.	The authors propose to represent instrument-tissue interaction as instrument bounding box, tissue bounding box, instrument class, tissue class, action classquintuples by extending the earlier works that represent them as triplets. Moreover, they localize these quintuples. They propose QDNet which aggregates spatial and temporal information through the use of a spatiotemporal attention layer (STAL) and a graph-based quintuple prediction layer (GQPL) which is able to infer tool-tissue relationships. As part as QDNet, they propose a spatiotemporal attention layer (STAL) to aggregate spatial and temporal information of the regions of interest between adjacent frames, and a graph-based quintuple prediction layer (GQPL) to infer the relationship between instruments and tissues. They build a cataract surgery video dataset with annotations named Cataract Quintuple Dataset. According to what is stated in the Reproducibility checkbox list, the authors intend to share this dataset.	The paper presents an approach for instrument-tissue interaction detection in surgery videos. In doing so, the authors use a Quintuple detection network (QDNet) and apply it to cataract surgery videos.	The proposed spatio-temporal attention layer can effectively take advantage of the domain-specific spatio-temporal features to boost quintuple detection performance. The experimental results confirm the effectiveness of these additional components in enhancing instrument and tissue localization performance.	The proposed method extends the state-of-the-art models of representing instrument-tissue interaction as triplets to quintuples of instrument bounding box, tissue bounding box, instrument class, tissue class, action class. Moreover, they model these quintuples in a localized manner. As part as QDNet, they propose a spatiotemporal attention layer (STAL) to aggregate spatial and temporal information of the regions of interest between adjacent frames. STAL is a modification of the commonly used spatio-temporal attention module (STAM) but aggregates spatial and temporal information of the ROIs instead. They build a cataract surgery video dataset with annotations named Cataract Quintuple Dataset. According to what is stated in the Reproducibility checkbox list, the authors intend to share this dataset.	Detailed method description Real data Ablation study	"1) The proposed neural network architecture is not re-producible since many important implementation details are missing. These details include: a) The dimensions of input and output feature maps of different sub-networks and layers (STAL, FC, and GQPL). b) The operations' details (size and number of the kernels in all convolutional layers after the ROI align operations, the location and type of the adopted activation functions). c) The number of trainable parameters in the proposed architecture compared to the rival approaches, and the number of trainable parameters in each evaluated network in the ablation study. 2) Regarding  STAL (the spatio-temporal attention layer), it seems that feature aggregation is mistakenly formulated using the term ``concatenation"". The authors should note that concatenating N feature maps of size $N\times H \times W$ results in a new feature map with dimensions $(N*C)\times H \times W$. Besides, the authors mention that the visual feature of the current frame are augmented by addition. Hence, I would expect that the features which are added together should have the same dimensionality. All mentioned feature maps and vectors (including queries, keys, and value vectors) should be formulated using the details of the layers they pass through (e.g., linear layers, convolutional layers, or activation functions). 3) It seems that the proposed multi-task learning approach is inspired by the previous work related to instrument-tissue interaction [16]. However, the authors have not provided any comparative results to show the superiority of the proposed network compared to this important reference. This competitor approach does not localize the instruments and tissues notwithstanding, it is expected that the proposed quintuple detection network outperforms this network in instruments, tissues, and action classification. 4) The evaluation metrics adopted in this paper cannot reveal the network's performance in quintuple detection. The two metrics used in this paper only consider the average precision for the instruments, tissues, or joint instruments-tissues. Indeed, while ``action"" appears to be the main component of the quintuple, no metric is used to demonstrate the network's ability to classify the actions. I would expect the authors to provide comparative results of mAP for joint instrument-tissue-action (as in triplet recognition performance measurement in [16], but for quintuple). 5) Regarding the dataset, the authors have mentioned that the frames which do not include any instrument-tissue interaction are removed. However, the prepared network exploits a number of consecutive frames before each keyframe for spatio-temporal feature extraction and refinement. Which strategy is adopted when the reference frames are removed? In case the frames with no instrument-tissue interaction should be removed before evaluations, this method has a major weakness of relying on manual annotations for the test sets."	They propose a graph-based quintuple prediction layer (GQPL) to infer the relationship between instruments and tissues and this is emphasized as a novel contribution, however there are examples of similar works in surgical domain. (Such as the somewhat recent work (2021) by Islam et al. titled STAN: Spatio-Temporal Attention Network for Next Location Recommendation.) A literature review on both the generic object-object interaction graph representations, and particularly tool-tissue interaction graph representations is missing.	Minor improvement Limited data	Due to lacking many details about the convolutional and fully connected operations, activation functions, size of the output feature maps of different layers, number of trainable parameters, and many other important details, the paper is not reproducible. In case the dataset will not be released: Since the dataset will not be released with the acceptance of the paper, there is no possibility to reproduce the results and explore the subject further. Indeed, only the authors themselves can improve the results provided, which is regarded as a major weakness of the current paper.	The authors have checked the boxes relating release of the source code and the dataset which is specifically build and annotated for this study.	The methods have been described in detail and should be reproducible.	I would suggest that the authors try to address the mentioned weaknesses. In particular, the authors should formulate all operations in the STAL and STAM, with a detailed description of convolutional and fully connected layers and feature maps' dimensions.	A literature review on both the generic object-object interaction graph representations and particularly tool-tissue interaction graph representation should be added. It should be clarified that a similar approach was proposed by some earlier works in the surgical domain, and the authors should distinguish their approach in comparison to these works.	"For me, this paper is more a ""pipeline"" paper, where several known methods have been stacked together for a specific application. Per se, that is not a bad thing, not every paper has to invent an absolute novel algorithm. However, I am not so impressed by the results, they seem more minor to me, compared to previous works. For me, it is hard to judge what ""effect"" this better results have on the surgery and the authors should comment on that. Another limitation is the dataset, which seems an in-house one: ""we build a cataract surgery video dataset"" ""labeled frame by frame ... under the direction of ophthalmologists"" With this little information, it is hard to say how valid this dataset is. In summary, the paper is borderline for me. Minor comment: Mean Average Precision (mAP) should be defined earlier in the manuscript (actually it should be defined when it first appears in the text)."	See main weaknesses.	The proposed method extends the state-of-the-art models of representing instrument-tissue interaction as triplets to quintuples adding localization. The stated contribution STAL is a modification of the commonly used spatio-temporal attention module (STAM) but aggregates spatial and temporal information of the ROIs instead. Although a graph-based quintuple prediction layer (GQPL) to infer the relationship between instruments and tissues is proposed as contribution, the authors do not address the state of the art models in generic object-object graph representations, and also the tool-tissue interaction graph representations in surgical domain, such as the somewhat recent work (2021) by Islam et al. titled STAN: Spatio-Temporal Attention Network for Next Location Recommendation. It should be clarified that a similar approach was proposed by some earlier works in the surgical domain, and the authors should distinguish their approach in comparison to these works.	The paper is well written and good to follow. There has been an evaluation on real data and an ablation study. However, the results could be better to state-of-the-art, and it is not clear to me how the improved results affect the application. The dataset is limited in my opinion.
260-Paper0943	Interaction-Oriented Feature Decomposition for Medical Image Lesion Detection	This paper introduces the whole image context embedding to enhance the classification branch of the Faster RCNN solution for the lesion detection task. The main idea is to encode the whole image as a fixed size embedding and leverage the self-attention mechanism to interact with the RoI align features before the final classifier. Evaluated on two different datasets, the proposed approach can consistently improve performance.	This paper aims at improving the classification accuracy in lesion detection problems. In some problems, classifying different lesion types is closely related to the relative location and size of the lesion in the image. Therefore, the authors propose to use attention-weighted global features to do classification, instead of ROI features. They designed the Global Context Embedding module and the Global Context Cross Attention module for this goal. On a private OCT dataset and a public MRI dataset, 6% and 3% improvement were observed in the classification accuracy, compared to Faster RCNN.	This paper proposes a new framework to fuse global context features into local lesion features for better lesion detection. Specifically, a global context embedding (GCE) module and a global context cross attention (GCCA) module are introduced to combine global and local features for classification. Experiments are conducted on an inhouse OCT dataset and a public brain MRI dataset. Results indicate the proposed method achieves state-of-the-art performance. Ablation study proves the effectiveness of the GCE and GCCA modules.	1, The global context embedding module receives two forms of supervision during training: one is standard whole image classification and the other is a feature adaptation branch for later interaction with the localized detection heads of the original Faster RCNN design. This design enforces a good embedding at the global level. 2, Self-attention module is leveraged for the interaction between global context and local features. This interaction directly enhances the local features and proved to be beneficial for the performance.  Combined with the Global embedding, the whole design is novel. 3, Experiments are well designed and conducted thoroughly. They provide good support for each proposed new component.	The authors pointed out that many types of lesions are distinguished by the position and the relative proportion of the lesion to the tissue, The idea of using attention-weighted global features to do classification is novel and intuitive.	This paper introduced two new modules, global context embedding module and global context cross attention module, to enhance lesion features for better lesion detection. The introduced modules are well motivated and simple. Experiments on two datasets show the effectiveness of the proposed method. Both GCE and GCCA modules are validated through an ablation study. This paper is well-written and easy to follow.	Despite Novel design and promising results, I do have the following concerns: 1, how is the model trained on the OCT dataset? This dataset is very small and only has 32 cases. Given the complexity of the proposed model, I am assuming each slice in each case is fed into the model for training and testing. If so, the positive sample should be highly sparse, how is this issue solved? If not, the dataset is too small to be significant. 2, Since the brain tumor dataset is public and exists for more than 5 years, I am wondering how the proposed solution compared to another published work specific to the same task? Even though many other solutions are listed in Table 2, they all are general solutions for natural image modalities, which is less persuasive.	"The writing of the paper can be improved. There are some word mistakes and some unclear parts, see detailed comments below. The Global Context Auxiliary module seems to only support one lesion type per image. What if there are two lesions of different types in one image? The OCT dataset is very small, only 23 cases. ""the recall is equal to the precision, which means the number of detections is equal to the number of ground truths. Therefore, GCE can effectively avoid missed detections."" I cannot understand the logic of this sentence. If GCE can avoid missed detections, it should have high recall."	What is the IoU threshold for reporting precision and recall? What are the APs for IoU=0.50 and 0.75 like? The proposed method only modifies the features for classification branch. Why not use the enhanced features for box regression branch?	Positive if code and the private dataset is released.	The authors have checked all questions in the Reproducibility Checklist as Yes. However, no code or data will be released according to the paper. No statistical significance was reported either.	The paper is well-written with clear description of technical details. It should be easy to reproduce.	Please address my concerns.	"""We adopt a 3x3 convolutional layer (Conv3) on input features of the module to avoid the optimization effect of the localization branch."" What is optimization effect? ""The proposed IOFD network superiors the state-of-the-arts algorithms"" -> ""The proposed IOFD network surpasses the state-of-the-art algorithms"", ""two modal datasets"" -> ""datasets in two modalities"", ""valid set"" -> ""validation set""."	"Section 2.2, ""Firstly, We adopt ..."" -> ""First, we adopt ..."" Section 3.2, ""In particular, the recall is equal to the precision, which means the number of detections is equal to the number of ground truths."" What does this imply?"	The whole idea is novel and each module design makes sense.	The method has some novelty. However, the paper writing needs improvement. There are inconsistency between the Reproducibility Checklist and the paper.  The proposed algorithm should be compared with some existing lesion detection / classification methods, instead of only general object detection baselines.	The proposed method introduced two new modules with good motivation and simple formulation. It is also validated on two lesion datasets. Despite some minor concerns, this paper deserves to be accepted.
261-Paper2064	Interpretable differential diagnosis for Alzheimer's disease and Frontotemporal dementia	The paper describes creating a tool to differentiate between cognitively normal (CN), Alzheimer's disease (AD), and Frontotemporal disease (FTD) since this is a tough task to do by just using cognitive tests or observing symptomatology. It describes using a deep grading (DG) framework with a support vector machine (SVM) to classify whether to assign a diagnosis of CN, AD, or FTD. Different groupings of data were also tested (AD+FTD+CN vs CN+FTD vs AD+CN vs AD+FTD) in order to test the best classification model.	In this paper, the authors propose a new method to perform specific-disease diag- nosis (i.e., AD vs. CN and FTD vs. CN) and differential diagnosis (i.e., AD vs. FTD and AD vs. FTD vs. CN). Their purpose is to expand the knowledge about dementia sub-types and to offer an accurate diagnosis tool in a real clinical scenario. They extend the recently proposed Deep Grading (DG) framework by training it with multiple types of dementia (i.e., AD and FTD).  Furthermore they propose an ensemble of the graph convolutional network and an SVM.	This paper aims at developing a machine-learning algorithm to classify multiple sub-types of dementia (e.g., Alzheimer's disease and Frontotemporal dementia) based on T1-weighted MR images.  The authors combine the classification results from a deep-learning-based grading framework that performs disease classification according to the structure-wised evaluation of the tissue abnormality, and an SVM algorithm that classifies the disease type by exploiting volumetric atrophy in dementia patients.  The proposed method reveals the abnormal brain structures that are characteristic of each dementia sub-type and allows improved classification results over existing methods.	This method seems novel in its approach to classification, using an ensemble of 3D-UNets and then applying an SVM to classify the data. Reproducing other methods that have been used in the past to solve the classification problem Testing the DG framework + SVM against other methods, using the same data, in order to test performance of their method. The authors demonstrate a clear understanding of the problem of disease classification, not just from a technical perspective, but from a medical perspective.	The combination of the graph convolutional network and an SVM for classification is interesting.	The authors explored the potential of using machine-learning methods to classify multiple dementia subtypes, while previous work mainly focused on the binary classification of either Alzheimer's disease or Frontotemporal dementia. The authors performed a careful ablation study for the proposed method and showed some interesting findings such as aggregating data from multiple dementia sub-types may improve the binary classification results, and the deep-learning strategy can provide complementary diagnostic information to the hand-crafted features such as volumetric atrophy. The proposed method revealed the signature ROIs for each studied dementia sub-type. The agreement between their findings and previous research work indicated that the proposed method might be clinically useful.	Not being as detailed about other models used in the pipeline, such as; everything used in the preprocessing pipeline, details of AssemblyNet used in image segmentation, and details about the SVM used.	They did not include subjects with early cognitive impairment. The images are available in the databases used. It is not clear how they partitioned the data in validation and final testing. They do not give details of the implementation of the SVM, kernel or optimization. There are no details of the equipment, or acquisition characteristics. There may be a bias because they are images of different characteristics.	"The proposed method seems to be a combination of previously developed diagnostic tools.  The innovation may need further improvement. The authors claimed that the ""early"" and accurate classification of dementia sub-types is desired.  However, there isn't sufficient clue of ""early"" detection demonstrated in their experimental setup and results. The number of subjects in the Frontotemporal dementia might be too small (45 for training and 29 for testing) to reach a concrete classification result.  Furthermore, as shown in Fig. 2, the abnormality map of Frontotemporal dementia appeared asymmetric between left and right hemisphere, why is that?"	Used data from open source MRI databases (ADNI, OASIS, AIBL, MIRIAD, and NIFD) Was not detailed enough in the preprocessing steps, for example, did not talk about algorithms used for each part of the preprocessing pipeline. There is quite a bit of information missing with respect to hyper-parameters used for some of the algorithms used in the paper. For example, AssemblyNet and for the Support vector machine used.	The authors give details about the implemented neural networks, but not about the SVM. The databases used are available. They don't have the code available.	"The data used are all from open-access databases.  The implementation of the algorithm is clearly described in the method section for an expert to reproduce the main results.  However, it might be better for the authors to further clarify how they perform ""oversampling technique"" to balance the number of training data from each category."	"Introduction When mentioning that other methods may be ""based on handcrafted features that may not fully exploit the image information"", it would be helpful to then give a few examples of features that these methods may be missing. It would also be helpful to also mention a few features that most methods take into consideration. After that, you could mention why this could be problematic (although this last part is not necessary). Materials and methods To help with reproducibility, you should talk about which methods you used for preprocessing. For example, for inhomogeneity correction, did you use FSL's inhomogeneity correction, ANT's inhomogeneity correction, or did you create your own intensity inhomogeneity correction algorithm? This should be done for each step of the preprocessing pipeline. You mention that you used AssemblyNet for segmentation, however you don't give any details about the hyper-parameters used for this. You should mention this in a few sentences. The author should include hyper-parameters used for the SVM, it may seem obvious, however you should write the number of hyperplanes chosen. Interpretation of deep grading map In the first paragraph, instead of saying 'regions around hippocampus', you should say ""regions included in the hippocampal formation"". I say this because, based on your Figure 2, it seems that the hippocampus, along with other areas around it, are highlighted in the grading map."	I suggest adding quantitative results to the abstract Justify why subjects with MCI were not included What MNI space was used? Indicate what software was used for pre-processing What kernel did you use for the SVM? And how were the parameters optimized? -In tables 1 and 2 I suggest adding the standard deviation, how many repetitions were made for the results of table 2? Same case for tables 3 and 4. Give details about the data used, standard deviation (if it is the case) and number of repetitions. It is not clear how the data was partitioned in validation and final testing. Can you clarify this?	Integrating multi-modality MRI data (e.g., diffusion tensor imaging, functional MRI) might be helpful in further improving the classification results. Probably the authors can use these open-access data to demonstrate that their method is useful for the detection and differentiation of dementia subtypes in the early stage, i.e., MCI.	The approach that the authors use is unique even though the models that are used within their network have been used. The authors are able to articulate, not just the technical aspects, but also the clinical aspects of the problem. This seems like a tool that can be used in conjunction to approaches currently used in the clinic for deciding treatment methods.	The contribution is not very novel, it is missing several details of the implemented classifiers and there may be a bias for using different databases without giving details of the images or equipment.	The paper is well written and the experiments are well designed to demonstrate the efficacy of the proposed classification method. However, there's not too much innovation in the proposed method.  Good results may be leaded by the cohort bias, How to exclude the cohort bias among datasets should be considered.
262-Paper0324	Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis	This paper proposed an interpretable GNN to find the group-wise-specific connectome-level features. The authors provided clear descriptions for the algorithm, which could contribute to the future work.	The article proposes an interpretable deep learning framework to predict disease and identify disorder-specific salient regions of interest and important connections driving predictions. The method is applied to data set consisting of HIV positive subjects, one containing subjects diagnosed with bipolar disorder, and the publicly available data set of the Parkinson's Progression Marker Initiative. The method reveals much higher accuracy scores than several baseline methods	The authors proposed an interpretable brain network-oriented framework, in which a GNN is used to extract embeddings of ROIs from the brain MRI images and an explanation generator is used to learn a disease-specific masking matrix. Their experiments showed that the proposed method achieved superior prediction performance and interpretations derived from the learned masking matrix aligned with existing clinical understandings.	This paper proposed a novel interpretable GNN framework for connectome-based brain disorder analysis. It's an interesting work that combined an explanation generator to make the GNN could interpret the brain biomarkers in the group level.	focus on interpretation of predictions model is carefully derived cross-validation on several datasets superior accuracy compared to 9 other methods !	Learning a sparse masking matrix is an interesting approach to impose implicit regularization on brain networks and produces more robust results under the limitation of data size. Visualizations for both salient ROIs and important connections are helpful to understand the clinical relevance of the proposed model.	For our brain, there's a common hypothesis that every brain is unique even if belongs to the same group. But this paper aims to find the group-wise biomarkers though good results were obtained, there's still a question that why the unique explanation for each subject is not good? Or why don't the authors combine the subject-wise and group-wise features together?	analysis fails to account for confounder (such as motion, sex, ... ) so that interpretation of findings is difficult fails to test for significant differences to baseline	"Main issue: the method's main shortcoming is that it cannot provide subject-specific interpretation and hence cannot be used in a more realistic clinical practice. Minor issues: 1, definition of node feature x_i is not clear (under eq. 2). 2, ""C"" is not defined in eq. 5 3, In section ""Prediction Performance"", ""IBGNN+ can further increase the backbone by about 9.7%..."". However, this observation is not supported by Table 1.  4, define ""HC"" before using it"	The authors provided clear code to make it reproducible.	partly performed on public data and provide source code so reproducibility of some of the findings should be excellent	The submission meets all criteria on the reproducibility checklist.	The abbreviations when first occured should be explained with full name. It's not very clear for the weights for the losses.	please see above	1, extending this method to learn subject-dependent masking matrices would be interesting. 2, It is unclear how the initial ROI embeddings were obtained. Given the limited size of labeled datasets, using self-supervised methods for pre-training may help to improve the tasks evaluated in this paper.	This work proposed a good method and conducted interpretable biomarkers for different diseases.	strong paper with some weaknesses that lower enthusiasm	The proposed brain network-oriented GNN is novel, the use of a masking matrix to improve the model's robustness and interpretability is an interesting idea. Evaluations on three different brain imaging datasets are solid.
263-Paper2611	Interpretable Modeling and Reduction of Unknown Errors in Mechanistic Operators	The paper extends the DAECGI method to improve the reconstruction of the forward operator when errors are present in the initial estimation. The method cyclically updates the estimated corrected forward operator by generating a latent vector z based on the initial forward operator and the current estimated forward operator. By using an SOM, the types of errors can be revealed by examining the latent vector z for a set of initial and final forward operators.	Use prior knowledge to improve the performance.	The paper described a physics-inspired neural method for  the inverse problem of ECG. The method shows advantages over a pure physics-driven method (fully data-driven) or a pure neural network-based method (requires less training samples and has more interpretability).	The method is capable of correcting errors in the forward operator when solving inverse problems. The authors claim that these errors are difficult to account for when solving inverse problems and instead update the forward operator in a cyclical process. By clustering in the latent space of the vector controlling the update to the forward operator, the type of error in the original forward operator can be evaluated.	A new model was proposed to reconstruct medical images.	The paper is well written and structured.  Interpretability is highly desired in clinical settings.	While the paper does propose a method for interpreting the refinements and sources of error in the original modeling of the forward operator, it is unclear why this interpretability is clinically useful. It seems interesting from a methodological standpoint to understand the sources of error in the forward operator, however the paper does not make it clear if there is any clinical use for this information. If interpretation of the correction of the forward operator has a direct usefulness it should be made clear. The motivation for the interpretability of the method should be much more clearly motivated as this is emphasized in the title and paper itself. The implementation details are not fully described and sometimes unclear. For example, I am uncertain if the SOM mapping plays a central role in the generative model or is performed after the fact to add interpretability to the latent variables z. Similarly a lot of training details and architectural information are missing. The results are difficult to interpret and do not make the overall effectiveness of the method obvious. Figures like figure 2, which shows the forward operators and is not human interpretable, could be replaced with more easily interpretable figures such as figures of the simulated electrocardiographic sources displayed on the heart and their reconstructions. This would help the audience verify that the inverse solver is working as correctly. The authors train on a variety of error sources. However, it is unclear if these errors correspond to those seen in actual practice. Motivation of these choices would be helpful to validate the experiments. Similarly, it would be interesting to see the quality of the reconstructions under different error conditions. For example, translations or rotations could be gradually increased and the error in the reconstruction could be plotted to show if the model is more capable of adjusting to different ranges of errors.	The results are not adequate to fully support the claims in the manuscript. Solid results are required to justify the advantages of the proposed model.	The authors could  have provided more results  (on both synthetic and real data) using the remaining space.	Given the level of detail in the paper, it would be difficult to reproduce the author's work. The architectures and other training details aren't mentioned. In addition, I am unclear on some portions of the method itself. The data used seems to be mostly synthetic so it seems like it would be possible to reproduce the data.	There is no reproducibility problem.	Synthetic and public real data are used. Codes are also submitted for review.	"In the introduction, the authors claim that there has been ""substantial interest"" in combining deep learning with prior knowledge of imaging physics. However they only cite two examples. More review of the prior work is necessary. Similarly, the method presented is not contrasted with prior approaches in the literature. Such a comparison would help evaluate the method's novelty. More discussion of the sources of error and how they affect inverse problems in general would be helpful for assessing the applicability of the method. The comparison to the DAECGI method given in the introduction is not very clear. It is important to make the comparison very understandable, as the present method seems to be a direct extension of the DAECGI method. It seems that the major difference between the two is that the presented method uses the both the original forward operator and the derived forward operator in inferring the latent vector z. The authors seem to claim that this is the innovation that allows for the sources of error to be identified. The SOM method applied to the latent vector seems to me to be a post hoc method to add to the interpretability of the algorithm. From the description in the text, it seems like the SOM clustering is central to the generative model. I may not fully understand but it seems like the SOM is applied after inference. If this is the case, any clustering algorithm could be used. The SOM method is presented in the text alongside the generative model but it is not clear that the SOM plays a role in the generation of the forward operator. If it does, it needs to be clarified. If it does not it should be presented in a separate subsection. I'm not sure I understand the motivation for using a U-Net as a generative model. It seems that any autoencoder architecture would be as applicable as a U-Net. Some motivation should be given for this choice. Furthermore, as the U-Net uses convolutions, it is not clear why the convolution operation is appropriate for the forward matrix H. In equation (5) the authors add a hyperparameter to the loss function. With the beta parameter I don't think the equation is quite equal to the ELBO and I'm not sure that the bound in the equation still holds. I think the addition of the hyperparameter is fine and makes sense to control the degree to which the KL term effects the loss, however I believe some care should be taken to ensure that it is described in a way that is mathematically correct. As the model is fairly complicated, I think the training procedure could use explicit description. The main text details how the forward and inverse problems are solved but I am unclear on how the U-Net and variational inference network q(z Hi, Hf) are trained. Are the networks trained simultaneously with simulated ground truth forward operators and operators with errors? What are the settings used for the hyperparameters beta and lambda_reg? What are the architectures used for each component. These details would be helpful in the understanding of the overall method and increase its reproducibility. If there is not space in the main text they should be provided as supplemental materials. The authors claim the cyclical inverse estimation procedure ""is expected to continuously reduce the error."" I'm not sure what this means. If the error is guaranteed to decrease they should explicitly state that. If not, they should also explain why the approach they have chosen is appropriate. This step seems important to describe in detail as it seems possible that a cyclical update such as this might not converge without some guarantees or some amount of appropriateness to the task. Convergence plots could be shown for these cyclical updates to verify the procedure is working as intended. I am unable to understand what is being depicted in Figure 2. The images are small and difficult to read. In addition the visualization of the forward operator is not human interpretable. Instead the authors should choose to display some metrics that might more informative. Similarly, Figure 5 is very difficult to interpret. I am not sure how to read these images. In figure 4, the reconstruction error of the different methods is presented. It may be helpful to show the simulated and inferred u's if they are interpretable. This would show that the model accurately reconstructs the underlying latent cartographic source data and that the inverse solutions are as expected. The paper contains spelling errors and awkward formatting such as ""variatioanl,"" ""gird,"" and ""Real Data : ."" The manuscript should be more closely spell and formatting checked."	In this manuscript, a new model was proposed to reconstruct images by relying on the forward operator and residual errors simultaneously. It is tough to see that the proposed model is more interpretable. The interpretability was not demonstrated in the manuscript. I suggest removing this word from the title. Otherwise, solid results demonstrating interpretability should be supplemented. The evaluation of the model is not complete. More quantitative results should be added to justify the advantages of the proposed model. The numbers and fonts in the figures are too small. It would be better to have larger sizes.	see point 3,4,5	I think the method is very interesting. However I think the presentation could be improved to make a much stronger paper. The differences between the previous DAECGI model are not obvious so it is difficult to assess the novelty of the proposed method, as these methods are closely related. Some of the training details are hard to infer from the exposition in the paper. Furthermore, the presented results could be strengthened by adding more interpretable figures and descriptions. In addition I think some of the paper organization, especially with regard to the SOM procedure, could be improved. For these reasons, I would recommend that the paper be rejected so that the authors might strengthen it and resubmit an improved and stronger version.	A new model was proposed, but there are no solid results to support the superior of the model.	This paper addressed an important topic in electrocardiogram. The method bears some novelty and is preliminarily validated.
264-Paper0742	Interpretable signature of consciousness in resting-state functional network brain activity	The Modular Hierarchical Analysis (MHA) linear latent variable model was used to differentiate the various conditions of consciousness using resting-state fMRI. The statistical analysis showed the signature of consciousness from 5 monkey data.	This paper proposed a four-stage strategy and used a constraint linear latent variable model to define the spatial pattern of different consciousness states.	This paper introduces a novel method for states of consciousness by understanding ROI connectivity patterns, proposing a new spatial biomarker for determining levels of consciousness.	The Modular Hierarchical Analysis (MHA) linear latent variable model was used to differentiate the various conditions of consciousness using resting-state fMRI. The statistical analysis showed the signature of consciousness from 5 monkey data.	This work adopted a constraint linear latent varialble model to identify the spatial signatures of consciousness, which provides a novel application of the existing method and probably holds potential in clinical applications.	The authors have a clear scientific/ medical understanding of the problem. This shows that the authors weren't just applying cool technology to a common problem, but chose a framework that was backed by having a clinical understanding of consciousness. All sections of the paper, including the introduction which introduces the problem in a very detailed matter, were very clear and thorough The statistical analysis seems robust, in that data was tested for normality and after discovering it was non-normally distributed, switched to using non-parametric analysis. All graphs are easily interpretable.	Authors used the Modular Hierarchical Analysis (MHA) linear latent variable model, which was already published by Monti et al. (2020), to uncover disjoint networks and associated activities for all subjects. For training, the MHA model is fitted on the 4 monkey dataset, and the log-likelihood is computed for 1 monkey validation dataset. Due to the limited number of subjects (only 5 subjects), the MHA model could be biased to the small number of subjects, and the uncover networks could be changed according to the sample size. Authors could describe how to interpret the difference of the disjoint networks according to the optimal K, and according to the different atlases (CoCMac, DictLearn, and CIVMR). The variation of functional patterns and associated activities across runs within same subject also could be examined for clarity.	The paper only adopted an existing method without any improvement and the references listed in the paper are not recent literatures, which weakens the novelty of this paper. Varol, A., Salzmann, M., Fua, P., & Urtasun, R. (2012, June). A constrained latent variable model. In 2012 IEEE conference on computer vision and pattern recognition (pp. 2248-2255). The adoption of three atlases in this work is not well explained and ambiguous. While the method can reveal the differences in network activities among different consciouness states, the inferred spatial patterns and dfifferent network activaties are group-wise, which can not be used, at least in current work, for subject-level recognition and thus limits the clinical application of proposed strategy.	N/A	Not Applicable	The paper provides reasonable reproducibility.	The method is reproducible, although the data is not since it was acquired using a custom surface coil (although this is perfectly fine). The atlases used are also readily available, as they are open source atlases. Each section of the 4-step framework is thoroughly explained with all models written out.	Authors performed the experiments using more data. The variation of functional patterns and associated activities across runs within same subject also could be examined for clarity.	The paper only adopted an existing method without any improvement and the references listed in the paper are not recent literatures, which weakens the novelty of this paper. Varol, A., Salzmann, M., Fua, P., & Urtasun, R. (2012, June). A constrained latent variable model. In 2012 IEEE conference on computer vision and pattern recognition (pp. 2248-2255). The adoption of three atlases in this work is not well explained and ambiguous. While the method can reveal the differences in network activities among different consciouness states, the inferred spatial patterns and dfifferent network activaties are group-wise, which can not be used, at least in current work, for subject-level recognition and thus limits the clinical application of proposed strategy.	"This was a solid paper that was very thorough, so there aren't too many changes to be made. Methods In the 'Atlases learned from the data' section of the 'ROIs definition' section, there was mention of using 'nilearn'. This should be referenced by (Python), along with the version. You should write out the definition of the acronym CIVMR before using it. I believe it's (Center for In Vivo Microscopy), however, I am not sure where the 'R' comes from. This is based on the authors reference #8. A suggested word change: Maybe instead of using the word ""perfect"" in the last sentence of the 'Discussion' section, use the phrase ""best tool thus far""."	limited sample size	The novelty and application value in clinical corhort are the major factors.	The paper introduces a novel framework for understanding connectivity patterns in different levels of consciousness. It does a great job of thoroughly explaining the problem and explaining the approach taken to solve the problem. The method is reproducible.
265-Paper1987	Intervention & Interaction Federated Abnormality Detection with Noisy Clients	This paper proposed an approach based in Federated learning (FL) for abnormality detection with noisy clients. The novelty of the work lies in way of addressing the noisy input given by local model. It is very much desired in practice.  Experimental results are shown to show the improvement in the results.	This paper presents a causality-inspired method for federated abnormality detection with noisy clients and  designs a debiasing solution namely Intervention & Interaction FL framework  to alleviate the client confounder effect.  The experiments on class-conditional noise and instance-dependant noise demonstrate the efficacy of the proposed method	The paper presents a framework for abnormality detection in the setting of federated learning where a centralized global model is trained using decentralized, local data. The paper addresses the scenario where such a modeling scheme suffers from noisy labels across distributed clients, by using structural causal modeling (SCM) to identify the clients causing confounding bias. To resolve the bias, intervention and interaction approaches are used. Interaction adaptively estimates appropriate weights that balance local training status with global noise levels. Intervention uses these weights to shuffle and mix features from the local client and the global model to gradually reduce the detrimental effect of local noise.	The novelty of the work is in mixing the features/evidence provided by local models so that if there is any noisy evidence then it will be suppressed. The second important aspect is to consider share information across local model while training.  I	The proposed method is well motivated and it can be considered as the typical effort to leverage Federated learning to solve the problems  in computer aided diagnosis. The proposed method designs two different strategies to solve the main issue-recognition bias.  The experimental setting is designed reasonably and ablation studies and comparison experiments demonstrates the efficacy of the proposed method. Finally, the presentation is good and the readers easily follow the method.	The use of SCM in a federated setting for noisy clients is novel. By identifying and intervening on the causal factor (noisy client), the overall recognition bias is reduced. Unlike approaches where noisy labels are handled by robust losses, regularization, etc., this method intuitively adapts to the distributed learning paradigm. Instead of only improving the globally learned model, it naturally also identifies the confounder clients. This may help in a clinical setting by automatically identifying sites that may consistently differ in their labeling strategy or data quality. The approach addresses two problems simultaneously. First, how to use the global model to intervene and debias clients causing recognition bias. This in turn, improves the overall global model as well. Second, how to find the optimal way to intervene, by using an adaptive weighting scheme for each client that best reflects the current noise status, locally and globally. Extensive experiments have been shown, including comparison with several related approaches. The paper is well written and nicely condenses many aspects of the problem and method within the page limit.	Not much	The paper leverages two strategies to handle the applications of FL in abnormality detection.  The motivation is good, I appriciate this idea.  However, I consider the authors should provide more details of how to design and incorporite the strategies into FL model. Specifically,  model training is crucial to employ the proposed model,  more details and discussions are neccessary. For intance, what is the  added computional cost compared to the common FL model.	The following causality assumptions are a bit counter-intuitive and will benefit from clarification. Does the link between C (client) and M (image features) indicate that each client has its own direct impact on the 'locally' learned model (and therefore, features). That is, by changing the client, we induce a change in the model and therefore, M. Or, is M the 'global' feature set, affected by each client?  Shouldn't there also be a direct link between client C (e.g. hospital labeling the images) and the noisy label Y_tilde as changing C will have a direct effect on the noisy label assigned by C's diagnosis. This causal effect is independent of the fact that changing C, changes M, and therefore Y_tilde during prediction. If so, how will that affect the modularity of the SCM and the proposed do-calculus? It is not clear why equation 1 doesn't include P(Y_tilde|X). P(c|M) is also counter-intuitive to the direction of the causality between C and M. As per Sec 2.2, the stratification assumes that the global model is an approximation of the confounder set. So, the shuffling and mixing of features between global and local makes sense - using globally learned information to suppress local noise (equation 4). However, in this context, what is the specific advantage of extending the mixing across different images from the same local batch (M_c_hat_1 and M_c_hat_2)? Sect 2.4 says that all clients are used for the initial training. If so, the learned global model may still be biased by local clients' feature distributions. How will this impact the ability to generalize to unseen test clients with a distribution-shift in the feature distribution?	Looks promising	I didn't check it	The paper provides sufficient details for reproducibility. However, the authors have not clarified if the code will be released upon acceptance. Some details can be added, e.g. in instance dependent noise, the proportion of labels that were flipped based on the proposed criterion, statistical significance of the reported differences in mAP values.	It is a very good approach and computational complexity of approach has to be mentioned	The paper represents the FADN task as a structural causal model, and identify the main issue of  recognition bias. Specifically,  two novel strategies are proposed to handle the applications of FL in abnormality detection.  The motivation is reasonable, the extensive experiments designed on the benchmark datasets demonstrate the efficacy of the proposed method. To further improve the quality,  the authors should provide more details of how to design and incorporate the strategies into FL model. Specifically,  model training is crucial to employ the proposed model,  more details and discussions are necessary. For instance, what is the  added computational cost compared to the common FL model.  How to define the training objective and whether it can be trained more efficiently?	Ideas for an extended journal version of the paper: It will be interesting to see the effect of different data partition strategies used to distribute data among clients (in the training stage, where data is fed into both local and global models- Section 3.1). Can a smarter partition (such that the confounder client is pre-identified) mimic the advantage of the SCM and interaction steps? The authors may want to explore and clarify the difference in the source of noise- making the labels noisy versus making the underlying images noisy. While both will impact the final prediction accuracy, they may potentially affect the structural causal model's assumptions differently. They may alter the modularity or independence of each node via unobserved variables and add confounding association effects on top of causation effects. In the interaction strategy (equation 5), while a good regularization is achieved, any truly exceptional (but legitimate) data points might be missed and the overall differentiating power of the local client will be suppressed. In such cases, the lambda_local value may be low and the global features will take precedence in the mixing. To counter this, it will be good to discuss this in the context of larger training data sets, more clients, or introducing semantic counterfactuals to increase the variety picked up and learned by the model. In the ablation experiments, doesn't removing intervention effectively make the interaction redundant? If so, is this the same as a base federated model with no intervention-interaction component?	It is very well written paper and novelty of the work is explained very well.	The paper proposes a novel FL framework for  the applications of computer aided diagnosis and specifically designs the strategies to solve the issue of FL. The framework is reasonably designed and the results are also promising.	I liked reading this paper as it nicely establishes motivation, provides the required background and method details. While some assumptions may be weak or unclear, generally the proposed approach can encourage broader discussion and innovation on an important problem for the MICCAI community (learning from data while respecting data privacy and heterogeneity).
266-Paper1938	Intra-class Contrastive Learning Improves Computer Aided Diagnosis of Breast Cancer in Mammography	The paper describes an extension of the contrastive learning approach for cancer detection from multi-view mammography images. They propose using contrastive learning with a triplet loss within normal and lesion classes to improve the separability of the embedding space. Comparison experiments demonstrate the benefits of their proposed approach.	This paper proposed novel loss functions that consider the contrastive properties among lesion and normal cases. The proposed methods can effectively work with various latest multi-task learning frameworks.	In this paper, authors introduced a contrastive learning framework involving Lesion Contrastive Loss (LCL) and Normal Contrastive Loss (NCL) to improve the overall accuracy.	Well written detailed paper The extensions to the contrastive learning triplet loss to incorporate label-free information is a unique extension Thorough experiments and ablation studies.	This paper is well written and easy to follow. The technical part is sound and novel. Various latest learning strategies are included to validate the effectiveness of the proposed methods. The subgroup analysis is interesting and integrates with domain knowledge.	The paper compared the results with the recent approaches and experimentally provided evidences that the improvement is statistically significant. The formulation of LCL and NCL is intriguing and adapted from the domain knowledge. The results are provided on test dataset and they shows the merit of the proposed solution.	Some details are missing. Question over some of the choices in the approach.	The authors mentioned the cancer/benign/normal ratio in the in-house dataset, but in validation and test sets, the ratio is not the same (almost 1:1:1 instead). Why to design the validation and test sets like this way? This might introduce a distribution gap between the model and the actual distribution in the real screening scenario.	The evaluation on independent test set in a prospective manner is important for testing the clinical feasibility. However, this can be future work.	Ok. The dataset is in-house, so I don't know if it will be released. Otherwise, the implementation is a straightforward contrastive loss setup with a ResNet. That should be reproducible, but the heuristics of the weighting is not provided (see below in my detailed comments).	The reproducibility of the paper should not be an issue.	The experimentation is well written and i believe it can be reproducible	The marked lesion in Fig 1 is very hard to see. Please darken the contour to mark the region. How do the Hard Negative mining examples vary as the learning progress. For instance, as separability improves, is it correct to assume that the triplet loss margins improve and the harder to distinguish (like the benign cases experiment) becomes closer in terms of their l2 distances in the embedding space? Can the authors comment on what they observed? Authors use this LCL + NCL + classifier loss to improve their accuracy over the baseline when trained. But the details on how the overall weighted combination of the losses (i.e., were the losses weighted equally or something else) is missing. Further, during training, was there a training policy in place? I would assume one batch had lesion anchors, positive and negative, and the other would be normals? I would suggest the authors provide more information on that. I recognize that space is limited so it could go in the supplementary section. In table 3 in the supplementary material, were the differences statistically significantly different?	The authors can more clarify the usage description of the in-house dataset.	Overall, I congratulate authors for their work breast cancer, which is the leading cause of women deaths. The technique is well explained. The formulation of LCL and NCL is intriguing. The results presented in the paper shows the merits of this approach. However, The evaluation on independent test set in a prospective manner is important for testing the clinical feasibility.	Well written paper. Good application extenstion of contrastive learning with thorough experiments to back up their hypothesis.	Novelty Performance Dataset description Consideration of domain knowledge	The claims are substantiated with the provided results. The experimentation seems to be proper.
267-Paper1542	Invertible Sharpening Network for MRI Reconstruction Enhancement	This paper proposed an invertible sharpening network (InvSharpNet) that adapts a backward training streategy that learns a blurring transform from the fully-sampled MR images to the underssampled images to improve the visual quality of MR image reconstruction.	(1) This paper proposed an invertible network for MRI reconstruction enhancement to make the image sharper. (2) The experiments demonstrate that the proposed InvSharpNet can improve reconstruction sharpness.	An invertible sharpening network (InvSharpNet) is proposed to improve visual quality of reconstructed MRI images. The authors propose a backward training strategy to train InvSharpNet from the ground truth image to the reconstructed image, and the inference is in the opposite direction.	This paper newly adopted an invertible sharpening network for reconstructing undersampled MR images which could be different from conventional deep-learning-based MR image reconstruction methods.	(1) This proposed invertible network is a one-to-one mapping which relieves the ambiguity of the final reconstructed image. (2) The motivation of this work is clear and the method adopted is intuitive. (3) Experiments verify the effectiveness of the proposed method.	The idea of applying the invertible neural network (INN) to enhance undersampled MRI reconstruction is novel. The INN improves the visual quality of the reconstructed image, which is not fully discussed in the current related works. The reconstructed image using the InvSharpNet has a better qualitative results, as shown in Table 1. The experiments are comprehensive - evaluation on 3 datasets, comparison on SOTA algorithm and GAN-based method. Generalizability & network size are discussed, the Lipschitz constant experiments are included. The paper is easy to follow.	1) The proposed model lacks novelty. It seems that the proposed model was built by simply using pre-existed invertible networks. To highlight the novelty and demonstrate the effectiveness of the proposed model, more explicit explanations and rigorous experiments would be needed. 2) Although results of radiologists' evaluation showed better performance than the baseline (Table 1), it is difficult to see the performance increment in the presented figures (Fig. 2, 3). Also, quantitative evaluation metrics showed degraded performance compared to other models.	(1) There is already some work using invertible network for image denoise, for example[1] [1] Invertible Denoising Network: A Light Solution for Real Noise Removal, CVPR 2021.	-Quantitative results with the proposed methods is inferior to the comparison method, as shown in Table 2.  -Fig.1(a) is not clear, and the font is too small.	The authors provided details about the proposed models, datasets, and evaluation. However, the reference code was not provided and and does not seem to be released after the review process.	The dataset used in this paper is publicly available. This paper has the reproducibility.	The training details are missing. (optimizer, # of epochs etc.) The evaluation is on 2 public datasets and 1 private dataset.	1) The specific reasons for the increase in performance fore reconstructing MR images from undersampled data by replacing an existed network with an invertible network is unclear. More explicit explanations and rigorous experiments would be needed. In particular, it seems that the proposed model was built by simply changing the existing CNNs with a pre-existed invertible network. 2) The experiemtns were conducted only with cWGAN and InvSharpNet models and lacked comparison with other deep learning-based MR image reconstruction methods. To show the effectiveness of the invertible networks, more rigorous experiments would be needed by applying them to other deep learning-based MR reconstruction methods and compared with other methods. 3) Although adiologists' evaluation showed outperformed than the baseline in Table 1, it is hard to see the performance increment in Fig. 2 and 3. Especially, the difference between cWGAN and InvSharpNet seems to be very minor. In addition, the quantitative metrics that did not follow the trend of the radiologists' evaluation. Though the authors mentioned about this, but still it is not clear. The quantiative metrics are widely used for evaluating the reconstruction performance in various papers. Please provide more supporting results and discussion about this.	(1) Please fully investigate the relevant literatures and compare with them in experiments. (2) The generalization validation part is not very clear.	Overall it is an interesting paper, and discussion on the visual quality is an important topic. More insights on the InvSharpNet will strengthen this paper, for example: -More discussion and analysis of how is this different from a loss function is necessary. Given that additional training and parameters are needed, is the proposed methods overweight the loss function? (such as perceptual loss etc.)	"Even though the experiments showed interesting aspects with the proposed method, the overall opinion is ""weak reject"" because of the mentioned weaknesses and execution of the idea."	The motivation of this work is clear, the method adopted is intuitive, and the experimental verification is sufficient, so I recommend to accept.	It is a novel paper with extensive experiments. While the quantitative results is not superior, the presentation, analysis and discussion worth presenting in the conference.
268-Paper2202	Is a PET all you need? A multi-modal study for Alzheimer's disease using 3D CNNs	This paper proposes a framework for the systematic evaluation of multi-modal DNNs and critically re-evaluate single- and multi-modal DNNs based on FDG-PET and sMRI for binary healthy vs. AD, and three-way healthy/mild cognitive impairment/AD classification.	The author proposed to investigate the utility of multi-modal MRI + FDG PET for Alzheimer's disease classification using deep-learning. They conducted a robust comparison of this two modality and different fusion schemes.	This paper designs a framework for the systematic evaluation of multi-modal DNNs and critically re-evaluate single- and multi-modal DNNs based on FDG-PET and sMRI for binary healthy vs. AD, and three-way healthy/mild cognitive impairment/AD classification. This study conforms with the established clinical knowledge on AD biomarkers, but raises questions about the true benefit of multi-modal DNNs.	This paper demonstrate that a single-modality network using FDG-PET performs better than MRI and does not show improvement when combined. This paper gives an evaluation framework for multi-modal fusion to systematically assess the contribution of individual modalities.	Paper is well-written and hypothesis are clearly stated. Evaluation framework is adapted and results are convincing. Experiments present interesting framework that can be used by the community for a better use of multimodale stratregy.	(1) This paper re-evaluates single- and multi-modal DL models based on FDG-PET and structural MRI for classifying healthy vs. AD subjects.  (2) The experimental results show that FDG PET alone is sufficient for AD diagnosis, which conforms with established clinical knowledge about biomarkers in AD.	The theoretical novelty is very limited and the experiment is not innovative enough.	No novelty, beside the fact that the validation strategy is novel and validate previous findings.	(1) The authors design three fusion strategies to show the comparison results, and the results show that  FDG PET alone is sufficient for AD diagnosis. However, whether do the results rely on the current classification models? Therefore, more SOTA classification models should be investigated with only using FDG PET.  (2) The current comparison experiment is comducted on one dataset. To effectively the effectiveness of  FDG PET, it is recommended to emply on more datasets.	The reproducibility of the paper is not very well.	The description of method and validation framework provides moderate reproducibility.	The authors mention in reproducibility statement that they will release code and trained models after acceptance	1) MCI vs. NC is more challenging than AD vs. NC, why not conduct experiments on MCI vs.NC? And the paper lacks the description of MCI.  2) The experimental results from Table 2 (CN vs. MCI vs. AD) cannot completely conclude that a single-modality network using FDG-PET performs best. Is the opinion too arbitrary? 3) In terms of quantity, the data with three labels is not balanced, how do you solve this problems?	An argument against the presented findings is that a ResNet is not the correct model to learn structural pattern associated with AD. Indeed, current classification framework obtained better classification performance compared to results for AD classification using MRI readout only presented in this paper.	(1) The authors design three fusion strategies to show the comparison results, and the results show that  FDG PET alone is sufficient for AD diagnosis. However, whether do the results rely on the current classification models? Therefore, more SOTA classification models should be investigated with only using FDG PET.  (2) The current comparison experiment is comducted on one dataset. To effectively the effectiveness of  FDG PET, it is recommended to emply on more datasets. (3) When using PET and random MRI, the model still obtains promising performance. However, whether does the  random MRI introduce incorrect information? More discussions should be included.	The theoretical novelty is very limited and the experiment is not innovative enough. The experimental results from Table 2 (CN vs. MCI vs. AD) cannot completely conclude that a single-modality network using FDG-PET performs best.	The paper proposed an interesting framework and highlight the need for futur paper to justify the fusion of sMRI and PET data.  Paper is well-written and hypothesis are clearly stated, tested, and discussed.	The findings are interesting, but more validations should be conducted on more datasets.
269-Paper1711	iSegFormer: Interactive Segmentation via Transformers with Application to 3D Knee MR Images	The paper shows how to use a pretrained Transformer model for interactive image segmentation. It obtains a model that uses a small number of clicks to segment knee cartilage MRI images.	"This paper presents a novel deep learning architecture based on a vision transformer (ViT) to solve interactive MR image segmentation. Interactive image segmentation takes in user input in addition to the image itself: the user may click on the image with a ""positive click"" to indicate a region that should be segmented as foreground and with a ""negative click"" to indicate a region that should be segmented as background. While ViTs have been applied to non-interactive image segmentation a number of times in the literature, this is the first use of ViTs in interactive image segmentation as far as I can tell (this is also what the authors claim). The authors compare their model to state-of-the-art CNN-based architectures (known for lighter memory requirements than ViTs) on 2D MRI knee cartilage segmentation. The authors make efforts to design their ViT-based architecture in a memory-friendly way with a Swin Transformer encoder and a lightweight MLP-based decoder."	The authors proposed an interactive segmentation method using a memory-efficient method combining a Swin transformer with a lightweight multilayer perceptron decoder. They applied their method to interactive 2D medical image segmentation on the public OAI-ZIB dataset for the segmentation of knee cartilage on MRI. The authors claim their method's performance is superior to its CNN counterparts while achieving comparable computational efficiency. They further extended their transformer model to 3D interactive knee cartilage segmentation borrowing techniques from video analysis to extend 2D slice-based segmentations into 3D within the other previously unsegmented image slices.	Obtains good results in interactive 2D segmentation of knee cartilage MRI images. Obtains acceptable results in cross-domain evaluation.	In my opinion, the paper's greatest strengths are the novelty of the architecture used and the demonstration of what seem to be new state-of-the-art results using said architecture. The architecture uses simple building blocks proposed in other works (e.g., Swin Transformer blocks); however, the conscious effort to design a lightweight network that is memory efficient results in an effective tool for the task at hand. With a memory overhead nearly identical to state-of-the-art CNN-based models, their new architecture demonstrates improved 2D interactive knee cartilage MRI segmentation (see Table 1). The authors also do a good job of demonstrating their due diligence in architecture design. They demonstrate that other transformer-based backbones with similar numbers of parameters have a considerably larger memory overhead than their proposed architecture and worse speed. They also perform an ablation study investigating different pre-training datasets and fine-tuning configurations, comparing to CNN-based models, and demonstrate that their architecture demonstrates the best results overall across all configurations. Overall, their design of experiments seems to make sense. Finally, the extension to demonstrate how their results in 2D can be used to segment 3D images using pre-existing propagation techniques accurately wraps up the paper nicely and ties back to their ultimate goal of segmenting 3D images with limited training data.	Super-interesting approach, especially the use of video-based methods to extend into 3D (which makes conceptual sense) although this aspect is not the main contribution of the work according to the authors (section 3.4) Code was made available Public datasets were used The comparison between methods was fair in that the different models were trained on the same datasets and evaluated on the same datasets	The literature review is not written well enough to clarify how the proposed ISegFormer model differs from SegFormer[10]. This casts doubts on the method's novelty. The method overclaims interactive 3D segmentation but only offers 2D segmentation that is compared with the state of the art. The 3D segmentation, achieved through segmentation propagation, obtains only so-so results that are not compared with any state of the art.	Throughout the paper, a couple of methods are left under-explained, which hurts the reproducibility of the work (see comments regarding reproducibility below). However, this may have more to do with the authors' efforts to fit their work into the word/page limit. A few methods/details are explained in the supplementary material, but a couple still seem to be under-explained even with the supplementary material. In addition, in many of the tested tasks (across domains) the improvements in performance seem very incremental (non significant) and many of the tasks are more compute vision and not medical datasets so might not be as interesting to the MICCAI community (though that does not take away from the contribution of the work here).	There are no statistics supporting any claims of superior performance. Just because one number appears higher (or lower) than another, does not mean that this perceived difference is statistically significant. The manuscript lacks error estimates. Whenever a performance metric is reported, whether this is the number of clicks, Dice, etc etc, this should be done in the form of a mean value and an error estimate such as a 95% confidence interval or, at a minimum, a standard deviation. I am unsure of the clinical utility of having a method that requires, say, 20 clicks to obtain a satisfactory segmentation (85% IoU is high though, so if less stringent then this number of clicks would be less). In my area of research which involves 2D/3D lesion segmentation, the number of clicks required by a clinician is limited as much as possible to 1 (approximate center), 2 (2D bounding box), or 3 (3D bounding box). Please comment.	The authors have provided the code.	Overall, the reproducibility seems to be good. The authors use public datasets for training/evaluation of their models (e.g., the OAI-ZIB dataset), and their methods are reasonably well-explained. There are, however, a couple of training-related methods that seem to be under-explained, which could impact the reproducibility of their work, should a researcher try to reproduce their exact methods: There are a few questions that are unanswered about the automated click generation procedure during training/inference: (1) How are the clicks initialized for a given sample? (2) What if there are multiple false positive or false negative regions? Is a positive or negative click generated at the center of each distinct region? As someone who is unfamiliar with training interactive segmentation models but very familiar with training non-interactive training models, how are samples/batches fed into the network? Especially since you're adding clicks to false positive/false negative regions during training, I'm assuming you feed the same training sample into the network multiple times during the same epoch, with different click configurations. If this is the case, are you consecutively feeding the same training sample (with different click configurations), or do you interleave with other training samples/slices?	Code was made available and the datasets used are publicly available.	Some details shoudl be added on what specific pretrained Swin transformer has been used.  In page 6 the training set has 1521 images, which should not be correct since only 407 volumes are supposed to be used, with 3 images from each.	"There are a few places where acronyms are missing definitions or should have been defined earlier. For example, on page 2, it would have been nice to define ""STCN,"" and I believe the acronym for CNN (following paragraph) should have been defined earlier, as CNN was used in the second paragraph of the intro. In page 3, you assert that U-Nets are not memory-efficient due to their symmetric encoder-decoder architecture. While I understand what you're saying, I'm not sure that symmetry between the encoder and decoder is sufficient for a network not to be memory efficient (in theory, I could imagine there could be such a thing as a ""memory-efficient"" symmetric architecture). There are works in the literature that support your point (i.e., that networks with a smaller decoder may be more efficient) - perhaps you could elaborate on this or cite one of these works (e.g., Paszke et al.'s ENet from 2016). In Fig. 1, the arrow going around ""interaction loop"" seems to be going in the opposite direction of what is indicated by the rest of the flow diagram. Also, I believe there may be a typo in the iSegFormer architecture graphic, but I'm not sure. If this is for segmentation, shouldn't the output size be H x W x N_cls? (as opposed to H/4 x W/4 x N_cls) In the first paragraph of your experiments section, it is unclear why you choose to focus on cartilage segmentation exclusively instead of femur or tibia segmentation. I understand your reasoning for choosing one for a short conference paper, but you may want to justify this decision in a larger paper. I wasn't sure why slices per second were the metric of choice for speed instead of seconds per slice? Slices per second seem to imply that the process of loading new slices is implicated in the measurement of speed, although this seems like it would be independent of network architecture. If what we're trying to measure is the speed for the network to make a prediction on a single slice (I could be wrong), it seems like seconds per slice would be a more intuitive metric? The cross-domain evaluation experiment was interesting and felt a bit strange at first. I didn't expect a model trained exclusively on COCO+LVIS data to generalize to medical imaging, let alone three separate tasks. You may want to discuss your reasoning for conducting this experiment in more detail for a larger paper. Would we expect to encounter a scenario where we need to train a model on non-medical imaging data and translate it, without fine-tuning, to the medical imaging domain? On an unrelated note, I appreciated how you still included results demonstrating that CNN-based models generalized better in this experiment than the transformer-based models. In the cross-domain evaluation first paragraph, you say you trained models (in the previous experiment) using 1,221 slices. This seems to be at odds with your earlier statement that you used ""1521 training slices, 150 validation slices, and 150 testing slices."" Perhaps one of these numbers is a mistake? In a larger paper, I think your ablation experiment merits more discussion. The HRNet32 model seems to outperform the Swin-based models in several ablation configurations, although the Swin-based models seem to achieve the best results overall (pretrained on ImageNet-21K with fine-tuning)."	"Minor comments: Caption of table 1: say 'difficult cases' rather than 'hard cases' Table 1 shows performance for 2D slices in terms of number of clicks required to obtain a pre-determined level of performance in terms of IoU. Table 3 shows 3D segmentation performance using different #2D slices to start propagation into 3D. it is unclear to me, however, which 2D segmentations were used to start propagation; the ones with 85% IoU or the ones with 90% IoU, or even 2D 'ground truth' segmentations? How does the location of the 2D starting slices impact the end result of the 3D segmentation? Why use Dice to evaluate the 3D segmentations instead of IoU which was used earlier for 2D when determining #clicks? I'd like to see IoU (which I think is preferred over Dice but I know Dice is used extensively) along with Dice. Related to the clinical utility comment above under weaknesses, could you comment on ""how good is good enough"" for this segmentation problem? There usually is a substantial inter-observer variability when determining a reference standard for segmentation problems and it would be good to know how your method compares to the variability in the reference standard itself, e.g., if the average IoU for the 'ground truth' of 2 clinicians is only 0.6 then your requirement of 0.8 or more may be making things unnecessarily difficult."	Obtains good results in 2D segmentation but unconvincing in 3D segmentation.	The weaknesses were relatively few, mainly related to the under-explanation of specific training details. I think this under-explanation was likely the product of trying to fit their work into the page constraint, and I don't doubt that their results could be reproduced if these method details were elaborated on. Meanwhile, the novelty of their methods/architecture and demonstration of superior results (state-of-the-art using an architecture not used in this style of task) seem to have the potential to move the field forward.	I think this is an interesting approach but I'd like to see a bit more in the Discussion about envisioned clinical utility.
270-Paper0679	Joint Class-Affinity Loss Correction for Robust Medical Image Segmentation with Noisy Labels	The paper addresses medical image segmentation with noisy labels. It introduces a joint class-affinity segmentation model to consider both pixel-wise label correction and pair-wise pixel relations to reduce the label noise rate. A DAR is proposed for affinity reasoning and an affinity-based loss function is designed for regularization.	This paper introduces a new method for robust medical image segmentation under noisy labels. The core of the proposed method is a novel Joint Class-Affinity Segmentation (JCAS) framework, which takes into account both pixel-wise class and pairwise affinity supersivions. Specifically, to rectify the pixel-wise segmentation mask, a differentiated affinity reasoning (DAR) module is developed. To effectively train JCAS, a class-affinity loss correction (CALC) strategy is introduced to correct supervision signals. Experimental results show that the proposed method outperforms the previous works under both synthetic and real-world noisy labels on one dataset.	This paper presents a novel segmentation framework using noise labels in the surgical instrument dataset. The proposed method is constructed by the affinity representation learning in the inter-class and intra-class manners. Extensive experiments demonstrated the effectiveness of the proposed method by outperforming comparison methods under various types of noisy labels.	The idea of exploring the pair-wise pixel interdependencies for reducing noise rate sounds reasonable.  The experiments are extensive, and the results look good.	(1) This paper takes into account the pixel pairwise affinity relationships for dealing with noisy segmentation masks. This perspective is novel and interesting. (2) The proposed DAR module is novel and simple. (3) The Class-Affinity Consistency Regularization loss is novel. In particular, this loss is developed with theoretical rationale. (4) The proposed method achieves the best overall segmentation results compared to previous works. This demonstrates the advantages of the proposed designs. (5) The proposed method is well motivated. The use of pairwise affinity is reasonable.	The proposed Differentiated Affinity Reasoning and Class-Affinity Loss Correction modules are novel and effective. The proposed framework is indicated to be effective under various types of noise labels. Overall, the paper is well organized and clearly presented with nice illustration.	"My major concern to the paper is regarding the affinity in Sec. 2. It seems to me that P' is computed for each pair of pixels in an image, thus, it can measure both the intra-class and inter-class affinities. However, why is it claimed to only ""reveal the intra-class affinity relations"" in Sec. 2.1? It is also not clear how the reverse version  P_re' measures the inter-class affinity. The ablation study is not comprehensive. The necessities of intra-class and inter-class relation learning (Eqn.1) is not well investigated. The class-level loss correction and affinity-level loss correction are not respectively studied also."	(1) The writing of this paper needs to be improved. Some descriptions are unclear and somehow misleading. For example, the paper mentioned that the affinity map measures the similarity between two pixels and reveals the intra-class affinity relations. But it is unclear why this map captures the intra-class affinity relations. In other words, where do these relations appear? Does this map also capture inter-class relations? (2) Although it is demonstrated that the proposed CALC is effective in ablation study, it is unclear if the Class-Affinity Consistency Regularization is indeed effective. It is possible that only the class-level loss correction and affinity-level loss correct play roles in CALC. Additional ablation studies are necessary. (3) The class-level loss correction and affinity-level loss correction are not novel. They are from existing works. (4) The computation of pairwise affinity maps is quadratic, and thus could be expensive. In the experiment it is unclear if the pairwise affinity map would need a lot of computation and memory. Some statistical numbers would be helpful in illustrating this.	For the overall loss function, the effectiveness of using different weighting factors has not been discussed. Some references to the figures are incorrect. For example, in the last paragraph of Section 3, the Jac curves are shown in Fig. 5, instead of Fig. 4.	The algorithm is simple and should be easily to reproduced.	This paper provides some details on the method implementation. But such details still may not be sufficient to reproduce the method. It would be helpful if the authors would release their source code.	The experimental details are clear. Code is not available.	More detailed descriptions should be given to elaborate how P' is computed. Why does it only reveal intra-class relation? And how will its reverse version captures inter-class relations? More ablative experiments should be conducted in terms of without intra-class relation learning, without inter-class relation learning, without class-level loss correction, and without affinity-level loss correction. Since P' is normalized, it may be not necessary to normalize for P_re'. Fig. 2 should be improved. It is not easy to be recognized from the figure about how P' is derived. Eq.1 is very similar with GCNs. Does it perform in an iterative manner or just one-step inference?	"(1) There are typos in the paper. For example, in equation 1 the P(k1) should be P'(k1). On Page 7, ""results in Fig. 3"" -> ""results in Fig. 4"". On page 8, ""curves in Fig. 4"" -> ""curves in Fig. 5"". (2) Additional ablation studies are needed to illustrate the benefits of Class-Affinity Consistency Regularization. (3) The writing of the paper should be improved. For example, it is necessary to clarify the ""inter-class"" and ""intra-class"" concepts. Also, why would the affinity label reduce the noise rate (see Fig.1, page 2)? (4) For completeness, it is better to also compare the proposed method with [15] and [20] in the experiment."	Please include a computational complexity analysis of the proposed method. For the visualization comparison (Fig. 4 in the main paper and Fig. 1 in the supplementary material), it would be good to include some discussions on how the proposed method outperforms other comparison methods.	"The paper overall is easy to understand and the results also look good. However, some important details (especially for the affinity computation) are not clearly presented and the ablative experiment are not through, making it hard to evaluate the effectiveness of some essential designs. Thus, in the current stage, I will recommend the paper  ""weak reject""."	Overall I think the proposed method is novel, in terms of motivation and method design. The proposed method is demonstrated to be more effective on one medical image dataset (although the number of datasets used is limited). However, this paper still lacks some critical ablation studies to justify its technical contributions. The major factors I take into account for scoring: novelty and experimental evaluation.	The proposed method is novel and has been indicated to be effective under various types of noise labels.
271-Paper0179	Joint Graph Convolution for Analyzing Brain Structural and Functional Connectome	This paper proposes a joint graph convolution for both structural and functional brain connectomes.	The authors proposed a graph convolutional networks (GCN) based model to represent the nodal coupling strength between brain structural and functional network by adding learnable inter-network edges between corresponding brain regions. By employing individual MRI data of 662 participants with 5-fold cross-validation strategy, they showed that this model performs better in age and sex prediction task than previous SVM or GCN methods.	This paper proposes a joint graph convolutional neural network to combine brain structural and functional connectome for further application, such as age prediction or gender classification.	The jointly analysis of functional and structural barin connectomes might provide some insight for medical use. The method is simple and effective. The performance shows some improvement.	The authors designed a novel learnable inter-network edge model to capture coupling strength between SC and FC in different tasks such as age and sex prediction.	The main strength of the paper is coupling the structural and functional connectome with inter-network edges between corresponding brain regions.	The proposed method seems to be related to weighted matching. Related methods might be considered in the baselines or at least be discussed. There are large scale dataser for age predictions. 662 samples are considerably limited in size. The statistical significance between the proposed method and the baselines is not clearly identified. The ablation study seems be missing, e.g., fixing the matching using one-on-one matrix?	The coupling strength is changing according to prediction tasks. It is a results-oriented weighting rather than the intrinsic properties of brain connections. Lots of studies have shown the coupling between SC and FC follows a typical spatial pattern from primary cortex to association cortex. The authors only found a little similarity between their findings and the classical brain function gradients. This is hard to be interpreted.	1) lt looks like only the edges between the corresponding ROIs are included as the inter-network edges. The latent hypothesis that the functional and structural connection only limited to the corresponding ROIs is used without theorical support and may affect the performance of the model. 2) The experimental results were obtained from only one time of 5-fold cross-validation. The standard deviation of the results is not reported. 3) The correlation of the predicted and real age is kind of low (~0.38) comparing with the results in other cohorts (Infant: ~0.85, Adult:~0.89).	Code not submitted.	The reproducibility is basically good. The method is present clearly.	1) It is better to clearly describe how the baseline methods were implemented. For example, for the multi-view GCN in comparison, how many views were uses in the experiments is not described.  2) The range of hyper-parameters considered, method to select the best hyper-parameter configuration are not described. 3) 831 individuals (ages 12-21) were recruited in NCANDA dataset. Since there are only 662 were used in this study, it is better to explain the exclusion criteria in the paper. 4) The age distribution for the 1976 pairs of scans can be added.	Please refer to sec.5	1) The coupling strength seems show a similar pattern with a typical spatial pattern of structural strength. The authors should test the correlation between coupling strength and raw SC/FC strength across nodes. 2) Whether coupling strength is stable in adult brain network is still a question. The authors should use HCP data to reproduce the analysis. 3) The coupling strength should be corrected by a random label task. 4) The influence of network thresholding should be taken into account.	1) Please analyze why only the edges between the corresponding ROIs are included as the inter-network edges.  2) Since different partition of the data may lead to significant different average performance of the n-fold cross-validation, at least 5~10 times of n-fold cross-validation is recommend to get a relatively objective assessment of the method. It may be better to include the standard deviation of the cross-validation results in the comparison, not only the mean. 3) Please explain why the correlation of the predicted and real age is pretty low (~0.38) comparing with the results in other cohorts (Infant: ~0.85, Adult:~0.89). 4) Why the coupling strength between SC-DC learned in the gender prediction and age prediction is similar should be simply analyzed. It is interesting to know why the gender related-feature is highly correlated with the age related-feature (time-variant).	The method is interesting but the experimental settings need some improvement.	The author should prove that the coupling model still works in adult data and make a clear interpretation on the meaning of these task-driven coupling values.	The rationality is not well explained. The experimental result is not good enough to support the advantage of the proposed model.
272-Paper1464	Joint Modeling of Image and Label Statistics for Enhancing Model Generalizability of Medical Image Segmentation	The paper proposes a method referred to as Bayesian segmentation, or BayeSeg, for probabilistic image segmentation with the purpose of improving method generalizability. The method uses a probabilistic autoregressive model consisting of a combination of ResNets and UNet to decompose the input image into a basis and a contour. The contour is used to infer the segmentation, as it is argued to be more site and modality independent. The approach is tested for cross-sequence and site cardiac MRI segmentation with superior results to U-Net and the probabilistic U-Net.	"A deep learning based segmentation algorithm that decomposes shape (""contour"") and appearance (""basis"") in order to better generalize across imaging protocols, centers and population bias. A dedicated analysis on cardiac MRI data shows substantial improvements in terms of transfer learning (e.g. training on LGE MRI, testing on T2 MRI etc.) compared to vanilla u-nets and related state-of-the-art. While the overall idea may not be brand new (e.g. tested for multimodal registration https://arxiv.org/pdf/1903.09331.pdf), the detailed model appears to be novel and seems to provide a thorough derivation and convincing results."	The author proposed a  deep learning-based Bayesian segmentation framework that decomposes an input image into components of contour and basis. The segmentation label is then inferred from the contour component, which varies less across different MRI sequences. Unlike conventional Bayesian methods, the framework is implemented with neural networks, where three CNNs were trained to infer the posterior distributions. The author evaluated the proposed solution on public databases, significant improvement in segmentation accuracy was observed.	The approach tackles generalizability of deep learning segmentation models in medical imaging which is obviously relevant to the MICCAI community. As far as I am aware the approach, such as the use of image decomposition and modelling of image basis and contour to achieve more generalizable representation, is novel and the architectural choices in this make sense to me. The results are surprisingly good, with much smaller reductions in Dice scores when the method is applied to new modalities and datasets than the compared to approaches.	definitely an interesting topic to all the segmentation & learning community, particularly given the challenges with image appearance and leveraging multimodal data. thorough derivation convincing evaluation  results	I believe the proposed method is a novel approach and addresses an important challenge in the medical image segmentation field. The theoretical analysis of the framework design seems to be relatively thorough. And the use of CNNs for posterior distribution inference avoids some technical challenges that could be difficult for conventional Bayesian methods.  Quantitative evaluation of the proposed method was done properly.	While the language is fine, I find the paper hard to follow. I would have preferred more motivation and intuition in explaining the model details. This is understandable given the short MICCAI format, but the paper does repeat information in a number of places, so a more conscisely written manuscript could have had room for this. Only cardiac substructures and two problems - generalization to a different site and sequence is attempted. I would have preferred more experiments to see if this type of modelling is also useful for different modalities and structures.	A complex but strong pipeline - I don't see any significant weaknesses.	Even though the paper is relatively well written, I found some detail about the network training is missing, which could be helpful for the reader to understand the proposed method. The author used 3 CNNs to infer the posterior distributions. However, it is not clear whether the three networks were trained together in an end-to-end fashion (which seems to be what the text implies) or trained separately or in an alternating order with individual ground truth computed from ground-truth segmentation(which is easier for me to understand). I believe the latter is more plausible because, in an end-to-end setup, there is a lack of regulation to force the two ResNets to focus on either high-pass or low-pass components. Another missing detail is how to generate the final segmentation labels. As the U-Net outputs distributions, a post-processing step is needed to convert the distribution to segmentation labels. Finally, as the contour and basis decomposition remind me of the high-pass and low-pass filters, it might be good to include a baseline U-Net trained on high-pass filtered images to demonstrate the strength of the proposed method.	The reproducibility file mentions code will be available, but this is not mentioned in the paper. Given the relatively complicated modelling, code availability could be important for reproducibility.	the authors use public datasets, which is great code would be nice as the model is complex	Public image databases were used. However, some training details are missing.	Title is hard to make sense of before reading the paper. Bayesian segmentation or BayeSeg is a very general name. Consider coming up with something more specific that actually describes what the model does. The Introduction is repeated somewhat needlessly in the Methodoly section. I find what the image is decomposed into somewhat unclear. E.g. what is the basis and contour? Some intuitive explanation could help. Similarly with the graphical model, what is the line and boundary? What is the reasoning behind these variables? What is the matrix D_x in practice, can you give an example?	"Just a typo to correct on page 6: replace ""underwent cardiomyopathy."" with ""suffers from cardiomyopathy"". I don't have any other comments for this manuscript. For future work (beyond MICCAI), it would be nice to test on larger pools of data, to see whether these benefits still reproduce (or could be mitigated by brute force ""no data like more data"") :-)"	The author proposed a  deep learning-based Bayesian segmentation framework that decomposes an input image into components of contour and basis. The segmentation label is then inferred from the contour component, which varies less across different MRI sequences. Unlike conventional Bayesian methods, the framework is implemented with neural networks, where three CNNs were trained to infer the posterior distributions. The author evaluated the proposed solution on public databases, significant improvement in segmentation accuracy was observed. I believe the proposed method is a novel approach and addresses an important challenge in the medical image segmentation field. The theoretical analysis of the framework design seems to be relatively thorough. And the use of CNNs for posterior distribution inference avoids some technical challenges that could be difficult for conventional Bayesian methods.  Quantitative evaluation of the proposed method was done properly. Even though the paper is relatively well written, I found some detail about the network training is missing, which could be helpful for the reader to understand the proposed method. The author used 3 CNNs to infer the posterior distributions. However, it is not clear whether the three networks were trained together in an end-to-end fashion (which seems to be what the text implies) or trained separately or in an alternating order with individual ground truth computed from ground-truth segmentation(which is easier for me to understand). I believe the latter is more plausible because, in an end-to-end setup, there is a lack of regulation to force the two ResNets to focus on either high-pass or low-pass components. Another missing detail is how to generate the final segmentation labels. As the U-Net outputs distributions, a post-processing step is needed to convert the distribution to segmentation labels. Finally, as the contour and basis decomposition remind me of the high-pass and low-pass filters, it might be good to include a baseline U-Net trained on high-pass filtered images to demonstrate the strength of the proposed method.	I am impressed by the solid improvements in generalizability compared to methods which are otherwise the goto techniques in the field. While I have problems following all details of the technique, it appears the auth ors have had a good idea that is worth publishing.	Highly relevant to the community and a seemingly new model, thoroughly evaluated with convincing results.	I believe the proposed method is a novel approach and addresses an important challenge in the medical image segmentation field. The theoretical analysis of the framework design seems to be relatively thorough. Even though some training details are missing, the overall quality of the paper is good.
273-Paper1557	Joint Prediction of Meningioma Grade and Brain Invasion via Task-Aware Contrastive Learning	The authors developed and implemented a network to simultaneously predict two binary clinical values from MR image data. Those values were meningioma grade (low or high) and brain invasion (no or yes). The input was the image data from three types of MR acquisitions (T1 with contrast, FLAIR with contrast, and ADC calculated from MR DWI). Using MR images collected retrospectively from 800 studies, the authors trained and tested their proposed network and compared the results with other networks they also implemented using quantified metrics such as sensitivity, specificity and AUC. The proposed method had highest values for most measures, and if not highest it was second highest.	The authors develop an approach for multimodal multitask learning for meningioma grade and brain invasion classification. The proposed architecture accepts multimodal inputs to produce a common feature representation that is then deconvolved into common and task specific feature vectors. Furthermore, a contrastive loss module is imposed to improve task-specific feature representations and model predictions.	The goal of this paper is to present a novel model for joint prediction of meningioma grade and brain invasion from multi-model MRIs. A multi-task learning approach is proposed that derives task-common and task-specific features from a shared encoder. A contrastive learning strategy is used to align the task-common features for each task and enforce similarity between feature embeddings contributing to the same task. The method is evaluated on a private database of 800 multi-modal MRIs (T1, Flair, ADC). Dataset is imbalanced with most meningioma being low grade. Results are promising with some interesting AUC for both task although a bit lower for meningioma grade prediction.	The strengths of this paper include: Using image data typically collected in MR brain tumor protocols. A good number of studies (n=800) for training and testing for proof of concept. Use of disentanglement contrastive learning layers to split image features into common and specific features to improve the prediction, and verified with ablation testing.	The authors state that this is the first example of multitask learning for meningioma grade and brain invasion prediction. The proposed multitask framework is intriguing and improves model performance. The use of a task-aware contrastive loss is unique and lends itself to multitask learning. Sufficient ablation experiments are provided to support the proposed method for multi-task learning.	Clinical interest (it could avoid invasive assessment by biopsy). Simplicity of the method	"The weaknesses of this paper include: Why was the input limited to T1c, FLAIR-c, and ADC? The addition of T1 may add more information as it would help the network learn where contrast was up-taken. Given the span of time (5 years) and number of scanner models, it is unlikely that all 800 studies were performed under the ""same scanning parameters"". Should add acquisition protocol ranges (TE/TR, gradients for DWI, etc). Was there any check to make sure randomly drawn training and testing sets had similar distributions of low/high, invasion yes/no? How did all three runs have the same distribution if selected randomly. Only reported means of three training/testing sets, should also report ranges or SD. Only used cropped ROIs for input scaled to be the same size - what were the original sizes (rows x cols x slices)? [Maybe this is in the reference.] Tumor type must be known a priori before using the network to predict the grade and presence of invasion, something not usually verified until after biopsy or resection. Not sure mean AUC is a valid measure. Also, could do some sort of statistical testing between the quantitative metrics to test for significant differences (e.g., software from http://metz-roc.uchicago.edu could be used to compare the ROC curves from the proposed method to the other methods)."	It is not clear how the common task features are aligned to the task specific features as described on page 5. The justification and method for this alignment should be made clearer. Why are non-task-specific features aligned to a specific task? How is doing so preferable to an initial disentanglement into only task-specific representations without a common task representation? The authors do not have a validation data split for their experiments. This suggests the possibility that hyperparameters were chosen in a manner that would overfit on the testing data split. It is difficult to interpret the true improvement in performance the model provides. The improvements for each ablation experiment are incremental but small, and the model outperforms baselines only by the AUC metric in the invasion prediction task. The difference between other comparative methods and the authors' method should be clarified. Why is the authors' method better than others? What is the rationale behind the authors' method should be clear. The authors mention that they adapt MMoE. The authors should clearly explain the difference between their method and the MMoE method? Which part is different and why do they make the change? The technical novelty should be better described 'Moreover, the accuracy of brain invasion determination heavily depends on the clinician's experience'. -> reference supporting this claim? how are the three conv and avg pooling operations different? - not clear Paragraph before conclusion -> not clear if this improvement is statistically significant. In fact, the meningioma prediction performance decreases. Premature to draw such a conclusion based on these experiments alone. I have the same concern with subsequent addition of modules in the ablation experiments. Please perform a statistical significant test and report if these are indeed significant changes. This might just be a function of the dataset split	Rather weak statistical analysis does not really show that the proposed method if more adequate than simpler ones (Table 3) or other approaches (Table 2).	Given that the authors would release the code upon acceptance, the reproducibility of the algorithm is high especially if a trained version is released. It could be higher with release of the data set used but understandably there are probably IRB, HIPAA or other issues to allow that.	Code and data do not seem to be made available for this work. However, the network architecture is described in sufficient detail for replication if the additional description of how the task-common features are aligned to the specific tasks is included.	The method is well described and code will be published. Reproducibility is good even if the dataset is not public.	"This is very good work based on a fairly large data set from one institution. I think future work based on this might benefit from: validating with prospective data not in training and testing data. adding other brain tumor types the addition of rCBV calculated from MR DSC. If MR DSC was collected for these studies during contrast administration, then rCBV could be added as an input as it has is known to identify invasion in other brain tumors ]L. S. Hu et al., ""Accurate Patient-Specific Machine Learning Models of Glioblastoma Invasion Using Transfer Learning,"" AJNR Am J Neuroradiol, Feb. 2019, doi: 10.3174/ajnr.A5981."	There are instances in which the methods section could be made clearer. Most significantly, further rationalization for the contrastive loss and alignment of the task-common representation should be provided in addition to details on how that alignment is performed. Furthermore, a rephrasing or additional discussion of the purpose of the auxiliary classification loss should be added for clarity. In section 2.1, the authors mention that they adopt a convolution layer and an average pooling to realize feature disentanglement. What is the rationale behind this? In section 2.2, the authors mention that they align the task common feature to the task specific feature. My understanding for task common features and task specific features is that they dont intersected with each other. Why do the authors think the task specific features can be transformed from task common features? Can the authors justify this? If the task common features can transform into two different types of task specific features (one for invasion, one for grading), why don't we use different features in the beginning of the feature extraction step? My concern is that since we want to generate two task specific features, why do we need to generate them from task common features?	The paper is interesting from a clinical point of view and the proposed method is simple yet not trivial.  But the rather weak statistical analysis does not make the paper completely convincing. Generalization is evaluated by splitting the dataset randomly in training/validation three times (and only mean AUC on validation is reported). Repeated cross validation could have been used for this purpose. This makes the evaluation of the improvement of the proposed method over sota or simpler approaches difficult as the numbers (e.g. AUC) are quite close. Significance tests (e.g. DeLong's) could have been used. Some reported metrics like accuracy are not really relevant for such imbalanced datasets, other ones (MCC) could have been considered and confidence intervals should be given as well.	This is a strong paper and although I listed a number of weaknesses, the strengths greatly outweigh the weakness I have identified. The weaknesses are listed mostly a points that could be addressed to provide a journal paper.	The authors present a novel approach for multi-task learning and perform appropriate ablation and baseline experiments. Their results demonstrate overall improved performance, especially in the multi-task setting. There are some instances where their methodology could be further explained or justified, which should be easily amendable. The lack of a validation split may have implications on the generalizability of their results and subsequent performance on independent datasets. However, this is be expected to be uniform across all ablation experiments and baselines.	The proposed approach is novel and elegant and yet simple to implement. Yet the validation is too weak to be completely convincing. The benefit of adding task-common features and contrastive losses is not clear.
274-Paper2040	Joint Region-Attention and Multi-Scale Transformer for Microsatellite Instability Detection from Whole Slide Images in Gastrointestinal Cancer	In this paper, the author proposed a new transformer based approach for MSI detection, which is a WSI MIL problem.	The paper describes a method to detect Microsatellite Instability from whole slide images stained with HE, as well as from regions. The method uses an attention map to sample patches to get more predictive power and uses a transformer architecture to combine two levels of infomation, region level and patch level in forms of extracted features. The region level architecture is leveraged to build a slide level architecture by aggregation regions in a modified architecture.	A transformer based model was proposed to detect microsatellite instability status from while slide images, which outperforms existing patch-supervision methods on the gastrointestinal cancer data set from TCGA. To preserve representative features and remove noisy and redundant data, a feature weight sampling method was proposed.	The framework is reasonable. The result is promising.	The paper uses powerful ML techniques such as attention and transformer architectures to solve a challenging and relevant problem. The architecture proposed is sound and novel and has valuable contributions. The sampling method using attention map sounds like a good idea, and is shown to perform well. The combination of region level and patch level features using transformers is also a good idea and seems to perform well for this task.  The combination of auxiliar and primary losses is a positive addition and it is convincingly explained.  The aggregation of regions in the slide level architecture is also very valuable. The results show a superior performance compared to state of the art methods for MSI. The ablation study is convincing and shows the superiority of using slide level and the use of the sampling technique with the attention map. The paper is well written and the main concepts reasonably well explained. Fig 1 is very informative and informs visually very well about how the method is designed.	Well motivated proposed approach, good analysis of the problem at hand, latest deep learning based approach to solve problem, careful experiments and clear analysis of the results, comparison with state-of-the-art methods are given.	The method proposed is quite complicated and details of a critical step is missing: FWUS. I cannot find description of how the scores of FWUS is generated. Since transformer does not make any assumption on the correlation between instance, one major problem of adopting transformer is the demand of large amount and rich training data. However, for this task, the amount of training data seems to be limited to a few hundreds or thousands. Thus I am a little bit skeptical on the result.	Since my theoretical knowledege of transformers is not excellent, I find insufficient the details given in the paper regarding their use. For example, it is unknown to me what are the positional embeddings and what's the role of the class token. Either a brief explanation or some references would be appreciated. It is unclear the use of the thumbnail image in the whole slide level architecture. The magnification level of that image is not given.	Nothing	Code will be made available. Experiment is based on public dataset.	Details on the transformer implementation, and the classifiers used, are not well detailed in the paper or suplementary material as well as other modules shown in Fig 1 (MLP, CNN, attention module).  However, this is less relevant since the authors are providing the code.	Datasets and experimental setup are clearly mentioned.	It will be nice if the author can visualized the token attentions on top of the WSI image to show what has been attend to by transformer.  Cropping patch from image region and extract CNN features is quite intuitive and standard. I would suggest authors simplify the method in this part and give more details on other aspects such as how Epos is designed - is it in WSI space or region image space? Is Epos different between x20, x5, and thumbnail? If so, how? Section 2.2 is kind of duplicated to 2.1. It can be simplified or combined with 2.1 to make more room for other more critical contents. Minor:  Some of the references seem to be missing in the paper, for example: ... state-of-the-art method DeepSMILE [] by 0.18%. ... MSI detection methods for comparison []. Typos: ... where c is the number of feature map channels..	I appreciate the work done by authors in this paper, and I think it is in general a solid work. The authors should clarify better the classification goal of the method (number of classes): my understanding is that it is a binary classification problem positive or negative for MSI, which I beleive is not clearly stated in the paper. There are repetitive sentences in the paper that can be removed, for example the showt summary of RAMST is reapeted in the abstract, the introduction (page 2 last paragraph), introduction again (page 3 second paragraph), method last paragraph Page 4 line before eq (1), M'{fwus} should be X^P{fwus}	Nothing	Reasonable method with some novelty and performance gain. Some details are missing but can be fixed.	I recommend acceptance of the paper given the novelty of the method, the convincing results and the proper explanations in the text. The weaknesses I see are mainly regarding to lack of details in some sections so I see way more strenghts than weaknesses and I think the paper adds value to the MICCAI community.	Main strengths of the paper.
275-Paper1974	Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification	This paper presents a novel framework for WSI classification. Its main contribution comes from: 1) leverage the spatial relationship of the patches and kernels, some representative points,  2) introducing a novel module of ViT.	This paper proposed a kernel attention transformer (KAT) for WSI classification. The information transmission of the tokens is achieved by cross-attention between tokens and a set of kernels related to the spatial relationship of the tokens on the WSI. KAT was evaluated on a gastric dataset with 2k WSIs and an endometrial dataset with 2.5K WSIs. Results have shown the proposed KAT is effective and efficient in the WSI classification and is superior to the state-of-the-art methods.	The paper proposes the Kernel Attention Transformer for classification of WSIs, which builds on the promise of ViTs whilst addressing some of the issues they face when confronted with WSIs. It also provided details on the pipeline into which this network fits.	Strengths: 1). constructs an abstract concept over the patch. 2). proposes the kernel attention module, its main novelties  comes from: (a) reduce computational cost by replacing patch-to-patch attention with patch-to-anchor attention, (b) introducing  hierarchical context information by adjusting N, a hyperparameter which can controls the scope of calculating the self attention.	A novel transformer named kernel attention transformer (KAT) is proposed. It can describe hierarchical context information of the local regions of the WSI and thereby is more effective. A very large (~ 5K WSIs) dataset is collected for experiments. Results have shown the effectiveness of the proposed KAT.	The paper is well-written and the methods clearly described with all details provided.  The authors have included computational and memory requirements, which is a useful addition. There is a thorough and fair appraisal of the method compared to SOTA and, although no confidence intervals are provided, the method's value is clearly shown. The method draws upon a number of already high-pedigree methods (ViT, efficientNet etc.) but adds credible novelty by combining them with other techniques such as Kmeans to compute the anchor points for the weighting masks.	1). In 2.1, I'm confused as to why Foreground needs to be used in both the Tiled WSI and the Feature matrix. That means, in my opinion,  if you have divided the tissue region into patches based on the mask, there are only patches of the tissue region. So you need not extract the patches again. 2). In Fig.2.,  you probably could explain what the T operation is. 3). It is unclear whether your results have statistically significant. 4). This paper is interesting. But why not perform experiments on another public datasets (e.g., Camelyon16) to prove its universal effectiveness?	One main issue is the lack of clear details to present motivations. The motivation of using kernel attention in transformer is not very clear. KAT cannot outperform baselines in large margins. The claim about the effectiveness and efficiency of the KAT is challenged. Some representative WSI models are not compared.	Very few. If I had to find one, perhaps it would be the arguably incremental nature of the innovation. With some of the metrics showing quite marginal gains, it would be useful to see confidence intervals for these figures.	There are some confusion in Pre-processing and Data preparation.  It would be better if you could explain it in more detail or give the pseudocode in supplementary.	Not clear if codes will be available.	No code availability is mentioned and the training dataset is not explicitly mentioned. The architecture is well-described but it would be difficult to reproduce the results from this paper alone.	Please see 5.	"The main contribution is to proposed a kernel attention transformer. However, such motivation is not clearly presented in the paper. The authors claimed that ""It makes the KAT be able to learn hierarchical representations from the local to global scale of the WSI, and thereby delivers better WSI classification performance."", but we could not understand such novelty in the Fig. 1 to see why such kernel attention is useful for WSI classification. Also, the authors didn't explain Fig. 1 clearly in the methodology section. For example, how to choose the anchors on the WSI and how to perform soft-masking and generate anchor-related masks shown in Fig. 1(g). In table 2, we could see KAT cannot perform better in Gastric-2K dataset compared with TransMIL in terms of accuracy, sensitivity and specificity in different tasks. Also, it is not clear if results in Endometrial-2K datasets are significant better than baselines. Also, the proposed KAT is not the fastest method and thus it is not easy to understand how the KAT could be efficient and effective. Std values should be reported as the authors said they used 5 fold cross-validation. ROC curves should also be presented and statistical tests could be done to test curves of each model. Attention-based MIL model should be discussed and compared because they have been widely used in WSI classification and diagnosis.  ""Data-efficient and weakly supervised computational pathology on whole-slide images."", Nature BME, 2021. ""Whole slide images based cancer survival prediction using attention guided deep multiple instance learning networks."" Medical Image Analysis, 2020."	"""is the most appreciate for the dataset."" - ""appreciate"" should be ""optimal""? Very little worth adding - this is a great paper and I won't waste our time trying to find any further niggles."	1) It handles the spatial relationship between patch-level  and WSI-level  information very well. 2) And, this paper implements hierarchical context information of the local regions in an innovative way. I vote for weak accept because the authors not implement their method on mainstream dataset like Camelyon16.	Though this paper has novel kernel attention transformer, motivations are not very clear and presented in the paper. The authors should emphasize why their framework could be very suitable for WSI classification. Otherwise, it just used a fancy technique. Experiments cannot support the claims that the proposed model is more effective and efficient than baseline models as we can see in some cases the proposed KAT cannot perform better than TransMIL or Nystromformer.	The paper is really well written, the results are strong and the discussion about other methods demonstrates an awareness and respect for the latest advances in the field. Although there is little in the way of architectural or algorithmic details, this is clearly a strong piece of work that deserves a place at MICCAI.
276-Paper0419	Key-frame Guided Network for Thyroid Nodule Recognition using Ultrasound Videos	The paper investigates akey-frame guided video classification model for thyroid nodule recognition and diagnosis. The overall framework contains two parts, the first part is for key-frame localization. In this part, a detection-localization network (based on Faster-RCNN) is trained to localize the frames with clinically typical thyroid nodules in dynamic ultrasound videos. The second part is a ultrasound video classification network (based on lightweight 3D convNet) for thyroid nodule classification/diagnosis. By making use of the adjacent N(N=32) frames of a ultrasound video, the video classification network can take the advantage of temporal information for a more precise classification. The authors have collected over 3000 clinical thyroid ultrasound videos labelled by three radiologists for the experiments.	The manuscript represents an automated localization approach for the key frame identification in thyroid US videos combined with a motion attention module in order to have a more focus on the significant frame.	The paper first proposes a detection-localization framework to automatically identify the clinical key-frames with typical nodules in each ultrasound video. Based on the localized key-frames, the authors develop a keyframe guided video classification model for thyroid nodule recognition. Besides, the authors introduce motion attention module to help network focus on significant frames in an ultrasound video, which is consistent with clinical diagnosis.	1.The idea of making use of ultrasound video clips to guide the CAD classification task. A video clip is the successive 32 frames centered by a key-frame which have been detected to contain clinically typical nodules. 2.There are ablation experiments to evaluate the effectiveness of each network module: the usage of keyframes, the motion attention, and the 3D SPP.	A novel framework for thyroid nodule recognition using US video. The proposed method is addressing an important clinical problem.	This paper, according to the actual clinical situation, extends the deep learning based thyroid nodule recognition method to the common B-scan ultrasound videos. The proposed method with motion attention mechanism achieves higher classification performance on self-collected dataset as compared to baseline methods, verifying the effectiveness of the method.	"The manuscript has given a brief review thyriod nodule classification method in the Introduction Section. There have been publications on ultrasound video-guided CAD method on detecting other organ/tissue. It is therefore suggested to also discuss the relative video-guided CAD works such as [U-LanD]: [U-LanD] M. H. Jafari et al., ""U-LanD: Uncertainty-Driven Video Landmark Detection,"" in IEEE Transactions on Medical Imaging, vol. 41, no. 4, pp. 793-804, April 2022, doi: 10.1109/TMI.2021.3123547."	No information regarding the computation time is provided. There is no discussion on the feasibility of implementing the proposed approach for real-time clinical application.	The Motion Attention module only has incremental innovation as compared to the similar temporal attention module in [2], using motion speed to replace video brightness as the radiologists' attention indicator. Because the baseline methods may preform worse on the self-collected dataset, additional experiments should be conducted to compare the baseline methods with the proposed method on other datasets, e.g., the used datasets reported in the baseline method papers. The authors should provide a complete description of the data collection process, such as descriptions of the experimental setup, device(s) used, image acquisition parameters, subjects/objects involved, instructions to annotators, and methods for quality control.	The deep network structures used have been described in details, including the number of layers, the kernel size of each layer, etc.  The experiments is conducted on a self collected ultrasound dataset with 3000 videos.  The Reproductivity report shows that the model and codes will be released if this paper is accepted.	A clear explanation of the implementation steps is provided. A private dataset is used for the evaluation and no link is provided to access the dataset. Moreover, no information about the data collection condition (equipment, ...) is provided.	The authors should provide a complete description of the data collection process, such as descriptions of the experimental setup, device(s) used, image acquisition parameters, subjects/objects involved, instructions to annotators, and methods for quality control.	1.The number of frames to be taken around a keyframe has been empirically selected as 32. It is recommended to give some explanation on this choice, in terms of the computational cost, performance, etc.  Besides, I am curious what if the difference of two detected keyframes frame index are no more than 32? Would there need to take some actions to handle such cases.  2.As illustrated in Figure 1, frame-index of the detected nodules is hard-encoded as a feature component of the nodule information. Is this kind of hard-encoding suitable? Would the variation of ultrasound video lengths effect this feature component?	Section 2, Key-frame localization: As the Faster-RCNN is not introduced before, an explanation about the main characteristics of this network and why you select it as your detection model is missing in the manuscript. Also, it is not clear if you adopt it to your aplication or if you used it as it is proposed. Please clarify this point. Please explain what IOU similarity is. Section 3, Dataset: Please clarify how the dataset's size has changed after data augmentation. Section 3, Experimental results, Table 1: It is not clear if the listed state-of-the-art methods also used the same dataset as you or not. please clarify this. Section 4, Conclusion: As there is no information regarding the computation time, it is difficult to evaluate the performance of the proposed approach in the context of real-time application inside clinical settings. I highly recommend you to add a discussion regarding this point and also focus more on the clinical significance of the proposed approach.	The Motion Attention module only has incremental innovation as compared to the similar temporal attention module in [2], using motion speed to replace video brightness as the radiologists' attention indicator. Because the baseline methods may preform worse on the self-collected dataset, additional experiments should be conducted to compare the baseline methods with the proposed method on other datasets, e.g., the used datasets reported in the baseline method papers. The authors should provide a complete description of the data collection process, such as descriptions of the experimental setup, device(s) used, image acquisition parameters, subjects/objects involved, instructions to annotators, and methods for quality control.	The paper is well-written, easy to follow and have practical value. The ablation experiments are solid.	The manuscript propose a  novel framework for thyroid nodule detection in US videos that can act as a CAD in the clinical settings. However, more evaluation and adjustement is necessary.	The paper presents limited innovation as compared with paper [2], by using a similar attention module and changing the imaging method from CEUS to B-scan US.
277-Paper0584	Knowledge Distillation to Ensemble Global and Interpretable Prototype-based Mammogram Classification Models	This work proposes and describes network that adds interpretability to a global model by assembling it with a prototype-based model. The proposed approach was tested in their own database and in a public available database. Results are similar to state-of-the art approach with the advantage of interpretability results.	To integrate the advantages of accurate global models and interpretable prototype-based models, the proposed ProtoPNet++ distills the knowledge from the global model to the prototype-based model. The performance and interpretability of the proposed ProtoPNet++ are valiated on private and public datasets.	This paper proposed ProtoPNet++, which ensemble a global model with a prototype-based model for mammogram classification tasks (cancer/no cancer). Such combination is claimed to be both more accurate than the baselines and can provide more interpretability.	A new proposition for ProtoPNet. - Interpretable results in the network.	The proposed method is simple but effective to improve the model performance and interpretability. The paper is well organized and description of the motivation is clear. The experimental results supports the validity of the proposed method.	There are three major contributions of this work (1) adding interpretability; (2) higher accuracy benefited by ensemble learning and (3) improve performance by introducing diversified training strategy. The research topic is highly interesting, since the interpretation of deep learning models have been a heated topic in recent years. The authors implement knowledge distillation from GlobalNet to ProtoPNet through minimizing KD loss, forcing the networks produce similar results on the same sample. The training signal, i.e. loss functions are well designed, with thoughtful considerations of margins/diversity/over-fitting prevention. The studied dateset, though private, is relatively well-sized. What interests me most is the activation maps that can visualize the cancer localisation across different methods, which allows deep model interpretation.	The accuracy improvement is insignificant.	Maybe limited by the paper length, this paper lacks of important ablation study regarding the hyper-parameters, such as the hyper-parameters in Eq. (1)-(3) and the number of prototypes.	"One issue unclear to me is in section 2.1: ""The prototype layer has M learnable class-representative prototypes....with M/2 prototypes for each class"". In this paper, the label space is only binary classification: with cancer or without cancer. Could the authors clarify on this for their specific application? Since there are multiple loss functions used to update the parameters, it should be essential to show the loss curve and evaluate how they change by the training epochs. In figure 4, there seems to be more than one activation on the ProtoPNet++ KD, which is less concentrated and accurate than the mere ProtoPNet. Is there any reasoning on this phenomenon? For the experimental sections, the authors did not report any statistical tests regarding the performance, thus we may not know whether there is a significant improvement."	Results presented in the paper are reproduceable.	-The authors declares they will share all the codes for the experiement when accepted. The authors also presented most of the relevant setting parameters for the expriments.	In the reproducibility checklist, the authors claim to release the training/evaluation codes with pretrained models which is a plus. Since the proposed method is a sophisticated ensemble learning framework with multiple loss functions, source code release could be a great help.	In my opinion this is an excellent work and I do not have any suggestion for improvement.	Although this paper is well-written and efficient, there lacks of important ablation study regarding the hyper-parameters, such as the hyper-parameters in Eq. (1)-(3) and the number of prototypes. I understand such experiments might be missing due to length constraint of the MICCAI conference. However, I think it would be better if the authors could provide more detailed ablation studies in the supplementary file.	I would encourage the authors pay attention to the PDF formats. The authors include several weight parameters (alpha, beta, lambda 1/2), it should also be critical to show the hyper-parameter tuning analysis. Loss curves for each term could also be favorable. The current project is a binary classification problem. The authors could consider studying a multi-category classification task and see how the ProtoPNet++ perform, especially on some public datasets competitions (CheXpert, ROCC etc.).	In my opinion this is an excellent work and I do not have any suggestion for improvement. The improvement of ProtoPNet is very welcomed.	This paper is well-written and easy to read. The motivaion and method are clearly demonstrated. The experimental results can support the method.	The paper is of high quality in writing and reasoning.
278-Paper2211	Landmark-free Statistical Shape Modeling via Neural Flow Deformations	This paper propose a novel shape model based on continuous neural flows, which produce natural shape deformations without relying on dense correspondence between training shapes	This paper proposes a novel shape space representation based on diffeomorphic deformation of templates parameterized by the PCA codes of the latent space of a MLP. The authors show on 3 examples that this shape space is expressive and is suitable to discriminate between healthy and pathological cases. The representation generalizes the shapeflow approach by considering multiscale deformations through the addition of  a local latent representation	Authors present a novel method for shape modeling based on neural flow deformations.	flow parametrization and latent representation	The paper is well written It introduces a novel representation of shape spaces (compared to shapeflow) by adding a local neural flow deformer to a global one. This local NFD is based on RBF with fixed control points. The authors provide examples of their approach on 2 datasets liver and femur and show the discriminative power of the latent space on one test case. The author provide a comparison with 3 other techniques one of them being also based on LDDMM. They show that their technique leads to improved results.	The proposal is based on a continuous model of a velocity field in 3D. The differential equation is solved via a parametrization model solved by a flow net. Results seem correct and applications to shape reconstruction, segmentation and classification.	the comparative experiment is lack.	The novelty introduced in this paper is somewhat limited to the i) addition of the local NFD compared to the shapeflow approach and ii) the use of PCA modes in the latent space instead of the whole latent space. The authors do not compare their approach with that of shapeflow from which it is based on. It is therefore difficult to check whether the additional shape space refinement are important or not. The proposed approach for NFD is fairly complicated as it corresponds to the PCA modes of the control points of an RBF function describing a latent space which is itself parameterizing a velocity flow. The authors do not  provide a lot of implementation details. For instance, they do not describe the optimization technique to perform inference of the shape  representation.  Why choose an encoder free approach ?	The mathematical method is described very superficially. It is difficult to check correctness and novelty. The idea of solving a differential equation by parametrization, using a neural network is not novel, but the application to shape modeling is interesting.	Yes	Many implementation details are missing (see above) such as : the number of RBF basis considered, the optimization methods, the number of PCA modes considered and how this number was chosen, the number of sampling points...	It is very difficult to reproduce the paper because of its shortness explaining the method. The supplement is not enough either for this purpose.	The innovation of the paper is weak How about the time complexity about your model compared other methods. Comparative experiments are slightly inadequate There are many expressions in the full text that I cannot understand Such as the last two lines in page 7, and so on. Pls revise the manuscript carefully.	To improve their paper, the authors should i) compare their approach with shapeflow ii) detail more the implementation parameters of their method iii) the computation times of the method	Authors should provide more details that allow fully reproduction of the method. The method is very well described in a general overview. It is elegant, but more details are needed to fully undersand the math background.	The experiment results may be satisfying although the experiment is not sufficient.	Albeit its complexity and the lack of implementation details, the introduced shape space is of high interest for the community.	The apllication to shape modeling is interesting. It is difficult to judge novelty beacuse of the shortness of the paper, but the proposal is interesting. I think the paper would turn into to a good topic for discussion in the conference.
279-Paper1397	Layer Ensembles: A Single-Pass Uncertainty Estimation in Deep Learning for Segmentation	The authors proposed an uncertainty quantification method that could estimate the segmentation uncertainty in a single pass. They conducted experiments on two datasets and compared the proposed algorithm with the deep ensemble method.	The paper proposes a method for lowering the cost of uncertainty estimation methods that are based on network weight sampling by introducing layer ensembles. Therefore, instead of individual networks an ensemble can be built from a single networks' different depths. Given the high sampling cost of state of the art uncertainty method of standard ensembles, the work is very valuable.	This paper proposed a new measure of uncertainty to evaluate segmentation on image level.	Most of the sections of the paper are well written, which are easy to follow. The idea of using layer ensemble to estimate uncertainty is interesting.	The paper is well written and easy to understand. The speed up of ensembles for uncertainty estimation is highly relevant given that it is frequently used and the state-of-the-art in the field. The motivation of the method is clear and well explained. The results are promising.	-The uncertainty assessment only requires single-pass testing -The idea of ensembling multiple layer output is newly proposed -The method is validated in one 2D dataset and one 3D dataset	The computational performance analysis is desired to show its computational advantages over Deep Ensembles. Unfair to only apply Simultaneous Truth And Performance Level Estimation (STAPLE) on Layer Ensemble but not on Deep Ensemble in the quantitative evaluation.	"the new metric is definitely on point to analyse the task at hand and yields good metric to compare different methods; however, it is not the first image-level metric of this kind for uncertainty estimation. This claim needs to be refined. See the area under sparsification error curve introduced in ""Uncertainty estimates and multi-hypotheses networks for optical flow"". epistemic uncertainty is not predictive it is rather empirically computed, this needs to be corrected throughout the manuscript. advantages of the method over multi-headed networks is not clear given that multi-headed networks are also light weight since they add small overhead at the late most layers. See the mentioned paper up. Authors highly correlate easiness of a samle to high confidence. I wonder if this is a general mistake in the community of uncertainty estimation. I believe not all easy samples are supposed to yield low uncertainty as in easy samples that might come in the test scenarios but totally new to the network since it has not seen such an image before (sample novelty). Correlating it to error in my opinion is a better termed way. I believe the comparison of the methods should have equal number of ensemble members for a fairer comparison. The idea in the paper is very novel and worth to be tested in different scenarios including big natural image benchmarks where the standard ensembles are still dominating. For me it would be interesting to see a comparison in this regard to see how the method generalizes to different domains. comparison to multi-headed network is missing in tables. Especially in runtimes since i believe that building ensembles on parts of the networks still might take some time than one forward pass of multi-headed network."	-The rationale of using multi-layer output needs further justification. Different from deep ensembling that only uses estimation from the last layer of each network, the proposed method considered the estimation from internal layers. Notice that internal layers are classifier determined by low-level (or low scale) features. This method considered low-level feature the same weight as high-level feature in determining a segmentation confidence or uncertainty. It is still questionable how much the low-level feature can be used to determine uncertainty. As shown in the experiment with more difficulty segmentation, more layers are needed. This can be a potential limitation for extremely challenging segmentation task. Significance of image level uncertainty. As the uncertainty evaluation is mostly used to highlight challenging region, it is not clear what is the specific application of image-level uncertainty.	Seems reproducible	the paper is reproducible to a large extent.	The data is based on publicly available data but code will be disclosed.	The computational analysis (such as number of floating point operations, computation time, and speedup) is desired to show the efficiency of LE vs DE. It's unfair to only apply STAPLE on LE. In Methodology/Ensembles of networks of different depths part, the authors claim that LE is equivalent to DE, which is not very solid. More evidence or theories are needed to verify this claim.	The idea presented in the paper is very interesting and finally challenges the standard ensembles which are state-of-the-art for uncertainty estimation for segmentation. For this reason, in order to show the benefit of the proposed approach in a strong way, i would like to see more comprehensive compariosns to multi-headed networks as well as runtime comparisons. This way the paper can be strengthened.	Other noise corruption in segmentation difficulty evaluation. In addition to Gaussian noise, the evaluation could be further evaluated on convolution with a filter kernel or intentionally corrupt the boundary region to mimic more challenging segmentation scenario with lower uncertainty. As another the-state-of-the-art, it will be great to also compare with MCDropout. The randomisation of data in testing can be improved by cross validation. In MnM result of Fig.3, there is a range (0 to 0.1) where DE-LV is lower than LE-LV. It will be great to add more discussion or justification on this issue.	The idea of using layer ensemble for uncertainty qualification is interesting.	Although some comparisons are missing, i believe that the work as is, is very insightful for the MICCAI community for future research direction in this relevant field of uncertainty estimation.	The justification of using low level feature to determine uncertainty is the major factor that impacts the overall score.
280-Paper1555	Learn to Ignore: Domain Adaptation for Multi-Site MRI Analysis	This paper proposes a supervised domain adaptation approach for classification of Multiple sclerosis patients and healthy controls on MRI scans. The target dataset contains images from one scanner type only, whereas the source data contains different scanner types. The authors propose to learn cluster centers based on the target data, and force latent vectors from the target and source data to be close to those centers. The approach outperforms several domain adaptation and contrastive learning approaches on the target domain and keeps good performance on the source domain.	The authors formulate a new domain adaptation paradigm where diseased and healthy participants come from different studies in the source domain. They propose a method to enforce the latent codes from different classes to be far from each other whereas the codes within each category close to each other by introducing a center point loss and a latent loss. Essentially, the losses are controlled by two parameters: the radius of each class sphere and the distance between the clusters. In an experiment of multiple sclerosis classification, the proposed method outperforms the other domain adaptation methods on target domain when training on both source and target domains.	The authors propose a novel domain adaptation/harmonization method for MRI acquired by different scanners. Adaptation is facilitated by adding specific constraints on the latent space of the MRIs to reduce the domain shift caused by scanner differences. Experiments on several MR datasets demonstrate the effectiveness of the proposed method.	the method is simple but elegant and outperforms baselines and state of the art approaches on the target domain by a large margin the approach is compared to multiple baseline and state of the art approaches the setting is clinically relevant, as data collected in a hospital is often from the same scanner	The paper is trying to tackle an interesting problem: how to generalize a classification model when the training data of each class comes from different studies.	[1] The research topic is of practical value for MR image adaptation/harmonization for dealing with the between-site variations caused by scanner differences. [2] The authors have clearly presented their motivation and insights for the model design. The overall architecture is appropriate. [3] The extensive experiment on diverse MR datasets is appreciated. The comparison with several state-of-the-art or classic adaptation methods makes the results solid. [4] The authors released the source which is helpful to reproduce the proposed method and the experimental results.	the method is restricted to applications where the target domain is labeled and source data is available, meaning a trained model can't easily be finetuned on a new domain when source data is not available anymore.	The proposed method is complex. I wonder how you can generalize it to multi-way classification? There are many hyper-parameters introduced in the proposed method, such as d, r, lambda_cls, lambda_latent, and lambda_cen. The authors need to include hyper-parameter search results in the paper; There is a flaw in the experimental design. Why do you train on both source and target domains? Then, why do you call your method a domain adaptation method? Here you have nothing to adapt to. And why do you bother to compare with those domain adaptation methods? They are not designed to train on both source and target domains;	[1] As ones of the key parameters, Lambda_cls in Eq (1) is not necessary. The other two parameters are enough to control the contributions of all the three losses. [2] Some discussions about the influence of the key parameters are missing. Since there are three losses during training, which one plays a more important role in the final classification result? [3] Some lightweight adaptation/harmonization methods, such as Combat, TCA can be used for comparison.	code online, inhouse data can't be published due to privacy concerns, but other data used is publicly available.	The github link is provided in the paper.	Good. The authors have released the source code which is helpful to reproduce the proposed method.	"the data sampling algorithm is only mentioned in the supplement, it would be nice to explain the data sampling in 1-2 sentences in the paper, so the reader doesn't have to switch to the supplement. did the authors try to train the center points based on target and source domain instead of just the target domain? This would be an interesting comparison. in contrastive learning the projection subnetwork (layers after the ResNet that project the feature vector to dim 128 in this paper) is usually discarded after the contrastive learning, as it leads to some information loss (see SimCLR paper), could the authors explain why they chose to keep these layers? Chen, Ting, et al. ""A simple framework for contrastive learning of visual representations."" International conference on machine learning. PMLR, 2020."	As stated above in strengths and weaknesses.	Remove the first parameter for classification loss. Add some discussions about influence of the key parameters on the classification results.	the authors show impressive results on the target domain, while keeping the performance on the source domain high. The method is simple and could possibly be useful for other tasks, e.g. image segmentation.	Overall, this domain adaptation paper cannot validate its claim by training on both source and target domains in the experiment. And apparently the authors still compare their method with the baseline methods on the target domain.	Appropriate model design. Extensive experiments on diverse datasets.
281-Paper1959	Learning Incrementally to Segment Multiple Organs in a CT Image	"The authors propose a method to perform incremental learning (IL) of segmentation models using datasets with disjoint, potentially non overlapping annotations throughout the dataset. The authors propose to use a ""light memory module"" to make the location and approximate shape of previously seen anatomies persistent persistent during model training and a loss function that prevents the effects of conflicting labels for certain regions."	In this paper, the authors aimed to tackle the partially labeled datasets training problem using incremental learning. The paper is clearly written and easy to follow. The general idea of combining multiple datasets in multi-organ segmentation drives its novelty. Evaluated using five public datasets and multiple backbone networks, the proposed method demonstrates its effectiveness.	This paper introduces an incremental learning mechanism to learn a single multi-organ segmentation model from partially labelled, sequentially constructed datasets. Able to deal with the catastrophic forgetting issue, the developed method includes a light memory module to stabilize the incremental learning process as well as new loss functions to restrain the representation of different categories in feature space. The experiments performed on organ segmentation from CT scans using five publicly-available datasets reveal the effectiveness of the proposed contributions.	"The paper describes a method that seems to yield interesting results in the scope of incremental learning. Particularly interesting are the results in Table 2, where models trained sequentially on different datasets do not forget the information learned in previous steps and yield good results on all the different anatomies even though training for these anatomies has happened in the past. We can see that the ""ours"" row in table 2 shows good performance on all dataset, with an average performance close to the upper bound. As far as I have understood the authors have trained MargExcIL in 4 (or 5) separate rounds, and obtained these validation set results using the model obtained after the last round. Models such as FT, LwF, ILT, MiB were actually trained in the same way but since they did not have any IL specific strategy they basically fail on the first two tasks yielding very poor dice."	1) The movtivation of using the partially labeled datasets for multi-organ model training stands for its novelty.  2) The proposed method is clinical practical and the overall pipeline is designed in principle.  3) The results has demonstrated its effectiveness 4) The paper is clearly written and easy to follow.	Building a single multi-organ segmentation model from partially labelled, sequentially constructed datasets is innovative Incremental learning is applied to multiple organ segmentation for the first time A newly designed light memory module is proposed to further mitigate knowledge forgetting in incremental learning Strong assessment on various datasets using Dice and Hausdorff distance and including comparisons with both existing approaches and ablated versions	"The clarity of the paper is in my opinion insufficient. Certain sentences are unclear: ""...Then the probability of classes not marked in ground truth or pseudo label will not be broken during training."" what does ""broken"" mean? ""These conflicts between prediction and ground truth break the whole training process."" I believe it would be much clearer to state that - for example - ""These conflicts between prediction and ground truth are the reason the network ""forgets"" the knowledge learned in previous steps"" There are more examples of such unclear sentences in the paper. (Eg. ""all datasets in the meantime"" which I suspect means ""all datasets at the same time"") Fixing small mistakes is not as important as fixing the presentation of the paper. The bigger issue is the fact that I honestly could not fully understand the method itself! For example equations (1) and (2) as well as Fig. 2 contain notations and terms that have not been discussed in the text of the paper. The purpose of the two terms q-hat and q-tilde is not clear to me. I suspect there might be also some mistake where t-1 should have been used instead of t. For what concerns Table 3 I am puzzled because it seems that huge variations in terms of HD metric are not associated with any variation of dice. If the contours aren't matching by THAT much, how can it be that dice stays more-or-less stable. In MargExcIL (Ours) I see a HD of 2.30 compared to HD 8.10 of the MarcExcIL (woMem) but exactly the same dice. The culprit might be extremely small (1- or 2-voxel-sized) mis-classifications by the model, especially when not using the memory module in intermediate steps. That could be solved by simple euristics and morphological operations."	I would have some concerns listed as follows: 1) The proposed method seems to learn one organ at a time incrementally. This training process could be tedious and time-consuming if one center contains multiple organ labels. 2) How to tackle the annotation style difference across different datasets? E.g., Center A would label the parotid's anterior tip, while Center B would not. The authors might want to discuss this potential labeling issue.	The global loss as well as sub-loss weights should be explicitly defined Both marginal and exclusion losses from Shi et al. could be better described	The clarity needs improvement to allow for reproducibility	Good	The code does not appear to be made available but can be reproduced using the explanations provided. All the datasets used for training and validation phases are publicly-available. The testing phase relies on 3 datasets including 2 which are private.	I would suggest to the author to do a down-to-earth explanation of everything before  writing formulas. What are you trying to do, intuitively? Also, they should try not to leave anything to the imagination of the reader.	Please refer to Section 5 - main weaknesses	"The method is of high interest for the medical image analysis community. The submitted paper is innovative and very well written. The following comments could be taken into account for further improvements. Main comments: 1- In Sect. 2, the multi-teacher single-student knowledge distillation (MS-KD) framework proposed by Feng et al. should be mentioned as related work in the paragraph entitled ""MOS with partially labelled datasets"" 2- $L_{Marg}$ and $L_{Exc}$ from Shi et al. [21] should be explicitly defined and linked to Eq. 1 and 2.  3- You should explicitly write the global loss and mention how all the sub-losses are weighted Minor comments: 4- The last sentence of the paragraph ""Framework of IL"" (Sect.3. 1) is unclear and should provide the definition of $\Theta_{t}$. 5- Dice and Hausdorff distance comparisons between the proposed approach and existing incremental learning methods - especially MiB [2] - could be confirmed using a statistical analysis through t-tests 6- The comparison with and without memory module report the same performance in both configuration. Your assumption to explain the instability (various field-of-view) should be confirmed by using other datasets.  7- Be careful with the differences of writing between Fig.3 and Eq. 4, 5 and 6 for $l_{mem}$, $l_{same}$ and $l_{opp}$ 8- $b$ (background) should be defined in Sect. 3.1 and not in Sect. 3.2 as it is used in both Eq. 1 and 2"	The clarity of the presentation, which reduces the reproducibility of the method, and the unclear advantage of using the memory module motivate my decision of weakly rejecting the paper. That said, the work has merit and should be re-submitted once it gets sufficiently revised.	The general motivation drives its novelty. The proposed method has demonstrates its effectiveness. Thus, I would recommend accepting this paper.	Strong innovation dealing with how to build a single multi-organ segmentation model from partially labelled, sequentially constructed datasets Incremental learning applied for the first time to multiple organ segmentation
282-Paper2474	Learning iterative optimisation for deformable image registration of lung CT with recurrent convolutional networks	The authors propose a deep learning-based approach called learn to optimize (L2O), that aims to emulate the structure of gradient-based optimization used in conventional registration. The proposed architecture consists of recurrent updates on a convolutional network with deep supervision. It uses a dynamic sampling of the cost function, hidden states to imitate information flow during optimization and incremental displacements for multiple iterations.	This paper proposes a novel recurrent framework to emulate instance optimization for deformable image registration, using an iterative dynamic cost sampling step.	The authors present a deep learning based approach to emulate the structure of gradient based optimization (Adam optimizer).	The authors propose a recurrent framework using an iterative dynamic cost sampling and a trainable optimizer that mimics Adam optimization but can substantially reduce the required number of iterations. The method uses a total of 45 features for each image voxel/control point. These features consist of 3 predicted displacements, 8 fixed grid of subpixel offsets (resulting in 24 features), 8 dissimilarity costs at the 8 fixed subpixel offsets, and 10 hidden states that are propagated through all iterations. Through these, the model is able to save information about previous update steps and incorporate them similar to how Adam uses momenta. All features get updated with each recurrent application of the network. That means the coordinates and dissimilarity costs dynamically change across recurrent states and mimic the iterative fashion of conventional registration. The authors show that a good initial condition generated by the VM++ algorithm is important for fast and accurate convergence of both the Adam optimized approach and their L2O approach.	(1) This paper employs a recurrent network to emulate instance optimization, which is meaningful for intra-patient lung registration. (2) It uses a dynamic sampling of the cost function and hidden states to mimic gradient-based optimization, requiring fewer iterations than traditional techniques.	The main strength of the paper lies with the fact that it uses recurrent network framework to emulate instance optimization. The paper is well written and easy to follow.	The authors do not give a theoretical basis for why their network mimics the Adam optimizer, i.e., a loss function gradient descent. The reason why this is important is that the results presented in Figure 3 shows that the Adam optimizer continues to improve with iteration while the proposed method appears to level out and possibly get worse as the number of iterations increase. The authors do not show the before and after registration results. It would be good to difference images and Jacobian images for good, average and failed registration cases. The method was trained and evaluated using a small number of data sets. The paper states 28 pairs of image volumes were selected from the EMPIRE10 (selected cases), DIR-Lab COPD and DIR-Lab 4DCT data for 5-fold cross validation. There is no indication of how many image registration cases failed (if any) there were in this study.	(1) Section 2.1 introduces a lot about Adam optimization. However, it will be better to focus on the techniques/methods of this paper itself (Learning iterative optimisation for deformable image registration), rather than the related work. The title of section 2.1 is also misleading and confusing. (2) Section 2.3 and Section 2.4 are difficult to follow. I suggest the authors use an Algorithm block to clearly state the entire optimization algorithm. (3) As shown in Table 1., Adam, as the main baseline method, outperforms the proposed L2O both with and without Pre-Reg. Though this work reduces the required number of iterations, the contribution is relatively limited. It will be better for the authors to add comparisons of running time to show its advantages in terms of efficiency.	The main weakness of the paper is that the motivation to develop an instance optimization emulator is not that clear. The results indicate that the Adam optimizer works better than the L2O for both the datasets used.	The paper is reproducible.	Good reproducibility.	The paper is reproducible and source code is available online.	I like Figure 2. However, the authors do not explain what the ground truth reference is that they are using and how they got this ground truth. Without this information it is not clear whether or not this visualization is biased or not. Pg 6. The authors mention that their method produces smoother transformations and less folding compared to Adam optimization.  The authors should report how much folding occurs in both the Adam and the L2O approaches. Fig 3. The authors should state what the shaded regions of the graphs represent and how they are computed.	Major point: (1) It will be better to more clearly discuss the relationship between the hidden states in this paper and the first and second momentum in Adam optimization. (2) It will be better for the authors to add comparisons of running time. Minor point: (1) Eq. (1) should be improved. (2) One more comma at the end of the first sentence of the caption of Fig.1. (3) 'For each sample coordinate a dissimilarity cost...' should be improved. (4) '...are used to asses the registration accuracy...' should be improved.	The authors should improve the motivation behind the development of the L2O method. Perhaps it would be interesting to look at the actual registration outputs and the diffeomorphism of the displacement fields.	See above.	Two major issues are clarity in the presentation and issues in experimental results.	The paper presents an interesting approach. Although, it is hard to see the impact and application in its current form.
283-Paper0492	Learning Robust Representation for Joint Grading of Ophthalmic Diseases via Adaptive Curriculum and Feature Disentanglement	The manuscript describes a method that addresses diabetic retinopathy and its associated diabetic macular edema together in a single framework. The main purpose of the work is to design an automatic grading system with good generalization for DR and DME. To avoid ignoring potential generalization issues, the authors proposed a dynamic difficulty-adaptive weight (YAW) and the dual-stream disentangled learning architecture (DETACH), in order to learn way different features with curriculum learning, and separates features of grading to avoid potential emphasis on bias, respectively. Experiments are conducted on three well-known datasets, either intra and cross.	The paper proposes a network for joint grading of diabetic retinopathy (DR) and diabetic macular edema (DME). The proposed network uses a dynamic difficulty-adaptive weight for weighting samples gradually, and two encoders with a detached shared features to model the correlation between the two tasks and find a disentangled representation of the features.	The authors proposed a CANet for automated DR and DME diagnosis and grading. The YAW is used to specifically dealwith hard samples, while the DETACH is used for disentanglement of DR and DME for more robust diagnosis.	Novelty of the task: it is the first strong attempt to propose a joint grading system for DR and DME. Novelty of the proposal: the authors, aiming to offer a generalization framework, realised some approaches to deal with the potential bias in grading the two pathologies. Experimental evaluation: well conducted and presented. Strong results in both intra-dataset and cross-dataset experiments. Conclusions: supported by the presentation and results.	Strengths Proposing dynamic adaptive weighting of samples and disentangled representation of DR and DME. Comparison with other existing methods Performing Ablation study The paper is well written	The relationship of DR and DME is considered and modeled. The DETACH does proved its improvement in performance and generalizability The paper is well written and easy to follow	I have no major concerns for this manuscript. The only suggestion I have is to improve the introduction in order to better state the unique challenges are associated with this task and to provide a deeper overview of the study.	details of the network needs to be clearer	The dataset is not well introduced, and the data split is not introduced.	The reproducibily is adequate. Perhaps, the authors could give more details regarding the key parameters involvedin their method.	details of the network needs to be clearer	The dataset is not well introduced, and the data split is not introduced.	Dear Authors, I read your manuscript with great interest and I found it of excellent quality. Also the results are quite impressive and opens the field for further improvements. I have no major concerns for this manuscript. The only suggestion I have is to improve the introduction in order to better state the unique challenges are associated with this task and to provide a deeper overview of the study.	The abbreviations (YAW) and (DETACH) are quite unrelated to the full words, so consider make it clearer. In the introduction, redefine (DR) and (DME) again. In Methods, Fig. 3 is referenced before Fig. 2. Reorder the figures in the paper. In Fig. 3, please put details of the network architecture, e.g., feature map sizes, and size of fully connected layer, or mention them in text. For all the tables, please, put vertical separators between different datasets and diseases to make the tables more readable.	the dataset should definitely be properly introduced.	Quality of the proposal: the method is well presented and described and offers the right insights to the task at hand. Experimental evaluation: well conducted and presented. Strong results in both intra-dataset and cross-dataset experiments. Conclusions: supported by the presentation and results.	the methods are strong the results and experiments are strong the paper is well written	The method is quiet novel, and experiments does proved the authors design of network and loss functions. The paper is well written and easy to follow
284-Paper1319	Learning self-calibrated optic disc and cup segmentation from multi-rater annotations	This paper describes a self-calibrated optic disc (OD) and cup (OC) segmentation model from multi-rater annotations. The major contributions are: A recurrent learning framework is proposed for the self-calibrated segmentation using multi-rater annotations. In the proposed framework, two models are designed for recurrently learning the multi-rater expertness maps, and separating multi-rater annotations from the estimated segmentation masks.	The authors propose a new method for calibrating multi-rater annotation for optic disc and cup segmentation. Unlike previous studies, the proposed approach leverages recurrent attention network and self calibration to simultaneously calibrate segmentation and learn the consensus between the annotations from multi-raters. The design was guided by half-quadratic optimization. They provided extensive experimental validations that outperformed state of the art.	The work proposes a novel method to learn a segmentation from multi-rater annotations and show the usefulness of the method on the segmentation task of optic disc and optic cup in fundus images. The segmentation output is produced through an iterative optimization of multi-rater expertness estimation and unified segmentation based on expertness.	The novelty of the use of multi-rater annotations. The authors propose a recurrent method to fuse-separate the annotations from multiple raters. The way that the DivM recurrently returns its output to the ConM makes good use of the Attention mechanism. Furthermore, the authors provide the proof of the estimation of multi-rater expertness from the segmentation, which is theoretically persuasive. The experiments and the evaluation are robust. The authors' evaluation methods are sound and the result comparison with SOTA is clear (the authors have taken both the calibrated and non-calibrated methods into account). The paper is well organized and fluently written.	This work is well-motivated and presents a novel approach for an important topic. The multi-rater consensus problem is ubiquitous in clinically relevant algorithm development and the proposed solution has the potential to be applied to other imaging modalities, like digital pathology. A novel and interesting application of half-quadratic optimization in multi-rater calibration for optic disc and cup segmentation. The proof and proposed recurrent design seem pretty solid. Solid experimental validation with two public datasets.	The idea of using self-fusion label and SSIM loss for the combined segmentation  is new.	No major concern can be raised for this paper.	A concern is how applicable/realistic it is to apply such a model in clinical settings: how time-consuming and how much computational power is needed?  Can the hyperparameters like the number of recurrence steps for self-calibration fixed during inference? Grammar issues throughout the manuscript. It is highly recommended that the authors proofread and modify.	The general idea of using a combination of meta segmentation loss and annotator specific loss is also suggested in [1]. [1] Liao Z, Hu S, Xie Y, Xia Y. Modeling Human Preference and Stochastic Error for Medical Image Segmentation with Multiple Annotators. arXiv preprint arXiv:2111.13410. 2021 Nov 26.	The experiments have been conducted on a publically available dataset. Most of the implementation details are provided.	Considering the complexity of the model design and missing detail for hyperparameters, it is not easy to reproduce this work and thus the authors are highly encouraged to release code, model architecture and hyperparameters for good reproducibility.	The authors stated that they will provide the relevant codes.	"Just some minor problems: references should be checked, such as the redundancy of 20 and 21; typo such as ""This enable us to supervise DivM..."""	"The three contributions listed in the introduction sections are all about the methodology. I would suggest the authors to reorganize this part and add experimental validation as one of the contributions. What are the number of recurrent steps used for each fused label sets in Table 2 and 3? Is it needed to tune this hyperparameter for each fused label set? For future work, it can be valuable to extend this self-calibration approach to the multi-rater challenge in other imaging modalities, such as MRI and histopathology images. Minor:  4a. Formatting: Space is missing between the text and the square parenthesis for citation 4b. Grammar: On Page 2 ""... which aware the uncertainty .. "" -> ""... which is aware of the uncertainty ..."" 4c. Page 2 ""potential correct label"" -> ""potentially ..."""	"There are multiple language errors in the work. Please consult with an expert. Few examples: ""Comparison with SOTA"" instead of ""Compare with SOTA"" Table 2: ""Calibrated"" instead of ""Calibrate"", ""Not Calibrated"" instead of ""No Calibrate"" It is stated that the calibrated segmentation and multi-rater expertness converges to an optimal solution. However, since the update (equation 2) is a form of alternating minimization algorithm, looks like it might also end up in a local minimum and not necessarily a global minimum. It will converge to a global minimum if W and V are convex measures, but it is not clear if this is the case. Evaluation metrics: it is not clear why ""self fusion"" is used as an evaluation metric when it is part of the method itself. Also, the given evaluation metric of ""Diag"" is actually another method for combining multi-rater annotations and should be compared to as well. On the other hand, the common simple multi-rater evaluation metrics ""Random"" and ""Average"" are missing. It is stated that ""self-calibrated segmentation consistently achieves superior performance on various multi-rater ground-truths"". In most cases this statement holds, but not always. This sentence is not clear as there is no reporting of AUC: ""The performance improvement is especially prominent for OC segmentation where the inter-observer variability is more significant, with an increase of 1.83% and 1.72% AUC over current best method on REFUGE-MV and RIGA-MV, respectively."""	The proposal is sound and the explication is in detail with theoretical proof. The evaluation method is very robust and the result is satisfying. The paper is quietly well organized.	The multi-rater consensus challenge is ubiquitous in any clinically relevant algorithm development, the method is novel and interesting and experiments are convincing, despite minor concerns about complexity and practicality of the proposed method.	The idea of using self-fusion labels and SSIM loss for the combined segmentation  is interesting. However, the paper has multiple grammar errors and is difficult to follow. There are incorrect statements and the algorithm evaluation has several issues (see details in the comments).
285-Paper2366	Learning shape distributions from large databases of healthy organs: applications to zero-shot and few-shot abnormal pancreas detection	This paper proposes a scalable and data-driven approach to learn shape distributions from large databases of healthy organs, in order to learn normative shape models from collections of healthy organs for anomaly detection.	They propose a scalable and data-driven approach to learn shape distributions from large databases of healthy organs	The authors aim to learn a shape atlas using a U-Net architecture with the ultimate goal to obtain a reference atlas for possible normal shapes. The approach is evaluated on a dataset with approx. 2600 images of healthy pancreas. The obtained shape model is then evaluated using images of a pathological and healthy pancreas and classifying them using a one-shot/few-shot learning approach.	Different from previous methods, this paper learns a variational autoencoder based on binary segmentation masks with data augmentation, which is used to reduce the risk of overfitting and make the model more focused on the anatomy of organs.	The problem proposed in this paper is quite interesting	"The overall strength of the paper is, from my perspective, that it focuses more on the idea than on the technical details. This makes the paper really interesting and might boost the discussion rised by it. The benefits are therefore: The intensive and detailed evaluation, which considers different aspects of the techniques and reports numerous experiments. An interesting technical approach that clearly stands out from the typical ""Segmentation or image classification"" tasks. An (for MICCAI papers) extensive discussion of the work. The clear and good organization and writing style of the paper."	The comparison methods in this paper have only one baseline, which is somewhat less.	Lack of compared method in experiment part.  In the experiment part, the authors only discuss the parameters or ablation study of their method. If possible, the author should add more discussion about related works.	The technical novelty is limited The technical descriptions are limited.	The specific details of the model are explained in the supplementary material, so I don't think the reproducibility of the paper is difficult. If the author can provide code, it will be more helpful for readers to understand the details of the paper.	it is good.	The technical details are comparatively sparse in the paper. However, given the fact that the underlying technique is well-known, the authors relying on public software (nnUnet) and the main steps are documented, I think that the paper is still reproducible.	I suggest the author explain contributions more clearly in this paper. 	Add more related works.	I would have wished for a comparison with other shape encoding approaches, especially with shape models. Releasing the code would further improve the importance of the paper.	The paper doesn't have significant novelty but it has some contribution for the field, furthermore, the clarity and organization of the paper is not bad.	The topic is quite interesting, however the results are not very solid.	The paper clearly stands out from other papers that I usually see during MICCAI review or on MICCAI. Given the number of experiments and details reported, it rather feels like reading a journal paper. Another strong asset is originality. Rather than proposing yet-another-deep-learning-tweak, the authors suggest a relatively novel approach that might boost additional research. The main weakness is of course the limited reproducibility due to in-house data and code.
286-Paper1507	Learning towards Synchronous Network Memorizability and Generalizability for Continual Segmentation across Multiple Sites	Authors propose a new Synchronous Gradient Alignment objective and associated dual meta objective. The paper also provides some technical and heuristic details on replay buffers in order to reduce redundancy and improve model generalisability.	The paper proposes a continual learning + domain generalization framework for a series of multi-site prostate segmentation datasets. A dual-meta algorithm aligns the gradients between the previous and new sites, and also between the train and test sites (for the given sites). A seven-site dataset on prostate segmentation was used for evaluations.	The authors in their study delivered a learning technique (SMG-Learning) which tries to deliver memorability and generalizability of a network in domain shifted datasets. They utilised a Synchronous Gradient Alignment (SGA) objective, a Dual-Meta algorithm to deliver the SGA objective without expensive computation overhead, and a replay buffer to verify the efficient rehearsal and to reduce redundancy.	Good description of the asymmetry and problems of joint minimisation. The proposed SGA approach is interesting and similar to the idea behind cosine losses in a multi-task setup. Very interesting heuristics and insights in how to setup the replay buffer for the SGA approach There was a quasi-ablation study due to the fact that completing methods have a subset of the proposed features	The paper is well-written It tackles an important problem of continual learning + domain generalization which is a relatively new task. The performances are promising, showing improvements on new unseen sites while minimizing the performance degradations on the old sites.	The authors by using the SGA learning method (objective, Dual-Meta algorithm and replay buffer) deliver in the same time memorability and generalizability of the network in unseen datasets.  Their method outperformed existing state of the art learning approaches (Continual Learning and Domain Generalization) and the baseline fine tuning technique.	The paper is very limited in scope to the domain of continual learning, but fails to appreciate that there are other approaches to train on data from multiple sites, such as federated learning. While continual learning is necessary in multiple-task setups and when transferring knowledge between tasks without forgetting the previous task, it is not necessary when the task is ultimately the same across sites/datasets. Comparison to a simple FL approach is necessary. The paper would have benefited from statistical comparison between methods (e.g. statistical tests) or at least providing some confidence bounds. The ablation study is incomplete and limited to the features implemented by completing methods. We don't actually know the performance improvement caused by each contribution.	"The paper seems to assume that all DG methods consist of pseud-train + pseudo-test splits, which is not true. The gradient alignment is also one of many DG methods. The authors need to clarify that the proposed method has chosen a particular DG method to solve the DG problem. Also, this makes the ""relationship with CL and DG methods"" argument a little weak. It would be ""relationship with CL and a meta-learning DG method"". There seems to be a little connection between the DG solution and the CL solution. The CL solution may be combined with other DG methods. In L_SGA, are the losses for the first and second meta-objectives of equal ratios?"	The manuscript has no important weakness.	Data is open source. The code will be made available at the time of publication, and authors have made a anonymous github link available. Note that no code is available now.	Reasonably reproducible.	This study is easily reproducable.	Most of the comments are already expressed above in the section highlighting the limitations of the work. I add below a couple of minor comments that the author might also like to address. Minor comments: Training on sequential data streams is not necessarily privacy preserving. Differentially private mechanisms would be needed to demonstrate this. Also, streaming data often poses more of a privacy risk (data-not-at-rest) than centralising the data at rest. Authors fail to recognise the existence of Federated learning as an approach. While Continual Learning is an important area of research, the justification for CL vs FL in a medical setting is not provided or asserted. Would be interesting to see the behaviour of the method when the task is actually changing between sites, eg. if the model starts learning to segment prostates then learns to segment livers, does it still remember how to segment prostates? Would be good to see a comparison to the model performance if all the data was co-localised as the optimal performance. In this setup it is hard to know if the demonstrated performance is good or not.	The paper is overall solid, with a little of concern on its novelty (little connection between the DG and CL solution, seems like they are just independent methods combined to solve their own problems). Please see my weaknesses sections for detailed concerns.	Very nice work. Well written and organised with a novel approach.  Some suggestions of further work can be: i) the application of the idea in further internal and external datasets to capture the behaviour of the learning approach in different domain shift effects of external cohorts. ii) test the learning approach in different segmentation networks to capture the variation of the results compared with SOTA learning approaches (CL, DG etc).	There are some flaws with the lack of reference to FL and subsequent comparison, and some of the motivations are ill-founded, but it is technically a good paper where merits slightly weigh over weakness.	The paper still has values and is well-written overall. The authors try to make a connection between the CL and DG methods, but given the proposed method handles a specific family of DG methods (meta-learning with pseudo-train/test splits), the argument sounds a little weak.	The authors developed a well organised and clear written study. They organise a well case study with an appropriate way of training, validation and testing hypotheses and datasets involved. The authors compare their learning method with state of the art approaches. Their method outperforms the existing learning approaches. This justified a well organised and delivered study.
287-Paper0967	Learning Tumor-Induced Deformations to Improve Tumor-Bearing Brain MR Segmentation	The authors aim to segment the brain despite the presence of large tumors. Given a brain tumor segmentation, the proposed method uses synthesized images with accompanying ground truth deformation fields of mass-effect and infiltration to learn the tumor-induced deformation to the rest of the brain with a point cloud network. During inference, a tumor is injected into an atlas (including both a new tumor label plus deformation of the rest of the brain), and this new patient-specific atlas is used in atlas-based segmentation. The main experimental results include an improvement of about ~4 Dice over SAMSEG for subcortical labels on the tumor side in simulated images, plus qualitative results on real images from BraTS.	The authors have proposed a method to learn brain deformation induced by tumors to improve brain MR segmentation. To this end, they have trained a point-cloud deep learning network to learn deformation caused by the growth of the tumor and used it to warp a healthy brain atlas so that it can be used for pathological images. They have mainly used synthetic data for the validation of the proposed method and real dataset is only used for qualitative evaluation.	The paper proposes a novel whole brain segmentation method in the presence of pathology (tumours). The method first predicts the displacement field induced by the tumour using a point based deep network trained from synthetic data. Next, an atlas is deformed with the predicted deformation field to account for the tumour growth. Finally this atlas is matched with the current image to produce the final brain segmentation. The regression deformation network uses two models - (1) a direct displacement field and (2) a diffeomorphism represented using a SVF. The network is trained using synthetic data - simulated tumours using TumorSim [ref 26] The method is evaluated in 3 ways: (1) by testing the brain symmetry after reversing the computed deformation field; (2) using data that synthesize pathological ground-truth by fusing well-labelled health brain images (MindBoggle-101) and tumor scans (BraTS) (3) using real tumour data from BRATs (only qualitative evaluation)	Very interesting problem to segment brain structures despite very large tumors Method is sound, interesting to smartly inject the tumor into the atlas including deformation of other brain structures in the atlas.	The paper is very well-written and clear. The use of synthetic data for the evaluation of the method is valuable.	The proposed point-based deep regressor used to learn the tumour deformations is interesting. I like the idea of improving on traditional methods with deep learning components rather than blindly training deep models to perform everything. This particular task is also lacking ground truth segmentation data that would be required by a deep learning method. The paper is well written and clear. Despite the lack of ground truth, the author proposed few ways to evaluate the method.	A big caveat of this paper is that the tumor must already be segmented. Validation is missing baselines, which could include simple ones like basic tumor synthesis, atlas registration with tumor masking, etc. The main comparison is against SAMSEG. If I'm understanding correctly, the second baseline (called Our(tau=0)) inserts the segmented tumor into the atlas but never actually deforms the atlas to the new image. In my opinion, this is an irrelevant baseline since no one would reasonably do this.	The novelty of the work is very limited. Lack of quantitative validation of the model using real data is the most important weakness of the method. Without evaluation using a real dataset, it is not possible to correctly evaluate the performance of the method. The method relies on an initial automatic tumor segmentation, and it is not clear how sensitive the method is to the quality of the initial segmentation. Training a network using only synthetic data especially There is no comparison with the state-of-the-art methods.	Evaluation: is there any explanation on why the SVF-based approach performs worse than the plain deformation-based approach I agree that the method shows clear improvement if compared with the traditional atlas based method. I am wondering how would it compare with a pure segmentation method trained on the same type of synthetic data the evaluation is done on also, the method could have been compared with a traditional model-based approach [ex. ref 1,11, 29] where the tumour growth is simulated using a reaction-diffusion equation Synthetic data used in testing - how are the tumour deformations simulated ? are the author using the same TumorSim method ? I would be a bit concerned that both training and testing data are simulated using the same method. In terms of references, there are related works on tumour growth simulation using deep learning. The authors could mention the connection. In conclusion, the method is sound, but I have some concerns with the evaluation.	Several aspects missing, e.g., the number of training and evaluation runs, details of baseline methods, details of train/validation/test splits, No standard deviations or statistical significance tests for the results of synthesized images, clinical implications The parameters for the point cloud network and its training (architecture, batch size, learning rate, etc) are not in the main text nor the supplemental material	The authors have not provided any code. The description of the model is clear and well-written.	The paper is clear and detailed. Precise parameters of the regression network are not given. There are also missing some details in how the synthetic data is generated.	"I'm not convinced that the inter-hemisphere symmetry validation is very insightful. For such big tumours it seems to be near impossible to attain true symmetry (since one would have to majorly deform the image to eliminate the tumor). The reported symmetry improvement is very small. Is this meant to capture the effect of tumor-induced deformation on the rest of the brain only? Small comments:  Please clarify the use of the parameter tau when defining the displacement field and stationary vector field. I don't see why this scaling factor is needed when def(q P) is given by the network, which would hopefully just return the correct deformations directly. Why does it make sense to have a user adjust the degree of deformation?  In the introduction, consider rephrasing ""grows a tumor in the atlas is still highly complex and challenging"" and ""warping (grow a tumor in) the brain atlas to reflect the tumor and spatial changes in the patient image"" because these phrases can sound very similar to what you're doing. Also avoid criticizing ""these approaches require a good tumor segmentation"" because your method does too.  Let the tuple... before eqn (1): there are two errors, it should be D_i(q_j^(i) j= [1...J] - i.e., subscript (i) and the ... between 1 and J.  Section 3.2, ""Our method, including the baseline, significantly outperforms plain SAMSEG"" - I wouldn't use terms like ""significantly"" if you're not doing a statistical test. Figure 5 is hard to understand without ground truth segmentations, which I understand may not be available. Can you use arrows or boxes to highlight the main areas the reader should look at and give more information about these areas in the caption or main text? Some typographical errors, paper needs a grammar and spelling check"	The method relies on initial tumor segmentation. How sensitive the method is to this initial segmentation? Please provide a measure of variability for the provided Dice scores. What does the author mean by weighted-mean-Dice? How was it calculated? The authors should quantitatively evaluate their method on real data and compare it with some of the state-of-the-art methods.	While a realized that adding the comparison with another method would be a lot of work, I would recommend at least adding to the discussion of the results (ex if there are any insights into why SVF representation performs worse in experiments 3.2; SVF guarantees a valid diffeomorphic field and adds implicit regularization ; would a PDE model-based method would perform similarly ?)	It's a good problem and the method is well-formulated and looks as if it should work, but the lack of suitable baselines in the experimental results is concerning.	There is no validation using real data. Technical novelty is very limited. The proposed point-cloud deep learning network has already been used in the literature.	The method proves an improvement over the traditional atlas-based segmentation. I like the idea of combining deep-learning and modeling approaches. The regression network on its own is well formulated.
288-Paper1591	Learning Underrepresented Classes from Decentralized Partially Labeled Medical Images	This paper proposed 'an under-explored problem' federated partially supervised learning. And they also proposed a framework to solve the problem with federated self-supervised learning, energy-based loss and a prototype based inference. Ablation study showed better results than some baselines.	The paper proposes a federated learning framework for learning underrepresented classes under partial label scenarios. The framework contains a self-supervised learning for warmup, an energy-based partial label learning for differentiating common classes and underrepresented classes, and a prototype-based inference stage for final testing. The results showed good improvement compared with baselines.	This paper studies a new problem, where underrepresented classes only have few labeled instances available and only exist in a few clients of the federated system. They proposed a novel FL framework FedFew, which consists of three stages and show good results on multi-label classification tasks.	The application is interesting and shows the real problems in clinical setting. The authors presented strong evaluation on the efficiency of their method compared to baselines.	The problem in discussion, i.e., federated learning with partial label and underrepresented classes, is quite interesting and underexplored indeed. From the results, the conventional baseline cannot solve the problem, while the proposed method provide a practical solution.	The proposed method is very effective in solving the under-explored problem in federated learning. The three-stage framework is novel, 1. use federated self-supervised learning to learn class-agnostic representations 2. learn from common classes 3. classify uncommon classes. The empirical results are impressive on Chest X-ray14 dataset. And the ablation study demonstrates that including EBM in the training can improve the performance.	The presentations of the method and experiment are not clear, I have difficulty to fully understand the method and the experiment.	SSL step seems effective for common classes but not useful for the underrepresented classes, which might be the main focus of the paper? Doesn't passing meta data from local servers to the parameter sever violates the privacy policy?	Practical issues. Whether the real-world application needs this method or not is not clear. The experiment is about simulation and the number of examples is so small that whether we need such a complex algorithm is a question. Related Work. The paper needs to cite more related works and compare them to general vision tasks and medical image tasks. Whether there are similar datasets in general computer vision and how the proposed FedFew works on general image datasets are interesting problems to explore. Format. Fig 2 and Fig 3 are not quite clear.	The paper provides sufficient details about the models/algorithms, datasets, and evaluation.	Would be good if the authors release the code as promised.	They could reproduce the results with some difficulty.	The algorithm 1 is too simple to represent the method, I can't understand the method only by the code provided in algorithm 1. I suggest the author to include a flowchat/figure in the paper to better present their method. In table 2, is the prototype based inference adopted to all the method, is the energy calculated for all the methods during inference? Or just FedFew? What is the difference between 'FedFew w/o EBM' and 'NN (MLC w/ FSSL)', and what is the factor that bring so much improvements between these two methods? Why (Cc + 1)-dimensional vector important? 'We sample 10 negative examples and 10 positive examples to simulate the class imbalance for UCs'. What's this?	"major points: Section 3.3: a 0-th class is used to determine whether the image contains any CCs. Then how would one know if an image contains any CCs if this image is from the UC clients? Section 3.4: Doesn't passing meta data from local servers to the parameter sever violates the privacy policy? Table 2&3: The FSSL step seems effective for common classes but not useful for the underrepresented classes, which might be the main focus of the paper? minor points: ""We first train an MLC model for CCs Cc"". It's hard to understand what is CCs Cc. Table 1: what are RN and DN? Table 2: it's recommended to explain A,P,R,and F in the table caption."	As shown in the weaknesses.	The topic is novel, The method looks novel but I think some important but detailed information is missed.	The study overall is interesting. Some points need more explanation to help readers better understand the paper.	As shown in the weaknesses.
289-Paper1723	Learning with Context Encoding for Single-Stage Cranial Bone Labeling and Landmark Localization	The paper proposes a method for cranial bone segmentation and landmark localization based upon the well known unet architecture.	This paper proposes a context encoding-constrained neural network for single-stage skull bone segmentation and landmark detection from cranial CT images. The authors designed a context encoding module based on U-Net for feature learning that considers the global image context, .and introduced an auxiliary regression task that models the relative spatial configuration of the anatomical landmarks to promote the landmark detection. The method was evaluated on pediatric 3D CT images, showing superior performance compared with the related methods	In this study, the authors developed a novel method for a single-step segmentation and landmark localization of the cranial bones. The authors achieved this by incorporating a context encoding network into the U-net architecture. This context encoding mechanism helped capture image related feature information in order to avoid isolating  pixel level prediction from the global image context. At the same time, the authors have added an auxiliary task of modeling the relative relationships of different anatomical landmarks spatially.	The method is interesting and appears to be performing well. The authors had a thorough analysis of their experimentation with both ablation studies and p-value tests. Adequate discussion about related works	The authors used displacement vector maps to learn landmark context to improve representation learning.	As segmentation and landmark annotation is a tedious process which has high variability between raters and subject to errors. While previous studies make use of a multi-step process, this study attempts to perform the segmentation and detect landmarks jointly in a single step and on a 3D image. The context encoding attention mechanism which is further augmented by landmark displacement maps to guide the learning of context. Together with the spatial relationships of all the other landmarks, the final objective for the model is formulated as a weighted sum of all the regularizing terms. Thus enforcing contextual learning in a novel way. The experimental design is well set up and the authors have also evaluated the contribution of the context encoding mechanism through an ablation study. Moreover, The paper is well written, with the information presented in a clear and logical manner. At the same time, the methods and results are well explained.	The methodology section could be written a bit better with some sections being a bit too consice for an easy understanding of the method- I assume this is the product of heavy editing to make the paper fit in the prescribed page limit. There are some questions though that naturally arise from the description of the method. * why is the context module only taking into account features from the bottleneck stage of the U-Net? It is well known that the most crucial part of a U-net is the skip connection - I would assume more interesting performance and information could be extracted if the skip connection information was also incorporated. Why were the landmarks regressed as heat maps and not as coordinates ? Moreover, there is comparison with only 1 other piece of literature - It would be helpful if the authors compared and discussed the benefits and limitations of their method as they relate to other papers.	It is unclear what are challenges of automatic skull bone segmentation and landmark detection from cranial CT images. Compared with related works (i.e., Ref. 14, 18, 19, 20), the task of this paper is less than difficult, as the skull structure is generally easy to extract from CT images and the number of localized landmarks reported in this paper is very limited. Besides, only a few of relevant methods were tested for this relatively simple task.	There are no major weaknesses present in the paper. However, the paper lacks visualization of the outputs generated along with the comparisons with other networks.	Paper appears to be meeting reproducibility criteria	It is not difficult to reproduce the method of this paper with the released code.	Reproducibility is highly likely, as the novel contextual encoding mechanism is well explained in detail. Furthermore, implementation details such as training parameters, software and hardware details are reported as per the best practices. However, access to the implementation code should increase the reproducibility further along with information of pre/post processing of data.	See above	The qualitative evaluation results should be reported. More relevant methods should be included in the comparison evaluation.	The authors should clarify if any pre/post processing was performed for  the images and the labels, along with any augmentation strategies that were utilized (if any). Moreover, the authors should specify the list of all the weighting parameters, from which the final values were empirically selected. Finally, the authors should consider adding visualizations of  the images and its corresponding predicted segmentations and landmarks for all the methods compared.	The paper appears to be interesting and could potentially be beneficial to the community	It is unclear what are challenges of automatic skull bone segmentation and landmark detection from cranial CT images. Compared with related works (i.e., Ref. 14, 18, 19, 20), the task of this paper is less than difficult, as the skull structure is generally easy to extract from CT images and the number of localized landmarks reported in this paper is very limited. Besides, only a few of relevant methods were tested for this relatively simple task.	The single-step joint prediction of segmentation and landmarks architecture with contextual encoding mechanism proposed by the paper is highly applicable in many medical imaging tasks. At the same time, the paper explained the novel mechanism with clarity.
290-Paper2333	Learning-based and unrolled motion-compensated reconstruction for cardiac MR CINE imaging	The authors propose an end-to-end  reconstruction and registration network for MR  image reconstructionn. Their proposed approach, effectively iterates reconstruction and  registration steps within the same network, which allows them to train and  execute them jointly.	The authors propose a learningbased self-supervised framework for MCMR, to efficiently deal with nonrigid motion corruption in cardiac MR imaging. A dynamic motion estimation was embed into the unrolled optimization, which can deliver more precise and deailed estimation.	The proposed work solves an important problem related to dynamic MRI data of accounting for motion compensation while reconstructing undersampled data. This method provides a framework to perform motion compensated and high quality MRI data acquired in low scan times.	The method is interesting and the results quite impressive. The network is also  extremely  fast to execute.  Testing is adequate for papers in this venue and all the information about training was provided.	A dynamic motion estimation was embed into the unrolled optimization, which can deliver more precise and deailed estimation. Furthermore, the proposed method provides a motion-resolved image sequence in which all frames are motion-corrected.	Paper targets an important clinical problem. High spatial and temporal resolution is important for dynamic MRI scans. While MRI data undersampling is performed to reduce scan time, motion corruption is tackled by methods such as group registration (and of course, lower scan time implies reduced chances of motion). Conventional group wise registration methods are effective, but take a lot of time. The proposed method reduces this processing time by multiple folds. The most important highlight of the work is how unrolling reconstruction along with undersampled MRI reconstruction helps in improving the results both qualitatively and quantitatively.	No major weakneses. The authors disregard methods that could iteratively register and reconstruct, although those  are computationally expensive. On this regard, the authors could test whether an iterative  approach of registration-reconstruction using  GRAFT and CG-SENSE (trained separately) performs similarly. The results using elastix are worse than expected. It may be possible to improve them with more work on that end.	A clear description of the mathematical setting is needed.	Primary weaknesses of the paper have been discussed well in the text. Kindly mention if actual k-space data was used for training and testing or synthetic k-space data was used.	No  data and  software are provided. The authors appear to provide sufficient details about the  training of the network and reproducing it may be feasible with some effort.	good.	If possible, authors should make the code available. Replication of this work is not straightforward because of the complex implementation framework and use of in-house data.	This is very nice work. I would recommend improving the competing results with elastix for a more fair validation.	This paper is written very well. So, I recommend it for publication. However,  one minor remarks: if I am nor wrong, references should be numbered consecutively in order in the text. so, please check the offical template of the MICCAI.	The current reconstruction method is using CG-SENSE. With the progress in DL based reconstruction, authors might want to undersampled MRI reconstruction using deep learning based techniques. In future work please consider testing on some prospectively undersampled data. Although computational metrics are important indications of the method's performance, would recommend having radiologist ratings for future work.	This is a very interesting approach with good results	This paper is well constructed with improved results and a good discussion.	The proposed work solves a very relevant clinical problem by following a well thought out framework. Authors have clearly stated the weaknesses of the method.
291-Paper0256	Learning-based US-MR Liver Image Registration with Spatial Priors	The paper proposed a registration framework for preoperative alignment of 3D ultrasound and MRI liver images. The initial alignment was achieved by using the right intercostal spaces from MRI as spatial priors for ultrasound positioning.	This paper develops a pipeline for within-subject in-vivo 3D ultrasound/MR liver image registration based on right rib segmentation, probe-orientation estimation, learning-based point-cloud registration, and refinement using image similarity. The method was evaluated on 18 subjects.	The paper presents a method to perform liver image registration using patient-specific magnetic resonance and intercostal ultrasound images. Clinically, alignment of pre-interventional magnetic resonance imaging and ultrasound images is frequently required and represents an interesting topic. An initial alignment between the MR and US images is estimated based on the spatial priors. A learning-based approach is considered for the rigid image alignment. Accuracy is improved by using the LC2-based non-rigid method. A detailed description of the workflow is presented with an assessment of performance and validation using 18 clinical cases.	The main strength of this work is the whole processing system including segmentation of MR and ultrasound volumes, initial alignment, surface-based rigid registration, and the final deformable registration. The whole workflow is a feasible solution for preoperative registration of liver MRI-ultrasound images.	This is an interesting and difficult problem. As the authors point out, a good initialization is required, and that is a key contribution of this work. The method was well-engineered, assembling a number of techniques into a pipeline that produced good results. The algorithm is, by necessity I think, somewhat bespoke.  While the specifics may not be directly generalizable to other domains, the strategies employed are valuable and will provide inspiration for further uses beyond this application.		The technical novelty of this paper is limited. Almost all the components used in this system is established methods, such as dense-vnet based segmentation, coherent point drift based point cloud registration, and the linear correlation of linear combination (LC2) image similarity based non-rigid registration.	There is no comparison to an alternative technique, other than to quote the results from Ref 23 which are in the same range although image resolution, etc, are not given.  An explanation of why the proposed algorithm is an improvement over Ref 23 is needed. How robust is the initialization technique?  Given the small number of cases, it is not clear that the rib and intercostal space modeling will always work.  How good does the initialization have to be to avoid failure? There is also no ablation study to give an idea of what aspects of this approach are important. MVTK is mentioned but not cited or explained.	-	Not so good.	No data (existing data used) or code available but other aspects of reproducibility included.	I believe it could be reproduced from the paper.	"The whole system is lack of technical novelty as described in the weaknesses part. Moreover, considering the whole registration workflow consists of several procedures, the errors may be accumulated to reduce the final performance. The motivation of the proposed initial alignment is not strong enough. Considering the registration is conducted preoperatively and in 3D space, in my opinion, the initial rigid alignment is not a challenging problem and can be achieved by conventional registration networks. The proposed initial alignment may be tedious. The authors used coherent point drift to register the surface point clouds from MRI and ultrasound images. The surface point clouds were generated from the segmentation results, which may contain incorrect segmented boundaries. However, the CPD is not robust to noisy point clouds, thus may result in inaccurate alignment. In data and material section, the authors mentioned ""from 26 subjects..."", ""Our MR dataset consisted of 67 liver abdominal..."", ""In these datasets, we have 18 subjects..."", I'm confused with these descriptions. In FCN-Based Tissue Feature Extraction section, why the authors call the segmentation as feature extraction? There is lack of comparison between baseline and current state-of-the-art registration methods."	It would be great to compare this approach to alternatives.	-	Please see the described weaknesses and detailed comments.	The paper shows impressive results on a difficult problem.	The US-MR liver image registration workflow presents sufficient amount of novelty. The application has significant clinical value. In-vivo validation and assessment of the performances are provided.
292-Paper1034	Lesion Guided Explainable Few Weak-shot Medical Report Generation	This paper proposed a multi-view lesion guided few weak-shot learning for explainable medical report generation, which is the first model in such a complex task. Different from existing report generation frameworks that use the global features of images for the generation, this method uses regional lesion features by jointly learning with a lesion region detection task.  for weak-shot learning, this method builds a soft label for exploiting the semantic relationship between seen and novel diseases.	This paper proposes to leverage the few weak-shot learning for medical report generation. The few short setting lies in potential unseen lesion types in the testing cases.  The report generation is coupled with the lesion detection task and uses the detection results for guidance. To better improve the model, the authors further propose a soft target of lexical embeddings training. Lexical features and visual features are combined before being fed to the generative model. Experiments on the Fundus Fluorescein Angiography Images and Reports dataset (FFA-IR) demonstrated the superiority of the proposed method.	The paper uses weak ZSL to transfer knowledge from a model trained on seen to unseen lesions/classes using images and pixels. It is a novel approach to reduce human annotation requirement for retraining models on new diseases using multi view embeddings.	Authors aim to tackle a novel task which has certain clinical significance. The method of this article is innovative enough. Experiments show the proposed method can achieve higher performance than the reference methods.	Overall, the paper is well organized by putting together all technical components in a systematic way. Connections and interactions across different components are clearly illustrated. Below are the detailed strengths of the paper: 1, the few short setting is novel while making sense. From a system evolution perspective, it is highly possible that new classes are gradually added to the dataset. For a smooth system transition or evolution, the few short learning framework should provide great help and makes the solution more practical. 2, lexical embedding bridges different modality and strengthen model training in a multi-task manner. This embedding also enables a good easy cold start for future new classes and makes the efforts for adapting models to new classes minimum. This is achieved by enforcing a soft label that is calculated by using textual similarity for the class branch of Faster RCNN with the RoI features. 3, when generating the report visual features and lexical embedding are combined to guide the generation process. The conceptually can better gears the report to a more lesion-focused fashion. 4, extensive experiments and promising results.	The authors propose a novel approach for reducing annotation requirements by using common global findings between seen and unseen classes. The promise of the methodology is seen in comparison with SOTA models and ablation studies.	"(1) Poor readability. Although it is not a technically difficult paper to read, the content of the paper is incoherent and often needs to be searched up and down. For example, in Eq, 2, the explanation of ""y"" need be traced back to the section ""Problem Setting"". It causes great trouble in understanding important formulas. (2) Some mistakes appear in formulas. In Eq. 2, it sees ""i"" has many possible values. Also, I'm not sure if this soft label ""L^s"" is one-hot coding, which need all elements sum to one. The explanation of the soft label is confusing. (3) What is the theoretical rationality of ""using KL-divergence can can force the network to learn the relationship between seen and novel diseases"". (4) I understand that the soft label establishes the relationship between seen and novel diseases, but why not just using the cross-entropy loss since you can straightly using the softmax to get \hat_{y}^s. (5) In the section ""Few Weak-Shot Report Generation for Novel Diseases"", it seems that the model has complex operations in the inference stage, which are not shown in Fig. 2(overview of the proposed approach). (6) In comparison experiments, since the method is not the best in all metrics, author should give reasonable explanations. Also, the conclusion ""Our method outperformed other state-of-art approaches to medical report (7) generation in six experimental settings"" is incorrect. (8) In Fig. 3, why not showing the results of the compared method Grounded. (9) Authors do not discuss the generality of the proposed method, e.g., data conditions necessary to use this method, generality of proposed modules.  (10) Since in the ""Problem Setting"", we see ""Each case in the dataset contains images of a patient at different periods"". Is this a necessary condition? The authors did not provide discussions about parameters ""N"". (11) Also, is there a limitation for the type of provided weak annotations?"	I have the following questions: 1, does the few shot setting also help the lesion detection task? I understand the main focus here is the text generation part. But I am still interested to see how detection performance is affected compared with the baseline Faster RCNN. 2, during inference, how are new classes classified? It is unclear to me that when the Q set images are fed in, how the predictions from the seen classes and novel classes will be combined. It seems like new classes are separated from the seen classes. Could you specify what the final prediction will be like from the detection perspective? Or simply, we do not care about the class label?	While the model allows use of common features between seen and unseen classes, it is unclear how big does the overlap of these features is necessary.	The paper lacks parameter details, so it is difficult to reproduce only by the paper.  The author states that the code will be published in the future.	Positive if the code is release and data partition is provided	Authors provided a checklist for reproducibility. The description of the methods, implementation and validation studies are also satisfactory.	Too long introduction is unnecessary, since ZSL is not highly relevant to this paper. Problem setting is unclear. Some symbols lack corresponding explanations and some symbols may be reused. Authors need to check them carefully. Fig. 2 does not show weight imprinting scheme and is lack of explanation of virtual, solid lines and color. There is a lack of discussion of some important parameters.	Please address my questions listed above.	Please comment on the implementation of this methodology between two classes where feature overlap is not significant. The BLEU scores of the the new methods outperforms other ablation methods but is still moderate when looking at the absolute value. Are there strategies to improve the absolute score?	This article may be innovative enough, but it has poor writing quality and insufficient experiments.	The few weak-shot setting is interesting. It provide an potential solution for smooth evolution of the machine learning system that requires expensive annotations.	Novel approach but needs demonstration between two substantially different classes.
293-Paper1939	Lesion-Aware Contrastive Representation Learning for Histopathology Whole Slide Images Analysis	The paper proposed a novel weakly supervised representations learning method with a designed lesion queue under the setting of contrastive learning.	The authors propose a weakly supervised contrastive learning framework called LACL, where WSI labels are introduced to relief the class collision problem. Specifically,  LACL builds a queue for each WSI class, thus negative samples are selected from different classes. To effectively update the queue, the authors select the representative samples based on the similarity distribution.  The authors validate LACL on two WSI benchmarks and achieves incremental performance gain.	Authors propose a novel class-aware semi-supervised contrastive learning framework LACL for histopathological images. LACL replaces the memory bank of MoCo with several class-specific queues. Negative samples are generated from these queues to alleviate class collision problem. To guarantee the queue purity, a queue refinement strategy is proposed for queue updating. Authors validate the proposed method on two WSI-level classification datasets.	The paper is well organized and easy to follow. The proposed contrastive learning method which can guide the negative samples selected from different classes is novel and interesting. Experiments conducted on different datasets with CLAM and TransMIL demonstrate the effectiveness of the proposed pretraining strategy.	The organization of the paper is clear and easy to follow. The motivation and method are clearly demonstrated. The experiment is basically complete. The proposed method achieves performance gain on different benchmarks.	The main strength of this paper is the novel class-aware semi-supervised contrastive learning framework LACL. Unlike MoCo, LACL maintains several class-specific queues, so it can alleviate the class collision and provide reliable negative samples for contrastive learning. Besides, a queue refinement strategy is proposed to guarantee the queue purity. Ablation studies show the contributions of class-specific queues and queue refinement.	The paper state that a lesion-aware contrastive learning method that is specially designed for pathological images is proposed. This seems to be over-sale. This method can be performed on any other type of image with a corresponding label. The authors need to clarify this in the introduction. Why a dynamic queue is needed (The lesion queue). As the label of each WSI is integrated into this framework, what if selecting negative samples from WSIs with different labels directly. More descriptions and experiments are needed. The datasets seem to be private. If they will not be released, more details about them need to be listed, like the patients/WSIs of each class, and the magnifications of WSIs. Please clarify the metrics reported in Table1-3 are in which level. Patient-level? WSI-level? Or patch-level.	The improvements are expected since more supervisions ( i.e., WSI labels) are used, which makes it an unfair comparison with other SSL methods. LACL is motivated by relieving the class collision problem in SSL. However, WSI classification is a typically multi-instance learning problem, where the tile labels are usually different from WSI labels. Though the WSI classification performance is improved, to what degree the class collision problem is mitigated, is not discussed and analyzed. Some settings seem unreasonable, e.g., the expected distribution is queue refinement strategy, where the similarity is 1 and 0 for same and different pseudo-labels, respectively. However, the ideal similarity between instances with different labels should be -1 under cosine similarity. More explanation should be provided, as well as the setting of threshold in eq.5 and the same length of each queue, etc. Experiment analysis in section 4.3  is not convincing.	The method comparison is insufficient/unfair. The proposed LACL is a semi-supervised learning method, it utilizes the labels of WSI during contrastive representation. However, authors compare their method only with self-supervised/unsupervised learning methods. Authors abuse the mathematic notation for i and y in Eq. (3) and Eq. (4), which may confuse readers.	Reasonable level of implementation details has been provided in the paper.	The code will be released for reproducibility.	Authors claim they will release the code. If so, the study could be reproduced.	See above.	As the WSI labels are weak supervision, it would help to provide more analysis about the risk of overfitting the noisy pseudo labels. To what degree the class collision problem is mitigated should be discussed and analyzed, e.g., the T-sne visualization. More explanations of settings: (a) The expected distribution is queue refinement strategy, where the similarity is 1 and 0 for same and different pseudo-labels, respectively. However, the ideal similarity between instances with different labels should be -1 under cosine similarity. (b) The threshold in eq.5: As the instances in mini-batch are randomly samples, how to understand the average KL divergence? (c) The same length of lesion queue for each class: As the class imbalanced problem (metioned in section 4.3), the update of each queue may be inconsistent under such settings. More details of experiments: (a) As the authors also split a  validation set, it would help to explain how to use it in self-supervised learning for early stop? (b) The authors should clarity the evalution protocal, i.e., fixed or fine-tuning. (c) The authors are encouraged to explain why the MLP is of fc-BN-ReLU-fc (BYOL style) instead of  fc-ReLu-fc (MoCo v2 style), as the MoCo v2 is chosen as baseline. Better experiment analysis in section 4.3.	"For the title, ""class-aware"" is more suitable than ""lesion-aware"". For fairness, author should at least compare the representation capacity of the proposed method with a ResNet50 trained with WSI-level labels. Authors can discuss the effect of queue initialization on the performance. Eq. (3) and Eq. (4) should be reformulated. Typos like ""MoVo"" and ""z_k = f_q(v_k)"" should be corrected."	There is some novelty in the method design, especially for the member back queue part. The experiment is solid and proves the effectiveness of this method.	The proposed method is well-motivated but not well-explained. There are some settings are unclear and more details should be provided.	Novel class-aware contrastive learning framework, insufficient/unfair method comparison.
294-Paper0847	Lesion-aware Dynamic Kernel for Polyp Segmentation	The paper proposes a method for endoscopic polyp segmentation that uses an adaptive/dynamic kernel for optimizing features relevant to a polyp; the method also utilized two new attention modules ESA and LCA. Rigorous evaluation on public data provides experimental support.	This manuscript proposes the lesion-aware dynamic kernel (LDNet) for polyp segmentation, which is generated conditioned on the global information and updated by the multi-level lesion features. The dynamic kernel endows LDNet with more flexibility to attend to diverse polyps' regions. Besides, LDNet use two tailored attention modules (ESA and LCA) to improve the feature representation and enhance the context contrast. Extensive experiments and ablation studies demonstrate the effectiveness of LDNet.	This paper describes a method for polyp segmentation that combines a U-net segmentation architecture with dynamic kernel update as well as self-attention and cross-attention modules. These components are brought together to produce accurate lesion segmentations validated across various datasets.	The main contribution is the technical addition of dynamic kernels and ESA LCA to the endoscopic polyp segmentation problem. While inspired by work in general vision/learning it is novel in this setting. Thorough experiments and comparative analysis are reported using public endoscopic datasets. Experiments on a new dataset are also reported with an indication (in form, not in the paper) that it will be released but no link.	This paper is well-written and -organized. The novelty is interesting in this field. The performance is competitive when compared with other approaches.	Authors bring together various elements including a U-net segmentation architecture, dynamic kernel generation and update, and self-attention and cross-attention modules. Evaluations show high accuracy in lesion segmentation across various datasets.	The authors should try to position the clinical value of the work a little better. At times, it is mentioned that the new method would improve concealed lesion detection/segmentation, however this is neither shown in experiments nor made clear as to logically how. Results are reported on a new internal dataset. These, however, use a slightly different splits to the other experiments and also it is entirely unknown if the data will be seen by anyone else. Hence somewhat limited value.	(optional) the authors could add more efficiency comparison in the benchmark table. There are several recent works [1,2] may helpful to the section of related works. [1] Progressively Normalized Self-Attention Network for Video Polyp Segmentation [2] Video Polyp Segmentation: A Deep Learning Perspective	Unclear what is meant by 'conceal' polyps. A quick literature search did not reveal a definition. Authors should define any terms they are introducing in the paper. While the paper shows improvement, it is hard to assess the significance of the improvements. Authors mention that their ablation studies revealed significant improvement upon adding dynamic kernels, for instance, however how the significance is calculated is not explained. How significant is an improvement of ~1% over baseline results in the 90% range? Is this improvement a result of the additional parameters introduced by this method and how many more parameters are needed to generate a ~1% improvement?	The code will be made available. The paper is clear enough to reproduce the method. The authors talk about a new dataset, and in the form indicate it will be released, however, details on this are very scant. Insufficient information on the data, labelling procedure, etc. etc. No link.	"The authors say that ""The code and collected dataset will be made available."". Also, the reviewer think it is not difficult to reproduce LDNet according to the details provided by authors."	Reproducibility could be improved (see detailed comments below)	"Overall, the paper is very well presented and easy to follow. Some points for possible improvement below. It would be helpful to explain the clinical use case better. For example, is this an approach to better segment the same polyp once detected in subsequent frames by using a the dynamic kernel? Or do you think the kernel will adapt to a particular colonoscopy? Also what is the segmentation need for polyps in terms of clinical utility? When making claims, it would be good to have experimental basis/support. For example, in the LCA description the section ends with ""which significantly improves the feature contrast and benefits to detect conceal polyps."" But where is this demonstrated? The new dataset should be described fully including size, data labelling procedure, etc. The experiments on public data are commendable and rigorous. What limitations to testing the proposed method do these datasets present? E.g. do you need videos to demonstrate the full utility of the method? Perhaps if the method is meant to apply/adapt better to temporal information (video) then it would be relevant to also ref works which incorporate the temporal features. Several of these in MICCAI, e.g. Puyal, et al. 2020 and Wu, et. al 2021."	Please see Section 3 &4	One aspect the authors should try to highlight is how their methods differ from prior work that introduced the various components brought together in this work. While putting various components together is commendable, it would be nice to see where authors have introduced changes to bring these components together. If authors can highlight these main modifications (e.g., after contributions in the introduction section), it could help bring out the impact of this paper. While authors explain various components of their method is detail, some further details critical for reproducibility are required. Some aspects that were unclear are below: Were all the other architectures that authors compare against also trained in the same way? I.e., using the same datasets and splits? Why is K=1? Were other Ks tried? Was this K chosen via experiments with the validation dataset? Can authors elaborate on how the binary cross entropy and dice loss were combined? I.e, are these weighed equally throughout training? How are recall, specificity, precision and accuracy defined? As in, how do authors decide that a particular lesion is correctly identified? Is this done via some threshold on the Dice score or IoU? If so, please describe. It seems that the Dice scores for held out datasets are quite a bit lower than datasets used for training while accuracy remains fairly unchanged. So understanding how accuracy, etc. are computed might help understand this disparity. Finally, while all the metrics shown in evaluation are useful to see, it may also be nice to include how many parameters each of the architectures are using in order to solve the segmentation problem.	This is a technically good and novel paper. It does need some clarification and also additional clinical context for improvement with potential ref to use in video.	This manuscript is ready for the acceptance of MICCAI. It has detailed methodology, enough experiments and competitive performance. I vote for 'strong accept' in first round.	There are several details missing and the manuscripts needs to be polished further.
295-Paper0586	Less is More: Adaptive Curriculum Learning for Thyroid Nodule Diagnosis	This paper aims at improving the accuracy of the thyroid nodule classification. The existed methods can't fit well with the inconsistent information between the label obtained by the cytological biopsy and the ultrasound imaging TI-RADS criteria, so the author propose an adaptive curriculum learning framework, which adaptively discovers and discards the samples with inconsistent labels. Moreover, the authors contribute a new thyroid nodule classification dataset to facilitate future related research on the thyroid nodule.	This paper presents a model for adaptive curriculum learning that can adaptively eliminate the samples with label uncertainty from the prediction function. The paper contributes a dataset to evaluate the model on.	This paper proposed an adaptive curriculum learning for thyroid nodule classification based on ultrasound images. The proposed confidence queue and certainty queue can help the model pick out hard samples. Moerover, the authors provide a new dataset for thyroid nodule classification. The experimental results show the superiority of the proposed method against SOTA methods.	The motivation of this paper is innovative and attractive. There are few papers about solving the inconsistent label issue between FNA and TI-RADS. The key innovations of this work include designing a function to discriminate the hardness of a sample, and designing a general learning strategy. There are few works about applying curriculum learning on thyroid nodule classification. The authors contribute a new thyroid nodule classification dataset.	Novel architecture Can be used with different backbone networks	The proposed adaptive threshold function T_{ada} can effectively reflect the predicting state of hard samples during training, and this has been illustrated in the experiment. The quantitative results show the superiority of the proposed method.	The author should supplement more information about the contributed dataset, such as the source of data, device(s) used, image acquisition parameters, instructions to annotators, and methods for quality control. The paper writing can be further improved.	Some unclarities in the methodology	The motivation of the paper is to solve the inconsistency between labels of dataset and the ground truth of biopsy. However, I cannot see any biopsy informations during description of method, e.g., in Eq.(2), there are predictions and the label information, respectively. Please clarify this confusion. What is the setting of Baseline method in experiments? How the model perform on the training setting without discarding hard samples? This will help exhibit the superiority of the proposed method.	The author should supplement more information about the contributed dataset, such as the source of data, device(s) used, image acquisition parameters, instructions to annotators, and methods for quality control.	Dataset will be available Code will be available	I think this paper can be reproduced according to the proposed training algorithm. It will be better to provide the new dataset used in this paper.	"The paper writing can be further improved. (1) In the last paragraph of Introduction, the "";"" before ""(2)"" should be changed to ""."" so as to unifying the formatting. (2) There should be punctuation after each formula. (3) In the first paragraph of 3.3, the left colon for the phrase ""less is more"" should be corrected. (4) The backbone names in Table.2 are better changed to uppercases, such as ResNet, VGG, and DenseNet."	"1- ""Let u and s be the average value and standard deviation of elements in Qh"" You may clarify the intended value to avoid mis-understanding the confidence value vs the number of elements in the queue Qh 2- I didn't get how can Ci (confidence) will be limited from 0.5 to 1, since as per Algorithm 1, the confidence uses the probability after SoftMax which can go outside the 0.5-1 range? 3- Could you please elaborate more how can the probability produced from the DNN represents the confidence of the samples 4- I mean, that T=the two queues Qc and Qh use the same shared prediction probability, however they are by definition representing two different concepts.  5- What is the network that generate yi and yi' in Equation 8? How this is related to P of Algorithm 1 and Equation 1 and 2. I understand that there is a network that should estimate the Ci independently from the backbone network, but the architecture of this network is not clear"	"Proofreading is needed, e.g., in Page 5, ""we propose to embeded"" -> ""....to embed"""	The motivation of this paper is innovative and attractive.	For the conceptual innovation, novel architecture and dataset	The new dataset which contains a large amount of samples and the corresponding biopsy labels. The training algorithm is clear and this guarantees the reproducibility. The better performances obtained by this paper.
296-Paper1674	Leveraging Labeling Representations in Uncertainty-based Semi-supervised Segmentation	"This paper proposes an uncertainty-based method for semi-supervised segmentation. Based on a common teacher-student framework, it introduces a labeling representation module, which is a pre-trained denoising autoencoder (DAE), in order to estimate a ""perfect"" segmentation map from the current prediction. The uncertainty map is estimated by the pixel-wise difference between the teacher and DAE predictions to guide the training of the student model. Experiments show the SOTA segmentation accuracy on the Left Atrium dataset."	This paper proposes a labeling representation-based uncertainty estimation algorithm for semi-supervised segmentation. It obtains better performance than SOTAs on left atrium segmentation from 3D MR volumes in two different settings.	The paper proposed a novel labeling representation-based uncertainty estimation method for  the semi-supervised segmentation, which requires only single inference. Specifically, a pre-trained denoising autoencoder is used to maps the predictions of the segmentation into set of plausible masks. Then the uncertainty is calculated based on the difference between the predicted mask and the reconstructed mask. The experiments show that the proposed network achieves the state-of-the-art results.	I like the idea of DAE, which encodes the global shape prior of the segmentation masks. It helps to predict a plausible segmentation mask for the teacher model and then provides a more reliable reference for uncertainty estimation. The proposed method reduces the total computation since it only needs one single inference for the uncertainty. The experimental comparisons with existing methods and ablation studies demonstrate the SOTA performance of the proposed method on the MRI dataset. This paper is very well-written and easy to follow.	This paper proposes a labeling representation-based uncertainty estimation algorithm for semi-supervised segmentation. It obtains better performance than SOTAs on left atrium segmentation from 3D MR volumes in two different settings.	-The paper proposed a novel way to estimate the pixel-wise uncertainty, which requires only single model inference. -The ablation studies demonstrate the effectiveness and robustness of the proposed uncertainty estimation method. The writing of the paper is clear.	Lack of comparison with existing methods on other datasets. Encoding priors of the segmentation masks with the DAE provides more guidance for uncertainty estimation. However, it might have drawbacks considering the generalization ability. While all the experiments were conducted on the Atrial Segmentation Challenge dataset, how does it perform on other datasets, for example, ISBI LiTS 2017 Challenge and NPC MRI dataset?	Quite simple: It is mainly improvement of mean teacher, why you design it, what is the main differences from mean teacher.  Can you give the motivation much clearer.	The motivation is not strong. Several uncertainty estimation methods which require only single inference have already been proposed. Besides, requiring an additional task to constraints shape prior is not an obvious limitation if it does not require any additional images or annotation for training. The comparison with previous methods is not sufficient. There is a lack of comparison results with several SOTA methods, thus the conclusion that the method of this paper achieves the best result is not strong.	The paper is easy to follow. Technical details are clearly described for reproducing.	yes	It can be implemented easily.	"About the parameter selection, in the ablation study, the authors say that ""we report on b=0.1 in all experiments for a fair comparison."" However, according to Table 4, when b=1, the method achieves the best performance. Then why b=0.1 for all experiments instead of using b=1? If b continues to increase, how does the performance change? While saying ""the proposed uncertainty estimates are more robust than those derived from the entropy variance, requiring multiple inferences strategy"", it would be better to also compare the uncertainty maps. The low computational cost is claimed as one advantage of the proposed method. It would be better to also compare the training time required for different methods (not only the number K). In the ablation study, this paper compares with threshold strategy variant and entropy scheme variant. More explanation is desired. Why the proposed method is better than others?"	This paper proposes a labeling representation-based uncertainty estimation algorithm for semi-supervised segmenation. It obtains better performance than SOTAs on left atrium segmentation from 3D MR volumes in two different settings. The topic appropriates for MICCAI and the technical novelty of the paper is somewhat novel. Its contribution is moderately significant and the coverage of the problem sufficiently comprehensive and balanced. However, following I have some minor questions that the authors should address to improve this work: I strongly recommend authors to release the source code along with the submission, since the learning based projects are typically open-source oriented to facilitate a fair assessment of the performance of the proposed methods for the community. It is mainly improvement of mean teacher, why you design it, what is the main differences from mean teacher.  Can you give the motivation much clearer. A suggestion: A similar uncertainty-aware method: mutual teaching for GCN, you can learn something from it.	Please see the answers of Q5.	The idea of using DEA to reconstruct a segmentation mask for the current non-ideal prediction and estimating uncertainty directly from the pixel-wise difference between the reconstructed segmentation mask and the estimated mask by the student model is novel. This uncertainty estimation only needs one inference stage, thus reducing the computational cost. The experiments on the LA dataset demonstrate the superiority of the proposed method. However, more experiments and deeper analysis would make the paper more convincing.	This paper proposes a labeling representation-based uncertainty estimation algorithm for semi-supervised segmentation. It obtains better performance than SOTAs on left atrium segmentation from 3D MR volumes in two different settings.	Evenly this is a well-presented work, its current motivation is not strong. I can't see the necessity of 'single inference'. My another major concern is the lacking of experimental comprisons.
297-Paper2800	LIDP: A Lung Image Dataset with Pathological Information for Lung Cancer Screening	The authors present a database of CT images suitable for the development and evaluation of pulmonary nodule classification algorithms, of clinical relevance, specially in lung cancer screening. The main contribution of the paper is that, to the authors claim and to my understanding too, this is the first lung cancer screening dataset that has pathological gold standard for its nodules. Previous databases use radiological interpretation of the type of nodule by experienced radiologists, however, there is an inherent likely interpretation error by such label. The well accepted gold standard for nodule classification is biopsy and pathology assessment. All the patients underwent surgery post their CTs and the nodules were analysed pathologically. The database is relatively large (990 CT scans), from a plurality of institutions (8) . The authors further show the poor performance of state of the art algorithms train on other databases on the new database.	The paper introduces a new Chest CT dataset consisting of labeled nodules (location and segmentation) with pathology-based ground truth and demonstrates its importance in comparison to limitations identified in the LIDC_IDRI dataset. Models trained on both datasets are used to demonstrate these findings.	This paper presents a new dataset for lung cancer screening with pathological information, called LIDP. It is emphasized that the LIDC-IDRI lung nodule dataset has been over-studied and has crucial generalization problems.	The strengths of the paper are: A new database of pulmonary nodules with pathology-based gold standard Proof of lack of generality of algorithms developed on the LIDC-IDRI database	The key strength of the paper is that it provides an open set of images by which future research can build off of to develop and evaluate new approaches (detection, classification, etc.) for lung cancer detection. It addresses weaknesses of LIDC-IDRI used in broad research topics by having pathologically confirmed ground truth. This is data, if easy to share and access, would be of a big benefit to the research community in general.	* A new dataset (according to the authors, the largest available dataset with pathological gold standard) is presented to be used as a benchmark for early lung cancer detection and has pathological information instead of radiological analysis.  * The presence of hard-to-classify samples.  * The main disadvantages of the LIDC-IDRI dataset compared to the LIDP are explained, and reasons are given why only one other dataset is used for comparison. * The advantages of using this dataset, which can be used as a supplement to the LIDC-IDRI, are explained in detail. * It correctly explores the statistical and demographic distribution. * Overall, good arguments for the dataset presented.	Clarity of presentations. Even after two reads, it is unclear to me the number of nodules in the database. What is the precise meaning of 'samples'? Do the author refer to CT scans, nodules or pathology reports? Please clarify. Please confirm that all visible nodules on the CT scans have pathology-based reference standard.  The section on the visualization of the datasets is not very informative, since there is a lot of processing involved. What are the features extracted? Are the features from the nodules or from the whole CT? A more interesting analysis would be the comparison on the image characteristics of LIDP and LIDC-IDRI with respect to reconstruction kernels, slice thicknesses, etc... Simply that information could explain the differences in visualization of figure 1.	"The paper attempts to cover a wide area of topics and ends up missing important details that make it difficult to assess the quality of the data curated. It would be better to eliminate extraneous material in the manuscript and focus on details around the dataset and its preparation to better emphasize why this dataset is of sufficient quality or to highlight its limitations for future research.  Proper adjudication appears to be missing, making the dataset less appealing for research without re-labeling. Most striking is the following: ADJUDICATION - section 3.1 - The contour ""was checked twice"" seems like a very unscientific way to establish a contour."	The experimental part should be extended, e.g. in Table 2 it can be confirmed that LIDC-IDRI does not generalize well for other datasets like LIDP, but it is not possible to confirm that LIDP generalises correctly. I also agree that LIDP is very complementary to LIDC-IDRI, so it would be interesting to improve the evaluation by using both datasets to train models. The dataset is very unbalanced, containing many more malignant than benign cases, which can lead to an increase in false positives (malignant) when training the models. This is evidenced in your table 3 by high recall values but low specificity values.  No information about the size (gigabyte) and the resolution of the dataset.	This paper will only be reproducible if the database is made publicly available. The authors do not mention that on the paper. I strongly recommend doing so.	The paper is proposing to share a new dataset making this portion extremely reproducible. In terms of the Machine learning approaches, they appear to be extremely difficult or impossible to reproduce.	Even knowing that the main purpose of this paper is to present a new dataset, it should have included a description of the computer infrastructure used (hardware and software), more information about the memory requirements and other specifications about the training and testing process.	Define LIDP. What does it stand for? It is very unclear what do the authors refer as 'samples'. Please clarify. It is sobering to see negative results published. Having another dataset for nodule classification is of great use to the community.	"The following details limitations in the manuscript that could be better if detailed or eliminated to allow for more room to detail the dataset further. *Claims made should be cited: DNN models have ""become one of the main computer assisted techniques for early screening of lung cancer"" - please provide a reference to this. Currently DNN is leading research approaches but in terms of actual products themselves has there been a study demonstrating that DNN products are dominantly used in lung cancer screening? Page 2- ""Data with a score of 3"" - what does this mean? This should be detailed further. ""the classic DNN model"" - page 3 top - this is simply too general as there are many different DNN networks. There is no ""classic"" model. This should be removed from the paper unless more detail could be provided. Reference [20] - Wu, G.X., Raz, D.J.: Lung cancer screening. Lung Cancer pp. 1-23 (2016) - appears to be incomplete. Section 3.1 - length of a nodule is introduced here and never mentioned elsewhere - do the authors mean diameter? ADJUDICATION - section 3.1 - The contour ""was checked twice"" seems like a very unscientific way to establish a contour The choice of patient age >18 is strange since most approaches either target age >50 for screening or > 35 for incidental findings. * ""Maligancy of 3"" is not clear - Does this mean the malignancy was detected 3 years later? Section 3.4 Visualization of the dataset does not add much to the paper - consider removing this section Consider removing or abbreviating Section 4 - there are simply not enough details to really warrant the multiple experiments. It leaves the reader with multiple questions that are not sufficiently covered in the manuscript and simply weakness the manuscript as a whole. How were operating points selected? How was the data split? What was the split?? How was the training and check point selection performed? What architecture was used?"	"* The introduction should be revised, as well as all references (the paper starts with the 18th reference). * Be consistent when creating acronyms, e.g., whether they start with a capital letter (Lung Image Database Consortium and Image Database Resource Initiative (LIDC-IDRI)) or not (Low-dose computed tomography(LDCT)). * Add reference for the Project Bell. * Explain the reason for the second sentence in point 3.1 (""When collecting...""), it is important for less experienced readers.  * Why do the legends in the diagrams (LC015) differ from the actual name of the dataset (LIDP)?  * - It would be interesting to train with the LIDP dataset and test with the LIDC-IDRI. * Please increase the quality of the discussion in point 4.3."	"I rate the paper as ""Strong accept"" assuming that the database is going to be made publicly available. If that is not the case, I would rate it as weak reject. As stated above, the main contribution of the paper is the very strong dataset that the authors have generated by using the gold standard pathology information. This could be a landmark database for pulmonary nodule classification AI models."	The paper introduces a much needed pathology-based cancer ground truth to the research community. This is a great step in enhancing both training, understanding, and evaluation among future papers. Focusing on the dataset preparation itself would greatly strengthen the paper. As it is, it attempts to cover too many topics with too little detail.	This paper presents good arguments for the presented dataset (LIDP) and identifies important weaknesses in the LIDC-IDRI dataset. Further testing and more details on these tests are needed to assess the quality of the new data, but it seems to have great potential and covers the gaps in the existing benchmark dataset. However, I am not entirely confident that the introduction of new datasets is appropriate for MICCAI.
298-Paper2088	LifeLonger: A Benchmark for Continual Disease Classification	The authors propose a benchmark for different continual learning scenarios (task incremental, class incremental and cross-domain incremental) base on the MedMNIST dataset. They provide the setup of the benchmark and run multiple state-of-the-art continual learning methods as baselines.	This paper presents a benchmark for continual learning algorithms on 4 datasets from the MedicalMNIST collection, for multi-class classification. They evaluate state of the art continual learning algorithms in task-, class- and domain- incremental learning settings. The cross-domain incremental learning setting is newly defined in this paper, where each domain is a different dataset, with different classification task.	"Similar to the ""SplitMNIST"" and ""PermutedMNIST"" benchmarks commonly used in continual learning literature, the authors propose using ""MedMNIST"" as a simple, computationally inexpensive benchmark dataset for continual disease classification. They evaluate five different methods in three (or four, depending on how these are viewed) settings and report the average accuracy and forgetting scores."	The authors provide an extensive evaluation of five baseline methods for task and class incremental learning. Using a public available dataset such as the MedMNIST dataset is highly beneficial for a benchmark.	a benchmark for continual learning algorithms on medical data is needed the authors compare state of the art continual learning methods on 4 medical classification tasks	"Using MedMNIST as a standardized dataset for benchmarking continual learning methods in the clinical domain is a simple yet sensible idea. The data is small enough that experiments would require minimal computational resources, but the results would be likelier to transfer to medical applications than those obtained with datasets such as SplitMNIST or SplitCIFAR. Figure 1 successfully helps illustrate the setting and the different learning scenarios. While many papers have compared EWC, MAS, LwC and iCarL, the fact that the authors include ""End-to-End Incremental Learning"" as a bias correction method is a good idea, especially for the class incremental experiments."	One of the main novelties of the benchmark the cross-domain incremental learning is poorly motivated. It is not clear how the knowledge learned on e.g. BloodMNIST could be beneficial for learning on data of e.g. PathMNIST. The evaluation for cross-domain incremental learning is limited with only three baseline methods compared. Why are the other approaches not used for cross-domain incremental learning? For all evaluations an upper bound by running joint training on all tasks/classes at once is missing. Such an upper bound is relevant to judge the performance of the continual learning methods.	commonly in domain adaptation, different domains are given by e.g. data from different hospitals, but the task stays the same. Here, domains are defined as different datasets with also different tasks, e.g. CT scans for organ classification vs kidney microscopy images for cell classification.	"Though I understand the advantages of a computationally inexpensive benchmark dataset, MedMNIST includes very unrealistic images with a resolution of 28x28. It is unclear how results in this benchmark would translate to more realistic medical image applications. Considering that the main contribution lies in sharing benchmark results, a major weakness lies in the choice and definition of the metrics. The forgetting metric, which observes the difference ""between the highest and lowest accuracy for each task"" does not follow conventions in continual learning literature that quantify forgetting as the difference in performance directly after training the model with a certain task and after continuing training with future tasks (Diaz-Rodriguez at al.). The authors also do not measure forward transfer or include any metric that quantifies loss in model plasticity. For the ""cross-domain incremental learning"" setting, the ""domains"" are too different. It is unclear how much this would mimic the situation that the authors describe, namely that of ""datasets originating from different institutions."" In addition, the authors name the introduction of this scenario as a key contribution, yet previous research has looked at similar settings within medical imaging (Perkonigg at al., Srivastava at al., Memmel at al.), even research that the authors cite (Srivastava at al., Memmel at al.). It would have been preferable to evaluate a pseudo-rehearsal method alongside iCarl, which can be considered an upper bound in many scenarios. References: (Diaz-Rodriguez at al.) Diaz-Rodriguez N, Lomonaco V, Filliat D, Maltoni D. Don't forget, there is more than forgetting: new metrics for Continual Learning. In Workshop on Continual Learning, NeurIPS 2018 (Neural Information Processing Systems 2018 Dec 7. (Perkonigg at al.) Perkonigg M, Hofmanninger J, Herold CJ, Brink JA, Pianykh O, Prosch H, Langs G. Dynamic memory to alleviate catastrophic forgetting in continual learning with medical imaging. Nature Communications. 2021 Sep 28;12(1):1-2. (Srivastava at al.) Srivastava S, Yaqub M, Nandakumar K, Ge Z, Mahapatra D. Continual domain incremental learning for chest x-ray classification in low-resource clinical settings. InDomain Adaptation and Representation Transfer, and Affordable Healthcare and AI for Resource Diverse Global Health 2021 Oct 1 (pp. 226-238). Springer, Cham. (Memmel at al.) Memmel M, Gonzalez C, Mukhopadhyay A. Adversarial continual learning for multi-domain hippocampal segmentation. InDomain Adaptation and Representation Transfer, and Affordable Healthcare and AI for Resource Diverse Global Health 2021 Oct 1 (pp. 35-45). Springer, Cham."	The code of the paper will be made available publicly.  From the checklist authors claim that the software framework and runtime/memory footprint statistics are included, however both are not included in the paper.	The authors plan to publish their code	The authors fulfill all necessary reproducibility criteria.	"Authors should provide more insight into how cross-domain incremental learning could be beneficial by e.g. showing results of an experiment showing that subsequent domains could benefit from previous learned knowledge when fine tuned. Rather than cross-domain incremental a commonly tackled continual learning scenario is domain incremental learning, where the domains are usually more related than in the proposed benchmark. Examples for such work are: - Perkonigg, M., et al. ""Dynamic memory to alleviate catastrophic forgetting in continual learning with medical imaging."" Nature Communications 12.1 (2021): 1-12. - Gonzalez, C., Georgios S., and Mukhopadhyay, A.. ""What is Wrong with Continual Learning in Medical Image Segmentation?."" arXiv preprint arXiv:2010.11008 (2020). - Srivastava, S. et al. ""Continual domain incremental learning for chest x-ray classification in low-resource clinical settings"" (2021). It might be worth to include this domain incremental setting into the benchmark. A more detailed discussion of the results could help to gain insights into the results. For example the differences in performance of class incremental learning on different datasets is large. Why is CI learning on TisseMNIST only reaching 32.0, while on BloodMNIST 67.7 is possible? Could authors offer an explanation/intuition for that? The benchmark presented is limited to 2D on relatively small images, which is an important step in defining benchmarks for continual learning in medical imaging settings. However, a 3D version of such a benchmark is needed to truly evaluate the potential of different CL methods."	"the definition of the cross-domain learning setting is confusing. Commonly in domain adaptation, different domains are given by e.g. data from different hospitals, different modalities or scanners, but the task (e.g. classification of blood cells) stays the same. I wonder if it is needed to have a model that can classify such different images as CT scans and microscopy images. how did the authors define the split into different tasks? Does the order of learning the different tasks/ classes make a difference? In Figure 3 it should be '...right column indicates the average accuracy....' it would be interesting to add an upper bound like joint training on the whole dataset, similar to [1], to the comparison. [1] Van de Ven, Gido M., and Andreas S. Tolias. ""Three scenarios for continual learning."" arXiv preprint arXiv:1904.07734 (2019)."	"When outlying the contributions, I suggest that the authors focus on the first of the three contributions listed. As previously stated, scenarios similar to the proposed ""cross-domain incremental learning"" have been explored in the past, so I would not deem this to be a separate contribution. The third point is ""We explore task and class incremental learning scenarios of continual learning, to respond well to new labels i.e. diseases for multi-class disease classification."", and it is unclear how this is different from the first contribution where the different scenarios are already presented. I suggest that the authors rephrase the definitions for the continual learning scenarios in the abstract, as the definitions in the main text are much more understandable. One could also argue that there are four, not three, scenarios as ""cross-domain incremental learning"" can be ""domain-aware"" or ""domain-agnostic"". What does ""fine-grained cross-domain incremental learning"" stand for? The explanations for well-known concepts in continual learning, such as the introduction of catastrophic forgetting, take up too much space. I would suggest that the authors instead focus on the necessity of a unified benchmark dataset to drive forward continual learning research in the medical imaging community. When introducing continual learning strategies in page 5, I would suggest that the authors combine the subtitles with the text, e.g. instead of ""\textbf{Regularization methods} They reduce"" ""\textbf{Regularization methods} reduce"". This would save space and be easier to read. In the definition of the ""average accuracy"" metric, ""t"" is used for both the number of tasks until a certain task as well as the index of the last task. I would suggest that the authors use a different symbol for the number of tasks. The titles in Table 1 are too close together, making the table difficult to read. I would also suggest that the authors separate Table 2 and Table 3 into different pages. Please explain directly why Table 3 only has 3 rows (excluding the lower baseline and several methods). Also, please explicitly state that Table 3 contains results for the ""Cross-domain incremental learning"" scenario. When stating ""we train the model [...] with the option of early stopping in the occurrence of overfitting."", please explain the early stopping strategy. ""by combining regularization term with the classification loss"" is missing a ""the"" or ""a"" before ""regularization."" ""toward classes, associated with the most recently learnt task."" I would suggest removing the comma and using ""learned"". ""clinical practise"" -> ""clinical practice"""	While task and class incremental learning are clearly defined and evaluated sufficiently, the motivation and evaluation of cross-domain incremental learning remains unclear. In addition more discussion on the results in all continual learning scenarios would be needed for accepting the paper.	Although a benchmark for continual learning methods for medical image classification is interesting and important for the field, the definition of the cross-domain incremental learning setting is not clinically relevant.	While I am unsure about how transferable MedMNIST is as a benchmark for continual medical image classification, establishing the three proposed settings as a first/complementary way to evaluate continual learning methods would help standardize the evaluation in the medical domain. However, metrics should follow existing conventions and include forwards transfer.
299-Paper0447	LiftReg: Limited Angle 2D/3D Deformable Registration	This paper proposes a new network for performing 2D/3D registration from limited view x-ray. Feasibility is demonstrated using CT + DRRs	This paper provides a deformable 2D-3D registration algorithm based on an artificial intelligence algorithm capable of regressing deformation field vector between the source and the target images. A unique aspect of this algorithm is the back projection of 2D projections into a 3D space and regressing the transformation between the back-projected 3D space and the source 3D space. The performance of the developed method has been evaluated on two a publicly available dataset and according to target registration error and dice coefficient metrics. As a core contribution, the training loss is calculated in a 3D space allowing for better representation of the deformation parameters in all directions (specifically along the projection direction).	The manuscript describes a method to predict deformation vector fields between a 3D source image and 2D limited-sampled projections.	This is a very well written paper with some interesting ideas as well as good experimental methods. It does a good job explaining the overall reasoning and details of the methods and goes on to do a good amount of experiments, especially comparing to multiple prior works.	"The proposed network architecture operates based on first, ""lifting"" the 2D projections into a 3D space. This 3D space can be noted as a pseudo CT representation, whereby a deformation field is regressed between the lifted 3D space and the source 3D space. This problem formulation is unique and may help in alleviating the spatial ambiguity of traditional 2D-3D registration methods. The training loss is also an interesting point, where it combined a regularization term dedicated to the deformation field's basis coefficients and the similarity between the warped and the original 3D volume. The provided evaluation study is feasible and compares the performance of this algorithm to the state-of-the-art techniques such as regnet. The paper is clear and follows a proper structure."	The use of deep learning to solve the iterative 2D-3D registration problem.	The main weakness of the paper is that the number of and variation of x-ray imaging angles is only briefly described in the introduction and figure 1, but not enough detail is given.	Although this work is presented as a 2D-3D registration paradigm, to the best of my comprehension, no evaluation is performed based on real 2D data. The evaluation dataset was in fact created following the same DRR generation framework used during the training process. This point undermines the clinical applicability of the developed algorithm. Furthermore, there appears to be a disconnect between the mathematical expressions and the illustration (e.g., Fig 1). Based on the provided evaluation study, it appears that the improvement made in the Dice coefficient metric compared to the existing networks (e.g., regnet) is very minimal and the authors' justification for this is not sound. As shown in the ablation study, the back-projection (aka., lifting) process has minimal impact on the accuracy, this is not well justified in the manuscript.	The use of a population-based PCA model lacks support and validation. Different patients may have very different anatomy distributions and mechanics, which result in very different deformation patterns. The Lift3D module mainly serves to back-project. Why not use a standard back-projection layer that is differentiable? The stacking of 3D volumes from all angles lacks justification.	Code will be released upon acceptance, and DIRLAB is public data. In order to reproduce this network, the most difficult step will likely be the spatial warping transformer, so hopefully this will be included in the code release.	The training datasets and the associated codes are disclosed in this work. The implemented code specific to this method however is not disclosed. The amount of implementation details provided in the main text and the supplementary materials is sufficient.	NA	I think this is a very interesting paper with not only novel methods but good experimental results. In the future, it would be good to have more of a description of real applications of this work.	An evaluation on real 2D X-ray images should be included to better motivate the clinical usability of this work. In the subsequent publications on this work, a comprehensive evaluation based on real clinical 2D data must be included. Another assumption made in this paper is the presence of X-ray calibration parameters for DRR generation purposes. This may not be a valid assumption in a clinical setting. The authors should amend the conclusion section to include aspects regarding the calibration parameters as well. In general, the illustrations are of poor quality and are not well connected to the mathematical expressions within the text.	The paper needs to justify the use of the PCA to serve a population-based deformation model. Also the use of Lift3D module needs to be justified and compared with standard 3D reconstruction layers. The paper should also make clear how the training/validation/testing division was done and whether they used different patients.	Overall this paper has everything I look for in a good MICCAI paper: Important application, technical novelty, and most importantly good experimental design and results showing improvement over prior published methods.	This work provides a new perspective on deformable 2D-3D registration using artificial intelligence. The developed network architecture and the back-projection process is unique and of value for the community. No evaluation of real clinical data (specifically 2D X-rays) is provided.	The overall study design lacks support, especially on using the PCA-based DVF model across the full population. The lift3D module also lacks justification.
300-Paper2336	Light-weight spatio-temporal graphs for segmentation and ejection fraction prediction in cardiac ultrasound	The paper proposes a graph convolutional network (GCN) for segmenting the myocardial border of the left ventricle in single-frame echo. They also propose a ulti-frame model with a GCN branch for ED/ES segmentation and other branches for EF regression and ED/ES frame classification.	In this work, the authors propose a CNN + GCN based approach for joint LV segmentation and EF estimation, from cardiac Ultrasound images. The segmentation is done via keypoint regression, as opposed to semantic segmentation. This seems to be the key novelty here. Spiral Net, a type of GCN, is used for the keypoint regression. They train a single frame model for keypoint regression. They also train a multi-frame model, which does both keypoint regression + EF estimation. EF can be estimated from either the keypoints, or the direct estimation. The direct regression based estimation seems more accurate.  The keypoints that are outputted help enhance explainability.	The authors propose a DL model for the segmentation of the LV in four-chamber-view using echocardiography. Thanks to the combination with graph neural-networks, they are able to recover 2D contours instead of just a segmentation mask. The model is fast thanks to the use of light architectures, and is evaluated against the SOA.	Application of GCN for efficient LV segmentation in echo is interesting and relevant. A public dataset is used for the experiments and the source code will be released.	The main interesting aspect is the adaptation of mesh based formulation of spiral net, to contours (which, makes sense since contours are even simpler than meshes), and using that to do keypoint regression of the LV boundaries. This sort of approach would be ideal if estimating well defined keypoints in an anatomy was necessary. Having the keypoints/segmentations is also helpful in enhancing the explainability of the model. Clinicians generally like it better if they can visualize the LV contours used for EF calculation. Different components of the method are presented as building blocks and can be mixed and matched as needed. Results are presented for both keypoint regression errors and the EF errors.	Strong comparison against the SOA The research is clinically relevant. Authors focus on prediction time, which is critical for real-world applications but often omitted in research.	The method has a failed to achieve improved results compared to existing methods (both in table 1 and table 2). The papers lacks the ablation study for the multi-frame GCN model. The GCN branch is parallel to the EF regressor; the results of EF regressor with and without GCN is not presented.	The keypoint based approach may not be easy to emulate in a different setting with a different set of training data. Do the keypoints have to be in anatomical correspondence? I guess so, given their name? If so, it'd be quite hard to build a large dataset with keypoints that are registered to one another. Or is it that only apex/base keypoints are labeled? Perhaps this part is just unclear to me. While more interesting, and also explainable, the keypoint based approach doesn't seem to give better EF  results. About spiral net - I think the reason its preferred in the original literature is that it bakes in a stronger inductive bias about the graph. Prior to this, message passing was done by aggregating messages from neighbors in a permutation invariant way. I thought that with spiral net, you don't need this requirement because now you have well defined neighborhood encoding (just like in CNN)? It'd be better to state that explicitly in the paper as opposed to just mentioning the efficiency. Because there's the single frame method and the multiple frame method with keypoints, ed/es classification, direct regression, it's a bit hard to follow right away.	I do not see any major weaknesses in the paper, but explanations on the methodology could have been improved.	The code will be released and the used dataset is public.	Reproducibility seems fine. Components are modular. Code is shared online. One issue is that there are multiple ways their approach could be used so, for someone else to compare to this work, it could be a bit tedious.	Details on the implementation are in the manuscript and supplementary material. Authors plan to release the code repository. They use public datasets. Note to the authors: You can use https://anonymous.4open.science/ for providing an anonimous link to your repository during review.	"The naming of the 4th row of table 1 is misleading (calling it ""GCN - Regression""). This row is basically a CNN video encoder plus EF regressor (does not have GCN layers). The GCN branch is a parallel path to EF regressor and does not directly contribute to regressed EF results. The author can add an ablation study to show in what extent the GCN path is contributing to the regressed EF results (reporting EF regression with and without GCN path in the multi-frame GCN - Fig 1 and Table 2). If the ablation results supports the paper's claim and verifies that the GCN is improving the EF regression results, I'd change my review rating to ""accept"". The paper could include power analysis to show whether the differences across reported results are statistically significant."	It wasn't clear how exactly is the ED/ES classification is used ultimately. In the multi-frame GCN setup, the output is (40 X 2) keypoints for ED and ES. Is the graph also composed for 2 frames only - the ED and ES? And is the information simply concatenated to the feature or used explicitly to form the graph from 2 frames only? Your main figure (figure 1) - has B, C, W, H variables which I didn't seem defined elsewhere. What is B by the way? Batch size? In 2.1 (Encoder) section - you say the original image/video needs to be compressed to meet the input requirements. This could perhaps be phrased a bit better. Because you're not just compressing - you're trying to learn relevant features that are useful downstream, right?	"Major comments: Current experiments make it difficult to correctly assess the exact contribution of the GCN head. It is missing comparison of the proposed model with MobileNetv2/ ResNet18  without the GCN part. In the EF prediction, the comparison between your EF regression against the SOA volume based methods is unfair. Again regarding the EF prediction, why the authors think that the whole  video results worse? Moreover, the best results need to be highlighted in bold in the Table. Why are the SOA networks different in the EF prediction and segmentation task? Minor comments (pg 1) ""EF describes the blood volume pumped by the heart  in each cycle"": That is the stroke volume. Ejection fraction is as defined in pg 4. More details are needed on the graph convolutional point decoder. What are spiral connections? Figure 1, Please state what are  B, C, W in the legend. ""Q1 How accurate, efficient and robust is segmentation of a single frame"", Fig 2: "" the GCN (with MobileNet2 backbone)  is more robust"" :   Add the Hausdorff distance should be added to Table 1), also, quantify the number of outliers. Details of the machine settings are important when reporting timing. Looking at the processing time per frame in Table 1, it seems that the authors managed to reach real time. Does it still hold even when adding the image preprocessing time/loading time of the network? If so, I would add it to the manuscript since it is an important achievement."	Please see the weaknesses. My main concerns are lack of the ablation study and significance of results.	Its a good paper, with a really nice formulation for the keypoints regression. However, the keypoints are hard to come by in real life - at least the way I'm understanding them - maybe wrong about it.	I like the fact that their model has potential to be used and deployed in clinical practise due to its high computational speed. 2D echocardiography is the main imaging modality in cardiology, and one of its drawbacks is the difficulty in its quantification. Therefore I think that this contribution is an important step towards overcome that issue. As stated by the authors in their text, the fact that a 2D contour of the endocardium is provided as output can be useful for many other applications.
301-Paper2383	Local Attention Graph-based Transformer for Multi-target Genetic Alteration Prediction	The paper presents a local attention graph-based Transformer for multiple instance learning with an efficiently adapted loss function to learn expressive WSI embeddings for the joint analysis of microsatellite instability and mutation prediction.	The authors proposed LA-MIL, a local attention-graph based transformer for multiple instance learning in whole slide images. The authors demonstrated LA-MIL could effectively predict microsatellite instability and tumor mutational burden jointly with genetic alterations in gastrointestinal cancer.	A local-attention graph-based transformer model is proposed to restrict self-attention calculations which improves the performance of mutation prediction comparing with the state-of-the-art methods in many cases.	A framework integrates a novel local attention graph-based Transformer that restricts self-attention calculations in Transformers by using kNN graphs to model regional regimes within a tile. Validations were done on two TCGA datasets for gastrointestinal cancer with reasonable baselines.	The proposed method addressed important issues in current MIL methods for WSIs. For example, passing bag labels to instances may introduce noises as instance labels might be inconsistent with bag labels. Also, a lot of existing methods are not leveraging spatial correlation between instances, which contains important information in WSIs. The method is clearly described and the visualization helps to understand the proposed method more intuitively.	Well motivated proposed approach, good analysis of the problem at hand, latest deep learning based approach to solve problem, careful experiments and clear analysis of the results, comparison with state-of-the-art methods are given.	"There are no explicit major concerns,  the following relevant work should be discussed, Shao, Zhuchen, et al. ""Transmil: Transformer based correlated multiple instance learning for whole slide image classification."" Advances in Neural Information Processing Systems 34 (2021). Please check my minor comments at 7."	While the paper is well-written, the experiments part of the paper could be further strengthened to better demonstrate the effectiveness of the proposed method.	Nothing	The authors claimed that the implementation would be published. The paper should be reproducible.	The reproducibility of the paper is good. The authors provided necessary details from the reproducibility checklist.	Datasets and experimental setup are clearly mentioned.	I assume all the results are validated with a statistical test; please clarify. Sensitivity analysis or relevant discussion should be added for the trade-off of using LA-MIL and T-MIL.	While multi-target prediction is very useful as it allows predicting different phenotypes through one single model. However, it would be interesting to see if these tasks are helping each other for more effective prediction. The author could perform additional experiments to train and predict each task individually using the proposed method and compare with the multi-target prediction results. For results in Table 2, while T-MIL and LA-MIL contain standard deviation, the existing methods do not. Are these results generated through predictions on the same set of cross validation partitions? If not, please ensure that the compared methods are trained and tested on the same set of training and testing data for a fair comparison. For the visualization in Fig.3, could the author also include visualization for T-MIL as it seems to have similar performance as LA-MIL?	Nothing	The first attempt to predict microsatellite instability or tumor mutational burden with genetic alterations. The first transformer-based approach for mutation prediction.	The paper addresses important issues in applying multiple instance learning to prediction tasks in whole slide images.	Main strengths of the paper.
302-Paper0578	Local Graph Fusion of Multi-View MR Images for Knee Osteoarthritis Diagnosis	This paper proposed a local graph fusion network (LGF-Net) to extract features from multi-view MR images for knee osteoarthritis (OA) diagnosis. Specifically, a knee graph is first constructed based on the segmentation of sagittal-view MR images and the intersection of the multi-view MR images. After that, a local graph fusion (LGF) module is devised for the fusion of multi-view patches. Moreover, a graph transformer network (GTN) is developed to aggregate the features among different patches and to predict the grading of OA. Experimental results demonstrate the effectiveness of the proposed LGF-Net.	In this work, the authors propose a graph fusion network to fuse local patch information from multi-view MR images for knee OA classification. The proposed framework includes a knee graph construction step using knee segmentation labels and interpolation points of multi-view slices, and a local fusion network (LFN) to encode each vertex's local patch information, and then a graph transformer network (GTN) to aggregate multi-view patch features along the edges of generated graph in the first module. The proposed framework has obtained higher performance by comparing with some related papers.	The authors used multi-contrast MR images of the knee to identify WORMS of the knee for osteoarthritis.  Unlike previous approaches of fusing the information from multi-contrast/multiview images at later stages of a network, the authors proposed to use a local fusion using a graph transformer network. The paper demonstrated that the local fusion with registered MR images using bone segmentation and a graph construction.	The paper is well organized and clearly clarified. It proposed a novel LGF-Net to gradually fuse the multi-view MR images for knee OA grading. Experimental results show the best performance of the proposed LGF-Net and the effectiveness of GTN. This work can benefit the clinic diagnosis of OA based multi-view MR images.	I agree that fusing local patch from multi-view images could benefit the knee OA diagnosis problem, and I think the local information can better reflect the degree of OA than the global information. Combining the local patch information from multi-view slices and the GTN obtains good performance increase.	The authors proposed a novel method to fuse images from multi-view knee images using graph transformer network and local information. Previous approaches of fusing multi-contrast MR images relied only on late fusion strategies. The authors evaluate their approach in a comparative study with the current available approaches and presented that the proposed approach provided superior performance. The proposed approach can be extended to other MRI relevant tasks where multi-contrast images are used.	"Several sections are not well illustrated. Please see as below, the selection of slices from the three views for intersecting is not well illustrated. It is the basis for the construction of graph. the number of vertices N is not displayed.  What is the meaning of the sentence "" Ni =9 can include the i-th vertex itself"" ? Doesn't the i-th vertex is the nearest vertex of vi?  It would be better if the meanings of S, C, and A in Table 1 can be displayed. Additionally, there is another S at the last sentence of the second paragraph in Page 4. Section 3.3 is kinda of redundant with Section 3.2. I suggest to merge the two sections. Moreover,  the specific pre-training strategy is not well illustrated. Do you pre-train the LGF module using an existing dataset or just load weights from an existing model?"	"The whole proposed framework and its application of this article is highly similar to this work: Knee Cartilage Defect Assessment by Graph Representation and Surface Convolution. The authors should give the discussion and comparison with the above work. I also find a previous paper called: Graph Transformer Networks, it also proposed a ""Graph Transformer layer"". But the authors did not cite this work, and there has no discussion or comparison with this work."	None	I am not sure whether the propsed method can be re-implemented because some points are not well illustrated.	I think the reproducibility of this work is ok.	It will be great to have the code availability included within the paper for reproducibility.	Some details are required to display. Please see the the main weakness.	"The authors should do a more comprehensive related paper review. If possible, I hope I could get a very detailed explanation in the rebuttal. When the ""PD"" abbreviation first appears, you should give it the full name (proton-density-weighted sequence). And, in table 1, in this sentence ""LGF-Net (No PT)"", does the ""PT"" mean ""PD""? Some specific implementation details of the ""Projection"" step in the ""Knee Graph Construction"" could be introduced. Dose the projection or ""multi-view slices alignment"" utilize slice registration or an external positioning hardware during data sampling?"	The proposed approach clearly outperforms the current available approaches for grading OA using multi-contrast knee MR images. Please clarify why did the authors defined crop size of the knee region by given fixed mms? is it related to the details of WORMS? For the future work, it would be interesting to see the effect of individual pipelines within the knee graph construction. for example, how crucial is to have an accurate segmentation or how bad bone segmentations could be and the approach will be still performing well.	The paper is well organized and propose a novel framework for multi-view MR image fusion to diagnose knee OA. The experimental results are the best.	Effectiveness and some possible innovations of method for the knee OA problem. But the article lacks discussion and comparison with high similarity articles, which is a big flaw.	The presented approach is novel and it is of community interest as multi-contrast MR images are used for diagnosis. Comparative and ablation studies are performed and explained in great detail.
303-Paper0919	Localizing the Recurrent Laryngeal Nerve via Ultrasound with a Bayesian Shape Framework	This paper presented a  learning-based framework to identify the RLN from a US image for pre-operative assessment of contraindication for robotic thyroidectomy, which introduces Bayesian shape alignment and Locate-Net for geometrical constraints and localization result refinement, respectively.	Clinically, this paper helps clinicians to find the recurrent laryngeal nerve with ultrasound images. Technically, it puts forward a coarse-to-fine method to find the nerves in a segmentation network.	The authThis paper proposed a recurrent laryngeal nerve (RLN) localization method in ultrasound images. The Bayesian shape alignment is applied to obtain the ROI center according to the surrounding organ segmentation results. Small and large ROI patches are cropped from the ultrasound image and then fed into a coordinates regression network.	This paper is well-written and organized. The idea of mimicking the standard approach surgeons take to identify the RLN according to its surrounding organs and using the inherent relative spatial relationships between organs is interesting and proved helpful.	This paper puts forward a method which could solve a practical clinical problem. The method's performance is good. The study design is impressive: lots of patients were involved; the new method is compared with many current methods; an ablation study shows that both local information and global context contributed to the good performance; visualization of the result is convincing.	The authors proposed a novel approach to obtaining the candidate ROI centers that utilized prior clinical knowledge. In particular, Bayesian shape alignment introduces the prior knowledge that models the spatial relationship of RLN, common carotid arteries (CCA), thyroid, and trachea, which gives a good initial location for subsequence refinement. The final results of RLN localization are significantly better than the baseline heatmap-based/regression-based methods.	More details about the experiments and analysis related to the results should be included.	It would be better if the dataset could include negative cases, so that the accuracy is more meaningful. For ultrasound images, it is better if authors could discuss how to select qualified frames from all images, because each sonographer could have a different acquisition habit. And this is a protocol that could be used for a multi-center study.	The writing is basically good, however, many errors are not checked in the manuscript, e.g., the result of heatmap-based method UNET is put in the coord-based section in Table. 1, missing the first name of the first author in reference [1], etc. Some details of the dataset are missing, the total scan frame number should be given, the original resolution of the frames is missing, and how the disagreement of different ground truths is handled as three clinical experts annotated the data. The Tabel.2 shows that the distance/hit rate are 7.52 pix/89.0% for left RLN, which is already significantly better than the baseline methods. Why the Coord-based/heatmap-based methods' performance is so terrible?  Could the authors interpret the poor results of the baseline models? Were these models well trained? Many hyperparameters are set without explanation, such as the crop size of local/global patches, the learning rate and batch size of model training, and the threshold distance for hit rate evaluation. If you follow the setting of previous works, please cite them.	I believe that the obtained results can be reproduced.	The dataset will not be public: this is a concern. However, authors mention that the network and trained weights could be found later.	All the experiments are conducted with private dataset. The performance of baseline method is too low, I don't know if the authors well trained the baseline models, however, the authors didn't give any explanation, such as the training losses curves of baseline models.	It would be better to explain more about the relationships among the surrounding organs.	It would be better if the dataset could include negative cases, so that the accuracy is more meaningful. For ultrasound images, it is better if authors could discuss how to select qualified frames from all images, because each sonographer could have a different acquisition habit. And this is a protocol that could be used for a multi-center study.	Besides the above comments, I have some further suggestions. The authors can test their method in some other public localization datasets to improve reproducibility. There would be more structure priors in 3D images, the author can consider extending the work in 3D images.	The novelty of the proposed method and the experiments are sufficient for weak accept.	This paper puts forward a method which could solve a practical clinical problem. The method's performance is good. The study design is impressive: lots of patients were involved; the new method is compared with many current methods; an ablation study shows that both local information and global context contributed to the good performance; visualization of the result is convincing.	The proposed method is reasonable and novel. However I am not convinced by the results, especially the baseline models' performance.
304-Paper1059	Local-Region and Cross-Dataset Contrastive Learning for Retinal Vessel Segmentation	The authors present a novel segmentation network for the retina vessel segmentation task. The main idea of the paper is to apply contrastive learning into the vessel segmentation network. The contrastive learning approaches are studied in the paper. One is the local region contrastive learning by selecting hard samples in a local region manner. The second one is the generalization of local regions to batches inside the whole dataset. Experiments show that incorporating contrastive learning to standard segmentation networks boosts the segmentation results.	The paper introduces a contrastive learning based algorithm for retinal vessel segmentation. The algorithm consists of a local intra-region contrastive learning strategy and a global cross-dataset contrastive learning strategy.	This paper proposes a framework to improve retinal vessel segmentation performance on some challenging pixels via the local-region and cross-dataset contrastive learning. In specific, the authors use a quality-aware anchor sampler to select the challenging pixels of false predictions and then construct contrastive loss in both the local area (with pixels from the same cropped patch) and global region (the other patches stored in the memory bank). The authors have verified the proposed method on DRIVE and CHASEDB1 datasets.	The strengths of the paper: The overall paper is well organized and the writing of the paper is great. Although deep learning segmentation network methods are extensively studied for vessel segmentation, the authors present a novel way to further improve the segmentation results by contrastive learning approaches.  The presented method consists of local and global contrastive learning means to select hard samples and provide additional supervision into the segmentation network. The authors provide experiments to show the effectiveness of the proposed contrastive learning methods. Results compared to previously reported methods are also analyzed in the paper.	The paper is well written, and easy to read. The algorithm is well presented.	Authors motivate this work concisely - to extract discriminative features from those challenging vessel pixels by using context in local and global areas. The paper structure is clear and all sections echo the motivation well. The contrastive strategy fits well with this retinal vessel segmentation task. Retinal vasculature is long and tortuous, some vessel pixels in complex vascular structures and low contrast background are elusive, which makes those vessel features are indiscriminative from the background. The idea of the contrastive strategy can enhance the model's capability of extracting discriminative features. A variety of results are shown, including segmentation visualisation, quantitative segmentation metrics (two connectivity scores), and t-SNE maps.	I have one question about the proposed method. The global contrastive learning can be treated as a mini-batch with a large batch size. Therefore the claimed global contrastive learning is also in local contrastive learning manner.	The novelty of the paper is limited. A very similar method has been introduced in [1], which, however, is not included in the reference list. It seems to me that the paper just adapt the algorithm from natural image segmentation to a medical image segmentation task. The quality-aware anchor sampler is very similar with the segmentation-aware anchor sampling in [1]. [1] Exploring Cross-Image Pixel Contrast for Semantic Segmentation. ICCV 21. Another major issue is that the performance improvements of the paper are minor on the two datasets.	"The technical novelty of this paper is moderate, considering these two contrastive strategies have been proposed and exploited early in [4]. The main methodological difference between this paper and [4] is that this paper uses a quality-aware anchor sampler to select a collection of challenging pixels for contrastive learning. However, this selection has also been introduced and verified in the paper [1], which is missed in reference. [1] Wang, Wenguan, et al. ""Exploring cross-image pixel contrast for semantic segmentation."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021. [4] Hu, Hanzhe, Jinshi Cui, and Liwei Wang. ""Region-Aware Contrastive Learning for Semantic Segmentation."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021. (cited by the paper) Correlating to the point above, some descriptions can be more objective. Instead of claiming ""we have proposed a local intra-region contrastive learning strategy and a global cross-dataset contrastive learning strategy..."", I would suggest changing it to ""we have applied ... into retinal vessel segmentation"". I believe a paper with a novel application can also show lots of strengths. The image sizes of DRIVE and CHASEDB1 are smaller than (1000, 1000). Some publicly available datasets with large image sizes, like HRF, can better evaluate the capability of segmenting thin and elusive vessels. Also, the performance is comparable to some miccai works last year [2, 3]. [2] Zhou, Yuqian, Hanchao Yu, and Humphrey Shi. ""Study group learning: Improving retinal vessel segmentation trained with noisy labels."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021. [3] Kamran, Sharif Amit, et al. ""RV-GAN: segmenting retinal vascular structure in fundus photographs using a novel multi-scale generative adversarial network."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021."	The details of the method are introduced in the paper. It should be easy to reproduce the paper.	It seems to me that the paper can be easily reproduced.	The reproducibility is moderate as parts of training hyperparameters are introduced and network structure is shown in supplementary material.	The overall paper is well organized and the writing of the paper is great. I have no further comments regarding the presentation of the paper.	"It is essential to provide a detailed comparison with [1] and clarify the contributions of the paper. [1] Exploring Cross-Image Pixel Contrast for Semantic Segmentation. ICCV 21. The term ""cross-dataset"" confuses me a lot. Would it be better to use ""cross-image"" like [1] because the contrast is conducted over different images rather than different datasets? I wonder whether it is possible to formulate the two strategies into only one formulation like [1]? Are there any advantages to compute them separately?"	"I suggest revising the paper writing to be more objective and application-motivated. A reasonable and novel application is a kind of novelty. It is recognise that the weights for contrastive loss are small (i.e., 0.001). Have the magnitude of L_ce, L_lc, and L_gc been studied? How do these tiny contrastive losses contribute to the model training? Please keep the naming consistency, like ""local-region"" and ""local intra-region"". Additionally, ""easy vessel samples"" and ""easy region-level vessel samples"" only appear once in Figure 1, they are called ""high-quality samples"" in the main text. Language needs polish - some sentences and words can be more precise. For example, the first sentence of paragraph three in the Introduction says ""For addressing the feature discrimination..."". It can be revised as "" To extract discriminative features..."" or ""To enhance the features' discriminability...""."	Although deep learning segmentation network methods are extensively studied for vessel segmentation, the authors present a novel way to further improve the segmentation results by contrastive learning approaches.	The algorithm proposed in the paper is mostly adapted from [1], which however is not cited in the paper. It is necessary to clarify the technical contributions of the paper against [1], before it can be accepted. [1] Exploring Cross-Image Pixel Contrast for Semantic Segmentation. ICCV 21.	The methodology motivates well, but the paper presentation needs to be objective and application-motivated, which requires a certain revision. Some other points also need clarification, summarised in the weakness and comments.
305-Paper1649	Longitudinal Infant Functional Connectivity Prediction via Conditional Intensive Triplet Network	The authors present a new Deep Learning architecture for longitudinal Functional Connectivity prediction in infants. Besides describing in detail new ideas for architecture design, they show the superiority of theirs with respect to other state of the art models.	The authors proposed a conditional intensive triplet network (CITN) for longitudinal prediction of the dynamic development of infant FC. This model is predicable for the common maturation pattern of FC and also maintains individual uniqueness. This model showed better performance than MLP and MWGAN methods.	A novel conditional intensive triplet network was proposed to longitudinal predict infant FC.	The main strength is, as in any other Deep Learning work, the better performance when compared with other available methods. The description of the architecture is rather clear and greatly appreciate from the reader's point of view. Personally, I think that given appropriate testing, this model shows great potential to be accepted in the clinical atmosphere given its simplicity with respect to the current competitors. Congratulations on designing from scratch an architecture that performs good.	The prediction of brain FC maturation at individual level is hard. The authors designed a simple but high efficient conditional intensive triplet network model to capture age and individual information separately in two extractors.	identity and age related information could be extracted in the network. the predicting result looks promising.	"A few concerns arise when inspecting the present paper. MAJOR COMMENTS: The so-called identity conditional module is critical for the good performance of the model, yet the description in section 2.3 is quite brief. The main idea behind its design is visible, but the inner details are not easily accessible. Furthermore, figure 2 does not provide significant help when reading the section.  Figure 4 is not really a must and perhaps should be displayed in the supplemental sections. I don't think that the result reported there should be taken into consideration when assessing the performance and validity of the model. Visual proof that the model correctly disentangles and differentiates attributes from the data is somewhat assumed whenever a model performs well.  When assessing the accuracy of the predictions, only two measures are reported. Although enough to see the good performance, perhaps other network measures would proof useful for transparency and acceptance across the clinical community. MINOR COMMENTS: As always, careful review of misspelling is advised, despite only finding small errors in the last lines of pages 2 and 7. Perhaps the reviewer misses a more extensive literature review. Even if there are not many previous works on this area, maybe larger comments on the ones already made would provide useful for anyone interested in the topic. Lastly, both pages 3 and 7 have two ""alone"" lines on top of the figures which might be a little bit misguiding and inconvenient for the reader. Even if space is a scarce resource, moving them to different parts of the manuscript would improve accessibility and clarity."	The influence of brain node partition should be taken into account.	The major problem of this paper is that method parts is not clear. More detail should be give, otherwise it's hard to follow.  With such complicated designed network, ablation study is necessary, e.g. is age inf extractor really helped?	All the steps of the method are detailed throughout the paper easing the task of external implementations from other teams. The data comes from the BCP so easy access for reproducibility is assured. Of course, freely available and easily readable code is always appreciated by the community. As in any Deep Learning model, this last step is crucial for testing and usage from external people, so the reviewer highly encourages to publish them.	The reproducibility is good. The method is present clearly. The data is public available.	Some hyper parameters were missing e.g. the width of hidden layer in E and G, training parameters. Thus it might be hard to reproduce this paper.	In this reviewer's opinion, the manuscript would greatly benefit from the deletion of figure 4. The space available from this deletion could be used to further expand section 2.3 (even a modification of the accompanying figure 2). More detailed literature review on other Deep Learning attempts would also be useful for any reader interested in diving into the topic of longitudinal FC prediction, regardless of the scope (infant, adult, Alzheimer, ...). Reporting of other numerical measures would vastly improve the credibility of the model. Some ideas would include network measures comparisons between real and predicted.	The influence of brain node partition should be taken into account. The statistical significance of comparison between the performances of three prediction models should be tested.	in the loss design, where is the loss about encoder and ID extractor? e.g the similarity between subjects? It's hard to follow how ICM module works. According to Fig2, will there be big difference if input times are 100 and 101? or is there and difference between 101 to 200? Dose the module simplify the regression problem to classification problem? For the compared method MLP and MWGAN, what their hyper parameters are? Since training GAN is not easy, it's hard to conclude your method is better. I'm not sure if the fmri data is preprocessed well, the development pattern of  two representative individuals in Fig.3 were quite different.	The presented work is sufficiently rigorous, transparent and engaging to be presented in a conference. It shows potential usage given further studies of its performance.	The authors make an important and explainable network model for the challenging task of FC maturation prediction at individual level. This topic is valuable and may raise the general interests in both clinical field and fundamental research field.	The major problem of this paper is that method parts is not clear. More detail should be give, otherwise it's hard to follow.  With such complicated designed network, ablation study is necessary.
306-Paper2874	Long-tailed Multi-label Retinal Diseases Recognition via Relational Learning and Knowledge Distillation	This paper focuses on mitigating the long-tailed effect in the multi-label classification of eye images. The proposed method first divides multiple diseases into several groups. A teacher model is trained for each group. Then a student learns from all the teacher models through knowledge distillation. The basic idea of this paper is similar to [16], with several refinements, e.g., mutli-task backbone pretraining, region-based attention, automatical relational subsets generation, and class-balanced distillation loss.	The authors proposed a framework that contains multi-task relation, feature relation, and region relation in a knowledge distillation manner to recognize ocular diseases. The framework uses lesion segmentation and grading information from subjects with diabetic retinopathy to perform an initial classification task. The authors tested the proposed method with one dataset and reported performance metrics and class activation maps.	In this paper, the authors propose a novel knowledge distillation framework for long-tailed multi-label retinal disease recognition. First, a network is pretrained with classification and segmentation tasks on a well-labeled public dataset. Second, the long-tailed dataset is automatically divided into three subsets to train multiple teacher networks, which can help reduce the label co-occurrence and class imbalance. Besides, an spatial attention mechanism and a class-balanced distillation loss is introduced. The superiority of the proposed method is evaluated on a public dateset.	This work is dealing with an important problem, i.e., the long-tailed effect in multi-label classification. The newly added components are reasonable for this specific task. Comparably thorough related works. Well written and easy to follow.	The ablation study and the qualitative and quantitative analysis performed for the authors.	The motivation, problem description, and the proposed solution are very clear The paper is well-written and pleasant to read The method is evaluated on a public datasets and outperforms other relevant approaches.	Given the main idea is similar to [16], the methodology contribution is limited. The performance improvement of the proposed method is marginal.	The class activation maps are not useful to highlight the obtained results. For example, In Figure 3 (a), (c) and (d) the highlighted areas are not specific to the diseases. The results are good but some issues are missing in the paper. Why CCT-Net method was not explored using ResNet-50 backbone? Which results are obtained using healthy images as input? What about the CAM in healthy images?	"The framework in this paper is very similar with existing methods, such as the ""Relational subsets knowledge distillation for long-tailed retinal diseases recognition (MICCAI 2021)"". In section 2.4, the authors extracted image features with a pretrained ResNet50 and employed k-means algorithm to cluster the images into subsets. I wonder if the samples are directly clustered into three subsets (i.e., {D, AMD, H, M}, {G, C}, {N, O}) or first clustered into 8 classes (i.e., D, AMD, H, M, G, C, N, O) then divided into three subsets based on the pre-defined rule? Besides, according to the section 3.1, the images are labeled as the 8 classes. Why not use the class labels to divide all samples into three subsets? In section 2.3, the authors preposed a novel region-based attention, which slightly differs from CBAM by combining a trainable convolutional layer. However, the authors did not provide the experimental comparison between the proposed attention and CBAM. I think this is a crucial comparison and suggest the authors to provide the comparison results. In section 2.6, the training and testing samples are randomly splitted. The randomness of data partition might affect the results. I would suggest 5-fold cross-validation to produce more solid results. In table 1, the performance of the proposed method is slightly worse than the existing method, i.e., CCT-Net. The performance is not satisfactory since a novel method is supposed to outperform existing methods. In fig.3, only original images and heatmaps are provided. I suggest the authors to present the pixel-level ground-truth labels to indicate the produced heatmaps are able to locate the lesion regions. Without pixel-level ground-truth labels, the readers do not know the location of lesion regions."	The authors provide sufficient implementation details, but no absolute guarantee for reproducibility.	Although the dataset are free public available, the authors should consider to release the code and some models in order to reproduce the results!	-The authors declares they will share all the codes for the experiement when accepted. The authors also presented most of the relevant setting parameters for the expriments.	If possible, provide the ground-truth locations of diseases in Fig.3 in comparison to the CAMs.	The class activation maps are not useful to highlight the obtained results. For example, In Figure 3 (a), (c) and (d) the highlighted areas are not specific to the diseases. Why CCT-Net method was not explored using ResNet-50 backbone? Which results are obtained using healthy images as input? What about the CAM in healthy images? The code and some model weights are missing, so the reproducibility of the paper would be impossible!	"Please refer to the ""main weaknesses"" part."	Please see the strengths and weaknesses.	The framework uses lesion segmentation and grading information from subjects with diabetic retinopathy to perform an initial classification task. Then, the dataset is split into several subsets to train different teachers and to apply attention mechanisms, then multiple teacher-student is distilled. The authors tested the proposed method with two datasets with the performance metrics and class activation maps.	This paper is well-written and easy to read. The motivaion and method are clearly demonstrated. However, there lacks of some crucial comparisons, such as comparison between the proposed attention and existing CBAM. Besides, the performance is not superior to existing CCT-Net.
307-Paper1411	Low-Dose CT Reconstruction via Dual-Domain Learning and Controllable Modulation	The paper proposes a dual-domain end-to-end deep-learning network that predicts CT images from degraded CT images and sinogram data. To enhance the adjustability of the reconstruction process, the authors also propose integrating a controllable modulation module that gives the user room to do a trade-off between denoising and detail preservation. Experiments demonstrate that sinogram data work in the proposed network, and its controllable modulation module can control the degree of denoising.	The authors propose a neural network strategy to denoise low-CT images that considers a dual-domain approach (images + sinograms) and also allows to control the level of noise with an additional parameter.	design an end-to-end dual-domain deep network to achieve accurate the adjusted low-dose CT recon.	The motivation is well clarified and meaningful. This paper proposed a novel method to learn from the image and sinogram domain, resulting in an improved final reconstruction. The experiments show that the introduced controllable modulation module can control the degree of denoising according to the needs of doctors' diagnosis.	It is an interesting research problem The analysis is performed in several databases The authors take into account the radiologist perception and not only image processing metrics	1 a dual-domain network to fully explore the mutual dependency between the CT image and raw projection data. 2 design a controllable module to achieve fine tuning of low-dose CT recon process with different noise levels.	"As for the dual-domain network, the results of PSNR/SSIM outperform other methods in most datasets, but the inference time and model complexity are not considered. They are all important indicators to evaluate the performance of the model. Besides, as mentioned in this paper, ""Current methods leveraging both domains by connecting them through some simple domain transform operators or matrices"". This work proposed a complex network but lacked an explanation about why and how these designs work. As for the controllable design, the details of CB are not clear. As mentioned in this paper, the controller parameter f_{\alpha} is learned from fully connected layers, which is dependent on \alpha and learned parameters of fully connected layers. This means that doctors can not ""manually"" select features based on \alpha because the value of \alpha is different from the value of f_{\alpha}. Besides, the training is conducted in two steps,  \alpha=0 for CB and \alpha=1  for MB, which represent two patterns of feature combinations. While the other combination patterns are not trained, it is supposed to show the overall evaluating results under other values of \alpha. The dual-domain reconstruction methods show an overwhelming advantage over image domain reconstruction methods, but the introduction of sinogram data is not the innovation of this paper. So the authors need to consider comparing more dual-domain reconstruction methods to demonstrate the superiority of their method.  The following papers can be considered for comparison: [1] Zhang, Zhicheng, et al. ""TransCT: dual-path transformer for low dose computed tomography."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021. [2] Adler, Jonas, and Ozan Oktem. ""Learned primal-dual reconstruction."" IEEE transactions on medical imaging 37.6 (2018): 1322-1332."	The performance improvement with respect to state-of-the-art is not very significant Neither the dual domain nor the controlable modulation are novel ideas	1 small dimension evaluation. 2 unclear description of the dual-domain module	The method is not clearly clarified, and it is supposed to provide the source code for reproduction. E.g., in Fig.1, the second block in FB takes four inputs, while the description in Section 2.2 shows that there are three inputs, which is not consistent. For the relatively complex reconstruction network, the authors do not provide enough information on its settings, such as kernel size. Even the setting values of many important hyperparameters are not given, like gamma_1 and gamma_2 in loss functions. One of the two datasets they used seems to be private too. Currently, the paper doesn't have a strong reproducibility.	They authors do not share resources to reproduce	The authors followed the rules of ethical and biological requirements for medical data processing.	"Demonstrate the need for complex designs of dual-domain networks. Make a more comprehensive comparison, including inference time and complexity of reconstruction networks. P.2, ""Besides, caused by mismatched resolution between the heterogeneous sinogram data and CT image..."": The authors raise the problem of mismatched resolution in dual-domain learning. It is interesting to see how the proposed method solves this problem. Maybe the author can explain it here? Experiments: The paper uses an iterative way to reconstruct CT images. How does its model size compare to other methods? Some suggestions:  (1) P.3, ""Each DDB is composed of a PI block that transforms ..., and a PS block ..."": Abbreviations should state the full name on the first occurrence. (2) P.4, ""Different colored rectangles are used to represent different branches and different operations in each branch."": ""branches"" -> ""sub-branches"" (3) P.4, Figure 1: The colored rectangles which represent full connected layers and adaptive average pool layers look too similar. Please consider changing the color. (4) P.5, ""the image details can be gradually preserved and simultaneously avoid the introduction of mottle noise and streak-like artifacts caused by FBP."" The argument will be more solid if the author can provide some examples of image degradation caused by FBP. (5) P.5, ""Each controller block is composed of two convolutional layers with an ReLU in the middle."": ""an"" -> ""a"". (6) P.7, ""We use real clinical dataset authorized..."": ""real clinical dataset"" -> ""a real clinical dataset."" (7) P.7, ""The dataset contains ten patients, in which..."": ""in which"" -> ""of which"". (8) P.7, ""the edge of the tissue reconstructed through the network is relatively blurred but showing the powerful denoising ability"": ""showing"" -> ""shows"". (9) P.8, ""Although the PSNR value is not the highest, but..."": remove ""but"" (10) P.8, ""For fairly comparison, ..."": ""fairly"" -> ""fair"". (11) P.8, ""which demonstrates the effectiveness of integrating both domain ..."": ""domain"" -> ""domains""."	Perhaps a more thorough discussion explaining the results: why is your strategy a good choice even if it does not outperformed in certain databases; how did the controllable modulation helped in the visualization of certain features?	The authors may apply their algorithm to the full dimension dataset which has been scanned in CT or CBCT machines. The computational efficiency and memory requirement are exposed to the real clinical data.  Dual domain network was proposed a few years ago. In previous publications, authors tried to link the info between image and projection domain. A more comprehensive evaluation of priors are expected in the introduction. Subtle improvements compared with existing algorithm. The real utility of the method is not clear.	There are two obvious problems. First, the clinical value of the proposed strategy is not clear. Second, the proposed method is not reasonable, and experiments do not provide enough evidence that the fMRI feature has hints for predicting gaze patterns. And its improvement over DP-ResNet is not significant in some cases.	Very nice approach that is novel. Good comparison with different dataset. Still the improvement with respect to state-of-the-art is not very significant	The authors are working on an old topic of low-dose CT imaging. Network designed in this manuscript is not new as the components within the network has been applied previously. Results are not impressive.
308-Paper0125	Low-Resource Adversarial Domain Adaptation for Cross-Modality Nucleus Detection	This paper presents a generative adverserial network (GAN) based unsupervised domain adaptation strategy for limited target training data case. The main contribution is achieved by a stochastic data augmentation module integrated to the GAN network. Unsupervised domain adaptation performance is increased.	The authors propose a domain adaptation workflow to enable nuclei detection across different microscopy image modalities. Although the approach is not new, the authors propose a workflow that can provide accurate results using small annotated datasets. They contribute a differentiable data module that could be connected to different approaches as it is agnostic to the architecture and the loss function. The accuracy of the proposed approach is compared with other existing approaches showing that it can outperform them in the situation of having a few target training images.	This manuscripts presents a method for domain adaptation for the task of nucleus detection from microscopy data, for the case of small amount of labelled target data. The authors present several improvements for state-of-the-art, including usage of task-specific model to support target-domain discriminator, stochastic data augmentation, and targeted consistency constraint. This results in superior performance with respect state-of-the-art domain adaptation methods.	The problem is defined well. The proposed stochastic data augmentation is formulated well. Experiments are quite comprehensive; they are conducted on 5 different datasets (4 public, 1 in-house) and consists of comparison with 4 other methods and ablation studies.	The authors present a creative approach of transforming a source image, force this transformation to have a coherent structural meaning and back (i.e., get a target image, transform it into the source's domain and compare the structural meaning again). In this case, the structural information is forced by training a nuclei detector. This way, they guide the network on the creation of results that structurally reassemble more to each other, despite changing the domain. The authors have worked on the definition of a fully differentiable data augmentation that works on the	* Strong and complete submission: clear and advanced methodology, convincing validation on versatile data. * Results superior to the current state-of-the-art and on pair with direct training in the target domain.	The proposed method has limited novelty. It adds a stochastic data augmentation module to a GAN network. In numerous papers, data augmentation techniques have already been shown to be an effective solution for training deep neural networks on small datasets. Also, the paper studies a particular case (limited target training data case) of unsupervised domain adaptation. If one has enough target training data, the proposed data augmentation module will not be necessary. Thus, the proposed method does not increase the general capabilities of GANs for unsupervised domain adaptation. Hence, this paper only repeats known contributions of data augmentation by proposing a new data augmentation module and integrating it to a GAN network. Compared to existing works, this paper proposes a slightly modified GAN architecture and a slightly improved data augmentation module.	"While the authors presented an original work, I found it difficult to understand what's the real motivation or the gaining that drive this wok. First, from the title it seems that the work is specially designed to allow nuclei detection when there's lack of annotated to train a specific model for it. However, the issue about lacking data is to train the domain adaptation (DA). DA is an unsupervised task for which you would only need the input images. For example, the following sentence ""In this paper, we propose a novel GAN-based UDA method (see Fig. 1) for cross-modality cell/nucleus detection in a low resource setting, where unlabeled target training data is scarce, a more realistic case but rarely explored."" is confusing because the low resource setting to be explored here is about the unsupervised method (non-labelled data). Also, the content of this sentence may relative: scarce labelled data in bioimage analysis is the common situation and there are plenty of works targeting it, so the authors may want to be more specific in this statement. Same for the conclusions ""With limited unlabeled target training data (e.g., 1 image),..."", you do not need to have the labeled data (i.e., nuclei detection) for the target to train the method, but only to evaluate it's performance. Second, the authors motivated this by the fact that acquiring data is expensive, but could it be possible to use publicly available datasets of the same modality? (even if they are acquired with different devices). Finally, they are not considering any other biological meaning of the generated image despite the nuclei detection, so it should be possible using an heterogeneous dataset of a single modality and biological organism. Related to the previous comment, along the text, the authors say that the domain adaptation is supported by the nuclei detection task. However this work was proposed to get nuclei detection supported by the fact that we may need domain adaptation. It is important to specify in the text that this approach cannot be applied to all type bioimages as it potentially requires, by definition, all nuceli in the image to be visible."	* Visualization of the qualitative results needs to be improved.	The method is defined clearly and all implementation details are given.	As far as I saw (please, correct me if I'm worng), the authors do not provide any link to the dataset or the code, which makes it incredibly difficult to reproduce due to the complexity of the architecture.	All the methods developed in this paper are clear and valid. Also the implementation details are properly described and values of all the parameters are reported.	The paper is well-organized and well-written. Also, the experiments are carried out extensively. It is a pleasure to read such a paper.	"When defining y^S_i in page 3, shouldn't it take into account the pixel size in images? The authors define a new approach for the differentiable data augmentation but I strongly recommend them to include a graphical illustration of this process (Section 2.2) to really understand what they are doing. In the limit, it could be in the supplementary materials. When explaining the data augmentation process, the authors sometimes speak about differentiable data augmentation and others, about invertible. Although these two properties are important to integrate the process in the training, I would specify in the text why you need both features and when they are used. Please, correct me if I am wrong, is the augmentation applied to the source image the same one as for the target one? The question raised from this sentence ""To address this issue, we apply data augmentation to both real and translated images before feeding them to the discriminators and conduct the augmentation when training both generators and discriminators"" Please correct the following sentence: ""... data distribution ONLY if the augmentation contains invertible transformations..."" Please correct: ""training data can posE serious challenges"" Please, elaborate more on this sentence as it is not clear to what you are referring ""This target task-based augmentation can also reduce the effects of data for which the discriminator has no access to the labels."" What is E in Eq 1,2,3? Are you referring to the estimated mean value? please, define it."	Presentation of the qualitative results needs to be improved, as the images are too small and hardly readable on print. It might be useful to add the size of each of the four validation data sets (in terms of number of images) to the header of Table 1. It would also be interesting to see comparison between the proposed method and the reference ones for the less extreme case, when more training images are available.	This is a well-conducted research work with good results. It proposes an improvement for unsupervised domain adaptation with GANs when the target training data is limited. However, its novelty is rather limited; a new data augmentation module is integrated to a GAN network. Moreover, it only addresses the low-resource case of the unsupervised domain adaptation (UDA). It is very likely that the proposed method won't be needed in the case of sufficiently big target training data. Thus, the proposed method does not offer a general improvement to GANs for UDA, although it presents an effective solution to GANs for UDA with limited target training data. It should also be noted that data augmentation techniques have been numerously proved to be an efficient way to efficiently train deep neural networks on small datasets. This paper mainly repeats known contributions of data augmentation and then proposes a new data augmentation module, which is a case-specific solution.	I like the idea presented in the paper and the approach followed by the authors. I think they need to work more on graphically show their contributions and technical approaches, as well as better introduce the motivation and what is the specific problem for which they propose this solution.	This is a very strong submission, with advanced methodology, clear presentation and convincing validation.  In particular, the authors have introduced several methodological improvements including task-specific model for supporting the source-target discriminator, target consistency constraint and stochastic data augmentation scheme (in both source- and target domains). This allows efficient training under the scenario of low availability of (unlabeled) data in the target domain. The authors have demonstrated the power of their approach by using a single image from the target domain. The results of this experiment were much better compared to the state-of-the-art methods (trained on the same data) and comparable to training directly in the target domain. In addition, the authors perform an ablation study demonstrating, in particular, importance of the introduced stochastic data augmentation step.
309-Paper2658	LSSANet: A Long Short Slice-Aware Network for Pulmonary Nodule Detection	The paper proposed a long short slice-aware network for pulmonary nodule detection, which have the ability to capture long-range dependencies. This is a relatively painful problem in this field, and it is very valuable.	This paper proposes a LSSANet, which has the ability to capture long-range dependencies for pulmonary nodule detection. This network not only reduces the computational burden, but also keeps long-range dependencies among CT slices. Experimental results show that this network has convincing performance.		The method is relatively new. The logic is clearer. The experiment is relatively sufficient.	Effective design and comprehensive experiments: authors compare their method with several proposed methods, and LSSANet achieves convincing performance. Clear writing: it is easy to follow the writing.		The description of the formula in the paper is not clear enough. Full writing and abbreviations in the paper are confusing and should be consistent. If the abbreviation has been abbreviated in the front, please keep the abbreviation in the back, instead of using the full letter and the abbreviation back and forth.	Some typos: Section 2.2 line 6: '5 our proposed LSSG blocks are integrated into the second and third stages of the ResNet50 encoder'. Table 1, LSSANet does not achieve the best performance, but authors still bold 89.87, which may mislead readers. Please examine typos carefully.		The paper is very valuable, but requires more information from the authors to illustrate reproducibility. Such as code, models, etc.	The reproducibility of this paper is good.		Full writing and abbreviations in the paper are confusing and should be consistent. If the abbreviation has been abbreviated in the front, please keep the abbreviation in the back, instead of using the full letter and the abbreviation back and forth. There are English spelling errors in the paper, please correct them carefully. The method proposed in this paper is based on the pre-training model of the current best method as the benchmark model. It cannot be clearly stated whether it is the advantage of the method in this paper or the overfitting of the original method.	Generally speaking, this paper does not have many weaknesses. The motivation and paper writing are all good. It would be better if the authors could be more careful about typos.		The research of this paper is very valuable and is a hot topic in the field. Although the experimental results are relatively good, the method used in this paper is based on the pre-trained model of the current state-of-the-art method. Papers will not be accepted unless the authors provide code or models to illustrate the reproducibility of the method.	The motivation and paper writing are all good.	
310-Paper0217	MAL: Multi-modal attention learning for tumor diagnosis based on bipartite graph and multiple branches	Authors proposed a multi-modal attention learning method for patient-level tumor benign and malignant diagnosis using a bipartite graph structure to model the correlation of different modality data. They also proposed a modal similarity loss function and intra-type similarity loss function PiTSLoss at patient-level.	The paper proposes a multi-modal attention learning framework MAL for tumor diagnosis. A bipartite graph structure is used to model the correlation of different modality data and the edges of the graph are predicted through the attention learning and multi-branch network. The modal similarity and intra-type similarity loss are calculated from the feature similarity matrix to extract better high-level semantic features. The multi-instance learning is used to obtained final diagnosis results of patients.	This paper proposes a multi-modal attention learning framework for patient level tumor malignancy classification. The results show the effectiveness of the proposed method, comparable and in some cases exceeding doctor performance.	The idea of multi-modal attention learning is interesting and address an imortantresearch problem. Authors used a relatively large clinical dataset to demonstarte the performance of their proposed method. Performing extensive experiments and acheived high performance.	1) In this paper, images of different modalities and different planes are considered at the same time. 2) It is novel to use bipartite graph structure to model the correlation of different modality data.	1) An interesting multimodal fusion framework based on bipartite graph and attention learning to explore the correlation between different modalities; 2) The PMSLoss/PiTSLoss seem to encourage the model to learn the inherent similarity between features from different modalities at patient level; 3) The results show the effectiveness of this approach, through ablation study and comparison to doctors.	Writing in my opinion is not very good and could be better than presented in the manuscript. Many long sentences and in some places unclear meaning is given. Lack of given results in terms of figures is one weekness, I would like to include the figures of the suplmentary material in the main manuscript.	1) The proposed method was only evaluated on three spine tumor datasets while the author emphasized its general applicability on tumor diagnosis in the title, Abstract and Conclusion. 2) It is mentioned in the paper that PMSLoss can encourage the model to extract similar high-level semantic features from different modalities, however, features of different modalities may be complementary to improve the classification performance. It is also demonstrated in Fig. 1 of the supplementary materials. Is it contradictory? Did the PMSLoss surpress complementary features during the training process? 3) The experiments are not convincing enough. In Table 1, when comparing MRI-Axi&Sag results of the proposed MAL method (the last row) with the method without AB and TS modules (the fifth row), the AUC is similar while the ACC and SP of the latter is higher than the former.  4) The author mentioned that the convolutional neural network branch and the attention auxiliary branch extract local and global features individually, is there any visual results or other experiments to further prove this? 5) The writing of this paper is extremely poor with plentiful grammar mistakes.	1) In the experiment results, it would be good to compare with the existing SOTA method. It is unclear from Table 1, which method is the current SOTA method.	The datasets used in this work are private clinical data which are not public. Authors mentioned their code will be publicly available on Github.	Implementation details have been described.	The code is claimed to be available at Github.	"Beside the comments I mentioned in section 5 (weaknesses), I have the following comments. Abstract is not very informative, no information about the datasets and performance accuracy. The selection of the axial slices from CT and sagittal ones from MRI is not demonstrated. Would it make difference if these slices are reversed. Also, which MRI sequences used and would it be better to use all sequences of MRI? Authors mentioned that they used the average value of CrossEntropy loss, PMSLoss, and PiTSLoss. Did you explored different weighted losses of these terms? So many abbreviations are not defined before using (e.g. CT, MRI and others). Many typos appeared in many places e.g. ""topk"", ""Experiment and Result"", ... References are a bit old. Below are some suggested references that authors may use to update their reference list: https://doi.org/10.3389/fgene.2021.690049 https://doi.org/10.1016/j.media.2022.102444 https://doi.org/10.1016/j.compbiomed.2021.104836 https://doi.org/10.1007/978-3-030-87199-4_10"	"1) Page 1, Introduction, Paragraph 2, ""Multi-modality methods can improve the diagnostic accuracy by multi-modal data analysis and model construct ... "" should be ""Multi-modality methods can improve the diagnostic accuracy by multi-modal data analysis and model construction ... "" 2) Page 2, Paragraph 1, ""The performance of these multi-modal methods are considerably improved..."" should be ""The performance of these multi-modal methods is considerably improved..."". Besides, please provide references with numeric results.  3) Page 2, Paragraph 2, ""... for feature fusion to segment brain tumor"" should be ""... for feature fusion to segment brain tumors"". 4) Page 3, Fig. 1, Please explain the meaning of the green line. How to obtain the interested regions and crop them from original images? 5) Page 4, Paragraph 1, ""patches with 8x8 pixels"" should be ""patches of 8x8 pixels"". 6) Page 5, "" 3 Experiment and Result"" should be ""3 Experiments and Results"". 7) Page 6, Paragraph 1, ""The benign and malignant labels of each patient"" should be ""The benign and malignant labels of patients"". 8) Page 6, Paragraph 6, ""Result of different methods"" should be ""Results of different methods"". 9) Page 7, Table 1, the font of the first line is not the same.  10) Page 7, Paragraph 2, ""it still can reveal ..."" should be ""it can still reveal ...""  11) Page 7, Paragraph 4, ""Compare with doctors"" should be ""Comparison with doctors"". 12) Page 8, Paragraph 2, ""The diagnostic accuracy and efficiency of doctors have greatly improved ..."" should be ""The diagnostic accuracy and efficiency of doctors have been greatly improved ...""  13) Page 8, Conclusion, ""The attention learning and multi-branch strategy in MIL are used ..."" should be ""The attention learning and multi-branch strategies in MAL are used ..."""	Adding comparison result with the current SOTA method could further strengthen the paper.	See my comments above.	The proposed method was only evaluated on three spine tumor datasets while the author emphasized its general applicability on tumor diagnosis in the title, Abstract and Conclusion. The experiments are not convincing enough.	The multimodal learning framework seems to be novel for the MRI/CT data. The results in comparison with doctors seem to be good.
311-Paper2233	MaNi: Maximizing Mutual Information for Nuclei Cross-Domain Unsupervised Segmentation	This paper addresses an interesting topic of unsupervised domain adaptation applied in Nuclei segmentation. The core idea of this paper is to use a lower bound based on Jensen-Shannon Divergence (JSD) to optimize Mutual information between different classes of source and target images, using the ground truth segmentation masks and pseudo-labels. Two series of experiments are made to show the effectiveness of the proposed method.	In this work, the authors proposed a domain adaptive nuclei instance segmentation method based on a two-stage training strategy. The proposed method facilitates the target feature generation by mutual information maximization mechanism. Extensive experiments on UDA nuclei semantic and instance segmentation benchmarks have indicated the effectiveness of the proposed method by achieving state-of-the-art performance.	The paper proposes to use a contrastive learning based mutual information maximization approach to maximize the mutual information between source and target domains The proposed approach is empirically validated on using different architectures on various datasets: TNBC -> KIRC/TCIA, TCIA -> KIRC/TNBC, CoNSep -> PanNuke.	While employing mutual information for domain adaptation is not a new idea (i.e. 1, 2,3), the proposed approach, using masked average pooling, contrasting the averaged representation, and leveraging JSD as a surrogate loss for MI, can be considered a novel contribution. Results show clear improvements over recent approaches for UDA in segmentation. The paper is well written and easy follow. The motivation and contributions of the work are clearly described in the introduction, and the method is presented with sufficient details.	1)The proposed method has achieved state-of-the-art performance under various UDA nuclei segmentation benchmarks. 2)This work has studied an important research topic, i.e., UDA nuclei semantic and instance segmentation tasks, which can enhance the models' generalization ability.	The paper is well-written and well-motivated. The use of Jensen-Shannon divergence (JSD bound) based mutual information estimator in the context of nuclei segmentation is novel. JSD has been used in feature alignment for text and image data. To the best of my knowledge, applying JSD for domain adaptation for nuclei segmentation is novel. The paper has strong empirical results on different datasets. The paper evaluates on Nuclei Semantic Segmentation (TNBC-> KIRC/ TCIA and TCIA -> KIRC/ TNBC shift) and Nuclei Instance Segmentation (CoNSep -> PanNuke) and demonstrates competitive results over the baseline approaches.	The paper motivates the choice of using JSD as a lower bound of MI by discussing its advantages over InfoNCE and by saying that JSD maintains good performance without using a large number of negative samples.  However, this evidence is not supported experimentally. If one carefully checks their cited papers, 4 said that InfoNCE provided better results compared with JSD in classification tasks, while 5 indicated in its supplementary material that reducing the batch size (thus the negative examples) degraded the performance.  Hence, it is not clear to me whether this statement holds. In addition, I am not entirely convinced about their provided experimental results. The performance values in the comparison methods were taken from reference papers, but the authors should at least report results using their own runs. Even while using the same codebase, many parameters such as the library version, random seeds, etc. can impact the results. Authors could better motivate the applicability of the JSD lower bound on MI to UDA. Specifically, I wonder how to model and sample the joint distribution in Eq (1) since z_s and z_t come from independent images. Isn't the joint same as product of marginals in this case?	1) The ablation study on removing the MI loss is missing. Since utilizing the MI maximization learning to facilitate the domain adaptation is a major novelty in this paper, it is important to report the model's performance without this loss function, to further indicate its effectiveness. 2) The motivation for enlarging the mutual information for the cross-domain features to facilitate the domain adaptation has been proposed in [16] and [a]. [a] MI2GAN: Generative Adversarial Network for Medical Image Domain Adaptation using Mutual Information Constrain, in MICCAI 2020 Although the MI maximization loss function in the proposed MaNi has a different implementation, detailed discussions and comparisons with [16] and [a] are missing. 3) Visualization results are missing. In addition to the quantitative comparisons, qualitative analysis of the instance segmentation prediction is also an important metric to evaluate the model's cross-domain segmentation performance.	"It would be interesting to compare the empirical results of JS with other types of MI estimators, e.g., infoNCE, Donsker-Varadhan representation 1. The selection of negative pairs in the MI estimator lacks motivation. What are the different ways to select negative pairs and why prefer this particular approach? 1 Belghazi, Mohamed Ishmael, Aristide Baratin, Sai Rajeswar, Sherjil Ozair, Yoshua Bengio, Aaron Courville, and R. Devon Hjelm. ""Mine: mutual information neural estimation."" arXiv preprint arXiv:1801.04062 (2018)."	I am a little concerned about the reproducibility of this paper. I would suggest the authors cite the values from previous papers and report their own performances using the same code based on their own runs. Furthermore, I would be happy to see that the author can make their code and dataset open-source.	All datasets used in the experiments are public. Source code is not available during review.	Not sure how easy/difficult it is to reproduce the paper.	"The second paragraph of the introduction mentioned multiple methods, but did not properly cite them. The authors are invited to carefully cite these papers. Regarding the related works, the paper only presented methods using adversarial training and pseudo-label. However, more methods should be mentioned, such as those using mutual information (2,3), using metric learning (6,7,8), and using co-training based frameworks (9,10) As mentioned previously, the authors used JSD instead of InfoNCE or MINE [11] to provide a stable estimation of MI, however, the authors do not verify this claim experimentally in the ablation study. Moreover, only one negative pair is used per positive pair, and the paper claimed this would not decrease the performance. However, no evidence has been shown to support this hypothesis. I would like to see experimental elements for both claims. The projection head is 1 x 1 convolutional layer. Parallel works usually used nonlinear heads. The authors may want to explain the motivation for this choice. The Word ""Actual"" on the top line of Fig. 1 should be ""ground truth""? The discriminator's structure should be explained clearly. There is no ""source only baseline"" for Table 1. As mentioned previously, the values reported in Tables 1 and 2 should include the runs in the authors' own codebase/environment. I encourage the authors to provide further results on these settings. For the presented tables, the authors can bold the best performing values in order to provide a better visual understanding. For the ablation study, the authors only tested MI loss weight and the different sampling/pooling methods. For table 3, I would like to see the values with a weight higher than 1, to see if it helps the performance. The authors should provide the mean and std for each experiment to show the variance of each method."	1) Please include a computational complexity analysis of the proposed method. 2) For the experimental results in Table 2, [27] also reported the performance under the nuclei classification and detection. Please also report the results under these tasks.	Please refer to main weaknesses.	The proposed method is interesting and comprises novel elements. Results are promising. However, some issues should be addressed (see comments).	The overall paper lacks novelty. Important ablation experiments are missing.	I think the novelty in proposing the JSD based adaptation approach and the empirical results outweight the weakness of not evaluating against other types of MI estimators.
312-Paper2849	Mapping in Cycles: Dual-Domain PET-CT Synthesis Framework with Cycle-Consistent Constraints	To obtain high-quality CT images while reducing the risk of radiation exposure, this paper proposed a novel framework that exploits dual-domain information to synthesize CT images from PET images. Specifically, the authors designed a main PET-to-CT synthesis task and a secondary CT-to-PET synthesis task, employing four networks to jointly learn both image and projection domain information. The FP and FBP are employed to connect the image domain and projection domain, thereby constructing a bidirectional synthesis framework with several closed cycles. Furthermore, a two-stage training strategy with dual-domain consistency and cycle consistency is adopted to facilitate network training for superior synthesis performance. The experimental results demonstrate that the proposed method significantly outperforms other state-of-the-art medical image synthesis methods in PET-CT synthesis.	The authors propose a novel dual-domain PET-CT synthesis framework. They design training strategy learning PET-to-CT mapping jointly in both projection and image domains. Additional CT-to-PET mapping is also learned to help the main PET-to-CT task. Extensive experiments show the new framework outperforms the SOTA methods.	In this paper, the authors proposed a novel dual-domain (image domain and projection domain) PET-CT synthesis framework. With using forward and backward projection to connect image and projection domain, the proposed two-stage training strategy with dual domain consistency and cycle consistency achieved better synthesis performance. The proposed method outperformed the SOTA methods through extensive validation on clinical PET-CT data.	a)  Different from previous cross-modality image synthesis methods that only exploit image domain information, the authors designed a novel dual-domain PET-CT synthesis framework to perform synthesis in both image and sinogram domains.  b)  To enhance the PET-to-CT synthesis performance, the authors introduce a secondary CT-to-PET synthesis task and a two-stage training strategy with dual-domain consistency and cycle consistency to build a bidirectional mapping framework, encouraging structural consistency and stable convergency. c) Experimental results show the effectiveness of the main contributions and the state-of-the-art image synthesis performance of the proposed method. d) The methodology section including the network architecture as well as the objective functions is clearly described and the entire writing is easy to understand.	The method is novel that using dual-domain learning in image-to-image translation. It makes sense to use FP and FBP operators to connect image and projection domain. The experiments extensive to support the conclusion.	This paper is generally well written. By exploiting the dual-domain information, the proposed method outperformed the previous SOTA methods substantially in both quantitative and visual evaluations.	"a) The motivation for this paper is not clear. In ""Abstract"" section, the authors mentioned that ""Considering radiation dose of CT image as well as increasing spatial resolution of PET image, there is growing demand to synthesize CT image from PET image (without scanning CT) to reduce risk of radiation exposure."" To the best of our knowledge, CT scanning is cheaper with sufficient training samples in clinical practice, while PET scanning is relatively more expensive with limited available data. In addition, PET scanning also suffers from radiation exposure risk. Why to use PET to synthesize CT? b) Although the authors claimed that the proposed method ""is the first time to exploit dual-domain information in cross-modality image synthesis tasks, especially for PET-CT synthesis"", I still doubt that the innovation of this paper is not sufficient. As shown in [7], the dual-domain network for improving CT image quality was proposed as early as 2019. And the bidirectional mapping idea that comes from CycleGAN [4] could also date back to 2019. c) As displayed in Table 1 and Table 2 of the ""Experiments"" section, the SSIM metric obtained by the Base variant has no significant improvement over the RU-Net, while the authors claim the secondary CT-to-PET task could contribute to the PET-to-CT synthesis task in structural consistency. Does the CT-to-PET task really work? What's more, in Table 2, the SSIM result of the model incorporating both image domain cycle-consistent loss and cross-domain cycle-consistent loss is even worse than that of the model only employing cross-domain cycle-consistent loss. d) In ""Experiments"" section, the training details, such as the number of training epochs and whether the cross-validation strategy is adopted, are not stated. e) The proposed method has not been compared with the existing cross-modality image synthesis methods. Furthermore, the comparisons with M-GAN [2], P2PGAN[6], and U-Net[12] are not appropriate, since [2] and [12] are designed for PET image synthesis, and [6] is proposed for natural image translation. It is unfair to compare the proposed framework with these methods that do not aim at the CT synthesis task."	The two-stage training strategy is complex and not elegant. Ablation study should compare models with and without domain-domain losses to claim the effectiveness of dual-domain learning.	This method is based on 2D image processing. However, volumetric information in 3D data is essential for the majority of medical tasks.	The description of the training details in this paper is insufficient. Moreover, the authors did not give any positive response regarding the code release in the reproducibility checklist. So I think this paper does not have good reproducibility.	The dataset is private and the codes are not released.	The method description is clear with sufficient details.	"a) The authors should state the motivation more clearly. To the best of my knowledge, this is the first work to synthesize CT images from PET. I wonder why the authors use PET, which is more expensive to acquire and has limited available data, to generate CT images that are much cheaper and easier to obtain? b) The authors should give a more detailed review of existing cross-modality synthesis methods, especially for CT synthesis. c) ""Training four networks in Stage 2 will bring more computational costs with little benefit compared with the case of only training image domain networks."" The paper lacks the analysis of the concrete performance and computational cost gaps induced by training two/four networks. It would be better to add a corresponding ablation study or a comparison experiment. d) To validate the superiority of the proposed method, the authors should compare it with state-of-the-art methods for CT synthesis. e) In the second paragraph of ""Ablation Study"" section, ""necessity of employing tje secondary"" should be changed to ""...employing the secondary""."	see p5	"In the Introduction, second sentence: ""PET records the consumption of glucose in organs, revealing their metabolic characteristics and thus potentially the disease status"". This is not accurate. Only FDG tracer records the consumption of glucose in organs, but there are other types of PET tracers as well. In the abstract and introduction, the authors claimed that ""existing works perform learning-based image synthesis to construct cross-modality mapping only in the image domain, without considering of the projection domain, leading to potential physical inconsistency"". This is not correct. Please refer and cite this paper for accurate literature review: Shi, Luyao, et al. ""A novel loss function incorporating imaging acquisition physics for PET attenuation map generation using deep learning."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2019. In Section 3.1, image volumes were sampled into 2D slices and then split into training/validation/testing samples. Did the authors make sure that the training/validation/testing slices are from different patients for each set? If so, please mention that in the text. This method is based on 2D image processing. However, volumetric information in 3D data is essential for the majority of medical tasks. I can understand that expanding this work to 3D processing can be challenging due to increased computation workload, more complex forward/backward projection and limited data. However, if the authors can show that the proposed method is better than a 3D U-Net or a 3D GAN (with a typical 32x32x32 3D patch size for example), it can make this work more impactful and more convincing for real clinical applications (optional)."	This paper implemented a PET-to-CT synthesis framework by exploiting both image and sinogram information and constructing bidirectional cycle mapping. The authors have conducted several comparison and ablation experiments to prove the effectiveness of the proposed method and the validity of the key components. However, its disadvantages are also obvious. As mentioned before, the motivation for this article was not stated clearly, since the cost of acquiring abundant PET images for training is much higher than that of CT. In addition, the details of existing CT synthesis methods need to be better described. Another serious problem is that current comparison methods are somewhat outdated and not sufficient to prove the superiority of the proposed method. Finally, the innovation of this paper is not sufficient, since the dual-domain information has been introduced to enhance CT quality before. And the cycle consistent constraint has also been widely used.	The idea is interesting and the experiments partly support the main contributions. It is novel to use dual domain learning in PET to CT transformation and cycle consistent losses help to boost the performance.	Despite focusing on 2D data processing (which is understandable in early development), the proposed method exploited both image domain and projection domain information and achieved significantly improved results. This might potentially make an impact in clinical applications once the method is adapted to 3D volume data processing.
313-Paper0549	Mask Rearranging Data Augmentation for 3D Mitochondria Segmentation	The authors propose a novel data augmentation method to improve the segmentation of mitochondria in 3D electron microscopy (EM) images. The method is based on (1) a pix2pix-like network to generate realistic 3D EM images from mask (label) images which is trained in an adversarial fashion, and (2) a 3D mask generator that produces realistic mitochondria labels using size, distance and morphological priors from the training set. The presented method boosts the performance of state-of-the-art segmentation networks (that use traditional data augmentation) on a public dataset, especially when training data is scarce.	The authors propose to boost 3D mitochondria segmentation by synthesizing images from synthetic instance layouts. It uses  one publicly available dataset for validation and shows the benifit  of the proposed data augmentation strategy.	In this paper the authors propose a generative adversarial modeling approach to augment data for 3D mitochondria segmentation in EM images. They first trained a pix2pix GAN model to synthesize realistic mitochondria EM images with instance masks. To increase the diversity of appearance for the synthesized mitochondria images, they design a pipeline to rearrange instances of mitochondria in the 3D masks and then feed these rearranged masks as input to the image synthesis network. The generated images are mixed with real images and used for training the segmentation network. The authors validate the method on one public dataset under the conditions of fully accessing all training data as well as reduced numbers of training examples, outperforming several baseline methods.	To increase the usually limited training data of 3D EM segmentation datasets, the authors propose an original method based on state-of-the-art image generation models adapted to 3D. The mask layout generator is a simple and effective solution to produce more diverse but realistic masks, from which synthetic EM images can be later generated. This is an interesting approach that opens the door to also generate synthetic images on other domains. The results of using this data augmentation method are evaluated against the state of the art in the field of mitochondria segmentation on EM volumes and its impact is analyzed with a proper ablation study and by simulating the scarcity of training labeled data.	The paper is fairly well written, albeit the method and presentation of results can be improved. The authors  synthesize more diverse  images by producing more synthetic instance layouts The paper performs rigorous analysis with ablation tests.	The proposed generative module is independent of, and thus can be integrated into, any segmentation network, which can be leveraged to benefit various existing and new segmentation network designs. The authors designed and conducted experiments with decreasing numbers of accessible annotations and demonstrated the increasing superiority of the proposed model compared with baseline methods. Such type of experiment dealing with data scarcity issue is very valuable to the community.	The method is only tested on a single public dataset, while others (Kasthuri++, VNC, etc.) are publicly available as well. Moreover, the selected dataset (Lucchi) is isotropic, while many EM datasets present a lower resolution in the z direction. This problem should be taken into account for a more generalist solution. In the comparison with the state of the art, the number of execution trials and hyperparameter exploration is unclear. There is no information about execution times, which would be very interesting for the final user.	The authors fail to review previous studies on  learning the mapping from instance masks to real EM images, ahlthough previous methods only synthesized 2D images. The authors fail to investigate the effect of the postprocessing.  When comparing with other methods, it is better to declare which method uses the postrpocessing strategy in this study. For example, did the 3D U-Net and 3D U-Net (w/ ours) in Table 1 use the same postprocessing? From the results in Table 2, it seems the model using the proposed method and 1/16 data outperforms the model without using the proposed method but using 1/8 data. Note that the second method use the same amount of data ( due to your special training setting of each batch) as the first method but with real data. It is insteresting to interperate this result.	The proposed method is a straightforward extension from 2D version for generating synthetic 3D mitochondria images, which directly utilizes pix2pix setup. The authors designed mitochondria mask rearrangement technique, but this is a general engineering tweak with incremental contribution: take the pix2pix GAN for example, in the test stage, one can use any hand-drawing as mask input to the generator, where the objects are arranged in any location the users want; such a general and already existing technique is not specific to mitochondria image generation and thus the first contribution the authors stated seems a bit weak. In addition, there are other concerns about the mask rearrangement technique; please see the comments section. The experiments are limited to one mitochondria dataset with two correlated volumes, which makes the second contribution (experimental validation) not particularly strong as well. Although the authors demonstrated improvements over previous works, it is hard to be fully convinced about the superiority of the proposed approach with results from one dataset. It will be highly valuable for the authors to validate the approach in other public mitochondria EM segmentation datasets, such as the rat and human datasets from mitoEM (Wei et al., MICCAI, 2020) and potentially also VNC III (Gerhard et al, Figshare, 2013).	The authors do not mention the range of hyper-parameters considered nor the method to select the best hyper-parameter configuration. They only specify some of the hyper-parameters used to generate results. The exact number of training and evaluation runs (iterations or epochs) is not provided. A description of the computing infrastructure used (hardware and software) is provided together with an analysis of situations in which the method failed (too limited data). There is no description of the memory footprint nor an average runtime for each result, or estimated energy cost. There is no analysis of statistical significance of reported differences in performance between methods. The results are not described with central tendency (e.g. mean) & variation (e.g. error bars). The specific evaluation metrics and/or statistics used to report results are correctly referenced. There are no details of train / validation / test splits nor details on how baseline methods were implemented and tuned.	"The authors listed ""yes"" for both code and pre-trained models. In this case, it can be an easy task for both training and testing. If the reproduction was only based on the descriptions in the paper, it could be somewhat difficult."	"No repo link is provided in the manuscript but the authors stated that related code will be released in the reproducibility response; considering that the key parameters about mask rearrangement is not clearly stated (please see comments section), it can be helpful to provide code or provide more complete description. In the reproducibility response the authors stated ""yes"" for the following list of information but did not include corresponding information in the manuscript: mean, variation, statistical significance of experimental results; runtime, memory footprint; failure cases. Could the authors at least report mean and variation of the results?"	"An effort should be made with regards to reproducibility and evaluation. More specifically, the authors should provide a better description of the range of hyper-parameters considered, the number of training and evaluation runs, validation split and validation results, etc. In that sense, I recommend to follow the code of good practices proposed by Dodge et al. (""Show your work: Improved reporting of experimental results"", 2019). If possible given the short review processing time, I recommend testing the method on other datasets as well."	It will be better to review previous studies on  learning the mapping from instance masks to real EM images. There are previous methods on synthesizing 2D images. It will be interesting to show the results by models training only on synthesized image.	"Page 2 Paragraph 2, the authors stated that ""... aims to synthesize enough diverse mitochondria EM training data ..."". The authors designed mask rearrangement techniques in order to generate diverse images. The proposed mask rearranging strategy takes into consideration the size, number and relative spacing of the mitochondria in real images, which makes a lot of sense. But there are concerns as follows: 1.1. Key details and supporting evidence of mask rearranging strategies are missing: (1) the authors studied the actual size distribution and distance distribution of mitochondria, but none of these statistics are included in the manuscript to support the design of two described strategies. (2) key details, including the mitochondria number and size distribution in the synthesized images, are missing. This also leads to the following questions: (a) are there other characteristics of the mitochondria images  (e.g. the overall density of mitochondria, ) needs to be matched in order to synthesize realistic looking mitochondria images? Whether it matters or not to match these characteristics of mitochondria to those of real images in order to improve segmentation performance? 1.2. The relative location of mitochondria and biologically important structures in the background is ignored.  Unlike natural scene images, or even fluorescent images only targeting and imaging specific cellular components, where a foreground  instance can appear in various locations in the scene and even unlikely locations (this can be justified since the natural scene image can be synthesized, like an advertisement image), the background in mitochondria images, however, has biological meanings. One question is, when the orientation and localization of mitochondria are changed by the rearrangement technique, how are the background filled? Do the synthesized biological structures in the background look realistic? Does it matter to have realistic background in order to improve segmentation performance?  1.3. Evaluation of the quality of generated images is missing. It is common practice and standard to have experts evaluate image quality. In addition, considering the abovementioned concerns regarding the mask rearrangement technique, it will be highly valuable if experts with domain knowledge on mitochondria EM images can validate the generated images in terms of how realistic they are. On Page 7, the authors showed that the proposed approach ""can improve the segmentation performance by alleviating the false and missed detection cases, respectively."" This is a generic statement and not very clear to me how the two examples in Fig. 2 can fully demonstrate how the proposed method improves segmentation. It would be great for the authors to provide more insights and reasoning in this regard as well as potentially show more examples. In addition, study failure cases: What type of failure modes are there? Is there any type of failure mode with the proposed method which are note present with the baselines? In Section 3.2 ""Learning from limited training data"", it is described that ""... decreasing the training volume on the z-axis, which are one-half, one third, one-fourth, and one-fifth, respectively"". How does the selection of sub-volume affect the results (for example, which ""half"" of the volume)? It seems that here only one consecutive block along the z-axis is selected, but does it preserve more data diversity to select multiple consecutive image blocks spread along the z-axis, each of which have smaller depth (i.e. smaller number of image sections in z-axis)? For example, to reduce a 3D volume of 100 images by half, could it preserve more example diversity to select No. 1-25 and 51-75 image sections, instead of No. 1-50 image sections? Confusing method description: 4.1. On Page 3, the authors stated that ""we apply 5 times downsampling with 3x3x3 convolution of stride 2, so the maximum downsampling rate is 32."" But in Supplementary material Figure 1, it showed 4 downsampling operations.  4.2. In Section 3.1, the size of each training example d by h by w is 32 by 256 by 256, but why it is changed to 33 by 306 by 306 on Page 7 ""Learning from limited training data""? I suggest that the authors explicitly clarify and explain the reasoning of their design choices. For future work, I would also suggest the authors consider more recent generative models for conditional generation of images, such as instanceGAN (Mo et al, ICLR, 2019), SPADE (Park et al, CVPR, 2019), SEAN (Zhu et al, CVPR, 2020) etc. Typos and grammar:  6.1. Page 2 Paragraph 2 ""... obtaining considerable accurate segmentation annotations"". Would it be ""... obtaining a considerable number of accurate ...""? 6.2. In Page 2 Paragraph 2 and a few other places in the text, ""perceptual realistic 3D EM images"" could be ""perceptually realistic...""."	The novelty of the proposed method is clear and the results are promising, although the presentation of the results should be improved and the use of more datasets is recommended.	The investigation of conducting mask rearranging data augmentation is interesting and seems useful. The paper is neat and in principle.	The topic of data augmentation is of interest to the community and the experiment on reduced number of annotations is intriguing. However, there are concerns about incremental novelty on methodology as well as limited experiments and validation.
314-Paper0438	MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation	The manuscript describes a method for training a model that is more robust to domain shifts with only single domain data. The authors do so by building two more things on top the MixStyle method: 1) positioning the MixStyle layers in the decoder instead of the encoder, and 2) introducing adversarial noise in MixStyle layers to encourage more robust feature extraction.	This paper propose a style augmentation method with adversarial training scheme.	This paper proposes a data augmentation framework which achieves out-of-domain robustness using a single-domain dataset. Proposed method maximizes the effectiveness of style augmentation by producing worst-case style composition via adversarial training. Experimental results shows proposed model can achieve little better performance than baselines in terms of dice score.	1) The method is innovative, especially the part where adversarial mechanism is used to further improve the MixStyle layer perturbation. 2) The experiments conducted are robust. The Reviewer appreciates the amount of MR data that the Authors are able to collect and test. The Reviewer also appreciates the paradigm of comparing training on high-data regime vs low-data regime. The Reviewer also appreciates the inclusion of prostate dataset in the Supplement.	"The idea of adversarial learning between segmentor and style augmentor to generate ""hard"" style for segmentation is kind of interesting. The organization is good, which is easy to follow. Many aspects of evaluation of the effectiveness of the method."	The idea of expanding style space with additional noise and search for harder style composition is interesting and somewhat noble. Extensive experimental results and ablation studies shows the superiority and effectiveness of proposed model. Also, the clarity and organization of the paper is good.	The Reviewer does not find many major weaknesses except one: during result comparison, it would be good to include a brief description of what those compared methods are. In particular, the Authors should describe what the baseline is.	There is one issue in Table1. As the images generated by style augmentor is feeded to train the segmentor, why the performance drops comparing with baseline on IID? In my opinion, the model has learned the variants and it should achieve better results than the baseline.	Although Fig. 1 shows visualizations of Mixstyle and Mixstyle-DA, it would be helpful to add few more visualizations from other baseline methods.	Before the publication of the Authors' codes, it is difficult to replicate directly. However, the description of the method is clear and the foundation on which this manuscript is based on, namely, the MixStyle method, is easy to reproduce.	This paper is highly reproducible with detailed information in paper and the code will be released.	The authors will provide the code. Used datasets are public.	"In Fig. 2, the Reviewer sees that the two backpropagation updates are both ""Maximize L-seg"". Should one of them be ""Minimize"" because of the adversarial nature of the training process?"	What is the baseline compared in Table 1. The authors should give clearly states. Is it just a encoder-decoder segmentation method? The results on prostate segmentation should be in the main text, or the tittle should be cardiac segmentation instead of medical image segmentation. Page 6, there is a missing right parenthesis in eq (5).	The idea of expanding style space with additional noise and search for harder style composition is interesting and somewhat noble. Although Fig. 1 shows visualizations of Mixstyle and Mixstyle-DA, it would be helpful to add few more visualizations from other baseline methods.	The method is innovative, the experiments are robust, the results are promising, and the presentation is clear.	The idea is interesting to augment style with adversarial learning. Extensive experiments are conducted to show effectiveness of the proposed method. The paper is well organized starting from preliminaries to the improved version. One issue has not been explained in the paper, which makes the result not convincing and needs to be clarified.	The idea of expanding style space with additional noise and search for harder style composition is interesting and somewhat noble. Extensive experimental results and ablation studies shows the superiority and effectiveness of proposed model. Also, the clarity and organization of the paper is good.
315-Paper0755	MCP-Net: Inter-frame Motion Correction with Patlak Regularization for Whole-body Dynamic PET	This work seems like an extension of the work in [10]. Full text of [10] is not accessible. The difference from [10] might be that, Patlak fitting error, which accounts for the tracer kinetics in dynamic PET imaging, is added into the cost function when training a neural network based model to learn the motion displacement fields for motion correction.	The authors propose an inter-frame motion correction framework called MCP-Net with Patlak regularization and a Patlak loss term to register whole body dynamic PET scans. The framework consists of: 1. a 3D UNet based motion estimation module, 2. a spatial transformation module to warps images, and 3. an analytical Patlak module to estimate Patlak fitting. This paper uses tracer dynamics to improve network performance for image registration, and it performed better than traditional non-rigid and other deep learning based algorithms in correcting spatial mismatch, reducing normalized fitting error, and improving spatial alignment of K_i and V_b images.	This paper proposed a way to correct inter-frame motion during the acquisition of a whole-body dynamic PET scan.  The proposed MCP-Net takes tracer kinetics into account, as opposed to other methods that treated motion correction as a registration problem.  Qualitative and quantitative analysis showed that this framework is promising for improving the accuracy of dynamic PET.	A kinetic model (Patlak) is added to account for tracer kinetics in dynamic PET imaging. Although Patlak only applies to irreversible tracers, the model itself is very favourable for motion correction, as the plasma input Cp on the right hand side of the equation is not affected by motion and can provide some robustness for using this as a constraint to estimate motion. The evaluation in this work is done extensively.	The motivations described in the paper define the need for an image registration technique in whole body PET very well. The integration of a Patlak fitting module into the registration framework is novel. The addition of a Patlak loss penalty through a mean squared percentage fitting error in the loss function is new. The comparison of the proposed approach against previous methods is insightful and the metrics chosen to describe the performance are adequate (e.g. normalized mutual information, avg-to-ref SSIM etc.). Statistical analysis was also conducted, which is a plus given that not many statistical tests are run these days in DL in medical imaging papers. The paper is easy to read and organized clearly.	This work makes use of tracer kinetics, which is a key characteristic of dynamic PET.  For this particular problem that the authors are trying to address, it makes a lot of sense to treat the motion correction not as a merely geometry problem.	The difference between the proposed MCP-NET model and the previous  B-convLSTM model [10] is not clear. The full text of [10] is not available. Is MCP-NET= B-convLSTM + Penalty(Patlak)? If so, why was the hyperparameter lambda set to different values (0.1 for MCP-NET and 1 for B-convLSTM) in the comparison? The proposed model aims for inter-frame motion correction in whole-body PET imaging. The frame length for motion correction in this work is 5 min. However for whole-body imaging lots of the movements such as cardiac motion, respiratory motion, sliding motion etc, are continuous. Thus the motion correction is, to some extent, limited in time resolution. Motion-introduced mismatch in attenuation/scatter correction is not mentioned. Based on the protocol given in the Supplementary, a single CT was taken before the PET scan, and the motion correction was done post reconstruction. I understand this is a problem for all post reconstruction motion correction methods, but hopefully the authors can be aware of this. The original image resolution is not given. 4X downsampling was applied before feeding the image data to the neural network, which could mean 5-10mm voxel size? The effects on the resolution of motion estimation is not clear. Fig2 does not have units for motion fields either. Why use LNCC as a similarity measure for dynamic frames?	It seems like the amount of data used to train the model is smaller than usual. I might be wrong, but it seems that there are only 19 frames per volume for 27 patients in total. Is this correct? If so, how many volumes were used for training/validation/testing? This point could be clarified further as I am not sure if the model is overfitting given a limited data quantity. 57 hyper metabolic regions were selected for additional evaluation by a nuclear medical physician. Are these the same regions that are shown in Figs. 3, 4, and 5? While it must be challenging to acquire data from additional scanners (other than the Siemens Biograph mCT), how well does this model translate to data acquired from different scanners within the same/different institute?	The lack of ground truth makes the results questionable to a certain extent.  The authors tried to address this by motion simulation, but it is not known to me how realistic the simulation was, especially with the low number of subjects involved.	The authors have given lots of information for reproducing this work, with some details missing.	The paper presents a link to the code, which I'm assuming will be public upon acceptance.	Good	The order of citation numbers in the main text is random.	"It looks like both B-ConvLSTM and MCP-Net seem to have very very slight difference in performance (0.9513 +- 0.014 vs 0.9523 +- 0.014 SSIM, 0.6390 +- 0.1005 vs 0.6197 +- 0.1032 torso NFE etc.). So, it is difficult to envision the scale of improvement; how much error is actually tolerable clinically by any method? Following this train of thought, the authors mention that MCP-Net achieved ""lowest remaining spatial mismatch"" and for ""significant motion in the hand and bladder, ..., MCP-Net still has the capability to reduce error."" But this error bound is hard to define; ideally, we want perfect registration between reference/source, so what is the tolerable level of error that we can get before this model will be practically (clinically or pre-clinically) useful? The data sub-section of the paper could use some clarity. From 27 subjects, how many volumes were used? If they each have 19 frames, then why were they resized/reshaped to 128x128x256 and not 128x128x19? Was there some volumetric resampling that occurred? I'm curious to know the instances where MCP-Net failed in registration? What caused its failure and how much was the fitting error? Perhaps B-ConvLSTM and MCP-Net could be used in conjunction in an ensemble registration framework?"	The authors should think of some ways to get more ground truth (or close to ground truth) data to make the results more convincing.  Even though the motion could be non-rigid, at least some measurable gross motion could still be useful.	This work includes a kinetic model to account for tracer kinetics in dynamic PET imaging for data-driven motion correction. It might be an extension of a previous work. By using a computationally simple kinetic model, the authors demonstrated the improvements in the final outcomes. This work has been successful in proving its motivation.	I still have a few concerns about the data aspect of this paper, but the results seem solid (despite being ever so marginally higher than B-ConvLSTM). If the authors can explain the data section better, it would be helpful. It would also alleviate concerns with overfitting.	Making use of tracer kinetics for motion correction is the right direction to address this problem.
316-Paper1909	Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction	This paper proposes a denoising diffusion probabilistic model based approach for under-sampled medical image reconstruction. A measurement-conditioned DDPM method is proposed in measurement domain. Promising results on MRI reconstruction are demonstrated in the experiments.	This paper presents a novel and unified mathematical framework-MCDDPM, for medical image reconstruction using  under-sampled reconstruction. It is very meaningful.	-This paper applies DDPM to undersampled MRI reconstruction.  -The condition is on under-sampling, which inherent the data consistency in the recon pipeline.  -The proposed method allows uncertainty quantification.	This paper is well organized. The presentation is clear. The diffusion and sampling process are defined in the measurement domain and conditioned on under-sampling mask, which makes the proposed method different from the existing DDPM methods. Experimental results show that the proposed method is superior than compared baselines.	As one of the highlights, the diffusion and sampling process are defined in measurement domain rather than image domain.	-The application of DDPM to accelerated MRI reconstruction is novel.  -The condition is on the measurement.	This paper restricts the under-sample matrix M to be a diagonal matrix with element to be either 1 or 0. It limits the impact of the proposed method. It is worthy to study how to apply the proposed method to other under-sample patterns, such as random or spiral. The assumption that the noise in equation (5) is set as zero needs supporting evidence/justification. Comparison with DDPM on the image domain is expected to justify the superiority to do DDPM in the measurement domain.	In the discussion part, the weakness of the proposed method is somewhat less.	-The paper is not clear. Apart from the typos, the paper introduces CT, rather than MRI in section 2.2. I understand the authors want to introduce a general framework, but it makes me feel confused.  -The authors only use magnitude image for DDPM, however, MRI is complex-valued in nature, and the phase information has clinical significance. How to apply to complex-valued MRI?  -The comparison with U-Net is not sufficient. More comparison with state-of-the-art models, like unrolled networks, is needed.	It is expected the results can be reproduced with the given information.	very good.	The dataset is publicly available fastMRI single-coil knee image. The detail configuration is described. However, it is not clear how to split the training/validation/testing dataset. Is this the same with the challenge?	See weaknesses. typos: Section 2.2:  reconstruct x from y as (accurate as) possible Therefore, the task of under-sampled medical image reconstruction (is) to reconstruct the posterior distribution.	"The order of references in the text is right? Does it match the template? please check it carefully. In page 2, paragraph 2: In this paper, We design our method. Maybe ""we"" is correct. In the discussion part, the weakness of the proposed method is somewhat less. Please give more discussion on the proposed method, if possible. This manuscript is well constructed, so I recommend it for publication."	-DDPM can be very slow, which may limits its application. The authors did not discussion/mention this point. Accelerate the DDPM needs further investigation. -More benefits of using DDPM, rather than current state-of-the-art unrolled network should be mentioned. I did not see any comparison in the experiments.	The proposed measurement conditioned DDPM for accelerated MRI reconstruction is novel. While there are some limitations and weaknesses of the proposed method, the proposed method demonstrates promising results on the test dataset.	This manuscript is well constructed and the method are well verified by adequate results.	The application of DDPM to undersampled MRI reconstruction is the novel part. However, the paper lacks the comparison with the current methods. It also not clear if DDPM is practically available. e.g. the discussion on the computational time is missing.
317-Paper1138	Mesh-based 3D Motion Tracking in Cardiac MRI using Deep Learning	This paper proposes an image-based mesh motion estimation network for 3D myocardial motion tracking, which estimates 3D mesh displacements from the intensity information of 2D CMR images in an end-to-end manner.	The authors propose a deep learning method for 3D cardiac motion tracking from 2D MRI. Rather than computing a dense displacement field, the proposed method learns to predict the vertex displacements of a 3D mesh from the 2D input images. This requires a novel mesh-to-image rasterizer to translate the motion estimates to 2D. This allows cardiac motion estimation and mesh-based segmentation.	The authors claimed 3 innovations: 1) Achieved cardiac motion tracking on a geometric mesh using a deep network. 2) Used a mesh-to-image rasterizer to summarize related 2D information into 3D motion estimation. 3) Can achieve both motion estimation and image segmentation.	The proposed method models the heart as a 3D geometric mesh and propose a network to estimate 3D motion of the heart mesh from 2D short- and long-axis CMR images. The paper is well-written and easy to follow. The authors provided sufficient details about the proposed method.	* Experiments are performed on a large cohort of 530 subjects from the UK Biobank (testing set of 80 subjects). * Ablation studies clearly demonstrate the design decisions for both the input image configuration (using 3 views) (Table2) and the loss functions (Table 3). * A (brief) discussion regarding the choice of algorithm hyperparameters is presented (Sec. 3, Discussion). * The proposed method demonstrates improvements in registration accuracy compared to three other cardiac registration methods. * The clinical problem and background literature are covered well.	The inclusion of the mesh-to-image rasterizer is a bright idea. Usually combining 2D image information into any 3D tracking result needs an underlying interpolation step which takes time and extra computation resource (also introduces error). The rasterizer used in this work extracts 2D shape information in multiple directions and summarizes its together for 3D, which is a computationally more efficient process.	(1) This paper utilizes no regularization loss for the registration step. Though the proposed method imposes Laplacian smoothing loss on predicted mesh, it would be better to provide discussions or analyses of the reason for the absence of the registration regularization loss. (2) Self-supervised differentiable segmentation losses have been widely used in related motion tracking works [1][2]. It will be better for authors to discuss these works in this paper. [1] Self-supervised Learning of Motion Capture. [2] Self-Supervised Learning for Cardiac MR Image Segmentation by Anatomical Position Prediction.	* Unclear how well the method performs quantitatively across the full cardiac cycle (results appear to show motion correction between end diastole and end systole).  * (minor) Comparison registration methods may also benefit from multi-view inputs, but these are never tested.	"1) Cardiac motion tracking on a geometric mesh model is not uncommon. 2) Simultaneous motion tracking and segmentation is not a completely novel achievement. 3) Comparison with only a few previous methods before claiming ""the proposed method quantitatively and qualitatively outperforms both conventional and learning-based cardiac motion tracking methods"" is not convincing."	Good reproducibility.	The dataset and imaging information used in this study are well described and the model training parameters are detailed in sufficient detail.	Reproducible. Data available. Method detailes described well.	"Minor point: (1) One more period in the title of Section 2.3.  (2) In Section 4, ""...propose a image-base...'' needs to be improved."	Sec. 3 (Tables 1, 2, 3): From the text in Sec. 3 it appears that quantitative motion correction evaluation is calculated at only two time points (end diastole and end systole). It would be interesting to report the values at different times in between as well to verify quantitatively that the method works well across the full cardiac cycle. Sec. 3: Based on the results of the ablation study to include different views of the heart (Table 2), it appears that a major source of performance improvement comes from having two or more views (the addition of the 2CH and 4CH views). While comparison to the three other registration methods using SAX only images (Table 1) is very strong, and the results in Table 2 demonstrate that SAX only imaging for the proposed method shows improvements over the comparison methods, these results raise the question about the other methods also being improved using multiple views. This may present a technical challenge for the other methods, but it might be worth discussing.	"This is a very interesting work with some good innovation (especially the mesh-to-image rasterizer part mentioned above). The paper is also well written. It is definitely worth being published in some form. Though considering the novelty required in MICCAI, I am not totally convinced to recommend a full acceptance. Cardiac motion tracking is a very wide field with numerous previous methods. The authors tried to separate this work from those ""voxel-wise deformation estimation"" methods by working in the mesh tracking area. This is a good strategy and vertex tracking is definitely beneficial in many ways. But you cannot help compare it to those voxel-wise tracking methods because they have been used for a long time. The hardest job for the authors is to show the value of this new method and justify its value. But it feels too weak for me. For example, in the initial introduction the authors briefly mentioned those voxel-wise methods [17,18,2,27], which are pretty recent but are also very limited. They are two MICCAI papers, a CVPR paper, and one journal, which are way less than enough to represent those previous works. Later in related works, the authors introduced more, but they were mostly registration-based ""conventional"" methods, which, again, are just too limited to cover the 3D cardiac motion tracking field. This is not to mention the overwhelming amount of deep learning-based tracking methods nowadays. Also, in the experiments, the authors compared the method to FFD, dDemons (representing the conventional methods), and U-Net (representing deep learning methods). And the conclusion was ""Experimental results show that the proposed method quantitatively and qualitatively outperforms both conventional and learning-based cardiac motion tracking methods."" This makes it a rash conclusion. As said above, conventional methods come in way more categories and FFD and dDemons barely suffice to represent the deformation kind. The same goes for learning-based methods. So the justification of proposing this method is not strong enough for me."	The paper is innovative and the experimental evaluations are convincing.	This is a well written paper with good experimental setup and validation of a novel  cardiac motion correction method based on deep learning. The authors provide a rigorous set of validation experiments to compare their proposed method to other registration approached and include a strong set of ablation studies to demonstrate their design decisions.	1) This is an overall interesting work. Very good innovation. Manuscript well written. Has scientific value. The novelty is good enough for a weak accept but not outstandingly impressive. 2) The part of comparison to previous works part is still weak. Not too convincing for me to recommend a strong acceptance.
318-Paper0422	Meta-hallucinator: Towards few-shot cross-modality cardiac image segmentation	This paper presents a method for few-shot cross-modality domain adaptation, with the goal of training a model on a label-scare source domain and then adapting the model to an unlabeled target domain. The method leverages meta-learning, mean-teacher based semi-supervised learning, and image-and-spatial transformation. The effectiveness of method is validated on a popular cross-modality domain adaptation dataset for cardiac segmentation, showing improved performance over prior works on this challenging scenario.	This paper proposed a novel method of learning cardiac image segmentation from few-shot cross-modality dataset. The core idea is using meta-learning to train a transformation-consistent meta-hallucination framework for unsupervised domain adaptation with a few labels.	To achieve efficient few-shot crossmodality segmentation, this work proposes a novel transformation-consistent meta-hallucination framework, meta-hallucinator, with the goal of learning to diversify data distributions and generate useful examples for enhancing cross-modality performance.	This paper tackles a challenging domain adaptation problem, for which the source domain only has a few labeled data and other data are unlabeled. A reasonable method is proposed and obtains improved and promising performance.	The paper is well written. The idea of meta-hallucination framework is novel. Unsupervised domain adaptation is a useful technique for segmentation problems where labels are sacrce. Combining meta-learning and semi-supervised augmentation might be a good attempt. The experiment results are outstanding and convincing. The performance seems to be much better than other few-shot methods, and comparable with the fully supervised method.	The manuscript is well-written and easy to understand. The idea to achieve efficient few-shot cross-modality segmentation with limited labeled source data is interesting. The experiment is sufficient to prove the effectiveness of the proposed method.	"The main weakness is that the methodology lacks clarity. The method consists of multiple components and needs to be trained in separate steps with different types of data. Without clearly knowing how each component works and how the data split, it is difficult to evaluate the method. It is not known how the labeled source data, unlabeled source data, and unlabeled test data are sampled and split respectively in the meta-training and meta-testing stages. How to simulate the structural variances and distribution shifts? What types of data are inputted into the teacher model, student model, and the hallucinator respectively? The effect of the hallucinator is confusing. In the method section, it is presented that the hallucinator is to produce ""more meaningful target-like samples"" x^{s\to t}. However, implementation details present that ""Since limited labels are provided in the source domain, we transform target images to source-like images for training and testing"". The two descriptions are conflicting with each other."	Since the main contribution of this work is unsupervised domain adaptation, it is expected to see experiment results on more datasets of different types and modalities. Currently, there is only results on heart images provided. The ablation study discusses only meta-hal and meta-seg. More detailed analysis could help to understand the proposed method better, e.g., how the choice of support images will affect the performance, what the hallucinator learned through meta-learning.	"The author should consider adding another experiment of few-shot learning (FSL) method, e.g. SSL-ALPNet[1], in target domain to show the upper bound of FSL; Some minor errors: The 'leverage' in the second line of page 3 should be 'leverages'. The proposed method adpots CycleGAN to achieve unpaired image translation for image adaptation, which is compution intensive. [1] Ouyang, Cheng, et al. ""Self-supervision with superpixels: Training few-shot medical image segmentation without annotation."" European Conference on Computer Vision. Springer, Cham, 2020."	The training hyperparameters are clearly described, but some methodology details are missing.	Authors agreed to release the code.	The dataset is public available. The authors promise to release the codes.	The inputs to each network shown in Fig. 1 are unclear and the training process also lacks clarity. The authors could consider adding an algorithm box to better explain the pipeline. Using only mean teacher (MT) significantly improves the performance from 14.0% to 49.9%. MT is a semi-supervised learning technique, but the result seems to show that MT benefits the cross-modality adaptation to a large extent. Could the authors provide more explanations on this result?	What is the difference between generators in GAN and hallucinator in this paper?	More badly case analysis More experiments on different types of distributions shift.	The methodology lack clarity, thereby it is difficult to determine how each module of the proposed method works to achieve adaptation under the challenging scenario.	Novel method for an important medical imaging problem.	the idea is simple and straightforward. the sufficient experiment to prove the effectiveness of proposed method
319-Paper2696	MIRST-DM: Multi-Instance RST with Drop-Max Layer for Robust Classification of Breast Cancer	The authors propose the Multi-instance RST with drop-max layer which includes a sequence of iteratively generated adversarial instances during training to learn smoother decision boundaries on small datasets.	The authors investigate the hot research topic of adversarial learning on robust self-training for image classification purposes. The generalizability and reproduction of the existing adversarial robustness on small medical image sets are considered to make some improvements. The authors proposed a multi-instance robust self-training with a drop-max layer to learn smoother decision boundaries on small datasets.	The paper proposes a method to defend against adversarial attacks for the task of breast tumor classification on ultrasound B-mode images. The method extends Robust Self Training by adding multiple instances of adversarial examples with gradually increasing perturbation during training and a dropmax layer to smooth the decision boundaries and achieve higher adversarial robustness.	The proposal of multi-instance RST and drop-max layer seems novel. The proposed approach performs significantly well against the adversarial attacks.	The authors investigate the adversarial robustness of the deep models on small medical image classification tasks. A multi-instance robust self-training with a drop-max layer is proposed to make robust training. The proposed drop-max layer can remove unstable features to learn robust representations. Experiments validate these claims for medical image classification purposes.	The lack of large datasets if very common in the medical field, therefore adapting approaches to work well will less data is beneficial. The paper is well written. Cross-validation is used, which is critical for such small datasets.	I'm not convinced about the clinical motivation of adversarial defenses in medical images. It is possible that this problem might gain traction and significance in future, however, I don't see the significant interest in this problem for wider audience.	In computer vision tasks, especially on adversarial machine learning, it exists on adversarial examples on small datasets. The authors may be one of the first attempts on this task, but the overall idea is still somewhat weak. Adversarial training or co-training is one useful trick for improving the robustness of one medical image classification task, while the overall learning scheme is hard to avoid the severe overfitting problem. Could the proposed method be adaptive to target attacks? It seems most of these efforts are devoted to un-target attacks.	The dropmax layer is an interesting and intuitive idea. However, choosing the second largest value does not guarantee that the perturbations are ignored. To give a simple example of an alternative approach; one could ignore or 'drop' the upper quantile of values instead of ignoring just the top-1. An interesting experiment would be showing how the performance changes as more of the max-values are ignored. The standard deviation across the five folds could have been reported.	The paper seems reproducible.	The authors should release their codes, or this paper is not easy to be reproduced.	The hyperparameters for the model training are clearly given. The datasets used are publicly available. There is no mention that the code will become publicly available upon acceptance.	The related work of this paper is quite long and can be shortened significantly to make more space to explain the proposed approach. Generally speaking the term multiple instance is used for semi-supervised (SS) learning methods. However, RST itself is a SS approach, so the use of multiple instance with RST is not clear.	See weakness.	There is a typographical error: CIFA-10 instead of CIFAR-10. In Table 2 the highest values for every row should be made bold. Specifically in the 'No Attack' scenario, the baseline has 0.831 f1-score over the MI-RST that has 0.830. I understand that the comparison is focused between RST and MI-RST but in my opinion making a lower value bold is confusing to the reader. Fig. 2 has low resolution and I would recommend a high resolution version of it to be added to the manuscript. The name multi-instance is a bit misleading since it could refer to multi-instance learning which is not related to the paper. An alternative could be multi-adversary. The word 'significantly' is used widely for the discussion of the results but there are no statistical tests performed so I would replace it with 'substantially'. To my understanding, all methods were trained using the same hyperparameters but that could be unfair for some of the baselines. I would recommend using the exact hyperparameters stated in the papers of the comparative methods to achieve a fair comparison of all approaches. An interesting experiment for future work would be to train on one of the two datasets and test on the other to see if the method is able to generalize better after robust training. That would show the benefits of the method in a more realistic scenario than adversarial  attacks. Another experiment for future work would be to test whether it generalizes well in larger datasets or if it is really tailored to smaller datasets and the multiple instances are not required when a larger dataset is present.	The technical novelty and impressive quantitative results make me to recommend acceptance.	The authors focus on the adversarial example robustness on small-size medical images, and there are some useful works. The authors present a method that is multi-instance robust self-training with a drop-max layer, which is simple yet maybe effective.	The paper is well-written and the results show a clear improvement between the baseline and the proposed approaches. Some aspects of the method like the dropmax layer heuristically perform well but it would be nice to better formulate the selection of the element used by the pooling.
320-Paper1972	Mixed Reality and Deep Learning for External Ventricular Drainage Placement: a Fast and Automatic Workflow for Emergency Treatments	The paper presents an augmented reality(AR) guidance system for ventriculostomy procedures. The system automatically registers the patient with a preoperative ct by aligning a model of the skin (segmented from CT) with the output of the depth sensor of the AR head-mounted display. Once registered, the segmented ventricules and sugical target (also derived from CT) are displayed in AR.	This study proposes a fully automatic MR and deep learning-based workflow to support emergency EVD placement. It provides a tool to automatically segment essential anatomies and register holograms on the patient's head via a marker-less approach.	A complete workflow for augmented-reality-guided external ventricular drain A deep-learning-based segmentation technique for ventricle segmentation	*The paper is very clear and well written *The validation method is sound for every aspect of the system presented *The validation procedure is run with a good number of surgeons	The main strength of the paper is the fully automatic MR and deep learning-based workflow, which can provide fast navigation for the emergency EVD placement.	A complete workflow is proposed Meaningful application in the clinic	*The paper is mostly an integration of existing work and presents very little novelty. *Comparison between guided and blind navigation may be partly unfair	The main weaknesses are: 1) the display of the MR visualization results is not smooth, with some display delay. 2) this study doesn't consider the brain shift during the EVD placement.	The novelty of the techniques involved is limited Only two anatomical models were used for evaluation, and can be limited in terms of anatomical variability	All the required methods are described in sufficient detail to reproduce the work, but no code or data is provided.	The reproducibility of the paper is good.	Sufficient details have been provided for replication, but due to data limitation, the difficulty can be high.	The paper presents a very nice integration of existing methods to solve a specific and important problem. It demonstrates the validity of the method in a rigorous manner. Two question remains: The comparison between blind and navigated catheter insertion may be unfair compared to a real surgical procedure as the surgeon is usually informed by the resistance of the anatomy to insertion of the catheter. In a real procedure, surgical draping may invalidate registration. The registration procedure presented cannot be performed after surgical draping.	The MR guidance is a good way to assist the surgeons to accurately perform the EVD placement. To achieve better results, I recommend the authors to 1)  improve the smoothness of the MR guidance. 2)  analyze the brain shift during the EVD placement.	The solutions introduced in the workflow lack of technical novelty As the point cloud-based registration relies on the facial features, registration validation should also consider points in regions that are not in the covered facial area but are often used in patient-image registration in the OR for fair assessment, such as the tragus and points on the head. No statistical tests were performed to confirm the results The manual EVD technique/protocol used to perform targeting as a comparison to the proposed method should be clearly elaborated. It is not clear if the participants are restricted to the holes as ROI for trajectory planning only. If yes, it is not the most ideal experimental condition. Although it is good to use the Foramen of Monro has a target for accuracy assessment, the EVD procedure does not necessarily need to target this point, but rather the catheter should in the ventricle at the level of the Foramen of Monro to ensure the pressure measure is valid. However, other protocols also exist depending on the position of the patient. Some discussion regarding the tolerance of accuracy will be appreciated for the particular application. The anatomical variability is lacking for the user tests, but it is key to demonstrate the robustness of the proposed method.	The paper is strong on all aspects but lacks novelty.	Brain shift is an essential issue during the EVD placement. The surgical tool interacts with the brain soft tissue, which will shift due to the external interaction. The target region may move to another position, which will significantly impact the accuracy of the surgery.	Good engineering solution, but lacks of novel technical development and validation is limited.
321-Paper0363	mmFormer: Multimodal Medical Transformer for Incomplete Multimodal Learning of Brain Tumor Segmentation	The paper has presented tranformer network for multimodal medical data like Brain MRI dataset. The presented approach is capable to handle incomplete information in the dataset. The same is validated with the experimental results with BRATS dataset.	The authors propose a hybrid network that combines CNNs and transformers for segmentation of brain tumors from multimodal MRI inputs with missing sequences. The network is well designed, the authors motivate the need and provide a good description of the various modules. The network was trained and evaluated on a standard dataset enabling easy comparison with SOTA models. However some implementation details are lacking. Model size, training duration and inference speed are not provided. Dice coefficient was the only metric used for evaluation. Though the splits similar to that in ref 21 were used, comparison with the results in [21] is not provided.	In this paper, authors exploit Transformer, named Multimodal Medical Transformer (mmFormer), to build a unified model for incomplete multimodal learning of brain tumor segmentation. Experimental results demonstrate the effectiveness and robustness of the proposed method. The paper is good, and the figures are clearly drawn.	The paper is well written and results are properly presented. The manuscript is focused on dealing with incomplete data which is a general issue in most of the medical datasets. The mmFormer architecture is presented well to discuss each individual module in it.	The model developed here addresses an important clinical need as in many cases a complete set of all 4 MRI sequences are not acquired. The network combines the extracted features per input MRI sequence using an elegant modality correlated encoder. Forcing the network to learn meaningful representations from individual encoder for a specific MRI sequence using the auxiliary regularizer is novel.	The paper is of certain novelty to apply transformer as a new tenchinique in solving incomplete modality segmentation. It has good knowledge of background about multimodal brain tumor segmentation.	"The results are shown on BraTS 2018 dataset, where as BraTS 2021 is also available now. The experiments can also be extended. There should be a subsection to discuss complexity of the proposed mmFormer architecture in reference to other methods. The last line in conclusion section seems to be is contradictory. Need a rewriting. (""Our method gain more improvements when more modalities are missing...""). However, from Table 1, results are not best when there is only one modality is present and three are missing."	No model size / number of parameters is provided and comparison with the Adversarial Co-training Network in ref [21] is not provided. Though ablation studies were performed to understand the contribution of various components of the model, it is hard to discern how modeling the long range interactions with transformer modules is improving the segmentation (does it help with better segmentation of larger lesions and/or does it have any effect on smaller lesions?)	"The motivation is clear and the significance is strong both in clinical and pure research. However, it is difficult to know why transformer is chosen. As a popular work, transformer is well used in several medical image processing tasks. From the sentence that ""the dedicated Transformer for multimodal modeling of brain tumor segmentation has not been carefully tapped yet, letting alone the incomplete multimodal segmentation."", it seems like that the work is proposed in order to apply a new method on a specific task instead of that the task needs the method to get improvement. Of course, it is easy to understand we try many methods and find one is good. But why it is good and why you choose it should be clearly presented in the submitted paper. The related work should be carefully abstracted. Compared with brain tumor segmentation, incomplete modality of brain tumor segmentation is a specific field, in which there is not that large number of papers. You may categorize and comment methods following their main ideas and talk about the pros and cons class by class. Why do the U-HeMIS and U-HVED are chosen as your benchmark? Is it because they are all latent space based model?"	The method proposed is reproducible.	Certain implementation details are missing: Number of initial or filters per level for the CNN encoders patch size for the intra-modal transformers is not provided / were the inner-most features of the CNN encoder just flattened? what features from the encoder are forwarded as skip features? If it is the features from CNN encoders at specific levels, how are they combined across modalities? from Fig.1, it appears as if the intra and inter-modal transformers are used only at the bridge/innermost level. If so, what is used as skip features at every level in the decoder?	reproducible if hyperparameters are all provided, however not	The paper is well written and explained. The results should be presented to BraTS 2021 dataset or similar other dataset in the extended version of the work. There should be a subsection to discuss the complexity of the proposed architecture in comparison to SOTA.	There is some redundancy in the text in the last paragraph of introduction and beginning of the methods section. This could be shortened to provide more / clarify the implementation details. Suggest writing the loss terms in terms of outputs from shared-weight decoder and convolution decoder with outputs at every level in Equation [9]. If the first term is the summation of losses from modality specific encoders + shared-weight decoder, does it imply that loss will be greater when more input modalities are available? Please provide model size in terms of model parameters, memory requirements and training duration for 1000 epochs. From the description in experiments and results section: 'For a fair comparison, we use the same data split in [21] and directly reference the results', but comparison with ACN model in [21] is not provided in Table 1. Please update Table 1 with missing information. For Fig.2, suggest to provide all input MRI sequences to help see what lesion information is available in each input modality and the lesion information could be provided as magnified insets. There is less tumor heterogeneity in the example shown, if space permits please include another example with tumor heterogeneity. Similar to Table 1, please update Table 2 with results from ACN in ref [21]. It is hard to interpret results in Table 3. The drop/improvement in segmentation performance for a specific tumor type could be explained by how much of a shared representation is present across modalities and how well the model is able to capture this shared information; and better capturing long range interactions with transformer modules. Please try to dissociate these factors for the ablation studies in Table 3. Maybe, use the ablation experiments with all input modalities to understand the contribution of transformers for modeling long range interactions and for missing modalities, group them with number of missing input modalities. This would be representing the data in Table 3 differently, as you already have the results for various models. This might shed more light on why you see more improvement for enhancing tumor even though its size is usually smaller than the core and whole tumor. It might be that most of the improvement could be driven by better learning of shared information than modeling long range interactions with transformers (probably not discernable from your current results, this would require a configuration without the transformers). The sample size of 285 is small for large models. Following similar training and validation folds as SOTA models enables fair comparison. Suggest considering CV-splits on a few subsets of input configurations to understand the variability in model generalization. [21]  Wang, Y., Zhang, Y., Liu, Y., Lin, Z., Tian, J., Zhong, C., Shi, Z., Fan, J., He, Z.: ACN: Adversarial co-training network for brain tumor segmentation with missing modalities. In: International Conference on Medical Image Computing and Computer Assisted Intervention. pp. 410-420. Springer (2021)	Basically speaking, I understand that you try a new way to solve the problem, although I don't know why you use it. If solid reasons are provided, it would be nice. Besides, the validation is limited, maybe due to the paper length limitation. There are several algorithms on this topic, more comparison would be  persuasive.	In general, medical data lacks in information in form of modalities and this is current area of research. The paper presented a new mmFormer architecture which can tackle this issue well.	Absence of implementation details and comparison with ACN in ref[21]. Looking at table 1 in ref [21], the best performance using the mmFormer (77.61, 85.78, 89.64) is marginally better than ACN (77.46, 85.18, 89.22). However, the average performance across the 15 models is slightly better for ACN model (61.21, 77.62, 85.92) compared with mmFormer (59.85, 72.97, 82.94). It would be nice to have justification in terms of model size / usability / generalizability. Not sure if comparison with Table 1 in ref [21] is valid as it is unclear if the splits in ACN were saved and used in this work or if another random splitting was performed!	written. The motivation to use transformer should be cleared presented.
322-Paper0649	Modality-adaptive Feature Interaction for Brain Tumor Segmentation with Missing Modalities	The authors address the brain tumor segmentation with missing modalities by modeling feature interaction among modalities. To learn and interact complementary features between modalities, graph structure and attention mechanism are used.	This paper proposed a modality adaptive feature interaction (MFI) with multi-modality code to adaptively interact features among modalities in different modality missing situations. The proposed MFI was incorporated with U-Net for segmenting brain tumors. Experimental results had compared with other state-of-the-art brain tumor segmentation methods and achieved superior segmentation performance.	The authors propose a modality adaptive feature interaction network (Net-MFI) based on graph structure for brain tumor segmentation with missing modalities. Compared to other approaches, Net-MFI focuses on learning the complementary information using an attention mechanism for adaptively missing modalities. Validation on the BraTS 2018 shows that Net-MFI enhances the tumor segmentation in different missing modalities situations outperforming other existing methods.	1) The problem addressed in this paper is important. 2) The authors address the brain tumor segmentation with missing modalities by  introducing Modalityadaptive Feature Interaction (MFI) with multi-modal code.  3) The method has novelty, although the novelty is not significant.  4) The validation results show the improved peformance.	1) To adaptively learn the complementary features among modalities (i.e., graph nodes), this paper introduces a multi-modal code to represent if different modalities are observed or not, to guide the learning process. Introducing MFI guided by multi-modal code into the different stages of a U-shaped architecture makes the multi-modal features hierarchically interact. 2) The experimental results demonstrated the positive effect of the proposed MFI.	1- Simplicity: The proposed MFI is a simple yet effective approach for tumor segmentation with missing modalities. 2- The paper was validated on the BraTS 2018 dataset achieving the state-of-the-art results in incomplete mutli-modal brain tumor segmentation.	It seems that the comparsion in Table 1 is not under the same cross-validation slpit, which may result in unfair comparision. The results of some methods are directly extracted from their papers. Significance test is missing. Since the proposed method has small improvement in comparision with RFNet, significance test is recomended. Disccusion about the limitations of this method is missing.	1) As the authors mentioned in Graph representation G=(V, E) in which E denotes the adjacency edge matrix representing the relations between nodes (modalities), the r_{ij} is finally computed using the formula (2) which is the output of Leaky Rectified Linear Unit by inputting the concatenation of voxels of v_i and v_j, why the authors defined the edge in such form? What's the main purpose to define the edge in such a form? What are the advantages? All of these are not described in the paper and the relative experiments are not conducted. 2) The experimental results showed that the highest accuracy is obtained by using all of these four modalities images including F, T1, T1c, and T2, which suggest that the proposed MFI module seems to haven't much better effect. 3) In Fig. 1, the GN hasn't a description.	Limited experiments: The proposed method was only applied to the BraTS 2018 dataset which is not the latest BraTS dataset.	"The authors listed ""yes"" for both code and pre-trained models. In this case, it can be an easy task for both training and testing. If the reproduction was only based on the descriptions in the paper, it could be somewhat difficult."	acceptable	The paper meets the standard requirement in terms of reproducibility.	Significance test is needed. Since the proposed method has small improvement in comparision with RFNet, significance test is recomended. Disccusion about the limitations of this method is needed. Means-> Mean  in Table 1	1) The GN illustrated in Fig 1 should be detailed in this paper. 2) The motivations and the advantages of the computational method of edge shown in Formula (2) should be detailed, and some additional experiments should be conducted to demonstrate the advantages of this computational method.	"Recently, Wang et al. proposed ""ACN: Adversarial Co-training Network for Brain Tumor Segmentation with Missing Modalities. MICCAI 2021"". Similarly, Azad et al. has published ""SMU-Net: Style matching U-Net for brain tumor segmentation with missing modalities. MIDL 2022."" The authors should compare the proposed method with these previous works to demonstrate the main difference. Net-MFI is applied with multi-modal code to the BraTS 2018 dataset which is not the latest BraTS dataset. The authors should apply to recent BraTS datasets such as BraTS 2020 and BraTS 2021."	The method has fair novelty with improved performance. The paper is generally well orgnized and clear. However, the validation is not solid.  It seems that the comparsion in Table 1 is not under the same cross-validation slpit, which may result in unfair comparision.  Significance test is missing. Since the proposed method has small improvement in comparision with RFNet, significance test is recomended.	The method proposed in this paper is more intuitive, the experiments are relatively sufficient.	Overall, the paper is very interesting, and the method shows great potential.
323-Paper1690	ModDrop++: A Dynamic Filter Network with Intra-subject Co-training for Multiple Sclerosis Lesion Segmentation with Missing Modalities	This paper demonstrates a deep-learning based dynamic filter network for Multiple Sclerosis (MS) lesion segmentation. The authors proposes dynamic head with filter scaling and intra-subject co-training for the scenario that some modalities might be unavailable during training and testing in the clinical practice. The proposed method can be adapted to any arbitary number of MRI input modalities for automatic MS lesion segmentation task.	The authors introduce a novel framework named ModDrop++ to train a segmentation netowork with missing input MRI sequences. This approach can easily be integrated in any existing convolutional neural network, and, compared to the state-of-the-art method ModDrop, shows improved performance for multiple sclerosis lesion segmentation on two publicly available datasets.	The paper proposes an improvisation to the Modality Dropout (ModDrop)  technique by adapting that the training technique for multimodal image segmentation and adding dynamic filter convolutional layer (head) coupled with a modality-specific weighting strategy and further incorporating intra-subject co-training  (fill modality vs missing modality) .	This paper presents a solution to the clinical practice where one or more MRI input sequences are absence during training and inference phase for a deep-learning method. Specifically designed dynamic head to scale the input and th intra-subject co-training to enhance the learning ability of network to learn similar features for different combinations of the input sequences. Extensive experiment of all possible input scenarios were conducted. The demonstration of the propsoed method and paper writing is clear.	-The proposed approach is designed to be a plug-and-play method that can be easily integrated in any existing segmentation CNN. -For the first time, dynamic filters are applied to the missing modality problem, showing interesting performance.  -A novel strategy for intra-subjects co-trianing is proposed to leverage the intra-subject relation between the full and missing-modality data. Ablation studies are performed showing that this further improved the segmentation performance. -Missing modalities are a persistend problem in MS imaging and, if confirmed by further studies, this method could have a significant impact on lesion segmentation automated tools. -The manuscript is well written and straightforward to understand.	novel application of dynamic filter network and modality weighting strategy to the ModDrop method of multimodal training novel application of co-training to improve segmentation results What I like about this approach is the way the fillter scaling matrices  adaptively adjusts to missing modality.  This is a much better way to handle missing modality than the original ModDrop approach that learns a fixed set of filter coefficients for all possible cases of missing modalities.	The design of the dynamic head is trivial and lacks proof, the filter scaling matrix is just served like a normal linear layer without much more modifications. Thus it is hard to justify the usefulness of such a matrix to scale the features extracted with missing modalities during training and inference phase. The loss used for features extracted after dynamic head are treated equally, however, the features extracted by more available modalities should have more confidence. The use of SSIM loss seems not appropriate for features in latent space, why not using the perceptual loss? Also, the reason to use fixed layers after feature extraction was not justified. The experimental result further proves my opinion where the performance of the proposed method dropped significantly when FLAIR is absent from the training and inference. While the performance was comparable to full modality setting when only FLAIR is available. This observation only proved that FLAIR is crucial for MS lesion segmentation tasks but not the effectiveness of the proposed method. It is actually true that FLAIR is important for lesion identification tasks in real clinical settings and the proposed method was not able to gain performance improvement. Furthermore, even Contrast-Enhanced (CE) sequence was provided in the dataset, I don't think this sequence can be used in this setting since CE is used to detect the active lesions. For the general lesion segmentation task, the information provided by CE on a MS lesion can be different (bright for active lesion and dark for MS lesion), thus including this sequence may lead to a negative performance gain (as shown in Table 1). The results of ISBI dataset showed no significant difference in performance between full and semi modality. The discussions on this observation and performance differences between two datasets were not found.	-The evaluation of the proposed method is somehow weak. Two publicly available datasets are considered, but the ISBI one includes only 5 patients. Throughout the manuscript there is no mention of the validation dataset. The core objective of this work is to increase the generalizability of MS lesion segmentation approaches. Thus, the two datasets considered could have been pooled together to examine the effects of the proposed framework.  -The lesion deliniation heavily depends on the sequences analyzed by the experts while performing the manual lesion annotation. Very often the only sequence used for MS white matter lesions is the FLAIR, and thus results of the automated approaches are considerably worse when FLAIR is a missing modality. This should at least be discussed in the manuscript as it is quite evident from Table 1.	I didn't notice any major weakness in the paper. The paper is well written and references have been made to relevant prior work.	The authors provided sufficient information to reproduce the propsoed method.	The code and data are publicy available.	Public datasets have been used, but no code repository have been shared. I encourage authors to publicly share the implementation as I feel this work could have noticeable impact in the field.	The authors defined a good problem to be solved based on the real clinical setting, however, the proposed solution was not good enough to address such a problem. Authors should consider a more convincing approach on feature scaling from missing modality setting to full modality. Features from the dynamic head can be treated differently based on their confidence. The experiments should be designed more carefully, since the problem was inspired from a real clinical perspective, the design of the experiment should follow this. The inclusion of the modalities involved in the experiment should be re-considered and discussed.	-The authors divide the ISBI dataset with a 4:1 ratio for training and testing, meaning that a single patient was kept for testing (5 patients in total). This makes the results obtained much less meaningful and should at least be acknowledged as a limitation. -No indication of how the UMCL dataset is split into training and testing is given. -A previous MICCAi paper from 2020 employed ModDrop in the context of MS lesion segmentation and should be cited as well: 10.1007/978-3-030-59719-1_57 -The validation set is not mentioned for both datasets, how is the binary threshold optimized? -Regarding the SSIM loss, was the windows size of 11x11 voxels chosen empirically or optimized somehow? Similarly the values of a, b and g in the overall loss function could be optimized.  -Fig 2. The choice of the colors could be improved (eg. swapping FP and TP, as green is commonly seen as a positive outcome). -The Discussion section is lacking the limitations of this study and future research directions.	Fig 2: The figure should be enlarged as its difficult to see the results - could be presented in the Supplement as a full page or half page Figure. Also, the description/interpretation of the results - along with the GT should be improved. It is unclear what the sub-image on the top right of each results image represent. Please clarify	This paper tried to address a realistic problem in the clinical settings for MS lesion segmentation, however, the solution proposed was trivial and lacked justification. In addition, the experiments were extensive but not well-designed. The information shown by the experimental results did not support the effectiveness of the proposed method.	The proposed framework shows novelty and could be of interest to researchers in the field. Its evaluation could be improved to strengthen the conclusions.	This paper proposes interesting improvements for the problem of missing modalities which is a real problem in medical imaging.
324-Paper1665	Modelling Cycles in Brain Networks with the Hodge Laplacian	This paper proposes a novel method to identify 1-cycles in brain networks. The algorithm starts with decomposing the network into MST and non-MST. Then a 1-skeleton with only one cycle is composed by adding one edge from the non-MST into MST. The unique 1-cycle can be identified by the zero eigenvalue of the Hodge Laplacian. The number of 1-cycles is the same as the non-MST edge number. The authors then propose that these 1-cycles forms a basis system over the collection of all possible 1-cycles, and thus can be used to discriminate networks with different topology. The authors validate the proposed method on simulated networks, against geometric measures 1-norm, 2-norm, infinity-norm and Gromov-Hausdorff distance. Contribution: The authors provide a new mathematical framework to extract cycles using the Hodge Laplacian over simplicial complexes.	The quantitative analysis of structural and functional brain connectivity by network (graph-based) measures helped to understand their complex properties. This submission proposes to extend current methods by including loops in the analytic assessment of networks. Authors introduce the Hodge Laplacian as a generalization of the graph Laplacian in order to identify and quantify 1-cycles (loops) in functional connectivity. Functional MRI data acquired in the resting state were used here, from a large, publicly available data base.	In this paper, the authors propose a 1-cycle basis in brain network analysis of fMRI-based graphs. The 1-cycle basis is extracted based on Hodge Laplacian using persistent homology framework. The authors first prove (Theorem 1) that the collection of 1-cycles (the maximum homology group for a graph is 1) spans the kernel L1 or the 1-th Hodge Laplacian of the graph. Then using this theorem, they extract the coefficients of 1-cycles across all graphs in the between-subject study. In the end, they also introduce an alternative to GH and other distance measures to compute the difference between brain networks, named statistics T. To validate the efficacy, they analyze the common 1-cycles between females and males in the brain network of normal subjects.	The authors propose a novel algebraic method to identify and represent 1-cycles in a weighted graph. The simulation experiment shows high accuracy in discriminating networks with different topology.	The topic of this submission is within the focus of MICCAI, and of potential interest to a broader audience. The text is understandable for a reader with  knowledge in network methods for modeling structural and functional data. Interesting on-going work is presented here.	The paper is well-written and addresses all the necessary theoretical aspects, both Hodge Laplacian theory and persistent homology. Their novel 1-cycle basis helps to extract brain multi-way interaction between regions as opposed to conventional connection-based analysis. This is even more fruitful for analysis of DTI based networks which are more sparse and can be more discriminative in normal-patients comparison.	** Although we appreciate the algebraic methodology proposed by the authors, the motivation of the proposed method is not clear. Computationally, simply using the non-MST edge and the MST will give you a cycle. This way of computing a cycle basis with a MST is very efficiently. It is not clear why we need the Hodge Laplacian and its eigen decomposition. Plus recomputing the Laplacians and their eigen vectors for each non-MST edge will be extremely expensive. ** There are already methods like persistent homology to differentiate networks with different topology. What is the benefit of the proposed method over persistent homology? Some empirical comparison with persistent homology features might need to be provided to prove the proposed method is better and necessary. ** More details in the validation experiment need to be provided. In equation (6), the authors define the difference between two groups with the same nodes. However, they do not mention how to calculate the difference between two different networks, like the difference between group 1 and group 3 in Fig.2. In such case, how to match the nodes between two groups is not trivial, even when the two groups have the same number of nodes. ** There are no baselines in the application experiment. Although we can see the proposed method can differentiate the male and female brain network successfully, the authors did not provide any baseline to compare with. Such baselines can include the other metrics used in validation and also methods like persistent homology. For persistent homology, here is a very relevant work that can be considered to be a baseline, T. Songdechakraiwut, L. Shen, and M.K. Chung. Topological learning and its application to multimodal brain network integration. Medical Image Computing and Computer Assisted Intervention (MICCAI), 12902:166-176, 2021.	Unfortunately, one potential major error was found (#1).	The authors could have better demonstrated the motivation for cycles in the brain network by providing more evidence in the neuroscience literature. Although this feature can be discriminative, the intuition behind finding some cycles with carriable length may not be very clear from a medical analysis standpoint. Also, there could have been a comparison to 0-cycles features based on the same approach to see the significance of 1-cyles are more important and meaningful.	The description seems reasonably clear. But I believe the code needs to be publicly available to ensure full reproducibility.	There are two questions about the validity of the statistical analysis (see item 8).	Good.	See above.	"p.6, Table 1: It is understood that p-values are shown here, and it is guessed that the \pm refers to a standard deviation, likely, from repeating the simulation. If this guess is correct, than, well, contents of this table are incorrect. It is not viable to perform this kind of arithmetic on p-values. Even more, p-values are not Gaussian distributed. Please, revise. Suppose Table 1 was corrected. Please, describe exactly how you demonstrated that ""the proposed method ... outperformed all (other) measures."" How do you compare two (or more) sets of p-values?"	"Table 1 in the paper is not well tabulated as there are no bold numbers to see the difference. The fact that just positive connections are chosen to construct the final network should be explained more, as those connections are important and some studies may use the absolute value.  There is a paper related to the cycles using the notion of persistent homology which is not mentioned in this paper as a related paper in medical brain network analysis "" A Univariate Persistent Brain Network Feature Based on the Aggregated Cost of Cycles from the Nested Filtration Networks""."	The authors do propose some interesting novel ideas. But the motivation is not completely clear at this moment. Also comparing to other topology-based baselines is necessary. I will be happy to improve my scores if the concerns are addressed.	This work presents a novel and useful extension of a relevant analytical method in medical image analysis.	This work proposes a novel theoretically sound approach to identifying 1-cycles in the brain network as a strong feature. This novel approach is proven based on an important theorem in the paper. The synthetic data and real-world data suggest the efficacy of the proposed method to extract 1-cyles in the brain network. This idea can be extended to any graph semantic graph analysis beyond medical image analysis. The paper is also clearly well-written and cohesive in all parts.
325-Paper0964	Momentum Contrastive Voxel-wise Representation Learning for Semi-supervised Volumetric Medical Image Segmentation	This paper introduces an approach to learn semi-supervised 3D medical image segmentation networks by leveraging four key objectives - (1) a simple voxel-wise contrastive learning objective against an EMA target network contrasting along the feature dimension, (2) a dimensional contrastive objective that contrasts along the batch-dimension, (3) a consistency loss which encourages to directly match the output of the EMA target, and a (4) supervised objective built around a cross-entropy and dice loss.	This paper proposes a contrastive semi-supervised learning scheme for 3D image segmentation. The contrastive objective is taken along the feature and the batch dimension, and the optimization is performed at low-level and high-level. The method is evaluated on a dataset for atrial segmentation, and a dataset for pancreas segmentation. The results for comparison to state-of-the art and for an ablation study are shown.	The contributions of this paper is to use a voxel-wise contrastive learning approach that leverage the contrastive loss in both the bottleneck feature space and in the segmentation space.	While the novelty of each component is limited, the combined 3D semi-supervised segmentation network learning pipeline is, to the best of my knowledge, novel. In particular however, the main selling point of this work is the final semi-supervised segmentation performance achieved by the proposed method, beating out competing methods by a notable margin especially in the low-supervision regime.	The method description is very detailed and clear. Over all, the paper is easy to follow. The evaluation seems to be complete, and the ablation study provides a good validation of the effectiveness of the different loss functions. Grid search for hyperparameter tuning is also performed. According to the results, the proposed method provides a good solution for 3D semi-supervised segmentation if only very few labeled images are available.	The strength of this paper is to adapt the contrastive learning idea in 3 dimensional-space and use the unlabeled dataset with dimensional-wise contrastive objective as semi-supervised setting.	"My primary issue with this paper is the structure and writing, which make it very hard to exactly parse how each of the proposed objectives operate, with confusing or in parts missing notations and definition. In particular, No additional information on the supervised loss is given, only that it comprises a cross-entropy and dice loss. Are they weighted? Is it a smoothed dice loss variants? Are any additional hyperparameters introduced here? Throughout the work, the EMA network is treated as a separate entity and effectively independent teacher network (see e.g. Fig. 1, and separate student/teacher notations throughout the text). In general, it is incredibly hard to understand where and how each objective is applied in the overall quite expansive pipeline setup - the notation of high- and low-level contrastive loss is introduced in the beginning of section 1.1 and 1.2, but with hardly any motivation. If I understand correctly, low- and high-level refer to applied in the latent spaces of the first encoder as well as the last encoder, which however is described as being of ""similar architecture"". What then makes the contrastive losses high- and low-level? And since parts of Fig. 1 and Fig. 2 are never specifically referenced in the paper it becomes quite hard to understand where what is applied. Similarly, equations 1-3 would benefit from more indices to much more precisely highlight which components are contrasted against which. As it is not made clear throughout the paper - are stop-gradient operations applied anywhere? Or does backpropagation also happen into the EMA teacher network? In addition, it is not entirely clear what the main novel contribution is - MT[21] introduces output matching to an EMA target, component (de-)correlation along the batch axis has been introduced e.g. in Barlow Twins (although with different formulation) and the components in the supervised objective are commonly used in literature. And while the authors claim their regularization to be ""anatomy""-informed, it is not entirely clear how this is reflect in the objective? Is it the fact that the volume cube is broken down into voxels? There is also no experimental support for the claims that CVRL makes the proposed setup less prone to dimensional collapse - just looking at the baseline Dice/Jaccard performances, while it is overall worse, there is no indication that each pipeline component does what the authors claim it should be doing. Some smaller issues: Fig. 3 is incredibly cherrypicked and makes CVRL stand out disproportionally. Table 2 techincally misses L^low + L^con and L^high + L^con references. Why where they not included?"	"On page 8, it is unclear what is meant by ""efficacy of both inter-instance and intra-instance constraints"". Both terms were mentioned before, but what do they mean? Please describe this. In Figure 3, what do the red and blue lines stand for. In Figure 2, the color coding (yellow, red, green and blue) is not clear. Describe what is meant by the hardness-aware property. While it is stated that the hardness-aware property is inherited, it is unclear what advantages this provides and why. -In table A2 in the appendix, I guess these are the results for tha LA dataset. This must be written in the caption. -In the appendix, I would make two seperate sections: Appendix A for additional results, and Appendix B for the proof of the hardness-aware property."	"The main weakness is having similar idea that existed for pixel-level contrastive learning with memory bank [1]. Figure 1. specifies the low-level contrastive loss and high-level contrastive loss. However, it is hard to follow the loss defined. Another weaknesses is lacking of contrastive learning baselines and citations for segmentation task. [1] Alonso, Inigo, et al. ""Semi-supervised semantic segmentation with pixel-level contrastive learning from a class-wise memory bank."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021."	While the majority of hyperparameters and pipeline settings are listed in the experimental section, the limited clarity of the paper make reproduction harder then it has to be.	The method and architecture is well described. I trust that the code will be publicly available, as the authors wrote in the abstract.	The reproducibility of the paper is good.	See Section 5 - in general, the results are really convincing. If the method details and paper are made clearer and the main novel contributions are carved out better, the overall quality would significantly increase.	"Please consider all points listed under ""weaknesses""."	"In the experimental perspective, I would recommend to add more contrastive learning state-of-the-arts and cite more contrastive learning methods for segmentation performance, such as: [1] Wang, Wenguan, et al. ""Exploring cross-image pixel contrast for semantic segmentation."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021. [2] Hu, Xinrong, et al. ""Semi-supervised contrastive learning for label-efficient medical image segmentation."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021. [3] Lee, Ho Hin, et al. ""Semantic-Aware Contrastive Learning for Multi-object Medical Image Segmentation."" arXiv preprint arXiv:2106.01596 (2021). I also have a concern on the innovation. According to the Figure 1., only unlabeled dataset is only contributed to the high-level contrastive loss, is it possible to also use the labeled dataset to calculate high-level contrastive loss? For the contrastive loss section, is the positive pair defined voxel-by-voxel? What is the benefits of using your proposed contrastive loss to the pixel-wise contrastive loss? It will be great to add experiments and provide more clarity on your innovations."	Given the results and the general pipeline making overall sense, I would still opt for acceptance, but strongly urge the authors to expand the method section with more details.	The method improves state-of-the-art approaches in cases where only very few labeled images are available. This is a likely scenario in real-world applications. The evaluation of the method is elaborate, and well described. The ablation study covers all building blocks of the method. Overall, the paper is well organized and easy to follow.	The lack of contrastive learning baselines and enough citations for contrastive learning methods to demonstrate the confidence of the proposed idea
326-Paper2100	Morphology-Aware Interactive Keypoint Estimation	An interactive X-ray image keypoint estimation method is presented in this submission. The proposed approach aims to reduce the manual correction cost, instead of fixing each of the wrongly predicted keypoint, a user only needs to correct one point, all other keypoints would be updated as the user's modification to one keypoint is propagated to other points. The proposed approach is evaluated on multiple datasets (AASCE and others).	The herein paper presents an X-ray landmark detection algorithm with the possibility of interactive corrections that can be made by the end user. Taking into account the morphological information of the anatomy, the revised landmark detection adheres to the structural constraints of the desired object (e.g., cervical spine). Although interactive segmentation networks have been previously proposed, to the best of my knowledge this algorithm is one of the first efforts on the use of artificial intelligence for interactive landmark detection in X-ray imagery. Although not directly translatable, a comparison to the state-of-the art methods for interactive segmentation algorithms is also provided.	"The authors present an approach for interactive landmark estimation refinement. They utilize iterative user input similar to attention weights resp. as gating mechanism for the main network and additional regularize the prediction of the network using a ""morphology-based loss"" that is derived from dataset statistics. The authors evaluate their approach on one public and one private data set with synthetic user interaction and demonstrate improvements compared to three (+1) frameworks originally developed for interactive segmentation."	This submission tackles the problem of automatic keypoint estimation correction in X-ray image, where manual correction of multiple keypoints could be time-consuming and inefficient. Brute force approach is to correct each keypoint independently, the proposed method aims to reduce user interaction by introducing an interaction-guided gating network, to propagate the user input across the image. A morphology-aware loss is proposed based on the observation that the degree of freedom between the keypoints is small in X-ray vertebra images, which regularizes the network to learn the inter-keypoint relationship to be similar to that of the ground truth.	The underlying assumption about keeping the clinician in the loop of landmark identification is very reasonable given that the presented network can fully benefit from the contextual knowledge of the clinicians in revising its predicted landmarks. The implemented network architecture (despite being already published as an RITM network) along with the morphology-aware loss seem appropriate choices for the application at hand. The provided evaluation study can support the paper's conclusions by providing a comparison to the state-of-the-art interactive segmentation methods.	The authors present an interesting interactive strategy for landmark detection with convincing improvements compared to other approaches. The authors evaluate their strategy against three other approaches. The method is comparatively straight forward and modular with an interesting combination of attention-based interaction and additional regularization based on expected morphology.	Part of the explanation and discussion in the experiments section is confusing. The definition of manual revision in Fig. 4 is not clear, the user interaction is also a type of manual revision. The performance of the proposed approach is close to the general method RITM [16]. As shown in Fig. 4, the MRE for increasing the number of user interactions, the curve of RITM is similar to the one of the proposed method. It is the same in the right figure as well in Fig. 4. Considering the proposed method is tailored for X-ray vertebra images, large improvement is expected but not observed. It is shown that the proposed method can correct most of the wrong keypoints based on the input where a user corrects only one point. However, which keypoint should be selected and corrected by the user to achieve such an efficient user correction is unclear. The limitations of the proposed approach and the directions for future research are not discussed in this submission.	In general, the amount of detail presented for this algorithm is not consistent throughout the paper. While some components of the network are explained in high detail, crucial information about the network architecture and the training process are completely missing. For instance, previous prediction of the network appears to be a separate input channel to the network while it's not clear how this is imbedded inside the training workflow. Furthermore, aspects related to the gating network and the morphology are loss are not well communicated therefore one may find it difficult to fully understand the underlying methodology behind those network components. Additionally, the writing tone and clarity can be improved to help with the general comprehension of the paper. For instance, the authors' statement about the NoC_5@3 and FR_5@3 is extremely hard to grasp due to subpar writing quality.	"There are some issues in the mathematical & formal description of the method, which I would encourage the authors to revise (see below). The concept of ""low-variance""/""high-variance"" landmarks is only described superficially (e.g., was there a specific threshold selected? how?). The authors do not evaluate nor discuss ""noisy"" interactive inputs (or repetitive corrections of the same point) by real users, but only work with ""simulated"" interactions (based on the ground truth). Only mean results are reported, without variance and / or effect of repeated training. Failure cases are not analysed further."	The authors include code with detailed readme as part of their supplementary materials. Demo video is provided as well to demonstrate the user interaction process.	As supplementary information, the authors have provided access to the code and an illustrative video showcasing the performance of the algorithms on their test dataset. Within the paper itself however, no reference to the implementation details including the utilized libraries, programing language and the network hyperparameters are provided.	The paper is partially reproducible. The authors aim to provide the code (training and evaluation) as well as the pretrained models, which means that it should be possible to reproduce the results on the smaller, publicly available data set. The in-house data set will not be made publicly available.	This submission tackles the problem of fixing the errors in automatic keypoint estimation from X-ray images for vertebrae applications, which can be improved from the following perspectives: Revise the explanation and discussion in the experiment section, especially the part for Fig. 4. Discuss why the proposed method is close to RITM in MRE for increasing the number of user interactions. Discuss how to select the one keypoint to correct so that the information can be used to correct all other keypoints, does it matter which keypoint is selected? Discuss the limitations and directions for future research.	In general, the writing tone and style can be revised to help with understandability of the implemented methods. Despite the fact the backbone network architecture is borrowed from an existing method, the authors should better explain the general information flow inside the network and the associated training process. Aspects related to the gating network and the morphology aware loss were hard to follow given the lack of proper explanations regarding the underlying requirements of those components and their added benefit. Although appreciating the effort in comparing the developed landmark detection algorithm to the existing interactive segmentation networks and despite the fact that the authors have performed this comparison on the basis of heatmaps (for consistency), one may argue that this is not an adequate evaluation given that those methods were inherently developed for completely different applications.	"General comments: Introduction: One clear example of where key points are used clinically (ideally with reference to the tasks evaluated) would be appreciated - the motivation is currently very general and superficial Since the authors work on X-ray projections, I would encourage them to include further citations citation for medical images/X-rays: Bier et al., MICCAI 2018, https://doi.org/10.1007/978-3-030-00937-3_7; Kordon et al., MICCAI 2019, https://doi.org/10.1007/978-3-030-32226-7_69 p. 2: ""Also, RITM ... "" - The authors may want to summarize the main idea behind RITM shortly (and introduce the abbreviation), furthermore, the terms ""HRNet-W32"" or ""hint fusion layer"" mentioned later in the text may not be clear to a majority of readers and could be shortly explained. Method: p. 4: The formalization of Eq. (1) doesn't seem to fully hit the nail on the head, and should be revised. The line above the equation kind of predefines n as a point with a user interaction. In the equation itself, there is then an additional condition, i.e., n \in {l_1, l_2, ...}. From my perspective, the subset of ""interaction landmarks"" should be more clearly defined (as it is defined now, it may refer to any landmarks...). Potentially, the authors want to define a subset of adapted landmarks L_adapted \subset L_all. In the same equation, I would expect to see the position described by the ""user interaction"" itself, not the ground truth position. This can be synthesized by employing the ground truth... Using a cross-entropy loss for heatmap regression (alone) strikes me as relatively unusual - did the authors also experiment with L2-loss/L1-loss? ""We apply the global pooling method on the feature maps to aggregate the most activated signal per channel; the resulting vector is in Rdw. Specifically, we adopt the global max pooling layer, which selectively retrieves the important interaction-aware features for each channel. "" - this is phrased rather complicated, wouldn't it suffice to formulate something along the lines of: ""We use channel-wise global max-pooling to obtain channel-wise activations. These are further processed by two fully connected layers (I presume - from the equation) and a sigmoid activation function to form per-channel gating weights for the main network."" (Eq. 2 doesn't really add more clarity to the paper) p.5: The authors state that a subset of landmarks were selected to add the morphology-based loss. How where these landmarks selected? Was there some threshold of variance? This should be explained at least shortly and is currently rather vague. It may not be clear to every reader that the soft-argmax function is an essential ingredient to get the morphology-based loss working. I would encourage the authors to make it more clear how the heatmaps can be used to derive a morphology-based loss. p. 6: The authors should clarify whether/that a patient-wise split was applied to the data Evaluation & Discussion: There is no guarantee that an annotated point will actually end up where the user wants it to be, correct? Were there any border cases observed? This should be shortly discussed. ""when comparing the performance of Vertebra-focussed and each model ..."" - not clear, there also seem to be some repetitions in the results section. I would encourage the authors to double-check this section again. I don't quite understand Fig. 4: What is the difference between the methods with and without manual revision? Does this just mean that one landmark is moved? If this is the case: Couldn't this also be reported for the ""Vertebra-focussed model""? Fig. 5 is not fully clear - what are the values mentioned below the images (Initial, After, delta)? Why is there gain for one image but ""delta"" for all others? The authors only report the mean error, here, a more detailed analysis of at least variance across images / repeated training etc. would be expected. Additionally, an analysis of failure cases would be desirable. The ablation study is not very clearly described. E.g., how are low/high variance differentiated? Are all adjacent points included? Also, the discussion of these results is a bit superficial. I am missing an ablation that only uses the morphology aware loss (without the interaction guidance). Minor comments & typos: Abstract: additions such as ""as shown in Fig. 1"" should not be contained in the abstract. p.2: ""motivated by SE block"" - abbreviation SE should be introduced. p.2: ""vertex points on the cervical vertebra have limited deformation"" - this is rather unclear - what do the authors mean here? p.2: ""... user modifications than manual revision"" - than > compared to p.2: ""Adding the proposed gating network on the model ..."" - unclear - how is a network added ""on"" the model - to? combined with? What is this ""model""? (see also on p. 3) p.4: ""are filled with zero matrices"" - why not simply ""are filled with zeros"" or ""contain only zeros"" ""It allows all pixel positions in the feature maps of the main network to attend to each significant pixel position with respect to the user interaction information."" - I don't quite understand what the authors want express here. p.5: ""the ones that rarely deviates ..."" - typo p.5: ""as the criterion to apply the proposed loss"" - What is meant here? p.6: ""... to achieve the MRE under 3"" - measurement unit of 3 should be mentioned p.6: Why were the thresholds selected like they were? i.e., 3/5/10? p.6: ""the images with high error than ... "" - grammar Fig. 4: ""reivision"" References: Capitalization seems a bit off."	The current rating is based on the following major factors: This submission tackles an important problem in fixing the errors in automatic keypoint estimation for medical images. The proposed method has the potential to improve the user correction efficiency. The discussion and explanations in the experiments are not clear enough. The discussion of the limitations/future work is not provided.	The proposed algorithm is novel and the targeted use case is of high clinical value. Significant details regarding the network architecture and the training process are currently missing in the paper. Aspects related to the gating network and the morphology-aware loss are not adequately explained.	"The approach of combining interaction with a morphology-based loss is very interesting and seems to guide the network in multiple aspects. I haven't yet come across such an approach for landmark detection (but I am also not fully familiar with the corresponding literature in the CV domain). It is generally well evaluated with multiple reference methods as baselines. Some of the weaknesses (description of the method, missing variance in the results, discussion) can be rectified rather easily, one aspect that I would have liked to see in addition is a user study that confirms the improvements in addition to ""simulated interactions""."
327-Paper2818	Moving from 2D to 3D: volumetric medical image classification for rectal cancer staging	In this paper, the authors proposed a volumetric convolutional neural network to discriminate T2 from T3 stage rectal cancer with rectal MR volume. A variety ways for combining 2D slice-level features into 3D volume-level features were compared. The authors selected the best performing model through extensive experiments in an in-house dataset of 567 patients.	Using a volumetric convolutional neural network, the authors developed an automatic CAD system to differentiate between T2- and T3-stage rectal cancer. The network contains a CNN feature extractor that maps medical volume to frame-wise features. As well as, a depth aggregation function summarizes the frame-wise features into a volume-wise feature.	1, a hybrid convolution model is introduced to extract rectal tumor features. 2, a bilinear scheme is employed to conduct pooling for every layer. 3, the classification performance is carried out over 3D MRI rectal volumes.	The paper is nicely organised and written. Very good introduction, nice background on the disease and current methods of generating 3D features for classification. Good experimental design and the results are clearly presented. The finding in this paper is a good start point for other work in this direction.	The manuscript is well-written and organized. The author creates a good combination between center loss, triplet loss, and depth aggregation function to enhance the results.	The method idea of this paper is acceptable. And it yields encouraging results which exceeds performances of radiologists.	The performance of the proposed method is only marginally better than the baseline 2D model. Specifically, a variety of network structures are tested but only the f-rMC5 model outperforms the baseline f-R2D model. A few details are missing. See costructive comments for justification.	"-The introduction should contain more details about the open research problems. The introduction should rewrite with a general overview of the study, such as what are the rectal cancer stages? What challenges face the physician to distinguish between T2- and T3-stage rectal cancer? How did you solve these challenges?  -why did the authors distinguish between T2- and T3-stage rectal cancer, not other stages? Is the manner of rectal cancer treatment depend on the cancer stages? Which one? you mentioned in the manuscript, ""Our goal is to solve a clinically challenging problem: to distinguish T2-stage rectal cancer from T3-stage rectal cancer with MRI."" Then please compare your automated accuracy and the clinical accuracy for this problem. -Please add a graph to show the value of the loss function during training. This will reflect the behavior of the new loss function. Is the segmentation for rectal cancer good preprocessing step for this classification? You didn't define the parameter p in equation 1. what are these appreciations of Acc(T2) and Acc(T3) mean? Why is there accuracy for T2 and T3 separately? would you please add the ROC cures? I think the conclusion should be more simple."	1, except the machine parameters, the MRI database of rectal cancer  need more details about tumors, patients, and data, such as tumor size, tumor volume, patient ages, patient gender, slice thickness, voxel size and so on. 2, Tome notations of x+,x-,f+ and f- are required to specified in for Eq(3) 3, The output of Layer4 should be some 2D data. How to conduct 3D convolution over 2D data on Layer5 is an import issue which needs more details in section 2.2. 4, The experiment section mentioned the loss many times. But all loss values of different models are missing  in Table 1 and Table 2. 5, I disagree the assumption of video and VMI's similarity since video is considered as 2.5 dimensional data while volumetric data is one kind of 3D data. 6, 3D volume contains more information than 2D slices. The results over 2D are better than 3D data in Table 1 which needs some analysis. 7, This comparisons in Table 3 occur among different datasets. 8, Table 1, Table 2 and Table 3 do not give their parameters adopted in experiments.	Good. Sufficient details are provided to reproduce the work. Dataset will not be public but it's understandable.	NA	1, The localization of rectum region needs more details since it contains both tumor tissue and other benign tissues. this paper doses not specify either the whole rectum region or ROI of tumor is utilized in the experiment. 2, In Fig 1, the input of this CNN Network and H-W pool need to be specified.  From 4th layer to 5th layer, the procedure is ambiguous. The structure of every layer should be plotted in Fig 1 or specified in Section 2.2.  3, Which feature is omitted in the GLCM features since PyRadiomics has 24 GLCM features while this paper chooses 23 features? 4, Some important training parameters are also absent, such as epochs and batch size.	In this work, the authors investigated the performance of various types of approaches for aggregating 2D features into 3D features for colorectal cancer staging. Specifically, network structures, loss functions and aggregation functions are tested. The authors adopted the approach where they first determine one factor accroding to the best performing model and gradually add more factors into consideration one at a time while keeping perviously defined factors fixed. It's understandable they chose this approach but it would be better if they could do more rounds of selctions or try all possible combinations. There are a few minor problems: How long is the interval between MR imaging and surgical resection for the patients in this study? Variations in scanner types should be taken into account. One big challenge for moving from 2D to 3D is the computation requirement. It would be helpful to include an analysis on number of parameters of different network structures. Attention weighting is implemented as softmax in this paper. It's better to call it softmax weighting/pooling to avoid confusion.	This paper is ready with a bit of modification to publish in the top journal.	To distinguish the T2 stage from T3 for rectal cancer is a big challenge especially over MRI dataset. This paper employed a reasonable CNN model for this issue.  The experimental results are fantastic. However, some aspects of dataset, CNN architecture and experimental analysis are required to be added and revised. Moreover, some evaluation figures such as ROC, Accuracy and loss should be necessary in the experiment section.	Despite having a few problems, the paper has proved its point through extensive experiements. Even though the performance is not outstanding, the findings in the paper are useful for future work in this direction.	The work contains  an excellent engineering contribution	The idea of this paper is fantastic and the novelties of the method also acceptable.  But some details of the dataset, network architecture and experimental outcome are required more efforts.
328-Paper2326	MRI Reconstruction by Completing Under-sampled K-space Data with Learnable Fourier Interpolation	In this paper, the authors propose a learnable method for k-space data completion and filtering. The missing Fourier coefficient are interpolated using a weighted summation of its neighbors with adaptive weights. Two CNNS are applied to regularize the data in both k-space and image space.  The proposed methodology solves the under sampling problem in MRI reconstruction. The authors claim that the accuracy of the proposed method compared to other learning based algorithms is computationally efficient for both training and reconstruction processes.	The main contribution of this paper is to improve MRI reconstruction via an interpolation strategy. The authors use a k-nn strategy along with two CNNs as means of regularisation. Whilst the paper has a strong motivation the technical description is limited as well as the intuition.	The authors proposed a novel deep learning framework to perform MRI reconstruction. This framework was decomposed into three parts: the first part aimed to interpolate the under-sampled k-space using a k-near neighbor method (where the interpolation weights are learned by a deep learning method); the second part aimed to denoise the obtained interpolated k-space using a convolutional neural network; the last part aimed to denoise the reconstructed image (the image obtained from the inverse Fourier transform applied to the denoised interpolated k-space) using a convolutional neural network. The authors evaluated their method on one clinical dataset (brains MR images from ADNI). They compared the performance of their method to those of six state-of-the-art reconstruction algorithms.	In MRI related studies, the reconstruction of missing/corrupted k-space data has been a challenge. In this work, the authors have suggested a deep learning methodology for MRI image reconstruction. It considers MRI reconstruction as an interpolation problem in k-space. The interpolation scheme suggested in this paper is focusing on  adaptive interpolating weights trained with DNN. For benchmarking, they have evaluated the performance of the proposed method with a few available algorithms such as zero-filling method (ZF), TV-regularization-based method, ADMM-Net and the plug-&-play methods. According to their results, it shows that their proposed method has outperformed all those work and it takes much less training and testing time. Further, the proposed method has outperformed all those methods in the presence of noise, too.	-  The interpolation strategy is simple yet it seems somehow effective.	The major strength of the paper relied on the novelety of the proposed method. The authors addressed the MRI reconstruction problem by interpolating directly the under-sampled k-space with a deep learning method, which seems to be not frequently done in the MRI reconstruction field. This proposed method provided better image quality endpoints values (PSNR, SSIM) than state-of-the-art methods and appeared to be computationally efficient.	A relatively small training 2D image dataset (300 slices for training and 21 slices for testing) has been utilized to investigate the performance of the proposed method. Its recommended to validate the claims on a larger dataset as concluding that the proposed method outperforms all the other benchmarking methods will not be a valid statement, otherwise. The article needs fixing some grammar mistakes found in a few places. Proof reading is recommended.	- The technical novelty is not well-explained and several notations are missing or need clarification. Therefore, it is hard to appreciate the level of novelty. -  Authors highlight in several places new optimisation schemes such as deep unrolling and PnP methods. However, it seems like the authors somehow are confused in the terminology. - The experimental comparison seems limited and a major drawback on the paper is the lack of discussion behind the fundings.	The interpolation technique which is the strength of the paper (and method) should be more detailed. Additionally, the method evaluation is not enough comprehensive to ensure the generalization and reproducibility of the findings of the paper (only one dataset was considered, the dataset size is not composed of 3D volume MRIs but 2D MRI slices, the testing data set is small, no cross-validation was performed, etc).	Experiments on a different larger dataset is recommended.	The list seems completed.	Some elements of the paper should be more detailed to ensure the reproducibility of the study. How do you determine the k-nearest neighbors of the missing Fourier coefficients in the k-space? Describe precisely the architecture of the deep learning method used to estimate the interpolation weights. What are the inputs and outputs of this deep learning algorithm? How much data are used for training? What is the computational time (at training and testing) of the network? What is the performance of this interpolation process? The considered dataset is small (notably the testing dataset). Due to this fact, a k-fold cross-validation will be more suitable for the fairness of the evaluation method (instead of use a simple split of the cohort in training and testing subsets).  The dataset is only composed of 2D MRI slices. To ensure that the method is suitable for clinical practice the method should be evaluated on 3D MRI volumes. The method was applied and evaluated in only one clinical dataset. Several datasets (in other anatomical sites) should be considered to ensure that the method could be used in clinical practice. Describe precisely the architecture of the two denoising convolutional neuronal networks (how they work?). Please describe the data. What is the MRI scanner used for the data acquisition? How many Tesla? What are the MRI parameters (TE, TR, flip angle, acquisition time, etc)? Please perform statistical tests (such as the Wilcoxon test) to compare the significance of the differences between the proposed method and the state-of-the-art methods.	The authors have attempted to solve a challenging problem in MRI reconstruction research, which is commendable. As claimed by the authors, it outperforms all the benchmarking algorithms in terms of visual and qualitative evaluations. Further the time less time taken for training and testing is a big achievement. However, the only concern to strengthen the above statements is validating the hypothesis for larger datasets. If this could be addressed, this proposed methodology will be a huge success in this area of research.	- [Novelty] Whilst the paper has indeed a strong motivation, the novelty is appreciated as incremental. This is  due to several reasons. Firstly, there is no intuition on the interpolation strategy that is somehow connected to a naive zero filling interpolation.  That is then refined through a couple of CNNs. Therefore, it is hard to appreciate the level of novelty in this work.  Secondly, the paper needs a revision on the technical description. What is the norm for both terms in (4)? Why is there not a weighting parameter in both terms in (4)?   Overall, the technical description is limited and no intuition is provided. Also, authors should introduce all notations, for example the Fourier operator in (1). - [Experimental Setting] the experimental setting is not well-explained which can be perceived as not more than case studies which demonstrates only limited advantages in both qualitative and quantitative terms. Authors are using ADNI. However, they are not explaining from which ADNI version is taking the data (e.g., ADNI-2, ADNI-GO etc.). Moreover, ADNI provides data in 3D, why did authors not show the applicability in 3D? (at least from the text it seems like they are taking 2D slices). In the medical domain, data is usually given in 3D. Authors need to clarify this part.  Baselines comparison,  Authors highlight in several places new optimisation schemes such as deep unrolling and PnP methods. However, their technique is not directly related to such techniques or novel in that research line. Authors mention [14]  as a PnP technique. However, [14] still builds upon ADMM. Author should notice the difference between deep unrolling and PnP methods. Why not include ISTA-net[*] (another deep unrolling technique with higher performance than ADMM-net see [] ) or a PnP method. The suggested work of [] is advised to revise. Therefore, a more stronger baseline could be good to explore or justify. [*]Zhang, J., & Ghanem, B.  ISTA-Net: Interpretable optimization-inspired deep network for image compressive sensing. CVPR, 2018 [**] Wei, K., Aviles-Rivero, A., Liang, J., Fu, Y., Schonlieb, C. B., & Huang, H.. Tuning-free plug-and-play proximal algorithm for inverse imaging problems. ICML 2020. No ablation study in parameters or intuition of the model is provided. For example, with a smaller and bigger K. All in all, the current paper has good motivation but the technical description and intuition needs to be strongly improved as well as the experimental setting.	"The paper is globally well written.  Please add in Fig. 3, 4, 5, 6, 7 the true images. That will help the readers to visualize the differences between the true images and the reconstructed images.  Define all operators and variables used in equations 1 and 4. In section 3.1 results, the authors wrote ""The best results were emphasized in bold and the second to the best results were marked in bold"". Bold appears two times in the sentence which is not matching what we can see in the tables. Please correct the typos. Discuss the limitations of the study? Discuss the performance of all methods in challenging subjects (worst cases, tumor areas, etc)?"	Its a novel idea that had been investigated on s small dataset. However, comparisons with some state of the art algorithms have also been performed. This work could be further expanded and the authors claims can be validated accordingly.	All in all, the current paper has good motivation but the technical description and intuition needs to be strongly improved as well as the experimental setting. Otherwise, the current version is appreciated with an incremental technical novelty and a limited experimental setting.	The paper overall is good and clinical relevant. The method proposed by the authors has some novelty. However, they is a lack of comprehensive evaluation of the proposed method to ensure that the reproducibility of the findings of the study (that may be a consequence of the MICCAI paper format).
329-Paper1984	mulEEG: A Multi-View Representation Learning on EEG Signals	To combine the temporal and spectral information for representation learning.	The proposed method is significant in EEG Domain for training with mult-view self-supervision approach. The experiment setup is good and the evaluation process. The training aspect is novel by utilizing two views.	The authors propose a multi-view self-supervised method (mulEEG) for unsupervised EEG representation learning. The results show that the proposed method has higher performance.	The manuscript is well organised.	The proposed method is significant in EEG Domain. The experiment setup is good and the evaluation process. The training aspect is novel by utilizing two views.	Using complementary information to construct positive pairs in contrastive learning for EEG is novel. The paper is well-written.	The proposed idea is not very reasonable and has not been fully justified in the manuscript.	Did the authors try other techniques for learning except for RESNET? Which data augmention bring better result? I don't see any discussion on this. Please clarity more on this.	More ablation studies about data augmentation are recommended.	It should be reproducible based on the information provided.	It seems reproducible. Data is available.	As the experimental descriptions are enough, I believe this paper is reproducible.	"In this manuscript, the authors proposed to combine EEG time series and their spectral representations to enhance the effectiveness of the feature extraction. It has been widely proven that the fusion of multiple features positively contributes to the classification. However, it seems inefficient to combine the EEG time-series and spectral powers. In general, limited information can be mined from EEG time series. I would like to suggest combining spectral powers with other types of features, such as functional connectivity. Only one channel was used in this study. Why did you use one channel only? More channels could lead to higher performance. In addition, different channels were selected for different datasets. The same channels should be used. If there are not the same channels, the channels located in the adjacent area should be selected. As the study's primary purpose is to have a good representation, it is therefore expected to have detailed comparisons in representations. However, only indirect results were shown in the manuscript. The number of training samples is 31. How did you set a bitch size of 256? It states ""an initial learning rate of 3e-4"". Why did not you use the format of 10e-4? Typos: e.g., contrastve on Page 5"	Overall the idea suggested for multi-view seems convincing.	The author claim that they design an EEG augmentation strategy for multi-view SSL. It is better to show the effectiveness of using augmentations from baselines.	The fusion of differnt kinds of features has been widely adopted. It is quite mature strategy that has been used in the field.	Based on experimental results, I suggest weak acceptance.	Using complementary information to construct positive pairs in contrastive learning for EEG is novel. The paper is well-written.
330-Paper0876	Multidimensional Hypergraph on Delineated Retinal Features for Pathological Myopia Task.	In this work, the authors developed a multimodal hypergraph learning approach for identifying pathological myopia using features from different retinal structures of fundus images and utilizing the associations between the hidden features. The authors have demonstrated that the combination of these features provide better prediction performance than other approaches, which focus, at most, on only one target structure.	The authors introduced a novel multimodal hypergraph learning technique to learn higher-order associations and modulate delineated retinal features. Their experimental results demonstrate the potential of the model to improve prediction performance, in addition, an intensity thresholding approach was proposed to extract choroid tubular patterns.	The authors develop an algorithm to classify pathological myopia (PM) from retinal fundus images, based on hypergraph learning.  Specifically, the authors extract multiple anatomical features from retinal fundus images (including choroid tessellation, for which they present a sub-algorithm in the manuscript), and build a hypergraph where each node is an image, and images with similar retinal features are connected by an edge (the edge weight is the similarity score).  They argue that this approach can learn to associate multiple interpretable features, and they compare the predictive performance of this model against a hypergraph trained on a reduced set of anatomical features, and a traditional CNN-based approach.  Overall, they demonstrate that the hypergraph trained on multiple anatomical features slightly outperforms the other two approaches, on a range of different inputs.	The authors propose a hypergraph learning to detect early or mild pathological myopia.  * This approach extracts features from several prominent retinal structures, instead of the conventional methods, which focus on only one structure. * The results from hypergraph learning on a mixture of retinal structure provides highest performance, which is statistically significant than conventional CNN based method.	The idea of modulating delineated retinal anatomical features from fundus images using a multimodal hypergraph learning technique is an important task. In this sense, the paper is attempting an important problem in a timely manner. The authors used a hypergraph to learn higher-order associations, which is novel and smart way to solve the overall problem. The proposed method is validated on a real dataset (I the paper and the supplementary material) and show promising results.	Main strengths of the paper: The authors present a timely and interesting application of hypergraphs to the problem of classifying pathological myopia They also present sub-algorithms for segmenting choroid tubular patterns, and make the code publicly available on github	* This paper has limited technical novelty, because the only contribution is using the hypergraph learning on the features extracted from other machine learning and deep learning techniques. * The authors use ResNet34 encoder to compare with the performance of hypergraph learning. What would be the performance using other state-of-the-art CNN, for example ResNeXt-50? ResNeXt-50 provides better performance in comparison with ResNet	The itroduction section needs to be more elaborated by discussing better the limitation of different related works. I see that it does not cover well papers related to the problem to solve. The authors mentioned that the use of CNN embeddings to generate statistical features is limited to the loss of interpretability and they did not explain that. I expected more discussion as well as mentioning related works about this point. The choice of the loss term R_emp(M) is not explained. What is the reason of suming Frobenius norm and L1 norm, why not one simply using one of them? The edges weights were defined as the similarity between retinal characteristiques. How such similarity was computed? The definition of the hypergraph should be better explained in the method section. If an euclidean distance was used, what is the dimension of its input variables? How do the authors define mathematically the retinal carachteristiques? To evaluate their method, the authors used a 80/20 split strategy for training and testing on the dataset. I expected a cross-validation evaluation strategy. How are they sure their model is not overfitting?	"Main weaknesses of the paper: Overall, the performance improvement of the proposed approach is minor.  One potential advantage of the proposed algorithm is improved interpretability, which the authors discuss; however, they don't investigate interpretability of the method on this dataset or compare interpretability against the standard CNN-based approach. Figure 1 caption: there is no explanation of what the symbols (circle, x, square) correspond to in the data.  From the text, I assume it is the level of pathologic myopia, but this should be clearly labeled in a legend. p-values should be reported as, for example, p-value < 10^-4.  They should not be reported as p-value = 0.000 Minor weaknesses: Why is AUC score missing for RGB CNN in Table 1? page 6 type: ""modals"" should be ""models"""	* The hyper-parameters for the proposed approach has been described, and the details of the training/testing set has been provided.	The checked points in the reproducibilty checklist match perfectly the information provided in the paper.	The reproducibility checklist was accurately filled out.  One thing that could improve reproducibility of the paper is to make the full code available after publication.	This is an interesting work to provide better prediction performance of the degree of pathological myopia, using multimodal hypergraph learning model. It would be interesting to see the performance of the approach in comparison with other CNN based methods on RGB images.	The paper is well structured and well written. I suggest that the authors consider answering my questions in the weaknesses part mentioned above, in order to improve this and future submission.	"Given that the performance between the standard CNN-based approach and the proposed hypergraph learning approach is so similar, I think it would strengthen the paper to investigate the interpretability of the hypergraph, to show that it is indeed an advantage of this method. Also, I found the reference to ""multimodal"" to be a bit confusing.  Specifically, the proposed algorithm only processes retinal fundus images - it does not take into account different modalities during the prediction.  I appreciate that the authors extract multiple anatomical features, and input this to their hypergraph model; however, I would not refer to this as multimodal.  Perhaps ""multi-dimensional"" would be more accurate."	This work has applications for predicting the degree of PM by utilizing the relationship between different retinal anatomical features using graph learning. However, most of the extracted features used are obtained from machine learning and deep learning methods.	"I see that this work tackled an interesting problem in a novel way. However, with the limitations I mentioned above I rate this paper with ""Weak accept""."	The authors evaluate a novel algorithmic approach on a novel dataset, and show competitive results.  Supporting algorithms were developed and made available to the community.  This is an interesting direction that I think would spur further research.
331-Paper1289	Multi-head Attention-based Masked Sequence Model for Mapping Functional Brain Networks	This paper proposed a new deep learning model, Multi-head Attention-based Masked Sequence Model (MAMSM), to extract functional brain networks in task fMRI data. They adopted many state-of-the-art deep learning methods in the natural language processing (NLP) field such as Transformer based on attention mechanism and Masked Language Modeling (MLM) used in BERT. Their approach considering fMRI time-series as one of the sequence data like NLP is reasonable and the application of current deep learning models is interesting. Compared to other methods to extract functional brain networks such as sparse dictionary learning and independent component analysis, MAMSM utilized a state-of-the-art deep learning method showing advanced results in extracting functional brain networks and some resting-state functional networks.	This paper proposed a Multi-head Attention-based Masked Sequence Model (MAMSM) similar to BERT model, aiming to learn the different states/tasks or meanings of the same signal values at different time points in a fMRI time series. Quantitative evaluation demonstrates that the learned feature has better interpretability. The authors also design a novel loss function that combine MSE loss and cosine similarity error to extract FBNs. The experiment results demonstrate the new loss function is more suitable for tfMRI than MSE only and can be used to extract more meaningful brain networks.	This work proposes a self-supervised a Multi-head Attention-based Masked Sequence Model (MAMSM) as an embedding technique to identify the task-evoked brain networks and temporal features	Novel application o One main strength of this paper is the novel application of using multi-head attention and masked method to extract features in fMRI time-series. Intuitive design o Also, the application of cosine similarity loss is also notable because it is simply and effectively making their model retain original task design curve during task fMRI scans. Systematic evaluation o They evaluated the results of MAMSM using more than three different models to extract functional brain networks and reported a comparison of extracted temporal features and spatial features, respectively. Especially, comparison to spatiotemporal attention autoencoder (STAAE) enhanced the results of this paper, because STAAE is the current proposed deep learning-based model for extraction of functional brain networks.	a) This paper extracts the functional brain network of tfMRI based on a pre-trained model for learning latent representation, which is novel. b) The new proposed loss function fully considered the potential feature distribution and the relationship with the task design curves. c) The idea is easy to follow and implement.	Contributions: 1). It is very interesting to introduce Natural Language Processing (NLP) techniques for revealing the time series and task-evoked brain networks from task-based fMRI. 2). The methodological validation is presented with the other algorithms, using the identified task-evoked brain networks, time series, and intrinsic brain networks. 3). This paper is written well.	Limited discussion of designed models o Because the proposed method was based on a deep learning model, the hyperparameter of the model is a crucial part when training the model. So, it would be more helpful to understand the effect of each component in MAMSM such as the contribution of multi-head attention, percentage of masking, number of hidden layers in the encoder, and decoder of feature selection layer.  o Also, it would be good to explain the training procedure and parameters in detail. It was confused about how the input dimension was changed during training, and what is the exact number of features in the feature selection layer. Limited evaluation o Since there was no gold standard method to extract functional brain networks from task fMRI, the author compared the results with GLM. But, as they mentioned in the introduction part, GLM also has some limitations when detecting functional brain networks.  o It also would be helpful to describe simply how functional brain networks were extracted from GLM.  o Additionally, using only 22 subjects in HCP data could be viewed as one weakness of this paper. HCP provided about 800 subjects' task fMRI data, however, the authors used a limited number of samples. So, using a larger number of samples would make this paper more robust and impressive.	a) Ablation studies of proposed loss function are needed. In Table 2, the MAMSM has a higher accuracy probably because of the Loss_cos which makes the first six features of the encoder output close to task designs. For SDL, the method in this paper should be included for comparison : Zhao, S., Han, J., Lv, J., Jiang, X., Hu, X., Zhao, Y., ... & Liu, T. (2015). Supervised dictionary learning for inferring concurrent brain networks. IEEE transactions on medical imaging, 34(10), 2036-2045. b) The writing qualilty should be improved with necessary details. For example, why mask approximately 10% tokens? Why add continuous masks besides random discrete masks? What is the ratio between random mask and continuous mask? c) The recent HCP S1200 release have more than 1000 subjects. Even the HCP Q1 release have 68 subjects. Why only 22 subjects are included in this study. How the training/validation/testing datasets are split among these subjects?	1). Validations It would be biased to validate a supervised method with unsupervised methods, such as SDL and ICA; 2). Discovery Limits Obviously, the authors employ an autoencoder ( a deep neural network) with embedding techniques to reveal the brain networks. The reviewers are very curious about the hierarchical structures identified by the proposed method. Some mistakes in mathematic formula In Eq (6), given all variables are matrices, it should be the Frobenius norm. The arbitrary comparison In Fig 4, the background of brain images is not consistent. For instance, the results of GLM mapping to the T1 weighted images but other background images are different. Some artifacts are reported in Fig 5(b).	"It seemed that reproducing their results is hard because as the authors filed out, they didn't provide the code, the software framework and version, and a detailed description of their model parameter and overall code. Also, even though they said ""yes"" to the question of ""The details of train/validation/test splits."", it seemed they didn't mention the detail of the split of the data. Also, I cannot find the information on the statistical significance of the reported Pearson's correlation coefficient."	I think this work is easy to reproduce.	The reproducibility of this paper is limited. This work does not release the source code but applying a public data set.	"* Major comments o For future works, it would be more interesting to extend the current 2D-based approach to 3D by utilizing the state-of-the-art 3D-CNN model. As the authors described in the conclusion, applying spatial attention would make the MAMSM more complete. In the same vein, extension to 3D can be helpful. o It would be helpful to describe the procedure of other methods for obtaining FBNs such as GLM, and SDL. o It can affect to use of neural network-based techniques to obtain final functional brain networks (FBNs' W) rather than simple LASSO regression, as other papers used. As we already know, a simple neural networks-based regression model also can be added to the LASSO regularization. o I am wondering if the ""token embedding"" in the paper is only for applying masked methods? The token embedding was commonly used for the embedding of input word to vector with fixed length (e.g., 512) in NLP, but in MAMSM, it seemed it just meant the input signal with discrete and continuous masking.  o Continuing from the above comment, it would be good to clearly explain the meaning of ""latent features"" from MAMSM. It seemed that it indicates the output of multi-head attention, but other readers can be confused if it is the trained attention scores. * Minor comments o In the first sentence of the Abstract, I think it is better to change ""brain functional networks (FBNs)"" to ""functional brain networks (FBNs). o In Fig. 1., it seemed to be needed to describe more detailed components of the figure such as what is ""CLS"", ""SEP"" in (c), indicating ""n"" is a number of voxels (or vertex) in (a). o In the ""3.3. Reconstruction Loss"" of the results section, the further explanation of MAMSM can be placed in the method section, instead. o In the conclusion section, it would be possible to remove the word ""model"" in ""MAMSM model"" because MAMSM have already ""model"" in the last alphabet ""M""."	a) Include ablation studies as mentioned in Q5. b) Include more subjects in HCP S1200 release. c) More evaluations can be done on other tasks like LANGUAGE and WM. d) The authors did not mention the value of K used in Eq. 5. Consider the new loss function is one of the main contributions, it would be better to add more ablation studies with different value of K. e) The MAMSM model has three transformer encoder layers and each layer contains six head attention, how does these number determined?  f) In Section 3.1, what is attention scores? There is no illustration when it firstly show up. g) What is the meaning of tokens in Section 2.2.3? Is it a scalar value in a fMRI time point? Or it is a vector? h) In Section 2.2.3 , how the original value (I think it's a scalar) can be replaced by [mask]? i) What is the difference between discrete and continues masks? What is the meaning of CLS, M, SEP in token embedding in Fig. 1. k) The optimization function Eq. (6) is denoted by L2 norm, but all variables are matrices. The authors need to clarify the L2 norm and Frobenius norm.	Major Concerns: 1). Validations The method proposed in this work is very novel. But the comparisons could be biased. For instance, the authors provide comparisons of the proposed method with SDL and ICA, which are canonical unsupervised methods. To further validate the proposed method, the author will be asked to validate the performance of the proposed method with other supervised methods. 2). Limited contributions In this work, the authors only provide spatial similarity as the most important standard to evaluate the performances. Obviously, in Fig.1, (d) further training, the reviewers are very curious about the hierarchical organizations of brain networks. But, in the results section, there are only conventional task-evoked brain networks, without any hierarchical structures, reported and compared with peer methods.  Here are some references for future validation: Sahoo, D., Satterthwaite, T.D., Davatzikos, C.: Hierarchical extraction of functional connectivity components in the human brain using resting-state fMRI. IEEE Transactions on Medical Imaging pp. 1-1 (2020). https://doi.org/10.1109/TMI.2020.3042873. 3). Artifacts of identified brain networks In Fig. 5, the authors provide the results of the other detected brain networks compared to the ICA. The reviewers are afraid that some networks would be artifacts. For instance, in Fig. 5(b), an identified network in the second column and row seems to be an artifact that occupies a large area of white matter. Moreover, the reviewer noticed that the figures are generated arbitrarily in Fig 4. Specifically, the results of GLM are mapping with T1 weight images, but other results are mapping with different background images. Minor Concerns: 1). Mathematical formula issues In Eq. (4), the authors need to provide more details of cos. In Eq. (6), the authors denote all variables as matrices, but the L2 norm is applied to generate the optimization functions. In fact, it should be using the Frobenius norm instead. 2). Quantitative Comparison Issues In Figure 4, obviously, according to the color bar, the intensity of GLM results is larger than the proposed MAMSM. Therefore, the reviewers are very curious whether the spatial overlap can leverage the spatial and intensity influence.	Because of an interesting application of state-of-the-art deep learning methods such as Transformer, systematic evaluation of the results.	The proposed method is novel and effective. The writting quality and experiments can be improved.	Poor validation. Again, it should be biased to validate the supervised model with unsupervised models. Some artifacts are reported as Identified Brain Networks. In Fig 5(b), the identified network at the 2nd row and 2nd column is an artifact. There is no consistency in qualitative comparison. For instance, in Fig. 4, the results are not mapped to the same background images.
332-Paper2758	Multi-institutional Investigation of Model Generalizability for Virtual Contrast-enhanced MRI Synthesis	This paper proposes a deep learning framework to investigate the generalizability of the proposed method through comparing MRI contrast synthesis results from some single-institutional models and a joint-institutional model. According to the results, joint-institutional model outperformed single-institutional models in both internal and external testing sets.	In this study, the authors evaluated the performance of a deep learning method (multimodality-guided synergistic neural network) for generating virtual contrast-enhanced MRIs from T1 and T2 MRIs. They explored the generalizability of the proposed method using multi-centric brain MR images. The images were acquired from patients having ongoing radiotherapy. The authors found that the model trained with multi-centric MRIs had better generalizability and accuracy than those trained with images from a single clinical center.	The purpose of this study is to investigate the model generalizability using multi-institutional data for virtual contrast-enhanced MRI (VCE-MRI) synthesis. This study presented a retrospective analysis of contrast-free T1- weighted (T1w), T2-weighted (T2w), and gadolinium-based contrast-enhanced T1w MRI (CE-MRI) images of 231 NPC patients enrolled from four institutions.	The paper is well organized and has presented enough figures and tables to support authors' ideas. It is novel to investigate the generalizability of the proposed method.	The problem addressed by the paper is clinically relevant. Generation of virtual contrast-enhanced MRIs from T1 and T2 MRIs may help for improving patient care and decreasing the acquisition time of MRI protocols. One of the strengths of the paper is the multi-centric dataset the authors gathered for studying the generalizability of their model. Studying the generalizability of deep learning models may facilitate their integration in clinical practice. The paper is globally well written.	1) Well organised and written paper. Easily Readable, and high reproducibility. 2) Well planning study which answers clearly the initial hypothesis. 3) Clinical contribution and conclusions about generalization.	Authors should provide data description in detail. Are all T1w, T2w, and CE-MRI images from the same institute obtained using the same MRI scanner with the same pulse sequence? Do all institutes use MRI scanners from the same manufacturer? These descriptions will help readers better understand the difference in MR images from different institutes. Another suggestion is to consider adding a reader study to evaluate the quality of synthesized images.	The strength of the paper is mostly based on the multi-centric cohort the authors gathered. For improving the generalization power of the model, no technical developments were shown in this study. The clinical application of the study is important but seems not novel. The proposed method was not compared to other state-of-the-art methods. The method evaluation lacks precise statistical analysis.	1)There is no statistical analysis about the significance of the testing cohorts so the authors can strengthen the final conclusion about the training in different cohorts and generalization. 2) More details about the institutes and the difference of the cohorts' modalities, resolution quality needed. 3) minor typos (conclusion needs capital C, reference of L1 loss or equation is needed)	Limited reproducibility.	Please add the batch size used for training the networks. Please provide the computational time (at training and testing) of the networks. Additional statistical tests are needed to evaluate the significance of the results. Please use statistical tests to compare the performance of the model trained with multi-centric MRIs to those trained with MRIs from a single center. Did data augmentations were performed to improve the method generalization? Please describe the data. What are the MRI scanners used for the data acquisition? How many Teslas? What are the MRI parameters (TE, TR, flip angle, acquisition time, etc)? Did this study consider the whole MRI volumes or only 2D slice MRIs? It will be great to consider the whole MRI volumes during the method evaluation (that is more needed for clinical practice). JIM model was trained with MRIs from institutions 1, 2, and 3 and evaluated on MRIs from institution 4. That seems not enough to conclude that the model trained with multi-centric MRIs had better generalizability and accuracy than those trained with images from a single institution. I think it will be interesting to train and evaluate the JIM model with MRIs from distinct combinations of institutions (e.g., train JIM with MRI from institutions 2, 3, and 4 and evaluate it on MRI from institution 1) and compare the obtained results (evaluation metrics + statistical tests).	high reproducibility.	This paper proposes a deep learning framework to investigate the generalizability of the proposed method through comparing MRI contrast synthesis results from some single-institutional models and a joint-institutional model. According to the results, joint-institutional model outperformed single-institutional models in both internal and external testing sets. The paper is well organized and has presented enough figures and tables to support authors' ideas. However, there are some major concerns. Authors should provide data description in detail. Are all T1w, T2w, and CE-MRI images from the same institute obtained using the same MRI scanner with the same pulse sequence? Do all institutes use MRI scanners from the same manufacturer? These descriptions will help readers better understand the difference in MR images from different institutes. Another suggestion is to consider adding a reader study to evaluate the quality of synthesized images.	Discuss the limitations of the study? Discuss the performance of all methods in challenging subjects (worst cases)?  Do the pathologies are preserved after generating the virtual contrast-enhanced MRIs?	A very nice study just some minor concerns: 1)Need of statistical analysis about the significance of the testing cohorts so the authors can strengthen the final conclusion about the training in different cohorts and generalization. 2) More details about the institutes and the difference of the cohorts' modalities, resolution quality needed. 3) minor typos (conclusion needs capital C, reference of L1 loss or equation is needed)	Novelty, experimental design, result presentation.	The paper is clinically relevant and may interest a lot of scientists. Studying the generalization of deep learning methods is important for facilitate their integration in clinical practice.	1) Well organised and written paper. Easily Readable, and high reproducibility. 2) Well planning study which answers clearly the initial hypothesis. 3) Clinical contribution and conclusions about generalization. A very nice study just some minor concerns: 1)Need of statistical analysis about the significance of the testing cohorts so the authors can strengthen the final conclusion about the training in different cohorts and generalization. 2) More details about the institutes and the difference of the cohorts' modalities, resolution quality needed. 3) minor typos (conclusion needs capital C, reference of L1 loss or equation is needed)
333-Paper0373	Multimodal Brain Tumor Segmentation Using Contrastive Learning based Feature Comparison with Monomodal Normal Brain Images	This paper presents the brain tumor segmentation framework by adopting normal brain images as a reference to compare with tumor brain images in the learned feature space. The main contribution of the proposed method is to use monomodal normal brain images as a reference to improve the segmentation performance and the contrastive learning-based feature comparison (CLFC) module that is designed to solve the incomparable issue between features learned from multimodal tumor brain images and monomodal normal brain images. The proposed method has been validated on two datasets which include the in-house dataset and the publicly available Multimodal Brain Tumor Segmentation (BraTS2019) dataset.	In this paper, the authors tackle the task of brain tumor segmentation from multimodal Magnetic Resonance Imaging (MRI) using Convolutional Neural Networks (CNNs). The main contribution is in utilizing MRI images of normal brains to help the network in contrasting the tumor to the normal region. The normal images are generated at runtime by reconstruction from IntroVAE. To further enhance learning, the authors propose that the features of normal regions are aligned between the features of the brain with tumors and features of normal brains. Finally, an attention map is generated to enhance tumor features. The method is evaluated in the publicly available Brain Tumor Segmentation (BraTS) Challenge 2019 dataset and in an in-house dataset. Results of the proposed method improve over the baselines.	This paper proposed a novel deep learning segmentation model to deal with brain tumor segmentation in multimodal MR images. The novelty of the proposed model is to integrate the appearance of healthy brain MRI in order to localize the anomalies that is further used to strengthen the segmentation accuracy. In specific, capturing the appearance of normal-looking brains does not necessarily require all four MR sequences as the model relies only on T1 MR sequence of the subjects. The normal-looking T1 images are synthesized from the original pathological T1 sequences through an IntroVAE model. To tackle the inconsistency between the multimodal image data and the mono-modal normal appearance data, a contrastive learning-based module was designed as well. The model was developed and tested on 2D orthogonal views of the BraTS19 dataset and compared against one standard segmentation model in which the superiority of the model is highlighted in terms of segmentation accuracy.	The proposed method take advantage of both multimodal and monomodal data to increase the performance of tumor segmentation. CLFC module, seems to be simple, yet effective. It improves the efficiency of segmentation backbone. The experimental results show that the proposed method improves segmentation over the method which does not use the contrastive learning.	The writing of the manuscript is generally good and clear. To the Reviewer's knowledge, the idea of using normal brain images to help brain tumor segmentation with CNNs is novel. This is further enhanced by the feature alignment module for regions of the normal brain. This idea is inspired by how radiologists learn to identify brain tumor, i.e., by knowing well the normal brain structure. Incorporating this concept is interesting. The ablation studies show the effectiveness of the proposed method over the baselines.	The paper well-developed the following points: 1)a novel method to generate normal appearance MR slice from the corresponding tumoral slice. 2)they employed an advanced model to efficiently capture the information from so called paired images of healthy-unhealthy slices. 3)Extensive experiments were conducted to evaluate the efficacy of the model. In general, the studied problem is of great interest for the MICCAI community, the proposed method sounds both theoretically and experimentally, and the reported results on comprehensive standard data represent the efficacy of the proposed method. The paper is well-written and easy to follow.	The only comparison technique is nnUNet. The Brats2019 data labels were not compared for each, so it is difficult to compare it with other techniques of the leaderboard. The results show that the proposed method performs well on binary segmentation task, but how does it perform for multi-class segmentation? This is what I am more concerned about because it seems difficult for normal images to help distinguish the types of tumors. Please provide some more details about it. Interestingly, the baseline 1 has already outperformed nnUNet even though it consists of a general Unet structure. It is necessary to explain how such a result can be obtained. Compared to the baseline methods, the improvement of the proposed method is not large.	The proposed method has a lot of components such as 1) segmentation network, 2) normal tissue network, 3) IntroVae, and 4) feature alignment module. This makes it harder to implement the method, especially because IntroVAE needs training just for itself. The authors claim to use Contrastive Learning to align the features, but the employed method is SimSiam, which is a non-contrastive learning method [1] (i.e., it only uses positive pairs). This requires adapting the text and title. The proposed method only deals with whole tumor segmentation task as a binary task. But, in BraTS, it is well-known that a brain tumor can be divided into 3 major regions: enhancing tumor (ET), Tumor Core (TC), and Whole Tumor (WT). The proposed method would be stronger if it was shown to address these classes. The results are obtained in a five-fold cross-validation way. Therefore, we can assume that they are computed from the held-out fold in each run. This can be susceptible to overfitting and overly optimistic results. In other words, a separate test set would be important.	The performance of the whole pipeline depends on the first step of using IntroVAE model. Quantifying the effect of this step separately will further improve the strength of the paper.	"Authors checked ""Yes"" for most questions on the reproducibility of the paper"	The proposed method contains many components, which makes it harder to reproduce from the implementation side. Still, in the Reproducibility Form, the authors promise to make the code available, which may help. Additionally, no details are provided about training IntroVAE. while this is not the focus of the work, it is a key component, so, details would be needed. The authors also use an in-house dataset for part of the results, therefore, it is not possible to reproduce these results. In summary, this Reviewer considers that there are some risks, and reproducibility potential is just moderate.	All the necessary checklist for the resproducibilty are provided.	It would be interesting if  the proposed method is compared with other contrastive learning methods such as MoCo (https://arxiv.org/pdf/1911.05722.pdf ) and SimCLR (https://arxiv.org/pdf/2002.05709.pdf ) instead of SimSiam network. Usually, contrastive learning methods require a large number of negative samples to avoid the mode collapse, the author used SimSiam which use only positive sample.  Then the proposed method should be compare with other the contrastive learning methods which only uses positive samples such as BOYL(https://arxiv.org/pdf/2006.07733.pdf ). The Normal Appearance Network performance depends on the reconstructed monomodal normal brain images from IntroVAE. It would be better to show qualitative and quantitative analysis of the generated samples. Please also explain if the segmentation backbone and Normal Appearance Network were trained from scratch or these are pre-trained models. The author should also include the hyperparameter sensitivities for the proposed method in the training details	"General This paper focuses on the task of brain tumor segmentation in MRI, and the main idea and contribution is the use of normal brains to contrast with the brains with tumors. To that end, the authors learn an IntroVAE from normal T1 scans and generate a normal brain at runtime. During training, the authors propose a loss that aligns the features of normal regions between normal and tumor scans, such that features of the tumor are enhanced by an attention map. The paper is well-written and the method is novel. However, there are some concerns such as dealing with the ""Whole Tumor"" class only or the evaluation procedure. Comments/questions to the authors (not in order of importance) 1) The proposed method has a lot of components, such as the segmentation network, but also the IntroVAE for generating normal brain images.  a) There are no details about the training of IntroVAE. It is also unclear how much the segmentation network is affected by this model. Could the authors add supplementary material with the recipe to train the IntroVAE, please?  b) Related to the previous question, could the authors provide some images with the reconstructed images, especially in the tumor region, please?  c) Could the authors release the source code, please? It would help the readers in implementing this approach. 2) The evaluation procedures raise some concerns.  a) The authors use five-fold cross-validation. While this is a valid approach to searching hyper-parameters, it is fair to assume that the reported metrics are the average of the held-out folds, used for validation during each training. Therefore, it can suffer from over-fitting and the results may be overly optimistic. In other words, it would be desirable to have an independent test set.  b) Related to the previous question. BraTS makes a validation set that can only be evaluated online with hidden ground truth. Could the authors submit their method and the baseline and provide such results, please? This wouls allow to compare with the state of the art.  c) The authors employ Wilcoxon signed-rank test and report p-values to compare across methods. However, the significance level must be mentioned, otherwise, the reader does not know the condition to verify the hypothesis.  d) No comparison with SotA is provided, other than comparisons with the baselines. Although the authors only tackle the Whole Tumor class, comparisons could still be made in these classes. 3) The authors only report the metrics about the binary whole tumor segmentation. However, in BraTS, the tasks also include Core Tumor and Enhancing Tumor.  a) The proposed paper would be stronger if the authors extend the method to the other classes. Why did the authors choose to tackle Whole Tumor? Could this be made clear in the manuscript, please? Further comments (suggestions/extra comments on future work) - NOT intended to be addressed during rebuttal 1) The proposed method is interesting and new. Here are some suggestions for an extension of the work that the Reviewer considers would make it a stronger work. As an extension of the work, it would be good to train the network with Core Tumor and Enhancing Tumor and report these metrics, too. Also, a comparison with the SotA in BraTS 2019 is necessary to show the benefits of the proposed work. Finally, the authors propose to generate a normal brain from the patient's acquisition, which is interesting. However, it is not clear how much the method is sensitive to this. A simple experiment would be to replace the generated brains with an Atlas of T1 sequence, registered to the patient's scans, and check the differences. References [1] Tian, Yuandong, Xinlei Chen, and Surya Ganguli. ""Understanding self-supervised learning dynamics without contrastive pairs."" International Conference on Machine Learning. PMLR, 2021."	"* Page2, ""anomaly detection based methods are unsuitable ..."" This statement needs to be reformulated. In fact, there have been quite many studies in the field of unsupervised anomaly segmentation of brain tumors. However, most of them relied on single modal MR images such as FLAIR in which at least a part of the tumor appears with hyperintensity patterns. Although the segmentation problem is simplified in such works, some interesting results have been reported. Therefore, this is recommended to reformulate the statement. * Page2, second paragraph, line8: ""in this was, tumor regions.."" this sentence is a bit confusing and is hard to follow. Please rephrase it. * Page2, second paragraph, line11: ""tumor brain images"" please reformulate this and the rest in the whole manuscript to ""brain tumor images"" * Page3, first paragraph, the last three lines: Does this framework function as the same for the testing phase? If so, how the pathological slices will be defined? Otherwise, if in the testing phase, all the slices are analyzed, this should be briefly explained to avoid confusion.  * Section 2.1: It was stated that T1 sequence is the most commonly used brain imaging modality, and therefore, the normal appearance model was developed based on T1 images. In addition to the conventionality, it would be interesting to investigate how other sequences perform in capturing the appearance of healthy images.  * Another important comment: Since the performance of the whole pipeline depends heavily on the IntroVAE, It will be of great interest to quantify the performance of the IntroVAE iteself. For example, some post processing steps can be conducted on the residual images between the original and the output of the IntroVAE and compare against the segmentation labels. This would be beneficial to quantify the functionality of this piece of the model. * Page 6, line2: Please describe briefly how histogram matching was performed (e.g, one image per modality was randomly chosen and used as reference or ....) .  * Regarding the external comparison against nnUNet: please specify if the nnUnet is trained on 2D or 3D.  * It will be very good to add another column in table 1 and show the best performance reported over the same BraTS dataset. * Page8, line9: briefly describe how the attention maps were calculated and visualized."	It is an interesting method for tumor segmentation that utilizes both multimodal and monomodal data to improve the performance, but there are lacks experiments to demonstrate its effectiveness against the baselines and the existing methods based on the contrastive learning.	The work proposed in the paper is interesting and new. However, there are some weaknesses, such as the evaluation procedure, the setting of just segmenting Whole Tumor that is different from the setting in BraTS and the brain tumor segmentation community, and the imprecise use of the Contrastive Learning concept, when what was used was a non-contrastive learning method (SimSiam). So, despite the interesting work, the Reviewer believes that some changes would be necessary.	This paper proposed an interesting novel idea to improve the segmentation accuracy. The method sound both theoretically and experimentally. It was well described and the experiments are sufficient to justify the model efficacy. Quantification over extensive experiments are done quite well.
334-Paper1961	Multimodal Contrastive Learning for Prospective Personalized Estimation of CT Organ Dose	This paper provides an interesting approach to estimate the tube current modulation (TCM) map from scout images and patient size. The approach includes a novel contrast learning technique to include information from different sources (images and size profiles). The contributions are clearly stated: Contrastive learning technique to include information from images and patient profiles. Estimation of TCM maps. Real time CT organ dose estimation from scout images and TCM maps.	This paper proposes a method to predict patient specific organ dose in CT acquisition.  The method takes as inputs both image-like information : 2 orthogonal 2D scout CT images, the scan z range in form of a binary 2D image, a tube current modulation map and 1D patient size profiles derived from the two scout images (Dw profiles). The objective is thus to be able to estimate dose from only scout images for tubes with TCM capability, alleviating for the need of a CT scan, which would be a major step towards dose personalization in CT imaging, which is of great clinical importance.  The authors contributions are : multimodality : both image-space and profile-space are embedded into compressed latent representation and combined for dose prediction contrastive learning is used to correlate learned features of both encoders from orthogonal views of the same patient while disentangling them from views of other patients generate a TCM map, a part I did not fully catch	This paper proposes a deep learning approach for CT dose estimation from scout images and size profile. It is based on multi-modal self-supervised learning (with contrastive-learning (CL)), followed by dose learning from the deep self-supervised representations. A Tube Current Modulation (TCM) modeling is also proposed as an intermediate step to improve the dose estimation.	The approach includes different sources of information. Excellent agreement with predicted TCM and reference organ doses. Low computation time required.	The clinical value is high. The idea to leverage contrastive learning between two profile views of a same patient to improve dose prediction is very interesting and quite original in the field, especially given the demonstrated improved performances	The paper is well written and the method seems sound, with an appropriate use of CL for learning meaningful representations.	Some details on the TCM map generation are not clear: What does DCT stands for in this case? Authors describe that the doses are modeled as the weighted sum of single-view doses with weights coming from the tube current profile. How are the weights calculated from the tube? How is the contribution of each organ calculated for each view? Was the training and test set of scans split randomly? The augmentation was performed only in scale of the same TCM maps. Differences in the scouts would be recommendable. How are the inconsistencies between scouts and CT scans palliated?	"Many ideas are combined (TCM map generation, contrastive learning in the profile domain,""multimodality""), which makes the reader wonder what is the core message of the paper.  It is not clear if the gain between pure image-based prediction (Scout-Net) and ""multimodal-based prediction"" (Scout-MCL) is actually due to the multimodality, as there is not an ablation study with respect to other image inputs of the ScE branch that are used by Scout MCL (i.e. the TCM map). More importantly, I wonder if the DwE is really useful as  contrastive learning could also be done directly in the scout image domain I Struggle to understand to what extent this information is multimodal, given that the Dw profiles are extracted from the scout images. Unless I am mistaken (which could be the case given my non proficiency in CT technicalities), this is more here a combination of a projected representations the scout views rather than indeed modalities that are combined. the whole method describing the generation of synthetic TCM maps from scout images is unclear. It is said that ""we generated synthetic 2D TCM maps by fitting profiles from DCT basis images to TCM profiles as is shown in Fig. 2"", which shows only a result. The paper is very technically oriented towards CT and thus requires expertise in CT that I unfortunately do not possess completely."	An ablation study is performed only for the CL and some augmentation, not for other design choices such as the inclusion of body size and TCM in the pipeline. The dataset is rather small and it is not clear whether it originates from a single or multiple centers.	The code is not available (although not required). The datasets are not available (although not required). Experiment parameters and configurations were detailed. Please clarify how the training and test were split.	The lack of technical description regarding TCM map generation (the DCT fitting part of the paper that I missed) is a minus. Maybe it's very common CT knowledge that I just do not possess.	The data does not seem public and no mention of sharing the code.	I believe this is a relevant paper for the community and proposes a novel methodology that is worth publishing. Please, consider the weaknesses already pointed out to improve the clarity of this work.	"why not perform contrastive learning directly in the scout image domain ? I disagree with the unsourced comment that ""Existing contrastive learning methods are heavily reliant on the availability of extremely large training sets. Especially in medical imaging, training data are limited making the contrastive learning methods less effective."". section 4.3 should be renamed results"	"Abbreviations are used before their definitions, also notations (l \in {1,2,...L+1}) Some abbreviations should be reminded in Fig. 1. Some are defined after the figure. In Introduction, ""in medical imaging, training data are limited  ..."" It is not always true, there are a lot of unlabelled data for instance in histopathology.  In introduction, sentence to fix: ""... in order to maximize and minimize respectively..."" In section 2., it is mentioned that the scout and Dw should learn similar representations. First it is not the image and Dw that lear representations, but even more I don't think the representations should be similar. The image contains much more information than the Dw.  More information could be provided for the model description, such as the number of filters, filter size, initialization for the 1D convolution. Equation (3) should be better motivated. It is also not well defined. Should it sum for all organs, or maybe one loss for each organ? Why are there L+1 organs? Also the notation l is used in the previous equation for the contrastive loss, and also for ""leaky"", maybe other notations should be used. It is not clear whether the data originates from a single institution or more (plural in 4.1). Below table 1 (4.2), ""l \in L"" should be ""l \in {1,...L}"" or L+1 that's not clear from before. In Table 1 it is strange to name the model ...CL (noCL)"	The weaknesses of this paper rely on the clarity of the presentation but not on the methodology itself. I think it could be easily improved with minor cosmetic changes.	This is a very technical, CT oriented paper that proposes to leverage pseudo multimodality and contrastive learning in an original way that I think would find an audience at MICCAI.	The method seems sound, although some more motivations and ablation studies could better convince the reader about specific design choices. Some notations and missing informations make the paper not very easy to read although it is well written.
335-Paper2386	Multi-Modal Hypergraph Diffusion Network with Dual Prior for Alzheimer Classification	In this paper, the authors propose a novel semi-supervised hypergraph learning framework for Alzheimer's disease diagnosis. Besides,the framework allows for higher-order relations among multi-modal imaging and non-imaging data whilst requiring a tiny labelled set. The experimental results show that the proposed approach is able to outperform current techniques for Alzheimer's disease diagnosis.	This paper proposed a novel self-supervised dual multi-modal embedding strategy aiming at Alzheimer's disease diagnosis. It utilized the imaging data and the space of the hypergraph structure. Moreover, the paper introduces a diffusion model to hypergraph learning. The experimental results show the method's good performance.	This paper proposes a novel semi-supervised hypergraph framework for Alzheimer's disease diagnosis. It introduces a dual embedding strategy for constructing a robust hypergraph and a better diffusion model for hypergraph learning.  Experimental results demonstrate that it outperforms other hypergraph techniques.	1.This paper attempted to introduce a dual embedding strategy for constructing a semantics robust hypergraph. 2.This paper presents a dynamically adjusted hypergraph diffusion model, via a semi-explicit flow, to improve the predictive uncertainty 3.This paper performed two ablations studies regarding their design and the modalities used to support the design of their technique. 4.The experimental results of'EMCI vs LMCI'shows outperform current techniques for Alzheimer's disease diagnosis.	Firstly, the method of this paper is innovative. Specifically, this paper uses multi-modal data (i.e. imaging and non-imaging data) to deal with the classification of Alzheimer's disease diagnosis. And based on those data, the paper introduces a semi-supervised hypergraph learning framework.  Secondly, in order to adjust the diffusion of hypergraph, the authors proposed an uncertainty hypergraph minimization of Eq. 3. Thirdly, the author's experiments are very informative. The authors performed experiments not only for binary classification but also for multi-classification to verify the effectiveness of the proposed method.	The overall framework is clear and the proposed model has shown its effectiveness.	1.The result lacks annotation. The discussion of the differences between the work and HF [23] HGSCCA [24] HGNN [10] DHGNN [15] is insufficient and unclear. 2.The result of HF in this work is ambiguous, as different parts in the HF [23] describe it differently. 3.In section 2,the terminology and wording chosen are in parts particularly intricate, which further reduces the readability that is already affected by somewhat deviant grammar. Considering the presented approach is mathematically elaborate , the work's reproducibility would be greatly enhanced if if it was written such that the international community could follow more easily, and the reception of a paper could likely be improved if it was easier to read. Minor revisions:  1.Problem with abbreviations 'late mild cognitive impairment (EMCI)',Please change it to'(LMCI)'. 2.In 'Fig. 3: Performance comparison for the four classes case',the vertical axis'HGGN'does not match what is mentioned in the text.	"Firstly, the methodology section of the paper is not presented in enough detail to be easily understood. For example, in 2.1, the authors propose to introduce T transformations without giving detailed reasons why they should do so and what the disadvantages would be if they did not. And for non-imaging data, the authors hold the view that creating a subject-phenotypic relation can mitigate the neglect of perturbing directly the data. This needs proof to justify. In Fig. 2, the authors do not go into detail about what each of the different colored nodes in the diagram represents and what the connecting lines between them indicate. Secondly, the authors did not cite the most recent references for hypergraphs, e.g., literature [30]. Thirdly, the experimental part of the paper is unconvincing. The authors do not list the SEN and PPV metrics for the binary classification task in the Supplementary. Moreover, the authors' experimental results surprisingly show that their proposed method is far ahead of other algorithms for both binary classification and multiclassification tasks (e.g., 81.69% accuracy on the AD vs NC vs EMCI vs LMCI classification task), and the authors' explanation of this result is unconvincing. What's more, the author's writing is hardly satisfactory, with frequent grammatical errors. For example, Page 1: ""has show have shown"" should be ""has shown"". Page 2: ""we introduce a a more"" should be ""we introduce a more"". ""pLaplacian setting"" should be ""Laplacian setting"". Page 6: ""EMCI"" should be ""LMCI"". ""5e-2"" should be "" "". Page 7: Table1 ""SEN PPV ACC"" should be ""ACC SEN PPV""."	"1.For the introduction, improving predictive uncertainty is not clear. 2.The proposed method is reasonable, yet hypergraph diffusion module is not clear, more in-depth analysis will be better. 3.Some typographical and grammatical improvements should be made, such as the performance of ""ours"" in Table 2."	Maybe the model and results can be implemented.	If the code is provided, the algorithm will be easier to reproduce.	The results in this paper are easily reproducible ?	Refer to the detailed weakness.	The authors could make changes to address the issues shown in the weakness.	"The authors need to solve the following issues: 1.For the introduction, improving predictive uncertainty is not clear. 2.The proposed method is reasonable, yet hypergraph diffusion module is not clear, more in-depth analysis will be better. 3.Some typographical and grammatical improvements should be made, such as the performance of ""ours"" in Table 2."	Due to the novelty of the proposed method, the effectiveness of the results, I prefer to accept for the manuscript.	Interesting paper where merits slightly weigh over weakness.	The proposed method seems like a sound extension of jointly learning feature embeddings strategy and hypergraph diffusion module. The idea is novel. Yet hypergraph diffusion module is not clear,  and some typographical and grammatical improvements should be made,
336-Paper1841	Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training	This paper integrated vision with language together using masked autoencoders for joint pre-training. Several important technical modifications and explorations were made upon the original masked autoencoders, such as masking ratios, reconstruction features, and decoder designs. The pre-trained vision-and-language model yields significant improvement over random initialization and other competitive baseline methods on three representative vision-language tasks, covering five public datasets. Ablation study was presented to demonstrate the efficacy of both vision (MIM) and language (MLM) parts of the model.	This paper presents a multi-modal masked autoencoder (M^3AE), which is based on the Transformer, for medical vision-and-language pre-training. Given a pair of image and text, this paper introduces a simple training strategy that trains the model by predicting the masked regions of the image and the masked words of text. The experimental results show that the proposed method outperforms the baseline methods on three downstream tasks including medical visual question answering, medical image-text classification, and medical image-caption retrieval.	This paper presents a multimodal pretraining method in a self-supervised manner for the medical field. Reconstruction takes advantage of different levels of visual and textual features to deal with different levels of visual and language abstraction. The evaluation was performed on multiple multimodal downstream tasks.	Open dataset: All results were obtained from publicly available datasets. Large improvement: Compared with existing methods, this paper achieves noticeable performance gain in multiple public benchmarks. Clear illustration: The description and illustration of the proposed joint pre-training are clear and easy to implement. Sufficient comparison: The proposed method is compared with several competitive methods in each benchmark dataset and shows great improvement.	The proposed approach is simple yet effective. Experimental results on three downstream tasks show that the proposed method is effective.	A multimodal pretraining method for the medical field is proposed. That is, the proposed multimode masked autoencoder learns in a self-supervised way. Due to different information densities, the input image and text using different masking ratios. Reconstruction takes advantage of different levels of visual and textual features to deal with different levels of visual and language abstraction. Evaluate three tasks, including Med-VQA, medical image-text classification, and medical image-text retrieval, and achieve great improvement.	No major weakness detected.	"Some key details are not explained well.  In section 2.2, representation selection for reconstruction paragraph, if you use the image features obtained from the k-th layer to reconstruct the input image, how will the last (N_m - k) layers be trained? How will you train the feedforward sub-layer of the N_m-th layer? There are some typos.  (1) In section 2.1, vision encoder paragraph, the dimension of $p_n$ should be $p_n\in^\mathbb{R}^{P^2\times C}$. (2) In Fig. 1, for text, it should be ""Text Embeb"" rather than ""Image Embeb""."	"For different masking ratios, there is no experiment to vertify your perspective and lack of experience to explain what ratio is the optimal choice. For the ablation study of 'Effectiveness of different layers to perform MIM', The reason for ""layer3 is the best, but the accuracy of layer4 to layer6 declines"" in the experimental results is not clearly and fully explained and a bit far-fetched. There is a lack of experiments to vertify your designs to make this simple approach work for decoder designs. Initialize the vision encoder with CLIP-Vit-B, which is equivalent to using the CLIP dataset. In experiments, other relevant methods seem not to be used in this way, leading to unfairness."	It is easy to implement the idea based on the existing method description.	The reproducibility looks good.	reproducible	"Here are some suggestions for improving the paper: Apart from vision-and-language downstream tasks, can this pre-trained model be useful for vision-only or language-only tasks as well. How to use the pre-trained weights for vision and language tasks separately? Please elaborate on how to determine masked 15% words in the input text. The word ""opacities"" is masked, but most words in a sentence do not carry such a critical meaning, such as ""chest"", ""radiograph"", ""shows"", etc. Did the authors apply any specific process for effective masking choice?"	This is a good submission in general. Please consider revising the paper according to the weaknesses.	The proposed contribution point should have experiments to prove that it is effective. Try to supplement relevant experiments to make the paper more solid.	This is a very nice study that integrates information from chest radiographs and reports for pre-training ViT. The proposed joint pre-training strategy is fairly novel and easy-to-implement, and extensive experiments show the pre-trained model is powerful across many vision-language tasks.	The proposed approach is simple yet effective. The experimental results on three downstream tasks demonstrate the effectiveness of the proposed method. The ablation study demonstrates the effectiveness of using low-level features for reconstructing the input images. The weakness such as typos and the detailed explanations could be easily modified in the final version.	The method for the medical field is novel, and the clarity and organization of this paper are very good. Besides, the method is evaluated in 3 downstream tasks to demonstrate its effectiveness.
337-Paper1488	Multi-modal Retinal Image Registration Using a Keypoint-Based Vessel Structure Aligning Network	The paper proposed a self-supervised learning method for multi-modal retinal image registration. The proposed method consists of feature detection using RetinaCraquelureNet and feature matching using SuperGlue.	The authors present an end-to-end method that combines RetinaCraquelureNet and SuperGlue networks into a single system to perform multimodal retinal image registration. Training is performed with a synthetically generated multimodal dataset of retinal images, in which warpings and noise is dynamically introduced into the images. to represent image acquisition variances.	The authors proposed to extract convolutional features from the vessel structure for keypoint detection and description and used a graph neural network for feature matching to achieve the multi-modal retinal image registration.	The proposed method in general works for different imaging pairs such as CF-FA, IR-OCTA-OCT The proposed method is unsupervised/self-supervised and does not require any manually labeled ground truth for training network Overall the paper is well written with good illustrations.	The authors provide an end-to-end method, combining the RetinaCraquelureNet (based on ResNet) and SuperGlue networks into a single system. The method is build on top of 2 individually proved neural networks, with extra refinements and fine tuning to improve both their individual and joint performances. So while it doesn't present a great innovation, is a push towards the good direction. The paper is very clear, and has a very detailed description of the system and the experiments.	The method is novel, the manuscript logic is clear, and the verification experiments are abundant.	"The proposed method only tested on narrow-field retinal images. More challenging cases needed to be tested. While the paper validates the proposed methods on three datasets, the baseline methods are limited. See other suggested baslines in ""comments for authors"""	"The method is trained only using synthetic data. Although this is quite understandable, given the difficulty of obtaining a large dataset of correctly manually annotated data utilizing multiple modalities. Synthetic data is created using homographies. This works mostly for images with a narrow field of view. This is shown in works like [C. Hernandez-Matas, X. Zabulis and A. A. Argyros, ""REMPE: Registration of Retinal Images Through Eye Modelling and Pose Estimation,"" in IEEE Journal of Biomedical and Health Informatics, vol. 24, no. 12, pp. 3362-3373, Dec. 2020, doi: 10.1109/JBHI.2020.2984483]. where it was shown than utilizing an eye model is more accurate than simple homography for images with a FOV over 40o While the method is geared to multi-modal registration, it would be interesting to see performance in single mode registration, as there exists at least one publicly available for evaluation registration performance on fundus images, the FIRE dataset that was also utilized in the paper mentioned above Color retinal dataset utilized contain only low resolution images (576 x 720), when it's almost been a decade where high resolution images (at least 2500x2500) are widely available, so such low resolution images should not be considered enough anymore."	The interpretation of symbols in the formula needs to be described in more detail.	The paper provides sufficient implementation details.	System is very well defined and detailed, although not publicly available. Dataset is very well defined an detailed, although not available at this time (although they indicate it will be available upon acceptance) Training and execution dataset partitions and epochs utilized is well defined.	The method is reproducible.	Overall the paper is well written and is of interest to the MICCAI community. A few comments to further improve the paper. (1) All test images are narrow-field. The transformation can be approximately modeled by homography. I suggest the authors to test on more challenging case including ultra-widefield retinal images (for example, PRIME-FP dataset https://dx.doi.org/10.21227/ctgj-1367) as secondary results. (2) The paper compares the proposed method with other deep learning based method such as SuperPoint and GLAMPoints. Please also consider other conventional methods for retinal image registration, such as vessel-based, intensityb-based or traditionally keypoint-based methods.  [Vessel-based] Registration of multimodal fluorescein images sequence of the retina [Intensity-based] Maximize mutual information between multi-modal images [kepoint-based] A partial intensity invariant feature descriptor for multimodal retinal image registration [keypoint-based]  Alignment of challenging image pairs: Refinement and region growing starting from a single keypoint correspondence (3) The synthetic images are obtained using CycleGAN. The generated images may contain artifacts. How robust the proposed method is regarding such artifacts? (4) Will the IR-OCT-OCTA dataset be publicly available?	I would like to congratulate the authors on the great description of the system, the dataset creation and the experiments. I consider this paper to be a good contribution to the field, even if not highly innovative. The main weaknesses I see are mostly related to the synthetic data utilized for the training of the method. Synthetic data is created using homographies. This works mostly for images with a narrow field of view, more complex warping, or if possible the utilization of an eye model would provide more realistic transformations, particularly when working with standard FOV images. Color retinal dataset utilized contain only low resolution images (576 x 720), when it's almost been a decade where high resolution images (at least 2500x2500) are widely available, so such low resolution images should not be considered enough anymore. I think the authors should aim for an upgrade of those images towards that direction. Additionally, while not strictly necessary, a nice piece of data to show the performance of the registration method in single mode images would be to utilize the FIRE dataset for testing, as there are results of competing methods readily available.	In the abstract, it is suggested to write quantitative experimental results. What is the meaning of the colored points in the image in the rightmost box in Fig. 1? What are the shortcomings of the existing methods introduced in the Introduction? What does x_ai and x^_pi represent in Formula (2), and what does d(x_ai, x^_pi) represent? What do N and M mean in Formula (3)? Do the M on the left and the M on the right have the same meaning? How are the hyperparameters in the framework determined? What is the reason for using different assessment measures in different experiments?	The paper has novelties and solves a challenging problem (self-supervised learning/multi-modal registration). The results show the improvement over prior works.	The system seems to be robust, and a push forward in the good direction. However, it's not highly innovative, as it mainly relies on 2 already established neural networks with some fine tuning on top. The utilization of homography as the only type of warping for the creation of the synthetic dataset and the utilization of low resolution RGB images are weak points, but not enough to prevent the acceptance of this submission.	The research is meaningful, the logic is clear, and the experimental results are convincing.
338-Paper1274	Multi-Modal Unsupervised Pre-Training for Surgical Operating Room Workflow Analysis	The authors describe a method for pretraining a CNN to aid in workflow analysis tasks (Activity recognition and semantic segmentation) from the room camera in an operating room. The method is evaluated on two public datasets and compared against 2 other methods from the state of the art.	This work proposes to use unsupervised clustering method, i.e., SwAV to pretrain the image encoder with multi-modal images. It takes two modalities as two different views of the same image, intensity and depth images from TOF cameras and train the encoder to predict the other modality's pseudo code. Compared to the offline clustering method, the SwAV fits the large-scale setting and achieves a superior result compared to the other self-supervised methods, especially on few-shot settings.	This paper relies on multimodal data to pretrain models. The model enforces generating similar prototypes from different modalities. The model has been evaluated on two task, where the proposed model achieved superior performance.	Relevant problem Novel approach Good dataset for evaluation Comparison against different methods	-Multi-modality usage: this work proposes to fuse different modalities to pretrain the model encoder. It forces semantically similar group together for different modalities. -Training process: the current methods fall into the directly end2end training process while rare works focus on the pretraining + finetuning process. This paper shows a potential way to pretrain the image encoder with multi-modal data.	experiment parameters are provided leveraging multimodal data to pretrain models experimental results on two tasks	Certain details/terms not explained in the paper No real discussion of results Some grammatical errors, the paper would definitely benefit from another read-through	-Online V.S. Offline: This work opts for the online manner to do the pretraining. However, as far as I know, the SwAV is not comparable to the other offline methods, such as MoCO. It is not clear why the online method is so necessary. -Fair comparison: This paper is about multi-modal unsupervised pretraining, however, the compared methods, i.e., pace prediction, clip order prediction are based on the uni-modal video frames, as far as I am concerned. This is not a fair comparison, since I will be confused whether the multi-modal data or the pretraining process proposed helps to improve the performance.	the proposed model relies on an architecture similar to [4]. It seems that the technical contribution is limited. [4] relied on single modality and used different augmentation to achieve the same goal, I believe the author should have used compared their results with [4] as another baseline. while the proposed model used both intensity and depth images, the baseline model only took intensity images as input. I think it would be interesting to establish another baseline that relies on the same data as the proposed model as depth data was used during pretraining. [16, 30] have studied the effect of using bot RGB and depth images on improving performance. The author could have used the both modalities when the models are trained for the task. there were multiview data. Were the cameras calibrated? As multiview data was available, it would be interesting to look at this aspect as well.	The authors present the method in such a manner that it should be reproducible. All the details provided on the checklist were truthful.	According to the materials, the paper is reproducable.	Model parameters are provided in the supplementary document. It is however important to ensure it will be published along with the paper. The used dataset are not publicly available.	"One of my main concerns is that a lot of details were missing/unexplained in the paper: First, it only becomes apparent relatively late in the paper that the authors are talking about workflow analysis from room cameras, not e.g. from the daVinci endoscope Some abbreviations are never introduced, e.g. SwAV, Bi-GRU The term ""prototype"" is also never defined, nor is any intuition behind the idea explained. There are also some other open questions/concerns from my side: The results of the different methods for segmentation appear close together, can you comment on the statistical significance of the difference in results? The way I understand sec. 3.1., you are pretraining the network to produce similar features for the depth and the RGB images, here it would be interesting to see if the final network actually needs the multimodal data in the end/how the network would fare with only one modality. How are the modalities merged? Is the input two channels? If yes, how exactly does it work for pretraining? Is one channel just set to 0? Did you also perform the analysis for K for the segmentation problem? On what data was K selected, did you use a validation set?? Seeing the results in table 3, wouldn't it have sense to consider larger Ks?"	-Except for the weakness, I'd like to see more details about the training process. Because the pretraining process in this paper is for a better initialization of the backbone encoder. How's that backbone utilized for different task, i.e., activity recognition, semantic segmentation, what the specific settings need to be applied, a few lines of explanation will make it better to understand. -Ablation study about the multi-modal data and the pretraining process -Multi-modal pretraining baseline to be added.	I think it would interesting to use [4] as a baseline, as the proposed model share many concept with [4] and the model in [4] can be applied into more data because it only relies on a single modality.  [20] used DeepLab V3+ as baseline, it would have been more interesting to use the same baseline. Similarly to workflow experiment, it would be interesting to see the performance of the semantic segmentation model when 100% of labeled data was used.	In my opinion, the paper tackles an in interesting problem, but it still has a lot of missing information and unclarities that need to be addressed.	The use of multi-modal data in pretraining.	The technical contributions are limited. The proposed model borrowed many concepts with [4] hence I was expecting to use [4] as a baseline.
339-Paper1682	Multimodal-GuideNet: Gaze-Probe Bidirectional Guidance in Obstetric Ultrasound Scanning	The authors propose a multimodal neural network to jointly predict the change in gaze position on the ultrasound image and the movement applied on the probe by a human operator to reach each of the many standard planes during a routine obstetric sonographic scan. The aim is to use gaze information to suggest probe motion, and viceversa, during training of new specialists. The performance improvement of the method over single-task networks is consistently shown by the experimental evaluation, which was performed on a dataset which is not mentioned to be public. The methodology used for data collection is however very well explained, such that replication should be feasible.	This paper explores a completely new paradigm to use Ai techniques to assist less experienced operators of obstetric  ultrasound to obtain high quality results. It achieves its goal through the novel combination of probe and the gaze of the sonographer,  and results demonstrate that such a joint approach outperforms single-task learning for both probe motion guidance and gaze movement prediction. This is a very tangible and practical first step towards making ultrasound more universally available as a diagnostic imaging tool, even in the hands of non experts. For me, this paper embodies the spirit and raison d'etre of the MICCAI Society perfectly.	This paper presents a multi-task learning framework to predict gaze position and US probe rotation for guiding obstetric US scanning, which is a novel approach to tackle this kind of problems.	In my estimation, this is a great paper overall. Well written, informative and innovative. The proposed method can be useful in the real world, shows improved performance over the state of the art and may inspire further innovative research.	"This approach is entirely novel - and as far as I am aware, this is the only group pursuing these ideas. While the use of eye-gaze has in the past been used as part of skills assessment for sonographers, this is the first time it has been used as part of a dataset to  train a network. The key point is the use of  gaze and probe movements as random variables to account for inter- and intra-sonographer variation. The underlying assumption is that a sonographer will react to the next image inferred from their hand movement on the probe, and conversely that the probe motion can guide gaze. To this end, the authors have developed a platform  ""Multimodal-GuideNet"" that observes  scanning patterns from a large number of actual obstetric scanning studies where  probe motion and gaze trajectory data have been collected along with the US images. The authors have shown convincingly that the model can generate real-time predictions to guide the operators, based on the probe  motion and gaze trajectory signals."	This paper explores a novel idea to leverage the gaze information to guide the US probe. The idea is nicely formulated in a multi-task learning framework where graph convolutional network is used.  The paper is scientifically sounding and also well structured and written. The experiments were thoughtfully designed with results clearly presented.	It is not completely clear to me if the paragraphs in the Methods section explain existing ideas that have been incorporated into the proposed system, or if they are also contributions. It would be nice if this distinction were made more clear.	In figure 4,  Multimodal-GuideNet* reports the error of the best generated gaze pointthat is closest to ground truth for the Multimodal-GuideNet procedure. It would be helpful to see this parameter reported for the Gaze-GuideNet procedure as well.	Not much weakness from my perspective, though the paper is a bit compact with a lot of details not well elaborated, but I guess mostly due to the page limit of the paper.	The dataset is not mentioned to be publicly available, but the methodology for data collection and for evaluation are very clearly documented.	No particular issues	The experiments design and data generation are clearly described in the paper, which matches what the authors claimed in the reproducibility checklist.	Hard to find room for improvement here, at least for me. I am not sure if there are any theoretical contributions (see previous comments), but the innovative approach already makes for a good paper on its own.	"Can the authors make some comment on the practical significance in terns of diagnostic accuracy of the improved error rate of Gaze vs Multi-modality versions of the network? It is not clear what the significance of the statement that ""the error of Multimodal-GuideNet* is within 10 pixels"" is."	In the experiment, all data were downsampled to 6 Hz. What is the rationale for it? Is it due to the computational constraints?	This work presents an innovative problem formulation, with measurable real world implications and that could be applied to further scenarios. I believe this idea deserves being advertised within our community.	THis is a ground-breaking example of the effective use of AI to facilitate the use of Ultrasound in under-resources communities. Very exciting work with an immediate practical application	The paper presents a novel idea with scientifically sounding formulation of the problem. The paper is also very well written and structured.
340-Paper0966	Multiple Instance Learning with Mixed Supervision in Gleason Grading	A mixed supervision Transformer is proposed based on the MIL framework for WSI classification/grading. The proposed framework suggest a novel manner to take advantage of both slide-level label and limited pixel-level labels. And a random masking strategy is proposed to avoid the performance loss caused by the inaccurate instance-level labels.	Propose a mixed supervision scheme for an improved gleason grading in pathology images. Propose a mixed supervision Transformer that utilizes the recent advances in NLP. Obtain superior results to other competing models.	Aiming at the Gleason Grading of WSI, authors propose a Transformer framework to utilize the slide-level labels and limited pixel-level labels under the MIL scheme. In particular, authors employ the superpixel techniques to convert the pixel-level labels into instance-level labels with improved quality. The random masking strategy and sinusoidal position encoding are also adopted to promote the Gleason classification.	A novel stragety to incorporte both slide-level and limted pixle-level labels. A random masking stragety to mitigate the effects caused by the inaccurate instance-level labels.	The authors proposed to use separate instance-level and slide-level labeling, which leads to a mixed supervision problem.  They proposed, so called mixed supervision transformer to mix the two labels and to improve the overall performance of the model. The experimental results supports the introduction of mixed supervision and Transformer.	The mixed supervision of slide-level label and instance-level label is an interesting idea in the field of digital pathology, which is reasonable to alleviate the lack of supervision for WSI classification. The work is built upon the state-of-the-art Transformer backbone.	Limited pixle-level labels are required for the setting. While the pixel-level labels might not be available. What would be time cost compared with just using the slide-level supervision.	By using mixed supervision transformer, the authors achieved the best performance on the SICAPv2 dataset but there was only a marginal improvement.  The improvement they obtained seems to be due to masking method, not due to the new design of their method, questioning on the novelty of the method. Hence, the effect of mixed supervision is not so obvious as considering the effect of masking.	Except for the instance label generation (Section 2.2), the novelty of random masking in Section 2.3 is very limited, which is quite similar to MAE [4]. The effectiveness of mixed supervision is not solid in the experiment. The experiments are performed on a small dataset, which is not fair for slide-level baselines. The algorithm should also be evaluated on a larger dataset comprehensively.	The combination of limited pixel-level labels with super pixels might cause challenges to reproduce. Suggest the code can be provided.	likely to be reproducible	Training hyper-parameters are complete The settings of backbone and position encoding are provided. The details of masking are missing. The details of limited pixel-level labels are missing.	In Table 1, there is one extra horizontal bar between ATMIL and TransMIL. And the results of ATMIL is very impressing, with the consideration that it only use slide-level supervision. Can the proposed framework also taking advantage of the strategies used in both ATMIL and TransMIL.	The authors used both instance-level and slide-level labels for Gleason grading, which is interesting. To generate labels, the authors adopted super-pixel, which is a reasonable approach. But, the improvement made by the proposed work is only marginal. It seems that the effect of masking is larger than mixed-supervision. The authors may provide an extended discussion on this matter.	"Although the idea of mixed supervision is interesting, its effectiveness is not impressive. In the ablation study, the performance improvement mainly comes from the masking and position encoding, rather than the mixed supervision. Without the masking, this work with pixel-level labels (92.67%) performs worse than SOTA work [9] using merely slide-level labels (93.73%). Except for the mixed supervision, the novelty of the entire work is not so adequate for the conference. In fact, simple masking strategy [4] and sinusoidal position encoding are widely used in computer vision. When datasets are small, classification methods may gain significant supervision benefits from additional pixel/instance-level labels, such as the SICAPv2 dataset with only 155 slides used in this paper. But the recent Gleason scoring dataset (e.g., PANDA [1]) has more than ten thousand of WSI, where slide-level labels are enough to train huge models. The authors should discuss potential limitations of the algorithm, and analyze whether significant supervision advantages can be obtained on advanced datasets. [1] Myronenko, Andriy, et al. ""Accounting for Dependencies in Deep Learning Based Multiple Instance Learning for Whole Slide Imaging."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021. Unclear meaning of limited in so-called limited pixel-level labels. Does it mean that only a small number of samples with pixel-level labels are collected, while most of the samples only have slide-level labels? The authors do not indicate how to extend the algorithm to the so-called limited cases, nor reflect such a setting in the experiment. The produced instance-level labels (not the predictions) should also be added into Fig. 2. This is critical to illustrate the advantage of the instance-level labels over the inaccurate pixel-level labels, as an important basis of the mixed supervision."	The authors propose a novel manner to make use of the limited pixel-level annotation to boost the WSI grading, and also a random masking stragety to train a mixed supervision Transformer, and obtain promising peformance.	Propose an interesting way of mixing two levels of information for an improved cancer grading. Still, some questions on the experimental results, though.	Although the idea of mixed supervision is interesting, the novelty of the entire work is not so adequate for the conference, where simple masking strategy and sinusoidal position encoding are widely used in computer vision. In addition, the effectiveness of mixed supervision is not so convincing as presented in the experiment.
341-Paper0404	Multi-scale Super-resolution Magnetic Resonance Spectroscopic Imaging with Adjustable Sharpness	To achieve multi-scale super-resolution, the author adopted a Filter Scaling strategy to obtain resolved results with different upscaling factors. The proposed network was conditioned on the weight of GAN loss and instance normalization to adjust the sharpness of the image. The experiments were evaluated on various metabolite types of 3D brain images from 15 high-grade glioma patients. And, given methods could achieve the best performance and have flexible operations in sharpness of the image.	This work develop  a multi-conditional module to incorporate multiple conditions into a MRSI super-resolution network that can avoid training a separate network for each combination of hyperparameters.	This paper proposed a blind super-resolution method for Magnetic Resonance Spectroscopic Imaging. It applied the filter scaling strategy based on the upscaling factor to realize the multi-scale super-resolution within a single network. Extensive experimental results demonstrated that the performance of the proposed model was comparable to the single SR model.	The organization of this paper is good, and the idea of this work is novel.  Proposed model could dynamically update the network with the different metabolites and be conditioned on the weight of adversarial loss to adjust the sharpness of the resolved image. Extensive experiments demonstrate that the proposed network could realize efficiency in training time and the number of parameters, and verify the effectiveness of adjustable sharpness. Besides, it achieves competitive performance compared with Hypernetworks and AMLayer.	The proposed framework can achieve comparable performance as the networks are trained under a single-scale setting. The developed model can  provide multiple levels of sharpness for each super-resolved metabolic map learn the super-resolution process based on the specific metabolite.	The idea of applying the filter scaling strategy for adaptive super-resolution was well-motivated. The proposed method considered the distinct spatial characteristics of different metabolites and proved to improve the performance of the model. The experiments were extensive and proved the effectiveness of each modulating module of the proposed model.	"Fig. 1 (b) is not clear for me. The function c(n) that outputs scaling factors(C_in x C_out) from n (input resolution) and E(m) should be deeply discussed. For instance, what is the dim of inputs of fc? Why outputs of fc could be acted as the upscale factors for filters (seems there missed a element-wise product.). Then, MCM conducted the next fc to represent \lambda into 2 x C_out in CIN. However, the \lambda is numerical scalar. The part of MCM is hard to follow and are not sufficiently motivation.These should be included in the corresponding ablation. What dose upscale factors for filters do? I don't find upsampling or deconvolution operations. What does ""as ground truth images contain missing pixels due to quality-filtering, we remove those pixels in network outputs ..."" mean? What the differences between quality-filtering ahead of network inputs and quality-filtering after outputs? It is strange for embedding 7 metabolites (text data) as the auxiliary variables in MCM. In Sec 2.2, E(m) is a trainable embedding layer that converts words to numerical vectors. The analysis on E(m) is excessively brief, not providing the complete description on specific operations, effects, sufficient discussions. This affects the quality of writing and the clarity of the presentation, also, scores. 3.""The adversarial loss uses a discriminator (4-layer CNN)"" , Whether Wasserstein GAN will be trained alternatively together with proposed networks. If not trained, there should lead to a mistake. In this time, how Wasserstein GAN train a 4-layer CNN without a fc layer. ""statistically insignificant p-value "" don't have illustration and reflect on the specific results. I can't follow its meanings."	"1: The idea of the paper that super-resolve the image in multi-scale manner is not sufficiently new. There even have been some works arbitrary scale super-resolution, such as ""Learning A Single Network for Scale-Arbitrary Super-Resolution"", ""Arbitrary Scale Super-Resolution for Brain MRI Images"". 2: The experiments are performed only on H-MRSI dataset, but the number of images are too small to obtain meaningful training results. There are only 15 3D samples. The authors are encourage to estimate the proposed model on another dataset. 3: From Table 1, the proposed method doesn't improve significantly than the unconditioned method but consumes more time and takes more memory for the parameters."	The numerical results are not so significant compared to the existing multi-scale methods. The visual comparison to existing blind SR methods should be also included.	Experimental data process depends on the professional tools (LCModel v6.3-1 and FSL v5.0) so that maybe missing the required details for reproducibility of the paper.	I am not so sure for the re-producibility of the paper. The code and the data are not available to aid reproducibility. However, the convergence of the proposed model is not discussed.	The paper gives a clear and detailed description to the experimental details.	"Please see Sect. 5. Besides, there are a few typos and errors: 1) Introduction states ""Furthermore, the adversairal loss was incorporated to ...."", was -> is. The tense of the related work.should be consistent. 2) ""such that S best approximates the high resolution ground truth ... "", the text below the fig. 1 could't be followed. 3) In Sec. 2.2, ""where m is the type of metabolite"" should be ""... types ..."". 4) In Sec. 3.3, ""To ensure fairness, we set the same layer number and latent size"" should be ""... numbers ..."", and ""... sizes ..."". ... Please check the overall paper carefully."	1: The writing of the paper can be improved. The description of how the proposed method can deal with the multi-scale input is unclear. : The utility of the T1 and FLAIR images is confusing. The author doesn't demonstrate the effect of the T1 and FLAIR images. Have the authors studied on using only LR Met images? The ablation studies are insufficient. For example, the ablation study of the pixel loss, structural loss and adversarial loss to explore the effect of different loss function.	As the proposed method was a multi-modal based super-resolution model, the author could discuss the influence brought by the different images in the proposed method. While Fig. 3 and 4 showed the relation of the sharpness to the parameter lambda, the effect of Equation (5) was still unknown. The visual comparison to the current blind SR method could be provided.	Based on the novelty and good organization, also, the insufficient comparisons and lacking of some details.	This work is similar with published work.	I suggest acceptance to this paper. The idea of using the scaling filter for blind SR is well motivated. The overall approach is clear and easy to understand. The experiment is aslo extensive to prove the effectiveness of the proposed method.
342-Paper1789	Multiscale Unsupervised Retinal Edema Area Segmentation in OCT Images	The authors present a novel Multiscale Unsupervised Image Segmentation (MUIS) framework for the Retinal Edema Area (REA) segmentation task. It has two steps. 1: the image-level clustering groups the images in two categories. This provides guidance for the downstream segmentation task. 2: The pixel-level segmentation yields pixel-wise labels for each image. Results are very good and approach the supervised approach.	In the manuscript, the authors have proposed a novel Multiscale Unsupervised Image Segmentation (MUIS) framework for the Retinal Edema Area segmentation task. Based on the observation that smaller lesions are more obvious on large scale images with detail texture information and larger lesions are easier to capture on small scale images for the large field-of-view, they introduced multiscale information into both stages through a scale-invariant regularization and a multiscale Class Activation Map fusing strategy, respectively.	The authors proposed a 2-stage network architecture for unsupervised segmentation of retinal edema. In the first stage images are classified as normal/edema using DCCS. In the second stage, multiscale CAM fusing strategy is applied to guide segmentation for an encoder-decoder network.	clearly written good results related work relevant experiments well performed	Overall, the manuscript is well written and well presented. The design of the framework in two stages has been effective, as well as the incorporation of multiscale information in both stages. The quantitative analysis on the public retinal dataset demonstrated the superior performance over state-of-the-art unsupervised approaches.	The authors introduced scale-invariant regularization in image clustering, which is considered novel since the proposed network outperforms the SOTA substantially.	weights in Eq 6 unclear.  extra examples in supplementary material would have been nice.  'failures not discussed (some 'no's in the reproducibility checklist) so open questions remain, see 8.	The design of the proposed architecture is heavily reliant on the existing DCCS approach. Section 2.1 consists of excerpts from the original DCCS study and hence, not a contribution of this manuscript. The main methodological contributions of the manuscript are actually presented in Sec 2.2 and 2.3. There are too many grammatical mistakes throughout the manuscript, which is affecting the readability.	Formulation for the main novelty, which is scale-invaraint regularization for clustering, is not properly explained. More experiments should be performed on lesion types of significant scale differences to validate scale-invariant capability of the network. Failure analysis missing.	no concerns	The contributions presented in the manuscript should be possible to be reproduced by modifying the base DCCS architecture. The authors have also claimed to release the code after review.	The work might not be reproducible because the AI challenge dataset is no longer available to the public.	"192 and 96 /192 seem to be a bit arbitrary   - why these? would e.g. 128/192 or 128/256 have made sense? Fig 2: It seems that the GT has a small layer at the bottom ""extra"" compared to MUIS - nnUnet has it. Any reason for this? Is this layer an artifact, or essential? is MUIS focussed on elevation? Any reason for answering sometimes 'no's in the reproducibility checklist?"	Along with the comments presented above (Q#5), I would like to ask the reason behind setting the number of training epochs at 20. This number seems quite small, and I would suggest to incorporate an ablation study over varying number of epochs.	"The abstract is not concise enough. Mechanisms of the two stages should be summarized in the abstract to bring out the novelty. Is the proposed network only applicable to edema? Edema is considered as a lesion of relatively large scale. How about other types of lesions of very different scales, such as hard/soft exudate and hemorrhage? Do they also benefit from scale-invariant regularization? More explanation is needed for eqn 5. What is R^(-1)F(R(X))? Do the modified DCCS obtain images of multiple scales? How many different scales? What is R^(-1)? Do you scale the features back? Why so? Since this is considered the only novelty of the proposed network, more explanation is needed. How scalable is the network if multiple lesion classes are considered? In a real clinical setting, a diseased image often contains multiple lesion types. How much degradation is expected on a dataset of this nature? Based on table 3 for ablation study, the most noticeable improvement comes from si. The improvement from ms is not considered significant. For this reason, I consider scale-invariant regularization the only novelty of the proposed network. In addition, multiscale fusion of CAM is a necessary step for combining CAMs from multiple scales, not something considered a novelty. How is ""MUIS w/o ms, ac"" done? Which CAM is selected for segmentation? The authors included a section explaining the original DCCS, but should avoid copying the words directly from the original paper, e.g. ""category-style latent representation in which the category information is disentangled from image style and can be directly used as the cluster assignment."" Failure analysis missing. What are the data sizes for training, validation and testing? Are all 85 OCT volumes involved in training? The validation process has to be clarified."	looks ok, but I see some open issues	Overall, the manuscript is well written and well presented, and the experimental analysis demonstrates the superiority of the approach over related state-of-the-art methods. However, the proposed architecture is heavily reliant on existing DCCS approach, making the theoretical contributions limited.	Since unsupervised segmentation has not yet gained enough attention from the research community, this paper does present an interesting network combining multiscale unsupervised clustering and segmentation to achieve unsupervised segmentation. However, clarification is needed for the main novelty of the work and the validation process.
343-Paper2517	Multi-site Normative Modeling of Diffusion Tensor Imaging Metrics Using Hierarchical Bayesian Regression	This paper evaluated, under the normative modeling Hierarchical Bayesian Regression framework, three model fitting strategies for the age effects on brain white matter microstructure across the lifespan. Using multi-site diffusion tensor imaging data, the authors found that compared to the linear fit, the polynomial and b-spline fits were better for modeling age trajectory. The authors further found that compared to the linear model, the b-spline model resulted in fewer ROIs with significant effect of a rare neurogenetic syndrome on microstructural brain differences, suggesting modeling complexity can impact statistical findings and therefore must be determined with caution.	This work aims to the comparison of three harmonization models, linear, polynomial, and b-spline in a large-scale dataset.	The authors test the usage of Hierarchical Bayesian Regression for multi-site imaging data, to adjust for site. They experiment with linear, spline and polynomial models for age and sex, since the distribution of these can be confounded by site. They compare the models in terms of associations that they can find, where they compare micro brain structure measures of controls and carriers of a rare deletion on chromosome 16 (16p11.2).	A novel application of the normative modeling Hierarchical Bayesian Regression framework in modeling age effects on white matter diffusion tensor imaging metrics. Strong evaluation of different model fitting strategies under the HBR framework, illustrating their impact on 1) modeling age effects in multi-site brain DTI data, and 2) subsequent hypothesis testing based on the fitted outcomes.	This study uses a large-scale dataset consisting of multiple site datasets, provide a more sensible explanation. Also, the work provides new insight into the detection of rare genetic copy number variants, noting how model complexity can impact findings.	The paper is generally well written, It is work on exciting ensemble of data in relation to clinically relevant metrics. The authors are not creating a new method, but rather trying to understand the impact of modelling choices on downstream analysis. This is important work that has a direct relevance for applications in this domain. The major finding of the paper is that it surely matters how you model age in your data. This is very often overlooked, but yet a very important thing to investigate. Nonlinear age effects can be pervasive in biological data. The authors do a good job of describing the dataset and the way they process and model their data. They provide clear references to methods used and clearly cite the source of all software that they use in their analysis. The visualizations are ok, and the statistical comparisons are mostly good.	"Normative modeling (https://dx.doi.org/10.1016%2Fj.biopsych.2015.12.023) and hierarchical bayesian regression framework are not particularly novel. Some aspects regarding the clarity and organization needs to be improved significantly. A general introduction about the Hierarchical Bayesian Regression framework needs to be provided first, followed by mathematical description in Methods. Texts introducing about 16pDel still appeared in the Methods section which should be moved to Intro. All the ROIs from the JHU atlas were provided in abbreviations without being fully spelled out in their first occurrence. Lastly, at the end of Results, it was stated that ""For additional data, please see the Supplements,"" but no Supplementary materials were found."	The motivation and evaluation of the study are limited in most clinical applications. The explanation from the results about genetic-driven neuroscience seems a bit far-fetched.	"There are some weaknesses worthy to point out: 1) The language is generally good, but thee re some typos, e.g. ""Sire"" instead of ""Site"" in the first row of table 1. Missing space in Site4 in row 4 of table 2. There are a few more issues like this that need to be fixed. Please use a spell checker tool or something like grammarly.com to fix this. 2) In the 3rd paragraph of section 2.2, the authors claim: ""Consequently, the algorithm does not yield a set of ""corrected"" data, i.e., with the batch variability removed, as in ComBat. It instead preserves the sources of biological variability that correlate with the batch effects."" Firstly, I would not call this an algorithm, but a model. If the biological variability correlates strongly with batch effects, then I doubt that you will be able to preserve them. 3) Links to software should be in footnotes, e.g. PCNtoolkit. 4) The paper suffers a bit from lack of novelty. This is mostly applying known and published methods to a new dataset, trying to solve a problem that many have attempted before. 5) Captions of images are lacking. There are so many acronyms, these should be spelled out in the caption, e.g. fig 1. 6) The y-axis numbers in Fig 1 should be numerically sorted, not alphabetically, the order is not 1,10,2,3,4...., 10 should appear at the end. It is not explained in the caption what the colors in the image mean. 7) Fig 2 also needs acronyms spelled out."	Sufficient details have been provided regarding the datasets. For code, it's stated that PCNtoolkit (https://github.com/amarquand/PCNtoolkit) was used which is a publicly available package, but it is unclear whether the specific codes for the data analysis in this work were part of the package or will be made publicly available to enable reproducing the results.	It should be easy for authors to provide source code for reproducibility analysis.	The reproducibility report seems to be honestly filled out.	"Per suggestion above, please work on improving the clarity and organization of the paper. 1) A brief introduction should be provided in Introduction about the Hierarchical Bayesian Regression framework for general audience. 2) Introducing 16pDel at Methods is deemed unnecessary. 3) All the abbreviations for the ROIs should be fully spelled out in their first occurrence. 4) Provide Supplementary materials. Page 2: Citation(s) are needed for the statement ""These deletions increase the risk for a myriad of neuropsychiatric disorders, including neurodevelopmental delay, autism spectrum disorder and attention deficit hyperactivity disorder."" Table 1: Fix typo for ""Sire 1"" Fig. 3: The texts are hard to read. Also I was wondering whether the analysis for the polynomial fit was performed and may be shown (if not, should justify in the texts more clearly), to compare the three fitting approaches."	Unclear why the study involves the evaluation of rare genetic copy number variants. The paper seems to be poorly prepared and very rough, e.g., equation, table and reference. Although the authors used HBR normative modeling to infer the distributional properties of the brain's WM microstructure based on large datasets from healthy subjects, there is lack of results in age analysis, or across lifespan. The discussion about the association between research motivation and genetic-related analysis is underexplained.	Investigate the points listed in the main weakness comment.	The reasons that I lean towards a weak acceptance is that this work presents an interesting application of normative modeling in hierarchical bayesian regression framework, with strong evaluation of the different model fitting strategies. The results clearly show the utility of this framework for evaluating the feasibility of analysing (and interpreting results of) neuroimaging data pooled from different clinical studies, a known challenge in data harmonization. The merits slightly weigh over the weakness that this paper may address in its revised form.	Lack of preparation. The manuscript is poorly organized. Interpretation of results needs to be improved	This is a decent dataset and relevant problem to tackle.
344-Paper0613	Multi-Task Lung Nodule Detection in Chest Radiographs with a Dual Head Network	The paper proposes an improved architecture for nodule detection. The implementation of multi-task in the architecture improves both performance on classification and detection. The dual head structure is reflected in both the overall architecture and a data augmentation approach.	A multi task pulmonary nodule detection algorithm for chest X-ray analysis is proposed.	The authors propose a multi-task lung nodule detection algorithm to reduce the false-negative rate in this manuscript. The dual-head network (DHN) is proposed to predict the global-level label of nodule presences as image-level prediction and the local-level label of precise locations. A dual-head augmentation is proposed for DHN training. The experiments on the NIH dataset demonstrate the effectiveness of the proposed model.	The paper has been well laid out, clearly presented, and with improved results. Ablation study has been conducted to study the effect of the proposed modifications to the original Faster R-CNN approach.	The authors use a double headed network to predict the presence of a global label indicating the presence of nodules, and use a local label to predict the location of nodules,In addition, the authors propose a new double headed enhancement (DHA) strategy, which proves the importance of DHN in further improving the prediction of global and local nodules.	The paper is logically organized and very easy to read. The paper proposed multi-task lung nodule detection to help the false negative detection in chest radiograph analysis. By learning global case-level prediction and local detection simultaneously, both tasks show performance improvements. Dual-path augmentation is novel as the global and local paths may need to adopt different augmentation strategies.	In multi-task training, there is an assumption that the training data contains images that may or may not have nodules. For those without nodules, the second task on nodule detection will not contribute to the model training. If most images do not contain nodules, what influence will it has on the overall model performance?	The method selected in this paper is resnet-18. The author should prove why resnet with 18 layers is selected in the experimental part. In addition, there are few methods compared in the experimental part.	The reason for properties of pulmonary nodules need adopting deformable convolutions is unclear How to use the two results is unclear. The experiments comparison with state-of-the-art on pulmonary detection performance is not shown.	Reproducibility of the paper is high.	Although the author does not give the source code, the network model is not complex and has certain reproducibility, but because all the experimental parameters are not fully explained, the reproduction results may be biased. Of course, it's best to hope that the author publishes the source code.	The source code will not be made available. The paper provides enough details to reproduce the paper.	It will be interesting to have a discussion on extending the proposed DHN and DHA to another stream of object detectors, namely the YOLO family of models.	It is suggested to add more experimental parts and describe the implementation details of the model as clearly as possible.	"Major: 'Considering the properties of the pulmonary nodule, we propose to use deformable convolutions [6] and the gIOU loss' - this needs more details of what properties of the pulmonary nodule lead to the use of deformable convolutions. The authors mentioned: 'Applying deformable convolutions allows more dynamic focus on particular regions in the image where the size of the nodule might be small .' We know that FPN can help leverage the nodule of various sizes, but how can the deformable convolution help detect the nodules of small sizes? The dual-path network contains global classification and local location prediction with a confidence score. Is it possible for the framework predicts several bounding boxes from the local path but with the negative prediction from the global path, or positive predictions from the global path but no bbox generated from the local path? How to deal with this?  Generally for the nodule detection, we have a prediction of the lesion and the corresponding location. What is the exact output of this network, and how to use the result of this paper to guide clinical practice? In the experiments, this paper compared the CONAF [19]. Why are only the classification metrics shown for CONAF? Additionally, in the introduction, both references [13] and [19] are discussed, so what about the performance compared to Li et al. [13]? It is reasonable that the author compares the performance with similar attempts in the experiments by comparing the CONAF [19] in Table 1. However, by reviewing the purpose of this paper - 'identify pulmonary nodules on chest radiographs', should this paper also compare the nodule detection performance with the state-of-the-art methods [1, 20, 26] (global) or [10,12,15,17,23] (local) cited in the introduction? Minor: A typo in ""default setup 'utilizs' a Smooth L1 Loss "" of the last paragraph in section 2 on page 3. It is recommended that Table 1 be placed near the corresponding text on page 7."	This paper is well presented, organized, with enough details for reproduction. The proposed contributions are supported by experimental results. Ablation study is conducted to study the effects of proposed improvements. A nice paper for reading.	In this paper, multi task detection of thoracic pulmonary nodules is a good work. Dual head network is used for more effective detection on the model. However, the detailed description of the method is not clear enough, and the experimental part is not perfect.	The paper provides a novel solution to improve the detection accuracy by adopting the case-level classification network with the lesion-level detection network. Instead of using lesion-level detection results, the authors provide an alternative way to implement global classification results. The performance also shows the improvement in both the classification result and detection results.  However, there are some explanation needed for the design of this network and the design of the experiments.
345-Paper2559	Multi-task video enhancement for dental interventions	This manuscript presents a multi-task network architecture that handles three tasks related to dental interventions, which are video enhancement, binary teeth segmentation, and homography estimation. Authors show that these tasks are correlated with each other and the overall performance of the proposed architecture on multiple tasks is comparable to that of state-of-the-art methods designed for a single task.	In this paper, a new dental video enhancement dataset is proposed, accompanied with a strong benchmark model. The dataset are tailored for 3 tasks: video restoration, segmentation and homography estimation. The proposed multi-task model is evaluated on all 3 tasks and compared with other methods.	The authors propose a novel deep network for multi-task video enhancement that works in near real- time for macro-visualization of dental scenes. The authors also release a new dataset of dental videos with multi-task labels for facilitating further research in video processing applications.	The paper is well organized and written. The figures are very well made, and the method design and experiment setup are well described with sufficient details for readers to understand the work. The multi-task architecture design seems interesting and reasonably thoughted, and the authors demonstrate the comparable performance of the proposed method against some state-of-the-art works on several tasks.	(1) First dataset of dental videos with multi-task labels. In this work, a new application of macro-camera for dental is proposed.  (2) A solid multi-task benchmark model for 3 important dental video tasks. The video restoration is important for deblurring and noise suppression. Segmentation could be used to assist doctors. And homography estimation could be used for video stabilization.	The paper is well-written, easy to follow and addresses an important issue. adequate amount of literature review, experiments have been done.	The evaluation results are not impressive enough in a couple of aspects.  Firstly, the ablation studies show that the performance degradation after disabling one module is often not significant enough, which makes me wonder if the architecture has created enough synergy among these tasks. Secondly, based on the results, it looks to me that simply having several state-of-the-art models run in parallel with the original image input for processing could result in higher FPS and similar or better performance compared with the proposed method. The authors also mention that MHN on the original video frame works better than the proposed method and will become significantly better if the high-definition image is used instead. All the above may suggest that there is still room for further improvement in the proposed method.	"It seems the restoration here is with an emphasis on video deblurring, while the author is also encourage to have some brief discussion on video stabilization, as they are closely correlated and discussed together. Actually some blurs are introduced by the camera jittering. The stabilization is related to homo-estimation and could be viewed as a smoothing task using estimated transformation.  reference: Yu, Jiyang, and Ravi Ramamoorthi. ""Learning video stabilization using optical flow."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2020."	Since the authors indicate that the method runs in near real-time, any indication about the computational time and space with respect to the current state-of-the-art would be useful Model is very task-specific; any indication if this can be used for other similar application would be useful.	This work is not reproducible because neither source code or dataset will be made available.	No explicit signs about whether the dataset will be public available and the code will be released or not.	No code/dataset link has been provided	"For Fig. 1, can the authors explain a bit why Fig.1 is equivalent to the architecture in Fig. 2?  The traveling path of Fig.1 is a bit confusing, and I would like to know the path of information flow from input to the final output. There are some inconsistencies regarding the uses of subscripts and superscripts for several symbols. For example, the same symbol O has different layouts of subscript and superscript at the 6th line of Sec. 6, Eq. (1), the 2nd and 8th  lines in ""Problem Statement"". Similarly, for symbol B, the appearance of B in the 4th line in ""Problem Statement"", Eq. (2), and the 2nd and 3rd line in ""Training"" are different. For the last two lines in ""Encoders"", in Fig. 2, H has subscript t-1 -> t, while the symbols in those last two lines do not have them. Please carefully go through the text and make the style of all symbols consistent. If there are different ways of using the same symbol, please consider adding some more notes to explain to the readers. Fig. 2 is a bit confusing because the symbols are in the same row as the modules and at some places, the arrows between two modules are not shown. Please consider making the data dependencies and information flow clearer. Fig. 2, in terms of the method design, why do authors choose to use f_t^{1, 2, 3} as one input to produce h_t^{1,2,3} instead of the feature maps after the channel attention? It looks like for the other two tasks, the feature maps after the channel attention are used instead. Fig. 2, why do authors only upsample the binary mask output to the finer-scale level? Why not also upsample the deblurred image as input features? For Sec. 3, ""Noise, blur, colorization"", is only camera C_2 used for this frame-to-frame training. Is camera C_1 also involved? Fig. 3, are images from C_2 only used for color mapping training? Are those not directly used during image deblurring training other than this color mapping training?"	Overall the paper is in good shape and more discussion on how these 3 tasks are related is appreciated.	Please see Weakness section	Evaluation results are not strong enough, suggesting the design of the multi-task architecture has room for further improvement. No dataset or source code will be available to the community.	Based on the contribution of new dataset and strong benchmark method, I make the rating.	Please see Weakness section
346-Paper1336	Multi-TransSP: Multimodal Transformer for Survival Prediction of Nasopharyngeal Carcinoma Patients	Use multimodal network to combine CT image data and text data to predict patients' overall survival. Demonstrate the effectiveness of Transformer model in this task	The authors apply a transformer and CNN to predict the overall survival from nasopharyngeal carcinoma patients from CT images. The proposed method is compared to competing techniques on a private dataset. The benchmarked method also include non-imaging methods based on clinical reports (text).	This paper tackles the task of predicting survival in nasopharyngeal carcinoma patients, by effectively utilizing the information from the CT images and the clinical text data. To do so, the authors propose a novel multi-modal architecture which leverages the feature extraction power of convolutional neural networks and the the feature fusion ability of transformers. The authors demonstrate the efficacy of their proposed model on a small in-house dataset.	A architecture to incorporate both CT image and text data to predict patients' overall survival New state-of-the-art performance on the task.	Broad benchmarking even including text Well-motivated task Related work is clearly discussed Figures are well-prepared	The paper tackles an important problem of fusion between text and imaging data, which is commonly overlooked. Most works focus on fusing the information between different imaging modalities, or imaging and genomics modalities. The paper employs the transformer model to perform the fusion. Given that the transformer model has been shown to work very well in other contexts, it is natural to try this model for key tasks with medical data. The paper provides an elaborate ablation study to evaluate the model.	The paper needs more discussion on the results (e.g. good/bad examples by case studies, interpretability of model's decision, how the model can be further improved) to help interpret the impact of the work and interpret what 0.02498 MSE and 0.6941 CI actually mean in terms of quality. The novelty of the work is limited to applying Transformer and multimodality on a new task, which is ok if more clinical insights can be provided to interpret the model performance as mentioned above.	Demographics and details about the non-imaging features used should be added The writing would profit greatly from a thorough grammar checking.	"The details of the use of the transformer model are inadequate. For instance, from the description of the input data in section 2.2, it is unclear what the exact ""sequence"" is for the input to the transformer. Is it the different feature channels for the same voxel, or the different z-slices of the image? How are the text embeddings included in the input? Where does the expanded text feature join the imaging features? What is N in the ""N sequences""? What is a space embedding? Why is it needed? How is it constructed? How does it capture the space? Is this a trainable embedding, or a pre-defined embedding? Why do the authors make use of mean-squared error loss for predicting survival instead of the more commonly used Cox-based losses such as those used in DeepSurv? The MSE loss does not take into account the censoring. Is the y_i used in the MSE loss corresponding to the time of death/last-observed?"	The paper uses in-house dataset which introduces concern on reproducibility.	The dataset demographics and non-imaging features needs clarification, otherwise it is reproducible.	The experimental details are sufficiently provided in the text, and the authors indicate that they will make the corresponding code available. However, they will not make the in-house dataset used in this study publicly available.	"Given the limited number of patients (only 384), the paper may consider cross validation. Is overall survival defined by duration in terms of weeks, days, hours or minutes? The ""text data"" (age, BMI, dose) is either numerical or categorical, rather than free-text. Strictly speaking, it should be called structured data rather than text data."	If you did not observe death for a subset of patients and treat the time to the last follow-up as the OS time: Do you check for patients that had successful treatment and were therefore not showing up any more? For the error you state (e.g., MSE), please indicate the unit (years, months, ...) Please indicate how the segmentations were obtained (e.g., manual, manually by an expert, (semi-)automated method, ...) Space embedding: I do not understand the self-learning part of the spatial matrix, since I assume the location information is a value you can simply retrieve from the slice position? Please add more details about: Demographics information of your data The text features you used, since it contains relevant information for the target task	The paper is overall well-structured. Details of the method can be improved as mentioned above. In particular, since one of the key novelty lies in the use of the transformer, this part of the model needs to be explained in good detail. Discussions on why the particular input to the transformer model was chosen is also essential. Why was the particular 2D expansion of text chosen? Has this been shown to be effective in earlier works? The paper will greatly  benefit from obtaining performance results on more datasets (e.g. other types of cancer) or different data splits of the same data. It is really hard to compare and contrast different methods based entirely on one run of one-split of the dataset.	The effectiveness of the proposed model is clear though more clinical insights will be helpful.	The experimental setup is sound, and the ablation study clearly shows the contributions of the individual elements. I am missing key information about the data and other used features that this research can be reproduced.	Overall, the paper presents an interesting approach to solve an important problem in multi-modality fusion of imaging and text data. Further evaluation on more datasets (e.g. of other cancers, or on different splits of the same data) could greatly help in understanding the true advantage of the proposed method.
347-Paper0568	Multi-view Local Co-occurrence and Global Consistency Learning Improve Mammogram Classification Generalisation	The paper cover multi-view detection of mammographic abnormalities. The method uses local and global information based on standard radiology assessment. Evaluation is based on three different datasets and uses global labels.	The paper introduces the Multi-View local Co-occurrence and global Consistency Learning (MVCCL) to consider features from ipsilateral mammographic views. The authors introduced Global Consistency module penalizing the differences in feature representation from the two views and also local co-occurrence learning module that uses multi-head attention to produce a representation based on the estimation of the relationship between local regions from the two views.	This paper presents a multi-view local co-occurrence and global consistency learning for mammogram classification generalization. It proposes global consistency module and multi-view local co-occurrence module to aggregate the information from two views of a breast. The generalization of the proposed approach was evaluated on four datasets, including 1) testing subset of ANON 1, 2) ANON 2, 3) CMMD, and 4) InBreast, and the results demonstrate the promising performance.	There is novelty in the local aspects and the specific application area.	1) The idea of using both view of mammogram through global Consistency and local occurrence is interesting and methodologically sound.  2) The authors proposed using global consistency module to enforce similar representation of both view of mammogram is very applicable since the important features for diagnosis should be available in embedding of both views. 3)  The multi-head attention modules is correctly used to derive interactions between samples from both views. 4) The authors provided good comparisons and ablation study to show the performance of their model.	This study considers both local and global information from two views for mammogram classification by designing global consistency module and multi-view local co-occurrence module. The generalization of the performance of the proposed approach was verified by four testing sets.	Lack of repeated n-fold cross validation. Lack of discussion on the performance.	"1) In table 2, the first two columns are named ""endtoend"" where there is no citation and I am not sure what the authors are referring to.  2) In equation 4, the authors didn't specify The query, Key and value vectors and left them in general format. I suggest that the authors mention specify the Q, K, V vectors. 3) There are a lot of grammatical errors such as  generalisation-> generalization right hand size of the image -> right hand side optimiser -> optimizer area under the precision-recall curve (AUC-PR) -> the area .... 4)  Provide additional details regarding the implementation of the model so the model can be reproduced."	There are minor issues as follows: What is FN in Fig. 1? It would be better if the detailed structure of each component is provided, including that for global consistency module, multi-view local co-occurrence module, and MLP.	Okay.	The authors declined to provide the code and the value for hyperparameters is not mentioned in the paper so the result is not reproducible.	The detailed structure of each component is required for reproduction. Two private datasets are used in the experiments.	"Would swapping the CC and MLO provide the same results? It seems only the projection aspect might make a difference. Could this be translated to other application areas. It seems most other applications don't have similar views, so the application area might be narrow. Could full n-fold cross validation results be included. I would suggest that the term ""significant"" is only used if relevant p-values indicate this. It would be good to see a detailed discussion on why the authors think their model preforms that much better that existing SOTA approaches."	1) Provide additional details regarding the implementation of the model so the model can be reproduced. 2) Fix the problem mentioned in the weakness 3) There are a lot of grammatical errors such as  generalisation-> generalization right hand size of the image -> right hand side optimiser -> optimizer area under the precision-recall curve (AUC-PR) -> the area ....	It would be interesting to demonstrate the gap of generalization. That is, for a dataset A, compare the performance of models trained by the dataset A with that trained by ANON 1.	Novel aspects, narrow application area, discussion could be more extensive.	The paper is novel and provide extensive comparisons showing the superiority of the proposed model in comparison with the previous works.	The writing is clear. The idea of using global and local information from two views is interesting. Particularly, a global consistency module and a multi-view local co-occurrence module were proposed, which are motived by the well-known transformer. The generalization of the proposed approach was evaluated on four datasets, including 1) testing subset of ANON 1, 2) ANON 2, 3) CMMD, and 4) InBreast. The experimental results demonstrate the effectiveness of the proposed approach.
348-Paper0534	MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep Models for X-ray Images of Multiple Body Parts	From the title of this paper, we can imagine the topic of this paper is to make self-supervised learning, continual learning and multi-task learning for deep X-ray image classification. The proposed method aggregates Xray images collected from different body parts for MoCo-based representation learning with continual learning (CL) procedure for X-ray analysis tasks.	"This paper proposes a self-supervised training method of pre-train models for X-ray images, which is named MUSCLE. The method consists of pre-processing, pre-training, continuous learning and fine-tuning steps. MUSCLE adopts MoCo method to train the backbone network from multiple data sets and continuous learning to avoid over-fitting and ""catastrophic forgetting"". In the results section, detailed experiments demonstrate the effectiveness of the training method."	MUSCLE is proposed to pre-train DNNs for X-ray images of multiple body parts for multiple tasks (classification, segmentation, detection) using self-supervised(SSL) and Continual Learning (CL) techniques. The pipeline has 3 stages :  1) MD-MOCO uses 9 xray datasets to modify MoCo-CXR to SSL pretrain a backbone DNN after preprocessing the data,  2) Continual learning is applied to further pretrain the backbone with task specific heads in a cyclic fashion across 4 tasks. Only 4/9 datasets are used from this stage onwards,  3) Independent task-specific fine tuning with 4 datasets is done by having both the backbone and 4 task specific heads. The experiments have 4 baselines named - Scratch, ImageNet, MD-MoCo, and MUSCLE- , to compare against MUSCLE. Task specific performance metrics are used for the comparison. MUSCLE is shown to have better numbers on many of the metrics.	As the authors claimed, the multi-task learning,  self-supervised learning and continual learning are jointly unified in one learning scheme. The overall learning paradigm is clear and the authors employ many tricks to improve the accuracy of the prediction on X-ray images. Many experiments validate their claims on different datasets.	The innovative point of this paper is the extension of MoCo framework into multi-dataset training with cyclic and reshuffled learning schedule and continuous learning strategy. It uses multiple datasets pre-training to solve the heterogeneity problem and uses continuous learning to tackle catastrophic forgetting.	The use of 9 publicly available datasets makes it possible for others to compare against. Evaluation of MUSCLE shows improvements for many of the reported performance metrics.	Generally, I have to say that just confirming very high accuracies is not the reason of accepting a top conference paper. The authors made a lot of experiments to show the superiority of the proposed method, while the contributions of this paper are a simple combination of existing well-known learning schemes. The authors only made some simple modification on two resi-net networks. The authors are hard to figure out why is it useful when combining the self-supervised learning and continual learning for multi-task tasks. For multi-task problem, using different datasets or collecting parts from one body is hard to validate its effectiveness. If possible, the authors need to compare your work to multi-view or multi-model learning from theoretic and experimental perspectives.	However, the main contribution of the paper lies in training pipeline innovation rather than training algorithm innovation. The experiment section just lists the results. More analysis is need to discuss about the inherent reasons for the results improvements.	"-Minor extentions and modifications to existing SSL and CL methods  are proposed in this work and applied to xray images. -7/9 are chest xray images. Only 2 datasets are for other body parts. The ""multi-dataset"" claim is much narrower than it seems, because there is less diversity in the input data. -The performance is evaluated only on 4 datasets. Why is the evaluation not done on all the 9 datasets that are used in the first stage SSL?  -Imagenet and Scratch are not strong baselines to compare against. ImageNet has RGB natural images, while this paper is dealing with grayscale xray images."	There are lots of detailed information that is neccesary to be realized for this work, because they employed three learning schemes for one well-testified task.	The reproducibility of this paper is good, the algorithm is relatively easy to implement.	The paper has descriptions on some of the hyperparameters (learning rate schedule, weight decay, DNN architectures) and training methodology that could be useful in reproducing the results.	See weakness.	The result section can be enriched with more in-depth discussions of the results, rather than simply listing the results. The column widths of Tables 2 and 3 need to be adjusted for clearer display. The texts in Figure 2 are too small.	"-As accepted by the authors, MUSCLE is proposed as a ""proof-of-concept"". It is not optimized for any single task and no evaluation is done to compare against other methods in the literature that could make it clinically useful.  -Since the proposed framework is quite elaborate with many components, it would very helpful to make the source code public."	The overall learning scheme is a simple combinition, while the experiments are very good.	The innovativeness is moderate, the method is effective and results are convincing.	This is a well-written paper that explains the method clearly. The paper proposes a framework that makes minor modifications to existing SSL and CL techniques and shows improvements on most of the evaluation metrics.
349-Paper0974	NAF: Neural Attenuation Fields for Sparse-View CBCT Reconstruction	This work adapts the Neural Radiance Fields idea for 3D reconstruction to the clinically relevant CBCT modality. A key element is a recently proposed network architecture which includes a learning-based encoder. Overall, the strategy resembles an iterative CT reconstruction, where the reconstructed 3D function is represented as a neural network. The method is evaluated on both simulated patient data and measured phantom data. It is compared against multiple state-of-the-art methods and performs competitively.	This paper proposes to learn attenuation coefficients in CBCT via an implicit function parameterized by a fully-connected deep neural network. A learning based hash coding method is utilized to help the network capture high-frequency and edge details of human organs.	A self-supervised model for sparse CBCT reconstruction is proposed.	To my knowledge the first paper which shows a variation of a CT reconstruction algorithm inspired by Neural Radiance Fields on measured data and in the practically CBCT acquisition geometry. Very good presentation, easy to follow. Also very good graphics to illustrate the method. Well prepared and fair evaluation against many competitive methods.	The organization and presentation is clear. The proposed neural attenuation field extends the application of implicit function for CBCT reconstruction. A tailored solution of hash encoder is adopted for position encoding for human organs. Experiments demonstrate promising results of this proposed method in terms of reconstruction quality and computation cost compared with iterative based methods and another baseline of implicit function based method.	As claimed by the authors, no external data or priors is required using the proposed self-supervised model.	The clinical need for low-dose CT using sparse-view acquisition is not very pressing. Currently the motivation for this technology is driven by advances in reconstruction methods, becoming better in dealing with this setup. However, manufacturers currently prefer to use lower intensity measurements because the resulting denoising problem can be handled well with current algorithms. In addition, while radiation dose should be kept as low as possible as a general principle, current clinical applications are rarely hindered by dose considerations. The simulation of CBCT datasets in the Chest and Jaw images seems to suffer from the so-called truncation artifact which is caused by projections not covering the entire acquisition volume which causes additional artifacts which are especially noticable in the FDK reconstruction. Also the FDK reconstruction seems to suffer from a bad handling of the sparse-view setup leading to a multiplicative constant distorting the attenuation values. This leads to a much worse image impression compared to e.g. SART which may simply be eliminated by scaling. While the method is interesting and the resulting images show better numerical results a visual examination suggests that the reduction in conventional artifacts is paid for by introduction of a new class of artifact which is characterized by noisy boundaries and a novel noise appearance exclusive to this method. In addition no more distinctive features of the dataset become visible than in the classical methods. This is also in line with theoretical expectations since the method does not introduce an explicit prior over the data distribution by incorporating information from multiple CT scans. It rather resembles a novel variant of a reconstruction method which should be theoretically as limited as any other iterative reconstruction without regularization.	The loss in eq. (3) is a standard reconstruction loss. The claim of self-supervision is incorrect in this paper. The proposed method is claimed to be useful for sparse-view CBCT reconstruction. However, only one setting of # of views is provided in the experiments. It is necessary to study the performance against different # of views to understand the sparsity of views.	Comparison results did not significantly demonstrate the superior performance of the proposed method. Network structure and training process are not clearly stated.	The method should be very well reproducible with the provided information and the code is promised to be released.	The training of the hash encoder is not clear. Is the hash function trained jointly with the fully-connected deep neural network? The setting of the sampling quantity is not clear.	The authors followed the ethical rules.	As described above I assume for the Chest and Jaw datasets projections were truncated left and right. This may highlight another advantage of your method, but is confusing if not explicitly mentioned in the article. I would recommend resimulating the dataset with a wider detector or a different parameter setting to avoid this. Alternatively this could be briefly discussed.	see weakness	1 The authors are expected to provide details on the network architecture. 2 The performance of the method may be further improved by fine tuning the parameters within the network. The current incremental image quality may not support the statement of 'clinical use'. 3 Computational efficiency is not provided.	This work is the first one I have seen which demonstrates the NERF concept applied to practically relevant CT with a measured dataset. The evaluation is very good and its very well presented. I admit to believe, low-dose CT using sparse-view CT is irrelevant. However, sparse-view CT also arises in motion-compensated CT and is therefore still a very relevant problem.	This paper proposes a new application of using implicit function to model the attenuation coefficient in CBCT reconstruction. This paper also tailors the position encoding with a hash encoder to adapt to the scanned human organs. This paper also shows promising experimental results.	As sparse reconstruction is a popular topic in CT reconstruction, the image quality is the major concern using various schemes. The authors need to demonstrate the superior image quality prior to the complicated network design.
350-Paper0929	NerveFormer: A Cross-Sample Aggregation Network for Corneal Nerve Segmentation	The authors propose a method for segmenting corneal nerves in CCM images. The method consists of encoder (pre-trainied ResNet34), two attention modules, and a CNN-based decoder. Experiments show that method performs better than a number of other methods.	The paper presents a transformer based network for nerve fibre segmentation in CCM images. The proposed transformer based part contains an intra-image local spatial attention and inter-image attention.	This paper presented a new transformer design for corneal nerve segmentation. The proposed method achieves state-of-the-art performance in two CCM dataset by learning features from single CCM image and common properties from multiple samples.	The problem is relevant, and the approach is sound.	The combination of internal local attention and external attention is novel. The proposed method outperformed other state-of-the-art methods on two public CCM datasets. Ablation study shows the effectiveness of the prosed two blocks.	The paper points out a common issue in corneal nerve segmentation - the background artifacts (e.g., Langerhans cells). The proposed method combines the merits of two transformer designs (deformable DETR and external attention) and successfully applied it to alleviate the identified issue. The state-of-the-art performance is achieved on the two CCM datasets, and an ablation study is included to verify the effectiveness of each component.	"The biggest weakness of the paper is a relatively poor presentation. In places the paper is unclear, which makes it less convincing. The introduction is very messy. First it introduces a number of studies on fibres - that's fine, but many of those studies (also quite recent) are not put in relation to the proposed method, and are also not present in comparison. Later it mentions [11], which seems to be improvement of [10]. Then it goes on mentioning the weaknesses of [10]. For other methods mentioned in introduction is unclear why are there relevant - some are general segmentation methods (not especially developed for nerves), while some are specialized pipelines. Moreover, the figure 1 uses results of [10] and [2] for motivation, but the text says almost nothing about why those methods are relevant (apart from having weaknesses). In the figure 1, the yellow text should be placed on the left of each row - as it is now it seems that nerve discontinuity is only relevant for CS-Net. The similar comment applies for figure 2. In places, language is informal and unclear: ""...Inspired by [21] we motivate our model too focus on..."", ""...method presents better immunization against artifacts..."". The key contributions: DEAM with TDA and TEA should be motivated and explained more clearly. E.g. the first sentence explaining TDA is not explaining much - and it just keeps on. Figure 3 is a bit confusing. For nerve continuation the arrow points to a place where the proposed network does well, but in other places (top middle) the nerve is broken by the proposed method, while other methods do better. Also, if other methods are performing so poorly on Langerhans cells, are those method relevant for comparison. Acording to the table 1, the methods shown in figure 3 are not competing to the best place. Also, table 1 seem to contains only one method developed for CCM (another one for fiberous structures). Why it that?"	The theoretical justification of the proposed method is weak. A lot of implementation detail is missing.	The generalizability. I am not sure about the generalizability of the proposed method, as no description of the test setting is provided. Based on Ref.8, external attention relies on two learned memory units which are the representation of the training dataset. The proposed method seems to preserve one such unit. However, such a dataset-level representation could be less representative if the pre-trained model is tested on a different dataset. Thus, it is important to know how the memory unit is used at the test time and how well the model can be generalized to different datasets. Insufficient baselines. As the proposed method is a fusion of deformable DETR (Ref. 21) and external attention (Ref. 8), it may be a good idea to include these two methods in the baselines to verify the benefit of this fusion design. The novelty of TDA is unclear. I cannot see any significant difference between TDA and the deformable attention module in Ref. 21.  If this is a major contribution, I would recommend to highlight the difference and demonstrate the motivation and novelty.	The explanation gives an ide of implementation. Still, it would be difficult, if not impossible, to fully reproduce this work - there are many implementations choices not covered.	Based on the description in the paper, it is difficult to reproduce the exact result due to missing details. No code is publicly available.	The detailed network architecture is not included in the paper. But the authors commit to releasing the code in the reproducibility checklist.	Please address the weaknesses.	"The paper presents a transformer based network for nerve fiber segmentation in CCM images. The transformer based part contains an intra-image local spatial attention and inter-image attention. The proposed method outperformed other state-of-the-art methods on two public CCM datasets. Ablation study shows the effectiveness of the prosed two blocks. The paper is generally well written, but could be improved by addressing the following comments.  -""The TDA enables the proposed DEAM to learn more crucial information in a single CCM image"". How ""crucial information"" is learnt? Requires more intuitive explanation.  -A_hqk and W_h in equation (1) was not explained. Explain how the sampling offset is determined.  -Please explain Norm in equation (2).  -What does ""xN"" mean in Fig. 2. N also represent the number of elements in page 5 line 4. Are they the same meaning? -Parameter setting, e.g. H in equation (2), N in Fig. 2. H is also the height of image.  -Please perform statistical test when claiming one metho is better than the other in Table 1 and 2.  -It would be interesting to see the visual result of Backbone+TEA and Backbone+TDA in figure 3.  This helps in intuitively understanding the effects of each block. Pick one of CS-Net or TransUnet, or use two rows of examples instead of four, if requires more space."	The paper demonstrates superior performance on two training datasets. But I am not confident about its generalizability, and I believe the two source methods (deformable DETR and external attention), which the proposed approach inspired from,  should be included for a more comprehensive evaluation. Plus, I wish the authors could illustrate the novelty of TDA if it is a major contribution. I cannot recommend acceptance based on current manuscript. But if the authors can address all these three major concerns, I would be inclined to accept the paper. Besides these three major concerns, here are two minor suggestions. Fig. 1 and Fig. 3 seem redundant to me. It may be better to merge them and provide more discussion and analysis in the saved space. For published papers (e.g. Ref. 21), it would be better to cite their published version instead of arXiv preprint, unless there is significant difference between these two versions.	Whille not convincing, the works seems to improve the state of the art.	Overall a sound method but justification of the proposed method is weak. A lot of missing implementation details, which is difficult to confirm if the comparison to other methods was fair (e.g. number of layers, number of parameters, if model was fully converged, etc.).	The model generalizability; The imcomplete baselines; The novelty of TDA module.
351-Paper0436	NestedFormer: Nested Modality-Aware Transformer for Brain Tumor Segmentation	The paper has presented transformer network for multimodal medical data like Brain MRI dataset. The presented Nested Modality Aware Transformer (NestedFormer) approach is capable to handle multimodal information in the dataset. The same is validated with the experimental results with BRATS and Meniseg dataset.	This paper proposes a model, termed NestedFormer, to fuse multi-modal information based the architecture of transformer. The multi-model information is firstly embedded separately using Global Poolformer. The core module of NestedFormer, called Nested Modality-aware Feature Aggregation (NMaFA), is proposed to fuse long-range dependencies of different modalities. A modality-sensitive gating (MSG) is proposed to utilize modality-aware low-resolution features by decomposing in three orientations.	In this paper, the authors propose NestedFormer that combines U-Net and Transformer for brain tumor segmentation. The effectiveness of NestedFormer is demonstrated through performance evaluation experiments using the BraTS2020 dataset.	The paper is well written. The results are presented on two different brain segmentation dataset. The proposed NestedFormer is discussed in detail.	This paper is overall well written and easy to follow. The usage of transformer is relatively novel and could be of interest to many readers. NestedFormer provides an alternative and new approach for multi-modality fusion, which still an active research problem, and look promising.	Proposal of a new combination of UNet and Transformer. Comparison with the latest methods such as TransBTS and Unetr.	There should be section or subsection to discuss key highlights of proposed architecture which make it better than other SOTA. There should be thought presented on extension of the architecture to any other medical dataset where multiple modalities are available.	If I do no understand wrong, the design of NMaFA is similar to channel and spatial attention network, which is widely adopted in the literature, but the authors do not mention that. Maybe the authors can further justify the differences.	There are some parts where explanations are insufficient. There is insufficient discussion of the experimental results.	The work is reproducible	The method proposes quite a few unique blocks in Nestedformer, making it hard to implement. The author could consider to provide the model source code to help reproduce the results.	If one reads this paper carefully, it is possible to implement NestedFormer. It is possible to perform experiments with BratS2020, but not with MeniSeg.	There should be section or subsection to discuss key highlights of proposed architecture which make it better than other SOTA. There should be thought presented on extension of the architecture to any other medical dataset where multiple modalities are available.	I suggest to provide an nnU-Net baseline, which can be done out-of-box, to help the reader access the relative performance gains.	In the encoder you named the feature embedding layer, why not 3D conv? 3D conv is easier to understand in terms of feature extraction by the encoder. For reproducibility, the version of the library used should be specified. Why did the authors not perform cross-validation in their experiments with BraTS2020? The dataset may be randomly divided, but any bias may not result in a correct evaluation. The results in Table 2 show that the NestedFormer has a larger HD95 value. There is no discussion of the results with higher accuracy in terms of Dice and lower accuracy in terms of HD95. Why is it that only limited combinations were evaluated in the ablation study in Table 3? 6.The font size in Figure 2 is small and the resolution is low. Figure 3 caption is incorrect. Figures and tables must be placed on or after the page on which they are referred to.	In general, medical data have huge information in form of modalities and this is current area of research to take benefit of modalities together. The paper presented a new mmFormer architecture which can tackle this issue well.	Overall, I think the method is novel. The method and experiments are well demonstrated. Although the method is relatively complicated, but a clear ablation study makes it easy to follow.	My judgment is borderline, but the paper needs to be revised, so I decided to give it weak reject.
352-Paper0117	Neural Annotation Refinement: Development of a New 3D Dataset for Adrenal Gland Analysis	The paper proposes a method to refine mask annotations to get better segmentation/classification results.	3D segmentation masks of medical images are often noisy.  The authors proposed an appearance-aware implicit function-based method to refine the human annotations of a 3D adrenal gland dataset.  The shape modeling method with a modified implicit function is technically sound.  The new dataset reduces label noise for downstream image analysis.	This paper proposes a novel appreace-aware implict surface model for ground truth repairments. This paper also contributes a new data set for adrenal gland analysis.	The mask annotation refinement problem that this paper tried to solve is an interesting problem; With the refined annotations produced by the proposed method, the model can diagnose adrenal glands better than the original ones.	The main strengths are as follows: Incorporating HU values of CT images to a part of the input of implicit function-based shape modeling method. The refinement of segmentation masks reduces noise and improve diagnosis as a downstream classification task. A new dataset with smoother shape and less noise for image segmentation and classfication.	Contribution of a new dataset. The neural refinement idea is interesting, the method novelty itself might be incremental but I don't see it becomes a major issue. This paper is addressing a common issue that the human annotation is normally not smooth and full of mistakes because of annotations on slices for instead the whole volume, surface-level correction is expected. This paper might also help active learning researchers. Extensive experiments.	The proposed method is not very clear. What's the symbol a specific represent for? The novelty of the paper low. It only changes the deep implicit surface by adding an a, and it aggregates features from multiple scale.Why is a needed? And multi-scale feature fusion is a widely used operation in deep learning. The experiments are not convincing. First, it doesn't compare with other label refinement methods. Second, for the segmentation comparison experiments (Section 4.1), the inference label uses golden annotation, while training label uses distorted annotation. Thus it is not a fair comparison with the baseline methods, since they don't have any label correction process.	There are some moderate weaknesses: Results.  x. In Table 1, the authors compare their method with Seg-FCN and Seg-UNet. The two models are trained on distorted annotations. However, it would be great if the author could show the upper bound, e.g., models trained on good annotations. As a repairment, one could consider an ensemble of multiple models to reduce segmentation noise and uncertainty. Other heuristic methods to remove noise, such as hole filling/connected component analysis might be other baselines methods.  x. In Table 2, the method without using the appreance as the input works worse. It would be great if the authors could discuss potential reasons behind, e.g. does it mean the basic refinement. method does not improve the annotation? Clarifications. x. in Table 2, which view is used in the 2D method?	My biggest concern are on the motivation and comparisons of the proposed method: For training on the distorted data set, maybe 20% distortion won't affect the downstream task performance and human experts only make inside of 20% distortion, it would be nice to find the distortion threshold on gold standard dataset to see how much distortion actually brings down the performance of the downstream task. For example, 30% slices are distorted or more severe level distortion. Such an analysis is expected. This method is data centric, but what if authors train a bigger classification network with more diverse and complicated data augmentation focused on the appreance and better regularisation? Wouldn't that be a simpler way to tackle this problem? The authors need to add comparison with model with more intense data augmentation and larger model, I noticed authors are using resnet-18, larger models can learn better generalisation and the classification performance gap between the proposed method and baseline is not as big as it seemed. I strongly suggest the authors to consider this issue and I am willing to raise the score after the rebuttal if the authors address this issue. Minor weakness: Only used on one data set. It would be nice to see how the method applies on another data set, that would make this paper really appealing. What happens when the proposed NeAR model is used on very sparse annotations? Do you think the models will have limits on discontinuous objects and extremely small objects? No stds on table 1	The authors don't have the central tendency & variation as they claimed.	all codes are available.	Code is attached in the supplementrary materials although I personally did not run it. The authors promise to release the dataset too.	See the list of weaknesses above.	"In Table 1, it would be great if the author could show the upper bound, e.g., models trained on good annotations. As a repairment, one could consider an ensemble of multiple models to reduce segmentation noise and uncertainty. Other heuristic methods to remove noise, such as hole filling/connected component analysis might be other baselines methods. Discussion on the limitation might improve the quality of this work. For example, x. The proposed approach incorporated CT intensities value to improve the reconstruct quality. Absolute intensity values contain tissue information. However, this might not work well for MR images.  x. This approach works for regular shape (e.g. organs) and might not work for abnormal tissue such as lesions due to shape heterogeneity or small structure. Some typos / grammar errors x. In Introduction, ""Unfortunately, such datasets are difficult to obtain in part because human annotations are known to be imperfect"". Isn't it because that expert annotations are expensive? x. In Introduction,  high-frequency artefacts include false positive/negative. x. In Introduction,  MLP appears without a full name. x. In Sec. 2.2, 'by changing the input of...', 'changing' should be 'including'? x. dice -> Dice x. 'lower bound' -> 'minimum'; 'upper bound' -> 'maximum'"	It would be nice to see how this method applies to 2nd data set, especially for very sparse annotations such as lung vessels or very tiny structures. It would be nice to see the comparison between this method with GNN based approach. It would also be nice to see how much human labour has been saved, for example, the authors could ask human experts to refine the annotations and compare the time cost. In table 1. what are the parameters no. of NeAR and Seg-Unet? It would be nice to see the real comparison of the perforamnce In table 1, what are the stand deviations? It would be nice to assess how realistic the synthtic distortion is and how much it affects the downstream tasks. For example, if human make 30% distortion of the ``ground truth'' and the classification network is robust when trained with 30% distorted data, then the refinement might not be needed or someone should focus on improving the model. It would be nice to add analysis of the effect of the distortion on the downstream task. refinement of data vs refinement of model. It is necessary to add comparisons in Table 2 with model-focused methods, for example, more appreance focused augmentation plus bigger model with better representation learning ability might simply improve the classification result and maybe refinement on dataset is not necessary. Could the authors add connections between the proposed method and NeRF (neural radiance field)?	The novelty of the paper is low, the description of the method is not clear, the comparison experiments are not conniving.	Improved segmentations based on human annotations.  Good methodology contribution for shape modeling.	Geometric deep learning has become more and more important in medical image analysis as the surface representation are closer to the real world ones. They deserve more attention in the community. Enough contributions in both datasets and methods. A few weaknesses need to be addressed in the rebuttal and I am willing to raise my score if the rebuttal addresses my main concerns.
353-Paper1091	Neural Rendering for Stereo 3D Reconstruction of Deformable Tissues in Robotic Surgery	This paper adopts a leading edge deep learning 3D rendering algorithm  called neural rendering  to reconstruct 3D surfaces of surgical scenes in the context of robot-assisted procedures that employ a stereo endoscopic cameras. It  seeks to provide an improved  rendering of the visualized surface based on a known depth model from the stereo-camera. As distinct from the   traditional 3D rendering approach, where the 3D surface is approximated using polygons and projected back to the camera using physical optical principles, the neural rendering technique is trained to create a predictor as a function of camera position and it is able to directly predict  the r,g,b, alpha(transparency) value for all camera rays.  In the paper, the authors use an existing stereo-matching algorithm STTR-light to obtain a coarse depth map for surface reconstruction, and then proposes an improved method based on the existing D-NeRF model for neural rendering.	In this work, the authors newly adopt neural radiance fields (NeRF) to reconstruct dynamic surgical scenes from single-view (left view) stereo endoscope videos. The proposed framework is based on D-NeRF, and applies STTR-light to estimate depth maps as the prior knowledge for 3D scenes reconstruction. Results indicate that this method is promising, and provides good dynamic scenes recontraction models even non-rigid deformation and tool occlusion exists.	This paper proposed a NeRF-based 3D reconstruction framework for deformable tissues during robot-assisted surgery. The framework uses neural implicit field for dynamic scene representation and incorporates mask-guided ray casting for occlusion issue as well as depth-cueing ray marching and depth-supervised optimization scheme. The method was evaluated on clinical dataset from DaVinci surgical robot and compared against the most recent SOTA method E-DSSR and outperformed significantly.	In addition to small optimization improvements and surface smoothness improvements, the biggest advance of this paper appears to be the incorporation of a frame-to-frame deformation model and surgical tool mask to reconstruct soft tissue surfaces lying beneath  the surgical tool. Figure 2 suggests that the algorithm is very effective in achieving this task, and the video  in the supplementary material reinforce this conclusion. The video  in particular clearly demonstrates the ability of this approach to render regions for which  other methods have problems.	1) Tool mask-guided ray casting is designed for eliminating tool occlusion.  2) Depth-cueing resampling and depth-map loss are introduced to make NeRF effective on single-view reconstruction. 3) The proposed method is complete and the results are impressive.	1) The authors proposed an innovative and effective design. For example, the NeRF structure is incorporated with mask-guided ray casting which solves the issue from tool occlusion.  2) The authors also did thorough evaluation by comparing against the most recent SOTA method. Furthermore, ablation study was done to further investigate the contribution of each customized modules to the significant improvement on performance. And lastly, the fact that the method was tested on clinical dataset is another highlight of the paper.	"Clarity. This paper is quite difficult to follow, particularly with respect to the details of the method. Some acronyms are not defined, and some methodological descriptions are vague. Work flow and block diagrams describing their methodology could be very helpful. Moreover, photometric error measurements are not clearly defined. It would help to describe the un-met clinical need more concisely, particularly with respect to the need for 3D reconstruction during a typical  procedure for a radical laparoscopic robotic prostatectomy (RALP) for example. How would RALP for example be improved if the proposed technique (or any other approach for that matter) if reliable surface reconstruction of the surgical scene were available.   I do agree however that ""robust scene reconstruction is important for augmented reality, surgical environment simulation, immersive education, and robotic surgery automation"". What is the clinical impact of being able to remove the surgical tools or reconstruct the 3D scene during tissue deformation in real time? It would be helpful if the paper indicated explicitly what makes the proposed method superior for dealing with tissue deformation. Why is ""single viewpoint"" important, since in a RALP procedure , the tools are in constant motion, and the camera is moved frequently to track them. In the reconstruction of tissue deformation during an actual procedure, I would have thought that eliminating the tools from the image would be counter-intuitive.  Is the purpose to superimpose the stereo representation of the real tools onto the reconstructed deformed scene? Validation: If I understand the  validation method correctly, the photometric error measurements are not definitive, especially for tissues under the surgical tool. While it is acknowledged that ground truth is difficult if not impossible in clinical cases, could not a phantom study have been employed to obtain ground truth on some examples? Some phrases are very obscure. For example, what does ""To capture high frequency details, we use positional encoding g(*) to map the input coordinates and time into Fourier features before fed to the networks"" mean?. How long does the algorithm take to reconstruct each frame? Is it feasible for real time application? Is E-DSSR the only available competing algorithm?"	"The proposed method requires per-scene optimization as the authors state in the ""Implementation Details"", it means that the proposed method seems cannot achieve the real-time reconstruction of tool-occluded areas. That is, surgical tools in the intraoperative video cannot be removed in the reconstructed scenes. Therefore, what is the significance of the proposed method for robotic surgery, in which the tool occlusion often occurs? The motivation of this article should be more clear."	In Section 2.5, the equation (3) and (4) are not well explained. Equation (3) is too dense, maybe consider dividing them and explaining each clearly and also make sure the meaning/definition of each parameters are described in the text. For example, the T in (3) and I in (4) were not defined in the text.	It is not clear whether the data will be made public.	It is easy to reproduce the work of this paper with released data and code.	The code will be available which is great. Thanks to the authors.	In spite of the paper being difficult to read, the results are impressive. More clarity in writing as detailed above, along with a  comment on using the neural rendering technique in conjunction with arbitrary conventional reconstruction techniques to improve performance would improve the paper.	"1) In the supplementary materials, some areas are occluded throughout the videos, such as pulling, cutting, and tearing cases. How to accurately reconstruct these areas, and perform the evaluation? 2) The authors point out that they perform the evaluation following Ref. 10. While, the evaluation method in Ref. 10 is only applicable to the situation where occlusion areas change frequently. So, how to achieve the reference of these areas which are occulated throughout the whole video? 3) Stereo endoscope captures both left and right images, but only left images are used in the proposed method, can the method be applied to right images? If it does, how to evaluate its performance on right images? 4) Some format issues: - Equation (1): What is the relationship of M_i and M_j? If M_j is a subset of M_i, M_j can be 0 (M_j is tissue). - Equation (1) and (3): The meaning of variable j is unclear, please give the interpretations or references.  - Equation (2): Is D_i [u,v] similar to {[[D_i}]] (i=1)^T used in Ref. 10? If it is, why {[[D_i}]] (i=1)^T is named as ""coarse depth maps"" rather than a reference, considering that it has been respectively used as a sampling guidance and depth supervision in subsection 2.4 and 2.5? - Equation (4): What is I_i [u,v]? I suppose it could be the images with the tissue information only, if it is, please give the definition explicitly. - Subsection 2.5: ""we firstly find residual maps ..."", what is D_i? Is D_i  equivalent to {[[D_i}]] _(i=1)^T?"	In Section 2.5, the equation (3) and (4) are not well explained. Equation (3) is too dense, maybe consider dividing them and explaining each clearly and also make sure the meaning/definition of each parameters are described in the text. For example, the T in (3) and I in (4) were not defined in the text.	Impressive results that seem to be superior to competition. Enthi=usiasm is dampened somewhat by the number of unanswered questions.	1) Tool mask-guided ray casting is designed for eliminating tool occlusion.  2) Depth-cueing resampling and depth-map loss are introduced to make NeRF effective on single-view reconstruction. 3) The proposed method is complete and the results are impressive.	In Section 2.5, the equation (3) and (4) are not well explained. Equation (3) is too dense, maybe consider dividing them and explaining each clearly and also make sure the meaning/definition of each parameters are described in the text. For example, the F, G, T in (3) and I in (4) were not defined in the text.
354-Paper0240	Neuro-RDM: An Explainable Neural Network Landscape of Reaction-Diffusion Model for Cognitive Task Recognition	"This paper introduces a new method for extracting brain states from fMRI via a dynamical systems model. The authors blend model-based and data-driven strategies by using neural networks to implicitly learn the parameters of a reaction-diffusion equation. The ""reaction"" component is implemented using an ANN, and the ""diffusion"" component is mapped onto a graph convolutional network for joint training. The authors evaluate their model on simulated data and task-based fMRI from the Human Connectome Project. The results demonstrate improved recognition accuracy compared to recurrent architectures."	The article rewrites the reaction-diffusion  model originally expressed in partial differential equations in terms of a deep neural network. The new network is used to study changes in brain states from BOLD fMRIs.	This work examines the problem of modeling regional fMRI BOLD signals as a dynamical system governed by reaction-diffusion process which is formulated as a set of trainable PDEs. They introduce a reaction-diffusion model (RDM) which characterizes evolution of brain states across the connectome and captures the interplay between brain states and neural activity. This in turn is formulated as a graph neural network optimization which is guided by the cognitive task being performed. They evaluate their framework on simulated and real HCP data against baselines on the basis of the ability to identify the latent generating states and identify the cognitive task being performed respectively	Interesting premise of inferring functional brain networks based on steady-state solutions of a reaction-diffusion equation. The authors map each component of the reaction-diffusion model onto a neural network that can be optimized using standard gradient-based techniques. The approach blends structured model-based assumptions with the representational power of deep learning. The authors demonstrate that this structure provides robustness over RNNs. The model achieves higher state-recognition accuracy than two RNN methods and a PDE solver.	The idea of re-expressing differential equation models in a neural network via component decomposition or equivalent counterparts	Novelty in the formulation is the salient point of this work. The utilization of GNNs as Neural ODE solvers for modeling the dynamics of functional connectivity is an interesting perspective in the field and a departure from more popular neural architectures such as RNNs, LSTMs or transformers. In my opinion, this line of analysis may provide interesting new insights in the field.	"While the authors weave a nice story to motivate their work, I find the claims about neurobiology to be overstated. For example, the statement ""the ensemble of evolving neuronal synapses forms a dynamic system of functional connectivity (abstract)"" is a simplification of the complex neural and hemodynamic attributes leading to the BOLD response. Likewise, the statement ""uncovering hidden brain states...becomes the gateway to understanding flexible and adaptive human cognition (page 2)"" is an exaggeration at best. Also, the statement ""we develop the machine intelligence of system-level explainability into a deep learning model...to yield new underpinnings of biological processes (page 2)"" is unsupported by the experimental results, which consist of a few predictive analyses. The is little acknowledgement of existing methods to analyze dynamic functional interactions or extract brain states. There is also no acknowledgement of existing work on interpretable AI or methods that blend model-based and data-driven techniques. This is a large oversight in an already popular field. Several details of the proposed method are unclear. First, how is the graph w_ij generated? Is it subject-specific or fixed across the cohort? Second, the diffusion process does not require an attention model, so why is it implemented? Third, how are the ""ground truth"" cognitive states defined for training and evaluation? Fourth, why is the BOLD time series truncated? I would expect a dynamical model to accommodate different acquisition lengths. Fifth, what does the statement ""the training data is mixed with the test/retest fMRI data"" in the evaluation section mean? If the data is truly mixed, then the results are optimistic due to data leakage. There is no ablation study to quantify the impact of the different modeling components (e.g., choice of W, attention vs. no attention, neural network sizes, etc.)."	It is not clear what is gained with the new network compared to the original model of differential equations	Architectural Description: There is very little information provided in terms of implementation details, eg. number of GNN layers, width of the embeddings, type of graph convolutions used, non-linearities etc. They also do not clearly mention the paradigm for training the models (epochs, learning rate, optimizers etc). This would make it very difficult to adopt their framework for any future applications. Evaluation is performed on a single dataset split into train/test/validation as opposed to a cross validated, which may not provide a reasonable estimate of the robustness of the improvements. Additionally, since a single dataset split is used, it is unclear how the distribution of the accuracies in Fig. 3 and 4 are generated . Attentional selection: a) A contribution of the work is in introducing graph attentions into the Neuro-RDM framework for discovery of new links. However, there is no experimental comparison that establishes whether having this additional component (increased parameterization) is necessary, for example via an ablation study b) The authors average patterns learned by the attentional framework as a proxy for studying replicability of the patterns. Since evaluation was performed only on a single data split, this does not provide a good indication for whether these patterns would be consistently replicated for a different subset of the population	Reproducibility statement is acceptable.	Very high. As aforementioned, with a little more description in the generation of the model forward, I think one should not have problems to replicate the experiments and reproduce the results	Upon reading the paper and the checklist, I found several inconsistencies. For example, I found no description of the parameter settings, architecture, range of parameter values, sensitivity to parameters, memory footprint or compute software in the paper.	"The ""weaknesses"" section details my major concerns and constructive suggestions. Here are some additional minor comments: The use of ""functional neural imaging technology"" in the abstract is awkward. I would suggest rephrasing. The characterization of fMRI as allowing us to characterize coupling mechanisms of brain functions on to of the structural connectomes is also overstated (page 1). FMRI quantifies hemodynamic changes, which are not necessarily linked to the structural connectome. The motivational comparison on pages 2-3 seems unfair. The one-particle trajectory is generated from a PDE, so naturally embedding a priori knowledge of this process into the model will perform better. The key question is whether such a model would perform well if the data were NOT generated from a PDE. It is unclear to me why an ANN is an appropriate model for the ""Reaction Process"". I would think neuronal firing patterns are more complex than a linear model with sigmoid activation. The similarity between test and retest in Fig. 5 may indicate that the model is learning a ""mean"" representation for the attention weights, rather than capitalize on subject-level differences. I would suggest the authors explore this phenomenon."	If one already has the model of differential equations, what is the benefit of re-expressing it in a neural network (other than for the beauty of doing it)? How much error is there between the trained NN and the original PDE model? although from figure two I think I can intuit how the diffusion-reaction model of equation 1 has been translated into the neural network, but perhaps a little more step-by-step explanation explaining the methodology to take the first model to the second would be convenient. to be able to generalize the idea. I suppose that the generic idea of decomposing differential equation models into fundamental elements with counterparts in neural networks is in principle generalizable to other models. Although I understand that this goes beyond the purpose of the article that is presented, may I ask the authors to what extent they believe that this idea can be developed? Is it possible to affirm that any model of partial differential equations can have an equivalent counterpart in neural networks? Add the PDE model to figure 1	"I would encourage the authors to improve the clarity of Fig.2 to clearly indicate the input BOLD signals and outputs, to aid readers in parsing the methodological description Several error bars in Fig 3 appear to be cut off. Perhaps the range on the y axis can be changed to prevent this Clarifications for the following would be helpful: a) Are the regional connectivities thresholded to generate W? If so, what is the threshold? In practice, how would this threshold affect the optimization of the Neuro-RDM and generalization. b) What statistical test is used to compare the performance of the methods pairwise in Fig. 4? c) ""we truncate the long time course of BOLD signals (usually includes more than eight functional tasks) into a set of segments where each segment primarily covers one functional task"" From my understanding, this implies that the time series was broken up into segments of a fixed length. How was this chosen, and how would the choice of this length impact performance? c) Definition of a latent brain state: For the purposes of this paper, it seems like the intrinsic brain states and cognitive tasks are used synonymously. Therefore it is unclear whether this method could be applied for a study of dynamic functional connectivity in a more broad context, for example for resting state-fMRI data. In this space, several models such as the sliding window or dynamic conditional correlation aim to address the problem of identifying the ""latent brain states"" from the regional time series. It would be great if the authors could discuss if and how their method could be applicable beyond the specific task evoked fMRI paradigm examined here. d) Scalability: From my understanding of the experiments, the Neuro-RDM was tested on Functional Connectomes generated by grouping regions into 8 subsystems such as the DMN, CEN etc. A natural question to ask here is whether the method would scale to using finer parcellation schemes such as the Yale functional atlas, which is also the scale at which several GNNs developed for functional connectivity work with"	Overall, I appreciate the methodological novelty in this work. I particularly like how the authors map elements of the reaction diffusion equation onto different neural network architectures. However, my enthusiasm is dampened by the overstated claims, lack of an ablation study, and several unclear details.	The flow of the article is excellent, the mathematical formalization is correct and elegant, the rationale described in the first paragraph of section 2 is outstanding, the images are very well taken care of and clearly contribute to the message of the article, the two experiments presented are well executed in my opinion and perhaps with a little more description in the generation of the model forward, I think one should not have problems to replicate the experiments and reproduce the results. In general, a very good article, from which I have learned a lot, and my congratulations to the authors.	While the methodology is interesting and novel, the major weaknesses of the paper are in the evaluation of their contributions and the design of experiments. The complexity of their model is not sufficiently justified. Additionally, several key details for implementation are missing.
355-Paper2027	Noise transfer for unsupervised domain adaptation of retinal OCT images	This paper proposes to transfer device-specific noise to OCT images taken with different device(s), thus mitigating the domain gap that often affects performance with models trained on one device source, but used on another device source. The transfer is done by singular value decomposition-based noise adaptation (SVDNA), which adds the reconstructed noise signal from some target domain, to a source image. With the SVDNA method, noise from various (known) domains can be included as part of the data augmentation in training a UNet++ segmentation model. It is claimed that this improves OCT image layer segmentation performance over state of the art unsupervised domain methods.	The paper proposes a new domain adaptation technique, named SVDNA, for improving the performance of segmentation models when trained in a source domain and tested on a different/target domain. The paper is validated when domains are images generated by different camera devices for the task of Optical Coherence Tomography (OCT).	In order to to bridge the domain gap between different retinal OCT imaging devices, the authors proposed a minimal noise adaptation method based on a singular value decomposition (SVDNA), without modifying the basic model architecture or training an extra style transfer model.	SVDNA is relatively simple to apply, and does not appear to require in-depth training with deep models Evaluation on public datasets	The proposed idea seems to be novel. The idea is very simple and it seems to properly adapt the domain when moving from a less noisy one to a more noisy one. The performance in the opposite scenario is uncertain. The paper is well written and clear.	It is interesting that style transfer between the source and target domain can be achieved by combining content components from the source image and noise components from a target image after singular value decomposition. The proposed method is simple and does not require the modification of the basic model architecture or extra training of a style transfer model. The experimental results and discussion presented in the paper and supplementary material are sufficient.	SVDNA evaluation in this study applies only to known domains Key contribution appears to actually be the final histogram matching, and not the SVD-based noise transfer, which might not have been emphasized Comparison against prior methods not very clearly presented	By design, and as mentioned by the authors, the proposed technique has limitations. Specially, when the source domain is more noisy than the target domain, the proposed technique will potentially not perform well. The performance is uncertain in this scenario. This important limitation reduces the applicability of the technique to more scenarios. The Algorithm 1 is not clear enough. Variables within the algorithm are never defined so that it cannot be properly understood. Authors should properly define all the variables employed and add comments on the algorithm when needed. The description of the algorithm in the body of the paper helps understanding the technique applied. There exists mismatches between Algorithm 1 and text. Clipping and histogram matching are not within the Algorithm but they described in the text. There are some variables defined in the text (e.g. probability 'p') which are never applied anywhere. If a variable is defined, it should be used somewhere (e.g. in an equation)	Some minor mistakes exist in the paper (see detailed and constructive comments).	The SVDNA transfer method (Algorithm 1) is fairly straightforward.	Code will be available upon acceptance Public datasets/benchmarks are used	The proposed method are described in details and the full code implementation will be made available, which would make it easy to reproduce the methods.	In Section 2.1, it is stated that the pixel values between the restyled image and target image is (often) still different, which is fixed by further histogram matching. The histogram matching is empirically shown to further improve performance somewhat (Table 1, supplementary material). However, it is not clear whether the (final) restyled image should necessarily actually share the same pixel value distribution as the target image, since the true source image (assumed with noise from source device distribution removed) and the true target image (assumed with noise from target device distribution removed) might well have different distributions. Indeed, the noise addition formulation would appear to have the restyled images be (true source image)+(source noise)+(added estimated target noise), which is then adjusted to fit (true target image)+(target noise). Theoretically, it might be more proper to first subtract estimated source noice, if the source is known. This might be considered. Related to the above, the clipping of values outside the interval [0,255] after noise transfer would appear a potential source of signal loss at its extremes. It might be clarified if such out-of-bound values apply to a significant proportion of the signals, in general. In Section 2.2, it is stated that for SVDNA style transfer, when a target domain is required, one image from that domain is randomly chosen as the source. While Figure 2 suggests that images from the same domain do have relatively similar noise characteristics, it might be clarified whether the adapted data does in fact mimic the intended target domain well. SVDNA to Bioptigen in Figure 2(b) appears clearly separate from Bioptigen, for example, and SVDNA to Cirrus/Topcon do not appear to be shown. Moreover, Table 1 in supplementary material suggests that the histogram matching alone contributes the majority of the performance improvement, with noise transfer alone contributing next to nothing. As such, the natural comparison would seem to be against histogram matching/normalization methods such as CLAHE etc. In Section 3.2, it is not clear why CycleGAN required a different training methodology. While Table 1 in the main text claims comparison of SVDNA to supervised trained models from 8 teams, only three results are presented. If only the best model from the 8 teams was used, this might be clearly stated.	Improve the Algorithm 1. Including the cohesiveness between the text description and the algorithm. Make sure that all variables used are clearly defined.	"Minor issues: ""For more details on the feasibility of the used values of k see supplementary figure 1"" in Page 4 should be modified to ""For more details on the feasibility of the used values of k see supplementary figure 2"". ""for examples of content distortions achieved by an optimized CycleGAN see supplementary figure 2"" in Discussions and Limitations should be modified to ""for examples of content distortions achieved by an optimized CycleGAN see supplementary figure 4"". The last line in Table 1 of the supplementary material seems inconsistent with the results of Topcon (left) in Fig. 3. Please check."	The main contribution of the proposed method appears to be from histogram matching, and not noise transfer as emphasized in the title and presentation. While this is not undesirable in itself, it should be clearly stated. Moreover, the comparisons against other methods might be more fully presented.	The paper is clear, well written, and the validation experiments are enough The proposed technique has minor novelty and contributions as the applicability seems to be very reduced to when the source domain is less noisy than the target domain. In real settings, target domain features might be unknown or likely more noisy, where this algorithm will not perform well	I recommend accepting this paper. The authors sufficiently demonstrate that style transfer between the source and target domain can be achieved by combining content components from the source image and noise components from a target image after singular value decomposition. Their simple method does not require the modification of the basic model architecture or extra training of a style transfer model. In addition, the proposed method might be applied in domain adaptation of other medical imaging modalities such as CT with different doses.
356-Paper0041	Noise2SR: Learning to Denoise from Super-Resolved Single Noisy Fluorescence Image	Authors proposed a self-supervised image denoising method to train a image denoising model based on single noisy fluorescence image. The propose technique consists of a sub-sampler module that generates sub-sampled noisy images from the original one; and an image SR module that improves the sub-sampled noisy image resolution to that of the original one. Quantitative and qualitative experiments are presented comparing the proposed technique and some state-of-the-art methods. The results show a good performance (PSNR, SSIM).	This paper aims to denoise a set of noisy images with the same noise distribution with only one observation per image. It cleverly improves Noise2Self idea by creating training image pairs through image subsampling and randomizing subsampling mask. In experiments, the proposed method achieves competitive results and outperforms Noise2Self.	The authors propose a self-supervised image demonising model using U-Net. Here, the authors used a single noisy observation for self supervision. During the main task the authors used paired noisy images of different dimensions. While the model takes advantage of well-known noise2noise, unlike this method the use of sub-sampling module allows to authors to train image pairs at different resolutions. The experimental results showed competitive results on fluorescence microscopy datasets.	Novelty: method does not require multiple noisy observations and external noise distribution assumptions. The method shows a good performance in terms of PSNR and SSIM when compared to some state-of-the-art methods.	Novelty. It's a neat idea to make use of image subsampling to generate the image pairs for self-supervision. (1) Compared to the previous Noise2Self work, the sampling of pixels for prediction is straightforward in this work. (2) Instead of simply predicting masked pixel intensities, this work trains a super-resolution network to effectively regress with 1/4 pixels to the rest pixels. Convincing experiments. On both simulated and two types of microscopy images, the proposed method significantly outperforms (0.2-0.4 dB) its counterpart Noise2Self and achieves competitive results with the SOTA N2N that requires multiple observations. The paper is well-written and easy to follow. The method is described in detail and straightforward to reproduce.	Simple idea of using self-supervision Easy to follow the paper	Execution time is not presented. Limited comparison to state-of-the-art:  Only PSNR and SSIM mesures are studied (but, it's enough for a conference paper). A computational time comparison is missing.	Lack of extensive evaluation: The experiments are done only on fluorescence images. It's unclear how applicable the method is for general biomedical images. E.g. N2N is evaluated on MRI.	The paper lacks enough rationale behind self-supervision and subsampling technique. The paper requires ablation to provide evidence on each stage improving results. Currently the overall gain is marginal and its hard to say where this is coming from N2N and propose N2SR have competitive results so why the proposed is better or effective? This is Not well established.	The author provides the code and all the information necessary to reproduce the results.	Yes. The paper provides enough details and the algorithm itself is easy to implement.	The authors have provided a link to the dataset and have agreed to provide train-test dataset splits.	Some suggestions: It would be interesting that authors gave some information about the computational time for proposed method. Summarize the research limitations and future research directions. Describe the computing system (hardware) used in experiments. Extend the Conclusion with details on the method performance when compared to other tested techniques (in terms of PSNR improvement and SSIM).	Model: The paper can be viewed as an extension of N2S, predicting a random set of  pixels. It'll be great if the paper can plot the performance vs. number of pixels to predict, showing the interpolated results between two methods. Experiments: The proposed method will be more convincing if evaluated on different image modalities.	What does super-resolved mean? Authors need to define SR in the text. In N2N, one can use patches to use training, i.e. can be trained using less samples so how does using sub-sampling module help boost performance? As commented in the weakness above, authors could benefit from providing more ablation details. Please add time comparisons to better understand more clearly if there is any added value to this method	The topic of the paper is relevant and interesting to the MICCAI community . It presents an innovative idea for Fluorescence image denoising based on single noisy observation. Authors propose a sub-sampler module that generates sub-sampled noisy images from the original one; and an image SR module that improves the sub-sampled noisy image resolution to that of the original one. Although some information is missing in the results section (computational time and hardware), the results are good in terms of PSNR and SSIM.	This paper proposes a neat idea improving upon previous method significantly. I don't give higher score due to the lack of thorough evaluation on different image modalities. In general, this paper is solid in terms of both idea and experiments.	The presented method is very close to noise2noise method with small changes such as able to use sub-sampling module. However, the results are very competitive to the former N2N method. The paper does not show any comprehensive analysis on why the presented method is of advantage over other methods. In this light ablation studies are not provided which makes it hard to understand whether the sub-sampling module is helping of the pretext assignment is giving an advantage. It requires more work and insight.
357-Paper1103	Non-iterative Coarse-to-fine Registration based on Single-pass Deep Cumulative Learning	This paper proposed a one-shot DL-based registration method to address image registration tasks with large displacements. In the decoder, the displacement vector field is predicted at several resolutions, and at each scale, the wrapped moving image is injected. The evaluation is performed on public Brain MR datasets, in which the displacements are relatively small. The proposed method is compared with two conventional registration methods as well as six DL-based methods, which are retrained in this study.	This paper proposed a Non-Iterative Coarse-to-finE registration Network (NICE-Net) for deformable image registration. Unlike the existing iterative deep registration methods, the proposed NICE-Net can perform coarse-to-fine registration with a single network in a single iteration.	The authors propose an unsupervised non-iterative coarse-to-fine registration network (NICE-Net) for deformable registration using cumulative learning. This includes a single pass deep cumulative learning (SDCL) decoder, a selectively propagated feature learning (SFL) encoder, and an enhanced loss function. Compared to other iterative deep registration methods, NICE-Net can perform more accurate registration with a single network in a single iteration. Validation on two public datasets shows that NICE-Net outperforms the existing deep iterative registration methods.	1) Injecting the deformed moving image into the decoder is a novel idea.  2) The paper is well written.	The main strengths of the paper can be concluded bellow: 1) Proposed a coarse-to-fine unsupervised learning-based registration framework by using a single network. 2) Relatively large experimental datasets.	1- This paper presents a non-iterative coarse-to-fine deformable registration for medical applications based on cumulative learning. 2- Better performance: Compared to other iterative deep registration methods, NICE-Net can perform more accurate registration with a single network in a single iteration with the advantage of being fast.	1) The evaluation is performed on brain MR datasets, in which the displacements are relatively small.	"1) The innovations of the proposed method are not enough for MICCAI: the main innovation of the proposed registration method is the different architecture of the registration framework. Using different scales of features to obtain different resolutions of the displacement fields to make a coarse-to-fine registration.  2) Only one UNet-like architecture may hasn't the ability to capture such a large amount of features produced in different resolutions, this is the main reason that lots of previous work adopt multiple networks to conduct the coarse to fine registration.  3) In Table 1 the highest registration accuracies are obtained by using \lambda=0, it seems that the Jacobian determinant regularization didn't work well in the loss function, which suggests that the proposed method may lead to image folding.  4) In Table 1 the registration accuracies are not higher than those of ULAE-net by using \lambda=10^{-4}, it demonstrated that the performance of the proposed method is not better than the ULAE-net. 5) Authors mentioned that ""while the \lambada is set as 10^{-4} to ensure that .... is less than 0.05%"", how to get this conclusion? The \lambada is a weight that is to balance the loss items. How about the other values of \lambda, comparison results using different values of \lambda should be shown in the manuscript. 6) In the ablation study, what parameters such as the value of \lambda were used?"	1- Limited discussion of the qualitative results and comparing with the state-of-the-art. 2- Lack of training results: The authors utilized a total of four public datasets of 3D brain MRI for training their proposed network; however, the training results are missing.	It is very nice that the code will be available and the dataset is public.	Authors will submit codes on Github.	The paper meets the standard requirement in terms of reproducibility.	1) The definition of large displacement is not clear. It would be helpful to mention the predicted displacement distribution in the testing datasets explicitly. 2) It would be helpful to report the total number of voxels in each dataset in Table 1 in addition to the NJD. 3) The ablation study in Table 2 is inconclusive. Up to what amount of L the improvement in Dice would saturate or degrade? 4) For the extension, I would recommend applying the proposed method to a dataset with large deformations, such as chest CT scans (e.g., DIR-Lab 4DCT). I would recommend comparing your results to the study of [Hering2021CNN]. Hering, A., Hager, S., Moltz, J., Lessmann, N., Heldmann, S. and van Ginneken, B., 2021. CNN-based lung CT registration with multiple anatomical constraints. Medical Image Analysis, 72, p.102139.	1) The parameters such as \lambda should be detailed in the ablation study. 2) Other datasets such as OASIS should be included in the experiments. 3) Multi-iteration scheme registration experiments using the same proposed architecture should be conducted in the ablation study.	1- The authors should compare their proposed network with the state-of-the-art qualitatively. This would help to highlight NICE-Net's registration performance. 2- Limited clarity: A description of the training results would enhance the paper clarity. It is also preferable to include a brief description on the training implementation details (e.g. How many image pairs were used?).	The paper proposed injecting the deformed moving image into the decoder as a novel idea. Although the evaluation is only on the brain MR datasets, this can be sufficient for this conference paper.	The main innovation of this paper is that a different registration architecture is proposed, some other aspects such as the loss function are the same as those proposed in previous work. In addition, the comparison experiments are not conducted using some common datasets such as OASIS.	Overall, the paper is very interesting, and the method shows great potential.
358-Paper0915	Nonlinear Conditional Time-varying Granger Causality of Task fMRI via Deep Stacking Networks and Adaptive Convolutional Kernels	The paper focus on effective brain causality. More specifically on some issues related to Granger causality which are the limit of linearity and poor performance with task-based fMRI data. The non-linearity is addressed by estimating the regressive coefficient by deep stacking networks. The idea is tested on a synthetic dataset and on a real dataset.	The manuscript proposes a nonlinear Granger Causality estimation model by reconstructing the target time series based on the nonlinear modeling of source time series by a neural network. In addition, potential temporal lags between the source and target time series are modeled by the adaptive convolutional kernels (ACK) for identifying the real time lag. The proposed model was evaluated on both synthetic and real fMRI data.	The paper proposes an extension to the conditional time-varying granger causality to the nonlinear setting. The target signal Y_t is modeled as a time varying kernel (learned by a neural network) convolved with X_t after accounting for covariates Z_t. Results are shown on synthetically generated data, fMRI simulated data showing that the method can recover the true lagged GC coefficients. Results on real-world task fMRI are also presented identifying an additional causal relationship from fusiform gyrus to occipital gyrus.	The non-linearity in Granger causality is a known-issue therefore the paper focuses on a relevant topic.  The paper is well-written.	It is intuitive and novel to substitute the linear regression in Granger Causality analysis into non-linear functions, such as the stacked Deep Stacking Network used in this work. Using a network will not only improve the performance of fitting the underlying relationship between time series (thus obtaining more faithful GC estimation), but also has the potential of extending the scope of modeling (like the ACK operator introduced in this work).	The presented method is novel and applicable to real world datasets. The problem considered has a long history and is relevant to the community.	The proposed method only test on real world data fMRI sequence of a known task. It lacks of a more practical application. Topology network to test the causality is unclear in the synthetic data (it seems just couple-triple) The case in exam for the real data is more suitable for other tools as DCM. There is no discussion or analysis for indirect connections The use of convolutional networks to estimate source-target relationship is new though already proposed for other causal physiological systems (Antonacci et al. PeeerJ 2021).	"1) In addition to what the author has claimed in the Introduction section, ""One prior method limited the time lag to exactly one time point to reduce computational complexity [9], while the other methods required the user to specify the time lag a priori."", there have also been works where different time lag setting (e.g., lag of 0, 1, and 2) will be tested independently to order to account for the possible different time lags when estimating GC. In the proposed model, the ACK filtering is conceptually similar to the mentioned approach. 2) The possible typo in the sentence in page 3, ""First, CNN-ACKs 1 and 2 are trained to transform previous time points of Y_t into Y_t, and Z_t into Y_t..."" makes the reviewer difficult to understand all the latter descriptions. 3) In the experiment results of ""Real-World Task fMRI Dataset"", are the six connectivities listed in Supplemental Fig. 1 all of the non-zero causalities, including time lag k Granger causality?"	Some papers are not included in the discussion, please cite and compare (if applicable). https://www.nature.com/articles/s41598-021-87316-6 https://arxiv.org/abs/1802.05842 I am still a little confused about the optimization. Is the optimization performed purely based on the minimization of the MSE between the predicted Y_t and the actual Y_t? In that case isn't the problem underdetermined? Is there only one possible kernel that can match the data? How is this problem solved in the proposed framework? What is Z_t in the synthetic dataset? How does the model perform in cases where there are bi-directional recurrent connections? How are the p-values evaluated? What is the performance of the model under non-Gaussian (or more generally non-iid) noise? What is the performance with increasing dimension of the data and covariates? How does this compare to other approaches for nonlinear connectivity estimation, such as convergent cross mapping?	The synthetic dataset is probably reproducible, the other dataset will be probably distribute after acceptance	Implementing the network proposed in this work and performing the reproducible study is feasible (although complex) given the Fig. 2 and section 2.1. It will be better if the author can provide the source code for generating the synthetic data as well for further reproducible experiments.	Code are not included, but there are sufficient details for reproducing the results.	"There is a growing interest on convergent cross mapping (CCM) (Siguhara et al. Science 2011) as an alternative tool to address non-linearity in Granger causality and estimation of different lags. Moreover, Granger and Siguhara causality (as well as dynamical causal model (DCM)) have been criticized to mere temporal correlation tools, and perturbation based approached should be more relevant. These aspects should be mentioned. In several papers investigating convergent cross mapping and Granger causality, it has been observed that the amount of noise added during the creation of the synthetic data (e.g. using the STANCE tool) can deteriorate the estimation of the causality directionality. In the paper, it is reported the defined relationship but not this critical detail. This is not a small thing, as you might be using relatively clean data  with your ground-truth and slightly noisy data in reality, with the latter leading completely wrong causality estimation. How sensitive is the tool about this aspect? Or for the simulated data the reported noise (0.1) is already big, and therefore your simulations are even noiser than real data? It is not clear the network topology of the synthetic data. The STANCE has some kind of DMN simulation and other nucluei, but in the presented paper it seems to be a simple couple-triple relationship. The authors are asked to clarify this. Ideally you should have networks with at least 5 nodes as in (Smith et al. Neuroimage 2011, or Crimi et al. Neuroimage 2021). Couple analysis is completely a different story than multivariate. This also raises the further question about indirect causality. With convergent cross mapping a time series should be able to reconstruct even indirect causality (Ye et al. Nat SciRep 2015), is the same for your case given the conditional estimation and non-linearity? Or would you need a propagator settings as described in (Crimi et al. Neuroimage 2021). It is not clear why the authors use up to 5 lag causality. BOLD signal is very low-temporal resolution, generally causality study with those data don't go beyond 3 lags due to this (depending on the data 1 lag is the max useful). Other times the order of lags is estimated by the  Akaike information criterion, but I haven't seen this here. I cannot grasp what you are trying to convey in Fig.5 regarding lags and inhibition/excitation, please clarify better. One of the advantages of Granger causality is that is computationally less demanding of DCM and therefore more suitable for brain-wide analysis rather than few ROIs. Yet, you use it in a scenario with 5 areas and task-based. Then, why are we even using Granger causality? We could stick with DCM, which is a physiological tool ideal for task-based studies and less prone of finding temporal correlation rather than causality. ""the method captures richer information than prior methods"" is clearly an overstatement. You haven't proved the superiority compared to CCM for nonlinearity, or to propagators for indirect connections.  Without saying that since you focus on task-based with few areas justifications against DCM has to be made. Minors: some capitalization in the bibliography is lost (""granger"", ""keras""...), notation in Sec2.1 could be improved e.g. ""time point of Yt into Yt..."" Then there is \hat{Y}, please clarify or rephrase, for example at the second row  of this section."	"In the original Granger Causality modeling, the corresponding statistics of the significance can be estimated for testing whether incorporating X can ""significantly"" improve the reconstruction of Y. The reviewer would suggest performing similar investigation to derive the statistics of Granger Causality in the current non-linear setting, if possible. While the idea of using a fixed number (six) 1*2 filter to model the time-varying causal relationship at specific time lags is intuitive and working, the reviewer suggests improving this approach into a more integrated framework."	See the weakness part	The use of neural networks might be novel, but already used by other people. Moreover, the autoregressor coefficient for the linear case are already something like like machine learning predictor. If we focus on the non-linearity aspect, a comparison to Siguhara causality is necessary. The reported experiment is just an analysis for which more suitable tools exists, a further practical application is missing.	This paper proposed a novel perspective of utilizing a traditional effective connectivity estimation method (Granger Causality analysis). Using network as a non-linear approximator can improve the field of cognitive neuroscience and neuroimaging at various aspects in a similar way of this work.	Since this problem has a long history and the corresponding methods are applied broadly by the community, it's important to investigate the properties of the model in detail to help the practitioner understand the limitations and strengths of the proposed method.
359-Paper0984	Nonlinear Regression of Remaining Surgical Duration via Bayesian LSTM-based Deep Negative Correlation Learning	This paper introduced a multi-task hybrid model, named BD-Net, for remaining surgical duration (RSD) estimation. In particular, multiple sub-models of the Bayesian LSTM with forced feature diversity were incorporated to estimate both RSD and uncertainty. In the end, the network achieved state-of-the-art results with RSD estimation on cataract-101 dataset.	The authors propose a novel model for predicting remaining surgery duration in cataract surgeries. By using deep negative correlation learning, the model can estimate uncertainty in a single inference step. The proposed method outperforms state of the art and the authors show promising plots which indicate effective uncertainty estimation. Uncertainty estimation is especially important in this task due to the inherent ambiguity of future events.	This paper proposes a novel BD-Net for residual surgical duration prediction, which ensembles multiple Bayesian LSTMs via deep negative correlation learning.	This work purposed a novel formulation for RSD. Based on the architecture of CNN+LSTM, the Bayesian LSTM(B-LSTM) is introduced and implemented with deep negative correlation learning method for this regression task. The network is trained in a multi-purpose manner which perform phase classification, surgeon's experience estimation, RSD estimation and uncertainty estimation simultaneously. The number of B-LSTM models allows estimation of uncertainty in a single sample during inference by setting different dropout probabilities for feature diversity with each model, rather than multiple sampling to detect uncertainty as in previous work. A comparison on MAE results with three other state-of-the-art methods is provided with significant test showing a reliable result on 5min and 2min to the end RSD estimation performance.	The proposed method is simple and achieves strong results, even without phase/expertise labels (as opposed to CataNet) The authors use a statistical test to demonstrate the significance of their results Uncertainty estimation is likely very useful for future-prediction tasks like RSD prediction. To my best knowledge, previous RSD methods have not considered uncertainty. The uncertainty-related plots (Fig. 2(A) + suppl. Fig. 2) look very promising. The proposed architecture is builds on the one in CataNet as it used the same backbone and same-size LSTM and is therefore directly comparable. (However, this could maybe be made clearer in the paper.) Also, the training procedure is similar but simpler than the one from CataNet (essentially the first 2 stages from CataNet).	BD-Net achieves better results than the SOTA methods when applied to predicting RSD for cataract surgery. This work validates the effectiveness of Bayesian-LSTM and DNCL for improving generalization ability on RSD prediction task.	The network is validated on a single dataset and surgery. The cataract surgery is showing to be generally short (5min-20min) where the performance on longer surgery such as cholecystectomy might need further justification in comparison with other methods that are designed for those surgery. The surgeon's experience estimation is described as a function of the network but with limited information provided in definition and setups for them. Having a future work paragraph showing a developing direction of this model in conclusion section may be helpful.	All the uncertainty-related results (Fig. 2(A) + suppl. Fig. 2) are restricted to selected surgeries - making it difficult to understand the average-case performance. The qualitative results (Fig. 2(A)) only seem to show easy examples with similar duration. It is not clear how the variance-maximization objective of DNCL is compatible with uncertainty estimation. It would be very helpful to include an ablation without both DNCL and Bayesian LSTMs (i.e. only phases/expertise). It is not clear from the paper if 6-fold cross validation was used like in CataNet. If not, then the SOTA scores in Table 1 are not completely comparable. The weaknesses and possible solutions are elaborated in more detail in section 8.	The authors state that they follow the Bayesian-CNN, DNCL, and uncertainty estimation methods in [13-19]. When describing the adapted methods for RSD prediction task in this artical, the authors should give more and clearer explanations, instead of forcing the readers to read the references [13-19] by themselves.	This work was implemented on the public dataset cataract-101 and a reference implementation model is provided. Network settings and hyperparameters are detailed in the text to ensure good reproducibility.	The method description is mostly very clear and detailed. The authors also promise to publish their code. Some details could be clarified: What kind of statistical test was used to obtain the p-values? Was 6-fold cross-validation performed like in CataNet or were all 81 videos used for training? In the former case, what validation metric was used to select the best model after training (e.g. MAE-ALL or loss)? In the latter case, how often were experiments repeated? How were the MAE scores computed from multiple models (either through cross-val or repeated experiments)? Are predictions from multiple models averaged to compute one score or are multiple scores computed and then averaged? It appears that the authors report the mean and standard deviation over surgeries but this is not specified in the paper. I am not sure what exactly the ablation 'w/o DNCL' means. Is simply the loss term from eq. 3 removed or is it a non-ensemble variant with only one LSTM prediction head?	The experiments are conducted on public dataset. The code will be released.	It would be worthwhile to validate the method on other datasets, such as Cholec80. The surgeon experience labels are not necessarily present in other datasets, so testing the method without predicting surgeon experience could also be considered. From now, the method only estimates the remaining time to the end of the surgery. As different tools are needed for different phases, preliminary tool preparation is required in surgery, and the network does have the ability to classify phases, predicting the time remaining in each phase could be a potential development for this BD-Net.	"MAIN WEAKNESSES Uncertainty evaluation All the uncertainty-related results (Fig. 2(A) + suppl. Fig. 2) are restricted to selected surgeries. This makes it difficult to understand the performance of the average case and not just the best. SUGGESTION: E.g. all pearson r's could be presented in a single plot in the supplementary material. Qualitative evaluation The example surgeries in Fig 2(A) are all ca 5 minutes long and are thus probably rather easy examples. SUGGESTION: It would be more insightful to also include more difficult examples (e.g. longer surgeries) and failure cases. Variance of predictions is maximized The DNCL loss explicitly enforces the model to have high-variance predictions. How is this compatible with uncertainty estimation? The model should be encouraged to have low variance when it is ""certain"" about the remaining time. The qualitative results indicate that the model does have low-variance predictions in certain cases and that uncertainty decreases over time (as expected). One explanation could be: Since negative RSD predictions are implausible and thus discouraged during training, the predictions automatically have lower variance when the predicted RSD is lower. However, this could mean that uncertainty predictions look better than they are. The DNCL objective might make it difficult for the model to give meaningful, input-specific uncertainty estimates and instead simply give higher variance with higher RSD values. Fig. 2(A) actually suggests that this might be happening. SUGGESTION: How do the authors justify the use of DNCL for uncertainty estimation? Has this been done in previous work and if so, how is it justified there? This should be discussed more in the paper. Missing ablation SUGGESTION: It would be very helpful to include an ablation without both DNCL and Bayesian LSTMs (i.e. only phases/expertise) since it seems like these two components are most effective if used together. So using only one could potentially degrade performance. E.g. without dropout in the LSTMs, the variance-maximization might be difficult to achieve. Adding this ablation would make the combined contribution of ""DNCL + Bayesian"" clearer. Additionally, this ablation would be very similar to CataNet but would be more comparable to the proposed model since the same training and evaluation scheme was used. Results possibly not completely comparable to SOTA It is not clear from the paper if 6-fold cross validation was used like in CataNet. Since the SOTA results were directly copied from the CataNet paper and not reimplemented, the scores might not be entirely comparable if all 81 videos were used for training. However, adding the missing ablation from 'Weakness 4' would alleviate this problem. REQUIRED CLARIFICATIONS The open questions from the reproducibility section could be clarified. MINOR COMMENTS There are some typos: ""annotaitons"" (section 1), ""incorporates"" (section 2) In the abstract, the phrase ""we deeply learn"" is a bit unusual. Maybe ""we learn a pool of deep, decorrelated ...""? What exactly is meant by the ""bias-variance-covariance tradeoff"". Maybe the authors can elaborate this in more detail in the paper. The LSTM visualization in Fig. 1 appears to be based on Colah's blog (https://colah.github.io/posts/2015-08-Understanding-LSTMs/). Maybe this should be referenced. The term ""end-to-end"" in the caption of FIg. 1 might be misleading since the model is not trained end to end. Adding statistics regarding the durations of videos would be especially helpful for RSD tasks. What is the mean, median, standard deviation? Or maybe a complete plot of durations could be added to the supplementary material if space permits this. The hyphen in the evaluation metrics ""MAE-ALL"", ""MAE-5"" and ""MAE-2"" appears to be formatted as a minus in the paper. Maybe using ""\text{}"" in math mode or not using math mode at all when not necessary would make this look nicer. By naming the ablations by their missing components, I sometimes found it difficult to interpret the ablation results. maybe by representing the ablations with checkmarks it might be easier to follow which components are used and which are missing (e.g. in the style of Table 4 of https://arxiv.org/pdf/1904.07601.pdf) In section 3 (experimental setup), the authors state that videos range from 5 to 20 minutes. However, Fig. 2 (A) shows two examples with durations of less than 5 minutes. Maybe the authors could provide more precise min and max durations."	To improve the readability, the authors are suggest to detail the following contents. Describe the training and inference procedure by inserting an Algorithm, so that the dropout, ensemble, uncertainty estimation, etc. can be clearly introduced. Explain more about Eq. (5). Introduce how the default hyperparameters are selected, which hyperparameters are sensitive and should be finely tuned. Given some comparison of different hyperparameter settings if possible.	The paper is well-organized showing a new approach for RSD prediction. The results are verified with good evidence in a public surgical dataset suppressing the state-of-the-art methods. The idea of averaging the predictions of multiple diversified submodels similar to the multi-headed mechanism shows good potential in this multi-task design.	Overall this is an interesting paper which achieves strong results with a simple model. Additionally, it addresses an important topic in RSD prediction (uncertainty estimation). Some questions regarding the evaluation should be clarified (cross validation?) and the proposed ablation study (w/o both DNCL and Bayesian) would be helpful in my opinion. I find it hard to understand why the variance-maximization objective of DNCL is useful for uncertainty estimation but the results look promising. I believe this should be discussed more in the final version.	This work provides a novel BD-Net for residual surgical duration prediction, whose performance surpasses the SOTA methods. However, although the proposed methods are derived from existing approaches, the method details and training procedure should be presented more clearly.
360-Paper0560	NVUM: Non-Volatile Unbiased Memory for Robust Medical Image Classification	This paper proposes a novel training module, non-volatile unbiased memory (NVUM),  which non-volatility stores running average of model logits for a new regularization loss on noisy multi-label problem. Experiments on multi-label chest X-ray  images demonstrate the superior performance of the proposed method.	This work proposes a new regularisation loss to address multi-label medical image classification with label noise and class  imbalance. The proposed loss aims to penalise differences between current and early-learning model logits. Besides, the proposed method leverages the logit adjustment technique to unbias the classification predictions arisen by the class-imbalance issue.	The paper focuses on a real-world robust learning problem, classification on noisy multi-labelled imbalanced dataset, and proposed a novel NVUM method based on non-volatile memory module paired with a new regularization loss to alleviate noisy label effect and introduce class prior knowledge in model update to unbias the classification. Experiments show that the method outperforms other SOTA methods.	The idea is relatively novel. Well written. The overflow and structure are very clear. The method is relatively clear. Experimental results are good.	Noisy multi-label imbalanced learning is of great significance for both computer aided diagnose systems and clinical applications. The idea of penalising differences between current and early-learning model logits together with logit adjustment is interesting. The paper is well organized and the implementation details are well described.	The paper has a well-organized writing and clear motivation for each part of the proposed method: memory module with regularization term for noisy label issue, and class prior update for imbalanced issue. The paper pay attention to the combined challenges in robust learning of MIA: noisy and imbalanced multi-label medical image classification, which is a common issue in the real-world datasets but rarely explored. The paper provides detailed analysis of the gradient effect of the novel regularization term in noisy label classification with BCE loss. The proposed method NVUM is evaluated on two benchmarks with real world noisy multi-label chestXray datasets, and achieves SOTA results with large performance gain compared with other SOTA methods. Comprehensive ablation studies are reported and the effect of different hyper-parameter and prior settings are fully explored.	"Some points are not clear, like: Between Eq. (2) and Eq. (3), what is the definition of \pi_c? In Eq. (3), why utilizes z_k^i-log(\pi)? It is better to show its motivation. Since this paper aims to solve the noisy multi-label problem with class imbalance, it is better to individually provide the experiments results on noisy multiple labels, class imbalance, and noisy multi-label with class imbalance. Some related works are missed, like: [1] Laine, Samuli, and Timo Aila. ""Temporal ensembling for semi-supervised learning."" ICLR (2017). [2] Shi, Xiaoshuang, et al. ""Graph temporal ensembling based semi-supervised convolutional neural network with noisy labels for histopathology image analysis."" Medical image analysis 60 (2020): 101624."	The explanation of the effect of proposed regularization loss is insufficient. The four cases listed in Sec. 2.1 are all based on the strong assumption that networks have high confidence in the classification results. The author should rethink the gradient analysis of the noisy situation. In my opinion, author may consider the distribution of the multiplier of Jacobian matrix for clean samples and noisy samples. There is a lack of discussion of the class-level AUC results. In the experiment part, it's insufficient to reflect how the method handles noisy and imbalance labels.	One concern of NVUM is about the space size of the memory module, which is S*C and will linearly increase with the size of training set. Since the paper focuses on real world large scale datasets, when the training set gets extremely large, the size of memory module might be a severe issue. There's no ablation study for different noise level in training set. Most paper focuses on noisy label problems will introduce different level of noisy labels to the clean training dataset and evaluate their methods under multiple noise levels. This paper does not contain such ablation study, so people would have no idea about the noise tolerance of the proposed NVUM method. Another potential problem is that, NVUM takes class prior distribution into account, however, this prior distribution is directly estimated from noisy training set, thus the prior distribution might be corrupted under severe noise level. This issue is mentioned in Future Works part. One minor issue: Figure 1 is never referred to in the paper, and I think it's demonstrating the training and updating pipeline of NVUM. Please add reference to Fig.1 in the paper.	The method is relatively simple and the implemental details are clear. So i think this paper has good reproducibility.	The implementation is enough for the reproducibility of the paper.	As the author indicates in Abstract, the code will be made available. In addition, the training hyper-parameters and preprocessing method are provided in paper. The datasets and model backbone are all open accessed. Thus, the work should be reproducible.	"It is better to explain the following points: Between Eq. (2) and Eq. (3), what is the definition of \pi_c? In Eq. (3), why utilizes z_k^i-log(\pi)? It is better to show its motivation. Since this paper aims to solve the noisy multi-label problem with class imbalance, it is better to individually provide the experiments results on noisy multiple labels, class imbalance, and noisy multi-label with class imbalance. It is better to discuss the relationship to the following related works, which also generate the predictions using the similar form as Eq. (3), like: [1] Laine, Samuli, and Timo Aila. ""Temporal ensembling for semi-supervised learning."" ICLR (2017). [2] Shi, Xiaoshuang, et al. ""Graph temporal ensembling based semi-supervised convolutional neural network with noisy labels for histopathology image analysis."" Medical image analysis 60 (2020): 101624."	The author should rethink the gradient analysis of the proposed regularization loss. The author should consider the change of the label distribution and the multiplier of Jacobian matrix during training phase. The authors should consider to provide more exhibitions to highlight the role played by the proposed method.	Please add a discussion about the potential size increase of memory module. It's better to evaluate the proposed method on different noise levels by adding multiple level of noise to a clean multi-label dataset. This is important to explore the noise tolerance of the method. I also suggest that exploring the class prior distribution estimation under different noise levels, since the prior might heavily degenerate when severe noise in training set.	The idea is relatively novel. Well written. The overflow and structure are very clear. The method is relatively clear. Experimental results are good.	The idea is interesting, although some explanation on method and experimental results are insufficient.	The paper provides strong motivation, clear method analysis and comprehensive experiments with SOTA performance and large performance gain on two large real world datasets, with only minor weakness in experiments, therefore I recommend the paper being accepted.
361-Paper2729	On Surgical Planning of Percutaneous Nephrolithotomy with Patient-Specific CTRs	This paper proposes an optimization algorithm that approximates the ellipsoidal geometry and orientation of kidney stones for patient-specific surgical planning in Percutaneous nephrolithotomy (PCNL) using concentric-tube robots (CTRs). The paper evaluates the algorithm using 7 sets of CT data which are segmented to construct point clouds of kidney stones for these cases. Some of the results show promise with respect to positional error generated.	This work proposes a method to design a patient-specific concentric-tube robot for percutaneous nephrolithotomy. Access to kidney stones is difficult and often requires multiple insertions. This work proposes a three-tube CTR to access difficult-to-reach stones, where the first one is straight, and the second two are both straight and curved. The stone is modeled as an ellipsoidal, the principal directions of the ellipsoidal are used to guide the CTR design.	A nested optimization-driven scheme for determining a single tract surgical plan along which a patient-specific concentric-tube robot (CTR) is deployed to enhance manipulability along with the most dominant directions of the stone presentations.	This work presents a novel optimization approach of kidney stones for the purposes of surgical planning. When further validated, this could prove informative for patient-specific planning of PCNL procedures. The use of CTRs is usually justified by the need for more granular maneuverability because of their ability to create complex curves. The importance of this work also lies in the fact that the algorithm doesn't require human intervention which makes the process much more autonomous, contrary to previous works cited.	Method evaluated on real patient data with clear clinical utility Thorough description of the optimization process Fairly accessible compute requirements	1- The topic is interesting  2- Well-Structural writing	Authors should further elaborate on the implication of their results and relate back to the original objectives. The conclusion appears abruptly after the results but more discussion of the cases outputs is needed. Error plot shows a deviation for all 7 cases. The authors should discuss what those deviations mean and what the implication of a 1 mm error means. Figures need to be larger and clearer to make it easier to see the scale (esp. Fig 1).	Minimal evaluation on whether the path is clinical useful (Unclear whether reaching the calyx is sufficient for successful PCNL and whether the whole path of the CTR is feasible without damaging surrounding tissue) No comparisons to other methods No discussion on the path to clinical deployment after calculating these parameters	1-The Novelty is not highlighted, the author should describe more about the Significant improvements of their work.  2- lack of comparison with the state-of-art  3-Some English error but its possible to understand	The paper describes the generation of point clouds using Slicer3D. More detail would help with respect to filters and marching operations used to segment the CT data. Answers to reproducibility checklist make sense and the authors do a good job explaining the algorithm and tools they used.	The optimization problem and the libraries used to solve them are clearly described. The actual values used for the optimization problem are missing (every variable in (4) should be defined as a real number) as are any hyperparameters such as stopping criteria. The small patient sample limits the reproducibility of the results.	he dataset is very small, but for primary results it's ok	"Revise sentence in section ""Constrained Inverse Kinematics for CTRs"": ""has been addressed with by"". Figures need to be more legible and larger. Need more commentary on the results and its implication."	"The paper presents a neat engineering solution to personalized CTR design for PCNL. It has a lot of potential for clinical application. An image like Fig 1. with the CTR path and tube segments overlaid would help clarify the contributions. One question I had was whether 5 h for optimization is a clinically viable solution. From section 5, it was unclear whether the 5 h was for all the runs of the algorithm or for one run (and then repeated twice more). Is the runtime feasible for every patient? How does the proposed method compare to learning-based methods such as [1]? These tend to be faster than traditional optimization approaches at deployment time. [1] Liang, Nan, Reinhard M. Grassmann, Sven Lilge, and Jessica Burgner-Kahrs. ""Learning-based inverse kinematics from shape as input for concentric tube continuum robots."" In 2021 IEEE International Conference on Robotics and Automation (ICRA), pp. 1387-1393. IEEE, 2021. Comparisons to previous works are needed. For example, what would the error in reaching the calyx be using the optimization technique in [1] or [2]? [2] Morimoto, Tania K., Joseph D. Greer, Elliot W. Hawkes, Michael H. Hsieh, and Allison M. Okamura. ""Toward the design of personalized continuum surgical robots."" Annals of biomedical engineering 46, no. 10 (2018): 1522-1533. What is the path to clinical deployment after this? Is it realistic and helpful to calculate the particular length of each section and custom make the CTR for each patient, or is it more realistic to expect an algorithm to need to optimize over a set of pre-made tubes? While the paper presents the clinical goal as being able to reach the calyx accurately, it is unclear how the ability to reach those points translates to the ability for total stone removal. The cases all differ greatly in how closely the ellipsoids resemble the kidney stones they encompass. Additionally, any discussion on how closely the paths match the constraints imposed by the anatomy would also help understand the clinical benefits this method could provide. It is usually desired that the robot does not exert much force on the anatomy to avoid damaging the tissue. Does the robot sufficiently match the whole path so that is the case? More discussion in general on the strengths and limitations of the proposed method would be helpful. Minor grammar note - ""2 cm"" instead of ""2cm"""	This paper could be more strength by comparing by literature works. in addition, the novelty of this paper need to be improved. However the objective is good but what is new your method compare to other methods? i.e. in terms of accuracy of or time assessment, is there any improvement?	Novel approach and a clear application of it. Paper is well organized and clearly identifies the methods and how they address the problem of limited manipulability in PCNL procedures using CTRs.	This seems to be a promising approach to a clinically relevant problem but as the paper currently stands, it seems insufficiently validated (in its choice of metric and in comparison to previous works). More discussion on the proposed method's limitations and path to clinical translation would also make it stronger.	1- topic 2-well explained 3-promissing results
362-Paper0175	On the Dataset Quality Control for Image Registration Evaluation	The paper identifies the very important issue of quality control for landmark points in public data sets used for the evaluation of the registration algorithms that underpin much of MICCAI. The paper describes a methodology to evaluate landmark point quality and demonstrates feasibility on two public data sets.  The basic hypothesis is that variograms can be used to quickly identify suspect landmark points in the data.	This paper presents a new efficient approach to test the quality of image fiducials in (medical) image data sets. The approach is quick, reliable and easy to use and interpret by applying the variogram from the geosciences.	The authors propose a method to assess the quality of paired landmarks that were manually labeled in corresponding images of registration datasets. Variograms, an existing statistical tool, are used to create 2D representations of 3D landmarks distributions. These 2D variograms can be easily checked by an operator to identify potentially problematic landmark pairs, as these cases with localization errors yield specific patterns. The method is applied to two open-sources datasets of MR and intraoperative US images of the brain. Among more than 700 landmark pairs, 29 were identify as potentially problematic. After a review by third-party clinicians, the poor quality of these landmark pairs was indeed confirmed. The proposed method is thus an interesting tool to check/improve the annotations of publicly available datasets.	Quality control and the ability to independently evaluate quality is absolutely key to the deployment of public data sets for registration evaluation. This paper proposes and evaluates a novel method to evaluate fiducial point quality using variograms. I am not aware of prior use of this methodology, nor I am aware of other practical methods for the evaluation of fiducial point quality. The methodology described in the paper (with some exceptions, see weaknesses) could be applied to most data sets used for registration evaluation, and therefore is potentially a very valuable contribution to the field.	A well written contribution that introduces the method to a new field and applies to the use case of fiducial / landmark selection within a clinical environment.	"The main strength of the paper is it topic itself: enhancing the quality of medical annotations. This topic is rarely addressed but is of prime importance as annotations are extensively used for training or evaluation of methods both in the MIC and CAI communities. The ""specificity"" appears good: most (if not all? see details below) variograms tagged as problematic correspond to landmarks of poor quality. Once identified, these landmarks could thus be improved. The methods is sound and clear, and several patterns of variograms are clearly described (isolated landmarks, clusters, ...). While not fully automatic, the proposed tool clearly fasten the control quality process."	The paper has two significant weaknesses that I believe could be addressed quite easily. The paper does not adequately evaluate the authors' key hypothesis that variograms can be used to rapidly identify suspect landmark points. The authors classify points into three groups (definitely suspect, maybe suspect, and OK). Samples of these points are then given to 2 independent radiographers. To test the hypothesis the paper needs to show that there is a statistically (and practically) significant difference in the number of suspect points confirmed by the radiographers in these samples. At present the paper does not show this. The first part of the point classification algorithm (construction of the variograms) is well described and I am confident it could be reproduced. The second part of the algorithm, (classification of fiducial points from the variograms) is less well described and I am not confident it could be reproduced. More details need to be given on the skills and training of the people doing the classification. More quantitative data could be given on this process.	"In the methods section, the pairs of image data sets are to be registered. Right? If so, the process of registration will affect the methodology here, as the quality of the registration depends on the choice of the landmarks. A vicious circle. If the pairs are not registered, how are the landmarks compared?? The work of Bardosi et al on FLE_image, IJCARS, presents very relevant information for this type of research and should have been addressed. This method is more or less a qualitative approach to testing the ""quality"" of selected fiducials that is based on the assumption (end of section 2.1) that wider separated landmarks have larger differences in their displacement. First, this is not backed by any reference and second, it is counter-intuitive as it would imply a ""bias"" proportional to the distance."	"the main weakness is that the ""sensitivity"" of the method is unknown. Especially, how many landmarks of poor quality are not identified through the variograms? (missed cases) It is difficult to assess this, since all landmarks would have to be thoroughly checked by third-parties (or maybe by looking at the most challenges cases in challenges like [30]?). Nevertheless, this limit should at least be discussed in the paper. they might be other patterns in variograms that were not identified. it would have been interesting to list (some) other datasets that could be directly checked with your method."	The paper uses publicly available data sets. The authors do not provide the code required to construct the variograms, however their description of the algorithm used is sufficient for this.  I am not confident of the reproducibility of the remainder of the classification process. As described it sounds like it may be quite operator dependent. I think the authors need to better formalise this part of the algorithm.	Data will be reproducible.	The method is clearly detailed and could be easily reproduced. Providing the code would be a plus, so that future dataset providers could use it while checking their annotations.	"Quality control and reproducibility of data used for registration evaluation is of vital importance to the MICCAI community. I believe this paper has the potential to make a very significant contribution to the field. At present the paper has a very significant weakness in that it does not properly test the key hypothesis, however if the authors address this weakness I would change my recommendation to strong accept. If the paper included further data on reproducibility and accompanying software to enable deployment on other data I would make an even stronger recommendation. As detailed in my answer at 5, the paper needs to be slightly restructured and more experimental data added to properly test the hypothesis that variograms provide a reliable way to evaluate the quality of fiducial points used in these data. The authors need to provide a three arm experiment showing that points identified as suspect are identified correctly, i.e. there are significantly more suspect points in this sample than points identified as OK or potentially suspect. Further to that the authors should provide a more formal description of the algorithms used to classify points from the variograms so that the work could be reproduced or applied to other data. Other comments: Page 1, para 2: ""FRE are uncorrelated [8], in practice, TRE is approximated by FRE, and these two terms are often used interchangeably."" You can't claim this without some evidence. If people are using the terms interchangeably they are wrong and need to be put right. Putting this sentence in your paper risks people citing your paper as evidence that TRE and FRE are equivalent. I think you should just delete the second part of the sentence, or say something like ""however a surprising number of researchers fail to understand this."" Page 2, para 1: ""they may contain FLE"". I think you should reword this. The source of errors of localising a target and a fiducial are not necessarily related. I think you should try something like ""they will contain a localisation error, i.e. the TRE is itself only an estimate of the true error"" page 2 para 6. Maybe a bit more on why you've chosen the variogram over other methods? What other methods are there? Figures 2 to 6 encode really important information using only differences in colour. Please consider using non colour cues (i.e. different shapes like squares and crosses) and/or a colour blind friendly palette (https://doi.org/10.1038/nmeth.1618)"	Please address issues in point 5.	"after review, average score [1-4] of Cat 1 (problematic): 1.4 after review, average score [1-4] of Cat 2 (atypical): 2.4 after review, average score [1-4] of Cat 3 (normal): ? For the problematic pairs, the average score of 1.4 ([1(poor), 2(questionable), 3 (acceptable), 4 (good)] generally confirms the variogram category. -> How many problematic landmarks were eventually noted 3 or 4, if any? ""Both datasets have manually annotated corresponding landmarks on pre-operative Magnetic Resonance (p-MR) and intra-operative Ultrasound (i -US) images [20, 30]."" -> since the landmarks are part of the datasets themselves, better cite the original papers here? [16, 29]. The other papers are mostly stressing the use and importance of the annotations Eventually less than 5% of all landmarks were of poor quality. These landmarks have to be improved, but this low figure eventually remain a good news!"	The paper has a significant weakness, however I think this could be very easily addressed during the revision and rebuttal stages.	This contribution is providing a novel approach without really assessing the current state-of-the-art in assessing image fiducials, buids on unfounded assumptions and is somewhat unclear / imprecise in the description of the methods.	The major factor is the importance of the topic and the proposed method to address it.
363-Paper1718	On the Uncertain Single-View Depths in Colonoscopies	The paper introduces bayesian neural networks to single-view depth estimation in colonscopy. Furthermore, the paper discusses synthetic-to-real domain challenges, self-supervised methods and introduce a new student-teacher model that considers the teachers uncertainty. These introduced methods and the results described in the paper can be valuable over a wide variety of endoscopic image analysis applications.	This paper applied Bayesian deep networks for estimating depth maps with uncertainty for colonoscopic images. The additional teacher-student model taking into account teacher's uncertainty further improves the depth estimation accuracy. The proposed approaches were validated on both synthetic dataset and real colonoscopic dataset.	This paper presents a method to estimate both depth and uncertainty in the depth estimates using a teacher-student model architecture and the method is able to generalize to real data even when trained on synthetic datasets.	The paper starts off strong, introducing the domain topic and terminology around uncertainty. The self-supervised learning and teacher-student model with uncertainty are novel. Especially, the latter as the paper links this well to domain shift and label reliablity. The paper combines both aleatoric and epistemic uncertainy and the results indicate a clear distinguishment between both. The results from the paper can have impact on a wide variety of endoscopic CAD applications, where a more accurate and reliable (monocular) depth estimation, could lead to a better performance.	The authors claim that they are the first to apply Bayesian networks for depth estimation with uncertainty on colonoscopic images.  The paper is well structured and both quantitative and qualitative validation results of the proposed methods are also provided.	Authors present clear explanation of the methods with thorough explanation of the intuition behind the methods.	Only an internal comparison are performed, against variants of the proposed methods methods. Continuously, this makes the fact that the teacher student model outperforms the other methods less significant/impactful. Furthermore, no mention on other works that combine AU and EU quantification. While two data sets are used for evaluation, only one of them consists of real data and focusses on a single application. In my opinion, this is a big loss, as I have the feeling that the proposed method could also show interesting results for other applications. Demonstrating this would really improve the impact. Of course there is limited space in a conference paper, but I would love to see some additional data/tasks for evaluation. The authors fail to clearly mention the limitations of the proposed approach and makes often claims that are a bit too bold. The uncertain teacher performs only incrementally better over the teacher-student (Table 2), and in all honesty, to me the increment is negligible, even though it seems to be consistent.  This combined with the fact that the training is only done over one split/initialization begs the question if the results are statistically significant or not, this is not mentioned in the manuscript. Paper lacks implementation details on the model besides information on the loss functions and the metrics are not explained. More in general, the paper feels fragmented at times. There could be more effort in to making the paper more coherent.	The authors may want to revise the literature review to better summarize the related works.	Details in implementation are missing. For instance, how many epochs were models trained for, what hyperparameters were used and how were they chosen, etc. Without these details, results from this method may be difficult to reproduce. Clarity in the results section could be improved (see detailed comments).	The paper lacks reproducibility but the novelties are well discusses and can be implemented by the reader. The paper could contain an appendix with details on their architecture and training methods. Metrics not well explained. It is a shame that a lot of the points on the given reproducibility list are not provided (the authors are honest and open about it though. The paper could be a lot better on this front by including these details. I have the strong feeling that the page limit has had a strong negative impact on the complete description of this interesting work.	Methods applied the paper are well referenced.	Reproducibility needs improvement. Model training details like hyperparameters used are not present in the current manuscript.	"Honestly I really like the work, but its quality is currently not sufficiently reflected by the manuscript in its current form. In my opinion, only two main issues should be addressed to address this: Clear and complete writing: The authors should add a more complete description of the proposed approach, that would enable reproducibility of the results. Also, some related work regarding aleatoric and epistemic uncertainty could have been included. Furthermore, some more discussions on the limitations of the proposed approach would be of great value to the manuscript. Complete and extensive evaluation: Currently, the evaluation is a bit meager at times, especially since the differences with respect to the state of the art is minor. Including more different applications here would really help strengthen the generalizability of the work. Finally, some minor issues: Pre-requisite -> prerequisite ""... single-view depth estimation in colonscopy"" An exhaustive analysis seems a bit too much. I would refrain using perfectly. MC dropout is a practical and scalable approximation of VI, but not the most reliable. ""Illumination"" in section two on bayesian preliminaries. 18 networks for endoscopy is quite expensive, perhaps Variational inference could still have been applied by making only a few crucial layers bayesian."	"Please revise the literature review of Single-View Depth Learning to provide a more precise summary of the existing works. On Page 7, ""However, we observe that it successfully generalizes to the real domain..."". Does the network generalize well because there is not much appearance difference between the synthetic dataset and the EndoMapper dataset? Do we expect bad generalization ability when applying to a real endoscopic dataset with a different surface appearance? How many colonoscopic procedures are included in the EndoMapper dataset? Please address the typos in the paper."	"Authors should reference Liu et al. 'Reconstructing Sinus Anatomy from Endoscopic Video- Towards a Radiation-free Approach for Quantitative Longitudinal Assessment' on pg. 4 where they describe their network's dual outputs of depth and variance (I believe this reference outputs a mean depth map along with the standard deviation). Are the rotation and translation of Eq. 5 the predicted relative camera motion? If so, please clarify in text. While minor, it should be pointed out that according to the manuscript guidelines (https://conferences.miccai.org/2022/en/PAPER-SUBMISSION-AND-REBUTTAL-GUIDELINES.html), the 'paper itself must contain all necessary information and illustrations by itself' and not require readers to refer to references to understand, for instance, the evaluation metrics. I understand space is a constraint, but if authors are able to include in parentheses, for instance, what AUCE stands for where it first appears in text, that would be helpful. To clarify, for evaluations shown in Table 2, the COLMAP reconstruction is used as ground truth? Since the model trained with supervised GT already performs fairly well on real data, to what is the teacher-student architecture contributing to domain transfer? The sentence ""Note that, in general, teacher-student depth metrics outperform the models trained with GT supervision in the synthetic domain and with self-supervision in the real domain"" is confusing and it is unclear what the authors are trying to claim. Are authors claiming that synthetic supervision for teacher-student models generates lower depth errors on synthetic data? Should this be shown in Table 1? Fig. 4 caption - should b) say teacher-student instead of self-supervised?"	The novelty is interesting but the metrics and experiments are not well conducted/explained. The authors are the first to apply uncertainty quantification to this domain and introduce new methods for it. However, the results shown provide only limited support to the made claims. Improving on this front would greatly improve the generalizability of the work. The novelty of the work and the potential impact on other applications still motivates an accept in my humble opinion, albeit a weak accept, given the current limitations.	This paper presents a novel approach for estimating both depth maps and the uncertainty of colonoscopic images. The proposed teacher-student learning model with uncertainty further boosts the network performance.	While the paper needs improvement, it introduces nice concepts and shows interesting results. Due to the missing clarity, I would not recommend direct accept, but is a good candidate for acceptance after revision.
364-Paper0816	One-Shot Segmentation of Novel White Matter Tracts via Extensive Data Augmentation	In this paper, the authors intend to implement one-shot segmentation of white-matter tracts by a novel data augmentation method. This work is extended from [10] (namely IFT) by borrowing its pretraining and fine-tuning framework, and the data augmentation is implemented by its proposed random cutout and tract cutout strategies. Then the augmented training images are used in constructing WM tract segmentation model (using TractSeg as the backbone). They demonstrate their segmentation performance using the downsampled HCP dataset (CQ) and the in-house dataset (IH).	The authors propose a method for segmenting novel white matter pathways with only a single annotated image by extensive data augmentation. Data augmentation is based on different types of masking out image regions. While the compared state of the art performs indeed well for images with few-shot annotations using a transfer learning framework, which relies on a fine tuning strategy, the segmentation based on one-shot annotations is challenging. In their experiments, the authors train segmentation models for each augmentation strategy separately and then ensemble the results of these models for the final segmentation.	This is an interesting paper about using transfer learning to predict white matter tracts in dMRI data.	The motivation for one-shot WM tract segmentation is sound, and their implementation in the random cutout and tract cutout is easy and clear to understand	The authors address the ongoing challenge of reducing the annotation time for novel white matter tracts, their presented method could lead to further improvements in this field. Using their approach, the authors find a solution to extend the performance of an already existing and established method (TractSeg) and enable it to perform well in a one-shot scenario. The authors use an in house and public available datasets and on both datasets the performance compared to the baseline is improved.	Given the fact that in the field the studies of analyzing white matter are limited by the number of available tract segmentations, the proposed method can be useful to generate potentially more based on existing data without requiring tons of manual seminations.	The data augmentation method is quite straight-forward, and it is not convincing that implementing cutout as the augmentation is the most suitable solution to this topic. Lack of literature on data augmentation for segmentation, and also no comparison between the proposed method with the SOTA augmentation method. Descriptions in Section 3 are not clear and should be organized in a more comfortable way.	The discussion is extremely brief. Limitations and possible ways for further improvements are not discussed. The authors use existing methods and combine them. No methodlogical novelty.	One minor comment that I have the private dataset used for testing has very similar quality with the HCP data. Wonder if the authors have any comments about applying to it to low quality, clinical style dataset.	The paper has some descriptions of the methodology with provided equations, which should help the readers to understand the main idea of the work and reproduce its algorithm. Though it is still suggested for the authors to provide the source code online, which can be used to fully validate the proposed method and understand its mechanism.	The authors use open available data of the HPC. The authors use TractSeg as backbone, for which code is available. No code is provided in for the framework their paper is based on, nor in their paper. However, mathematical definitions are provided and based on the checklist the reproducibility seems to be given.	good	It is suggested that the authors provide an overall pipeline of the framework, which can help the reader to make a clearer view of the major works in this paper. Descriptions in Section 3 are too complicated and unclear to understand how they manage to demonstrate the validity of the proposed method, there are many Abbreviations jumping everywhere in that section, and I strongly suggest that they write Section 3 and organize all the terms there in a more comfortable way. The authors seem to try augmenting the training data for WM tract segmentation, by implementing cutout to the one-shot data. I believe that the authors should further claim why they think that cutout is the most appropriate to this task, as there are a lot of researches in data augmentation for segmentation, many of whom are even for one-shot task. The authors have discussed none of the augmentation literature, which is also surprising to me, and they also haven't compared their method with the alternative augmentation methods available.  Besides, it is also curious to know what is the upper bound of the WM tract segmentation performance, when using normal number of training samples instead of one-shot manner to construct the segmentation model. In this way, it is clearer to know if this one-shot segmentation has reached its limits, or there remains some spaces for further improvements.	For the training process, the authors used the original HCP data. To mimic a more challenging and realistic scenario, they decided to downscale the original HCP data and use this downscaled data to further train the model with respect to the new wm tract. This simulates that the data for further training was obtained differently than the already annotated data from the first training. However, it would be interesting to show the performance of the network even if the further training was also based on the initial data (in this case, the original HCP data). It would also be good to directly compare the one-shot scenario with data augmentation to the few-shot scenario without data augmentation, both in the context of the IFT. This would display on whether few-shots may eventually become some kind of obsolete.	This is an interesting paper about using transfer learning to predict white matter tracts in dMRI data. Given the fact that in the field the studies of analyzing white matter are limited by the number of available tract segmentations, the proposed method can be useful to generate potentially more based on existing data without requiring tons of manual seminations. One minor comment that I have the private dataset used for testing has very similar quality with the HCP data. Wonder if the authors have any comments about applying to it to low quality, clinical style dataset.	The work introduced in this work is complete, the experiments seem to be nice, but their lacking of discussion in the SOTA augmentation methods hinder their contribution, and the writing in Section 3 also need improving.	The paper is well structured and the method is evaluated properly. The authors address a the weakness of an already existing approach and solve this weakness by introducing their approach of extensive data augmentation. I do not recommend oral presentation since the manuscript does not include any methodological novelty. It should be accepted though, since the approach to enable successful one-shot training is of interest to the community.	Given the fact that in the field the studies of analyzing white matter are limited by the number of available tract segmentations, the proposed method can be useful to generate potentially more based on existing data without requiring tons of manual seminations.
365-Paper0658	Online Easy Example Mining for Weakly-supervised Gland Segmentation from Histology Images	This paper proposes an online easy example mining (OEEM) method for weakly-supervised gland segmentation, which can distinguish easy and confusion pixels in the pseudo labels. In training, the proposed metrics are used for weighting the loss functions, where an easy pixel has a higher weight, and a confusion pixel has a lower weight. In the experiments, the proposed method was better than the compared method.	The paper proposes a method for weakly-supervised gland segmentation from histology images. The method can mine the credible supervision signals in pseudo-mask and mitigate the damage of noisy regions.	This paper introduces a technique for weakly supervised semantic segmentation (WSSS) of glandular structures in histopathology images with image-level labels. To address the challenge of segmenting glands with similar/confusing homogeneity and low contrast, Online Easy Example Mining (OEEM) is proposed to mine confident regions in the pseudo-masks and reduce noise by suppressing ambiguous regions via a novel normalized loss.   Extensive experiments on a public glandular dataset validate the effectiveness of their method over prior state-of-the-art; including several ablations on the variants of the OEEM losses.	*The approach that weights the loss based on the confidence of the segmentation network is reasonable. *In the experiments, the proposed method was better than the compared method.	(1) This paper analyzes the difference between histology image and natural image in segmentation task, and the difficulty of gland segmentation. (2) The paper proposes a method that encourages the network to focus on credible supervision signals rather than noisy signals.	* The paper is well motivated, clear and easy to understand. Weakly supervised glandular segmentation is non-trivial due to the underlying homogeneity of tissue morphology and low contrast, especially since existing WSSS methods for natural images may underperform. * The  proposed OEEM and normalized losses are novel with several ablations highlighting it's effectiveness. * The use of a publicly available dataset is a plus, with a strong supervised baseline. * I appreciate the intuition employed for the different variants of the losses, especially considering the several assumptions drawn regarding normal and diseased images.	*The related works are not sufficient. The most related work was not cited and there is no discussion about the contribution from the following paper. This paper also focuses on the problem that pseudo labels generated using CAM are noisy labels, and introduces weights to the loss function, where the weight is obtained on the basis of uncertainty. The method is not completely the same but the idea and approach are very similar to the proposed method (the reviewer feels that the technical contribution is minor from this related work). Please check it. In addition, the adaptive training strategy is a common approach for learning with noisy labels. These are not cited and discussed. There are cited in the related works in the following paper. Li et al., Uncertainty Estimation via Response Scaling for Pseudo-mask Noise Mitigation in Weakly-supervised Semantic Segmentation, AAAI2021. *Evaluation was not sufficient. In the evaluation, this paper listed many supervised approaches, but only one weakly-supervised method. In particular, the reviewer recommends the authors to compare with a typical pseudo labeling that usually takes pixel sampling using the confidence or uncertainty and trains a network with the masked loss, which ignores the non-selected pixels. This sampling strategy also has the ability to avoid confusing pixels. To show the effectiveness of the weighting by OEEM, this ablation study is required. Then, add the discussion of why weighting is a better strategy than the thresholding-based pixel selection method. *It is unclear whether the training and test data were separated by patients (i.e., these data do not contain the same patient or WSI). Because the image features have similar features in a WSI, the network may be overfitted in training if the test and training data contain the images captured from the same patient or WSI.	(1) The method description is not clear enough. For example, how patch-level labels are composed and how they differ from image-level labels. (2) Why equation 6 performs best requires careful analysis. (3) As a weakly supervised method, the paper only compares with a weakly supervised algorithm SEAM, and the experiment is inadequate.	* The difference in performance between SEAM and PSPNET+ResNet38 (w/o OEEM) is questionable. This suggests the improvements are from the architectural choices.  * For fair comparison, OEEM should have included to prior/compared works to validate that performance gains are not based on the network choice. E.g., UNet + OEEM, MedT + OEEM. * It is unclear if results on the fully-supervised methods on Glas Dataset are re-implemented or taken directly from the prior papers.  * It is unclear how loss map $L$ was obtained from $\hat{X}$. Descriptions regarding this point were limited.	Reproducibility is fine.	Reproduction is difficult because the method description is not clear enough.	Descriptions of the method are sound. Authors will provide code upon acceptance.	*In the fully supervised learning, PSPNet was described as 'ours w/o OEEM'. However, this is not contribution from this paper. The description 'ours' causes miss-understanding. *In ablation study, there is a mIoU in CAM. CAM is a not mask data, whose pixel has a value. How to compute the mIoU? *What does the SEAM CAM indicate? The reviewer could not catch up the setup of the ablation study. Please clarify it. *In the paper, several metrics were proposed, and only the empirical conclusion was described. Please add the discussion why the l_normal was the best. *Citation [15] and [16] are the same paper (duplicated).	(1) It is recommended that the authors describe the method more specifically and clearly, and present not only the common weakly supervised methods but also the special features used for medical images. (2) It is suggested to add more experiments to validate the methods in this paper.	I commend the authors for an interesting take on glandular segmentation with image levels with OEEM and its variants. My main concern is whether the evaluation of SEAM, including the training procedure are sound. Based on Table 2, SEAM and the proposed fully supervised (PSP+Res38) report a significant difference in performance. They both employ the same backbone to produce pseudo masks for segmentation training, yet the reported scores vary. I hope the authors can clarify.  By the authors own admission, MIL methods are commonly employed for this task. However, no recent MIL methods were included in the evaluation. This can potentially better support the argument. Based on the current results, it serves to suggest the technique is not model/backbone agnostic. (see prior comments regarding UNet+ OEEM etc)	As described in weakness, a very similar idea has been already published and there is no discussion about the contribution from the related paper. In addition, the evaluation was not sufficient. In the paper, comparing weakly-supervised methods is important, however, the authors listed many supervised methods and only one weakly-supervised method. Therefore, my rating tends to 'reject'.	Some details in the paper need to be improved.	The proposed method improves weakly supervised semantic segmentation performance over prior art, including an improved supervised baseline. However, comparison is somewhat limited. For instance, only a single WSSS model i.e., SEAM was included in the evaluation. Therefore, performance improvements may be mainly dependent on other factors such as multi-scale testing and architectural choices. If the authors can adequately clarify/address the concerns, I am willing to raise my score.
366-Paper1267	Online Reflective Learning for Robust Medical Image Segmentation	This paper proposes a test-time adaptation method for segmentation. The authors propose to synthesise the images based on the segmentation prediction and the sketch of the input image. By iteratively improving the quality or the similarity of the synthesised image and the input image, the segmentation performance improves. Due to the domain shift, the authors propose to use two similarity losses and the GAN losses to train the model.	The paper proposes a segmentation framework with test-time adaptation to adjust the model for each test image (before inference), which can be useful if the test image is from a slightly different domain.	This paper introduces a novel approach for test time adaptation, employing a cGAN-based image synthesis network to guide the segmentation network finetuning. Specifically, at test time, the segmentor is finetuned to minimize a structure similarity loss computed on the input image and the synthesised image generated by the cGAN (conditioned on the network output and an auxiliary canny map).	"The paper is well written and easy to understand. The problem is well motivated. The method of synthesise and ""reflect"" is interesting, novel and technically sound. The demo/video included in this paper is a plus. The experiments and results are extensive."	"The paper addresses an important topic in medical imaging of ""on the fly"" model adaptation to a new test domain.  The approach appears to be novel, and most design choices are sound. The method is evaluated on 3 datasets, and achieves the best results. The method is compared to relevant competitors as well as to general sota segmentation models."	The proposed structure similarity loss is sophisticated yet effective, which is a combination of mutual information loss and the normalised cross-correlation loss, weighted by predicted heatmap. Extensive experiments were performed on three cross-vendor datasets. Results demonstrate the efficacy of the method. Ablation study has also been conducted to verify the contribution of each component. The online demo provided is impressive.	"Meta-learning DG approaches are not mentioned in related work. At least the ones in previous MICCAI [1, 2]. I also suggest the authors to cite the TTA papers in previous MICCAI. There are also some relevant papers in last DART workshop. It is unclear how the segmentor for differentiable learning works. It is unclear what is the label intensity value. The multiplication and addition are not clear. I suggest the authors to update and clarify. If use the normalised bad heatmap (with failures) as the attention matrix, will it not ""emphasise"" the wrongly classified area (such as the RV area in Fig.2)? I mean the purpose is to penalise the wrongly classified area. But with the attention, it does not work as expected. I think ablation should be conducted and discussion about this needs to be included. The results of the proposed method are relatively very similar to some strong baselines. I think statistical significance analysis needs to be conducted, where the authors clicked the check box about this in the reproducibility response. In fact, I don't think the inference time is a selling point of TTA method. Typically, TTA method is model-agnostic. Although, nnUNet based models are slower in terms of inference time. TTA + nnUNet models will definitely achieve better results, which has not been touched by the authors yet. For TTA, one problem is overfitting when the number of iterations goes higher. It is unclear if this happens for this method. The number of iterations in this paper is predefined as 10. It will be good to see some more experiments on this hyperparameter. The overall objective is missing. The approximated MI is not clear. Self-contained description needs to be added for the readers to understand that loss. References: [1] Liu, X., Thermos, S., O'Neil, A. and Tsaftaris, S.A., 2021, September. Semi-supervised meta-learning with disentanglement for domain-generalised medical image segmentation. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 307-317). Springer, Cham. [2] Liu, Q., Dou, Q. and Heng, P.A., 2020, October. Shape-aware meta-learning for generalizing prostate MRI segmentation to unseen domains. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 475-485). Springer, Cham."	Out of 3 datasets used for evaluation, 2 are in house datasets (which makes it difficult to validation or reproduce the results).  On the public dataset M&M the comparisons against nnUNet is not conducted, but included on the private data (why not include on everything).  Several steps in the method are not well explained, which makes it difficult to reproduce.	Missing reference. The idea of using synthesized image for failure detection has been explored in [1], which should be cited and discussed in the related work. Methodology design. For the test time adaptation, why the synthesizer needs to be finetuned in together with segmentor?  I am concerned with the instability of the optimization with such a dynamic image synthesis network at test time. Have the authors tried to optimize the segmentation network only? Methodology design. Any stopping criterion for the test time adaptation? From Fig 4, it seems that the performance can drop after a number of iterations. Unstable performance observed on test cases. The Dice curves over the test set in Fig 4 shows that there are still quite a few cases where RefSeg fails to refine the segmentation (especially on the M&Ms dataset), yielding even lower Dice scores during test time adaptation. Are these failure cases all from the unseen test domain due to the domain shift? A discussion on those failure cases should be added. The definition H_att is not that clear to me. Better to give a mathematical definition on it. It seems that H_att is the network predictions after softmax multiplied by label values. Why not simple feed the multi-channel output to guide the image synthesis? Please clarify. [1] Li, K., Yu, L. and Heng, P.-A. (2022) 'Towards reliable cardiac image segmentation: Assessing image-level and pixel-level segmentation quality via self-reflective references', Medical image analysis, 78, p. 102426. doi:10.1016/j.media.2022.102426.	I have the concern on the statistical significance analysis. I suggest the authors to address it.	Overall the method is well explained, but several components are not clear.  Please see below.	Reproducible if the code is released.	Please see the weaknesses. I have included the suggestions there.	"a) what is the exact edge detection algorithm used to produce the sketch? cite the paper or the implementation of the method, or explain better b) In Figure. 2: what is ""Label values"" block? is it a ground truth label or just and integer index of the class ?  c) How do you create the ""Heatmap""? explain better.  Is the Heatmap a single channel image?  It seem you create it by  multiplying  each probabily channel by the corresponding index  = 0 * p(0) + 1 * p(1) + 2 * p(2).... is that correct? d) what is the reason to combine (add) the sketch and Heatmap into a single channel image?  alternatively you could have concatenated them into 2 channel input to the Synthesizer.  e) what is the ""normalized Heatmap H_att"" on page 5, how is it produced? where is it on the Fig 2?"	"Fig 1, the construction of the input to the image synthesis network is not that clear to me. Is it a multi-channel input or a single-channel input, which combines the heatmap with the edge map? Page 4, ""in default"" - ""by default"" Page 5, 3), please explain what is I? Identity matrix?"	Overall, this is an interesting paper though with some issues to be addressed. I strongly suggest the authors to handle these problems. Due to the good writing, well-motivated problem, interesting and novel method, extensive experiments and results and the demos, I recommend to accept this paper.	The paper is missing details on some of the method components. Nevertheless the architecture choices are sound,  the application is important, and the paper is easy to follow.	The idea is novel, though I have concerns regarding the design of the methodology, e.g., the instability of the optimization at test time, the missing stop criterion to prevent overfitting. As a result, quite a few failure cases can be found in the results.
367-Paper1419	OnlyCaps-Net, a capsule only based neural network for 2D and 3D semantic segmentation	Authors propose OnlyCaps-Net for 2D and 3D multi labels semantic segmentation for improving accuracy and inference speed by replacing the squashing function with  Softsquash and Unitsquash function with introducing unit routing, paramter free single pass routing mechanism. Authors also propose a new convolutional capsule, depthwise convolutional capsule. Authors evaluate proposed method on  public datasets using Dice Similarity score. Contibutions are as follows: -Implementing two novels squashing functions softsquash and unitsquash. -Introduce unit routing, a parameter-free single pass routing mechanism -Comparing with sota public method on public dataset -New convolutional capsule, depthwise convolutional capsule.	"The contribution of this paper is 3-fold: * the authors combine separable depthwise convolution to reduce the complexity of capsule networks, * they provide two new squashing functions, * they introduce a parameter free single pass routing mechanism which does ""normalization""."	The proposed method proposes an optimized capsule network architecture from state of the art for 2D and 3D segmentation. The authors implemented some efficiency strategies, such as Separable Convolutions and approximation of the Squash-Softmax functions for Dynamic Routing, to speed up the training process and limit the model memory footprint and could be of interest for the community.  The method was tested on different datasets for 2D and 3D image segmentation, and the results were validated with 3-fold cross-validation.	-Implementing two novels squashing functions softsquash and unitsquash. -Introduce unit routing, a parameter-free single pass routing mechanism -Comparing with state of art public method on public datasets -New convolutional capsule, depthwise convolutional capsule.	The three contributions seem interesting liste above, in particular the gain in matter of speed and memory consumption (while preserving the accuracy).	The rationale of the method is clear The performance of the method are validated using a cross-fold validation Ablation study The improvements introduced in this work can be adapted to 2D and 3D applications	"-Only using one metric for comparison. Authors could use Hausdorff, accuracy, sensitivity and specificity metrics beside DICE metric because using just one metric such as DICE is not very reliable for showing segmentation results quantitatively. -Authors can update one reference for capsule paper with a newer one because it fits more to medical field. ""Capsules for biomedical image segmentation"" https://www.sciencedirect.com/science/article/abs/pii/S136184152030253X"	I do not see real weaknesses in this paper, except that the given formulas are not easy to read for someone which is not expert in capsule networks and should be completed with parentheses for operators priorities (see formula (1)).	Introduction of optimisation strategies known in the literature to a state-of-the-art architecture. The contribution in terms of performance compared to state of the art is limited. Unclear savings of memory footprint of the current method compared to state of the art. No visual examples for 2D segmentation. Lack of statistical tests to strengthen the discussion of results	Work can be reproducible.	This paper seems reproducible since the formulas are given (even in not always very clear to me).	The authot won't release any code or dataset	Authors could use more metrics for more fair comparison such as Hausdorff, accuracy, sensitivity and specificity.	I do not have any particular remark to do on this paper: references are given, formulas are clear (except (1)) the notion of capsule network is given with its comparison versus standards CNN's. Perhaps that a comparison with separable convolutions (used in some CNN's) should be done since the concepts seem to be similar.	The work does not address any clinical problem directly. However, it is interesting from an implementation point of view. However, it is not sufficiently justified, especially when looking at the results, which are numerically comparable to existing state-of-the-art methods. In the absence of significantly better results than state of the art, the contribution would have been complete if the author had provided an in-depth analysis of the impact in terms of computational cost and memory of this type of model. An analysis of variability through graphs (e.g. boxplots) and more comparison images for 2D and 3D segmentation results would have provided more insights into the discussion of the results obtained. One of the aims of the work is to reduce the memory footprint of a MatwoCapsNet-based capsule network (cost 2z^2) to z, where is the achievement of this aim demonstrated?	Authors propose a new way for optimizing current capsule network on public dataset with novel squashing functions such as softsquash and unitsquash, and compare their work with another public work from literature.	The major factor which decides me is the gain of speed and the reduced memory consumption compared to usual capsule networks.	Although it does not solve any critical problems, it is, in my opinion, a topic of interest to the community to try to optimise the resource cost of complex models to make them more accessible. Unfortunately, these observations have not been satisfactorily discussed by the author.
368-Paper0347	Only-Train-Once MR Fingerprinting for Magnetization Transfer Contrast Quantification	OTOM can be applied to any MRF schedules unlike the previous deep learning based studies dedicated to only a single fixed MRF schedule. It enables transfer learning of the pre-trained OTOM on each dataset of new MRF schedules to further improve the accuracy while significantly reducing data preparation and network training time.	The authors propose a model that can sustain changes in the MRF schedule and yet produce consistent tissue parametric maps.	The authors propose a recurrent neural network-based (RNN) approach that is able to estimate MTC tissue parameters from MR fingerprints acquired with various MRF schedules, avoiding the time-consuming process of generation of training dataset and network training for different MRF schedules. The proposed method shows similar accuracy to the conventional fully connected neural network (FCNN) method.	An Only-Train-Once MR fingerprinting (OTOM) framework that estimates MTC tissue parameters from MR fingerprints regardless of the MRF schedule was proposed, thereby avoiding time-consuming process such as generation of training dataset and network training for different MRF schedules. The deep neural networks were constrained to a single MRF schedule corresponding to the training dataset. If the MRF schedule is changed, the deep neural network has to be trained with new training dataset that was generated with the new MRF schedule. This process is very time consuming and inefficient.So the utility of MRF techniques would benefit greatly from the development of streamlined deep learning frameworks or even only-train-once methods for various MRF sequences. And OTOM can be applied to any MRF schedules unlike the previous deep learning based studies dedicated to only a single fixed MRF schedule.	Application of the bidirectional LSTM seems apt for the magnetization evolution signal The ability to apply it to different MRF schedules is attractive although a clear explanation of how this happens needs to be detailed. Please see next section	The proposed Only-Train-Once MR fingerprinting (OTOM) seems a promising solution of estimating quantitative parameters from different MRF schedules.  Solid investigation of the OTOM method in digital phantom and in vivo data.	It's hard to reproduce, because they did not share their codes.	The study needs to incorporate explainable AI components to demonstrate the tasks that the bi-LSTM is actually learning. The lack of a gold standard MT experiment and only comparing the MRF reconstruction methods is weak given that the MRF MT maps themselves need to be validated for clinical use. This needs to be at least included in discussion. Random sampling of schedule components enables the generalization to different schedules but the use of TL to specific schedule contradicts this feature. So, the implementation can be a container model reference which would be optimal once a TL is employed. It is best to state this explicitly and underscore the benefit of a generalized model as a baseline rather than claim that the model can deal with any schedule. The reviewer recommends toning down this claim which might be confusing.	The state-of-the-art baseline method of dictionary matching is not included.	Negative for the reproducibility of the paper, because they did not share their codes.	The reproducibility metrics have been met	The proposed method is well described though the code is not made available	It is difficult to reappear, but the content is very interesting.	The authors have attempted an important problem in the field of saving training time and computational needs for MRF acquisition schedules. The approach of using a bi-LSTM seems appropriate. However, there are some improvements related to explainable AI and moderating of claims which might make this manuscript more impactful. These are outlined in the strengths and weaknesses sections.	1: The dictionary matching method should be included as one of the baseline methods. 2: The in vivo data is acquired with 4x acceleration. Any reconstruction to reduce the undersampling artifacts before the parametric mapping? 3:  How was the Gaussian noise level determined? Please clarify.	They proposed an Only-Train-Once MR fingerprinting (OTOM) framework to estimate water and MTC parameters from MR fingerprints regardless of the MRF sequence. Unlike the previous deep learning studies, the proposed method was trained with numerous patterns and lengths of MRF sequence, allowing them to plug any MRF sequence rather than a fixed MRF sequence. The flexible OTOM framework could be an efficient tissue quantification tool for various MRF protocols.	Problem is well motivated LSTM approach seems appropriate Claims need to be toned down - random sampling necessarily does not extrapolate to every scheme and especially needs further testing and validation for generalization. The use of TL indicates some of these challenges.	The proposed Only-Train-Once MR fingerprinting (OTOM) seems a promising solution of estimating quantitative parameters from different MRF schedules, eliminating the process of lengthy data preparation and model retraining for a different MRF protocol.
369-Paper1164	Opinions Vary? Diagnosis First!	This paper proposes an idea for improvement of Optic and Cup Discs (OD-CD) segmentation in eye fundus images in a multilabel by various experts scenario. The idea is based on rating expertness of the different experts by analyzing contribution to the diagnosis using a diagnosis network trained for glaucoma diagnosis.	The paper proposed a method to segment the cup/disc of fundus images and also use these masks as the auxiliary input to increase the glaucoma detection accuracy. For this reason, a multi-rater fusion and expertness map are also proposed along with a smoothing method.	The authors propose a novel OD/OC expertness map generation method called DiagFirstGT.  2.The authors developed the ExpG is developed to improve the performance of DiagFirstGT The experimental results show that the method has achieved comparable results with the state-of-the-art methods.	The idea of somewhat value the expertness level of each expert by using a diagnosis network is quite original and, and at the same time, sound. Proposal is well explained and elaborated	High frequency filtering technique learning multi-rater expertness maps comprehensive results	The authors displayed the relationship between the glaucoma diagnosis and the OD/OC segmentation labels. It's a good view to improve the performance of the glaucoma diagnosis by fusing the multi-rater OD/OC segmentation labels.	Results show that the proposal is somewhat a bit convoluted for the improvement that, in some cases, for instance DICE in Cup Disc Segmentation is not present in a consistent way. These differences, some times in favour of previous SOTA, are not properly addressed and diminish the real contribution value of the proposal as is	in general the network architectures never discussed. Authors need to provide a pseudo-code for training and testing phases and explain the details of training steps In equation 2 and 4, the size of ExpG and m are not consistent. (how n is missed) why only dice score is considered? we need more metrics, including false rates I can not find/understand the details of ExpG As far as I know, the masks in refuge-2 was based on 7 raters, but I can not find the 7 masks. it is also not mentioned in the cited reference. Authors may had access to the data in other way. please make sure the claim is correct. what was the architecture of attentive diagnostic network in the figure 1. sharing the codes would be helpful	The experiments are not sufficient. (1) In section 2.3, the statistical results of the various high-frequency elimination methods should be supplemented. (2) Except for MV, other multi-rater fusion methods in Table (a) should be compared in Table 1 (b). The related works should introduce in the introduction. Some writing details of the paper are confusing and need to improve (1) The symbol ~ represents equal in section 2.1, while it means a range in formula (4). (2) h and w denote the size in formula (2), while H and W are used in formula (4). (3) The DiagFirstGT is defined as the result of formula (1) in the second paragraph of section 2.2, while it is redefined as the optimal expertness map in the third paragraph of section 2.2	Reproducibility is quite well covered in terms of method explanation, data and implementation details.	developed codes are not shared. data is publicly available	The code may reproducible based on the paper descriptions.	Authors propose an interesting idea, well explained and that could have many uses not only in the particular application field that they have chosen. Therefore it has some merit in it. Experimentation also shows some good results but fails to allow to definitely conclude that the way to measure and consider multilabel by means of the expertness analysis in diagnosis network. Some results show a decrease in performance that is not correctly discussed.	see section 5	In section 2.1, a private dataset is used to show the diagnosis performance of segmentation masks with different qualities. The details of this private dataset should be given. Why not use the public dataset REFUGE-2 as section 3? Which diagnosis network is used to show the performance of segmentation masks with different qualities in Figure 1? The result of reference [26] based on ExpG in Table 1 is different to Table 2.	The experimentation and results are somewhat lacking and need more work in order to propel this idea into a valid proposal.	clarity of the paper,  only one metric	The topic of this paper is good. But the writing of the paper is needed to improve and more experiments needed to supplement.
370-Paper2723	Opportunistic Incidence Prediction of Multiple Chronic Diseases from Abdominal CT Imaging Using Multi-Task Learning	The authors present a method to predict the incidence of a set of chronic diseases in the five years post abdominal CT acquisition. CT input is sliced in an axial, a coronal and a sagittal plane using landmarks to homogenise them. Outcome data is obtained from the medical records by analysing the ICD codes. A neural network (ResNet-18) is used to predict outcomes, either one at a time or all together. Results are presented for the network having only one slice, having multiple slices or having multiple slices and multi-task learning. Results with respect to the number of training samples are also presented. All measured as AUC of ROC curves. The dataset used for training and evaluation is large (>14,000 CT scans from >9,000 patients).	This paper designed a multi-task low-label learning method for opportunistic incidence prediction of multiple chronic diseases from abdominal CT imaging. A multi-planar 2D CT processing method is designed to extract useful information for five diseases, which reduces the dimensionality of the volumetric 3D data and outperforms 2D single-plane approaches. The proposed method achieve outperformance in 5-year incidence prediction of CKD, DM, HT, IHD and OST.	Development of a multi-task DL model, levarging on reanalysed CT scans for a 5-year multiple chronic disease detection tool.	The strengths of the paper are related to the problem statement. Once we have a CT, we have a clear view of the status of the patient and a lot of information can be extracted. The method presented, planar reformats guided by anatomy, and outcomes obtained from ICD codes are an 'easy' way to obtain both the input and the output of the system. The evaluation with respect to the percentage of training points is interesting and valuable. The more training points, the better performance, as one could imagine.	A multi-planar 2D CT processing method is designed to extract useful information for five diseases, which reduces the dimensionality of the volumetric 3D data and outperforms 2D single-plane approaches. A multi-task low-label learning method is designed for opportunistic incidence prediction of multiple chronic diseases from abdominal CT imaging, and achieves outperformance in 5-year incidence prediction of CKD, DM, HT, IHD and OST.	The concept is an interesting one, trying to utilise all information available on an image for predcitions - mimicing a real world situation - as most DL models focus on a particular singlar disease/prediction. Therefore the idea of having a source and target disease is interesting and novel for this type of application when used together with a multi-planar apparoach. I like that you include a statistical test into your work - always appreciate seeing this in comparative anlaysis. The paper is nicely laid out and easy to follow.	The main weak point is the doubts that the results pose. If we look at table 2, there is almost the same result for ischemic heart disease (IHD) when using the axial slice, that does not contain information about the heart, as when using the coronal or the sagittal, that have at least the lower part of the heart on them. Similarly, the coronal slice does not have information on the vertebrae, but osteoporosis is almost as well predicted as with the sagittal, that has most of the column or the coronal. Multi-slice representation has almost as good performance as the others. These data suggests that the network is focusing on information that is not of relevance for the task at hand.	The segmentation performance of aorta is slightly unsatisfactory. Whether the segmentation results would affect the prediction accuracy? If that's the case, other SOTA segmentation methods can be consider to perform segmentation tasks. The reason of concatenating three slices laterally did not explain clearly, dose concatenating three slices in channel dimension can also work? The training process using sparse labels did not explain clearly, more details should be supplement in the article.	I don't see the clincial aspect/translation - does the source and target combinations/disease affinities make sense from a clinical perspective? (apologies if missed) I feel the dice score for the segmentation is quite low? Any comments about this?	This paper will only be reproducible if the database is made publicly available. Otherwise, it is straightforward to implement.	The code of this work was not provided and the detail of proposed network have not been explained clearly. The reproducibility is slightly worse.	I think the details wrt the methodology is there but I think the actual data utilised after slice selections is an important aspect to share as often the methods are clear but when one tries to replicate the data ithe outcomes may create vastly different results.	Please assure that scans from the same cases are not used for training and testing.	This paper designed a multi-task low-label learning method for opportunistic incidence prediction of multiple chronic diseases from abdominal CT imaging. A multi-planar 2D CT processing method is designed to extract useful information for five diseases, which reduces the dimensionality of the volumetric 3D data and outperforms 2D single-plane approaches. The proposed method achieve outperformance in 5-year incidence prediction of CKD, DM, HT, IHD and OST.	I enjoyed reviewing your paper, well done on your work. I have some points to kindly raise: How do you discard the predictions of the secondary disease? Not clear to me. The optimal parameters chosen - how did you conclude this, trial and error? or the strongest associations/lowest losses drove these choices?	The problem is interesting and the training and validation are well performed, even though doubts about the results arise (see above).	This study provided a meaningful approach for opportunistic incidence prediction, and the multi-planar 2D CT processing method can provide reference for other researches. If the details can be added after major revision, this is a meaningful work. Thus, I suggest receiving it after major revision.	I feel the concept and work related to multi-task learning and multiple disease prediction is the way forward for DL models in healthcare, and I found the paper enjoyable to read. There are some aspects that might require some working - perhaps to answer some of the questions I have and any other comments from other reviewers.
371-Paper0303	Optimal MRI Undersampling Patterns for Pathology Localization	A new iterative gradient sampling algorithm was employed for finding optimal undersampling patterns in different medical tasks and applications.	In the manuscript, the authors investigated MRI acceleration strategies for the benefit of downstream image analysis tasks. They proposed to optimize the k-space undersampling patterns and studied the effect of the proposed paradigm on the segmentation task using two classical labeled medical datasets, and fastMRI+ annotations. They demonstrated an improvement of the target metrics when the sampling pattern is optimized.	The authors propose a novel way to find the optimal MRI under sampling pattern based on the downstream image processing tasks such as segmentation and pathology detection. They provide the convincing validation on large public dataset such as BRATS and fastMRI.	The idea is interesting, and the paper is well written and content rich.	Overall, the manuscript is nicely written. The Methods, Experiments, and Discussion sections are nicely presented. The Introduction also nicely summarises the motivation and related research works. The proposed methodology is interesting, and as the authors have demonstrated, retains the segmentation quality in ACDC dataset even at moderate to high acceleration factors.	The main strength of the paper is the development of MRI under sampling for pathology localization. Unlike other MRI under sampling based SSIM or PSNR on whole image domain, the proposed method can be more effective on specific tasks such as organ segmentation or pathology bounding box detection, which are clinically  meaningful. The paper is well validated on existing public data and showed the improved performance for downstream tasks.	Have you ever tried to use the mixture of the two or three datasets for the training?  It would be interesting to see the results.	I would suggest the authors to provide additional quantitative analysis for the fastMRI dataset. My additional comments are presented below (Q#8).	The main weakness is the lack of explanation about optimization like IGS and LOUPE. The gradient computation on these optimizations are not clear. The convergence analysis should be included to check the stability of the method.	Please provide the code.	The authors have presented their developed tool as supplementary material, making their approach and analysis reproducible.	The optimization implementation details are needed for reproducibility. The hyper parameters are given in the supplementary files.	I find the the network training details in the Supplementary material. It it better to include some important part in the paper. Even though the results look promising, how to validate the  learned undersampling patterns working well?  Will it miss some important information?	There are few minor grammatical issues; I would suggest the authors to thoroughly check and revise them. There are too many footnotes in the manuscript, which could have been easily written inside the main body text. In Table 2, it is not clear which null hypotheses the p-values are referring to. In Eq (2), Y_hat is not defined. After Eq (3), c_{1,2} should be c_1 and c_2. Page 4: Covariance is calculated between two variables; hence, it is not clear what the authors referred to as 'covariance of the pixel intensities'.	The authors propose a new paradigm for accelerating MRI which takes account into the downstream pathology localization tasks. They need to provide the implementation details about IGS and LOUPE optimization. They can compare more advanced MRI under sampling methods like pg_mri (with greedy policy search) instead of Center or FastMRI.	This paper tries to get the optimal undersampling patterns in k-space that maximize a target value function of interest in the segmentation and localization problems, and then employing a new iterative gradient  method. The effort is novel. The fact that the authors have run their approach on different MR data is quite helpful. Main comments: Even though the results look promising, how to validate the  learned undersampling patterns working well?  Will it miss some important information?	Overall, the manuscript is well presented, and the proposed method is well developed and analysed.	The major factor for my score is that they aim to find the optimal MRI under sampling pattern for downstream pathology localization instead of whole image quality. The paper shows improved performance on validation on large public data. The hyperparameters are given for reproducibility.
372-Paper0371	Optimal Transport based Ordinal Pattern Tree Kernel for Brain Disease Diagnosis	The authors propose an approach to tackle the problem of brain network connection pattern estimation. they use a new graph kernel called optimal transport based ordinal pattern tree kernel. Experimental evaluation shows significant improvement.	The paper presented a novel method called Optimal Transport (OT for short) based Ordinal pattern tree (OPT for short), for achieving better performance on classification and regression tasks in brain disease classification. This method is achieved by extracting OPT from brain network generated by fMRI, and measuring optimal transport distance in different OPT levels. Experiments from 4 datasets are reported and compared with other graph kernel classification methods.	In this submission, the authors introduce a new data structure to transform a graph to a tree structure. An advantage of doing so is that the proposed tree structure contains hierarchical relationships. An optimal transport distance between two trees is computed and used to build a kernel. Based on this kernel, a series of learning tasks can be applied.	The paper details a method that on the base on my (limited) knowledge of the state of the art is novel. The approach is explained in detail and the paper could be reproduced by other research groups engaging in similar topics. Table 1 shows a comparison with other state of the art approaches. The proposed method yields better results.	The adoption of OPT for brain network analysis is novel. The OT kernel is also novel for the OPT comparison.	This paper introduces a novel idea of utilizing the brain network to analyze brain disease patterns. The proposed ordinal pattern tree is a good idea of establishing hierarchical relationship of nodes in a brain network. The whole paper is written in a smooth way and it is easy to follow.	The notation could probably be simplified to make the topic more approachable.	It is unclear why a tree structure is a good way for brain network analysis. It may reflect the hierarchical structure but it also loses important brain connections and its connection strength information. In other words, the proposed algorithm just analyzes partial information. Brain network is different from general graph analysis since brain network node structures are limited and fixed. The resulting tree structures are not necessarily consistent from person to person. It may need some further study to analyze whether the OPT or the OT makes significant contributions to the improved performance. For example, the OT can also be applied to other structures like Tree++. Such experiments will help justify their discoveries. The description of OT algorithm is not clear. It hurts its reproducibility.	(1) Motivations of using OT distance are expected to be discussed. Tree edit distance is commonly used to measure the similarity of two tree-structured data and it is easier to compute. Some comparisons are preferred. (2) Ablation studies are desired, such as directly applying OT to the brain network and build the kernel, different OPT starting from different regions, etc (3) More statistical metrics are expected. Considering the possibility of imbalanced data, some other metrics are expected in classification experiments	The paper is well explained and algorithms are detailed in a way that favours reproducibility	The reproducibility is okay. However, a more detailed description of OT will help further improve it.	No code is provided.	Due to my limited expertise in this field I cannot comment further.	The authors may add a more detailed description of how OT is implemented to compute the OT distance between sub-trees. The authors may add more controlled experiments on a different combination of methods. The authors may further justify the relevance between the OPT and brain network analysis.	Besides the question in Section 5, here are some other questions: (1) How to guarantee that two graphs have the same number of levels in the form of your tree? (2) If the same region has multiple nodes, how to choose which one is the root in the tree structure? (3) Evaluation of classification results is expected to have precision, recall and F1 score.	the paper yields very good results, it seems technically sound and it improves the state of the art.	The paper organization is clean. The work has certain novelties.	My current judgement is based on the methodology in this submission. I look forward to seeing the rebuttal from the authors.
373-Paper0654	ORF-Net: Deep Omni-supervised Rib Fracture Detection from Chest CT Scans	"This paper proposed to adopt an Omni-supervised learning strategy to better leverage heterogeneous annotations when training Rib Fracture detectors. In addition to the Omni-supervised strategy, the authors further incorporate the ""aggregated"" confidence from different heads into the final loss calculation to strengthen the ""focal"" idea introduced by focal loss. Extensive experimental results confirm the effectiveness of each technical component."	The author proposed a label assigning strategy for training a rib fraction detection framework using data with three different levels of annotations, i.e., boxes, centers of objects, and none. Data with different types of annotations will be used to train their dedicated classification branch. All annotations are utilized as pixel-level supervision (in or out of the object box) during the training. Inter-guided maps (IGM) for each branch (using predictions from the other two branches) are computed and used as the GT(after thresholding) whenever annotations are not available. A private dataset of CT images is employed for the experiments and evaluation. Superior results of the proposed method are reported in comparison to previous Omni-supervised methods and other label assigning strategies, i.e., self-guided maps (SGM).	This paper presents an omni-supervised learning framework to train a CT-based rib fracture detection model. The proposed framework features a shared feature pyramid network backbone and an omni-supervised detection head, which supports supervision from box-annotated, dot-annotated data, and unlabeled data. A dynamic label assignment strategy is introduced to combine multiple branch outputs and guide the model training. Experiments are conducted on a new rib fracture dataset of 2239 images. Results demonstrated the proposed method's efficacy.	"Overall the paper is well organized making it easy to follow. Below are detailed strengths from my view: 1, Using Omni-supervised learning to handle the heterogeneous annotations. This helps to simultaneously leverage the different forms of supervision for model training is novel. 2,The (1-W_i) multiplier introduced in the classification loss further strengthens the ""dynamic focal"" idea. This effectiveness is confirmed by the ablation study 3,Extensive experiments. In table 1, the paper fairly compares other state-of-the-art semi-supervised strategies for the task and proves the effectiveness of its proposed ORF-net. Table 2 further proves that the soft confidence multiplier can help to improve the model."	The manuscript is overall well-written and easy to follow The proposed method tries to utilize all levels of annotations and actually transform them all into pixel-level supervision. The idea is valid and sound Ablation study on the components in the proposed method is conducted and discussed.	This paper introduced an omni-supervised learning framework, which can leverage box annotation, dot annotation, and unlabeled data. This paper introduced a dynamic label assignment strategy to combine the output from multiple branches for model training. Experiments on the new dataset demonstrated the efficacy of the proposed method and the dynamic label assignment strategy.	"Though the paper is well illustrated in general, I do have the following questions: 1, Fig 1, is a bit confusing. From my understanding, the model should have a single backbone, multi-heads structure. Namely a single image is passed through the network and different branches will work together to provide supervision based on annotations for the image. However, from Fig 1, it seems like an image triplet is sent to the network. Could you confirm which one is correct? 2, What will the score be like during inference? Since we have three branches, how do we aggregate the confidence? 3, Does the ""CA"" version of experiments in Table 2 also apply to the box regression? From the paper, it seems to claim only the classification loss adopts the confidence multiplier. If so, any explanation why? 4,Given the 3D shape of CT scans, how are the 2D slices selected to train and test the model? Only use the key slice? Moreover, since 310 box annotated images are used for testing, does it mean all testing cases have fractures?"	I have several concerns and unclarity of some parts: It is not clear how many patients are involved in the private dataset. And, is the data split patient-wise? Have the authors considered or experimented with how the amount of data with different types of annotation varies? SGM and IGM are not that different. And have the authors considered other maps, e.g., a uniformed W with P_bP_dP_u?	"Some details of the proposed method need further clarification. What is the rationale of combining output from two other branches to guide the training of one branch? What does ""three different annotation types of data are equally sampled"" mean? Are the regression results combined using non-maximum suppression? Is the i in W_i, p_i, b_i the index of different pixels in a feature map? The threshold t is set to 0.5 for the dynamic label assignment and confidence-aware classification loss. How does the threshold value affect the model performance?"	Positive if code and the private dataset is released.	Enough information to reproduce.	With more details about the proposed method, the paper should be fairly easy to reproduce.	Please address the questions I listed in the weakness section.  In addition, I would suggest to reformat the equation 1, following the best practice of   matrix , vector and scale format.	See above	"Figure 1 should be improved about how the label assignment's input and output. Current figure is a little confusing. Section 3.2, ""Note that, We enable ..."" -> ""Note that, we enable ..."" ""As shown in Table 1, By simply ..."" -> ""As shown in Table 1, by simply ..."""	Even though I have many question regarding the training and testing process, the idea of the paper is relatively novel for the task.	Pretty clear presentation of an add-on training strategy (though the innovation is limited) and grounded comparison and justification of the effectiveness of the proposed method. I only have some suggestions for further investigation, which is better to have.	This paper introduced a new omni-supervised learning framework which is useful for training models with different types of supervisions. This can be extended to many different problems and applications. The proposed method also achieves state-of-the-art performance on a new rib fracture dataset.
374-Paper1581	Orientation-guided Graph Convolutional Network for Bone Surface Segmentation	This study proposes a method to segment bone structures from ultrasound imaging. In contrast to widely used pixel-wise prediction, the proposed method relies on an orientation-guided graph neural network to perform the segmentation of bone surface. The proposed method was compared against a few other neural net based methods such as UNet and MFG-CNN. The proposed study uses 1042 images and 20% of the images were designated for testing the algorithms.	A method for bone surface segmentation which addresses segmentation discontinuity is proposed, based on orientation-guided graph convolution network.	This paper describes a graph convolutional network approach to segmenting bone surfaces in ultrasound images, using orientation information to improve the performance and quantifying the connectivity of the resulting segmentations to ensure realistic results are produced. The paper compares results to three other existing approaches to this problem, showing improvements in connectivity and traditional Dice score.	The author(s) proposed an orientation-guided graph convolutional network for bone surface segmentation. The proposed method outperforms UNet and a couple of other machine learning methods on the test set. Overall, the paper is written and organized very well.	The dataset is fairly large. The method has been compared with the common UNet approach and 2 other methods used in the literature on bone surface segmentation. Evaluation is fairly thorough with suitable metrics.	-Experimental Rigour: The experiments presented in this paper are thorough, assessing the proposed approach with both the connectivity metric and traditional Dice score, including comparison to existing approaches and an ablation study, as well as providing many details about the implementation and hyperparameters used. -Novelty: Although this problem has been investigated in other works, this paper provides a new approach with novel features, including the incorporation of orientation information.	The paper does not report inter or intra rate variability. Bone segmentation from ultrasound imaging is quite challenging and often results in a low inter-rater agreement. The impact of the uncertainty of the ground truth segmentation on the final quantitative evaluation is not clear. The study does not seem to utilize any data augmentation techniques. Other studies have shown that the performance of UNet and other machine learning algorithms could be improved through data augmentation.	The authors can strengthen the justification by describing the consequences of surface discontinuity. Also, some terminology needs to be better explained to the unfamiliar reader earlier in the text, such as orientation and connectivity.	Clinical Motivation: The unmet clinical need addressed by this work is not well-described or clear to the reader. Discussion: The Discussion section does not provide much insight to the reader, beyond what has already been described elsewhere in the paper. The limitations of the proposed approach are not addressed.	Although the authors indicated that the data and code would be available publicly, no information is provided in the paper. In other words, the paper does not indicate when and how the code or data will be made available.	The authors provide details of the method used, however it is not clear if the data itself is sharable.	The paper has a reasonable level of repeatability, providing details about the hyperparameters and systems used. The authors also indicate that code will be made available. The dataset appears to be acquired internally but the authors indicate in the reproducibility checklist that the dataset could be accessed by others.	It might be worth including the inter or intra rater variability for the test set as it will allow the readers to better understand the performance of the proposed neural network. It might be worth assessing the impact of data augmentation on the overall performance of the proposed neural networks.	"It would help the reader if the authors explain what they mean by 'bone connectivity' or 'bone shadow' where it is first mentioned. Perhaps a figure would help. Also, it would help the reader to understand the effect of disjoint segmentation if authors describe how that would affect CAOS procedures. What level of discontinuity would affect the results? Looking at fig 3, row 2 method MFG-CNN the slight discontinuity does not appear harmful if guiding the surgeon is  the goal. Perhaps explanation of the severity of the consequences can help the reader better understand the significance of the solution.  Pg 2 ""next we propose utilizing orientation as an ..."". Can you please clarify the orientation of what with respect to what? Also in pg 3. ""By tracing them in a specific orientation"". It is not clear what orientation means here.  2.1. Please clarify what portion of this is the authors' novelty and which is existing work. Pg 3. ""Existing bone segmentation networks only utilize..."" Please provide reference. Please read the text to correct some typos.  The dataset consists of images from 'healthy' volunteers (I assume this means healthy from an orthopedic point of view). Additionally, in the introduction, application for orthopedic surgery is mentioned. I'm curious to know whether and how images from patients requiring orthopedic surgery would differ from the dataset used. If it does differ (e.g. non smooth bone surface?) how the performance of the algorithm would be.  In what situations does this method not work well?"	"Overall, this paper was well done with detailed methodology and experiments described clearly. Major revisions: 1) The clinical motivation for the task is not well-explained in the Introduction. Only the first two sentences introduce the clinical problem but the unmet clinical need is not clear and should be described more fully to allow the reader to understand the clinical relevance. 2) Limitations of the proposed approach should be acknowledged and described in the Discussion. Minor Revisions: Abstract: -Second last sentence: ""in"" is missing before ""vivo"" -Second last sentence: Define all acronyms on first use (US has not been defined) Introduction: -Paragraph 2, Sentence 2: The sentence beginning with ""Note that the difference..."" is very unclear. Please rephrase and clarify to strengthen the rationale. -Paragraph 3, Sentence 3: Replace ""good"" with ""better"". -Last sentence: ""in"" is missing before ""vivo"", as in the Abstract. Discussion: -Ablation Study should be with Experiments and Results rather than in the Discussion section."	Overall, the study is designed well. The study clearly indicates the benefits of the proposed method in comparison to other machine learning methods The paper is well organized	Some sections need more clarification to better understand the work and its significance.	The work presented in this paper was performed rigorously and is described with a thorough level of detail. This work would be of interest to the MICCAI community, appealing to both the MIC and CAI backgrounds. The paper could be strengthened by more complete descriptions of the clinical motivation and limitations of the proposed approach.
375-Paper2185	Orientation-Shared Convolution Representation for CT Metal Artifact Learning	The authors propose a method for a deep-learning-based metal artifact reduction algorithm using the orientation-shared convolution representation.	Improve the DICDNet by integrating rotationally symmetrical streaking (RSS) property and filter parameterization. Contribution is incremental.	This work introduces a new image-based neural network architecture for metal artifact reduction in X-ray CT. The architecture is built around an additive artifact reduction model which aims to predict an artifact layer which is to be subtracted from the input image. The artifact layer is estimated by using a formulation of steerable convolutional filters to represent rotationally symmetric streaking artifacts. Those filters are learned together with networks interpreted in this work as proximal operators which together predict the decomposition in artifact layer and artifact-free image. The method is evaluated on simulated data against multiple state-of-the-art methods.	The methodology has a solid theoretical background and shows promising results in both synthetic and real image data. The unnecessity of the sinogram is preferable from a practical point of view, where sinograms are not available. The proposed image-domain algorithm can also reduce the secondary artifacts due to the error in the geometric calibration (i.e., slight inconsistency between the physical imaging geometry and the geometry calculated by the calibration.)	Novel formulation for the existing DICDNet by integrating the rotationally symmetrical streaking property and filter parameterization. The method is also demonstrated on clinical data.	Evaluation against multiple competitive modern methods. Novel and innovative, independent ideas.	The experimental materials involve limited anatomy. I am very interested to see various other anatomies, including total hip arthroplasty.	Compared with NMAR and DuDoNet, large streaks between metals are not reduced. The backbone of the method is based on the existing DICDNet method.	The prior knowledge exploited does not hold true for all image artifacts caused by metal. E.g. in Fig. 5 the black area inside the mouth cavity can clearly be seen still contain the black band between the metal implants in the presented method, while other methods can remove it. In general the visual impression of the results does not match very well with the quantitative evaluation. CNNMAR leaves a very convincing visual impression compared to the proposed method. Only simulated data for evaluation. The method section is extremely hard to follow for me. No ablation studies. This work introduces steerable convolutional filters and a network architecture inspired by an unrolled optimization scheme. Without an ablation study it is unclear what parts of this method are essential.	The authors will release the code and the trained model after acceptance, which is very helpful for the community.	Good	The paper will not be reproducible without code. The description lacks to many details. However, the authors promise to release their code which would render this point moot. If the code is published the method should be well reproducible.	This is excellent work. It would be even better if the authors also discussed the limitation of the proposed method more in detail.	This paper addresses the metal artifact reduction problem in CT reconstruction. The main method is based on a recent method called DICDNet. In this work, based on the prior fact that metal streak artifacts are rotational because of the back-projection algorithm along different angles, the authors improve the existing DICDNet by integrating an orientation-shared convolution representation into their network design, which can reduce rotationally symmetric metal artifacts better. The overall structure of the manuscript is clear and the method has been compared with state-of-the-art methods. Not only qualitative results are displayed, but also quantitative evaluations are performed. The clinical feasibility is also demonstrated by the experiments on clinical data. These are all the good points from the paper. However, in Fig. 5, we can see that compared with NMAR and DuDoNet, large streaks between metals are not reduced, which is a major disadvantage of the method. The authors should find a way to overcome this limitation. Since the backbone method, i.e., the DICDNet, has already been published. This work is an improved/modified version. Hence, the contribution is incremental.	"""Furthermore, it is difficult to collect the sinogram data in realistic applications [9]."" This sentence is factually incorrect. In order to reconstruct a CT image this raw data is necessarily acquired. I believe this sentence is supposed to express the practical difficulty of acquiring this data if one does not collaborate with CT manufacturers or builds an own CT. Please reformulate this sentence if this is the intention. This is especially important because classically approaches in projection-domain where favored since they allow dealing with the rotational streaking artifacts easier, before they occur. In addition for deep-learning approaches different prior works have shown that correcting metal artifacts in projection domain is advantageous e.g. in https://ieeexplore.ieee.org/document/8331163. I don't understand equation 5. Why are all the terms multiplied element-wise by ""I""? Doesn't that make the term superfluous? From equation 9 I get the impression that ""I"" serves to direct the loss at image areas which are metal artifact affected. If that is the intention ""I"" could just be introduced in equation 9 and described like this. I also don't understand equation 9. Isn't the objective minimized if X + C * M = Y ? That would just mean that any additive artifact model automatically minimizes this model. So simply setting up a global residual connection in a network where Y - A = X, where A is predicted by a network (A = f(Y)) would automatically minimize this. What am I missing here? I am generally unconvinced about the necessity to use the presented prior knowledge. The rotational artifact structure stems from the fact that it is caused by inconsistency in projection domain, between different projections which causes such artifacts on circular trajectories. Therefore, projection-domain (sinogram-based) methods implicitly also use this prior knowledge. Even better, if more complicated trajectories are used, which can cause different artifact patterns, the methods in projection-domain would still exploit this appropriately. The evaluation should be improved by providing results on measured data. The method description should be simplified, e.g. by providing a sketch of how it works. In addition an ablation study should be done to show that all aspects of the method are relevant."	Nice work.	The novel formulation using rotational symmetry property.	The paper is extremely hard to follow and the proposed prior knowledge is unconvincing. The mathematical justification of the network architecture seems superfluous. Once those motivations are stripped, the paper essentially proposes a novel network architecture. The evaluation is only done on simulated data and the results are visually inferior to other methods evaluated. Therefore in summary I am unconvinced this method improves over the state of the art.
376-Paper1923	Out-of-Distribution Detection for Long-tailed and Fine-grained Skin Lesion Images	The authors present a mixup augmentation and prototypical learning based OOD detection method for skin lesion images. They first group the classes into 3 main groups (Head, middle and tail) according to their occurrences in the dataset. They show that mixing up samples from middle and tail classes helps in learning better representations for the purpose of OOD detection. They integrate their mixup strategy into prototypical learning to enhance the capability of their model.	The paper proposes an out of distribution (OOD) detection method with application on long-tailed skin lesion datasets. The method consists of two parts: i) an augmentation strategy (mixup) targeted to middle and tail classes in the dataset (classes are categorized according to number of samples in the dataset); and ii) prototype learning integrated with the augmentation strategy. The authors experiment with mixup augmentation targeted to different parts in the dataset and concluded that targeting the middle and tail classes yielded the best performance. They've done experiments on ISIC 2019 and an in-house dataset to evaluate the ability of the proposed method to detect out-of-distribution samples.	The paper describes a method combining the Mixup method with prototype learning aiming to improve out-of-distribution detection in the context of dermatology imaging. The authors propose several mixup strategies and report a comparison to a baseline as well as several state-of-the-art methods demonstrating the superiority of the proposed method in the context of unknown data of the same nature (i.e., unknown classes). The paper uses both, private and public (ISIC2019) datasets.	As far as the reviewer knows, mixup between different occurrence groups is a novel approach (at least for skin lesion images).  Authors extensively tested on various combinations of mixup.	The paper is generally well-written and ideas are well-presented. The authors does a good job in citing the previous work, exploring the different OOD methods, and briefly explaining the idea of the relevant papers. The authors identify a limitation with the current OOD methods as these methods are not suitable for practical clinical applications where the differences between in-distribution (ID) and OOD samples are visually subtle. They design their study to be able to tackle this limitation and also evaluate on a real-world clinical dataset collected by the authors. The idea of using targeted mixup augmentation and prototype learning in OOD is novel and seems to give better performance than baselines. The method is evaluated against several OOD methods and on two different datasets. The proposed method has superior performance in OOD in both ISIC and the in-house dataset, this shows the robustness of the proposed method in real-world clinical datasets.	Overall the paper is clear and quite comprehensive. The addressed topic is clinically and methodologically relevant and the proposed method has a potential to be applied to other types of the imaging. There is a reasonable amount of ablation and comparison reported to allow a fair understanding of the method's interest .	It is not clear if mixing up the lesion images in the input pixel domain makes sense. The mixed up images does not look realistic and it is not clear why the model should learn a better representation from them. For example, authors mentioned that they also included some unusual samples, such as blured images as the OOD class. Depending on the mixup weights, a mixup lesion can easily look like a blurred image, so it is not clear if training in this way helps during such situations. It is not clear how susceptible the model is to the selection of the 3 groups (head middle tail). How did the authors pick the cutoffs between the groups? Did the authors try to pick different cutoffs to group classes in a slightly different manner and conduct experimetns? Authors did not touch any of these issues. It is not clear how did the authors train the prototypical network.	The prototype learning part needs more explaination. Confidence score section is interesting but needs more explaination. How is it computed? No statistical significane of the results is reported	Some parts of the paper may be worth revision for more clear and straightforward presentation. That is, the variety of the mixup strategy might be better presented to facilitate the comprehension. Moreover, the results are worth further discussion as the reported Mixup results appear to outperform the proposed method.	The used framework is not mentioned.  The imaging modality for the inhouse dataset is not mentioned.  Sensitivity regarding hyperparameters is missing	"Regarding #1 in weaknesses, please ellaborate more on the prototype learning part. What is the standard prototype loss? How do you know/compute the class specific prototypes {p_i,p_j}? Why does the authors think it will help detecting the fine-grained samples in the dataset? Regarding #2 in weaknesses, please provide more details on the confidence score computing. Is that the predicted softmax probabilities? It would be interesting and more convincing to readers to see statistcal significance of the reported results. I guess the reported results on the in-house dataset is on the test set, but on ISIC2019 the authors report results on the validation set (cross-validation). That needs to be mentioned in the text, e.g. in section 3.1. ""It can further be noted that our proposed approach of M-T mixup (MX5) strategy combined with prototype learning performs the best for OOD detection of all different categories while maintaining or slightly improving the overall ID performance compared to the baseline on both the datasets."" I disagree, the proposed strategy gives the best for OOD detection, but it does not improve the overall ID performance compared to other methods, as seen from table 2. For example, ARPL is consistently better than the proposed method on the in-house dataset. This is okay as the method is primarily an OOD method, but the sentence above needs to be adjusted. Table 2: please either make all best scores in bold or unbold all numbers. For example, in the pre and rec columns for the in-house dataset, there are no bold values, unlike the rest of the table. What are the values of the different lambdas used in your experiments, e.g. in eq. 2, eq. 3, lambda_mse, and lambdas in fig. 2? You might mention only the values used for the best performing method (MX5) in the main text and move the rest to an appendix. You might also want to use a better notation for clarity, rather than refering to all of them as lambda. Minor:  8.1: ""... verified dermoscopic images categorized amongst 65 different ..."": might be better to say categorized into  8.2: ""... This simple technique has shown to increase ..."": has been shown  8.3: ""... thus limiting it's advantages ..."": its  8.4: ""... which are the standard parameters for measuring ..."": these are metrics not parameters"	The authors give some details on the used private dataset and also describe how they used the ISIC dataset. The authors provide reasonable amount of information on the training and implementation, as well as on the metrics being used.	"An ablation study on the susceptibility of the model against the selection of grouping is missing. It is not clear how did the authors train the prototypical networks. How did they decide on the feature level to be picked for extracting embeddings? Equation 3 is not clear. Why do the authors put d_{xpi} through the network again? Assuming that ""f"" represents the model, why and how would the output of f(d_{xpi}) be y_i? What type of images is the in-house dataset composed of. Dermoscopy images, clinical images? The authors did not compare their model against the algorithms from the literature that were designed explicitly for OOD detection for skin images. Some examples are; -M. Combalia, F. Hueto, S. Puig, J. Malvehy, and Veronica Vilaplana. Uncertainty estimation in deep neural networks for dermoscopic image classification. 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops (CVPRW), pages 3211-3220, 2020. -Abhijit Guha Roy, Jie Ren, Shekoofeh Azizi, Aaron Loh, Vivek Natarajan, Basil Mustafa, Nick Pawlowski, Jan Freyberg, Yuan Liu, Zach Beaver, et al. Does your dermatology classifier know what it doesn't know? detecting the long-tail of unseen conditions. arXiv preprint arXiv:2104.03829, 202 -On Out-of-Distribution Detection Algorithms with Deep Neural Skin Cancer Classifiers. Andre GC Pacheco (Federal University of Espirito Santo, Brazil)*; Chandramouli Sastry (Dalhousie University, Canada); Thomas Trappenberg (Dalhousie University, Canada); Sageev Oore (Dalhousie University, Canada); Renato Krohling (Federal University of Espirito Santo, Brazil)  -Torop, M., ""Unsupervised Approaches for Out-Of-Distribution Dermoscopic Lesion Detection"", arXiv e-prints, 2021. -Subhranil Bagchi, Anurag Banerjee, and Deepti R Bathula. Learning a meta-ensemble technique for skin lesion classification and novel class detection. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition Workshops, pages 746-747, 2020."	The authors provide most of the implementation details except for the values of hyperparameters (lambdas). Providing these additional hyperparameters in the revision would facilitate the reproducibility.	" There are several modification that could allow the improvement of the paper.  Method:  - Could the authors detail how the lambda is selected as it does not clearly stand out from the paper?  Experiments:  - could the authors detail why 20 classes were chosen as OOD? Is there any ablation having been done?  - Could the authors revise ISIC class labels as it might be not clear for unaware reader?  - Could the authors specify how the precision, recall and F1 are calculated for multi-class classification in imbalanced dataset?  - Could the authors motivate the choice of the ResNet34?  Results:   - Is it possible to report the classification metrics (prec, rec, f1) for the tail classes?  Mixup Ablation  - could the authors clarify/discuss more their choice of M-T strategy for further experiments?  Conclusion  - The authors state ""OOD techniques are still far away from clinical deployment"". Could the authors provide further details of such conclusion?  Syntax and writing:  Introduction:  - in ""There are two shortcomings in this aspect - 1) ... 2)"" could the authors revise the syntax of the items 1) and 2) to facilitate the reading  - The main contribution appear to be somewhat drawn in the text. Could the authors consider outlining it differently?  "	The paper is overall welll written and clear except at a few places.  Good results. The detail of the prototypical network training is missing.  Missing references and comparisons against the literature.	The paper is well-written and organized. The presented idea is interesting, has been adequately evaluated on two different datasets, and performs better compared to other methods. The idea is motivated by the limitation of other methods when applied on real-life clinical datasets. I think the paper will be interesting to MICCAI audience after doing the requested revisions to some parts.	The paper appears to be quite clear and the reading flows well. The OOD issue, studied on the dermatology imaging, is relevant to other imaging areas as well so may be interesting for the community.
377-Paper0142	Overlooked Trustworthiness of Saliency Maps	Paper attempts to quantify the trustworthiness using the resilience and resistance of saliency maps. Inspired by adversarial attack methods, they propose a formulation to study how saliency maps change if the adversarial attack changes the classes (relevance) and if the adversarial attack changes the saliency map but maintains the same class (resistance). A comparison with six different saliency maps empirically highlights the drawbacks of current methods.	This work focus on evaluating the robustness and trustworthiness of common interpretability saliency maps. The authors propose two fundamental properties to evaluate the trustworthiness of interpretability saliency maps: relevance (if a model's prediction change due to alterations in the input, the saliency map should also change) and resistance (if the model's prediction does not change with alterations in the input, the saliency map should also remain the same). In the experiments, the authors show in a Chest X-ray application, that several common interpretability saliency maps demonstrate both a lack of relevance and a lack of resistance.	This work investigates the problem of trustworthiness of the saliency maps in the medical imaging research domain via proposing quantitative criteria (relevance and resistance). Experimental studies demonstrate the effectiveness of the criteria in revealing the problems of the overlooked trustworthiness of saliency maps.	Important class of problems where in saliency maps for trustworthiness is quantified and explored. First attempt at quantifying the lack of trustworthiness by saliency maps. Good experiments and coverage of the saliency maps techniques used to demonstrate their case.	This work helps to draw the community's attention to the (lack of) trustworthiness of most interpretability saliency map methods. It presents a novel analysis of the interpretability saliency map methods' quality by introducing two desired properties: relevance and resistance. Conclusions are well-supported by the experiments.	The authors propose the relevance and resistance criteria to evaluate the trustworthiness of saliency maps. The authors experimentally demonstrate that some popular saliency map-based methods either lack relevance or resistance qualitatively and quantitatively.	Some of the details are missing. A few questions on the basic premise.	"Although there is novelty in the work presented, the authors fail to recognize previous works that analyse the reliability of saliency maps (e.g., Adebayo et al. [https://proceedings.neurips.cc/paper/2018/file/294a8ed24b1ad22ec2e7efea049b8737-Paper.pdf]). Even though the authors present seven different saliency map methods, some of them are very similar (e.g., VG and VG * Image) and they do not present the results with some other different and well-know methods (e.g. LRP, DeepTaylor). It would be interesting to check if the lack of relevance and resistance still happens in these ""relevance""-based methods."	"The novelty of this work is limited. The fundamental equations (2) and (4) are similar to the equation (4) in the paper [1]. There are many existing works studied on the trustwoorthy of interpretations, e.g., [2-6] and many others. Although this work is solid with experimental evaluations, there is no mathematical analysis for the proposed criteria and no discussion of how to improve the trustworthiness of saliency maps. [1] Improving Deep Learning Interpretability by Saliency Guided Training. Advances in Neural Information Processing Systems, 34.  [2] Ghorbani, Amirata, Abubakar Abid, and James Zou. ""Interpretation of neural networks is fragile."" In Proceedings of the AAAI conference on artificial intelligence, vol. 33, no. 01, pp. 3681-3688. 2019; [3 Dombrowski, Ann-Kathrin, Christopher J. Anders, Klaus-Robert Muller, and Pan Kessel. ""Towards robust explanations for deep neural networks."" Pattern Recognition 121 (2022): 108194;  [4] ""Proper network interpretability helps adversarial robustness in classification."" International Conference on Machine Learning. PMLR, 2020; and many others.  [5] Kindermans, Pieter-Jan, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Schutt, Sven Dahne, Dumitru Erhan, and Been Kim. ""The (un) reliability of saliency methods."" In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 267-280. Springer, Cham, 2019. [6] Kindermans, Pieter-Jan, Sara Hooker, Julius Adebayo, Maximilian Alber, Kristof T. Schutt, Sven Dahne, Dumitru Erhan, and Been Kim. ""The (un) reliability of saliency methods."" In Explainable AI: Interpreting, Explaining and Visualizing Deep Learning, pp. 267-280. Springer, Cham, 2019."	ok.	Experiments were done with a public dataset (CheXpert). Code will be made available. Reproducibility is guaranteed.	The authors provide enough information for reproducing the reported results.	For the relevance experiments, the authors fine-tune the network and enforce the condition that the image looks within an \epsilon of the truth image while also optimizing for the saliency map to look identical. The result ends with an adversarial attack where the class changes, but the saliency maps stay the same. For some saliency maps, e.g., Grad-CAM, which back propagates the highest output class probability, is it correct to think the network is somehow learning the saliency map invariance? What would happen if you took the Grad-CAM of the truth class, i.e., atelectasis, in the authors' example? How would that change the Grad-CAM? Can relevance be established then? Further, can the authors comment on the real-life scenario where such an attack can happen? For both methods, there was a need to fine-tune the network for a few epochs to enforce this saliency relationship and create the adversarial image. Without fine-tuning, if you have a white box or a black-box attack, isn't the expectation that the performance of a regular model drops, and the saliency will also be meaningless? What would be the outcomes if an adversarial image was run through this model without the fine-tuning of the model eq 2-4 and using a standard attack?	"There are some minor typos in the paper: page 5, ""Each adversarial image xp is is"", ""optimizerfor"", ""Adversarial images xs for evaluating resistance is""; page 8, ""The propose properties"". In terms of the existence of transferable saliency map attacks, it would be interesting to see a more theoretical analysis of the methods and results obtained (something in the line of Ancona et al. [https://arxiv.org/pdf/1711.06104.pdf?ref=https://githubhelp.com]). Maybe to consider for a future journal version of the work."	Please refer to the weaknesses of the paper. Although this work has solid with experimental evaluations that is valuable for the MICCAI community, how to apply the concepts and results to improve the trustworthiness of saliency maps in medical image application is unclear.	I question some scenarios where the paper demonstrated the saliency relevance and resistance. Nevertheless, this is an interesting topic that needs exploration, and I base my evaluation on these points.	This work helps to draw the community's attention to the (lack of) trustworthiness of most interpretability saliency map methods. They propose two novel properties to evaluate the trustworthiness of saliency map methods. Experiments are well-performed and demonstrate the lack of robustness of several widely used saliency maps. This work is worth discussing at MICCAI as it demonstrates clear failures of currently used interpretability methods.	NA
378-Paper0938	Parameter-free latent space transformer for zero-shot bidirectional cross-modality liver segmentation	This paper addresses the problem of cross modality learning from the perspective of abdominal MR/CT segmentation. A nominal intensity model for MRI and CT seeks to create an invariant latent space. The target is for zero shot cross-modality learning.	The authors propose a zero-shot bidirectional cross-modality liver segmentation method by investigating a parameter-free latent space through the prior knowledge from CT and MR images,which address the domain shift in cross CT-MR liver segmentation task. The evaluation is done on a variety of datasets. The structure of the manuscript is clear. This is an interesting and good paper.	1.This work provides a new paradigm for the task of cross-modality liver segmentation: solving the problem of modality transfer and domain shift through parameter-free latent feature space. 2.Based on the prior knowledge of liver intensity information, a bidirectional cross modalities latent feature converter is proposed to project CT and MR images into a common space.	use of modern transformer approaches integration of a clever transform model to adapt target intensity solid descriptions of latent spaces	The authors propose a zero-shot bidirectional cross-modality liver segmentation method by investigating a parameter-free latent space through the prior knowledge from CT and MR images,which address the domain shift in cross CT-MR liver segmentation task. The evaluation is done on a variety of datasets. The structure of the manuscript is clear. This is an interesting and good paper.	This paper proposes a novel way to solve the cross-modality segmentation puzzle: its main focus is on feature commonality from different modalities images, without relying on deep learning models, and most of the existing research is devoted to the optimization of neural network models. Through the inherent prior knowledge (liver intensity distribution), a bidirectional parameter-free latent feature space is found. When one modality is not marked, the cross-modality segmentation is realized by using the data of the other modality.	The writing quality renders much of the text very difficult to parse. Lack of baseline zero-shot approaches numerically varied / heterogenous zero-shot performance unclear degree of domain specific information embodied in the transfer function lack of robust baselines for core intramodality model	The evaluation is not adequate. More experiments should be done to compare the proposed method with other related works.	The comparative experiment is not sufficient: The performance of the current backbone in the case of single-modality, cross-modality and cross-site images is listed in Table 1, and it can be seen that the backbone performs moderately well in the case of single-modality and poorly in the case of cross-modality and cross-site images. However, this experiment can only show that it is the defect of current backbone, and cannot prove that most of the existing methods cannot do cross-modality and cross-site images segmentation. The comparative experiment can not fully explain the problem: In Table 1, for the experiment of CT - > CT of backbone setting1, after replacing the test data, dice decreased from 97.08 to 85.07, which seems acceptable; However, in the MR - > MR experiment, after replacing the test data, dice decreased from 87.03 to 41.09, which can not indicate whether it is a problem with the model or the data itself. LST bidirectional problem: It is mentioned that LST is a bidirectional latent feature space, and its feature mapping is the common feature closest to different modalities. However, in the third experiment in Table 2(IG kernel setting3), there is a large gap between the experimental results of CT -> MR and MR -> CT under the same dataset, which seems to fail to reflect the bidirectional nature of LST, and there should be no large difference in results in the same latent space.	The reproducibility of the paper is adequate with open datasets. Code / models do not appear to be shared.	The idea is clear. it is easy to be reproduced.	This paper uses the LST method proposed by itself and a common backbone to form the model architecture. The dataset adopts the public dataset. For the LST method proposed by the author, the specific calculation formula and brief derivation process are given in the paper, which is less difficult to reproduce. On the whole, it has high reproducibility.	The lack of baselines and alternative approaches renders this approach interested, but not well connected with the literature. It is uncertain if these approaches would be superior to established techniques.	The evaluation is not adequate. More experiments should be done to compare the proposed method with other related works.	Adding different comparison experiments and using the common shortcomings derived from different backbone experiments to show that the problem proposed to be solved in the article is a problem that exists in many current studies. The use of only one backbone method does not effectively indicate whether it is a problem of the backbone itself or a common problem of existing studies. Optimize the common feature representation of the proposed potential space to better balance the features between the two modalities. The current latent space formed by the a priori knowledge of liver intensity seems to be more biased towards the feature representation of CT images and less expressive for MR images, which makes the segmentation from CT to MR to a greater extent than the segmentation from MR to CT under the same conditions, and also the segmentation from CT to CT to a greater extent than the segmentation from MR to MR	The driving factor is the lack of inclusion of baselines for zero shot learning.	The authors propose a zero-shot bidirectional cross-modality liver segmentation method by investigating a parameter-free latent space through the prior knowledge from CT and MR images,which address the domain shift in cross CT-MR liver segmentation task. The evaluation is done on a variety of datasets. The structure of the manuscript is clear. This is an interesting and good paper.	For recommendation 1: The paper uses a backbone network to verify the difficult problem of cross-modality, which is not very strong to prove the problem if it is the only experiment, and the experimental results it produces may also be the deficiency of the backbone model itself. In addition, the backbone is not one of the best backbones (e.g., nnunet, swin-transformer) available. For recommendation 2: The important innovation of the article is to propose a potential space to map the features of two different modalities, which makes the difference minimized to achieve better cross-modal segmentation results. The experimental results of the article show that there is a large difference between the experimental results of CT to MR and MR to CT, and this difference cannot effectively explain the commonality of the features of the potential space proposed in the article for the two modalities, and it can be found from the results that the potential space is more biased to the representatio
379-Paper1793	Patcher: Patch Transformers with Mixture of Experts for Precise Medical Image Segmentation	This work presents a new transformer-based network with three branches to predict a Gauss map, a boundary map, and a contour map. Then, a MoE-based decoder is presented to disentangle features. Experimental results on stroke lesion segmentation dataset and polyp segmentation dataset show that the developed network outperforms state-of-the-art methods.	The paper proposes a novel transformer-based model that is composed of multi-scale Patcher blocks and a Mixture-of-Experts module. The methods achieves SOTA results compared with existing methods.	The authors present a new Neural Network Architecture for image segmentation, combining ideas from convolutions and transformers for the feature extraction and a Mixture of Model approach for the reconstruction. They show that their model beats SOTA on two medical imaging segmentation tasks.	The authors present a transformer-based network for medical image segmentation by embedding mixture of experts, and it has achieved a superior performance over state-of-the-art methods. The writing of this work is easy to follow. Two datasets are employed for evaluating the proposed segmentation method.	The proposed model structure, including Patcher and MoE, is novel and interesting. The experimental results are comprehensive and convincing. I appreciate Figure 4 showing the function of each expert.	The design choices of the presented architecture are justified properly, the paper is well written and organized. The ideas are either novel, a novel combination of existing ideas or well executed.	It is unclear why the authors include the mixture-of-experts (MoE) into the decoder for boosting the segmentation method. What about the performance of removing the MoE from the decoder? According to Section 2.4, the authors just claimed that the upsampled MLP features as the expert features and then utilized an attention block to weight the so-called expert features for predicting the segmentation result. Hence, the novelty of MoE-based decoder is limited. The technical novelty of the Patcher block is unclear. It seems that the Patcher is based on vison transformer block. What is the main difference? And an ablation study is required to evaluate the effectiveness of the main difference.	Can you explain or show the limitations of the proposed method?	The experiments do not accurately depict the performance of the model. All the presented results are provided by the authors, which does not guarantee that each model was set up to perform in optimal conditions.	The authors have claimed that they will release code, trained models, and results.	All the code and models will be released.	The authors will release the code upon paper acceptance. The implementation details in the paper could be enough to reproduce the network architecture. The stroke lesion dataset is private, making future result-proofing and comparison impossible in that dataset.	The technical novelty of the Patcher encoder is not clearly explained. It seems that the MoE-based decoder tends to be over-claimed. It just utilizes an attention block to weight features at different CNN layers.	Please make the sub-subsection titles in Section 3.1(and others if applicable) in consistent formats.	"As the main contribution of the paper is the creation of a new neural network architecture, it would have been welcome to compare it against established benchmarks in the field of medical imaging. Comparing this new architecture with SOTA methods on a new dataset, or a dataset where other state-of-the-art technics had not been applied yet, does not let us know whether the model is globally better, only excellent at the selected 2 tasks, or whether the other methods were not applied with the same amount of efforts. To clear up any doubt, please evaluate your model on a segmentation benchmark where some other technics have already been applied. As an example, [1] does not report the same result for UNet depending on training conditions. For the ablation study in Table 3, putting the relative increase/decrease in accuracy would be more enlightening than the absolute results achieved by each combination. Try to answer: ""What is the relative increase in DSC/IoU when swapping SETR decoding method with ours?"". Absolute results are helpful, but are not enough to get the full picture. [1] Huang, C. H., Wu, H. Y., & Lin, Y. L. (2021). Hardnet-mseg: a simple encoder-decoder polyp segmentation neural network that achieves over 0.9 mean dice and 86 fps. arXiv preprint arXiv:2101.07172."	Please refer to the weakness.	The main factors for my rating are mentioned in Question 3 and 4.	The paper is clear and presents a novel model. It is not certain that it beats all other SOTA method, as some papers claim higher scores on the KVASIR-seg dataset, but it contains many nice ideas that could be used in future research and good justification for the different design choices of the presented network architecture.
380-Paper1843	Patch-wise Deep Metric Learning for Unsupervised Low-Dose CT Denoising	The paper proposed a novel unsupervised learning approach for low-dose CT reconstruction using patch-wise deep metric learning. Experiments confirmed that the deep metric learning plays a critical role in producing high quality denoised images without CT number shift.	In this work, the author applied a patch-wise deep metric learning method to the hidden embedding space in the mid-layer feature map of GAN's generator to maintain the structural information and suppress the noise. The method achieves better PSNR and SSIM with less CT numbers shift compared with existing unsupervised Low-Dose CT denoising methods.	This paper introduces deep learning in a deep feature space. This is combined with an adversarial loss to preserve feature consistency. This improves the denoising performance, while maintaining greater CT number accuracy.	This paper proposed a novel method for low-dose CT denoising based on the patch-wise deep metric learning. The algorithm can successfully make the network focus on the anatomic information and neglect the noise features by the push and the pull between the features in the embedding space.	Use deep metric learning by setting two hidden embedding space in the generate network before the output layer to make the GAN more stable Set the positive pair from same location of noisy input and denoised output to maintain the structural and set the negative pair from different location of same image to suppress the noise level	Most denoising papers do not take into account CT numbers, but purely consider PSNR and SSIM. This is an excellent view of the problem. The method is simple and can be easily reproduced. The results are very promising statistically. The ablation studies provide good explanation for hyper parameter choices.	The sample size in this study was limited and the one-time split could not verify the generalization ability of the network well.	The purpose of the proposed metric is not well explained. Why can setting the positive pair and the negative pair be used to maintain the structural information and suppress the noise level? Why is the metric method used in the mid-layers of the generator? Why is the metric loss L2 normalization with a temperature parameter? All these setting is quite similar to contrastive learning instead of metric learning. In the ablation study results (supplementary material table 1), how the proposed method addresses CT numbers shift is not demonstrated. The motivation of maintaining previous CT number statistics is questionable. The LDCT has worse CT number statistics when compared to NDCT. The proposed method should learn the CT shift and make the output as much close as the NDCT. The objective function in metric learning is to pull the patches from the same location together while pushing the patches from neighbors away. The trivial solution is to make network output the same input; that is, doing nothing. This is contrast to denoising. The description of the datasets and the experiment implementation is insufficient. Especially for the AAPM challenge, even the quantity and the division of the dataset are not given. How the unsupervised data is constructed is not stated.	This paper proposes learning in a deep feature space as novel. Actually, using deep feature extractors, particularly the VGG extractor, is a very common method in LDCT denoising problems. Reference 22 in this paper has introduced this method. While several GAN based approaches are discussed here, I am interested to see comparisons with other directly supervised approaches. Perhaps the GAN loss is not sufficient. The choices for the windows used in Figures 3 and 4 are very unusual. The authors have used the 2016 AAPM dataset. Actually, this dataset is out of date, and has been superseded by the 2020 Cancer Imaging Archive dataset, which is significantly larger.	Good.	The approach is quite difficult to reproduce. On the one hand, it's not clear to know about the specific setting of the forward processing of the generative adversarial network, such as the convolution blocks. On the other hand, the description of the datasets and the experiment implementation is insufficient. Especially for the AAPM challenge, even the quantity and the division of the dataset are not given.	Reproducibility is sufficient. All implementation and dataset details are provided.	The paper proposed a novel unsupervised learning approach for low-dose CT reconstruction using patch-wise deep metric learning. Experiments confirmed that the deep metric learning plays a critical role in producing high quality denoised images without CT number shift. However, there are some concerns as follows: The sample size in this study was limited and the one-time split could not verify the generalization ability of the network well. Cross-validation should be conducted for further validation. Authors should provide more detailed descriptions of the pipeline in section 2.1. The description should include how the GAN is applied to obtain output images from noisy input images, how features are extracted from these two types of images and whether two generators have the same parameters. Moreover, authors did not present how to combine output images with low frequency images to obtain final results. In fig. 3, using high frequency images as the gold standard is beneficial to present the difference images. In table 1 and table 2, the title should be written before the table.	The approach needs more ablation study to demonstrate the improvement in performance with CT numbers shift. The approach needs to be compared to more existing state-of-the-art approaches, which include both supervised and unsupervised methods. The motivation of maintaining the original CT number is questionable. The motivation of the objective function is also questionable. More experiments should be conducted to show how it works. The approach needs more detailed description of the model blocks, datasets and experiment implementation to access better reproducibility. The related work needs more content about deep metric learning and the related methods applied to the medical image processing. I wonder whether the image of domain Y can be called HDCT, because the model does not use any HDCT image, but only denoised LDCT images.	Perform the quantitative comparisons with a larger and more modern dataset. Justify the choices for the windows, or choose more conventional windows. Perform more comparisons with non-GAN state-of-the-art methods.	The limited sample size and one-time split could not verify the generalization ability of the proposed algorithm.	There is a lack of explanation of proposed methods used in the study to demonstrate the novelty . There is a lack of more reliable experiments and the ablation study to demonstrate the effectiveness and generalizability, especially the CT numbers shift.	This paper shows a new perspective on a relatively old problem, and then proposes a novel, yet simple method to deal with it. The statistical results display this method's superiority over others, and ablation studies well justify the choices made for hyperparameters.
381-Paper1020	PD-DWI: Predicting response to neoadjuvant chemotherapy in invasive breast cancer with Physiologically-Decomposed Diffusion-Weighted MRI machine-learning model	This study proposes a method for prediction of pathological complete response (pCR) to neoadjuvant chemotherapy for breast cancer. The method exploits diffusion weighted MRI data, taking into account both pseudo-diffusion and pure diffusion, using an approximation of the bi-exponentional IVIM model fitting. Machine learning is performed using a radiomics approach, using XGBoost classifier to combine all features (including some clinical features). On the BMMR2 challenge dataset, the method shows improved performance compared to other DWI and DCE MRI based methods.	The study introduces a physiologically decomposed diffusion-weighted MRI (PD-DWI) machine learning model to predict the pathological complete response (pCR) from DWI and clinical data. The proposed model improved the performance of predicting pCR when applied to a public breast data challenge (BMMR2).	This work provided a PD-DWI method for the breast cancer pCR prediction, which could decompose DWI data into an ADC 0-100 map and an F map. And the new maps-based radiomics model could get the optimal performance that goes beyond the top performance in the BMMR2 challenge.	1) Very well written manuscript. 2) The clinical relevance is well motivated (avoiding the need for contrast-enhanced MRI). 3) The methodological choices are well motivated. 4) The method is validated on a public challenge dataset, and shows promising results.	Used the BMMR2 challenge dataset for training and testing for the proposed machine learning model. Breast DWI data were analyzed by the bi-exponential signal decay model, generating D (pure diffusion coefficient), D* (pseudo-diffusion coefficient), and F (pseudo-diffusion fraction). Both ADC and F were used to extract 3D radiomics features for model prediction.	The main strengths of this work: (1) One novel medical imaging method, called Physiologically-Decomposed Diffusion-Weighted MRI (PD-DWI), demonstrated a substantial improvement in pCR prediction, without the need for lengthy DWI acquisition times, Gadolinium-based contrast agent injections, and DCE-MRI imaging. I think it's interesting, and it is a good study of medical physics. But the work only demonstrated that PD-DWI could improve the machine learning model performance, not further finding new imaging patterns or biomarkers, that were the most important for clinical application. (2) Some interesting finds. The work found the relation between DWI signal attenuation decay and pCR prediction and accounted for the different physiological cues associated with pCR as reflected by the DWI signal rather than using aggregated information by means of the ADC map. (3) The top prediction performance. The model got an AUC of 0.8849, which overperformed the top-performance in the challenge (AUC = 0.8397).	1) From a technical point of view, the novelty is a bit incremental. The technique for extracting approximate pseudo-diffusion fraction and pseudo-diffusion maps is based on ref [11]. The radiomics method using XGBoost is fairly standard. The novelty is in the combination of techniques and the application to this particular task. 2) From Table 2, it seems there is a substantial improvement compared to state of the art methods. From Table 1, we see that the largest part of that improvement is already achieved by the ADC_0-800-only method, i.e., the one based on a simple ADC estimate. The advanced PD-DWI approach only adds another percent. This makes me wonder what was the key innovation in the overall framework that explains the improvement compared to state-of-the-art. Is it the use of XGBoost? The manuscript would have been more insightful if this question was answered. 3) Confidence intervals and/or results of statistical testing on the AUC measures are missing, so it is not clear whether the reported improvements are statistically significant.	It was not clear how clinical features were combined and used. AUC with and without including clinical information will be required. The feature selection process needs to be clarified with some justification (the number of features, selection criteria, and consistency and stability).	The main weaknesses: (1) The overall innovation may not be enough. The main innovation was in DWI processing, and the parts of feature engineering and modeling were normal. And the DWI signal processing methods were not original innovative, the formulations were others in medical physics [1-2]. (2) The model architecture was simple (Fig.3). (3) The work found some interesting findings and demonstrated that PD-DWI could improve the machine learning model performance. But the work was not further finding new imaging patterns or biomarkers using the ADC 0-100 and F maps, which was the most important for clinical application. (4) In Table 1, all the ADC-based machine learning models overperformed the BMMR2 challenge Top-3 performances. I think the reliability of the results may be questioned by readers. [1] HST.583 Functional Magnetic Resonance Imaging: Data Acquisition and Analysis. https://dspace.mit.edu/bitstream/handle/1721.1/51692/HST-583Fall-2006/NR/rdonlyres/Health-Sciences-and-Technology/HST-583Fall-2006/Assignments/ps3.pdf. [2] Le Bihan D, Breton E, Lallemand D, Aubin ML, Vignaud J, Laval-Jeantet M. Separation of diffusion and perfusion in intravoxel incoherent motion MR imaging. Radiology. 1988 Aug;168(2):497-505. doi: 10.1148/radiology.	The dataset (part of a challenge) and experiments are very clearly described. Code will be made publicly available.	No issues.	The data was from BMMR2 challenge (https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=89096426), and the authors would provide the code and trained models upon acceptance. I think the reproducibility of the paper was good.	Sec 3: the the -> to the scale_pos_weight -> please explain to which method/component this hyperparameter belongs, and what it does. Addressing weaknesses 2 and 3 would make the paper stronger.	It is interesting to see the model was only based on breast DWI data, but it would be useful to investigate a way to combine different imaging sequences, particularly DCE-MRI. DWI sometimes suffers from insufficient image quality, and a combination of DWI and DCE will either improve the performance or the reproducibility. Different feature selection approaches with and without clinical information need to be further investigated to improve the generalizability of the model.	(1) May consider the other feature selection and machine learning modeling methods, not only ANOVA and XGBoost. (2) The work selected 100 features with highest ANOVA F-values for the modeling. I don't think it's an appropriate decision, usually a feature need at least ten observer samples. (3) Whether most samples all meet the rule of Fig.1? (4) Lack of radiomics feature repeatability analysis in the new maps. (5) Please add the discussions why the new maps-based radiomics models all got the optimal performance.	I enjoyed reading this paper, it is a solid investigation, but the novelty and impact might be too limited.	The proposed model showed improved prediction of response to neoadjuvant chemotherapy in breast cancer using publically available BMMR2 breast data challenge.	At a time when deep learning is rampant in Medical Image Analysis, we need exploration and application in traditional signal processing and medical physics algorithms. The work was not innovative enough, but overall it was a good study.
382-Paper1435	Personalized Diagnostic Tool for Thyroid Cancer Classification using Multi-view Ultrasound	The manuscript presents a new framwork for thyroid nodule classification using double view US images.	The authors propose a new multi-view thyroid tumor classification network. It is mainly composed of three parts: a swin-Transformer for feature extraction, a personalized weighting allocation network that customizes the multi-view weighting for different patients, a self-supervised view-aware contrastive loss that considers intra-class variation inside patient groups and can further improve the model performance.	This paper proposes a personalized diagnostic tool for thyroid cancer diagnosis, consisting of  a multi-view classification module for feature extraction and a personalized weighting allocation network that generates optimal weighting for different views. Experiment results showed that the trained model outperform state-of-the-art approaches in thyroid cancer diagnosis.	A novel approach with three main components towards personalized diagnosis. Application of multi-view US images for better diagnosis output.	The research topic is relatively novel. There are few works on multi-view ultrasonic image classification at present. This work designs a view-weighted fusion module. The existing multi-view ultrasonic image classification work treats different views without distinction, however different views have different degrees of importance in different tasks. Based on this, the authors design a personalized weighting allocation network to dynamically fuse different views. This work designs a self-supervised view-aware contrastive loss. Based on the original contrastive loss, the authors design a contrastive loss from the perspective of multi-view, and verify that the new Loss has better effect than the original loss through experiments.	The overall framework is clear and the proposed model has shown its effectiveness.	It is not clear how many patients are included in the study.  Hence, it is difficult to evaluate the effect of personalized aspect of the proposed approach. There is no discussion on how feasible is to have this tool for the clinical application.	The conducted experiments are quite limited. The reproduced AdaMML has poor performance, but no explanation is given. The description of the data collection process is not completed, such as descriptions of the experimental setup, device(s) used, image acquisition parameters, subjects/objects involved, instructions to annotators, and methods for quality control. Were the 4529 sets of multiview US images collected from 4529 patients or not?	Personalized weighting generation module is functionally similar to the attention mechanism, which is used more frequently. For experimental implementation,  the l is set to 0.01, the authors should provide the explanation for it. It will be better that if the authors provide the actual time cost.	The implementation steps of the three main contributions are very well explained. An in-house dataset is used for the evaluation. There is no information regarding the data acquisition process.	The description of the data collection process is not completed, such as descriptions of the experimental setup, device(s) used, image acquisition parameters, subjects/objects involved, instructions to annotators, and methods for quality control. Were the 4529 sets of multiview US images collected from 4529 patients or not?	The results in this paper are easily reproducible.	Section 3: Please clarify how many patients are included in the dataset. Section 3: It is not mentioned how many images were included in the dataset after data augmentation. Please clarify this. Section 4, Table 1: Are the results in this table showing the performance of each approach on one common dataset, or each approach is tested on a different dataset? Section 4: Proposing a personalized diagnosis tool that is one of the main claims of this paper needs to be more discussed. What are the main advantages of having such a tool in comparison to the other methods while the performance of the proposed approach is slightly better than the state-of-the-art?	The authors should conduct more experiments to demonstrate the effectiveness of the proposed mothed. The authors should explain why the reproduced AdaMML has poor performance. The description of the data collection process should be completed, such as descriptions of the experimental setup, device(s) used, image acquisition parameters, subjects/objects involved, instructions to annotators, and methods for quality control. The authors should clarify if the 4529 sets of multiview US images were collected from 4529 patients.	The authors should provide more details of experimental implementation and computation complexity for the proposed method.	The paper presents an intresting approach for thyoid nodule classification. There is no discussion on how feasible is to implement this tool in a real-time clinical scenario (on a matter of processing time) while we are talking about multi-view images.	The research idea is interesting and relately novel, but the conducted experiments are quite limited.	The proposed method is simple and reasonable, yet both personalized weighting generation and view-aware contrastive loss in the model are frequently used. Furthermore, the personalized weighting generation module is similar to the attention machanism, why didn't the authors choose it?
383-Paper2497	Personalized dMRI Harmonization on Cortical Surface	Harmonizing site-dependent effects on diffusion MRI is critical in multi-site clinical studies. Most methods take a volume-space-based approach to harmonize the imaging measures in a reference space. But these methods are not optimal to study cortical gray matters because of the heterogeneous structures of cortical surfaces. This work introduces a method to harmonize diffusion MRI measures at the cortical surface for individual subjects. The method is based on a distance measure to find corresponding vertices on the surface. The performance of the method is evaluated based on the HCP and HCPD data sets.	The paper introduces a surface-based harmonization method to reduce inter-site variation in diffusion-weighted images.	dMRI harmonization method that personalizes inter-site mappings	1) The problem of the harmonization of surface-based imaging data is very relevant in clinical studies. To the best of my knowledge, this is the first work to consider this problem. 2) A method is developed to analyze inter-subject local correspondence which includes the geometric properties of the surface and the cortical thickness information.	The idea of reducing cortical mismatch for better harmonization is novel. The paper is well-written. The authors stated the problem and motivation well and clear.	Well-motivated Decently written Well structured Strong study design Strong technical implementation Neat figures	"1) The proposed local correspondence matching method only considers the geometric information and cortical thickness which does not require diffusion MRI data. It is not clear why matched geometry indicates matched diffusion MRI measures.  2) The ""grayordinate"" format as included in the cifiti data in HCP has become standard for surface-based analysis. It is not clear why that is not an option for surface-based harmonization."	No statistical test was done to prove the significance of the numerical results. There is little to no discussion about the limitation of the work.	Certain colloquial language throughout the submission can be substituted	I don't see any limitations on the reproducibility.	Study can be reproduced	Not reproducible - although publicly available datasets are used, implementation code has not been made available	1) It will be helpful to compare with the standard cifity surface-based analysis.  2) The diffusion MRI data from HCP has a very high spatial resolution. For standard clinical data, the resolution will be much lower. Then the partial-volume effect will be a significant problem for surface based analysis. Adding more comments or a solution to this problem will certainly improve this work.	"The authors stated ""Alternatively, surface-based registration can alleviate some of this anatomy misalignment problem for dMRI harmonization, but it is still insufficient to resolve this challenge."". Please explain why surface-based registration is not sufficient to solve cortical mismatch or at least provide reference for the statement. Are the improvements in Table 1 significant? Please provide the p-value of the test. Is there any explanation that HCPD harmonization task is always better than HCP harmonization task? The authors only harmonize b=3000 shell, then how FA and MD was calculated? Only from b=3000 shell or from the unharmonize b=1000 shell?"	Recommend authors to avoid colloquial language Evaluate on more datasets	This work points out an interesting question on dMRI harmonization. But the proposed method is not convincing since dMRI data was not naturally used to define local correspondence and there is no comparison with the standard cifity format.	Novel surface-based DWIs harmonization. Minor weakness due to lack of statistical test for numerical results.	Strong study design and implementation
384-Paper0842	PET denoising and uncertainty estimation based on NVAE model using quantile regression loss	The authors present an approach to denoise PET images that relies on a VAE using quantile regression loss, which enables uncertainty estimation.	"This manuscript, ""PET denoising and uncertainty estimation based on NVAE model using quantile regression loss"", reports an improved PET denoising method by applying a NVAE model using quantile regression loss method (NVAE-QR) compared to a Unet-based and another NVAE model using Monte Carlo sampling (NVAE-MC). The proposed framework evolves from the NVAE model by minimizing quantile regression loss and Kullback-Leibler (KL) divergence term. The proposed model was tested using the real 11C-DASB dataset. The authors first generated a low-quality PET image by down-sampling the full list-mode data by a quarter, and 20 subjects were used for training, 3 for validation, and 3 for testing. By comparing the original image to a denoised PET image from each model, the authors evaluated the performance based on the peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM)."	The paper proposes a deep learning model for simultaneous PET image denoising and uncertainty estimation. It uses the NVAE Variational Autoencoder which is trained  with the quantile regression loss.	Enabling uncertainty estimation when processing images is important to increase trust in computational tools, especilly in a clinical context. Thanks to the quantile regression loss, uncertainty is estimated in a more computationaly efficient way than in previous works.	The major strengths of the work are reflected in 1) improved image denoising based on the PSNR and SSIM metrics, 2) NVAE-QR model avoids the variance shrinkage issue, and 3) shorter processing time compared to the other two methods.	Using the quantile regression loss avoids variance shrinking and allows estimating the variance directly from the quantiles, which is faster than using Monte Carlo sampling.	The novelty compared with a previously published approach also based on the NVAE and enabling uncertainty estimation seems very limited. The gain in synthesis accuracy is very small and the improved computational efficiency is not quantified. The usefulness of estimating uncertainty is not really demonstrated. The data set used is small (20 subjects for training, 3 for validation, and 3 for testing), meaning that no strong conclusion can be drawn.	Considering that the low-quality PET images are generated by down-sampling the list mode data, it is also important to consider other noise factors related to PET, such as noise distribution, motion, imperfect attenuation correction, etc that differentially contribute to the noise in the PET images. To bring this algorithm to a real-world application, have the authors investigated combinations of other noises to improve using the deep learning approach? Do the authors then expect that the NVAE-QR would still perform better than the other methods? In addition the methodology while useful and applicable to an important problem in PET imaging is not that novel.	There is no reference to prior work where the quantile loss has been used. The dataset is very small, only 3 subjetcs were used for validation and testing. English is not very good.	"The authors ticked ""Yes"" for most items of the reproductibility checklist, which is sometimes in contradiction with what is provided in the paper (e.g. range of hyper-parameters considered). It seems that the code will be made available."	The authors did not provide their statement on code availability. This would be highly desirable.	ok	The assumption that the output distribution follows the logistic distribution should be justified. The authors mention supplementary material but none was available. From the authors' point of view it is a good thing that the proposed approach has a larger variance but would the clinicians, i.e. the users, think the same way?	It would really benefit the paper to present example cases or datasets to highlight the performance of the model in a research/clinical context (in an ROI or lesion-based approach), as well as other voxel-based metrics to indicate performance. While PSNR or SSIM are important metrics, a smoothed denoised image might provide high quantitative values on these metrics while missing important high-frequency information or small lesions for example. Page 1, Abstract: Not all abbreviations were first defined in the abstract section. For example, PSNR and SSIM. Page 2, Introduction 2nd paragraph: While the authors have included the description and extension of VAE and NVAE to provide a better understanding of the improvement, the Unet-based model literature was relatively lacking. It would be equally important to provide a bit more context rather than briefly mentioning Bayesian neural networks and suDNN. Page 2, Introduction 2nd paragraph: The authors used tumor identification as an example to describe the urging need for the novel technique to improve the PET denoising and uncertainty estimation. However, the dataset used to test the model of interest in the study was a brain imaging PET data. Page 5, Dataset: It would be great to include more detailed information on the PET data acquisition such as the dynamic framing, different types of corrections (transmission, scatter, random) applied, etc. Page 5, Data analysis: Based on the manuscript, the authors have reported using 200 training epochs and a 0.01 learning rate. How do the PSNR and SSIM change over different training epochs and different learning rates? Do the authors expect any further improvement? Discussion: Would this model still perform better than the other models for different PET tracers? Figures with a color scale bar should indicate the unit.	"Explain what is meant by "" group of low-quality and high-quality training pairs"". Include a reference or an expression for the structural similarity index measure (SSIM). section 3.2 is confusing because it describes the usual VAE loss but it refers to it as the loss used in this work. Was there a reduction in variance shrinking?"	The idea is interesting and valuable but the novelty seems limited and the results not strong enough.	The topic of this study carries a great interest in the research field of using PET imaging to improve the image signal-to-noise ratio. Moreover, the application of the deep learning approach has gained great support to improve PET image signals and other medical imaging. I would accept this paper to be presented at the conference.	The use of the quantile loss is interesting.
385-Paper0844	PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation	This paper presents a mixed network architecture that combines convolution and transformer for better performance.	propose a novel hybrid architecture for medical image segmentation called PHTrans, which parallelly hybridizes Transformer and CNN in main building blocks to produce hierarchical representations from global and local features and adaptively aggregate them, aiming to fully exploit their strengths to obtain better segmentation performance.	This work looked at a popular and important problem in medical image segmentation - how to efficiently hybrid CNN and ViT. To this end, the authors proposed a hybrid architecture, in which convolution and self-attention (from ViT) are performed simultaneously at each downsampled and upsampled scale in U-shaped architecture. The manuscript is well written with extensive experiments demonstrating the benefits of the proposed method on two datasets.	The studied problem is important. The designed trans&conv block is neat.	It outperforms nnUNet and some medical image segmentation transformer models such as TransUNet and CoTr.	This work looked at a popular and important problem in medical image segmentation - how to efficiently hybrid CNN and ViT. Reasonable novel method has been proposed. Extensive experiments have demonstrated substantial benefits of the proposed method on two datasets. The manuscript is overall well written.	Missing reference to closely related work. Datasets are small.	This paper cites but do not show numerical results of UNETR, which is very important and necessary SOTA method.	This submission has good quality of finishing hence I am in general satisfied with acceptance, if have to list weaknesses the following are minor rather than major: The authors argued Volume-to-Sequence (V2S) and Sequence-to-Volume (S2V) operations as one of their key contributions, however, it's not clear to me, unless I have missed, what are those operations? Are they learnable or simple transformations?	I think the paper is reproducible.	I think some numerical results of baselines are not consistent with other one. For example, in the same dataset of BCV dataset, nnUNet (most important medical image segmentation model) from UNETR(https://arxiv.org/pdf/2103.10504.pdf, table 1) is 88.8, while in this paper it is only 87.75 and PHTrans is only 88.55. I have added my comments below if the train&val dataset are not split identically.	implementation details are provided, code is not submitted	This paper presents a network architecture that combines the design of convolution and transformer blocks so that both local and global visual representations are captured. The motivation is clear and experiments validate the effectiveness of the proposed approach. I have the following concerns. The relationship to prior work is not well analyzed. In particular, In ICCV 2021, there is a paper named <Conformer: Local features coupling global representations for visual recognition>, which delivered a very similar idea to this work. The authors shall add discussions on this topic. In addition, it is helpful to provide some visualization (or other analytical results) showing how the mixed architecture helps recognition - please also refer to the above paper for examples. The improvements on both datasets (BCV and ACDC) are marginal compared to the baseline and other competitors. While I understand that the baselines are already high, but, I strongly suggest the authors to provide additional results to support that the improvement is solid (e.g. by qualitative or other quantitative studies). This is very important considering the small volume of the studied datasets. Overall, introducing a novel architecture is helpful for medical image analysis. The paper is well written and I recommend weak acceptance.	"Recently, many transformer model paper for medical image segmentation have been made. However, most of them do not make fair comparison. I am curious why the author have cited UNETR but do not show its performance. And why the author followed the paper setting of nnFormer(https://arxiv.org/pdf/2109.03201.pdf), where it outperforms nnUNet in BCV/Synapse dataset, but in this PHTrans paper, nnUNet is better than nnFormer. And also, in UNETR (https://arxiv.org/pdf/2103.10504.pdf), its results are better than PHTrans, so I suggest author add detailed results of UNETR(https://arxiv.org/pdf/2103.10504.pdf) and Swin UNETR(https://arxiv.org/pdf/2201.01266.pdf) because I think those two transformer models are SOTA right now. Back to framework of this paper, I think it lacks novelty. The most novelty part in PHTrans is adding Conv Block parallelly in ""Trans&Conv Block"". Too many parts of PHTrans are referenced from Swin Transformer and UNet(encoder-decoder) architecture. Moreover, some SOTA models such as CoTr and nnUNet are also trained from scratched, they outperform certain transformer models which have pretrained models. So I think PHTrans does not need pretrained model, which is great, but that is not enough."	The manuscript will benefit from a clear definition of Volume-to-Sequence (V2S) and Sequence-to-Volume (S2V) operations; The definitions of W-MSA and SW-MSA are not given; The authors declared PHTrans w/o ST essentially have the same architecture as nnU-Net. While we observe a difference between table 4 (PHTrans w/o ST have DSC=87.71 and HD=14.37) and table 2 (nnU-Net DSC=87.75 HD=9.83). Can the authors provide some intuition about what caused such difference, particularly in HD?	Please see the above comments.	Add detailed experiments of UNETR and Swin UNETR to make fair comparison,  I would change my opinion and tolerate lack of novelty if it proves it is a real SOTA which outperforms extremely than UNETR and Swin UNETR. Besides, I would be more appreciated if the author would release the code in Github to check its reproducibility. Because this paper has been released on ArXiv,  the total community would testify its real performance and so do I.	"This work looked at a popular and important problem in medical image segmentation and proposed reasonable novel hybrid architecture with extensive experiments that justified the proposed method. The work is also well written. Hence I recommend ""6: accept""."
386-Paper0288	Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography	This paper tackles the problem of displacement estimation from beamformed RF data in ultrasound elastography. The contribution is a physics-based regularization - in particular, using the fact that human tissue has a relatively small range of Poisson's ratio close to 0.5. This allows the lateral and axial displacements to be correlated.	The paper proposes a new loss function to regularise the training of Ultrasound Elastography. It is based on physical properties that need to be present in a pysically correct estimate of Ultrasound Elastography. In terms of network architecture MPWC-Net++ is employed to conduct the experiments. Experimental results demonstrate significant improvements over the state of the art.	The authors propose a physically inspired constraint (PICTURE) to improve lateral displacement estimation on ultrasound elastography. The experiment results on phantom and in vivo data show that PICTURE substantially improves the quality of the lateral displacement estimation.	The problem of displacement estimation in ultrasound elastography (and similar applications) is still an ongoing unsolved problem and this paper's approach of using biomechanics-motivated constraints is a solid approach.	The paper proposes a novel Loss function that enforces physical correctness during unsupervised training of the network. The experimental results demonstrate strong effectiveness.	The motivation and innovation of this work are good. A new constraint based on the Poisson's ratio for lateral displacement estimation.	Biomechanics constraints in displacement tracking is not completely novel. Also, limited testing is performed and only one previous method is used for comparison. The work does advance the field of ultrasound displacement tracking but is not revolutionary in its approach and still needs to be combined with other displacement tracking innovations for reliable accurate tracking.	"The regularisation during the training is of course only present at train time. As such here is no guarantee that the predictions will always be physically plausible. Therefore, I recommend the authors to investigate whether known operators [1] could be applies in this scenario. This might lead to another paper on MICCAI 2023... [1] Maier, Andreas K., et al. ""Learning with known operators reduces maximum error bounds."" Nature machine intelligence 1.8 (2019): 373-380."	The amount of in vivo data is unknown. The PICTURE appears to be limited to uniform axial strain (see detailed and constructive comments).	Reproducibility is adequate from the clear mathematical derivation.	OK	The code will be publicly available after the paper acceptance. Hence, the reproducibility seems fine.	The proposed method is called PICTURE and is compared to unsupervised learning and OVERWIND from reference [2]. The methods are compared on both phantom and in vivo liver scanning data using CNR and SR (strain ratio), two common metrics of comparison. No simulations are performed where the ground truth of displacement is known. The main contribution of this paper is the development of a learning-plus-physics based approach to displacement measurements that does appear to perform well in these limited test cases with some exceptions where it performs worst.  As negatives, the amount of testing is rather limited, there is no sensitivity analysis to the parameters, and there were no simulations performed with known ground truth. This is therefore a good but early contribution that needs more analysis to prove its benefits with confidence	Paper is very good and in good shape. Other than that I recommend to look into hard-coding the physical properties into the network (see weaknesses)	In Eq. 11, how do you warp I2? How does data loss constrain the displacement W? In Eq. 12, the axial strain is constrained to be near the mean, while the strain in other directions is constrained to be zero, and all first-order derivatives are constrained to be zero. As a result, the strain is the same for all frames, which is not the case in reality. In Eq. 14, how do you select the window, especially the background window? The authors should describe detailed information about the dataset, such as the number of sweeps for experimental phantom and the number of patients for in vivo data. The unsupervised results in Fig. 1 are a complete failure, which questions the validity of data loss and smoothness loss. The authors should analyze the reasons for the failure and conduct further other ablation experiments. The authors should show more cases in Fig. 2. Why does the EPR histogram of PICTURE (Fig. 1 in the supplementary material) range beyond v_emin and v_emax, with two peaks? Can PICTURE be applied to out-of-plane displacements?	Although the methods are sound, the paper makes only a small contribution to the very large body of literature on ultrasound displacement tracking and is of interest to only the subset of ultrasound researchers in elastography who don't already have their own solutions.	"See section ""strengths""."	The authors design a new physically inspired constraint to improve lateral displacement estimation, but it is limited to uniform axial strain. The experimental results show that the proposed framework is superior to the current methods but lacks methodological innovation.
387-Paper0495	Physiological Model based Deep Learning Framework for Cardiac TMP Recovery	The authors introduce a deep-learning method to perform inverse solutions in ECG. The network the authors use, emulates  the general approach of  a Kalman filter with the non-linear  model provided by the deep-learning model. They test  this  method on  synthetic  data.	The paper presents a novel framework for predicting TMP from BSP using data-driven Kalman filtering network. The paper introduced a novel method and compares its results to various recent methods with favorable results for the presented method.	Authors propose an ECGI framework that combines deep learning with the physiological model of TMP dynamics. They argue that they address the limitation in the related works where they consider ECGI as an static mapping at each time step and ignore the temporal pattern of this problem. Considering the ECGI problem as state-space model they introduce a Kalman Filtering network consisting of a transition network and a kalman gain network.	Using dynamics to approach the inverse problem in ECG is  indeed  necessary and useful. That is particularly true for  3D (or 2D endo-epi)  cardiac models. Adapting a Kalman  filter to work for  a non-linear  dynamic  model with neural networks is interesing as well.	The paper presents a novel framework for predicting TMP from BSP using data-driven Kalman filtering network. The paper introduced a novel method and compares its results to various recent methods with favorable results for the presented method.	Designing a hybrid physiologically meaningful DL-based network for recovering TMP from BSP and embedding state update equations into the DL-based framework and which makes this approach interpretable. Taking into account the inaccuracy in the state transition equation and adding the noise variable to compensate for that. Interesting design of the loss function to ensure the accuracy of the state transition results and the final results. Careful design of experiments and superior performance compared to a few related works.	The main limitation of the paper is validation.  The authors provide little details about  it. Despite the claim that they tested this method on 600 subjects, in reality ECGsim provides a  maximum of  3 geometries, since the authors did not specify, the logical assumption is that they only used 1 and simulated multiple cases on that geometry. Failure to test on multiple geometries leads to over-optimistic results,  specially for data-driven approaches. The authors did not  specify  whether they are adding noise to the measurements or not.  In its absence, all the results will be over-optimistic. Separating  experiments by  location and  ischemia is  logical and acceptable, however, the reader is left wondering whether the network could not generalize to all cases. No information about which activation times or localization method is provided. Without it, it is not clear how generalizable are the results on the corresponding metrics.	The results are produced on simulated data without discussing the size of simulated data (is it good enough size) and what would be the challenges faced with application of this method on real data. The paper also doesn't have a thorough discussion, including any limitations of this work, in particular with respect to its application on real data or comparing to other existing recent methods.	"Authors argue the lack of literature in considering the temporal dynamics in this problem. However a recent work, Jiang et al 2021 titled ""Label-Free Physics-Informed Image Sequence Reconstruction with Disentangled Spatial-Temporal Modeling"", consider a very similar approach to solve the ECGI problem. They similarly consider the state space model approach and learn the transition model and the temporal dynamics in the latent space of their model and use a decoder network as emission to the observation space. Authors need to discuss how their work is compared to this work. Authors need to revise the text as there were a few grammatical and punctuation errors in the paper."	Data can be reproduced from publicly available sources, although the specific data for training is not specified.  Little detail is provided in the generation of the data. Source  code for the method is not provided. Not enough information is provided with regards to  validation.	The paper lacks in sharing details about implementation and network architecture and it's not open source. It uses a software to generate simulated data but generation parameters are not shared and the generated data is not shared. This will likely make it impossible to reproduce the results.	Authors will make the code available.	This paper would benefit from further improvements in validation. The outstanding question that all inverse ECG papers should answer is how  generalizable is this approach to different subjects. This is particularly sensitive  for data-driven approaches that require  training and  testing separately. More importantly, the authors should include noise of some form in the data.  Otherwise, the results will always be good, but not realistic. I would encourage exploring the ECGI database in  EDGAR (https://edgar.sci.utah.edu/). As a minor comment, it would be useful to clarify what are the axis in Figure 4(b)	proofread and fix grammatical errors e.g. 'is verify' on 6th last line of abstract; 'those powerful' on 2nd last line of abstract; 'a imprecise' after eq 9; 'a precise results' after eq 9; 'to instead the' in conclusion; proofread and fix typo/formatting errors e.g. text after eq 1, text after eq 4 fig 4 right part is not clearly legible	See above.	Despite the lacking validation, the  method novelty is interesting  and worth discussing.	interesting method results shown on limited size simulated data without a commentary about its application on real data	Embedding the physiological model of TMP recovery inside the DL-based network makes this approach novel and interpretable. The only concern is that how this work is compared to a recent study with a similar approach.
388-Paper2184	Physiology-based simulation of the retinal vasculature enables annotation-free segmentation of OCT angiographs	This paper proposes a novel method for the synthesis of OCTA 3D tomographies with associated vascular segmentation ground truths, that are used to pre-train  segmentation networks over OCTA images. The experiments presents promising results.	Segmentation of vessels from OCTA images is a clinically relevant problem, but a large amount of publicly available datasets is not available to train a deep learning based segmentation method. The paper proposed a pipeline for physics-based simulation of OCTA images with corresponding ground truth labels for segmentation. Low amount of annotated data is a typical problem in medical imaging, this paper tries to address this issue by generating synthetic images. It also followed physics-based methods for augmenting datasets and introducing several artifacts generally introduced in OCTA image acquisition.	In this work, the authors present a method to generate highly realistic, synthetic OCTA images with intrinsically matched ground truth labels. To some extent, it solves the problem that deep learning methods need time-consuming and labor-intensive manual annotations to train blood vessel segmentation models on OCTA images. The quantitative and qualitative performance show that this method could be a versatile tool to advance OCTA analysis. In addition, they quantify the intrinsic scalability of the proposed approach and investigate how it can facilitate segmentation of the retinal vasculature in three-dimensional OCTA images.	The proposed formulation is novel, to the best of my knowledge. The preliminary results shows advantages in the use of the synthetic data for pretraining + refinement on real data, improving the performance of the 2D segmentation achieved with only training on real data. Preliminary results on 3D segmentation of real OCTA volumes using the synthetic data only. Ablation study.	The paper is very well written. The paper addresses an extremely important and clinically relevant medical imaging problem. The results are promising and authors did a lot of experiments.	The simulation of OCTA images and their corresponding labels are based on two novel components: a physiology-based simulation and a suite of physics-based image augmentations. The authors demonstrate the feasibility of the proposed method by successfully training several segmentation algorithms, providing an effective tool for solving manual annotation. It appears that the proposed method holds considerable promise for expansion beyond vessel segmentation and ultimately to advance the quantitative analysis of OCTA in clinical practice.	Lack of specification of the training details, and network architectures used.	he paper used one synthetic dataset but as it did not have ground truth annotation, the quantitative performance measure was not reported. This is a crucial information as the motivation of training only with synthetic examples can only be validated if the trained model performs similarly well in real experimental data.	The citation and explanation of some methods are not intuitive, which bring concerns to the reproducibility of the method. The description of the manipulation of 3D-level deformation of vessels is too general and needs more details. In the experimental part, the description of the evaluation dataset is vague.	"Despite github links have been provided for third party implementation of the networks and experimental settings, these details are not described in the paper. This is important, as repositories evolve, while reported results are fixed to a given version. Should the authors cite the paper (i.e. [16]), and ensure that the repository version used is preserving the settings reported, and otherwise tell the difference. Moreover, the cited paper/repository from Ma et al. IEEE TMI:40(3) 2020 [16], reports a fixed number of epochs with exponential learning rate decay (which requires a fixed number of epochs). Instead, the authors report that they ""introduce a validation split for model selection"", which indicates the use of an stopping criterion that is not specified. This is a relevant detail for reproducibility that is not sufficiently described."	The notes on reproducibility seems satisfactory	It is still necessary to further explain the implementation details, such as the selection of three-dimensional deformation-related parameters, node positioning when simulating a blood vessel tree, etc., to ensure the repeatable implementation of the method.	Overall, this is a novel and original paper. The proposal is interesting and the use of synthetic data for training demonstrates advantages on pre-training, as well as providing a way to train vascular segmentation on 3D OCTA. However, the description of the experimental setting is lacking details. Specifically, the reproducibility issue described above is important, in my opinion, because of the following reasons: Adding an stopping criteria significantly changes the settings with respect to those reported in [16]. The authors should clearly report what this criteria is, otherwise the amount of training is difficult to evaluate, and, as a more relevant issue, depending on the specific setting, some networks may have stopped way earlier than others, inducing biases in the comparison. Considering a fixed number of epochs, and comparing networks trained with different dataset sizes, induces a bias regarding a different level of overall training of the networks. This is because the number of network updates depends on the total number of minibatches, while the decaying learning rates (i.e. the strength of updates) depends on the epoch number (using the poly rule reported in [16]). Thus, larger datasets imply a larger number of updates with larger learning rates, i.e. more training. If the networks have not reached their full potential, the comparison may be biased. Related with this topic, there is a potential issue with the results reported in table 2. On the one hand, the trainings with 32 vs 320 vs 3200 synthetic datasets imply completely different training settings. While larger datasets could imply larger diversity, the question arises on what would happen if the 32 images where presented the exact same number of times, with the exact proportion of learning rates as the for the 3200 images dataset (i.e 3200 over 200 epochs vs 32 images over 20000 epochs with decaying learning rates accordingly). Moreover, the case of the synthetic + finetunning (320 images over 200 epochs + 32 images over 200 epochs) vs real (32 images over 200 epochs) may also be imbalanced wrt the refinement level.  This is something that worth look into, as it is not guaranteed that the networks have reached their maximum refinement (and potential of the data) when the training stopped, nor any note has been provided regarding this limitation, nor regarding any actions taken to prevent this potential bias in the comparison. It is important to explicitly report if augmentation was used, and their details. As a minor detail, the authors should add the cite number after Liu et al. in page 3 (i.e. [14]), and after Ma et al. in page 6 (i.e. [16]). Otherwise, the citing style is not coherent.	"This is overall well-wrritten and holds a lot of potentials to tackle an important clinically relevant problem. There are a few statements like ""Qualitatively, we find our segmentations to be superior (see figure 4). "" Can the authors comment on this and how generalized this superiority of results hold for different example images."	"It is necessary for the author to explain in detail the number of samples, size, sampling method, etc. contained in the dataset, which is helpful for other researchers to conduct repeated research. In section 3.1, the authors only stated that they extended a method by Schneider et al. However, for a vessel tree, how to determine the parent or child tree and how to select the root node, I think it is necessary to give an example to explain in detail, which is a key step in the simulation. In Section 3, the authors describe that they have performed image deformation to simulate the typical curved shape of the retina. However, the details (e.g., method and setting) of how you exactly perform the deformation are not given. I have doubts about the operation of three-dimensional deformation of blood vessels, including how to confirm the connection point of SVC and DVC, and whether this bending deformation conforms to the actual retinal anatomy, all of which need to be verified. By observing the examples shown in Fig. 3, I feel that there still have room for improving the model. The synthetic data is still different from the realistic data in terms of the noise level and vessel intensity distribution. Since the authors use the ROSE data for evaluation, it would be more intuitive to also compare with the OCTA-Net developed for ROSE data. In the introduction part and Fig. 1, the authors declare that their method is the ""proof-of-concept of 3D segmentation of OCTA images for the first time"". As far as I understood, this is an inaccurate statement. The authors should better review the literatures published on IEEE-TMI, IEEE-JBHI, MICCAI and ISBI for the methods developed for 3D OCTA segmentation and analysis."	Overall this paper is novel and with fair contributions. The weaknesses are lack of details on the specific trainings in the paper, which are moderate at most.	The paper addresses an important clinical problem, the physics-based simulation algorithm used here, is novel and the synthetic dataset performed similarly like real data. It can be a useful technique in the long run as data availability is an issue in medical imaging.	This paper presents ideas for OCTA simulation. There are still room for improving the model with better performance.
389-Paper0677	Point Beyond Class: A Benchmark for Weakly Semi-Supervised Abnormality Localization in Chest X-Rays	The paper follows the framework of Point DETR and tries to introduce it into the field of abnormality localization with chest X-rays. In my opinion, the main contribution of the paper lies in the proposal of the two regularization terms (multi-point consistency and symmetric consistency) with the Point DETR framework, which seem novel and are proved to be efficient.	In this paper, the author improves the existing abnormality localization pipeline with self-supervised learning. More specifically, they emphasize the importance of multi-point consistency and symmetric consistency to improve the model robustness.	Abnormality detection in chest X-rays (CXR) is notoriously hard as the boundaries of lesions in CXR images are not clear. Training a model with full supervision i.e. pixel annotation is the gold standard, but producing these masks is really time expensive. On the other hand, models trained using only image annotation perform poorly. A middle ground can be reached at limited time-cost by providing point based annotation for lesions. The authors propose a new regularization-based method to improve the results using the latter annotation and propose ablation studies to evaluate the impact of their regularization terms.	While the work focuses on applying Point DETR to abnormality localization with chest X-rays, the two regularization terms (multi-point consistency and symmetric consistency) proposed to improve the original framework seems interesting and novel. The multi-point consistency cleverly utilize the strongly labeled data to generate more point-labeled data and drives the model to generate the consistent bounding box from different point annotations inside the same abnormality.  The symmetric consistency adopts a self-supervision scheme and drives the model to generate consistent predictions under different transformations (flipping and masking).	The presentation is clear and easy to follow. The proposed solution is driven by the medical formulation, and the self-supervised learning loss make great sense. The authors have done detailed analysis on a dataset by changing the backbones and the children model. The released benchmark is likely applicable to other researchers in the same field.	The method. Point DETR is a promising method for weakly-supervised object detection (WSSOD) and to the best of my knowledge the authors propose the application of this method to CXR images.   The regularization. Most importantly, the authors propose some novelty in the form of stronger regularization for Point DETR. This regularization makes sense for the chosen application and improve the mean Average Precision.   The experiments. The method's evaluation is quite thorough as an ablation study is first performed before comparing their new method to the existing ones for WSSOD of CXR. This evaluation is notably performed on two publicly available datasets.	"The novelty is relatively limited. The work seems an application of the Point DETR framework into the field of Chest X-Rays. Although the two regularization terms seems novel and interesting, the work is an incremental improvement of the Point DETR framework since there may be numerous such kind of improvement based on the framework. Some details may need to be made clearer: (1) In the description of Step 3, it is said that ""After the above two steps, we get a well-trained Point DETR (Fd(*, *)), which is regarded as the teacher model to generate pseudo box labels for point-level weakly-annotated data."" However, I think the training stage of Point DETR is still needed to train the model to predict box labels from point labels. Please make clear. (2) It is said that for the student detector, two models (FCOS and Faster R-CNN) are adopted. So I wonder which model is used for the systems in Table 1, and why the results in Table 1 do not match with the results in Table 2."	I am not fully convinced by the effectiveness of the proposed model on a single dataset. If possible, enhancing the current results by another dataset will be extremely helpful. In Figure 2, it will be better if you could point out the key differences from the existing DETR baseline.	"My main critique is that while the overall paper is quite clear regarding the technical aspects, some of the authors claim need to be further refined or additional information needs to be stated: The starting point of the paper is Bearman et al.'s analysis, which was done on the PASCAL VOC and not CXR images. This should be stated. In the abstract, an improvement of ~5% mAP is claimed. This improvement should be nuanced, esp. with regard to which detector arch. / %age of labeled data was used, as the improvement can vary greatly. The authors' claim to have produced a ""publicly available benchmark"", while there is no mention that the code used will be made available. Only the training data is currently available.  "	The author did not mention the code in the paper, but according to the checklist, it seems they will release the code after acceptation. Please make clear. I suggest that at least release the splitting of the training set and the test set, which may help in pursuing the goal of setting a benchmark for the field.	Although the dataset is generally available, I do not see the code for replicting it. The result may be reproducible.	Although the code is not made public, the data used is publicly available. The author's new method is also clearly explained and the parameters for each experiment / augmentation are given. So while these exact results are not reproducible based the information present in the paper, I would expect that similar results could be reproduced when implementing from scratch.	I think the symmetric consistency has less to do with the point label and may be also used with full-labeled data.	Overall, I believe this is a good paper with clear motivation and reasonable design. It may be stronger if: the result is effective on at least another dataset and the code is released Improve the caption of Fig 2 for better presentation.	"Suggestions to answer my concerns: Clearly state that Bearman ar al.'s work was done on the PASCAL VOC dataset and that the data was not CXR images. Slightly nuance the mAP improvement claim in the abstract e.g. give the average improvement for each detector arch. or when using a certain %age of labeled data. Regarding the claim of having produced a ""publicly available benchmark"", either state that the code used to produce the results will be made public or change the claim to having produced a ""new method"".   Only a couple typos, good job: 4 Experiments, subsection ""Dataset"": this subsection should rather be called ""Datasets"" Caption Table 2: WSOD - WSSOD Overall ""chest x-rays"" appears often even after the acronym ""CXR"" has been defined. You can probably replace a few occurrences  "	The two regularization terms (multi-point consistency and symmetric consistency) proposed to improve the original framework seems interesting and novel, and experimental results proved the effectiveness. However, it seems an incremental improvement on an existing model, and some details need to be made clear.	Overall, I believe this is a good paper with clear motivation and reasonable design. It would help some of the researchers in this field with a new benchmark and a way for self-supervised learning.	"The authors claim to have produced a ""publicly available benchmark"" without publishing any code, which does not fit my description of such a benchmark. If this is because of the double-blinded review process, they should clearly state that. The authors should either publish this code or change their claim to ""a new method""."
390-Paper0008	Poisson2Sparse: Self-Supervised Poisson Denoising From a Single Image	This paper proposed a novel approach to denoise given noisy image. The considered approach  in this paper is  very relevant in the domain of medical image processing. Self supervised de-noising approach does not require ground truth image to learn the de-noising model. The objective is achieved by using sparse representation based approach. Here the dictionary, to obtain the sparse representation, is learned using convolution sparse coding network. A framework is worked to denoise image, where it is assumed the noise has poisson distribution. The experimental results show that the proposed approach is performing better in comparison with the existing approaches.	The paper has introduced a modification of the ISTA iterative optimisation dictionary learning algorithm which uses a recurrent neural network. The authors demonstrate this shows improved denoising performance compared to stat of the art self supervised methods.	This paper presents a self-supervised approach for single image denoising for Poisson corrupted images, which requires only one noisy image to generate the clean version. The method is extremely practical in situations where the acquisition of clean data can be difficult. Meanwhile, embedding deep neural networks into the framework of traditional iterative optimization methods provides a new inspiration for related research.	Use of convolution sparse coding network to denoise the  image by maximizing the likelihood function for poisson distributed noise	This paper leverages existing methods and combines them with neural networks. This is both interpretable and in this case may actually reduce the number of free parameters in the process. The challenge dealt with is about self-supervised learning, which is a well-known problem in biomedical imaging. The authors have demonstrated their results on a variety of problems, which are not limited to a single modality. The mathematics of the paper are very strong and equations are clear and easy to follow.	1) This paper presented a novel self-supervised approach to denoise biomedical images that follow the Poisson noise model. 2) The proposed method only needs a single noisy image which is desirable in many practical applications like biomedical imaging. 3) This work unrolls traditional iterative optimization methods into neural networks-based methods, which provides a new perspective of using modern deep learning technologies to deal with the traditional learning methods.	The solution  proposed in this work, is being studied in the literature.  The only novelty comes here is work out the solution for poisson noise.	This paper operates under the primary assumption that noise in medical images is Poisson distributed. This is incorrect, noise in the collected medical image projections is Poisson distributed, however, the reconstructed volumes no longer follow the Poisson distributed noise. The noise in reconstructed volumes is in fact very non-stationary and follows no distribution at all. The FMD dataset has no ground truth. The authors write that the ground truth is obtained by averaging. While qualitative results may be relevant, I don't think such a dataset should be considered for quantitative analysis. The paper is fairly light on experiments and is heavily theoretical. Figure 2 states that the ribs have been recovered. To my untrained eye, the ribs have not been recovered, they are little better than random noise. Table 1 shows the highest SSIM in the DIP for the FMD dataset. In Fig. 2 and Supplementary Fig. 3, The DIP image is visually the most blurry image, which usually implies a very low SSIM. In my opinion, the results have not been reported correctly.	1) The authors proposed to approximate traditional iterative optimization algorithms for image denoising with a recurrent neural network. However, the application of the recurrent neural network is not reflected well in the paper. 2) Some mathematical expressions are not standardized. For example, the coefficient of loss L_N in Eqn. (12), \mu_N, is inconsistent with that in training details. 3) The description of Fig. 1 is inconsistent with Eqn. (10), please check carefully. 4) The method overview in Fig. 1 is not described in sufficient detail, and does not correspond to the text well. 5) The visual results shown in Fig. 2 and Fig. 3 should be accompanied with quantitative evaluations such as PSNR and SSIM to compare different approaches better.	Convincing	The reproducibility is good. Datasets and code are available. Only possibility for major improvement is the inclusion of the pre-trained model.	Good reproducibility	1) The assumption of noise to be poisson distribution is very restrictive. It will be good to devise an approach where there is no assumption on noise. Or specifically , when the signal strengths is comparable to noise. 2) It will be good to observe the behaviour of alpha and check it susceptibility for different type of noise.	More experimental detail and better quality of data is needed. The optimization and denoising needs to take place in the projection/k-space domain, rather than the reconstructed volume domain. A comparison to the original ISTA optimiser needs to be shown, to demonstrate the superiority of the neural network based modification. The advantage of this method to standard supervised learning approaches needs to be explained and experiments need to be conducted to show it.	1) The method overview presented in Fig. 1 should be more clearly described to highlight the innovation of the proposed method. 2) Why the structure of autoencoders is described as a recurrent neural network requires further clarification. 3) In the experimental section, the format of table titles of Tab. 1 and Tab. 2 seems to be unconventional. 4) As the proposed method is an extension of iterative optimization methods, using deep neural networks, it would be better to compare it with the iterative ISTA method. When it comes to practical applications, computational efficiency should also be considered in the experiments.	The proposed approach is devised to use the DNN in the framework of analytical method	The scientific premise of the paper is incorrect. The experimental depth is insufficient. The paper results may be reported incorrectly. The data used is of questionable quality.	Good novelty, but not clear writing and illustrations.
391-Paper2552	Pose-based Tremor Classification for Parkinson's Disease Diagnosis from Video	This paper presents a method to classify Parkinson's tremors in videos. To this end, the authors propose an attention module with a pyramidal channel-squeezing-fusion architecture (Spatial Pyramidal Attention Parkinson's tremor classification Network, SPAPNet). The proposed system shows an accuracy of 90.9 %, which is 3.2% higher than ST-GCN [1]. [1] Yan, S., Xiong, Y., Lin, D.: Spatial temporal graph convolutional networks for skeleton-based action recognition. In: AAAI Conference on Artificial Intelligence. (2018).	The authors propose a strategy to classify different tremor classes using postural inputs and a graph neural representation (GNN). Also, the proposed architecture includes an attention mechanism to enhance relationships among joint distances. The authors validate the strategy over a public dataset with some tremor patients diagnosed with Parkinson's disease. The authors report promising results with around 90.9 % in the classification task of Parkinson's tremor vs Parkinson's no tremor.	The authors propose a binary classification (and an extension to multi-class) framework to diagnose Parkinson's disease in video recordings of subjects using the seven upper body joints of a 2D skeleton extracted with OpenPose as input to their framework. The body joints are used to build a graph with intra-skeleton and inter-frame connections which are fed to a GNN with Spatial Attention and a novel Pyramidal Channel-Squeezing Fusion Block. The presented results show that the proposed method consistently outperforms prior work.	1) The proposed network is explained in detail and in an easy-to-understand manner. 2) The reasons for the design choices are thoroughly explained.	The proposed methodology based on graph neural nets to represent postural configurations is promising and coherent. Also, the attention models proposed inside the strategy avoid the loss of weakly relationships among joints. Also, the authors show an interesting back propagation of probability output to support the explainability of results.	The solution is light weight (only seven 2D body joints as input), low-cost, and only needs videos of the subject to get an indicator about a potential Parkinson's diagnosis, therefore the system is very accessible, and the potential clinical impact is high. The authors present an extensive evaluation, including a comparison against the state-of-the-art and an ablation study and show that the proposed modules improve the model's performance. By inspecting the attention weights, the network performance and also failure cases can be interpreted.	1) The framework relies heavily on OpenPose performance. 2) It is hard to see why the system is an interpretable automatic PD diagnosis system. 3) The paper insists that using seven keypoints can be more beneficial than using all keypoints, but there is insufficient proof. 4) ST-GCN [1] was introduced in 2018, and it can be considered outdated.	The main limitation of the paper is related to the understanding of the clinical context of Parkinson's disease, and the related motor and no motor symptoms. From the title, the authors claim that the approach allows for early support diagnosis but tremors mainly appear in advanced stages. But also, there is not any validation that shows that the proposed approach deal with tiny tremors that appear at early or promodal stages. In fact, there exist resting and postural tremors, in the recorded dataset only use postural tremors that magnify tremors but introduce external tremors as a consequence of muscle and movement of particular activities. How does the proposed approach filter out Parkinsonian tremor??. Moreover, the authors omit scale index and protocols to stratify Parkinson, such as UPRDS y H&Y. In such cases, how the approach may assume that carried out early diagnosis?. In fact in the state-of-the-art, the authors also omit some works that amplify tremors from optical strategies, which also discover natural discrimination among different types of tremors. Regarding the methodological pipeline, the main drawback is the recovery of joint points from OpenPose which result highly noise with high frequencies among frames. How do filter such movements from real Parkinson's tremor?	It's hard to assess if the system really has clinical significance and if tremor classification in video recordings is enough to make a diagnosis - maybe it would be better to describe the system to be an indicator for further patient examination. The system heavily relies on the 2D joints computed by the OpenPose framework. A more thorough discussion (and comparison of different pose estimation frameworks and framework-specific advantages/disadvantages) would greatly improve the presented work.	Its reproducibility is very high. The method and parameter explanations are excellent, and the data-related parts and learning environment are also well explained. I look forward to the code release of this paper.	It is very difficult to reproduce the paper and achieved results from description of the manuscript. The authors use a subset and crop the videos but any of this information is reported.	According to the authors, the code will be made public and the dataset is publicly available.	It would be nice to see the final results when OpenPose shows wrong results or the results of applying other recent pose estimation approaches.	The authors should better identify the support that has the proposed tool and how this characterization may impact Parkinson following. Also, the authors should study the OPenPose outputs to determine the level of noise, similar to tremor.	The authors should already specify in the abstract which type of movements / posture the videos should contain - the method does not require any type of video. The authors should specify (or at least estimate) the detection accuracy of the OpenPose 2D human pose detection framework. Noise in the predictions of the 2D joint locations could be interpreted as tremor by the proposed framework. The authors should discuss how this issue could be addressed. The authors should specify the exact inputs and outputs (size and format) of the proposed model for reproducibility. The authors should discuss the multi-class classification performance in more detail.	The manuscript is clear and easy to follow, and the results are plausible.	The main limitation of the paper is related to the understanding of the clinical context of Parkinson's disease, and the related motor and no motor symptoms. The work is not useful for early diagnosis.	The paper presents a novel, low-cost and computationally efficient approach for a problem with great clinical relevance.
392-Paper0672	Position-prior Clustering-based Self-attention Module for Knee Cartilage Segmentation	In this work, the authors propose a self-attention module based on a computational process on class centers, and they call it as the position-prior clustering-based self-attention module (PCAM). The PCAM is a plug-in module, which could be integrated into an up-sampling layer of the decoding half in a UNet/VNet-like network structure. From the experimental part, the proposed method could achieve the best overall results by comparing with other existing approaches.	This paper proposes a position-prior clustering-based self-attention module (PCAM) for automatic knee cartilage segmentation in MR images:  1). lightweight PCAM that can be plugged in to networks 2). Application of clustering-based self-attention module on knee cartilage segmentation. 3). The proposed method outperforms State-of-The-Art methods.	The paper present a self-attention module with clustering to improve the possible discontinuous segmentations of cartilage from knee MR images. Proposed module can be added to different upsampling layers of U-net type segmentation models.	Based on some experimental results and some related work that this paper referred, I think the PCAM is a flexible plug-in module, and could be treated and utilized as a self-attention module to strengthen the relative features for the segmentation targets in certain layer. Although the authors only apply the PCAM for the knee cartilages segmentation problem, I think this module could be further used for other organ/tissue segmentation. The reproducibility of the PCAM is not difficult.	The proposed position-prior and clustering-based attention module is technically novel. The paper is well-structured. Validation is thorough. Comparisons between the proposed method and previous methods are provided.	PCAM is a novel formulation that can be added to an available DL model to improve knee cartilage segmentations. The evaluation of the approach was done by comparing the approach with the current state-of-the art segmentation methods.	"The continuity of segmented results may negatively affect the diagnosis of knee cartilage diseases. For example, the cartilage defects in [2]. A higher continuity in the segmented outputs may fetch up or conceal the defect situations in cartilages, which may cause diagnostic errors. Thus, I don't think the key contribution (i.e., higher continuity) that the proposed method claimed is completely matched to the medical scenario on knee cartilages. The proposed module has a high similarity to the ""Class Center"" step in [10], which seriously reduces the novelty of this paper. And the author does not use lots of contents to do method-level comparisons with this highly relevant method. Although the coarse-to-fine approaches ([3, 4]) and the ROI-fusion approach ([5]) did the knee cartilage segmentation, their goal is to increase the resolution of processed data while overcoming the limited GPU memory resources. Yet your goal is to propose a class-center based self-attention module and add it to a baseline model to increase some performance (e.g., the continuity). Adding your module will increase the memory cost of any baseline. Thus, comparing with these coarse-to-fine/ROI-fusion approaches may not directly demonstrate the superiority of your method. 3D visual comparisons could clearly show what your method improved in detail, yet I only saw some quantitative results."	N/A	The authors included Ref. 9 in the paper for dilated convolutions. Adding an approach that uses dilated convolutions to the comparative study will improve the quality of the paper.	The reproducibility of the PCAM is not difficult.	The code is not provided and the reproducibility is questionable.	Please include a link to the code within the paper.	"I think the proposed method is a flexible module, and I think it should be applied for more organ extraction problems (including the ""health"" knee data). The proposed method is an improved/modified self-attention idea, and the authors should focus on the comparisons to the self-attention or attention-based approaches. E.g., describing more about the improvement to [10] or its follow-up papers, or comparing with some different attention-based papers and their follow-ups (Attention U-Net: Learning Where to Look for the Pancreas). Adding 3D visual comparisons for 3D segmentation. I have a concern. The experimental data has a very large size in 3D, and the GPU has 11-GB memory, how did you implement your method on a 3D network with 4 down-sampling and 4 up-sampling layers, and also the batch size is 4? I suggest that if you used the Erode operations, you should clearly say it, and avoid some vague implementation descriptions in the sub-section ""Position-prior module""."	N/A	The use of PCAM clearly improves the performance of the cartilage segmentations. PCAM includes 3 submodules, as a future study, is it possible to experiment the effect of individual submodules to the performance gain?	If the authors emphasize that the proposed method is for a medical problem/scenario, then it must be shown that the method is highly matched to the problem (the methodology and experiment to support the method to match the problem).	Technical novel; Outperform the previous methods.	The paper presents an add-on module that could be used to improve global connectivity of segmentations on different tissues. The approach is flexible to be incorporated into any encoder decoder -based segmentation models.
393-Paper0764	Predicting molecular traits from tissue morphology through self-interactive multi-instance learning	In order to bridge the divide of usually two separate steps, i.e. tile embedding and feature integration, the paper proposes an alternative optimization method to fine-tune the CNN encoder and to learn attention pooling. The CNN encoder is fine-tuned on three sets of tiles from the decomposition of WSI tiles into (1) attention, (2) supplementary, and (3) negative tiles with respect to their attention scores.	The authors propose a self-interactive multi-instance learning framework for predicting molecular trait. Specifically, the backbone and aggregation network are optimized alternately for fine-grained and global feature, respectively, where an instance selection strategy and adversarial optimisation are further proposed. The authors validate their methods on multiple genetic and molecular analyses and achieve promising results.	The authors of this paper present a method for the prediction of molecular traits or biological types from WSIs. In particular, they present a multi-stage method that iteratively fine-tunes a WSI-tile attention module and a feature extraction one. The authors present results on four classification endpoints and report better AUC scores to a number of competing methodologies from the literature.	The proposed method achieves the best performance (AUC score) on all four benchmarking datasets; the paper is well-written and the analysis is performed thoroughly.	The motivation of this paper is clear and reasonable. The paper is well-organized and the method is clearly introduced and demonstrated by figure 2.	The paper is very well written, easy to follow with and with a clear presentation of results. I find the multi-stage framework for both feature and endpoint learning quite interesting and well motivated considering the current literature on WSI classification.	"One weakness of the paper is that the proposed method mainly focuses on attention pooling/integration from tiles, and thus restricts itself to binary classification tasks.  Binary classification is a relatively easy task compared with instance segmentation. While instance, e.g. tumor, detection and segmentation is a crucial step towards WSI classification, this method may not be used to improve instance segmentation for WSI. Again, in Table 1, only the AUC score is reported for the classification task. What is the accuracy of each method on each dataset? Fig. 2 is confusing. What does ""subtype and non-subtype"" mean in this figure? Should it be e.g. tumor and non-tumor?"	Lack of ablation experiments demonstrating the effectiveness of three kinds of instance selected. Lack of experiments for hyper-parameter sensitivity, e.g., the number of attention tiles, supplementary tiles and low-attention tiles.	There are some weaknesses in the experimental configuration that need clarification (refer to details comments).	"On page 6, it states that ""more details are available in the (temporarily anonymous) source code"", but its URL is missing."	Good.	Publicly available datasets. Code release promise. Clear description of they utilized hyper-parameters.	"On page 3: ""...we use k representative tiles with high attention scores to fine-tune CNN encoder f_res..."": It is not sufficiently clear how this f_res is fine-tuned, since it is pre-trained on ImageNet. It would be better to also give a brief description here. On page 4: k^1 and k^2 are better to be k_1 and k_2."	One question about dataset: why spliting training and test sets can alleviate the small sample size problem? More experiments: (a)  Quantitative ablation experiments for investigating the different parts of selected instances (i.e. the attention tails, supplementary tails and negative tails) are necessary. (b) Analysis of hyper-parameter sensitivity, e.g., the (defined/sampled) number of three kinds of tiles, the weight of adversial training and the chosen of L_final/L_init. (c) Since selected instances are further used in fine-tuning the backbone+fc, one can directly use the output of fc for instance selection, which seem more reasonable than attention as it is unconstrained. Visuliazation:  (a) T-sne: Instead of increasing the number of top tiles, it would be help to visulize the lowest-attention ones, as this is the difference between inter and adInter training. (c) As attention score is generated after softmax, it would be help to clarify how blue-green-yellow-red/blue-white-red colour map are defined.	"I am concerned with the statement: ""The training and test sets are generated using bootstrapping for 10 folds  ..."", to my understanding this scheme would let the same samples to fall in both training and testing sets, since bootstrapping is performing sampling with replacement. In this case, there will be some bias in the presented results. The authors state that they did not utilize a validation set in order to select their models and instead they just utilize the last snapshot of the model for testing. This is quite tricky since in fact there is no certainty that the different models converge in a similar manner or even that their convergence is stable. Hence, it could be the case that a competing method would reach similar performance with the proposed with a proper train/val scheme. I believe that AUC can be quite cryptic in terms of classification performance, considering also the fact that it is quite low in some cases and there is the extra concern of data leakage to the test set. I would suggest the authors to complement it with additional metrics like BACC, F1, Sens, Spec as well as the ROC curves."	An interesting paper but contains some flaws.	The method is well-motivated and the experimental results are convincing.	It is mostly based on some weak points in the experimental configuration. Even though the overall experimental structure can be sufficient, there are a number of points that need clarification before acceptance.
394-Paper1668	Predicting Spatio-Temporal Human Brain Response Using fMRI	This paper proposed a method to predict high resolution brain response in space and time using fMRI data.	In this paper, a recurrent memory optimization method for predicting the behavioral state of spatio-temporal brain activity is proposed. The proposed method uses Optimal Polynomial Projections to capture the long temporal history with robust online compression, and predicted the recurrent brain states through a Siamese network based on fMRI data and MEG data in the training phase. During the testing phase, using only fMRI data to predict the spatiotemporal corresponding neural response of each voxel within the brain, millisecond and millimeter-scale brain responses were predicted.	This work proposed a novel framework to predict the brain response with high spatial and temporal resolution only based on fMRI data. In the training stage, the proposed method uses Optimal Polynomial Projections with robust online compression to take the fMRI and MEG data as inputs and uses a Siamese network to predict the brain state. In the testing stage, only the fMRI data is used.	The paper proposed a novel framework based on Siamese network to predict high temporal and spatial resolution brain activities using fMRI data. This framework is interesting since current brain observation techniques have limitations in either spatial or temporal resolution. The current study adopts Siamese network to establish the relationship between cross-modal network prediction and original brain activity time series, and uses Polynomial projection operators to overcome the problem of gradient vanishing when long time series are taken into consideration.	This paper proposes a general framework for discretizing time points and projecting them onto a polynomial basis by a novel recurrent memory optimization method, which connects the mapping between brain regions and time points. This method enables high-resolution temporal signal prediction for each voxel of the brain using only fMRI data. The method can simultaneously consider the spatial and temporal aspects of neural activity.	a) Modeling the brain dynamics is an interesting and important problem. b) The proposed method can predict the brain state with both high spatial/temporal resolution. c) A feasible approach for dealing with gradient vanishing in modeling MEG data with RNN.	Some technical details are missing. It is not clear how the fMRI graph and MEG graph in Figure 1 were generated; It is also not clear what is the role of the connectivity matrix in model training. As consequences, the theoretical soundness and the reproducibility of the study could be contaminated, and it is a bit confusing when matching Figure 1 with its descriptions in the main text. It seems the model was trained at ROI-level as the time series within a brain atlas ROI were averaged before input to the model. Considering the functional heterogeneity of the cortex, this average operation could largely degenerate the specificity of brain activities. I have a sense that this average operation is related to high dimensionality of the fMRI and MEG data, but how would this operation affect the prediction performance of the proposed model? The authors are encouraged to justify this issue explicitly if my understanding is correct. The authors provided qualitative and quantitative evaluations of how well high-resolution fMRI signals can be predicted. Besides Table 1, the authors are encouraged to provided more details about the performance difference among different brain ROIs. If possible, they are also encouraged to provide fMRI time series prediction for a single voxel rather than averaging over ROI.	This paper compares the proposed method with only two baselines, LSTM and GRU-D, with few comparisons.	a) It's better to include more experiments or discussions about how behavioral representations evolve with the proposed model. I think this is the major point of the paper, however, the experiments only compared the spatial and temporal patterns from fMRI and MEG, respectively. b) Only 3 brain networks are shown ion Section 3.1. It's better to show more networks in Fig.2. Meanwhile, the quantitative measurements should be included for a comprehensive evaluation. c) In Section 3.2, MSE may not truly reflect the similarity between the prediction and ground truth. It's better to include more measurements such as PCC. In addition, it is not clear the similarity is averaged over all subjects? Or the time series is firstly averaged and then similarity is computed.	The reproducibility of the paper can be improved after the author completing the missing technical details.	Reasonable	I think this work is reproducible.	Please see the comments on the main weakness of the paper.	This paper can further improve the comparative experiment.	a) The details of how LSTM and GRU-D are implemented for comparison are missing. At least, it should be included in the supplemental material. b) It's better to include more visualizations in Fig. 2 and Fig. 3. For example, the time series in Fig. 3 is from one of visual networks. What about the other ROIs? c) I think the prediction performance varies from region to region. In which ROI, the proposed model performs worse? It's interesting to exploring the performance degeneration.  d) Does the number of parcels have an effect on the model's performance? e) What is the limitations of the proposed model? f) Correct the grammar errors.	The study is interesting. The framework is novel. However, some technical details are missing. The experimental results are relatively not sufficient.	This paper has a rich theoretical derivation process, and has a more detailed explanation for the model proposed in this paper, which has strong persuasive force.	This work proposed a novel approach in modeling the brain dynamics. The results seems promising, and it can be potentially applied for super resolution of fMRI data. The experiments and discussions may not be suffient.
395-Paper1700	Privacy Preserving Image Registration	The authors describe a distributed image registration, where two parties provide a fixed an moving image, without wanting to share their actual image data; they show that some established image registration algorithms can be approximated using full encryption, with significant performance loss, but comparable accuracy.	The paper shows how to perform image registration while preserving privacy using cryptographic tools.	The authors present a privacy preserving image registration algorithm using cryptographic tools such as secure multi-party computation and homomorphic encryption.	This is a pretty cool idea! In light of all the hype around federated learning, it is quite interesting to see a work that addresses the low-level implementation details of image registration algorithms using encryption.	the problem of preserving privacy is pratically valid	The main strengths of the paper lies in the fact that the authors have used cryptographic tools to develop an image registration algorithm. The paper is well written and easy to follow.	It is not straightforward to come up with a real-world use case of this, outside of research/academia (but then after all, this is an academic conference), this could be motivated better in the manuscript. Tailoring the image registration algorithms to suit the encryption is done by classical steps one would do on a very slow or old computer, i.e. sub-sample with smart schemes as much as possible without compromising quality - there is not much novelty there.	The contribution is not clear. Like, is the cryptographic tools are adapted for the registration task? Not sure why the evaluation metric would concern about the image similarity. Is there any difference for the results among CLEAR and other methods? Or the encrypting way will make the decoded image different with the original one? What is other baseline methods? In reviewer's view, such proposed framework is use the cryptographic tools to perform registration task. And why not apply the framework to other tasks, i.e., segmentation?	The main weakness of this paper lies in the evaluation and time effectiveness. The authors motivation to develop a PPIR algorithm is weak.	Good enough, a bit more details about the software implementation framework & programming languages could be mentioned.	The reproducibility should be good.	The authors have indicated the source code will not be available (not aplicable). I am not very sure regarding the reproducibility of the paper.	Regarding the motivation of the work, the authors could make a bit more effort to come up with a realistic use case. The SSD metric itself is mostly suited for intra-patient registration, and this would pretty never require your PPIR. For inter-patient, atlas- or multi-modal scenarios on the other hand, this might become relevant, mostly in academic & research scenarios similar to federated learning, i.e. where sophisticated registration-based anatomical atlases can be added to, with data from multiple privacy-preserving sources. The limitation of simple SSD metrics & gradients might be overcome to do something like pre-processing on each party's computer (self-similarity, a modality synthesis GAN etc.). Please improve the presentation of the results a bit. Show the computation times in total (not per iteration), also showing the time of the original method (it won't be 0.0 seconds as in the table now), to allow to estimate the relative performance loss. Maybe you can also print the required network bandwidth for the original scenario, i.e. if one were to send the pixel data for each iteration in an unencrypted fashion. Can you come up with anything more innovative than subsampling? Are there specific mathematical properties of the encryption methods that lend themselves to more specific algorithm changes during the registration? If the overall description of the MPC and FHE approaches can be further improved for people not familiar with such encryption methods, this becomes a really nice manuscript that bridges the gap between two otherwise quite disconnected technical domains.	Authors are suggested to apply for other tasks, i.e., segmentation, detection.	The authors should provide clear evidence motivating the proposed algorithm.	All described above already.	The problem is pratically valid with little discussion.	The work is interesting, but the authors do not show evidence of the need to develop such an algorithm.
396-Paper0683	ProCo: Prototype-aware Contrastive Learning for Long-tailed Medical Image Classification	This paper studies the long-tailed imbalance problem in medical image classification. Typically, the authors adopt contrastive learning to tackle such an imbalance problem. The category prototype and adversarial proto-instance are used for generating representative contrastive pairs with the prototype recalibration strategy. The authors apply such a learning scheme in highly imbalanced data for medical image classification. Experiments vandalized their claims on different datasets.	The paper proposes a prototype-based contrastive learning framework  for long-tailed medical image classification. A mixup-style sythesis is adopted to generate adversarial instance, a prototype recalibration strategy is proposed, and a proto-loss is proposed. The reported numbers show outperforming performance over baselines.	To address the long-tailed dataset problem, the authors propose a novel end-to-end framework using prototype learning and contrastive learning, namely prototype-aware contrastive learning (ProCo).  Specifically, adversial proto-instance is generated from the combination of learnable category prototype and feature of representative instance to enhance the robustness of contrastive learning over all classes on the long-tailed dataset. A prototype recalibration strategy is adopted to alleviate the prototype bias. Two long-tailed medical datasets are adopted to evaluate the proposed framework, and the experimental results support the effectiveness of ProCo.	Long-tailed medical image classification is studied in this paper, and the authors make an investigation on this task. Contrastive learning and prototype learning are jointly considered in the overall learning framework. A recalibration idea is given to make the alignment. Experiments show the effectiveness of the proposed method on different datasets.	The paper addresses an important problem in practical medical image classification, class imbalance. The motivation is good. The experiments show promising results.	The proposed framework, ProCo, is a novel contrastive learning method for the long-tailed medical classification problem. By introducing synthesized adversial proto-instance into the contrastive learning, ProCo can encourage the network to rectify the decision boundaries of the tailed categories. A prototype recalibration strategy is also proposed to address the prototype bias problem during training. Sufficient experiments are conducted on two public long-tailed medical datasets (ISIC2018 and APTOS2019). Accuracy and F1-score are reported on these two datasets, and the experimental results are consistent with the conclusions. Ablation studies were performed on the three proposed modules. The results show that each component contributes to improvement.	The main contributions of this work are built on the contrastive learning and prototype learning. For the widely-studied image classification task, they are well testified on different vision-based learning schemes. By contrast, such a learning model is not extensively studied in the medical field. These are the advantages and also the disadvantages. For the concept of category prototype, this has been widely used in few-shot learning or detection or segmentation, or others. This is not a new concept. For recalibration, the reviewer may doubt the effect is very little to the whole learning scheme. The motivation of this work is also less attractive. Technically, the proposed method can be adaptive to any stations, rather than the specific long-tailed situation. The experiments are less sufficient in the present form.	I think the paper is not in a complete form. There are many missing details to fully understand the contributions. The problem of interest is long-tailed classification, while there is no discussion on the inference phase in Sec. 3 at all. For a technical presentation, I think it is good to present a complete pipeline. The idea of sythesizing hard examples for contrastive learning has been proposed before. I think more dicussion and analysis for Sec. 3.1 are needed. A good example could be found in [1]. What's the intuition behind Eq. 4? I think Sec. 3.2 could also be expanded. Similarly, Sec. 3.3, as a major contribution, should be expended. I am a bit confused: how do you train the classifier? Based my understanding, F and G are only projectors, right? [1] Hard Negative Mixing for Contrastive Learning, NIPS 2020	The test accuracy of each category is not presented. Because this work is proposed to address the long-tailed problem, a confusion matrix or a table containing the test accuracy of each category is a better way to reflect the effectiveness of the proposed method. Improvements in tailed categories are expected. The number of instances to compose adversarial proto-instances and the random interpolation coefficient are important hyperparameters in ProCo. However, they are not specified in the paper, and no sensitivity analysis or selection criteria are provided.	The overall learning scheme is clear but it is hard to reproduce, and the review highly suggest the authors release their codes.	I think the paper could be reproduced but I have no access to source code.	The proposed method is evaluated using two publicly available datasets, and most of the hyperparameters are included. Authors claimed in the Reproducibility Response that they will release code once the paper is accepted, but no reference to that is included in the text.	Please see the weakness.	Small Issues: In the title, I think Medical Classification should be Medical Image Classification, unless there are additional experiments presented (e.g. clinical text, audio, video).	Why the calibration factor reflects the difficulty of each category? Why add the factor with the prototype? Please provide more details or proper references. Kappa metric is a popular metric for an imbalance dataset. The author should consider reporting this metric. As stated before, the hyperparameters gamma and E are important for ProCo. The authors should put more words on them.	The authors aim to tackle the long-tailed medical classification task by using prototype learning and contrastive learning. Experiments seem good in the present form.	I don't think the paper is in a complete form. But I believe the future version could be improved.	The proposed ProCo is a novel method to address the long-tailed problem. The paper is well written, well organized, and sufficient experiments are conducted. There are no major weaknesses in this work. The sensitivity analysis on hyperparameters might increase the importance of the method.
397-Paper1110	Prognostic Imaging Biomarker Discovery in Survival Analysis for Idiopathic Pulmonary Fibrosis	The paper proposed a framework for prognostic imaging biomarker discovery and survival analysis based on contrastive learning and ViT, and exemplified its application in IPF.	The document aims to derive a method for survival prediction from lung CT scans. Patch representations are learnt by a modified contrastive learning method. Next, these patch representations are clustered using spherical L-Means. The final survival prediction is made by a clustering Vision Transformer (ViT) using the patch representations and their cluster assignments.	The authors propose a two-stage approach to predict survival of CT images from patients with idiopathic pulmonary fibrosis. In the first stage, the authors learn descriptors of image patches via self-supervised learning. In the second stage, the authors group patch-descriptors via K-means and and pass that information to a ViT to predict survival risk scores. Evaluation is performed based on internal cross-validation and a separate hold-out dataset.	the proposed framework could detect novel biomarker, which is very useful to guide the radiologist.	I believe this paper to be novel not only in the proposed method (that combines contrastive learning, spherical k-Means clustering, ViT clustering, etc), but also by the fact that using the proposed method a novel biomarker was found.	The proposed approach is interesting, because it offers some degree of explainability (by relying on image patches) and lowers the complexity of ViT by assigning image patches to clusters (via K-means). Evaluation on hold-out data indicates a strong improvement of the proposed approach over 3D ResNet approaches.	None	The main weakness of this work is the comparison with rather old methods (3D ResNet-18 and 3D ResNet-34).	The proposed approach is only compared against two 3D ResNet models. Other baselines would be helpful. A regular ViT trained end-to-end for survival prediction would help to judge the benefit of the proposed clustering scheme. A shallow model (e.g. Random Survival Forest) based on texture features (e.g. radiomics) or features extracted by ResNet from step 1 would help to justify the two-stage approach. Many hyper-parameters (patch size, number of patches, number of clusters, size of latent representation, ...), but it is unclear how the sensitive the proposed framework is their choice. In particular, the number of clusters K seems to be critical as it seems offer trade-off between expressiveness and complexity of the ViT.	very good	The authors claim that the code will be released with publication. I thus believe the work to be reproducible.	Reproducibility seems fair. Datasets used in the study seem to be not publicly available.	The only minor point is that the physiological meaning of the  novel C36 biomarker should give more explanations.	I would like to congratulate the authors for their impressive work. My only comment is that comparison should be performed both with standard methods and with other recent techniques.	"The proposed framework follows a two-stage procedure. In stage 1, self-supervised is used to train a ResNet to extract features from image patches. In stage 2, patch-descriptors are clustered and a ViT is trained to predict risk scores of survival. Overall, the proposed approach is interesting and combines interesting ideas from unsupervised deep learning and transformers to predict survival of patients with idiopathic pulmonary fibrosis. However, there are some issues with the paper that should be addressed. Major issues: Additional baseline should be included in the experiments (see above). How sensitive is the proposed approach the various hyper-parameters, in particular the number of clusters K? CT images are 3D images, yet, image patches seem to be 2D. In which plane are the patches, and how is the third dimension treated? In section 2.2, the authors mention sequence length N, but it is unclear how it relates to the patch-descriptors computed in section 2.1. Does N correspond to the number of extracted patches? The experiments seem to indicate otherwise, which does raise the question how exactly this sequence is defined. In section 2.2, the authors write that ""queries within the same cluster can be represented by a prototype"". Please clarify what query and prototype are? Are queries patch-descriptors (from step 1) and the prototype the cluster centroid (from K-means)? In section 2.3, the authors discuss how novel biomarkers can be discovered, however important details are missing? First, how are ""existing biomarkers"" defined? What is the measure of correlation? What does ""relatively far"" exactly mean? How is the p-value to measure ""predictive of mortality"" computed, and has multiple testing be considered? The ablation study in table 2 mentions two entries, which are not sufficiently explained. ""w/o contrastive learning"": How is the ResNet from step 1 trained in this setting? ""w/o attention pooling"": How are per-patient predictions formed in this model? In table 1, how can the proposed model (ResNet-18 and ViT) have less parameters than the ResNet-18 model? Minor issues: Please provide more details about the datasets, in particular about the follow-up period and the amount of censoring. A Kaplan-Meier curve would be helpful. At which time points was the IBS evaluated? Please add the Kaplan-Meier curve as a lower-bound of the IBS to table 1. The Cox-loss has only been re-discovered by ref. 18, but was originally proposed by Faraggi D, Simon R. A neural network model for survival data. Stat Med 1995, which should be the preferred citation."	this is good paper in every aspect, which can be potentially extended to broader applications for different diseases and image modalities.	This is a novel work with a clear and interesting application. Methodology is sound and statistical significance has been computed. I thus recommend this work for acceptance.	The paper proposed an interesting framework and the empirical evaluation suggests that it is effective, however additional baselines in the experiments and a justification of the selected hyper-parameters would strengthen the paper.
398-Paper0362	Progression models for imaging data with Longitudinal Variational Auto Encoders	The authors proposed to endow the latent space of a VAE with a linear mixed-effect longitudinal model to generate MRI or PET images of elderlies that progress with time. The latent representation, including patient onset time, acceleration factor, and individual space shifting, can be applied to characterize Alzheimer's disease progression.	This paper proposes a method for the generation of images following a disease progression model. The method combines a variational autoencoder with a temporal linear mixed-effect model that allows learning from the data a latent representation that is able to disentangle age from disease effects in image generation. The ability of the method to correctly generate the images has been demonstrated in a simulated experiment. In addition, the authors show how the method is able to generate images with well known patterns in Alzheimer's Disease progression from MRI and PET data.	The submission works on longitudinal image regression by integrating the variational autoencoder with mixed-effects model in the latent space. The method is evaluated on a synthetic dataset and the ADNI dataset.	provided a progression model that disentangles temporal changes from changes due to inter-patients variability, and allows sampling patients' trajectories at any time point, to infer missing data or predict future progression; proceeded to dimension reduction using a convolutional VAE with the added constraint that latent representations must comply with the structure of a generative statistical model of the trajectories; demonstrated this method on a synthetic data set and on both MRI and PET scans from the Alzheimer's Disease Neuroimaging Initiative (ADNI), recovering known patterns in normal or pathological brain aging.	The paper successfully addresses the difficult problem of generating disease progression models from images. The solution is original and very innovative. It successfully combines the finding of a latent space representation with the effects of disease evolution in a very original way. The simulated experiments are smartly selected and they show a proof of concept of the ability of the method to generate appropriate diseased image models. Indeed, the experiments in MRI and PET data show that the method is able to provide patterns typically seen in the real images.	Although the idea of mixing VAE with Euclidean regression models is not new, the proposed method is a good try to futher move this direction forward. The results on the synthetic data domenstrates the effectiveness of the proposed method to some extent.	The authors proposed a novel way to synthesize images of AD patients, and provided numerical estimations of disease progression for each individual. I like how it combines linear mixed effect model and a generative model, but additional analysis of the performance of the model can be provided to support the model. For example, the authors can generate different average images for different disease stages (NC, MCI, and AD), and try to find the difference between modalities. They can also analysis different onset time and acceleration factors of disease stages, and apply statistical tests to see whether the difference is significant. Sometimes it's hard to evaluate the results with generated images for a random subject, so quantitative evaluation may be helpful in this scenario.	In my opinion, the method has not apparent weaknesses. To say something, I would like to read something about the use of interpretability with the parameters estimated by the method in a clinical application.	"The main weakness of this paper is its negligence of existing tranditional methods on image regression that works on the same problem and misses the comparsoin to traditional methods and the existing deep learning based methods. For example, this sentence in the abract is misleading, ""few progression models for entire medical images have been proposed that allow missing data imputation and prediction at any timepoint."" In the past decade, a bunch of researchers work on image regression for longitudinal/cross-sectional medical images and many papers (just google regression on image time series or image regression) were published and achieved good performance in interpreting and prediting the entire images, images with even much higher resoultions compared to the ones used in this manuscript. The downsampled images used in the submission show missing details in many brain structures, which also indicates the high computational cost of the proposed method. This questions the motivation of this work. Since both the regression quality and computation cost are not improved by comparing to the traditional methods, what is the motivation for using this method? Just because it is a deep learning based method? Also, the mixed-effects model in the latent space is linear, how about nonlinear changes? Should it be limited to an age range that the evolution is almost linear? Otherwise, how to handle the nonlinear case, which is very often in brain degeneration or disease evaluation?"	The authors used public datasets in the experiment, and they claimed that the code will be made publicly available upon acceptance of the paper.	The datasets used for evaluation are publicly available, although the exact images used for training and testing are not provided. The authors mentioned that the codes will be available upon acceptance. Otherwise, I would not feel able to reproduce the paper from scratch with the information provided in the manuscript. The parameter values were provided. For the evaluation, the authors provided a clear description of metrics. Full memory footprint was not provided, although the size of the 3D images (80x96x80) makes me guess that the method is expensive.	The reader probably has difficulty to reproduce this work, releasing the source code could be a big help on the reproducibility of this work.	What disease stage groups are included in the training and test group, respectively, and why? How does input of different disease stage groups influence the result? I like the style and the amount of information delivered through Figure 1, but I think more contents can be added to it to make the model more clear. For example, the authors could add symbols besides the lines and arrows in the middle figure to make it clear. During training, what parameters have actual meaning and what parameters are from a Gaussian sampling? In algorithm 1, it would be good to elaborate the simulation and approximation part in the main text, or refer to some citations so that readers would know how this process is simulated. In the result section 3.2, the authors claimed that the minimum dimension to capture the dynamics of structural MRI is 16. Could you provide citation where this number comes from? In Figure 3, only the synthesized images are shown, and it doesn't seems to be clear enough for a T1 MRI image of the whole brain. Also the resolution (809680) does not seem high enough. Could the authors provide a sample original image in Figure 3 for comparison?	I don't have much to say. The paper is very well written, the problem challenging and interesting, and the solution smart and appealing.  I would like to read something about the use of interpretability with the parameters estimated by the method in a clinical application, maybe within the last line of the conclusion. I believe this is a great paper for Miccai.	Please check the weakness section. A thorough suvery of related work and the comparison to existing methods, both traditional and deep learning based methods are necessary for demonstrating the effectiveness of the proposed method in the work.	It is a good method to generate images along time and predict meaningful parameters that can represent disease progression at the same time.	My recommendation is based on the difficult of the problem and the smart solution proposed by the authors. I believe this is a strong paper suitable for publication in Miccai. I expect that the authors follow on with this interesting research and move into the use of the latent space parameterization for prognosis and interpretability.	A work has its merits but desiring more work to domenstrate or justify them.
399-Paper1523	Progressive Deep Segmentation of Coronary Artery via Hierarchical Topology Learning	-Proposal of novel segmentation framework of coronary artery region. -SAD that consider relationships between large and small (artery) anatomical structures for segmentation was proposed. -HTL that effectively models topological characteristics of artery structure was proposed.	This paper proposed a progressive learning-based framework, a spatial anatomical dependency module and a hierarchical topology learning module to realize the accurate coronary artery segmentation.	This paper proposes a two-stage coronary artery segmentation task framework. The framework consists of a spatial anatomical dependency module and a hierarchical topology learning module. The former provides rough spatial localization and introduces cardiac anatomical information, while the latter emphasizes topological information by using multi-task CNN networks, which helps to maintain thin vessel continuity.	-Anatomical structure relationships and topological features of segmentation target contribute to segment small targets. The proposed framework well combines this information with deep learning-based segmentation models. -The proposed method achieved better segmentation accuracy in the quantitative evaluation.	There are two main strengths of the paper: 1) This paper proposed a novel architecture that takes into account the dependency of vessel and chamber. 2) The HTL proposed in this paper improves the segmentation results.	Clear structure: The workflow of proposed framework is clear and detailed, which visually illustrates the main method flow. Valuable motivation: The paper addresses the optimization of practical details in coronary artery segmentation problems. Detailed method: It provides a very detailed description of method details, including formulas and diagrams. The method is reasonable in design and specific in description. Intuitive experiment: Different evaluation indexes are compared objectively with those of advanced methods. The ablation experiment also demonstrates the necessity of each network module.	-Adding evaluation results using public dataset is better.	1) For the SAD module proposed in the method, if the coarse mask is severely fractured or has a large number of over segmentation, is the distance map accurate enough to be used for subsequent training?  It is not mentioned in the discussion of the paper. Meanwhile, if the image is cropped, the chamber structure will be destroyed, can the distance map be used correctly? 2) HTL proposed in this paper is functionally more like an integrated learning or multi-task model, and there is no detailed description of how topological constraints are provided between tasks. 3) Inadequate experimental evaluation. This paper claimed to achieve the accurate segmentation, but the results did not show the segmentation performance for the regions with the stenosis. Meanwhile, this paper proposed the use of the topology, but the paper did not use OV, OF and other indicators to evaluate the continuity of main artery segmentation.	"There are some minor reference problems. Section 2.2 - Cube-Connectivity Prediction: <1> Qin, Y. , et al. ""AirwayNet: A Voxel-Connectivity Aware Approach for Accurate Airway Segmentation Using Convolutional Neural Networks."" 2019."	Reproducibility is OK. The authors will release codes and models.	The reproducibility of this paper is good, the framework described in this paper is very detailed and the algorithm is also very clear.	Although the code is not public, the authors list most of the details of the method in the paper. It should be straightforward to reproduce the results.	Adding evaluation results using public dataset will clarify the effectiveness of the proposed method.	"1) The evaluation does not properly analyse a vessel-level view as adopted by the scoring methodology used in the 2008 and 2012 MICCAI challenges. This evaluation is especially crucial as the correctness of the stenoses segmentation is much more important than the correctness of the ""healthy"" part of the vessel. Hence, the authors should make an effort to provide results on cases with stenosis and justify objectively. 2) The author should pay attention to the expression in the figure. The distance map proposed in the method is dependent on the surface of the chamber, but the figure shows the center of mass.  3) The author should analyze the specific reasons why the method brings improvement, rather than engineering the method and boasting the advantages"	"References are incomplete. The key points and cube-connectivity branches in HTL module come from existing algorithms in the field of computer vision, which are applied to coronary artery segmentation in the paper. Reference <1> has proposed a similar approach to cube-connectivity branches. The paper adds a channel on the basis of 26 neighborhood, but the main idea is similar . <1> Qin, Y. , et al. ""AirwayNet: A Voxel-Connectivity Aware Approach for Accurate Airway Segmentation Using Convolutional Neural Networks."" 2019. The experimental results in this paper are very intuitive, and it will be better if the following supplements can be made in the future. It is suggested to add some intermediate result diagrams in the supplementary material in the future to show the interpretability of each branch, such as distance factory diagram and key point diagram. According to the network design in this paper, it may be helpful for some clinical research fields that want to directly output the centerline or key points of coronary artery."	-Key factors to segment small segmentation target, including anatomical structure relationship and topological information, are integrated in the FCNs in the proposed framework. This is important work in segmentation of small anatomical structures. -Evaluation results prove effectiveness of the proposed method.	The two main reasons that influence my decision are: 1) the experiment in this paper is not sufficient to demonstrate the innovation points proposed by the author, and the experimental results did not show the segmentation quality for the lesion areas, which was insufficient to demonstrate the realization of accurate coronary segmentation. 2) the method proposed in this paper is not a very interesting innovation, and the use of the spatial dependency has been weakened.	The methodology is in general well described and evaluation has been done comprehensively with very good results.
400-Paper0603	Progressive Subsampling for Oversampled Data - Application to Quantitative MRI	The paper addresses the joint problem of sampling scheme optimization and signal reconstruction/q-space superresolution posed in the MUDI 2019 Challenge. The authors present a method improving upon and comparing with the challenge winner (SARDUNet). Similar to SARDUNet, they use two MLPs (one for subsampling and one for superresolution) and introduce two changes: 1. an improved iterative method to build the subsampling mask leaning on RFE and 2. a hyperparameter optimization scheme which (presumably) at its core increases the network capacity (i.e. the number of parameters). In a quantitative signal evaluation (MSE) the method achieves better results than the baseline. Additionally, several qualitative downstream analyzes reaffirm the improved reconstruction quality.	This work introduces an improved neural-network based method for solving the MUDI2019 challenge task of recovering a large set of volumes from different parameter combinations of diffusion MRI measurements from a smaller set of volumes.  Two neural networks are used in this method, where one is tasked with choosing the smaller set of measurements and another one is tasked with reconstructing the large set from the chosen set. The improvements over previous algorithms are in the training scheme especially for the network choosing the smaller set. Here, a exponential moving average is used to gradually select the smaller set and a learning schedule to gradually decrease the cardinality of the set. In addition the training is embedded in a neural architecture search component. The method is evaluated on the MUDI2019 challenge data against versions of the winner of this challenge.	The authors propose a method to recursively eliminate features to know the limits of subsampling	"The paper recognizes signal reconstruction is only a means to obtain downstream results and provides qualitative downstream results. The paper adopts the task framing from the 2019 MUDI challenge and compares from the winning baseline from that challenge. This work presents results clearly superior to the baselines, however the reason for the improvements is unclear (see weaknesses). The RFE idea seems to be a good idea, but it is unclear at this point. (I would assume this is connected to ""Alg.-line 8 is m et = max {m t - (e - E d )I e-E d * I iD (i), 0}"", see Table 2; Judging from Table 2, this would be the critical bit, but it remains a guess)"	The method is well described and the paper well written. The description of the method is very precise in algorithm 1. The building blocks for improvement over previous methods like the exponential moving average for sample selection and the neural architecture search are well motivated. I appreciate the effort making an anonymous version of the code available during review and the long-term usable links to the relevant websites.	Replacing a weighting scheme with an elimination of features is a more robust implementation approach Comparison with the SARDU-Net and related new versions is commendable and shows improved stability and performance	"The merits of this paper are difficult to analyze/understand between changing network capacity, interacting (because joint) mask and hyperparameter optimization, writing style, and evaluation metrics. Despite the additional Ablation Table in the Suppl. Mat.s (which is unclear), it is difficult to attribute the performance gains. Misleading performance numbers and unclear ""reason"" for performance. The difference in MSE between the SARDUNet paper [15] and here are unclear ([15] reports 5-8x better MSE in Table 2, making a better description of the evaluation mandatory). ""altering the second network's input across different batches, producing instability"" indicates a problem in the setup of SARDUNet (randomly changing network inputs are incompatible with a MLP). Across all tested architectures, the authors employ MLP architectures. MLPs scale extremely well for Diffusion SuperResolution, so increasing both the number of hidden layers and/or features (also units) are always beneficial (even beyond the depth of 4 used here). While obfuscated and ignored in the paper, the performance increase by more layers/features (=more parameters= network capacity) is - in my eyes - trivial. To clarify this, authors should 1. report the network size (parameters, maybe even FLOPS), and 2. Normalize different architectures to one network size (In contrast, finding the ""sweet spot"" for the ratio of hidden layers to features/units as an ablation/hyperparameter would be useful). As is, I expect the ""largest"" architecture (which coincidentally is larger than the baselines) to achieve the best performance (This seems to explain improvements for M <=50). Performance of SARDUNet-NAS. Since the SARDUNet hyperparameters are in the superset of the SARDUNet-NAS hyperparameters, the fact that SARDUNet-NAS does not consistently achieve the performance of SARDUNet implies the used hyperparameter search is not stable. Quantitative downstream performance metrics. Signal MSE can be misleading in light of the signal-inherent noise, so quantitative downstream analysis is required to verify the validity of predictions, e.g. FA, NODDI, fODF, etc. The results (table 1) seem to be an MSE across N=1344, which implies all 1344 DW signals are predicted despite M = {500, ..., 10} being in the input. For these M signals the optimal network behaviour is to return the ""noisy"" input value. This implies 1. performance numbers might be biased (the difference between M=500 and M=100 might be an overfit to those M noisy input signals) and 2. it is optimal to use high-noise DWIs as input to reduce the impact of the measurement noise (e.g. use high b-value measurements which typically are low SNR). As a consequence, I believe the direct comparison in Table 1 is invalid (this effect becomes more dominant with increasing M, maybe explains performance in M>50). [see also details] Is this paper employing NAS? This is incorrect terminology in my opinion. [see details]"	I am unsure how relevant this task is for clinical applications. I do understand, that being able to reconstruct all those measurements from a subset allows for a fast acquisition. But what is the value of having access to all those parameter variations? From my point of view, if one is able to accurately recover those additional measurements the information content of those additional measurements must be negligible. So why would I recover those? For applications calculating properties of the diffusion tensor this should hold true as well. The introduced improvements over previous algorithms are well motivated but the neural architecture search leaves the impression of being orthogonal to the work on this method. The architecture search would be expected to improve any method based on neural networks independent of the task. So I am unconvinced, whether it adds substantially to the state-of-the-art. If this is taken into account, this work leaves the impression of an incremental improvement over previous methods. Evaluation only considers variations of a method which previously won the MUDI2019 challenge based on neural networks. It is hard to judge the merits of these methods in absence of other methods.	It is important to discuss how this subsampling scheme might be used during real-world application (prospective deployment) towards an accelerated acquisition. Difference images in figure 2 will enable comprehensive comparison as well as SSIM values A visual interpretation during the iterative elimination of the features will be useful to enable explainable AI	"Run times. Run times are provided to some degree in a text file to the code, but I would expect a different format for the reporting: There run times are reported as time for a single cross-validation split, but with ""NAS"" (or hyperparameter optimization) employed, instead the TOTAL GPU hours/days should be reported, because that is what is required to achieve those results. I am fine with estimates, but this is insufficient. ""MRI signal prediction MSE"" is missing a definition or a reference to a definition that is very clear on what is compared."	Excellent. The authors even went to the lengths of making their code available to reviewers in a way respecting anonymity. Data is also available and the description of the method is detailed.	The reproducibility criteria has been met	"""network architecture hyperparameters e.g. number of layers and hidden units, is a task-dependent problem"" - NO. As long as your dataset is large enough, more layers etc. ALWAYS increases performance. This is not task-dependent. The balance between hidden layers and number of features is task dependent though. NAS terminology. This paper ""only"" performs a (smart) hyperparameter optimization (which also falls into the broad category of AutoML, which notably is also the terminology used by AutoKeras [20]). Both the supplementary materials and the code imply, that the following, critical NAS characteristics are missing: changes to the topology (e.g. skip connection), different choices for layers (see also NASNet, DARTS, such foundational papers are completely missing from the related work). This issue can be easily fixed by replacing neural architecture search (NAS) by Hyperparameter Optimization (HO) or AutoML. Evaluation. The framing of the challenge makes a clean evaluation difficult, as some biases are engrained in the challenge framing. As is, I would strongly recommend to exclude a subset of signals (for some direction + b-value) from the selection (1st) network manually, but include them in the second as outputs only and only report results on these ""calling them test signals"", otherwise your network is encouraged to recreate measurement noise. I understand this will be out of scope for a rebuttal. At a minimum, we would need to understand inhowfar the noise characteristics of the different signals differ. I.e. I believe, the direct comparison in Table 1 is invalid. Example for SOLUTION: New Table, which reports the performance grouped by b-value (as a proxy for SNR) and whether the ground truth signal was in the input or not. Table 1 does not have to list results for 8 different values of M (I'd be happy with 4 to make space for a second table, also M>=100 is in my opinion irrelevant for the superresolution aspect). Generally, the writing is often unclear/hard to follow/imprecise. To just understand the text, most of the paper has to be reread multiple times. To illustrate: ""PROSUB has an outer loop: steps t = 1, ..., T where we simultaneously perform NAS and RFE, choosing the measurements to remove via a score, averaged across the steps, whilst simultaneously updating the network architecture hyperparameters."" Long (check), convoluted (check), number of verbs (6 in one sentence) with parts that is not even a proper sentence (steps... where) ""PROSUB is not limited for subsampling MRI data sets"" -> ""PROSUB is not limited to subsampling MRI data sets"" ""We determine this by (i) by constructing"" -> ""We achieve this by (i) constructing"" ""Recursively over steps t = 1, ..., T RFE prunes the"" -> ""Recursively over steps t = 1, ..., T, RFE prunes the"""	This works appears as an excellent engineering paper, which sets some best-practice standards.  My biggest concern is with the importance of the task it sets out to solve. This concern may be adressed by explaining it clearer in a rebuttal and also adding explanatory sections in the paper. In addition, the evaluation leaves a very one-sided impression. All the comparison methods are variants of SARDU net. It would be more convincing if other methods were used. E.g. one simple comparison could be a simple linear model where each volume is predicted by a linear combination of the set of subsampled volumes. The set of volumes for the subsampling could be determined e.g. by random search. Alternatively I expect a look towards compression methods and dictionary learning methods should yield strong alternative baselines.	The authors have used a meaningful approach to solve the dual problem over a standard dataset, especially improving the performance of a previous winning submission and related versions. The authors are requested to look at the strengths and weaknesses section for further comments.	Misleading/unclear performance claims	I doubt the motivation of the task to be solved. If I could learn about the significance of the task or application of the method on a more obviously useful compression problem my opinion could change. However, I also highly doubt the evaluation. Since the task is not well studied, I believe only comparing it against versions of one related method leaves a high risk that many existing methods could perform similarly well.	The authors have demonstrated improvement over an award winning submission with a different approach that is easy to implement although recursive. The quantification of the results can be improved as in the comments.
401-Paper2710	Prostate Cancer Histology Synthesis using StyleGAN Latent Space Annotation	This work trained and tested StyleGAN2 on prostate histology dataset to generate new prostate cancer images. These images were draw from the GAN latent spaces, and the author demonstrate that the latent space learned by GAN can accurately disentangle and model prostate cancer features without exposure to labels in the training process.	A GAN network was trained and then the network accurately modelled Prostate Cancer features without exposure to labels in the training process.	This papers describes an experiment showing that the latent space of a deep generative model (StyleGAN2) can contain structures that reflect clinically relevant information (here, Gleason score grading). The model was trained in an unsupervised manner on image tiles (digital pathology). Random samples from the learned latent space were generated and annotated in a first round. From these landmarks, cluster regions in latent space are estimated via PCA. To validate the approach, the authors then generated samples from the latent clusters and let a pathologist annotate them. They found a considerable agreement in the respective grades (exact match or neighboring category).	(1) The paper is well written and easy to follow with.  (2) The authors asked independent pathologist to evaluate the generated images, who provides professional opinion from clinical view.  (3) The paper provides limitations of their work.	A novel approach to produce and evaluate learnt latent features in PCa; An evaluation by a pathologist with annotations; Challenging indication (PCa) and relevant problem (synthetic data generation and validation).	"The paper describes a very interesting approach The finding that there seems to be ""semantic"" structure in the latent space of an unsupervised generative model that can in principle be used for downstream classification opens some interesting directions for further research for digital pathology. For a first proof-of-principle study, I think the paper has some encouraging results."	The major weakness of this paper is the clinical application of this approach. Generating new pathological images using GAN is not new. Though this paper demonstrate that the learned latent space by StyleGAN2 is able to disentangle different prostate cancer features, but how we should use this technique and whether it is beneficial to the clinics is not clear to me. Some potential application might be: (1) use the generated images for some downstream tasks such as PCs classification or segmentation; however some previous studies show that this approach is not very effective; (2) treat this technique as an approach to anonymize patient's data; however the accuracy of the generated images labeled by the pathologists v.s the latent cluster is not good enough; especially when we see the big discrepancy of G3 and G4FG.	The dataset is tiny especially the evaluated one (160 256x256 patches, right?); The paper would contribute from public repo with the code otherwise it'd be hard to reproduce; Why not validate the results on public data (e.g., PANDA Kaggle competition data)?	The paper as it stands doesn't give any direct insights (e.g. via visualization) into the latent space structure or its geometry, which would be very interesting to see Not all technical details or general ideas behind the methods are explained well enough.	The authors provide some details of the training process; StyleGAN2 code is public available; however the training dataset seems un-available to the public.	Without the code and validation on public data (PANDA dataset) it'd be hard to validate the results. Also, some technical details  are missing, for example, how GAN was trained?	Would be nice if the trained model could be made available if possible.	Clinical application of this approach is not clear. I would suggest the author to provide some evidence to show that this approach can be beneficial to some clinical tasks.	It'd be great it was possible to validate the method on public data (PANDA dataset). Also, some technical details are missing, for example, how GAN was trained? More details on the training process would be useful to share, ideally the code.	"A general comment: I think not all choices or ideas are clearly explained in the paper. It would be good to check the manuscript in general and try to explain all ideas and assumptions as clearly as possible. Could you briefly explain why you used annotated samples in the first place to train the model? Was the annotation necessary to have a balanced dataset for the unsupervised training part? (Maybe I have missed it while reading) p2. ""Finding the latent point from an image - whether real or synethetic - is known as GAN inversion. While recent work improves GAN inversion [19,7,22], we found these approaches not pixel accurate on histology."" Could you explain this a bit clearer? What was the problem with those images? p2 ""Considering all the points sharing the same category, we apply principal component analysis (PCA) to describe the variation of these points within a unimodal latent cluster."" What was the (geometric) intuition behind this idea? Could you briefly explain it? p.6 ""Each latent point was truncated toward the mean of entire latent space using factor of psi=0.6 [15], reducing the number of unrepresentative features within the cluster while preserving diversity."" Also here, could you briefly explain why you do this? p/.6 ""In addition to the Z channel, the StyleGAN network has a random noise channel that influences the layout of features within the image [8]. In generating images within a category, the noise channel was fixed so that the layout of glands, nuclei, etc, in the images would remain fixed while the classification of the images changed."" A few, brief explanations here would help: What are the different components of the model, what kind of information do (we think) they capture? p.7 ""Second, the GAN provides a quantitative approach to comparing Gleason grades. Gleason patterns are often arranged from least-cancerous to most-cancerous tissues. The categories confused in Figure 2 (b) follow this same scheme"" This is a very interesting finding. Have you tried to visualize the geometry (e.g. via dimensionality reduction) of the latent space to see if it fits your interpretation?"	The major consideration for the recommendation is the lack of clinical application of this approach.	I think it's a very challenging indication (prostate cancer) and important problem of generating synthetic data and validating it by pathologists.	I like the idea in principle. I think the paper as it is needs to be improved in terms of clarity.
402-Paper1589	PRO-TIP: Phantom for RObust automatic ultrasound calibration by TIP detection	The author designs a phantom consisting of nine cones with different heights for ultrasound calibration. In addition, with the tip detection, the probe can be calibrated without requiring a tracking target on the phantom.	The authors present a novel calibration technique that allows feature and intensity-based calibration. Their proposed method is fully automatic that extracts the locations of cones in their proposed phantom and using segmentation they can track them across the sweep.	This paper presents a procedure for combining phantom-based and CNN image-based calibration methods to provide accurate, automatic calibration of tracked ultrasound probes. The proposed method was also tested on a dataset using an ultrasound probe not used in development, demonstrating generalizability.	The main strength of the paper is a new calibration phantom for US probe. The tip of the phantom can be detected using machine learning automatically for easier probe calibration.	"Their goal is not achieving better accuracy than current state-of the-art methods, but they compare their results to the ""Expert"" calibration without human intervention. Accordingly, their method achieved similar results to the expert calibration both in average error and distribution shape. Their proposed phantom design is easy to manufacture"	Generalizability: The inclusion of multiple set-ups with different probes and vendors in the dataset is a key strength of this work. As well, the experiment explicitly introducing images from a new system and quantifying the error strengthens the claim of generalizability. Dissemination: The commitment to providing openly available plans for 3D printing the phantom in different scales and the code is another major strength of this work.	This paper designs a phantom consisting of nine cones with different heights for ultrasound calibration. However, the heights' choice of nine cones should be further discussed and analyzed. Why it is the best design in this study.	Augmentation method is not explained well Testing is performed on devices from three manufacturers only. The weaknesses of the method are not described well.	Need: Although the limitations of current phantoms are described in the Introduction, it is not made clear how the proposed phantom would overcome some of these limitations (ex. manufacturing tolerances will still affect the present design) and therefore it is unclear why a new phantom is needed rather than adding an image-based refinement step to an existing phantom. Limitations: No discussion of the limitations of the proposed method is included in the paper. Details of CNN: The authors describe using a CNN for segmentation of the images for image-based refinement; however, many details about the implementation are missing, most critically a description of the dataset splits used for training and testing. More information on the generation of the ground truth labels is also necessary.	Satisfactory	The phantom CAD model and machine learning model are freely available online. They mentioned a reference implementation that is also are available on github.	This paper demonstrates a strong commitment to reproducibility by making the 3D printing plans and code freely available; however, key implementation details, such as hyperparameters and details of the architectures and training performed are missing from the paper itself.	"(1) The designed phantom consists of nine cones. The relationship between the number of cones and the calibration accuracy should be analyzed. (2)The author said ""we simulate Gaussian and speckle noise of various scales"". Please add more explanation. (3) The author uses 4 pairs of tips to produce a calibration hypothesis. Whether the more pairs of tips, the higher the calibration accuracy? (4) Weather the proposed calibration phantom is suitable for various ultrasound probe? For example, the linear probe and the curve probe."	Explaining the augmentation method	Overall, this paper was interesting and well-written, solving a useful challenge; however, the following need to be addressed to properly understand the work. Major Revisions: 1) Introduction: Although thorough and clearly written with the limitations of current methods described, it is not entirely clear how the proposed approach will overcome the limitations of previous methods and why the need for new phantom rather than combining image-based detection with any of the existing phantom approaches. 2) The paper should acknowledge the limitations of the proposed approach and discuss. 3) Approach: Details of the CNN implementation should be provided, most notably the dataset split. 4) Approach: Data labelling - Details of how the tracked sweeps and label map are registered are not provided. This is critical to explain, as this affects the ground truths used for training and evaluation. Were the automatic segmentation maps validated in some way prior to use? Minor revisions: -Fig. 1: as the two sub-figures (a and b) are extremely similar, including both does not add much value. It would be better to show a photo of the physical phantom in (a) instead. -Experiments and Results: given the large disparity between the average and median distance metrics (~10mm vs. ~1 mm), a rationale for the outliers should be provided, as well as a normality test on the data to aid in the interpretation of the results	The scientificity of the designed phantom needs to be further verified.	-Novel phantom design, easy to manufacture and open source code.	The paper is well-written and the techniques proposed would be of interest to the MICCAI community; however, the amount of critical information and discussion that is missing in the current version must be addressed in order for the work to be properly understood.
403-Paper1074	Prototype Learning of Inter-network Connectivity for ASD Diagnosis and Personalized Analysis	The paper combines prototype learning and topological relational learning to learn high-order inter-network functional connectivity (FC). Empirical results are reported on the ABIDE dataset.	The authors propose a transformer based deep learning framework for topological relational learning to model higher-order characteristics of inter-network functional connectivity. They combine this with prototype learning to uncover differences between patients and controls, while simultaneously modeling individual characteristics. They experiment on ABIDE and examine diagnosis of ASD/controls and ability to generate neuroscientific explanations for intra-class variations and inter-class variations.	This paper proposes a method for analyzing functional connectivity (FC) data for interpretable classification. The proposed neural network uses multi-head attention to learn global inter-network relationships for FC reconstruction. The pretrained model is then finetuned in the classification task, where prototypes of each class in the embedded space are learned. The prototypes can then be used along with an individual's FC representation to explain inter- and intra-class differences for a subject. The methods were tested on classification of autism spectrum disorder (ASD) vs control subjects using the ABIDE dataset.	The pape is well-written. The design of the model, problem formulation and experimentation are in good standing. They demonstrate better results than GNN methods with predefined ROIs and other convolutional methods with and without prototypes.	The paper is well organized with clear writing and explanation, and is easy to follow. Adopting prototype learning to generate neuroscientific explanation is an interesting premise and is novel for this application. The experimental results demonstrate improvements over state-of-the-art baselines for classification	The paper proposes an interesting method for learning global inter-network relationships - considering each ROI a seed of a network, the sequence of FCs values for each ROI are analyzed using a transformer encoder architecture. The classification approach uses prototype learning, thus embedding some interpretability directly into the classification model learning. I greatly appreciate that the parameter settings for all the models are shared (in the supplementary), enhancing reproducibility and assessment of experimental results. The experiments use the public ABIDE dataset, also enhancing reproducibility. The general flow of the paper is well-organized.	Lack of ablation study: Are all of the suggested components (inter-network encoder, prototype-base classifier, transformer, etc.) contributing to the high performance of the model? It is not clear how much each component is contributing, and whether, e.g., replacing transformer with a non-attention module would degrade the performance.	"The authors use 5 fold cross validation, but do not explicitly mention a validation set for hyper-parameter tuning. It is unclear how key hyper parameters for the methods and baselines have been selected and whether this was done in an unbiased way. I think it is an important point for clarification, since there seem to be at least 4 hyper parameters (\lambda_1, \lambda_2 \lambda_3 for losses and m for the prototype margin) which seem to dictate generalization. I found some of the claims of the paper a bit strong and not as well explained: a. Table 1, the authors claim that prototype learning provides improvements in performance when comparing BrainNetCNN and BrainNetCNN+P (with prototype learning). The AUCs/Accuracy for both are very close (in fact BrainNetCNN+P has larger error bars). There seems to be some tradeoff between sensitivity and specificity. I am not sure whether the 'improvements' would be statistically significant. b.  ""To be specific, if a summary feature vector of a TD subject is replaced with the ASD prototype (pc=pASD), we can predict the functional degradation of FC as if the subject were suffering from ASD."" ""It should be noted that our proposed method can generate counterfactual FC patterns for a subject, which can be of great benefit and used to obtain deeper insights into the functional characteristics of a brain in regard to ASD."" It is not immediately clear why replacing the prototype of an typical individual with that of the ASD class would necessarily correspond to generating a valid counterfactual. Could the authors please provide further explanation/references on why this is a reasonable assumption to make?"	"While I like the idea of embedding interpretability directly into the classification model, I have concerns about learning just 1 prototype for each of the normal and ASD classes. ASD is known to be extremely heterogeneous as it describes a spectrum of disorders - thus, I feel that having only 1 prototype to represent the whole class may not be the right model. There is also a large range of ""normal"". Can the authors comment on/justify this modeling choice? There are missing details/explanations in the methods/experiments that need to be added/clarified in order to improve the understanding of the paper and results. Detailed comments are given below in question 8. There is also some missing analysis I feel in the experimental results section that should be included to strengthen the arguments that the proposed method outperforms other approaches. Detailed comments are given in question 8."	The code is released, no reproducibility concerns. All good.	The authors have provided an anonymized link to the code base. In my opinion, this is a point in favor of the reproducibility. However, the range of hyper-parameters considered, method to select the best hyper-parameter configuration in the paper is still ambiguous	Reproducibility is fairly good - based on the checklist/submission it appears code will be shared with acceptance. However, there is some missing experimental analysis (e.g. parameter sensitivity, statistical significance, when does method fail) that could be added to improve the paper.	"We advise the authors to assign an editor to revise the paper, examples of such improvements would be: Abstract, ""by comparing to competing"" -> ""by comparing with competing"" ..."	Additional Points and Clarifications: In section 5-Personalised FC Analysis, the authors base their analysis on randomly selecting two ASD and two controls subjects for identifying the top 5 ROIs with the largest variation. However, since this sampling was done only once, it is unclear how stable these variations (and thus the selection of the top ROIs) are. How is the positional encoding matrix e in Eq. 1 calculated?	Detailed comments are in order of appearance in the paper. The paper makes use of the self-attention mechanism and multi-headed attention modules. However, the original paper proposes such structure is never cited and should be included: Vaswani et al., All you need is attention, 2017. I am not sure what the variable z_r^L represents. I think that z_0^L is the summary vector representing the entire FC networks. But where do z_r come from? This needs to be defined/clarified. Perhaps labeling where these variables appear in Fig. 1 would also be helpful. For the 5-fold cross-validation setup, are the partitions done subject-wise? If so, it is confusing because this setup is mentioned immediately after discussing data augmentation, so it is not clear if the split is performed just on the augmented data as a whole, or whether it is by subject. And if not by subject, this would greatly inflate classification performance, since the augmented data per subject is highly correlated. Related to the cross-validation setup, is the pre-training done using the same partitions as the classification training (i.e., same test set is left out the whole time)? While the pre-training does not make use of labels, the same network is being used to learn the classification in step 2, and thus the test data needs to be left out the whole time. As mentioned in the paper, the training is performed in 2 steps - pretraining of the transformer reconstruction network, then learning of the classification model. How does the performance change when trained in 1 step, end-to-end? There is no additional data added in the pretraining as far as I can tell, so I am wondering about the advantage of pre-training vs. end-to-end training, since the classifier is trained in step 2 in an end-to-end model. Comparison to 1-stage training would strengthen the case for 2-stage. The hyperparameter settings, e.g., for the lambdas in the loss function for classification learning are given in the supplementary. However, I wonder how these parameters were chosen? Was any tuning involved, and if so, was a validation set used, or is based on the testing set? Clarification on if any tuning was performed (for the proposed method and other methods) and if so how this was done would be appreciated. For the hyperparameters such as in the loss function, how sensitive are the results to the choice of parameter settings? In Table 1, the AUC means are around 0.6-0.7, but the standard deviations (or standard error? please clarify) are reported in the range of 2-4 - are the decimals off and this should really be 0.02-0.04? The overall means are also reported as proportions, but the standard deviations appear to be in percent. Please check values. For the main classification performance results, while some sense of variation is given, there is no statistical significance testing reported - this could help strengthen the case for the proposed method.	This work properly reuses the well-known neural components to solve a real clnical problem. Eventhough there might not be novelty in each component of the proposed method, the mixture of them and how the problem is formulated is deemed novel. The results are promising compared with similar methods. The method is well-explained.  As a  bonus point, it is worth thanking the authors for their prospective code release.	I found the idea of utilizing prototype learning for generating neuroscientific explanations interesting and possibly novel for functional connectivity analysis. However, I had several concerns about the experiment design (hyper parameter settings, soundness of clinical interpretation), due to which I would recommend a  weak reject.	This paper proposes some interesting ideas for analysis of FC data, using multi-headed attention to learn relationships across a sequence of networks seeded by an ROI, and using this representation for prototype-based classification. However, the choice of 1 prototype per class is somewhat questionable, and many details need to be clarified to be able to fully assess the work.
404-Paper0648	Pseudo Bias-Balanced Learning for Debiased Chest X-ray Classification	A pseudo bias-balanced learning algorithm, which first captures and predicts per-sample bias labels via generalized cross entropy loss and then trains a debiased model using pseudo bias labels and bias-balanced softmax function.	The paper proposes a novel model to learn prediction using biased dataset. The methodology is two folder. The first part is to estimate a Pseudo Bias label from the sensitivity and specificity on the training set. The second part is considering this bias label into the Bias-Balanced softmax function at which a generalized cross entropy is used to capture the discrepancy between biased training and non-biased testing set.	This paper proposed a novel algorithm, pseudo bias-balanced learning (PBBL), to tackle the dataset bias problems in medical images. The underlying method first estimate the bias level for each case, then use this pseudo bias label to train a debiased model which avoids the shortcuts and directly learns from the intended information.	novel approach to debiasing	Conceptual innovation Novel methodology	"Dataset bias is truly noteworthy problem in medical image domain. A simple example, most of the subjects who choose to receive specific medical scans may have similar symptoms and diseases. It is often hard to handle such conditions since the labeling processing is troublesome and require additional experts labor. The method proposed by this paper tries to deal with this problem without explicit labeling process. The method itself, PBBL, is straight forward, easy to follow and seems feasible to me. The use of data is also very reasonable. The authors first generate two types of highly biased dataset which are biased towards data source and gender information respectively. Then the models is trained with a bias-balanced softmax. To generalize the algorithm to unknown bias, rather than data source or gender information, the authors took a step further by applying a generalized cross entropy loss. The major assumption is ""dataset biases would be preferred when they were easier to be learned than the intended features"", which matches the intuition."	not sure how realistic is the training sets, what happens when the data has no bias, or multiple forms of bias.	Questionable experiments given the problem statement and dataset definition	The major concern of mine is the paper lacks proper visualization of the biased/unbiased model. For example, model A could be biased towards gender information while model B is trained using the proposed method and is bias-balanced. It is important to show the class activation maps of the models before/after the generalized learning.	not easily reproducible	All data and codes will be open-source	The authors provide all the source code in the checklist and supplemental materials. And they use public dataset which is beneficial for reproducing the work.	important topic has been studied, and not needing a labelled bias info makes the approach practical; but, I am missing the discussion on the impact of the method in no-bias case, and when multiple forms of bias are existing in the data.	"1- In problem statement: Source-biased Pneumonia (SbP): ""We then sampled 5, 000xr% pneumonia cases from NIH and the same amount of healthy cases from MIMIC-CXR. Here, the data source became the dataset bias, and health condition is the target to be learned."" Do you mean here that the subset of data can cause this bias? as I see that you randomly sample the exact number from each set/class. I didn't get how this can mimic a bias. 2- In problem statement: Gender-biased Pneumothorax (GbP):Here it is much straight forward. However, I just wonder why didn't you consider a direct bias from the label? Instead of having this sort of indirect bias through one of the covariables? 3- In algorithm 1: Are fB and fD independent networks? Does not share weights? 4- In Page 5 ""Giving f(x) the softmax output of the model, denoting fy=j (x) the probability of x being classified to class y = j and th the parameters of model"" It would be better if th is included in the f function definition. 5- In table 1: G-DRO looks having best results with ground truth bias label. Is that comparison performed on an independent datasets? If used on the same dataset so the GT bias should be available for your model as well. 6- The improvement in results looks marginal."	Consider superimpose the class activation maps to the x-ray images to see: based on which part of the image, the biased/debiased model is making the final decision. This may help the readers get a more intuitive understanding.	Please see the comments in 8.	Questionable experiments but can accept if appropriate explanations are provided.  Weak accept for incremental results	The authors explained their method in a clear and reasonable way. Each step  is well founded and seems feasible to me.
405-Paper1613	Radiological Reports Improve Pre-Training for Localized Imaging Tasks on Chest X-Rays	The authors propose a downstream evaluation framework with 18 localized tasks on chest X-rays, including object detection and semantic segmentation on five public datasets. The authors conduct a comparative study of pre-training methods, including text-supervised and image-only contrastive methods. The authors pre-train their models on MIMIC-CXR and evaluate the studied methods on their localized chest X-ray evaluation framework.	The authors propose a evaluation framework consisting of 18 localized tasks, including semantic segmentation and object detection, on five public chest radiography datasets. They test many different SOTA self- or text-supervised methods in many downstream tasks.	This paper studies the performance of different self-supervised pre-training methods on localized imaging tasks on chest x-rays. In this paper, two types of self-supervised pre-training methods are studied, including contrastive visual representation learning and text-supervied learning. Three evaluation protocols including fine-tuning, backbone frozen, and linear evaluation are employed. Extensive experiments on five datasets show the advantages of text-supervised learning over contrastive learning methods.	The authors demonstrate that text-supervised methods outperform all other methods on 13 out of 18 tasks and are less sensitive to the downstream dataset size on some tasks. The authors show that transfer from classification does not perform well and common supervised classification methods seem to be unable to utilize image labels effectively for localized downstream tasks. The authors provide a good justification for the results.	I appreciate the huge amount of experiments in this paper. They show the effectiveness of existing text-supervised methods and compare them with image-only self-supervised methods and transfer from classification. The experimental results show the text-supervised methods outperform all other methods on 13 out of 18 tasks.	The comparison of the performance of contrastive and text-supervised on localized imaging tasks such as sementic segmentation and object detection is interesting. It is inspiring to show that text-supervision is even better than contrastive supervision in most of the tasks. The experiments are extensive and includes several datasets.	The methodology is not novel in general but the authors provide comprehensive evaluation framework for 18 localized tasks.	I think this paper is more like a technical report but not a scientific paper. The authors spend lots of time conducting tons of experiments to show the effectiveness of existing self- and text-supervised methods. However, I do not see any new methods in this paper. I think it is also okay to analyze the effectiveness of existing methods in different settings. But the authors should give a detailed analysis that why the text-supervised method is better and how the radiological reports help to improve the performance. The conclusion in this paper does not focus on the medical images, and the authors should analyze the differences in the performances of these self- and text-supervised methods in natural and medical images.	While this paper provides detailed comparison between different self-supervision methods, no principled analysis is provided for choosing the best pre-training methods for a given task. For example, given pre-training dataset A and downstream task B, how to choose the self-supervision methods based on the characteristics of A and B?	The authors employ well-known datasets and architectures. It seems that the paper is reproducible though the authors do not provide code.	All the methods and datasets are public in this paper. I think it can be reproduced since the existing methods have open-source codes.	The hyperparameters of the experiments are described in detail, which are sufficient to reproduce the results.	The authors study the effectiveness of existing text-supervised methods and compare them with image-only self-supervised methods. The authors did a good job of evaluating the text-supervised methods to contrastive methods and in-domain and cross-domain transfer from classification methods. The results and justification look reasonable and might be useful to the community. It would be interesting to see the results for the deeper backbone for UNet and the higher resolution of the input image.	Please add a detailed analysis that why the text-supervised method is better and how the radiological reports help to improve the performance. For instance, show some cases with original images and radiological reports, analyzing which parts of the radiological reports helps. Or, using some grad-cam method to show the attention regions of radiological reports. Meanwhile, please show analyze the differences in the performances of these self- and text-supervised methods in natural and medical images.	Could you explain more on the sensitivity of pre-training methods to the size of the downstream tasks shown in Fig. 1? If I understand this figure correctly, the results shown is the performance on 1\% or 10\% data of the downstream task relative to the full data. However, it is not clear why this metric is important. Providing an example could better explain the importance of this sensitivity. It could be better to show the results of combined contrastive and text-supervised learning. Does this combination improve the performance? If yes, does it always outperform each of them in the studied datasets?	Technical novelty, reproducibility, and results achieved.	Novelty is limied. No new method is proposed in this paper. They just analyze the the effectiveness of existing self- and text-supervised methods in different tasks, and do not analyze that why the text-supervised method is better and how the radiological reports help to improve the performance.	This paper is a comprehensive and extensive study of contrastive supervision and text-supervision for pre-training on unlabeled dataset. By providing the detailed comparisons, this paper has the potential to inspire more principled studies on self-supervised learning for localized medical images tasks.
406-Paper1231	RandStainNA: Learning Stain-Agnostic Features from Histology Slides by Bridging Stain Augmentation and Normalization	The authors propose a pre-processing method to perform joint stain augmentation and stain normalization (SA & SN) in computational pathology. The SN process generates color templates using the LAB space intensities averages and standard deviations. The SA generates only synthetic images within the ranges of the generated SN templates. The approach is evaluated on the downstream tasks of colorectal cancer image classification and nuclei segmentation outperforming (outdated) SA and SN approaches separately.	The article proposes an image augmentation metod that combined stain normalization and stain augmentation. At first they use Lab color space and then increase the number of color-spaces used during processing to 3. The method seems to work regardless of the task (segmentation/classification).	This paper proposed RandStainNA which unifies stain normalization (SN) and stain augmentation (SA) for histology image analysis. Specifically, randomness is introduced in the conventional SN process to generate more realistic stain variations, i.e., random virtual templates from pre-estimated stain style distributions are generated and incorporated into the SN process. Additionally, random color space selection scheme is also introduced in the framework.	Combination of SA and SN is a great way to augment the dataset size and train more robust deep learning networks in computational pathology. RandStainNA has a simple yet effective manner to combine both. The evaluation is done in two standard open-access computational pathology datasets for colorectal cancer classification and nuclei segmentation. The paper is well written and easy to follow. The proposed method is evaluated with many recent backbone DL architectures which highlights the superior performance of  RandStainNA to the baselines.	the idea of combining SN&SA is interesting	The attempt of combine SN and SA into one unified framework is interesting and there are few previous research works address on this topic. Overall, this topic has some degree of novelty. The experimental design is relatively complete. It considers two different tasks (classification and segmentation), different baseline CNN architectures, three color spaces (LAB, HSV, HED). The organization of this paper is clear and easy to follow.	"The combination of SA and SN is not novel, and has been already presented in the past in combination with more recent techniques (domain adversarial learning) and more pathology-informed color space extraction (stain absorption matrices), see for example[1,2,3]. The method is evaluated at the patch level, and thus computing average and standard deviation is fast, but I don't see this method scaling well at the whole-slide image level. The method was not compared with state-of-the-art methods such as CycleGANs (that keep morphological information) or domain adversarial learning, not even with other combinations of SA & SN. The main weakness of the paper is that there is not a statistical analysis of several runs of the methods. Given the stochastic nature of the generation of the templates and augmentations, the average (and std) for several runs for the method and baselines should have been reported to have a more robust estimation of the real performance. [1]: Van Eycke, Yves-Remi, et al. ""Image processing in digital pathology: an opportunity to solve inter-batch variability of immunohistochemical staining."" Scientific reports 7.1 (2017): 1-15. [2]: Marini, Niccolo, et al. ""H&E-adversarial network: a convolutional neural network to learn stain-invariant features through Hematoxylin & Eosin regression."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021. [3]:Otalora, Sebastian, et al. ""Staining invariant features for improving generalization of deep convolutional neural networks in computational pathology."" Frontiers in bioengineering and biotechnology (2019): 198."	a lot of description is unclear it is difficult to understand even for experienced reader lack of cross-validation of the results	In my own opinion, the idea of combining SN and SA is straightforward and is trivial to deal with.  For example, a simple solution is a naive serial combination of the two components. The authors should state clearer on the advantages of your work against some simple combinations of the two components. From methodology perspective, the overall pipeline of the proposed framework is lack of depth. For example, the random color space selection is just doing random choice among three color spaces with equal probability. Some experimental settings are intricate. Since the proposed method belongs to data pre-process and augmentation, did other data augmentation methods (geometric transforms, noise, rotation, contrast, cutout, mixup and so on) are performed when running baseline models? In other words, although the proposed method has large performance improvements against baseline, I am concerning whether other augmentation methods can also achieve such improvements and whether the proposed method can consistency gain performance improvements besides these augmentations. Also, why all models just running for 50 epoches? Did all the models convergence?	The datasets are available, but the code has not been released. The method is simple so it should be easy to reproduce. There is no information about hyperparameters such as learning rate used in the experiments or how many epochs were used, limiting the reproducibility.	it seems that method is reproducible	Good.	Providing computation times for the whole pipeline with and without preprocessing would have been useful. This might be a great contribution to the computational pathology community if it is included in a library and distributed to researchers to use it. Or include the method in actively developed computational pathology libraries such as[4,5] I think the method is indeed useful, but for me its lack of statistical analysis and comparison with more recent methods makes a borderline accept decision needed. [4]https://github.com/TissueImageAnalytics/tiatoolbox [5] https://histolab.readthedocs.io/en/latest/index.html	"Comments: The subsection 'Virtual stain normalization template' consist of not clear description, please rephrase sigma_j is not defined (p.4) (p.4)""The empirical results suggest [...]"" - meaning that there is no proof, please expand on that (p.5) please provide a proper reference for the MoNuSeg dataset (p.5)""we generate different virtual templates for images that vary at every epoch during the training"" - this is not described enough, please expand on p.7 please double check if there should be SN1 & SN2 or maybe there should be SA1 & SA2 instead the proposed ablation study is questionable - please reconsider with proper testing."	Please refer to the weakness section.	Lack of statistical analysis given the stochastic nature of the method. Lack of comparison with state-of-the-art methods	The overall idea is quite interesting but with the unclear description and no cross-validation of the results it seems that the reliability of this article is questionable.	Although the combination of stain normalization and augmentation is interesting and has some degree of novety, the overall novety of this paper is limited and the propsosed framwork is lack of depth in methodology perspective.
407-Paper0281	Real-Time 3D Reconstruction of Human Vocal Folds via High-Speed Laser-Endoscopy	The paper presents a method for extracting a 3D mesh of the vocal folds using laser endoscopy as well as a dataset of such laser endoscopy images.	Motivation for the study is examination for vocal folds for diagnosis if laryngeal and voice related disorders. The study proposes a new framework for real-time (~25 fps) reconstruction of 3D geometry of (vibrating) human vocal folds. The framework uses laser projection unit (LPU) connected to a high-speed camera to acquire information about the vocal folds geometry and well-established methods of parametric reinterpretation of the M5 vocal fold model as a tensor product surface for geometry reconstruction.	This paper presents a structured light-based method to reconstruct human vocal folds. A symmetric laser grid pattern is projected on the surface of interests and their 3D locations are estimated after a localizing them in the endoscopic image and correspondence between the camera and the projector is established. Using a parametric model of the vocal folds and the estimated 3D locations of the projected dots, the authors obtain a dense reconstruction of the projected surface. The major contributions are: An automated method for dense reconstruction of the human vocal folds using a monocular laparoscopic camera system augmented with a laser dot projector. The quality of the reconstruction is compared to the state-of-the-art using in-vivo datasets. The dataset will be released upon acceptance.	The dataset is an important contribution for anybody working in vocal fold reconstruction.	The proposed framework appears to be clinically feasible:  (a) The images are acquired using a clinical grade endoscopic projection unit (and high-speed camera); (b) The framework is automated and consistent with the clinical workflow time constrains; (c) Real-time (~25 fps) performance is achieved using off-the-shelf computing hardware (i7 CPU and NVidia Quadro RTX 4000 GPU). Surface reconstruction: appears to be an innovative extension and application of well-established methods (M5 Model to 3D using B-splines) Reasonably extensive evaluation of the proposed framework using a physical (silicone) model of the human vocal folds and established labelled image datasets is  reported.	A fully automatic method is presented for dense reconstruction of human vocal folds using a structured light endoscopic system, and a parametric model of the vocal folds. The entire pipeline runs in 25fps, which seems adequate for the intended application. The dataset used in this work will be made publicly available which will encourage further development in the field.	The literature review is very weak. In particular, it fails to explain how the method differs from the other laser endoscopy based methods [13,16,20,21], especially [21]. The algorithm description is not clear, for example it is not clear where the first two steps MS (mask sweeping), GA (global alignment) were described, since they were not named so in Fig 1 or in Sections 2.1 and 2.2. The dataset is quite small, containing only 10 videos. The 21 Phantom videos should also be included in the dataset. The quantitative evaluation is lacking in many respects. First, it is not clear what kind of labeling is evaluated. Second, a quantitaive comparison with other state of the art methods such as [21] is missing.	In qualitative evaluation, L1 error and standard deviation of grid offsets are used (Table 1). The following criticism can be raised here: (a) Justification for using these particular error measures is not provided; (b) It is unclear how relevant are the error measures used given the context of study (diagnosis of laryngeal and voice related disorders); (c) Are the reported accuracy and robustness of the proposed framework sufficient (in quantitative sense) for application in diagnosis of laryngeal and voice related disorders (which is the motivation for the study).	The reconstruction pipeline is very similar to what's presented in the reference [20]. The authors do not clearly distinguish their contributions in contrast to reference [20].	The method is reproducible with some effrot.	"Appears to be satisfactory. However, given the statement in the manuscript that ""we publish a dataset containing laser-endoscopy videos of 10 healthy subjects that can be used to drive further research in this area"", the reference (or web link) to this publication will to be provided in case the manuscript is accepted."	Adequate details of the algorithms are provided with references where further details can be found. In addition, the source-code and the dataset will be made publicly available upon acceptance. No information on the sensitivity of the parameters used in the reconstruction method is reported. Several important implementation details are missing as well.	Improve the lit review to show how your method is different from existing methods, especially [21]. Crystalize your naming of the different parts of the method so that they are described the same in Fig 1, in their text descriptions and in Table 1. Add a quantitative comparison with [21]. Explain what labeling error is being evaluated in Table 1.	It is a very well written manusctript presenting scientifically sound (and potentially amenable to cilinical workflow) framework/pipeline for real-time 3D reconstruction of vocal folds geometry. My main reservation regarding the proposed framework is that while the reported results seems sufficient to support the conclusion that the obtained 3D reconstruction is visually appealing and can be provided in real-time, they do not appear to satisfy the requirement for quantitative accuracy and robustness that would be required for clinical application in diagnosis of laryngeal and voice related disorders. Providing justification for the error measures used in the study and interpretation of the results in the context of the accuracy required for clinical applications, would improve the manuscript.	Overall, the paper reads well: adequate background to the problem is provided with reference to the state-of-the-art methods, methods and the results are presented well. The described reconstruction pipeline is very similar to the one presented in reference [20]. Without explicitly describing how this paper differs from [20], the authors contributions are difficult to identify. The authors assume a calibration between the camera and the projector. How is this calibration estimated? How are the camera intrinsic parameters estimated? How good are the estimates? This information is crucial to the reproducibility of the paper. The authors use epipolar lines to constraint the correspondence search. If a centroid of a projected dot hits an epipolar line, it is considered a potential match. However, with errors in calibration, practically, the dots do not exactly hit the epipolar lines. Therefore, some distance measure (between the centroid and the epipolar line) with a threshold has to be considered. What distance did you use?	Weak literature review and weak quantitative evaluation.	It is a very well written manusctript presenting scientifically sound (and potentially amenable to cilinical workflow) framework/pipeline for real-time 3D reconstruction of vocal folds geometry. It appears that the weakness/weaknesses in terms of interpretation of the of the results obtained in terms of the accuracy and robustness that would be required for clinical application can be addressed by revising Results and/or Conclusiosn section without any need for obtaining additional results.	The presented methods have limited novelty. However, the authors have validated their method on in-vivo human data. In addition, the dataset will be made publicly available encouraging further research in the field. Considering all these factors, I would accept this paper if my concerns listed above could be adequately addressed.
408-Paper2438	Recurrent Implicit Neural Graph for Deformable Tracking in Endoscopic Videos	The authors present a novel graph based method to track an arbitrary number of key points through a video sequence. It is designed to cope as different obstacles are introduced into the scene, and to track foreground and background objects without requiring explicit segmentation steps. The method appears to be accurate and fast.	The paper presents a self-supervised method for estimating dense pixel flow in endoscopic videos. The main contribution compared to prior work is the addition of a temporal component which tracks deformation over time inside a recurrent neural network.	The paper proposes a deformable tracking method on endoscopic videos using a recurrent implicit neural graph (RING). It extends a previous method by accommodating temporal information using a RNN. Its inference is quite fast enough to be used for real-time application.	A method that is adaptable to varying number of points over time.	The method is a novel combination of Graph Neural Networks to parse keypoint information, an attention mechanism to refine displacements, a recursive network to carry information to the next images and a sampling strategy to turn the encoded sparse displacements into a dense output. The result works well on datasets that were not part of the training data (generalization is shown for datasets collected at different sites).	Detailed descriptions of the method; Extensive experiments; Fast inference.	None as such. Im a little concerned that the standard error of pixel tracking errors seems almost too small to be true? Might be worth checking. There seems to be very little explanation of what the method is for, or ultimately what the author hopes it will be suitable for.	My main concern is that I found the Methods section quite hard to understand. This is in part because the method is complex and requires many different concepts like Graph Networks, Recurrent Networks, positional encoding, Attention... and the MICCAI format is very limited in space. I assume reference [1] explaines some of these in more detail, but it was anonymized for the review. However, even so, I think the Method section could be made much more readable by slightly changing some sentences (and often the order of sentences). At multiple points, it was not clear to me whether a new concept was being introduces or the previous concept was refined further (some examples in the details below). Similarly, some concepts are explained at one point in the text and the corresponding equation comes multple sentences later, making it hard to follow. Overall, I think the Methods section should be reworked to ensure a clear flow through the paper, introducing one step after the other.	As mentioned in the paper, it extends a previous method. There's nothing wrong about this but then a lot of space is used for explaining the previous methods.	Good. The code is even already available in a github repo.	The major components are explained, but reproducing this work without access to the code would be extremely difficult. The system is so complex that it is difficult to describe in such a short paper. Even so, the authors do make a strong effort to describe as much of the system as possible, and many parameters and setup details are given.	Thanks to its detailed explanations on the method, it seems reproducible.	I have to admit to being a little out of my depth here. The paper is generally clearly written, with lots of technical detail.	"I don't quite understand the sentence ""Each of these layers can be thought of as a new initialization: no weights are shared between each ph or g."" Does this mean there are multiple g? Or does 'each' only refer to the ph? In this case, this may be cleared up by writing ""... are shared between ph_1, ph_2 and g""? Maybe the word ""initialization"" could also be replaced, since this already refers to choosing initial weights in the context of neural networks? ""with the difference being that we add in a relative positional embedding to let the network select based on position."" I did not understand this sentence, because Equation (1) uses the absolute pixel positions p_i and p'_i (and the distance between them), but this sentence instead mentiones ""relative"" positions. Can you explain what you mean by ""relative positions""? To what are they relative, and how are they used exactly? Or is this refering to the p_i - p_j in Equation (4)? In this case, maybe you can move this sentence to the next paragraph? I assume the gamma in Eq. 1 and Eq. 5 are not the same? If so, could you use a different letter? ""We set the base offset to be the barycentric estimate, helping similarly to how a skip connection helps learn the residual in CNNs. We perform barycentric interpolation on the Delaunay triangulation of the refined neighbor node displacements"" -> I had to read this a few times. Maybe you could start by saying that the base offset is set to an interpolation of the refined displacements in the neighborhood, and then go on to saying how it's done (via barycentric interpolation)? Otherwise the two sentences sounded to me like you were performing two different steps. ""The information at the query point is broadcast to each of these neighbors, run through two graph convolutions and then pooled."" and later: ""We first broadcast information from q to each neighbor."" - This is the same concept twice, maybe only mention once? Suggestions for Fig 2: If I understand correctly, these are two ""unrolled"" steps of the RING network? Maybe the difference between the two boxes would be slightly clearer if the titles would be changed to: RING (at time t-1) and RING (at time t) or similar. In both boxes, h_q^(t-1) is used. I assume this should the t-2 in the first box and t-1 in the second? For the SCARED dataset, the ground truth is calculated - could you show the errors (as images) of each sample in the supplementary material? That would be interesting to see (are the errors larger on the tools, or distributed equally etc.) Minor: ""we calculate new features for and refined displacement estimates for each match."" -> remove first ""for""?"	My only complaint is, as mentioned, if the 2/3 of the method is from a previous method, those could have been more brief. Then, there could have been more space for experiments etc.	First use of graph methods that I've seen in this medical field.	The method is quite complex, making it difficult to explain. However, I feel like the explanation could be made clearer by moving around and reworking some sentences in the methods section. Alternatively, this may be better suited for a (longer) journal paper?	The method seems to be working well. Also, the fast inference time makes the proposed method quite useful.
409-Paper0524	Reducing Positional Variance in Cross-sectional Abdominal CT Slices with Deep Conditional Generative Models	This paper is dealing with a novel application, which is to reduce positional variance in cross-sectional abdominal CT slices by generating subject-specific target vertebral level slice given an arbitrary abdominal slice as input. This paper proposes C-SliceGen to capture positional variance in the same subject with conditional generative models. Experiments show the effectiveness of the proposed method.	The paper aims to reduce the positional variance in cross-sectional abdominal CT scans by extending the conditional generative models to C-SliceGen that takes an arbitrary axial slice in the abdominal region as the condition and generates a vertebral level slice. Experiments are performed on 1170 subjects from an in-house dataset and 50 subjects from BTCV MICCAI Challenge 2015 dataset.	Authors propose conditional SliceGen (C-sliceGen) to synthesizing slices to target vertebral level (axial position) to reduce the positional variance problem. Authors extend classic conditional generative generative method and input random axial slices. They use one private dataset for generating images and  two dataset such as   BTCV MICCAI Challenge 2015) and  BLSA dataset for external validation. They also report their results with SSIM, PSNR and LPIPS GAN metrics.	The application is interesting. As the paper mentioned, this is the first method proposed to tackle the 2D slice positional variance problem. Since this is the first attempt to handle such a problem, the proposed method can be treated as a pioneer for the subsequent studies. This paper proposes a new way to evaluate the position variance.	The paper addresses an important problem of reducing the positional variance in cross-sectional abdominal CT scans. The qualitative and quantitative results are quite impressive and show the superior performance of the proposed approach.	I believe that there is not enough novelties except proposing dealing with positional variance problem.	The main purpose of reducing position variance is a little bit vague. In the abstract and introduction, the paper just emphase tacking the 2D slice positional variance problem. But what are the direct clinical application is not clearly written. It is not friendly for readers who are not experts in cross-sectional abdominal CT slices. I do not know why it is necessary to synthesize an image at a pre-defined vertebral level. I would like to see an explanation why not conducting registration to find a pre-defined vertebral level slice. Or use organ/vertebral information as reference to find the pre-defined vertebral level slice (see the last column in Fig. 3, it is not difficult to localize the red line according to this view). The synthesized CT slice seems very different from the target (see Fig. 3). Is it really helpful for clinical applications? The author should discuss more on this. Why do we need to reduce positional variance for single slice longitudinal analysis instead of 3D patches? Since this paper proposes a new application, it is important to illustrate more. Muscle area and visceral fat area may change during time. Does the results in Fig. 4 verify the effectiveness of the proposed method in reducing positional variance?	I'm concerned about the clinical use of this work as this cross-sectional 2D scans are generally done to assess body composition. I doubt that a generative approach would be the best way to solve this problem, given the fact that it doesn't take into account shape, boundary information and heterogenous soft tissues. The backbone of the proposed approach is VAEGAN, so it can't be considered a novel approach from technical perspective.	"Although i appreciate authors' effort, some words used in the current manuscript are not clear. For example, harmonizing "" By computing body composition metrics on synthesized slices, we are able to harmonize the longitudinal muscle and visceral fat area fluctuations brought by the slices positional variation."" or  ""we demonstrate that the proposed method can consistently harmonize the body composition metrics for longitudinal analysis."" I expect that  authors would show the difference between VAE and their proposed method because they claim that they extend VAE method in their paper. It is not clear how authors extend current method in the literature. I would also expect to see CycleGAN and Pix2pix for fair comparison."	The author says the code will be published upon the acceptance of the paper. Since this is a pioneer work on the new application. Publishing code is important for other researchers to follow.	The paper seems reproducible.	Work can be reproducible.	This paper should illustrate more on the new clinical application and technical motivation. I also have a question regarding the experiment part. Fig. 4 shows spaghetti plot of muscle and visceral fat area longitudinal analysis. But muscle and visceral fat may change overtime. Some important detailed illustration is missing here, or in the introduction. See detailed comments in the weakness of the paper.	The colors in figure 2 are quite faint. The paper lacks extensive quantitative evaluations from other registration and generative model based baselines.	Authors may use other methods for comparison such as CycleGAN and Pix2pix.	The application seems novel. But the paper misses a detailed illustration on the clinical potential of the application. The author should answer my concerns in the weakness part.	Interesting and important problem. Some novelty in the formulation.	Application seem somehow new but there is not enough experiments  for fair comparison such as popular defacto GAN methods  CycleGAN or Pix2Pix.
410-Paper1513	RefineNet: An Automated Framework to Generate Task and Subject-Specific Brain Parcellations for Resting-State fMRI Analysis	In this paper, the authors introduce RefineNet, a Bayesian-inspired deep network architecture that adjusts region boundaries based on individual functional connectivity profiles. RefineNet uses an iterative voxel reassignment procedure that considers neighborhood information while balancing temporal coherence of the refined parcellation.	The authors developed a novel network architecture that can update parcellation scheme based on resting state information.	This paper proposed a framework for task-specific individualized functional brain parcellation.	They present RefineNet, a new method to optimize the functional brain parcellations  based on the existing parcellations,  wihch is a a Bayesian-inspired deep network architecture.	The network is flexible that it can be used solely to derive the best parcellation for resting state data, or can be used for joint training to optimize some task performance	The framework proposed in this paper integrates the prediction task with the process of individualized parcellation learning, which may benefit to understand the different brain functional organization contributes different prediction task.	The architecture of RefineNet was not cleary described.  And the authors stated that they obtained the optimized functional brain parcellations with higher temporal coherence, in order to validate it, they plotted parcellation cohesion in  Fig.3, but why is the cohesion of RefineNet than that of Combined?	none	"1) The clarity of the paper, especial the method section, could be improved. 2) The method to select the best hyper-parameter configuration are not clearly described. 3) It is not clear the improved accuracy is from the integration of the prediction task with the process of individualized parcellation learning. It seems like the RefineNet-only model cannot consistently improve the performance comparing with group-level parcellation that most of the individualized parcellations claim to outperform. Then, it is necessary to obtain the brain parcellation with the state-of-the-art individualized parcellation method and adopt the corresponding deep learning framework used in the three tasks. Then compare the results with the ""Combined"" proposed in this paper."	The reproducibility of the paper seems not very high, the RefineNet  architecture is not very detailed and clear.	no concern	Good.	"In addition to the above problems, the author should rearrange the sturcture and content of the paper, especially in method and result sections. there is only a Section 3.1 in Section 3, and acctually there are various results in section3.1. The authors should rehearsal expression and description, i.e., in ""The  coherence term S uses the pearson correlation coefficient with each mean time series"", Whose and whose relevance is this?"	"The paper is well written. the results are convincing, particularly when trained for tasks, the cohesion measure for resting state is still higher using the proposed method than the original one. Maybe one super minor comment: the RefineNet by itself is very shallow and may not be called ""deep"" neural network."	"1) Please improve the clarity of this paper, especially the method section.  2) The framework of the proposed method illustrated in Fig. 1 can be improved. Some key words or symbols in Fig. 1, e.g., network weights, ne(v),A_v^j,... are not explained clearly in the main content. It is better to place the definition of s_{v,p} after mu_1,...mu_P and near S. 3) It looks like the experiments did not include all the subject in the respective datasets, please explain the subject exclusive criteria clearly to improve reproducibility. 4) It is better to describe the method of how to select the best hyper-parameters in the proposed method. For example, how to determine that epoch= 5 and I=20.  5) To validate the effectiveness of the proposed method, is it possible to design another comparison to make sure the better accuracy comes from the co-training strategy? In the comparison, first, obtain the brain parcellation with the state-of-the-art individualized parcellation method; then adopt the corresponding deep learning framework used in the three tasks to get the prediction results; At last, compare the results with the ""Combined"" proposed in this paper. 6) An extra ""a"" in the first paragraph of section 2."	This paper provided a method to optimize the functional brain parcellations based on the existing parcellations according to functional profiles,  wihch can be appended to existing networks.	method is novel. writing is clear. results are convincing.	The paper proposes a method for task-based individualized parcellation, which is different with the traditional parcellation framework.
411-Paper2050	Region Proposal Rectification Towards Robust Instance Segmentation of Biological Images	This paper proposes a region proposal rectification (RPR) module which involves two components: a progressive ROIAlign and an attentive feed-forward network (FFN). RPR shows improvement in region proposal location rectification and achieves favorable performances in instance segmentation for both anchor-based and anchor-free approaches (e.g., Mask R-CNN and CenterMask) in three different biological image datasets.	This work tackles the incomplete instance segmentation using existing methods such as Mask RCNN by proposing a novel region proposal rectification (RPR) module that rectifies the region proposal locations with an expanded view.	The authors propose a region proposal rectification module, which includes a progressive ROIAlign module and a self-similarity attention based feed forward network module, to address the issue of bounding box not enclosing the entire object in object detection.	The paper is well organized and easy to follow. The proposed region proposal rectification (RPR) is well motivated and improves performances for both anchor-based and anchor-free approaches.	This work is well motivated with a clear problem setting and analysis of existing works. The method development is lucid and easy to follow with a nice method diagram. The improvement of the proposed method in the experiments is impressive.	The proposed module RPR module enriches the features for bounding box regression by progressively looking at the neighboring areas of the initial proposal and a self-similarity based attention module.The module was evaluated on three datasets using objection detection networks with anchor-based and anchor-free region proposal networks.	I am both fine with the methodology part and the empirical results part. This paper has some novelty and provides new perspectives to redefine the procedure of biological instance mask generation. It's relative good and solid application paper, and I don't see much weakness.	No evaluation experiment on the hyperparameter K. I guess for different datasets, the best K would be different. How many parameters and memory consumption are introduced for the attentive FFN? Since K is relatively small, I guess the memory consumption would be ignorable. However, these details are still expected to be included. If possible, add a section about related works.	The rationale for RPR module was to improve volume and shape quantification. However from the results in Table 1, the ASD improvement for segmentation is lower than that of bbox regression. Only average precision is used as the evaluation metric. Dice coefficient and average surface distance for segmentation task might provide additional information on the value of RPR module.	This paper is reproducible based on the detailed descriptions of the proposed method.	The key idea of this work is simple. Though no code is released, it wouldn't be difficult to reproduce the results on the public datasets.	Sufficient detail provided on the module and the experiments to evaluate the approach.	See the details above.	The weakness I mentioned above might be helpful for the authors to improve their work. I would suggest the authors test on more challenging datasets and if possible give some failure cases of this method. As shown in Table 1, the proposed method does not beat the baseline on every metric. It's interesting to see and analyze some failure cases.	The authors propose an RPR module to improve object detection and thereby improve segmentation quality for better volume and shape estimation. The paper is well written. Given that the improvements expected are small, mainly along excluded areas near the boundary of the object, average precision as the only evaluation might be insufficient. Consider including more metrics like Dice coefficient and Average surface distance / Hausdorff distance. The improvement in AP for segmentation task when using mask RCNN with RPR was <2% in Table 1. However the improvement in IoU in Fig 4a is >20%. Please provide the IoU plot after non max suppression. From Fig.5,the proposed module improves the regression of bounding boxes but the segmentation AP improves only marginally. Suggest looking into other segmentation metrics to understand if the proposed module improves sensitivity. Also, the edges could be weak, making it hard to segment; i.e., good bbox regression might not always improve the segmentation quality.	This paper looks interesting and achieves good results.	This work is well-written with clear motivation, novel method, and impressive results.  The limitations of this work do not hurt the overall quality of this work.	The proposed module is novel and improves bbox regression. However, the segmentation quality improves marginally.
412-Paper2308	Region-guided CycleGANs for Stain Transfer in Whole Slide Images	This paper introduces a GAN-based stain style transfer method for WSI. The method adopts Cycle-GAN as baseline and introduce a ROI-based discriminator in GAN.	"The authors extend the CycleGANs with the proposed ""region of interest discriminator"", naming it Region-guided cycleGAN. The proposed discriminator performs a soft segmentation on the generated stain transferred image. This leads to performance improvement in stain localization. Results are validated on one public (Camelyon16) and one private dataset on which it outperforms the existing methods. The qualitative results are presented on the private dataset, while qualitative and quantitative results are provided for both datasets. The generated binary mask from synthesized DAB stain is compared with the GT mask for qualitative performance evaluation."	This paper proposes a method to transform H&E stains to IHC stains for histopathology images under an unpaired setting. The idea is based on CycleGAN, but instead of using a patchGAN discriminator (from the original CycleGAN approach), it proposes a region-based discriminator. The discriminator takes cell bounding boxes as additional inputs so that the GAN loss is computed on the RoIs, and thus better generation could potentially be achieved.	The paper proposes an interesting application in WSI analysis. The manuscript is well-written and easy to follow.	Strengths: Methodology: Introducing region guidance in the discriminator through segmentation is interesting. The proposed discriminator removes fixed size constraints for the PatchGAN, and can be applied to varying size regions.  As a  natural implication of this modification, the authors have utilized this discriminator for the detection task. This task is also introducing supervision in the GAN that leads to better performance in stain localization as compared to CycleGAN that lacks any supervision. As the quality of the stain transfer depends upon the stain localization, the proposed modifications lead to better quantitative and qualitative results (Fig-2 and Table-1) in comparison to CycleGAN (without any supervision). It also motivates to introduce some sort of supervision for the related applications. Analysis: A detailed analysis is presented on two datasets in terms of qualitative and quantitative performance. The analysis is able to highlight the effect of supervision introduced through the proposed region-guided discriminator in the CycleGAN. Quantitative results (Fig-2) shows the superior stain transfer quality because of region guidance and in turn supervision over vanilla CycleGAN. Similarly, Table-1 shows the better performance of the proposed method as compared to CycleGAN trained under different settings. However, the analysis is limited to one application but it validates the proposed approach. Performance: Qualitative performance shows significant performance improvement over the existing methods (Table-1). The quantitative results also show good performance of the proposed region-guided cycleGAN.	The proposed method is sound. When extra knowledge about the image is available, it is a good idea to find ways to incorporate such knowledge during training. This paper achieves this via a region-based discriminator, and the knowledge is leveraged in the form of bounding boxes. The use of RoIAlign properly consumes the bounding box inputs and induces the generation of more biologically meaningful outputs.	The generation of the bounding boxes for region-guided discriminator needs manual operations. That is, a series of carefully-designed image processing techniques is required. This weakens its feasibility in practice. The experimentation setting is problematic. The introduced method focuses on an ROI-based discriminator that can take patches in various sizes as input. However, in the experimental setting, the authors fix the bounding box size as 48x48. The motivation to introduce an ROI-based discriminator and the experimental setting is conflict. The original patch discriminator also works if the bounding box size is pre-fixed.	Annotation efforts: The method requires additional annotations to train the region-guided discriminator. The authors have used an image processing-based approach for generating the library for training the discriminator. This implies that the quality of the annotations will be a key factor in the performance of the discriminator. Limited application: The usage of the proposed discriminator is provided for a specific application of stain transfer. Can there be other potential applications of this approach?	The clinical effectiveness of this approach is not clear. What is the actual clinical application of this approach? In other words, who will be the eventual consumer of the synthesized IHC images? If it is the medical professionals, then there are missing evaluations of how this method may facilitate the identification of metastatic cells by medical professionals. In particular, how many chances will they find the transferred stains make it easier to spot metastatic cells, and how many chances it may compromise the reading of images? If the synthesized IHC images are going to be used to train models to automatically detect metastatic cells, then there is missing comparison of methods using H&E stains directly. There is missing ablation study on the effectiveness of library generation. The validity of the proposed method largely relies on the correctness of the cell boxes. From Section 3.2, it seems that the generation process is quite heuristic, and thus the generated boxes may not be accurate. What is the actual accuracy of the library generation, and how poor and good cell boxes may affect the performance of stain transfer? There are missing comparisons with baseline ideas. While the proposed method is reasonable, it is unclear how this method is positioned when compared with the straightforward alternatives (if not better, why apply this idea then?). Two possible ideas: 1) applying loss attention masks to the patchGAN map. The loss attention mask is generated from cell boxes, i.e., we have high attention values in box regions and low attention values otherwise. 2) Cropping the input image to the cell box regions (we will also need negative samples, of course) and using the cropped samples to train CycleGAN models. During testing, we could simply use input with larger sizes thanks to the translation invariance of CNN.	The authors claim the reproducibility of the results reported in the paper.	Reproducibility response is followed in the paper.	The core idea of the proposed method is straightforward and can be easily implemented. The implementation details of the CycleGAN are sufficiently discussed. The missing part is the datasets. Although this paper used a public dataset, to train the model it still requires the private IHC images.	Please refer to my comment in the weakness section. In addition,  (1) the authors claim that the original CycleGAN fails for the task. Is there any reason for the failure? Is the failure attributed to the discriminator? The discussion would help to motivate the work. (2) It is suggested to include a discussion on the mis-localizing DAB by the proposed method in figure 2.	I would recommend following for the future work: Extension to other applications Is it possible to make the approach less dependent on the annotations?	I made some suggestions to help improving the experiments of this paper. Please see the Weaknesses section.	My two concerns on bounding boxes (the generation of bounding boxes and their fixed size) weaken the paper's contribution. So I would like to give rejection to the paper before reading the rebuttal.	The proposed modifications improve the baseline as validated by the qualitative and quantitative results on two datasets. A bottleneck in the model performance is due to the annotation quality. However, the utilized annotation pipeline is based on image processing that does not require manual efforts for GT preparation (to train a segmentation model). With this annotation pipeline also, the performance is better in contrast to compared methods. However, the effect of this factor should be explored in future work. Although the analysis is limited to one application, it is sufficient to validate the proposed approach.	I think methodology-wise this paper is good. My concern is the experimental results part which I think should be improved.
413-Paper1821	Regression Metric Loss: Learning a Semantic Representation Space for Medical Images	This paper considers learning semantic representation space for medical image regression tasks. The authors propose a novel regression metric as loss function and use it to low-dimensional manifold that matches high-dimensional labels feature space. The experiment section demonstrates that the proposed loss is better than existing state-of-the-art metrics.	In this paper, the authors present a new metric loss specifically adapted to regression. The specificity of this loss is that it takes into account a semantic aspect related to the data. Results are obtained on two regression tasks based on medical images, propose by the  RSNA Bone Age Assesment Dataset and the NLST CAC score  estimation dataset.	The main contribution of this paper is proposing a novel loss for medical image regression tasks that is the Regression Metric Loss (RM-Loss). This loss could decrease MAE and other indicators to make DNN more robust.	The paper is very well organized and written. I have pleasure to read it. It is clear and self-contained. The RM-Loss proposed in EQs. 2-3 is novel and interesting as it allows to capture interpretable representations and could be optimized with small dataset. The authors provide clear and in-depth analysis with ablations. The results are consistent and the performance achieved by the loss supports the claim about performance superiority.	The proposed loss  appears as pertinent for medical applications in the results section. The methodology also appears as mathematically solid.	This paper provided ablation analysis to clarify the results. And visualization of the learned representation space on provided dataset is given. The organization of this paper is good.	Minor aspects of the paper require better clarification (see details in section 8. below)	The description of the methodology as well as its practical justification however lacks of clarity. This makes the paper difficult to read and its methodological impact not clear enough for a conference like MICCAI. I would not accept this manuscript in its current form, but I would also recommend the authors to give more maturity and clarity to their work, as I believe it could have a good potential. More specific comments are given below.	There is no significant test to valid the proposed loss. And it is not clear that if the results are stable during different runs.	The paper looks reproducible. The hyper-parameters are provided in experiments. Analysis and ablation study is provided in the paper. The DNN architectures used in the paper are mentioned and cited.	The source code will be released on GitHub.	There is no detailed hyperparameter settings and the version of GPUs.	I found EQ.6 somehow ambiguous, given a test sample x_t,  and its label y_t, why the distance only computed on semantic representations fi of training space. what is the relation between f_t and f_i? In fact, Do we need f_t during testing? or maybe f_p is f_t? In supplementary: the radius r used in Fig1. is less than 1. It is unclear why \epsilon=10. Is it a step? It is unclear how the performance is evaluated between r and r+10? while r\in{0,1}. In proof of Lemma 1. What is D? In the case of closed geodesic, the authors claim that the uniqueness is violated. Could the authors elaborate a bit more? Does this condition breaks the bijection property and so global isometry becomes not true?Does this means obtaining multiple representations f in the manifold that match a single label representation - like multiple-to-one correspondence? The ablation on full CAC datasets are somehow difficult to interpret. For example, we observe that not using the mask m was better than using m for same values of sigma and alpha, and when sigma is infinite (the linearity case), the results look comparable to non-linearity. why this behaviour?	In section 1, the claim << To learn a meaningful representation space for regression, a loss should be able to ... margin in loss functions >> is central to justify the methodology but is clearly not enough justified or even simply illustrated by a convincing example. I would recommend the authors to make cristal clear the pertinence of this claim in medical imaging before developing the methodology; In section 1, the sentence << It guides the deep learning model to learn a low-dimensional manifold that has the same semantic meaning as the label, ... >> introduces the notion of << semantic meaning >>, which is not described and far to be obvious when talking about regression. What is the meaning of this notion? In section 2.1, is a $d_t$-dimensional vector, a one-hot encoding representation of different labels? If this is the case, why binary vectors would live in Euclidian spaces? It is impossible to know from Eq. (1) what is actually optimised. The authors should use a  $\hat{\theta} = \arg\min_{\theta} ... $ like formulation of the problem. What is $l'$ in Eq. (2) and where is it used later? Just before Eq. (2) how the sample pairs are selected in a training batch? If I understand well, the loss is computed for a whole mini-batch. Could we use it to compare a single $f_i$ to a $y_i$ (which leads to the information that is usually backpropagated and then averaged into the mini-batch)? Maybe an algorithm explaining how to train a neural-network using this loss would help understanding how this loss can be used in practice. The paper contains many typos. The authors should use a spell checker before submitting the paper, eg: << Various clinical risk or measurement  ...>> << Resent studies ...>> <<...  and E a Euclidean ... >>  ...	"The authors should better pay more attention to the details, 'cause there are some typos in the context. E.X. ""Mean Squared Error (MES)""."	My rating is based on the good quality of the presentations and novelty.	Although the methodology seems interesting, the method description and motivation clearly lacks of clarity	This paper proposed a novel loss to make DNN more valid. But it is lack of more validation analysis to clarify the superiority of the proposed loss.
414-Paper1003	Reinforcement Learning Driven Intra-modal and Inter-modal Representation Learning for 3D Medical Image Classification	The paper presents a novel Reinforcement Learning (RL) driven approach to get semantic, meaningful inter-modality features and complentory inter modality features. This is achieved using dynamic weighting using RNNS.	This paper introduces RL into the intra-modality learning and inter-modality learning and proposes a novel hierarchic feature enhancement framework for multi-modality learning. The results demonstrate its effectiveness.	This paper presents a novel Reinforcement Learning (RL) driven approach to comprehensively address these challenges, where an independent learning mechanism is proposed to choose reliable and informative features within modality and explore complementary representations across modalities with the guidance of dynamic weights.	Paper is well written. All the section of the papers are well explained. The idea of combing the reinforcement learning for inter and intra modality representation learning is novel.	The authors present a  novel RL based method for 3D image classification: A RL module is used to learn the most discriminative features from each modality. Another RL module is used to learn to weight the features from different modalities. Experiments are extensive. The authors not only compared with the several other comparison methods, but also conducted comprehensive ablation studies.	(1) This paper introduces the RL strategy into multi-modality learning. (2) An iterative hybrid-enhancement network is proposed to integrate intra-features and inter-features. (3) The overall structure is clear and well-organized. (4) Experimental results demonstrate the superiority of the proposed method against other existing methods.	1) Experiments are shown only on one dataset making its generalizability a bit limited.  2) The size of the dataset used is too small. 3) No qualitative results are shown on how sematic inter/ intra modality networks are performing.	Major concerns: I totally agree with the challenges of multi-modality learning (paragraph 1 of Introduction). Could the authors show that the proposed method actually addressed the problems, for example, with feature visualization? Is it possible that the accuracy boost is due to the increasing of model sizes? The authors compared with several other models with different model architectures, hence different model complexities. Is it possible that a larger/smaller baseline model can outperform the proposed method? The organization of the paper is very poor. Without reading about the experiment, I don't have any idea what's the task and why it needs to be solved by reinforcement learning. What are the actions of the agents, a continuous number? According to Fig1, the actions of intra-agent is used to modulate an intermediate layer of the modality-specific network. But how? Questions regarding the experiments: The authors preprocessed the input images and only used the lesion area for classification. This is concerning, as the whole process can be regarded as semi-automatic method. The training procedure seems very strange to me. Why three optimizers? Did the authors only used fixed learning rates? If so, is it possible that a different learning rate will greatly boots the prediction accuracy? The authors mentioned that they used 10-fold cross validation. However, they mentioned again that the best model is used to evaluate the testing set. It seems contradictory to me.	(1) For the proposed model, how to obtain the final classification results?  (2) Compared M2Net with the proposed model, they all adopt the modality-specific network and shared network. There, what are the main differences and its advantages? More discussions should be included.  (3) Multi-modality fusion is a wide research topic, thus the related fusion methods and multi-modality learning algorithms should be discussed. (4) The dataset looks small, with only 165 subjects.	The method is generic mathamatically but the reproducibility is not depicted in the paper. With different dataset and task other challenges may show up which makes it hard to believe that the method is easily reproducible.	The authors will open source the code. It's reproducible.	The dataset is public. The authors mention in reproducibility statement that they will release code and trained models after acceptance	1) May be discuss some qualitative results.  2) In table 2, please make the best performing number as bold. 3) May be add another dataset in future.	Please properly motivate the use of RL for classification and show how it addressed the mentioned challenges. 2, This paper is not properly motivated. After introducing related works, the authors suddenly jump to the discuss of the novelties. I am kind of lost why RL for classification. Why it can be used to address the mentioned challenges? Please properly formulate the problem before introducing the details model structure. Are the comparison methods the state-of-the-arts? If not, please compare with the top methods for the open challenges.	(1) For the proposed model, how to obtain the final classification results?  (2) Compared M2Net with the proposed model, they all adopt the modality-specific network and shared network. There, what are the main differences and its advantages? More discussions should be included.  (3) The dataset looks small, with only 165 subjects. This is a huge limitation in medical imaging, and it is an issue of the data for other researchers, thus the small sample issues and some related works should be discussed. It is also expected to discuss where the proposed model fails to predict some samples.	The idea is novel and strong. Paper is well written.	This paper doesn't meet the standard of MICCAI yet.	This paper innovatively introduces the reinforcement learning strategy into the intra-modality learning and inter-modality learning and also presents a novel hierarchic feature enhancement framework for multi-modality learning.
415-Paper1608	Reinforcement learning for active modality selection during diagnosis	The paper introduces an RL based method to select modalities and input information given a constrained budget	This paper works on active modality selection for clinical diagnosis and proposes a reinforcement learning (RL) formulation to maximize the accuracy/ cost balance. The proposed Q value-based RL algorithm can actively select the next modality or end the examination to get the diagnosis for a specific patient. Experiments are conducted on a heart disease dataset and an echocardiographic hypertension dataset and show that the proposed RL algorithm is better than the population-based selection method.	This works presents a reinforcement learning (RL) strategy for modality selection during diagnosis, which accounts for both diagnosis accuracy and modality-specific acquisition cost. The authors have shown the validity of the proposed approach in two datasets (one public and one private), demonstrating the clinical utility of RL over population-wise feature selection.	The motivation and writing is clear-  The method is interesting and well supported	The paper works on an interesting topic that can actively help select the next exam modality or end the examination to get the diagnosis and minimize the cost while maintaining high accuracy. The topic is promising as the decision is made for each individual patient. The decision-making process is formulated into an RL problem and solvable by traditional RL algorithms. The authors validate the proposed method's effectiveness on two datasets and its superiority over the population-based selection method. The authors show that the proposed decision-making framework can indicate which modality/ bio-marker is important for diagnosing on a population level.	Novelty: the authors present a novel RL method that extends their previous strategy (ref. [1]) into a modality- and cost-aware method able to handle high dimensional data, further enhanced with strategies to avoid data sampling imputation.  Clinical interpretability: the authors have thoroughly analyzed the results of their strategy with respect to the clinical knowledge of the selected application (hypertension), showing the validity (and clinical utility) of the decisions made by the RL method.	Not that extensive comparison with related literature in theoretical and practical terms. Major assumption that each new modality will give information that is beneficial for the diagnosis and that there is no overlap in these potential new bits of information. A causal analysis of the necessity and sufficiency of the inclusion of the modalities	The scope of the study: The authors simplify the usable modalities to be represented as valued vectors. However, it is non-trivial to get these representation vectors for some common modalities in real practice, such as images. The number of selected modalities is small, which may be solved by heuristics-based methods. Meanwhile, the cost for the modality specified by the authors is arbitrary. Hence, it is trivial to make a comparison. The decision-making process is quite unclear as the method can only get some value numbers for each modality and does not get any meaningful interpretation for a specific patient. The discussion by the authors in 4.3 is result-driven. Given the simplified assumptions, this work is more of a proof-of-concept. The work needs to be carefully designed for more complex real-world problems.	SOTA comparison: Despite having adequality shown the superiority of their strategy with respect to a simpler population-wise feature selection (both in terms of prediction error and stability, which I acknowledge them for), comparison with other SOTA approaches were not included (namely those mentioned in Section 1 based on patient-specific feature selection, etc.).	Code and experimental settings provided - very good	The authors have provided implementation details with supplementary code. The evaluation is performed on a public dataset and a private dataset. The authors are encouraged to make their data and code available.	No major concern regarding reproducibility.	This is a very well written and interesting paper , nice work !	The class predictions given the current state are computed using support vector machine classifiers. However, the authors do not evaluate the classifier's quality, thus may lead to some bias in the final results. Intuitively, the reinforcement learning algorithm can probe these classifiers during training, which can learn some shortcuts from them. Further, a classifier needs to be learned for each superstate, resulting in many classifiers. In Fig. 1, the authors can show the meaning of each point and explain why the number of points is different for reinforcement learning and the population-based method. In Fig. 2, the authors can explain how the Dice coefficient is computed. The authors can elaborate on why the modality selection can be formulated as a Markov decision process (MDP). Typically, reinforcement learning has a parameter gamma in the MDP formulation, and the authors choose to set the gamma to 1. The authors can give some discussion on it. The authors can give a more precise description of s_{n+1} given s_n and a in section 2.2. The authors can further refine the paper writing and organization, e.g., abstract, notations.	"Please consider increasing the opacity of the plots in Fig. 4 for better visualization. Please comment on the high similarity between GLS curves across individuals of all groups (and why higher importance was given in the third group). Small typos were found throughout the manuscript (e.g. in page 3, twice appears ""an MDP"" rather than ""a MDP""). Please revise it."	This is a good paper that would benefit the community - provided there is some extra justification on the limitations of this approach the paper should be accepted	The paper works on an interesting diagnosis decision-making problem. However, the paper is limited in terms of modality scopes and experiment clearness.	The manuscript presents sufficient methodological novelty in a clinically relevant task. It is well written, presents adequate experiments and a great clinical analysis/discussion of the method and its results.
416-Paper2249	Reliability of quantification estimates in MR Spectroscopy: CNNs vs. traditional model fitting	This paper does a systematic comparison between CNNs and traditional model fitting for MR spectroscopy quantification, and identifies major concerns about the CNN approach.	This is a simulation study highlighting bias in the estimation of model parameters and uncertainties when using neural networks with MR spectroscopy data.	The main contribution of this paper is to conduct a synthetic study of the robustness, in terms of bias and variance (due to aleatoric and epistemic causes), of a CNN trained for metabolite quantification for MR spectroscopy. The main conclusion is that the CNN displays significant bias and variance (in both noisy and noiseless data) which depends on the parameter value; predictions for parameter values near the bounds of the generated data display more bias/variance. In contrast, they find that traditional model fitting shows no parameter value dependence with respect to bias/variance of estimates.	The paper is well written and raises awareness of a previously-unknown issue for the application of deep learning methods to MR spectroscopy.	A solid simulation study nicely executed with clear hypotheses, results and conclusions. This is timely work extending recent demonstrations in other quantitative imaging scenarios that significant bias can arise when using regression type methods, e.g. CNNs as here, to estimate model parameters from high dimensional data.  The approach is appealing, as it can be much faster than traditional model fitting and potentially avoid problems like local minima. However, care is required to select an appropriate training set and even then it appears difficult to avoid bias towards the mean particularly for rare cases.	The main strength of this work is that it provides a valuable counterpoint to previous studies on deep learning quantification for metabolite quantification, showing the possible pitfalls of deep learning in comparison to traditional model fitting algorithms. The analysis of bias and variance were extensive.	The investigation was somewhat limited in scope - however, I do not perceive this as a major limitation, as the authors have clearly identified this limitation in the paper, and have provided a roadmap for further investigations. The quantification network seems to be based on generic (non-spectroscopic) references [25,26], but it is not clear how these methods compare to deep learning methods designed specifically for spectroscopy like those in [10,11,12]. It would have been nice to see this same evaluation on additional methods, since not all deep learning methods are expected to behave in the same way.	The application area is quite niche - model-based spectroscopic MRI - but nevertheless of interest to some MICCAI attendees and the problems and conclusions highlighted are more broadly important (they have been shown before in diffusion MRI - ref [9] in the paper - but the confirmation in a different application is valuable). The study is simulation only.  A demonstration of how the issue might manifest using real data sets would have added a lot to the paper.	The main weakness of this paper is the lack of an example on actual MRS data (whether in-vivo or a phantom). I am aware that this is a synthetic study, but just a small real-world example of the DL failing in a pathological case would have greatly strengthened the paper. Furthermore, one of the papers cited as an example of deep learning quantification [10] also conducts an uncertainty analysis, and concludes favorably on the deep learning method. I think it is necessary to address this/discuss the potential contradiction.	Code and data is not included, but the paper is reasonably detailed and relies on simulations, so it should be possible to approximately replicate the study.	Seems fine.	I am not sure how difficult it would be to reproduce the synthetic dataset used for training; however, the training of the networks (given the data) seems to have been described sufficiently in detail for reproducibility (although the learning rate is not specified, I assume this means ADAM with the default learning rate).	This work gives a nice warning about the potential pitfalls of relying completely on deep learning for MR spectroscopy.  There may be value in discussing the link between this observation and similar observations that have been made in the context of MR imaging: Antun et al, On instabilities of deep learning in image reconstruction and the potential costs of AI. PNAS 2020 Chan et al, Local Perturbation Responses and Checkerboard Tests: Characterization tools for nonlinear MRI methods. MRM 2021. There is very likely a similar underlying principle	A few specific comments: Regarding estimation of the different types of uncertainty, one approach the authors don't consider is to train the network specifically to output an estimate of uncertainty cf. (Tanno et al NeuroImage 2021). The authors might consider that when extending this work. Figure 7 it's a little hard to decipher.  Could be reorganised to make clearer what is experimental result and what is ground truth.	As perhaps a naive question (but one which could be addressed in the paper), what if one simply simulates an extremely large range of concentrations such that one is sure that these concentrations will never be reached. Would this perhaps reduce the variance but increase the bias? It would be interesting to see the model retrained with different data where the parameter bounds are increased. As mentioned, the study is restricted to a single CNN architecture which was found optimal after a hyperparameter search. If expanded to a journal, I think it would also be worth it to reproduce the architectures of previously published studies. Furthermore, I think it is even worth it to discuss whether a CNN is the optimal architecture for inference from spectrograms (see e.g. https://towardsdatascience.com/whats-wrong-with-spectrograms-and-cnns-for-audio-processing-311377d7ccd, which applies to sound spectrograms but some of the criticisms are transferable). It is possible that an architecture more tailored to the data may alleviate some of the problems noted in the paper. In general, the DPI/resolution of the figures are quite low, and detract somewhat from the paper while reading/zooming in. For the future, I would suggest to remake the figures in a higher resolution.	This paper sheds light on a previously-unrecognized major problem with one of the popular new approaches for MR spectroscopy quantification.  This has the potential to be very impactful	It's a solid piece of work, albeit a little niche and simulation only.	I found the work to be novel and detailed in the analysis, although limited in scope (single architecture, synthetic dataset). I think it is a good fit for MICCAI and a valuable contribution for addressing potential weaknesses in deep learning approaches compared to traditional methods.
417-Paper0724	Reliability-aware Contrastive Self-ensembling for Semi-supervised Medical Image Classification	The paper describes a semi-supervised classification method based on reliability analysis. MT method is used to contrastively analysis the reliability of the classification results, which is then used as the fake-label for fine training. The proposed method achieves the SOTA performances.	The manuscript presents a novel reliability-aware contrastive self-ensembling framework, which can leverage the reliable unlabeled data selectively. The authors introduce a weight function to the mean teacher paradigm for mapping the probability predictions of unlabeled data to corresponding weights that reflect their reliability and also design a novel reliable contrastive loss to achieve better intra-class compactness and inter-class separability for the normalized embeddings derived from related unlabeled data. Extensive experiments are conducted on two public datasets to verify the effectiveness of the proposed method.	This paper aims to effectively use unlabeled data for semi-supervised medical image classification. The challenge in using the unlabeled data is that they can be acquired from different populations or equipment, which may result in difference between these data. To address this challenge, this paper proposes to assign different unlabeled data with difference weights, instead of assigning equal weights. The weight function is learned together with the consistency loss and contrastive loss. Experimental results on two datasets show improved accuracy over other semi-supervised learning methods.	Detailed formulation of the problem and methods, formulas are easy to understand and follow. Corresponding codes are also published for accessing. Bi-level optimization is addressed properly, which is interesting to explore in the other different tasks Solid experiments and analysis	The proposed method is novel since it can concurrently capture both the reliable data-level and data-structure-level information of the images, thereby improving the robustness and generalization power of the model. The proposed method achieves state-of-the-art performance on two public datasets.	The idea of assigning each data different weights by a learned model is novel. The paper is well-written and easy to follow. I enjoy reading this paper. By employing the proposed weight function in the consistency loss and contrastive loss, improved accuracy is observed on two datasets.	For me, it is a nice miccai paper submission without obvious weakness	Some details about the experiments are missing, which should be added in the revision.	While this papers describes the proposed learnable weight function from the perspective of reliability, no evidence or evaluation of how this weight function improves the reliability is demonstrated. Instead, only the accuracy metrics are used.	Author provided code for reproducing the results	The reproducibility of the method is good, since the author provide us with almost all implementation details and the code is also released on the Github.	The experimental setup such as data pre-processing and learning hyper-parameters are provided in detail. These descriptions are sufficient to reproduce the reported results.	Maybe in the future, it is also interesting to see the results of the segmentation tasks.	"(1) There is a typo in ""Input of min-batch of images"" in Fig.1, please correct it. (2) How many training iterations are operated between the update of the parameter of the weight function and the network parameters on two datasets? (3) Which dataset is used to conduct the ablation study to investigate the role of each component in RAC-MT. Please specify it in the manuscript. (4) Please discuss the limitation of the proposed method and the possible solutions/future directions in the manuscript."	It could be better to replace the word reliability with words such as importance. The learned weight function is novel, but is not designed for data reliability. Could you also include comparison with contrastive learning based method such as SimCLR on the skin dataset, following the protocol of contrastive pre-training and fine-tuning with labeled data? From the ablation study in Fig. 2, without using the weight function, the simple combination of CST-MT (consistency loss and contrastive loss without weighting) already achieve a good accuracy, which means the improvement by the weighting function is not as effective as it appears. Could you explain more about this result?	Well organized and explained paper, for me it is wonderful	The proposed method is of great novelty, and the experiments are extensive and sufficient. The proposed method achieves very good performance on two datasets, outperforming compared methods significantly. The manuscript is well organized, clear and easy to follow.	The proposed learnable weighting function for each data is novel. Applying this weight to the consistency and contrastive loss improves the overall model performance.
418-Paper0235	ReMix: A General and Efficient Framework for Multiple Instance Learning based Whole Slide Image Classification	This work introduces a novel framework coined 'ReMix' for whole slide image (WSI) classification that leverages latent space augmentation (LA) on WSI instance cluster prototypes under the multiple instance learning (MIL) paradigm. WSI bags are reduced by replacing instances with cluster prototypes, enabling MIL parallelization, with several LA augmentation strategies applied to the prototypes facilitating generalization. The work is well motivated and provides extensive experiments on two public datasets showing very competitive results. Also, the introduced framework is agnostic to existing state-of-the-art MIL models - highly scalable and has plug-and-play functionality.	This paper propose ReMix, a general and efficient framework for WSI classification, which has two steps, reduce and mix. ReMix is evaluated on two public datasets and experimental results have shown its effectiveness and efficiency.	To address the WSI classification with high resource consumption, authors propose a simple yet effective MIL framework with two steps. In the first reduce step, the centroids at the feature space serve as instances for MIL, instead of original patches. Then, the mix step augments the reduced instances to regularize the training of the whole MIL framework. With the help of advanced pretraining for high-quality instances, the proposed ReMix framework can complete the WSI classification efficiently.	The paper is easy to read and well-motivated.  The use of latent space augmentation for Histopathology is novel and addresses a relevant problem for WSI analysis in healthcare. The authors report competitive results on public datasets with significant gains over recent methods. Augmentation strategies for WSI classification are often under-explored, thus; the use of LA augmentation and it's variants is in this work very interesting. Especially in the multi-class setting when considering different pathologies, LA facilitates better model generalization. I appropriate the extensive experiments to validate LA, including ablations on computational efficiency and several hyper-parameters regarding.	A general, simple yet effective method to improv the training efficiency of MIL framework for WSI classification. An efficient latent augmentation for MIL-based WSI classification. Improved performance can be observed on two public datasets.	This work aims to address the resource consumption of MIL framework, which is a bottleneck in WSI classification. From the experiments, the proposed ReMix can improve the performance and reduce the resource cost at the same time. The proposed reduce and mix steps are simple yet effective, and can be easily extended to existing MIL framework. Significant improvements for two baselines on two datasets. The work is well written.	While the authors ablate each of the proposed augmentations, it is unclear which augmentation is more useful in a general sense i.e., the considered baselines (ABMIL,DSMIL) show gains with different augmentations. 'Mix-the-bag' is a key contribution of this work. However, results only show isolated evaluations on different components of the strategy. It would be beneficial to have included 'Mix' i.e., all combinations of the augmentations ( append + replace + inter. + covary ) in the evaluation to better assess the generality of the idea. It is unclear how such augmentations would work with recent works that have spatial WSI reasoning such as TransMIL [1]. This work should have been included in the evaluation to better hightlight potential failure cases. [1] Shao et al. Transmil: Transformer based correlated multiple instance learning for whole slide image classification. NeurIPS (2021)	The novelty is limited, either Reduce or Mix. They are somewhat straight-forward processing strategies which have been used in WSI-related application before. See my detailed comments below. No comprehensive comparisons with other MIL baselines. Not easy to be used in the practice.	The training budgets comparison in Table 2 may be over-claimed, ignoring the cost of necessary pretraining. The reduce step may lose the spatial information, which is important for specific WSI tasks.	"Authors checked ""Yes"" for most questions on the reproducibility of the paper"	The authors claimed the code will be made available.	Hyper-parameters are complete. Authors are going to release the code.	I am bit concerned the choice of baselines (ABMIL,DSMIL) is not very indicative of the generality of this method; though I do understand the limited space in the manuscript can be a factor. To expand on this, from a technical standpoint, DSMIL implicitly computes prototypes and thus LA could be directly applied with without resorting to K-means cluster learning as a prior step. Is there a reason only these baselines were employed? It would be interesting to see whether ReMiX can work with recent methods such as TransMIL [1].  Especially given that ReMix-DSMIL and it's variants had marginal improvements on CAMELYON-16 over the baseline.	The proposed model contains two parts, reduce and mix. Though it somewhat looks like interesting, novelty is still limited. Reduce part uses K-means on the patches' representations to obtain K clusters as prototypes to represent the bag. Such similar idea has been proposed in the previous WSI work, like WSISA. In addition, Mix part includes latent augmentation and the technique behinds such part does not sound novel. WSISA: Making Survival Prediction from Whole Slide Histopathological Images, CVPR 2017 Though two baselines are compared, it is not clear how are they implemented. Does the author implement the baseline models following the original setting? Also, several new WSI-related MIL methods are not compared.  CLAM: A Deep-Learning-based Pipeline for Data Efficient and Weakly Supervised Whole-Slide-level Analysis, Nature Biomedical Engineering, 2021. It is not easy to use the proposed model in practice. From Table 1, we could see different augmentation strategies achieve different results. How to decide the one for use is not clear and easy. Another concern is the lack of interpretability of the proposed model. Many MIL-based models show attention maps on patches/instances, it seems there is not applicable for the proposed model to have such visualizations.	"The training budgets comparison may be over-claimed. The success of reduce and mix depend on the high-quality pretraining, which would demand a lot of resources and time. However, the comparison in Table 2 excludes the time and memory demand of pretraining, which is not fair to SOTA baselines. Moreover, compared with the training budgets, the resource cost of inference is more significant for the practical applications. The summary of existing WSI works that reduce the resource consumption may be improved. For example, [1] randomly samples specific number of patches as an augmentation of bags, and [2] utilizes the attentive regions with a sparse tree. Authors are suggested to discuss the differences or advantages over these works. [1] https://dblp.org/rec/conf/cvpr/HashimotoFKTKKN20 [2] https://dblp.org/rec/conf/aaai/0013ZCHHY21 For some WSI tasks related to regional proportion (e.g., HER2 scoring), the reduce step may lose necessary spatial information of massive patches, which would restrict the performance. Authors are suggested to discuss potential limitations of the method. In abstract, ""the descent performance of deep learning comes from harnessing massive datasets"" is a little confusing. Please check the description."	Overall, this work provides strong empirical evidence and has no major weaknesses. It would be interesting to see more comparisons with other recent MIL methods, however the work in its current form is sufficient to be considered at MICCAI. I believe the research community working in WSI can benefit from this simple and effective technique.	The authors proposed an efficient ReMix for WSI classification. Though promising results have been shown, the novelty is limited, and it looks like an add-on model based on existing MIL methods. Also, the lack of interpretability and selection of augmentation technique will make it more challenging in practice.	This work address reduce high resource consumption for WSI classification. The novelty of this work is enough for the conference. Significant performance improvements.
419-Paper1023	RemixFormer: A Transformer Model for Precision Skin Tumor Differential Diagnosis via Multi-modal Imaging and Non-imaging Data	This paper proposed a transformer based multi-modality classification framework for skin tumors to simulate the diagnostic process of dermatologists in realistic situations. The proposed method achieved SOTA results.	The paper proposes a disease- wise pairing of all accessible patient data. Further a cross-modal fusion module is also proposed and integrated with a transformer based multi-modality fusion module.	A cross-modality-fusion module integrated with transformer-based multi-modality deep classification framework that can fuse multi-source data (i.e., clinical images, dermoscopic images and accompanied with clinical patient-wise metadata) for skin tumors. Validation on 1011 cases.	As above	1)The idea of multi-modal fusion with transformer is smart.  2) Experiments show superiority of the model in terms of F1 score and Accuracy.	a new multi-modal cross-fusion transformer for multi-modality data fusion Disease-wise Pairing as Augmentation to address the problems of missing modality. a new cross-modality fusion model to use global features solid experiments	"However, there are some places that are not clearly expressed. In Section 2.2, ""When DWP is turned on (based on p > Tp, p  [0,1])"", what does p mean here? How is the p obtained? What are the global features and local features? In fig. 2, what does patch token mean? It's never shown in the main text. Section 2.3 is very confusing.  For example, the authors mentioned gc or gd are generated by GAP layer, gm is generated by LN layer. But in the formulas below, it's zx and gx' that are generated by LN and GAP. I don't know if the gx' here represents the same gc, gd, gm mentioned before, or it means something else.  Also, I suggest to add the notations lc, ld, and lm(if there is one) to the figure 2."	1) Although the chosen problem is exciting, technical novelty is still missing. Authors use state of the art Swin Transformer for a given task. 2) The proposed architecture seems to have multiple branches for each modality, however the computation complexity, number of parameters are not discussed.  3) In table 1, the second best performing network would be Inception-comb. With respect to this the performance exceeds by 5.5% in terms of average accuracy and not 12 %. 4) Augmentation might not be clinically sound.  5) Experiments sound a bit weak. 6) May be please add significance test to check the significance of the results.	It is hard to reproduce the method as lots of details are missing in Figure 2. How patch are embeded? What is RS? What is M? parameter settings of detailed architecture not disclosed.	"However, there are some places that are not clearly expressed. In Section 2.2, ""When DWP is turned on (based on p > Tp, p  [0,1])"", what does p mean here? How is the p obtained? What are the global features and local features? In fig. 2, what does patch token mean? It's never shown in the main text. Section 2.3 is very confusing.  For example, the authors mentioned gc or gd are generated by GAP layer, gm is generated by LN layer. But in the formulas below, it's zx and gx' that are generated by LN and GAP. I don't know if the gx' here represents the same gc, gd, gm mentioned before, or it means something else.  Also, I suggest to add the notations lc, ld, and lm(if there is one) to the figure 2."	The model looks computationally expensive, hence technically it should be reproducable. This claim is supported by experiments on two dataset for one application. Being said that, it is also the model seems computationally expensive.	It is very difficult to reproduce the paper as the abbreviations in the flow chart, and detailed parameter settings of the architecutre are now disclosed in full detail. It is suggest to release the code, at least the model architecture part.	"However, there are some places that are not clearly expressed. In Section 2.2, ""When DWP is turned on (based on p > Tp, p  [0,1])"", what does p mean here? How is the p obtained? What are the global features and local features? In fig. 2, what does patch token mean? It's never shown in the main text. Section 2.3 is very confusing.  For example, the authors mentioned gc or gd are generated by GAP layer, gm is generated by LN layer. But in the formulas below, it's zx and gx' that are generated by LN and GAP. I don't know if the gx' here represents the same gc, gd, gm mentioned before, or it means something else.  Also, I suggest to add the notations lc, ld, and lm(if there is one) to the figure 2."	1) Although the chosen problem is exciting, technical novelty is still missing. Authors use state of the art Swin Transformer for a given task. 2) The proposed architecture seems to have multiple branches for each modality, however the computation complexity, number of parameters are not discussed.  3) In table 1, the second best performing network would be Inception-comb. With respect to this the performance exceeds by 5.5% in terms of average accuracy and not 12 %. 4) Augmentation might not be clinically sound.  5) Experiments sound a bit weak. 6) May be please add significance test to check the significance of the results.	To demonstrate the contributions in addressing missing modality, it is suggested to give details of percentage of missing data. Besides, experiments should include validations on different percentages of missing data and modality.	"However, there are some places that are not clearly expressed. In Section 2.2, ""When DWP is turned on (based on p > Tp, p  [0,1])"", what does p mean here? How is the p obtained? What are the global features and local features? In fig. 2, what does patch token mean? It's never shown in the main text. Section 2.3 is very confusing.  For example, the authors mentioned gc or gd are generated by GAP layer, gm is generated by LN layer. But in the formulas below, it's zx and gx' that are generated by LN and GAP. I don't know if the gx' here represents the same gc, gd, gm mentioned before, or it means something else.  Also, I suggest to add the notations lc, ld, and lm(if there is one) to the figure 2."	The paper show an exmaple of how to use a transformer based model for nulti-modal data fusion. Experiments are performed two datasets. But technical novelty is missing, so I weakly accept it.	The paper has clinical application metrits and technical innovations. The paper will benefit the research community. The only concern is the reproducibility. If the authors could release the code, that would be great.
420-Paper0090	Removal of Confounders via Invariant Risk Minimization for Medical Diagnosis	In this work, the Authors propose a variant to the framework of Invariant Risk Minimization (IRM) to reduce/remove the effect of confounders in an X-ray classification task with binary class labels. Specifically, the Authors create ad-hoc IRM environments, based on confounders values, to reduce their effect. The main contribution of this work is the use of the IRM framework to remove confounders; a second contribution is the application to a medical imaging task of X-ray classification. Differently from IRM, the proposed method (ReConfirm) introduces class-conditional penalties to improve stability of features across class and promote feature diversity. Experiments are presented to support the claims.	The paper presents a modified invariant risk minimization (IRM) , namely ReConfirm to remove the effect of the confounders.  The proposed method were applied to NIH chest X-ray classification tasks where sex and age are confounders. The experimental results outperforms baseline CNN models trained under the traditional empirical risk minimization framework.	A learning strategy based on the invariant risk minimization framework [4] is proposed for medical image classification, such that the classification can be done without reliance on confounding variables such as age or sex. The main idea from [7] is used to define training environments, based on agreement between the known confounding variable and the class label. The original loss function of [4] is extended to include class-conditional penalties. This potentially allows the model to learn different environment-invariant representations for each class.	The paper is very clearly written and tackle an interesting problem: removal of confounders in nonlinear models. The use of the IRM framework is very welcome in this community and the proposed variants have sound explanations and descriptions. The two experiments - sex as confounder, age as confounder - are compelling.	This work proposed a modified IRM framework, namely ReConfirm to accommodate class conditional variants for NIH chest X-ray classification tasks , where the invariance learning penalty is conditioned on each class. This work designed a strategy for optimally splitting the dataset into different environments based on the maximum violation of the invariant learning principle.	An important problem is considered, and a viable solution is presented for the same. I have not seen application of the invariant risk minimization framework to medical image analysis before. For situations where the exact confounding variables are known, the proposed method seems to be promising. Writing is fairly clear. Experiments with two confounding variables (age and sex) show that the proposed method improves performance over empirical risk minimization.	The numerical improvements of the proposed ReConfirm are sometimes marginal with respect to the traditional Empirical Risk minimization principle, so further testing could improve the result of the experiments, especially in view of a future submission to a journal.	The comparative studies seems limited. More datasets and backbones like transformer can be added to verify the generalization ability of ReConfirm. More existing methods could be included for comparisons.	The evaluation of the method is weak, in my opinion. In particular, comparison with adversarial learning based invariant representation learning would have been useful. I suspect that with the used environment definition strategy, such invariant representation learning methods would also work quite well. The effect of the class conditional penalties is unclear. Although I understand the intuitive motivation of allowing learning of class conditional invariant representations, I do not understand why one would apply such a penalty to only one of the classes in question. Further, I could not find a satisfactory explanation as to why the setting with the penalty on only the control class (cReConfirm y=0) leads to the best performance most of the time.	In the manuscript, the Authors conduct experiments on a publicly available dataset and they claim to publish their code, at a later stage. The procedures explained in the manuscript looks sufficiently detailed to attempt the reproduction of the results presented in the article. Unfortunately, the reproducibility statement given by the Authors look vastly incomplete.	Good. The author will open source the codes for research purposes.	The authors have agreed to make code publicly available after the review period. The data used in the experiments is from publicly available datasets.	"The manuscript is pretty good. I'd suggest the Authors to use more intuition and examples to explain their concepts and keep the formal description only after that. Especially for a future extension to a journal article, I invite the author to present the topic of removing confounders also from a more historical perspective - which is the one related to linear models - to guide the audience in this very interesting topic. Minor: it is not clear why the proposed method is specifically called ""ReConfirm""."	The paper presents a modified  invariant risk minimization (IRM) to remove the effect of the confounders. The proposed method were applied to NIH chest X-ray classification tasks where sex and age are confounders. The experimental results outperforms baseline CNN models trained under the traditional empirical risk minimization framework. This work proposed a modified IRM framework to accommodate class conditional variants for NIH chest X-ray classification tasks , where the invariance learning penalty is conditioned on each class. This work designed a strategy for optimally splitting the dataset into different environments based on the maximum violation of the invariant learning principle. However, the comparative studies seems very limited. I would like to suggest: More datasets and backbones like transformer can be added to verify the generalization ability of ReConfirm. More existing methods could be included for comparisons.	A big limitation of the paper, in my opinion, is the lack of comparison to invariant-feature learning methods from the domain adaptation literature (e.g. [22]). This is especially the case as the experiments done in the paper are in cases where the confounding variables are known in advance. A discussion on the limitations of the invariant risk minimization framework (e.g. Rosenfeld et al. The Risks of Invariant Risk Minimization, ICLR 2021) would have been useful. In particular, it is unclear why the proposed method should work if all confounding variables are not taken into consideration during the training.	The manuscript is very clear and the proposed solution is appealing. The experiments are interesting and the contributions are important. The manuscript has only very minor issues. The main drawback - which is understandable for a conference article - is the experiment and results section that could be more extensive. But again, it is a minor issue.	Removing the effect of confounder variables is an interesting research area.	The paper's treatment of confounding variables in the medical image analysis literature is timely, and the usage of the invariant risk minimization framework is interesting. Although I have concerns regarding the class-conditional penalty's applicability and the comparison with respect to invariant feature representation learning methods, I will argue that the paper is strong enough to be presented at the conference.
421-Paper2844	RepsNet: Combining Vision with Language for Automated Medical Reports	Paper attempts to address two applications: (a) classification of answers for given questions in VQA-Rad dataset and (b) text generation task in IU dataset.	This paper proposes an approach for generating clinical reports using an image/text encoder-decoder model. Notably, it comprises a bi-linear attention network for image/text fusion, and a self-supervised contrastive alignment with NL descriptions. Results are provided on medical visual question and radiology report generation.	This paper presents an encoder-decoder method that combines two modalities (medical image and text) for medical visual question answering. The method consists of the contrastive image-text encoder and conditional language decoder. Experiments are performed on two public VQA datasets(Med-Rad and IU-Xray).	Prior context knowledge for open ended answers is an interesting idea.  Contrastive loss for vision and text learning is also a good idea. Paper is well written and understandable as well as reproducible. Huge experimentation as well as a demo is shown in supplementary material.	The paper presents an important and original contribution. The work appears to be technically sound and well-thought. Results are well presented.	This work introduces contrastive learning into feature representation learning; The proposed method combines medical images and text for multi-tasks ( categorical and descriptive natural language answers).	Novelty in terms of network design is limited but the proposed methodology is new. Comparative in terms of space and time is missing. Network training needs more elaboration.	The transfer of the developed methodology to clinical practice remains uncertain. Limitations of the work are not illustrated.	Most parts of the proposed method are a combination of existing approaches: bilinear attention network (Kim 2018), contrastive vision and language learning (Chen 2020), and prior context knowledge (Johnson 2017). Experimental results:   1) No BLEU evaluation results on Med-rad 2019. The results only reported accuracy evaluation in Tabel I on Med-rad.    2) The experimental setting of Med-rad is not clearly stated performed on the validation set or the test set.    3). The author claims their RepsNet for medical reports generation task. However, the authors only performed generation task evaluation on the IU-Xray dataset.	Reproducible with some efforts.	It appears that authors meet the requirements for the reproducibility.	Is the reported CLEF (Abacha 2019) results of Tab.1 evaluated on the test set of  Med-rad 2019? Are the author's results evaluated on the validation set of  Med-rad 2019?	Few other questions need to be addressed: There is no mention of questions and answers in the IU dataset? Where is misalignment defined before generating a heat map? How to address data bias as abnormal patients being way more than normal patients?	The transfer of the developed methodology to clinical practice remains uncertain. What are the limitations of your approach? How would this approach integrate into a clinical routine? What are the difficulties? What is the uncertainty associated with the prediction?	Please improve the manuscript clarity, i.e., the results discussions, motivation, etc. Please check the consistency of notation system, i.e., the decoder part Equation 5 with conditional inputs {y, \had(X), \had(C) } and Fig.2  with inputs  {\had(X), \had(C), \had(Q), \had(Y) }	It is a nice application.	The gap between technical pipeline and transfer of it to the clinical practice.	The method is not attractive, and Med-rad results are not convincing enough.
422-Paper1632	Residual Wavelon Convolutional Networks for Characterization of Disease Response on MRI	The main contribution of the paper is a deep learning framework using wavelets as activation functions and short-cuts within wavelon network blocks for residual learning. The authors applied the framework to three data sets, two for prediction tasks in rectal cancer, and one for a prediction task in Crohn's disease.	This paper proposes a RWCN method, and it is an efficient utilization of wavelet functions as activation unit for convolution response.	The author develope a new architecture form networks, layers and activation perspective using wavelet theory. They evaluate their outperform in different cohorts and topics. They deliver a variation of different network comparison in each topic.	The main strengths of the paper are that it is written very clearly, the rationale for the approach seems fundamentally sound, and there is a dedicated experiment and set of results on the optimization of the skip connection weights, which is critical for understanding the impact of those connections.	RWCN solves the problem of gradient disappearance	Novelty in architecture, design in network to layer and activation level. A very well organise manuscript with very sientific evaluation of the hypothesis. Nice Figures and verification of the ideas.	There is already a body of literature on wavelon networks, and the addition of short-cuts for residual learn is not very novel, but is worth investigating. Overall, the results produced by the best wavelon residual network (RWCN-ResNet-15) are only slightly better than the best conventional CNN (ResNet-50). Also, the numbers in Table 2 and Table S-1 are generally quite high, which indicates the chosen clinical tasks may not be challenging enough for the RWCNs to demonstrate their theoretical advantages. The three data sets used for the experiments are small and the results presented cannot be assumed to be representative of the distributions of rectal cancer and Crohn's disease patients.	(1) Some description in section 2 is not correct. (2) The contributions of this paper are not clearly described, and lack of innovation. (3) The experimental part is not compared with the improved method, nor with the advanced method in recent three years; (4) Too little validation data.	No obvious weakness. Just I suggest a source code of the deveolped layers and networks.	The data sets seem to be private, but the main algorithmic concepts of paper are described in sufficient detail that a reader has a reasonable chance of implementing the framework and testing it on their own data.	Easy to reproduce.	Need of source code url link.	Looking at the main weaknesses noted above, the greatest improvement that can be made to the paper is to expand the range of data and clinical tasks used in order to explore how RWCNs compare to ResNets under a wider range of conditions. I realize this would be difficult to accomplish during the rebuttal period, so it is not something I would insist on.	"(1) ""We also present the theoretical basis for the technical advances offered by wavelet activation functions being utilized within our unique RWCN formulation"". This is only the theoretical description of the corresponding method in the paper. It is the workload, not the contribution. (2) The innovation point is only the proposed RWCN method, the innovation is single, and it is suggested to add innovation points. (3) ""To our knowledge, no previous work has specifically examined the properties of residual skip connections within WNs in conjunction with CNNs."" The description of is not accurate. As early as ten years ago, there was an article on the combination of CNN activation function and wavelet. The article only focused on the architecture of wavelon integrated into convolutional neural network, abbreviated as WN. Pay attention to full literature research before expression. (4) VGG was proposed in 2014 and RESNET was proposed in 2015. These two methods were proposed seven years ago. It is suggested to conduct a comparative experiment with the latest in-depth learning method proposed in the current three years. At least compare the effect without combining wavelet with that after combining wavelet, so as to highlight the advantages of your method. (5) There is too little data in the experimental verification part. For the results after training and testing, multiple groups of experimental comparison should be carried out in the verification experiment. If the image display is required due to the length limitation of the article, the list can display as much data as possible."	Very nice study. Just include the source code please.	Overall, this is a nicely written paper exploring a concept that, while not dramatically novel, is worth investigating. Wavelon networks have not made a major impact on medical image analysis so far, and based on results of this paper, it is unclear whether adding residual links will increase the overall impact. If larger data sets and/or a wider range of data and clinical tasks were used, it could be more convincing.	Poor innovation and experimental results	The author develope a new architecture form networks, layers and activation perspective using wavelet theory. They evaluate their outperform in different cohorts and topics. They deliver a variation of different network comparison in each topic.  Novelty in architecture, design in network to layer and activation level. A very well organise manuscript with very sientific evaluation of the hypothesis. Nice Figures and verification of the ideas.  No obvious weakness. Just I suggest a source code of the deveolped layers and networks.
423-Paper2639	Rethinking Breast Lesion Segmentation in Ultrasound: A New Video Dataset and A Baseline Network	A new benchmark dataset for automatic breast lesion segmentation in ultrasound video is presented. Dynamic parallel spatial-temporal transformer (DPSTT), implemented on the basis of temporally, and spatially decoupled Tansformer blocks, is proposed. Dynamic memory selection scheme is presented to dynamically update memory frames of the DPSTT. The dataset and network are assessed through a comprehensive ablation study and comparison with other SoTA models.	The authors publish the first annotated breast lesion segmentation dataset using ultrasound video. The paper presents a dynamic parallel temporal and spatial-decoupled transformer. The neural network efficiently reduces the amount of computation and enhances performance. The extensive comparative and ablation studies, it is shown that the accuracy of the proposed network outperforms existing methods.	The paper introduces an ultrasound video dataset with pixel-wise annotations for breast lesion segmentation. It additionally proposes a video segmentation method based on general segmentation architectures, i.e., STM. The results of the model on the dataset look good.	The author presents a breast lesion segmentation dataset in an ultrasound video. The temporal information contained in the proposed dataset contributes to accuracy in automatic breast lesion segmentation. A dynamic parallel and spatial-decoupled transformer framework is presented. The DPSTT achieves computational efficiency through the proposed non-local spatial, and temporal transformer module. A dynamic memory selection scheme is proposed to eliminate unnecessary features of the past frames. Through ablation studies, the author verifies that the scheme contributes to the segmentation accuracy. Quantitative assessment is conducted by comparing DPSTT with other SoTA models including image-based, and video-based segmentation models. The results demonstrate that the DPSTT outperforms other SoTA models with a large margin. A comprehensive ablation study is provided. The study well explains the effectiveness of the proposed temporal and spatial transformer model, and dynamic memory selection scheme.	A. The paper introduces an automated breast lesion segmentation dataset using ultrasound video. Recently a line of works has demonstrated that proper utilization of the spatio-temporal information enhances the reconstruction accuracy. The proposed temporal breast data is expected to be applied in a diverse neural network that is using temporal information and contributes to the accuracy of breast lesion segmentation. B. The proposed spatial and temporal transformer reduces computation compared to baseline while showing enhanced performance.  C. Extensive quantitative comparison and ablation studies are provided. The proposed decoupled spatial and temporal transformer network outperforms other State-of-the-art neural networks with reduced inference time. D. Overall, the paper is well written. The organization is well structured.	The dataset is probably a valuable resource for the community.	It would have been valuable to describe some details of the proposed video ultrasound segmentation dataset. e.g Types of lesions, the number of the subject would be helpful. Description of the loss function is insufficient. e.g. How the binary cross entropy loss, and dice loss are weighted.	A. The result does not present the computational complexity required for the proposed dynamic selection algorithm. The sorting and cosine similarity calculation is carried out for every frame, which might have adverse impact on the inference time. B. Descriptions and qualitative assessments are insufficient for a dynamic memory selection algorithm.	The methodology contributions are not significant. Most models are well studied, especially in natural video segmentation community. The model is like to combine existing techniques together.	The authos will release the dataset and the code.	Author will provide the code and the results seem reproducible.	Most technical details are well provided. I believe that the model can easily be implemented. However, more training details should be given like the optimizer used, and the learning rate scheduler.	The spatial resolution of the dataset is down-sampled to 300x200. It is concerned that such down-sampling affects the reconstruction accuracy. Additional results about the relationship between the number of memory frames and accuracy would help understand the effectiveness of the proposed spatially, and temporally decoupled transformer module.	The proposed temporal decoupled transformer split the memory, and query key into s^2 non-overlapping patches. The relationship between the accuracy and the s value would provide a deeper understanding of the temporal transformer and local similarity. The dynamic memory selection scheme employs cosine similarity to calculate similarity among each frame. Diverse similarity metrics such as Euclidean distance could be an alternative option. It would be good if the reason for choosing cosine similarity is recommended.  	My first major concern about the paper is the baseline model for comparison. The reviewer assume that the baseline is STM, however, it is hard to align the results of STM with the ablative results. I will suggest that the authors clarify the baseline and then incrementally demonstrate the contributions of the components proposed in the article. This is essential since the segmentation framework has been explored in other places. My second concern is about the generalization of the method. Is the method also applicable to other medical video segmentation tasks? What makes it unique to US video segmentation? Memory-based networks have been explored in medical segmentation like Quality-Aware Memory Network for Interactive Volumetric Image Segmentation. Thus it should be carefully discussed. The ablation study is not sufficient. Some detailed investigation of memory hyperparameters (e.g., K in Eq.5) should be performed.	The paper is well written.  A novel video ultrasound lesion segmentation dataset is presented.  Nice application of the Transformer modules in automated lesion segmentation	The introduces automated breast lesion segmentation dataset using ultrasound video is novel. The proposed decoupled spatial and temporal decoupled transformer is efficiently formulated and properly assessed.	"Though the novelty of the methodology is not very significant, the dataset is a contribution. Overall, I think that the merits weigh over weaknesses and recommend ""weak accept""."
424-Paper0739	Rethinking Surgical Captioning: End-to-End Window-Based MLP Transformer Using Patches	The authors proposed a window-based MLP transformer (with patch-based shifted window) to achieve surgical captioning tasks on video data. Two surgical datasets are adopted for benchmarking purpose, where comparable results are obtained with much less computation burden.	This paper proposes an architecture for Surgical Captioning without the need of an intermediate feature extraction or detection step. The authors evaluated the use of Swin transformers with MLP and designed an Encoder-Decoder caption architecture. They evaluate their method on two datasets including Qualitative results.	This paper designs an end-to-end detector and feature extractor-free captioning model by utilizing the patch-based shifted window technique from the recently twin-transformer. Compared to the conventional swintransformer, it replaces the multi-head attention with window-based multi-head multi-layer perceptron. It releases the limitation of human annotation of bounding boxes and boost the real-time performance. The authors validate the model on the surgical video captioning task and compare with the baseline methods.	The authors proposed to borrow patch-based shifted window technique to realise real-time robotic surgery. Surgical video captioning is an interesting topic, which is still not well-explored yet. It's encouraging to see studied proposed to apply advanced vision techniques into this domain. The paper is well-written with clear demonstration.	Surgical captioning is an interesting research direction that goes beyond a simple phase, tool or activity recognition. Novel architectural choices for task of surgical captioning creation Addition of Video is interesting and adds to the value of the work	-Unify the structure: this work releases the limitation that the transformer-based captioning model needs a feature-extractor or detector ahead, which shows a transformer-only unified network structure for the surgical video captioning task. This make the network not limited by the pre-trained detection models/ feature extraction models.  -Parameter efficient: Interestingly, the model replace the multi-head attention with group convolution to keep the design of transformer and save the computational cost. The time complexity comparison between conventional swintransformer and this work is discussed and provided. This operation is based on the observation that it is the structure of the transformer that make things work, not the self-attention.  -Evaluation: this paper evaluates upon the surgical video captioning task on the large public dataset and compared with suitable baseline methods.	"Although the authors specified their difference from ViT and Swin Transformer, I may still find it a lack of their own uniqueness and technical contribution. This judgement might somehow reduce its overall impression, as Swin Transformer might be the main reason for the reduction in computation burden. The authors claimed to have their design with less-expensive computation cost, however, I did not find it well-justified in the result section. A runtime comparison with other counterparts and efficiency analysis (GPU, #FLOPS) is suggested, so that their efforts paid in efficiency improvements can be quantitatively demonstrated. Besides, the authors mentioned about ""real-time robotic surgery"", could this be justified by numerical results as well? Qualitative comparisons with SOTAs on captioning demonstration are preferred but missing in the manuscript. The quantitative results in Tbl.1 does not look convincing enough to demonstrate the superiority of the proposed method."	"""Nonetheless, the feature extractor still exists as an intermediate module which unavoidably leads to inefficient training and long inference delay at the prediction stage"" ""De- spite the impressive performance, most of the approaches are required heavy computational resources for the surgical captioning task which limits the real- time deployment"" ""take the im- age patches directly, to eliminate the object detector and feature extractor for real-time application"" ""These lead to inference delay and limit the captioning model to deploy in real-time robotic surgery."" ""reduces the training parameters, and improves the inference speed."" In the work the authors highlight the advantage of their approach in terms of efficiency and speed (point 1 to 5). No comparison regarding flops or fps between detection and detection free models. The model speed and efficiency is not necessarily restricted if its not end-to-end. In summary I cannot follow the argument of the reduced complexity and efficiency. In Fact ResNet has significantly less parameters and flops compared to the used Swin-L model. Parameters resnet-18: 11mio Parameters Swin-L: 197mio Flops (224x224) resnet-18: 2 G Flops (224x224) Swin-L: 34.5G ------  The metrics for SwinMLP are significantly improved compared to Swin. However, the performance of Swin and SwinMLP should be comparable and Swin should also be comparable to Transformer[5]. It would be good to see if there is any rational for this difference as this makes the results less trustworthy.  ------  Evaluation missing against: Zhang, J., Nie, Y., Chang, J., Zhang, J.J.: Surgical instruction generation with transformers. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 290-299. Springer (2021) ------  Ablation missing between 2D, 3D and 2D/3D. Also, this should be compared to a 3D baseline e.g. 3D ResNet. ------"	-Limited novelty: the paper's main improvement is from the visual backbone, which is swintransformer. However, there is little discussion about why a unified structure is needed and is there any other more powerful convent is not suitable for the caption task.  -Experiment: The paper modified replaces the multi-head self-attention with multi-head MLP and provides the time complexity comparison. However, the third row in table 1 does not show the FPS and the table 3 does not show any parameter improvement compared to the swin transformer.  -Baseline: the baseline methods in the experiment are not the state-of-the-art method. I think there are many captioning methods using the fully transformer.	The authors included their source code in supplementary, and with clear specification in their manuscript, I do not doubt its reproducibility.	"Reproducibility Response is set to ""Yes"" for every question."	The paper is able to reproduce and the dataset for training and testing are public.	The authors did not promise to release the code, which might be somehow disappointing to the community I suppose.	"Table1: why does ""Ours"" does not have FPS (Frames per second?) especially in robotic surgery. --> why is it especially for robitc surgery a problem? Isnt it also a problem for e.g. minimally invasive surgery? ------  In Table 1. SwinMLP-TranCAP, Swin-TranCAP, V-SwinMLP-TranCAP i think its better to say SwinMLP-TranCAP-Encoder (likewise for the others)  for FE Column.  Add a X to Det. for same group  ------  ""Self-sequence and AOA originally take the region features extracted from the object detector with feature extractor as input. In our work, we design the hybrid style for them by sending image features extracted by the feature extractor only"" --> doenst this modification make the AOA and Self-sequence method less capable? ------ How does the window size of 14, instead of 7 influence the results and what is the rational for this change to the  baseline? ------  Revisit sentence: ""The shifted window and multi-head MLP architecture design make our model less computation""  ""Replacing the multi-head attention module with a multi-head MLP also reveals that the generic transformer architecture is the core design instead of the attention-based module."" ------  Avoid using these terms in academic writing: extremly simple, very compute heavy https://www.scribbr.com/academic-writing/taboo-words/"	-This is a great work, however I'd like to see more discussion, such as the difference between Swin's encoder and this work's is the multi-head MLP by group convolution. How that differs from the conventional convolutional network? -The decoder is standard transformer structure, and can we replace it with the other sequence modeling methods, such as LSTM, GRU? -For the video model, is there any fundamental improvement compared to the 2D-based method? Can we simply use 2D-based method to handle the video caption? -The comparison should be complete and fair, the work has a larger window size compared to the twin transformer, I think it is better to evaluates win with the same setting in the table 1. Otherwise, I will be confused about if the window size affect the performance or the modification boost the performance. -Inference speed, FPS problem as indicated above. -In the ablation table 2, why not show the patch size of SWIN-MLP, but the vanilla transformer? Also, it seems that the patch size as 16 works better, why do we finally choose patch 4 in the table 3. The param column in table (b) does not show the difference between this work and swin transformer, is there any explanation?	Considering both its advantages and drawbacks, I find it an interesting paper overall.	Interesting work on Surgical Captioning using the novel SWIN architecture. Evaluation and Ablation could be in more detail e.g. comparing to other works on the same task and dataset. The main concern is that there is no evidence that this work is more suitable for real-time applications than other methods with separate detection and feature extraction step - This concern should be addressed.	Experiments
425-Paper0641	Rethinking Surgical Instrument Segmentation: A Background Image Can Be All You Need	Use of simulated data to train a segmentation network.  Without the high cost of data collection and annotation, the authors claim to have achieved decent surgical instrument segmentation performance.	The proposed work has the aim to reduce the data collection process specifically for the problem of segmentation of instruments in images acquired during surgical procedures. The main idea is to use a single background tissue image and a few instrument images and apply multiple augmentation and blending techniques to synthesize new data that could be used for training. The approach is based on a chained augmentation mixing approach used during training and tested using publicly available datasets.	"The paper presents a data augmentation strategy for surgical tool assessment in which a background video/image is used and a foreground surgical tool image is then superimposed on top of it, allowing for a gold standard multiple-instance foreground mask to be readily available and to control data distribution elements. The two images can also be augmented separately to increase variety. The method is then evaluated by training a U-Net on the EndoVis-2018 dataset once with the proposed synthetic data (using a massively reduced amount of data, i.e. 2-3 foreground stills per instrument) and one with the same background augmentations applied to the full data (i.e. augmentation without ""simulation"") and garners surprisingly good Dice given the very few annotations provided and with good generalisation to EndoVis-2017. A second experiment in which a previously unseen instrument is then added to the simulation database shows a further Dice improvement of ~1.6% generally."	Clarity and the simplicity of the approach	The use of limitate annotate images to train a segmentation pipeline is quite relevant for the medical imaging community. The paper is well written and well organized. Public available datasets are used in the experiments. The code is provided.	"The strength of the paper is its simplicity: it proposes an intelligent method of ""augmentation by simulation"" similar in spirit to using driving video games to help train self-driving cars. Although the results given in Table 1 look uninspiring at a glance, knowing how the last three rows are generated is actually quite impressive."	dataset size for Syn-A, B, and C are different *the segmentation performance is not very good for purely synthethic case. The authors don't emphasize the Table 2 results properly. That is the most interesting part of the paper.	It is not clear what is the technical novelty of the paper. The approach is very similar to the AugMix and there is no comparison with other existing approaches used to create synthetic datasets (i.e. GAN based). The results in the experimental section are not very convincing. It seems that the approach produces good results on the simulated images. However, when the approach is tested on the unseen target domain (EndoVis-2017) the improvements drop significantly. Fig 3b is not discussed in the paper.	The main weakness in the paper is that the results are not separated/collected in a way that would clearly show the magnitude of improvement caused by the method. The authors have done some work to provide contextualization, but much more could be done to really make the method shine. See the Constructive Comments for particular suggestions. The paper is also missing some brief discussion of where the method may be conceptually lacking (i.e. instrument shadows / interaction with anatomy, keeping instruments from crossing/obstruction in multi-tool scenes, etc...) which could help contextualize where it fits in with more data-hungry methods such as GANs which could theoretically handle these things.	likely reproducible	The authors have provided the code and the approach can be reproduced.	The paper is conceptually highly reproducible, regardless if code/data is provided or not. The authors should be commended for this as it means it can be applied more broadly by the community and is not dependent on their particular implementation.	It is a nice paper to read and follow.	The authors should provide some comparison with other data augmentation approaches. They have also to highlight what is the novelty, especially in comparison to AugMix.	"Given that this is a MICCAI paper with limited space, some of the comments are about removing content that is not strictly necessary. A small section (~1/3 pg) is dedicated to blending which is really not needed given how common the technique is generally in film. Eq 1 in particular doesn't help explain the method and Eq 3 is largely a copy of Eq 2, meaning that all three could be more readily expressed as Eq 3 with a sentence defining H_i and \Theta. Other small comments: The number of foreground stills is giving in Section 3.1 but not the number of background stills. This is stated in the introduction, but briefly restating it in Section 3.1 would be good to remind the reader of the quantity of annotated data used. For Fig 3b, it would be nice to have the Dice for the vessel sealer vs the other instruments shown separately. As it stands, a ~1.6% improvement seems small, but one imagines that this may be because of the prevalence of the instrument in the Testing dataset. And one big comment: I stated that Table 1, despite how it looks superficially, is actually a strength, but there is, I feel, a way to make it even better. Firstly, it lacks an idea of a reference bounds. It would be improved by a row that shows an a priori reasonable lower bound on performance. (For example, this could training with the same few stills but without simulation, which would should just how much using simulation-based data augmentation as training a U-Net with only a dozen or so images is probably going to fail miserably without the nuanced data augmentation procedures suggested in the paper.) It would also be improved by a row indicating an a priori reasonable upper bound as well, such as using the same large database but with on-the-fly simulation. This latter part is important to the optics of the paper as it would show that the method presented does outperform the argument ""just get more data."" Typos: ""minimal human efforts"" - ""minimal human effort"""	Not very novel; but, decent result.	It seems that the results presented in this work are very preliminary. In particular, the lack of comparison with existing approaches makes it difficult to evaluate the contribution of this work.	The major factors to influence me is the analogy of the method to things seen outside of the community (i.e. self-driving cars and video games) to make it more widely interesting to a broad MICCAI audience, the simplicity of the method, and the attention taken to perform ablation studies to begin the quantify its performance more robustly. Yes, there are some weaknesses, but nothing to distract from those more core strengths.
426-Paper2077	Retrieval of surgical phase transitions using reinforcement learning	The paper proposes a new RL formulation for offline phase transition retrieval. Specifically, a network TRN is proposed which searches phase transitions using multi-agent RL. The proposed method is validated on Cholec80 and an in-house dataset.	The manuscript proposes an offline phase transition retrieval method using reinforcement learning. The method predicts phase transition timestamp instead of classifying all frames. The method is evaluated in two settings - first with a sparse number of frames resulting in subpar results than SOTA; the second where all frames are processed and claim to outperform SOTA which are TeCNO and Trans-SVNet. Evaluation is measured at frame level (accuracy, precision, recall, and F1-score) and at event level (event ratio).	The authors propose a novel method for offline surgical phase recognition using reinforcement learning. While most work views phase recognition as a frame-wise classification task, the authors rather define the task of finding the start and end point of each phase. This way, predicted phases are supposedly guaranteed to be contiguous. Two different initialization strategies are proposed which either use all (RMI) or only a subset of the video frames (FI). Methods are compared to SOTA online methods on 2 different datasets. The RMI-variant achieves superior performance on the cholec80 dataset.	The idea of using a multi-agent RL to find phase transition is interesting.	The abstract is clear and well written. There is a clear contribution in this paper. The results were compared with the relevant SOTA models (TeCNO, Trans-SVNet) in the domain. The paper presents significant insights for research continuation.	Reformulating offline phase recognition as the task of finding phase transitions is a novel approach and more closely resembles how humans would solve this task. The proposed task/method has the advantage that it (supposedly) produces contiguous phases. The authors discuss limitations of the approach.	(1) The experimental validation is not convincing. In Table 2, the last two rows only list the performance of the proposed method under not full coverage which is not convincing. It is suggested to show the performance of the proposed method under full coverage. It is also suggested to compare with more previous work. (2) Reference [1] 's information is not given. It is suggested to give this information. (3) Since it is well known that RL methods take more computational resources for training. It would be good to compare the computational efficiency of the proposed method to previous work besides the performance comparison.	The novelty of the method is limited, and lacking in details. Paper title does not match the content and work presented. The results are below the baseline on one dataset which makes its hard to justify the efficacy and generalization capability of the proposed method.	"The authors propose an offline method but only compare to online methods There are several open questions regarding the method design which indicate limitations of the proposed task formulation. These open questions are mostly related to how the model behaves in edge cases (phase does not occur, phase is predicted inside another, average frame index is out of range, constraint ""f_nb < f_ne""). The way windows are traversed by the agent might make the task unnecessarily difficult. Why was DQN chosen since there are many newer RL methods? It is not clear how the modification of the baselines affects performance. The weaknesses and possible solutions are discussed in more detail in section 8."	The reproducibility of the paper looks fine.	The paper provides enough information for this.	"For the most part, the method and training procedure is described in detail. The authors also indicate that they intend to publish their code. Some details could be clarified: How many epochs and episodes were used for the ResNet backbone and the DQN respectively. The authors say they used a validation set for model selection. Which metric was used here? There are some open questions regarding the design of the method which are elaborated in more detail in section 8 (main weakness 2) How exactly are the metrics computed? E.g. how are NaN values in precision and recall handled if a phase is not predicted or does not occur? Were the metrics with relaxed boundaries used like in Trans-SVNet and other previous work (e.g. TMRNet, MTRCNet-CL, SV-RCNet)? Other metrics are fine but it should be made clear how they were computed. Were experiments repeated? This is not clear since the standard deviation is computed over videos. If they were repeated, how were scores computed? Are predictions first averaged to compute one score or are scores computed for each prediction and then averaged? For the RMI approach, the authors state that the indices of all possible transitions are averaged to initialize the agents. How are ""possible transitions"" defined? The authors state they used window sizes of 21 and 41. Is this L? Or is it the complete receptive field (i.e. window size of 21 = 2*L+1 with two search windows of L=10 plus the center frame)? Section 2.1, however, states that the state of each agent consists of 2L features, not 2L+1."	Please see Weakness.	"Title matching content: No, the manuscript describes a method to correctly predict the phase transitions and that itself is a ""refinement"" task over the initial set of predictions rather than a ""retrieval"" task. Abstract summarizing content: The abstract mentions the use of the reinforcement learning paradigm but did not fully justify the rationale behind their choice. The abstract talks about two configurations with full and sparse number of video frames, however, the paper does not elaborate on the usefulness of the two settings beyond just computational efficiency The abstract mentions the ""comparable computation cost"" of the new method but it is not specified in any form in the paper. It will be good to provide numbers for the computation cost as well. Motivation: The motivation for the work is short although clear and matches the goal provided in the abstract. However, it will be great to provide more details about the implications of ""erroneous phase transitions"". The readers are left looking for more details on the said problem. Novelty in contribution: The novelty lies in adapting Deep Q-Learning Network (DQN) and Gaussian components, but more details on the rationale behind DQN use would have been better. Knowledge advancement: The work provides a method of reducing erroneous phase segmentation by the use of multi-agent DQN and Gaussian smoothing. The work outperforms the re-implemented SOTA work as baselines for the Cholec80 dataset but not for the in-house dataset.  The weak performance (~10% less from baselines) on the in-house dataset is justified by the processing of less number of frames (~20%) which is encouraging but it's defined on only one phase transition. More details on the dataset and for more phase transitions would have been better. Positioning with existing literature: The manuscript mentions recent works in the field of phase recognition but misses out on mentioning other papers such as MTRCNet-CL, Surgical phase recognition by learning phase transitions (Sahu et al.), etc. Related references are covered for DQN, phase recognition, datasets, but missing for LSTM, ResNet. Method description and rationales: The method is aptly divided into three modules with sufficient details and provides enough purpose for all the modules. The clip size in the Average ResNet feature extractor is set as 16. It will be nice to see the results for K=8 or K=32 to justify the K=16 choice. For example, the K=8 setting might make the predictions less noisy or more refined. The DQN Transition Retrieval subsection does not provide the reasoning for choosing the RL-based network compared to standard CNN/RNN based methods or possibly share some results to ascertain why RL is necessary. The second FC layer after DQN is 50-dim in size and is mapped to ""2 Q-values Right and Left"". This part is confusing - is the final output vector is of dimension 2 or 50 and how is it divided into vectors responsible for ""Right"" and ""Left"" action.  An example of a sample input and output feature dimension through the DQN + LSTM/FC setting makes it easy to comprehend which is not provided. Without giving the input dimension, it is important to mention the dimension of the downsample features. In the DQN subsection, two characteristics provided are similar as the position of the agents defines the nearby clips that will be used for training. There is a scope for further clarity in the text. RMI initialization is not clear from the text. Is there another ResNet trained with transition indices as the final output? More details should be provided on RMI as reported results are better in ""RMI"" setting than in ""FI"" setting.  The motivation behind the third Gaussian composition module is clear. The loss in Algorithm 1 is not explicitly specified and there is no mention of the type of loss used.  The data structure used for the replay memories is not specified - Is it a tensor? What is the size of the replay memories? Is the sampling random or sequential? The author refers to the RL paper [12] but it should mention small details as mentioned above. Standalone figures and tables: The problem statement in Fig 1 is clear and summarizes the goal of the paper. The architectural diagram in Fig 2 is clear and legible but missing small details like an arrow pointing from DQN to its expanded view. Reproducibility of the experiments: The basic hyperparameters are presented. Is there no weight decay used during training? Are both ResNet-50 and DQN trained end to end? This is not clear from the experimental setup section.  The maximum number of steps for agent exploration is mentioned as 200. Is it because the network converges by 200 steps? Is there a lower and upper bound on the number of steps where convergence starts or stops?  The manuscript should mention the maximum number of episodes used for training which is missing from the text. Data contribution/usage: The method is implemented/evaluated on a publicly available dataset - Cholec80. The paper uses the recommended train/val/test splits in the original dataset paper. A private in-house sacrocolpopexy dataset is used for evaluation however it's focused on only one phase transition compared to multiple phase transitions in Cholec80. Results presentation: The results in the tables are clear but the best results must be highlighted in bold. The results are specified with mean and std which makes it easy to comprehend with others. The manuscript stresses the improvement in the performance based on the metrics - Event and Ward Event ratio which is promising but did not discuss further the case for Sacrocolpopexy where TeCNO/Trans-SVNet in spite of having a much higher F1-Score than TRN does not have better Event/Ward Event ratio.  The manuscript mentions TRN21/41 FI for Sacrocolpopexy in the results and discussions but the results are provided only for TRN21/81 FI.  To maintain uniformity, the manuscript should have presented the results for the RMI setting for Sacrocolpopexy which is missing, and no rationale is provided.  One of the SOTA, Trans-SVNet, is said to be reproduced in this paper for comparison, but the reported numbers on the Precision and Recall metrics for Cholec80 are ~8-9% less than the published performance. This raises the question of the quality of the baseline used and reported in the paper. Discussion of results and method justification: The results for Sacrocolpopexy are not ""slightly"" under the baselines, the word manuscript used is misleading as the difference between TeCNO/Trans-SVNet and TRN is ~ 10% under F1-Score/Precision/Recall metrics. The improvement in the performance for Cholec80 is clearly mentioned and the reasoning provided. However, it is important to know which part of the TRN is largely responsible for smooth transitions. Is it because of DQN or the gaussian composition? For example, the caption for Table 1 is confusing - the results for individual phases do not perform Gaussian Composition but the Overall F1-score is after applying Gaussian Composition. What is the reason behind this? Clinical relevance of the proposed method and obtained results The clinical relevance of the work done is not discussed. Conclusion: The paper presents significant insights for research continuation Reference is adequate but ResNet and LSTM are not cited. Arguable claims: The videos are center-cropped but might miss out on surgical activities or motion patterns happening around the video frame. Most works resize the video frame rather than center cropping. The performance might also be stunted due to center cropping. (page.5). The manuscript should provide reasoning behind this. Manuscript writing and typographical corrections We implemented the standard DQN training framework for our [netwrok]: [network] (pg: 3) We perform a Gaussian composition of [of] the predicted phases: [] (pg: 4)"	"MAIN WEAKNESSES Offline method only compared to online methods The authors propose an offline approach but only compare their results with online methods. Since offline phase recognition is a considerably easier task than its online counterpart, it is not clear if the performance gain is because of the effectiveness of the proposed RL method or simply due to the easier task. TeCNO and Trans-SVNet should be fairly straighforward to reimplement as offline methods (especially TeCNO). SUGGESTION: I believe the authors should compare with offline methods (e.g. offline TeCNO, offline Trans-SVNet) to demonstrate whether the proposed RL formulation is competitive with the standard frame-classification formulation. There are several open questions regarding the method design. These questions partially indicate limitations of the proposed task formulation: The paper does not mention what happens if a phase does not occur in a video. If the model cannot handle this case and always predicts all phases, this would be a major limitation. The authors state that their method guarantees contiguous phases. However, what happens if the start and end points of one phase are predicted to be within another phase (e.g. f_1b < f_2b <_f2e < f_1e). Due to the Gaussian composition, this would result in the 'outer' phase to be split into two segments. How is this case handled? Has this happened in any video? Is the constraint ""f_nb < f_ne"" somehow enforced by the model? How would the model behave if this constraint was violated? Has this ever happened? For the FI approach, the authors state that transitions are initialized at the ""average frame index"". For short surgeries and late phases, this average frame index likely often lies outside of the range of the video. How are these cases handled? Or is it rather a relative frame index is measured (i.e. the average progress of the surgery in percent)? IDEA: Making the agents predict ""f_nb > f_be"" might be a way of handling missing phases and might kill two birds with one stone. Not sure if this is a good idea. The way windows are traversed by the agent might make the task unnecessarily difficult. How are the windows traversed by the LSTM agents? If they are traversed sequentially, then the most relevant frames are likely somewhere in the middle of that sequence. The LSTM might forget relevant information or it might be difficult to remember their exact location. E.g. if the agent is currently at the correct location, the LSTM would have to remember that the transition happened at exactly the middle of the sequence. This seems like an unnecessarily difficult task. Adding a positional encoding or traversing the sequence from outside to inside might be alternative strategies. Why was this sequential strategy (supposedly) chosen? Did the authors test or consider other traversing/encoding strategies? Why was DQN chosen? The standard DQN algorithm is quite old and many improved or different RL approaches already exist? Why did the authors not opt for more modern RL methods like PPO[1], SAC[2], A3C[3] or HER[4]? SUGGESTION: The authors should explain why they chose DQN or consider a more modern RL method. The authors modify the baseline methods but it is not clear if this modification improved or hurt performance. Modifying the baselines to be more comparable to the proposed approach is definitely a valid approach. Nevertheless, the original approach should still be reported to understand how this modification affected performance. [1] https://arxiv.org/abs/1707.06347 [2] https://arxiv.org/abs/1801.01290 [3] https://arxiv.org/abs/1602.01783 [4] https://arxiv.org/abs/1707.01495 REQUIRED CLARIFICATIONS The open questions from the reproducibility section and 'main weakness 2' could be clarified. MINOR COMMENTS Apparently a mistake happened in the supplementary material. All plots show the results of the same video. Why was the RMI approach not evaluated on the sacrocolpopexy dataset? It is quite cumbersome to understand how big the receptive field of the model is in terms of seconds. If I understand correctly if would be 2L16 / 2.4 seconds (with a window size of 2*L feature vectors; the averaged ResNet producing 1 feature vector from 16 frames and an initial framerat of 2.4 fps). Maybe this could be made clearer in the paper? Or if I am incorrect, the correct receptive field could be given. Typo: ""netwrok"" in Section 2.1"	The insufficient experimental validation is my major concern.	There is a clear contribution which is relevant to the community. The results were compared with the relevant SOTA models (TeCNO, Trans-SVNet) in the domain. The release of the in-house dataset would add value to the research community	The authors present an interesting new task formulation for offline phase recognition. However, while the idea is promising, there are many open questions regarding the method's design which indicate poor behavior in edge cases. E.g. it does not seem like the model can handle missing phases. Another weakness is that the results are only compared to online approaches (which is a considerably harder task). It is not clear if the proposed RL formulation could compete with the offline variants of standard phase recognition models (e.g. TeCNO or Trans-SVNet).
427-Paper1808	Revealing Continuous Brain Dynamical Organization with Multimodal Graph Transformer	This paper proposed a novel spatio-temporal graph transformer model to explore the dependency of functional connectivity on anatomical structure.	The paper proposed a novel spatio-temporal graph Transformer model to integrate the structural and functional connectivity in both spatial and temporal domain, and exemplified its application in HCP data.	This work proposed a novel spatiotemporal graph Transformer model to learn the heterogeneous node and graph representation via contrastive learning based on multimodal brain data (i.e. fMRI, MRI, MEG and behavior performance). The experimental results reveal the significance of regional heterogeneity in modeling structure-function relationship of brain dynamical organization.	The proposed study incorporates the areal heterogeneity map (T1-to-T2-weighted MRI) to improve the model fit to structure-functional interactions. The authors also introduced a novel graph transformer pooling layer to learn the global representation of the entire graph. Meta-analysis was adopted to evaluate the proposed model to explore the behavioral relevance of different brain regions.	adopt graph transformer with multi-head attention use contrastive learning to integrate multimodal imaging details evaluation with meta-analysis	a) Incorporating the heterogeneity map constrained by T1-to-T2-weighted (T1w/T2w) to improve the model fit to structure-function interactions is novel. b) Contrastive learning is used to associate the different data modalities. c) The experimental results of behavior related global gradient are very interesting.	Some critical technical details are missing. For example, how was the T1-to-T2 heterogeneity map incorporated in the model? How do the fMRI and Meg graphs relate to functional correlation matrices in Figure 1? Where does the functional gradients in section 3.2 come from? Considering the space limit, part of 2.2 can be removed to Supplementary materials. What do the results tell us about the dependency of functional connectivity on anatomical structure, or structural connectivity? Model training details are missing, which may degenerate the reproducibility of the study. The authors showed the connectivity patterns for one hemisphere. Why not both hemispheres as there exist both strong structural and functional connections between some symmetric brain regions?	Some explanation about the concepts are not detailed provided in the paper, such as heterogeneity, structure-function coupling index.	The major weakness of the paper is the writing quality, which significantly confuses audience. Here are some examples: a) G_i = (V, E_i) is used to represent the heterogeneous graph representation. According to the illustration, i=1,...,N represents the brain ROIs, which means for each ROI, there is a graph G_i. However, I think G_i represents the graph for different modality in the first paragraph of Methods section. b) In the same paragraph, what are the multivariate values X_A and X_B? Are they features of nodes in graph? Similarly, what are the Y_A and Y_B? c) In Section 2.1, N was used to represent the number of neurons. What is the meaning of neurons here? Is it corresponding to the brain ROIs mentioned above? d) In Section 2.1, it is mentioned that fG, L^j_G, L^y_G are graph encoder networks. What is graph encoder network? How is it implemented? Z^+_t is introduced to represent the value after discrete operation. What is the discrete operation? I think Z^t+_t is not used in Eq.2 and afterwards. e) How can we reach Eq.4 from Eq.3? The graph encoder shows up here again. Is it same as the graph encoder network in Section 2.1?  f) ... Overall, it is hard to figure out the input of each module, data flow, implementation details of the whole framework.	The reproducibility of the study can be improved by providing more technical details.	The reproducibility is acceptable.	I think the paper is not reproducible based on the current submission because the method section is vague and too many details are missing.	Please see comments on the main weakness of the paper.	some details should be added to make the paper easier to understand. the preprocessing of SC is not mentioned. if the findings in the paper could be validated in another dataset, the method may be more reliable.	a) The paper writting should be greatly improved. b) Ablation studies should be included to evalute each part of the proposed method.	The study is interesting. The framework is novel. However, some technical details are missing. The justification about experimental results is relatively weak.	The research topic is very important to the research field, and the proposed method provides a new avenue to deepen the understanding of brain dynamical organization.	The method is novel but the writting quality is very poor.
428-Paper0359	Rib Suppression in Digital Chest Tomosynthesis	This paper presents a rib suppression and lung enhancement network (TRIPLE-Net) for chest tomosynthesis. This is the first work on rib suppression using deep learning in tomosyntheses.	The paper presents a novel deep learning-based rib suppression approach in DCT by modeling the rib artifacts. The method is based on three subnetworks to model 2D and 3D rib components, and merging the results to make the final rib suppressed DCT prediction. The proposed TRIPLE-Net is validated against DCT dataset simulated from CT as well as clinical dataset.	The paper proposed TRIPLE-Net with three subnetworks (projection-net, volume-net, aggregation-net) that can model Rib in Digital Chest Tomosynthesis as a linear suppression. It can be trained in hard constraints from 2D and 3D domains. Therefore, such downstream tasks as pulmonary analysis will be beneficial. The improvement over RSGAN is to have a Filtered Back Projection that resembles the 2D images to 3D volume and captures more accurate rib modeling. A user study that involves two doctors is carried out, which shows that the proposed model gives a slightly better rib disentanglement.	The paper is very well-written and easy to follow. The physics of DCT is clearly explained followed by the logic of the design. Good job! The method is a dual-domain method that first learns in the projection domain, then learns in the reconstruction DCT domain. The experimental results on simulation data show clear quantitative and visual improvements. Additional reader study is performed to validate the method.	The idea of modeling rib artifacts in DCT with information from both 2D and 3D domains is novel and interesting. The evaluations with the simulated data as well as the clinical study also demonstrated the feasibility of this idea. The paper is fairly well written and the exposition of the results is good.	The proposed method leverages 3D information with the FBP operator to remove the rib artifact from DCT images consistently.  The paper is well written and backed up with a clear description of mathematical image formation.  Experiments were carried out qualitatively and quantitatively.  A user study shows that two doctors appreciate higher TRIPLE-Net ratings than its ablations and other state-of-the-art methods.	The network design aims to predict the I_delta. What if the network predicts the I_rs and then reconstructs and then refined with another 3D network? That would also be a rational design. Table 1, is there any statistical difference in the improvements? One of the major concerns the reviewer has is the real data study. The authors only collected 5 real data in total for the reader study, which should be expanded. Following the above comment, it is clearly seen in Table 2 that the proposed method does not really outperform the baselines on average for the clinical real data set. This means the method cannot generalize well to the real-world dataset, unlike the 3rd and 4th columns where it performs reasonably well on the simulation dataset since there is no domain shift. To alleviate the domain shift and to use this method, it is required to construct/acquire real data with and without rib which is impossible. So how can this method be deployed in real-world scenarios still remains unclear.	The main concern is the actual evaluation on the clinical dataset : small datasize. Details on the networks and training configurations are missing.	"Data preparation is unclear: ""in-paint rib mask with surrounding tissues"" lowers the confidence of 2D projection. There is no tissue inside the 3D rib regions, and if someones want to eliminate rib artifacts, they need to set the maximum transparency of those regions. Hence, final pixels compositing will not count the presence of ribs. Filling the 3D rib regions with interpolation from surrounding tissues will cause inaccurate tissue appearance, leading to unreliable analysis. The role of the merging module (aggregation-net) needs further discussion. One counter-example is that the difference of two inputs to the \mathcal{F} should be minimal, which results in V^M_{\alpha, \Delta} being the average of these two inputs. It leads to having the \mathcal{F} model will be over-parameterized."	Good	Without the network and training specifics, the work is not straightforward to reproduce.	The reproducibility is feasible so that a skillful graduate student can replicate it.	Please see above	This is an interesting study. However, more analysis is required, especially on the network's robustness to varying sizes of patient anatomy and disease types. The paper mentions M2D and M3D are trained separately before F; But it would be interesting to see how the joint training of the three networks works. It looks like the proposed TRIPLE-Net is almost on the same level as M3D (PSNR), both Doctors A and B rated M3D predicted images higher. The authors should add an explanation. Perhaps, a domain adaptation approach would be feasible with a larger clinical dataset. To make it more reliable, the simulated DCTs should be quality checked. The paper doesn't report the per scan execution time of the proposed model.	The CT data is orthographic on sagittal and coronal orientations but entirely perspective on axial planes. However, once the X-ray beam source and detector rotate a complete circle around the object, the integration compensates both directions and turns the collected voxels equidistant on axial planes. Please clarify whether the projection and simulated datasets were made on an orthographic or perspective scene? Experiments on simulated 3D CT datasets need to explicitly clarify whether the full HU scale is used and normalized to (0, 1), or a specific window is applied to the data. This matter will affect the evaluation metrics L1, L2, and the intensity range of 2D projections. On the other hand, L1, L2 norms, and PSNRs metrics are somehow related. Perhaps, other interesting metrics would be performing the maximum-intensity projection (MIP) on 3D rib segmented data compared with the 2D rib segmentation artifacts in Dice Score. Assuming that the proposed method can separate the rib artifacts in the image domain, the bone shadows still presented, given the results in Figures 3 and 4. It poses another question: Is the artifact well-observed if one performs gradient transform on the rib-suppression images?	The paper is well-written, the method is interesting, and the application is important. However, the major concern lies in how can this method be deployed in real-world scenarios with real data with and without rib, and how to achieve better performance on the real clinical dataset.	Interesting idea, writing and organization of the paper	In conclusion, revealing the nodules covered by rib shadows on a single or limited view of XR (such as DCT) is another highly ill-posed problem. The proposed solution addressed it in a way that separates the rib artifacts. There have some unclear points that can be minorly clarified to improve the current exposition of the paper.
429-Paper2294	Robust Segmentation of Brain MRI in the Wild with Hierarchical CNNs and no Retraining	Novel robust automated segmentation framework on brain MRIs by adopting an hierarchy of conditional segmentation and denoising deep neural networks. Thorough evaluation of the proposed model by comparing it against the SOTA and demonstrating its effectiveness in one of the volumetric studies related to ageing.	The authors present SynthSeg+, a novel hierarchical architecture that enables large-scale robust segmentation of brain MRI scans in the wild, without retraining. According to the authors, the method shows considerably improved robustness relatively to SynthSeg, while outperforming cascaded CNNs and some state-of-the-art denoising networks. The authors demonstrate SynthSeg+ in a study of ageing using 10,000 highly heterogeneous clinical scans, where it accurately replicates atrophy patterns observed on research data of much higher quality.	This paper proposes a SynthSeg+ method based on SynthSeg network to conduct medical image segmentation. The proposed SynthSeg+ consists of two U-net components and one denoiser part in the manner of a hierarchical architecture.	Proposal of a novel segmentation framework for clinical MRI scans that is robust against varying MRI protocols including contrast, resolution, deformations, low SNR and partial volume effects. No retraining necessary. Clearly stating the issues with current models and how SynthSeg+ tackles them with its novel architecture and how the configurational changes of intermediate stages impact the robustness and accuracy of the respective frameworks. Further, the paper also provides the rationale behind the outcome with respect to those changes. POC volumetric study on age-related atrophy demonstrating the volumetric trajectories of respective brain structures. This fosters their work on adopting the framework toward neuroclinical use cases.	The authors evaluate a previous tool, SynthSeg, on an uncurated, heterogeneous dataset of more than 10,000 scans, which is a remarkable number.  The authors propose SynthSeg+, a novel method which uses a hierarchy of conditional segmentation and denoising CNNs. The authors show that this method is considerably more robust than SynthSeg, while also outperforming cascaded networks and stateof-the-art segmentation denoising methods. The paper is clear and readable. The figures are high quality.	The description of issues is very clear. This method yields promising results and is feasible in clinic.	I don't see any mention about when the proposed method fails either in not extracting the desired structures or producing outliers. Also, it would be beneficial to know what MR conditions affect the model's outcome.	The authors only provide comparison in terms of Dice parameter. Other metrics should also be provided.  The authors could provide a fair comparison with other state-of-the-art techniques (in recent challenges) using public databases so the benefits of SynthSeg+ could be better displayed. A deeper discussion on related works should be provided, as the authors focus mainly in SynthSeg as a precedent.	1, Technically, the novelty of this paper is relatively weak because its major idea is a modified SynthSeg network where both U-Net and Denoiser network used in the paper have been proposed in other references.  This paper just put them into a new project to carry out the segmentation for brain images. 2, The article mentions that the training dataset in S1 and S2 is augmented with four linear transformations. However, the denoising model is trained with some distorted images which will bring some negative effect on the training of S2. Therefore, the experiment still needs more reasonable schemes. 3, The composition of rotation, scaling, shearing, and transformations should be a liner transformation as well. It is not a nonlinear transformation. The author does not tell us how to choose the parameters of four transformations and how many mappings are used in the article.	It looks like the authors are planning to release the code and trained models once accepted. This will help reproduce the results and help improve the model toward next frontiers.	The code is not available yet but the authors guarantee that they will provide it if the paper is accepted. In that case I do not have other way to proof the reproducibility of the method without the database and code.	The reproducibility of this method is a challenge since there are several ambiguous aspects such as some details of S2 and the parameter used to generate the low-resolution image.	It would be beneficial to include conditions under which the proposed framework doesn't perform well. This will help the reproducibility of the model and manage data accordingly. Why Gaussian mixture model has been employed in step(b) of SynthSeg+.  Can we use General mixture model instead considering the MR noise distribution (Rician) and possibly other unknowns.	The authors only provide comparison in terms of Dice parameter. Other metrics should also be provided.  The authors could provide a fair comparison with other state-of-the-art techniques (in recent challenges) using public databases so the benefits of SynthSeg+ could be better displayed. A deeper discussion on related Works should be provided, as the authors focus mainly in SynthSeg as a precedent.	No	The proposed framework can be a potential benchmark in MRI automated  segmentation of brain structures. This will also aid clinicians in analyzing neurological disorders better.  Very well written paper by providing sufficient details for each section considering the space constraints. Clearly defined Additional details were provided in the supplementary material.	The authors propose and improved version of SynthSeg. However, the comparisons implemented are not enough, as there are plenty of works (in challenges for instance) with public databases which could have been used for comparison purposes.	This method is based on SynthSeg network. And its three components including two U-Net blocks and one denosier block which have been proposed and widely used in other references.
430-Paper0042	RPLHR-CT Dataset and Transformer Baseline for Volumetric Super-Resolution from CT Scans	The paper introduces a dataset for volumetric SR and concurrently proposes a transformer network for super resolution. The data is evaluated for multiple network architectures, and an experiment for the domain gap and an ablation study for the proposed network is presented.	In this paper, the authors address the problem of super-resolution of CT images in the height dimension. The main contribution of the work is the following: a new dataset of 250 images of real-paired thin CTs and thick CTs, and a new transformer-based super-resolution model. Authors compare the model's performance with the state-of-the-art models and perform an ablation study of the main components of the model.	1) The authors developed a public real-paired volume dataset, RPLHR-CT which contains real paired thin-CTs (slice thickness 1mm) and thick-CTs (slice thickness 5mm) of 250 patients. RPLHR-CT is the first benchmark for volumetric SR, which enables fair comparison between different methods. 2) This work explored the potential of transformer for volumetric SR and proposed a novel transformer volumetric super-resolution network to alleviate the inherent shortcomings of convolutional operations, i.e., the issue of long-range dependencies. Besides, the proposed TVSRN network achieves a better trade-off between image quality, the number of parameters, and running time. 3) The authors re-implement and benchmark state-of-the-art CNN-based volumetric SR algorithms developed for CT. This indicates that the work provides some benchmark comparison and reference for the community of volumetric CT image SR.	The authors aim at creating a public real-paired dataset for volumetric SR and provide a benchmark.	A new publicly available dataset for super-resolution benchmarking is a valuable contribution for the medical imaging community. Authors demonstrated the superiority of the transformer-based model compared to the standard convolutional models in terms of PSNR and SSIM. Ablation study is present.	1)  Artificial pseudo LR/HR training samples make the SR model have poor generalization for application scenarios. The RPLHR-CT dataset presented in this paper consists of real LR/HR image pairs, narrowing the field generation gap between handcrafted samples and real samples. 2) The writing is great and easy to follow and understand. Moreover, the work is technically novel due to two-fold reasons: (1) the first public real paired dataset RPLHR-CT as a benchmark for volumetric SR; (2) the TVSRN based on the volumetric transformer. 3) The authors re-implemented several state-of-the-art methods tested them on the proposed RPLHR-CT datasets, providing benchmark comparison and reference for volumetric CT SR.	The paper aims at two objectives at the same time (introduce dataset and propose network), which results in non of them are explained in a sufficient manner.	"1) The importance of the ""medium-sized"" dataset is hard to comprehend. Authors mentioned the work of Li et al. [1] that collected 880 real pairs also. What is the main difference with that dataset? 2) It is challenging to understand the difference between ResVox, SAINT, and TVSRN in terms of SSIM in Figure 3. The statistical significance tests need more detail to really show the benefit of the proposed model. A table with confidence interval with these results can be placed in the supplementary material to help the reader. 3) Table 1 shows the importance of the picked architecture, suggesting a performance boost compared to the ViT. However, the rest of modifications differ in the 3rf digits of SSIM. So, the improvement is at best marginal. Perhaps, other image quality metrics are needed to support authors claims."	1) The patch examples of the RPLHR-CT dataset may be supplied in the supplementary. Meanwhile, the visual comparison between RPLHR-CT and the other dataset is recommended to supply.  2) TVSRN should be tested on other datasets to further reveal the robustness. 3) For the visual comparison in Fig. 4, it is better to give quantitative results like PSNR or SSIM. Because the visual differences of the compared methods are not easy to observe, it is more intuitively displayed through quantitative indexes.	Data and code are available at https://github.com/*.	Looks OK. The code is submitted but I did not launch it.	Good reproducibility	"Further Questions: It is not clear to me why a modeling of Long-range (!) dependencies is an issue for volumetric SR. Can you elaborate on this argument for the transformer network a bit more? For the dataset analysis you are saying that ""We use PSNR and SSIM to access the changes in the similarity of three slice-pairs"". However, the PSNR should be computed between a noise free image and the noisy representation. Not as a measure for image quality between CT sets. Can you explain the reasoning behind this measurement? Can you elaborate on the difference for the match slice between the thick and thin CT a bit more? How have you handled motion in the dataset etc. ? where is the external test set coming from? is this also public? are the parameters the same? are the patients the same? benchmark: I was a bit confused, that two of the used benchmark algorithms are changed (cite: ""For ResVox, the noise reduction part is removed. For MPU-Net, we do not use the multi-stream architecture due to the lack of available lung masks.""). By removing parts of the algorithm, the network is changed and it is not longer the originally proposed algorithms.  Can you comment on this? Structure of the paper: there are a lot of graphics in the paper (and supplemental material), but most of them are not mentioned/explained in the paper, and the caption is to short. Therefore, the reader has to figure out the findings by himself. (examples are: Fig. 1. ""Summary of our RPLHR-CT dataset"" is not exactly what i see there. Fig. 2 what are the colorful boxes in B)TAB?. ) the abbreviation on the heading (RPLHR) is never introduced!"	1) More image quality metrics comparisons 2) Some bar plots from Figure 3 will be very valuable in the table form in supp. material	None	I had the feeling that a lot of points are mixed up and not clear to the reader because of the broad range of points that are tried to make here. The pure description of the dataset and baseline and domain gap analysis one one hand or the proposed network and subsequent analysis on the other hand would have been more appropriate considering the paper limit.	"Authors promise to present a new dataset for the benchmarking, yet it is not clear how the dataset is different and if it is any better than Li et al. [1] (880 images vs. 250). Authors should elaborate the advantages of their data. Authors showed the superiority of the model w.r.t. SOTA models. However, it is not clear which parts of the model were important, i.e. TVSRN-Encoder is worse than TVSRN only by 0.002 in terms of SSIM - really marginal and precludes me to give a higher score. Authors ought to provide stronger support for why their model's architecture is advantageous. Phrase ""TVSRN outperforms existing algorithms significantly."" needs to be justified."	Good novelty, adequate work, and great contribution to the community.
431-Paper2738	RT-DNAS: Real-time Constrained Differentiable Neural Architecture Search for 3D Cardiac Cine MRI Segmentation	The paper proposed a differentiable neural architecture search (NAS) method for 3D Cardiac Cine MRI Segmentation. The experimental results found a suitable network architecture with good segmentation performance as well as satisfying the latency and throughput constraints. Experimental results on the extended 2017 MICCAI ACDC dataset showed that the proposed method RT-DNAS obtained better overall results.	The paper presents a method for neural architecture search which considers latency (in this case, 50ms) and throughput (22 fps) constraints.  Latency is incorporated in the loss function and the architectures which do not meet the throughput constraints are ignored and the search is conducted again.  Genetic algorithm is used to create new network architectures.	The authors propose an extension of the Neural architecture search (NAS) to incorporate latency/throughput trade-off for cardiac cine-MRI application. They extend MS-NAS and use a genetic search algorithm to find the optimal paths with proposed trade-offs. Ablation experiments and comparisons with state-of-the-art architectures and other NAS methods demonstrate the benefits of their approach.	* The proposed method RT-DNAS incorporated MS-NAS with a genetic algorithm to consider the latency and throughput constraints * RT-DNAS could be a good application for real-time cardiac MRI images which needs on-the-fly segmentation to avoid noticeable visual lag * RT-DNAS was evaluated on the extended 2017 MICCAI ACDC dataset and obtained overall better results when compared to both manually and automatically designed network architectures	The paper is well-written and provides a good description of the proposed method. The inclusion of latency and/or throughput in NAS has relevance for many other medical imaging applications. The method finds an architecture which produces better performance compared to baselines segmentation and NAS methods.	meaningful extenstion to neural architecture search to optimize for latency. good set of experiments to make a case for their hypothesis.	* MS-NAS [1] is an existing method. The NAS part of the proposed method is also similar to one of the comparison methods HW-Aware NAS [2] and [3] with cell-level and network-level search, the only technic contribution would be the combination with a genetic algorithm which is used for network path selection optimization  * Details of data processing are missing, which may add difficulties to the reproducibility of the paper * The experiments setting is not convincing. the comparison methods may not be the state-of-the-art, there is one paper from Medical Image Analysis that worked on the 3D cardiac cine MRI segmentation with manually designed network architecture and obtained better results [4]. Also, there are other latest NAS methods for medical image segmentation, such as [5]. * The results gap between ICA-UNet and RT-DNAS is quite trivial based one the second row and last second row in table 2. It seems the difference between these two methods will decrease when the hyperparameter n of ICA-UNet and the N of RT-DNAS increases [1] Yan, X., Jiang, W., Shi, Y., Zhuo, C.: Ms-nas: Multi-scale neural architecture search for medical image segmentation. In: International Conference on Medical Image Computing and Computer-Assisted Intervention. pp. 388-397. Springer (2020) [2] Zeng, D., Jiang, W., Wang, T., Xu, X., Yuan, H., Huang, M., Zhuang, J., Hu, J., Shi, Y.: Towards cardiac intervention assistance: hardware-aware neural architecture exploration for real-time 3d cardiac cine mri segmentation. In: Proceedings of the 39th International Conference on Computer-Aided Design. pp. 1-8 (2020) [3] Bosma, M., Dushatskiy, A., Grewal, M., Alderliesten, T. and Bosman, P.A., 2022. Mixed-Block Neural Architecture Search for Medical Image Segmentation. arXiv preprint arXiv:2202.11401. [4] Dong, S., Pan, Z., Fu, Y., Yang, Q., Gao, Y., Yu, T., Shi, Y. and Zhuo, C., 2022. DeU-Net 2.0: Enhanced deformable U-Net for 3D cardiac cine MRI segmentation. Medical Image Analysis, 78, p.102389. [5] He, Y., Yang, D., Roth, H., Zhao, C. and Xu, D., 2021. Dints: Differentiable neural network topology search for 3d medical image segmentation. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 5841-5850).	Some information/comments about the computation/time required for neural architecture search would have been nice.	few bits of information is missing.	Satisfactory, if the code and extended data will be made public.	The paper provides some details but there are still things which are not described sufficiently, probably not possible in the allowed page limit, so reproducing the paper is probably going to be challenging.  If possible, it would be nice to publish the code.	Ok. Authors mention in the reproducibility checklist that the code will be available.	* More thorough comparison experiments should be done. For example, other latest CNN and NAS based methods should be compared as mentioned in the main weaknesses of this paper * Although this paper is not difficult to read, the writing quality could be further improved by careful proofreading. There is one typo in the formulation (2).	"Slightly larger text in Fig. 1 would be more easily readable. Minor language issues: ""In the past a few years,"" -> ""In the past few years,"" ""each paths is formed by"" -> ""each path is formed by"" ""the performance each path can be"" -> ""the performance of each path can be"" on page 4: ""L_{up}"" -> ""L_{ub}"" ""After the all the parameters are fixed"" -> ""After all the parameters are fixed"" ""metohd"" -> ""method"""	In terms of the computation time, how long did it take to run these hyper-parameter searches in the RT-DNAS method? Once the architecture has been settled on, the authors find which optimal path had the highest latency to accuracy trade-off? Can they describe/quantify if it was more Conv or skip connections, downsampling/upsampling? It would be exciting for the readers to see what optimized this final set of layers and overall network architecture was.	* This paper is an incremental work from the prior art, and there are some similar works as mentioned in the weaknesses of this paper * The comparison experiments are not comprehensive. Lacking comparison to the state-of-the-art methods	Latency is relavant for some medical image segmentation problems and the proposed method incorporates it in the NAS and achieves better results than the baseline manually designed methods and other NAS derived methods.	Their logical extension of the MS-NAS to incorporate latency and optimizing for both backed up by good experiments.
432-Paper2234	RTN: Reinforced Transformer Network for Coronary CT Angiography Vessel-level Image Quality Assessment	The paper presents a method to assess image quality of 3D coronary computed-tomography angiography, i.e., with contrast enhancement, scans. Image quality assessment is approached as a binary classification problem per coronary branch. The problem is addressed as multiple instance learning problem using artificial neural networks. As a minor contribution a reinforcement learning-based instance discarding module is introduced that selects the most relevant instances within the multiple instance learning framework for final classification.	This paper formulates CCTA vessel level image quality assessment (VIQA) as a multiple-instance learning (MIL) problem, and exploit Transformer-based MIL backbone (termed as T-MIL) to aggregate the multiple instances along the coronary centerline into the final quality.	In this paper, the authors present a novel Reinforced Transformer Network (RTN) model for the Vessel-level Image Quality Assessment task on Coronary CT Angiography images. This model contains two parts: The transformer-based multi-instance learning (T-MIL) backbone and the Progressive Reinforcement learning based Instance Discarding module (PRID). T-MIL takes the responsibility for feature extraction (from image cube instances), providing states for the latter PRID and final classification. PRID is to prevent the intervention of irrelevant instances. This model outperforms other existing approaches by a large margin on a private dataset.	On the used data collection, the experimental results clearly show the benefit of adopting the suggested approach and additional network components in comparison to using less elaborated schemes. The ambition to formulate the method in a formal mathematical manner has to be appreciated.	The author formulate the CCTA vessel-level quality assessment as the typical multi instance learning problem, and introduce transformer to aggregate multiple instances and map them to final quality. The authors proposed a progressive reinforced learning based instance discarding strategy to mine the most informative instances for transformer network.	Overall, this paper is well organized and all technical modules are well explained. Below are the specific strengths: 1, the idea to leverage the RL based instance discarding strategy to exclude irrelevant instances is novel and proved to be quite efficient in the end to improve the performance. It adopts Markov Decision Process and uses the output of T-MIL as the state. This design enables an end-to-end training pipeline and keeps the architecture light. 2, the transformer based architecture fits well with the dynamic input sequences introduced by the instance discarding process. Experiments show that without PRID, this T-MIL is already a very strong baseline outperforming other existing approaches. 3, the ablation study on the pooling layer inside PRID (Table 3) justifies the choice of PMA versus other common pooling strategies such as max and average pooling.	One of the major weaknesses of this paper is the relatively small amount of data (CCTA scans from only 40 patients) that is used for evaluation. It isn't publicly available, either. From my point of view, the number of small issues pile up to a significant overall weakness of the submission.	The proposed method was evaluated on one hospital which did not include CCTA images with different resolution. Only one doctor did the visual assessment of the quality for the CCTA images. So, the presented work might need more evaluation on its accuracy, effectiveness and robustness	Despite the strengths, I have the following concerns and questions: 1, the dataset is relatively small making the significance of the finding suffer. Also given the fact of a private dataset is used here, more details of the labeling process should be disclosed. 2, since the centerline tracking algorithm comes first place before the proposed algorithm, I am wondering how the performance will be if the failure of this algorithm happens. 3, how is the crop size is determined? Would it affect the final performance?	Challenging: no obvious access to the data and the implementation	The paper describes the method clearly, and listed their training details such as: input size, batch-size, training epochs, loss function and evaluation metrics, so the readers should be able to reproduce the paper.	positive	"The first step in the processing pipeline, i.e., centerline tracing, may suffer from image artifacts as well. I don't see how this is addressed by the suggested method. English spelling and in particular grammar should be carefully double-checked once again. (""We follows ..."", ...) It's not entirely clear to me, which problem is solved at all. Are all centerlines processed at once and then a final quality score for the whole image is computed? The definitions and equations on page 4 are hard to follow: how do I have to imagine an instance embedding Z_L[1:n]? Is this a family of values?  . As an improvement one could think of using the following convention for mathematical notation: scalars as regular small letters, vectors as small letters in bold, matrices as capital letters in bold, sets as capital letters with \mathcal, functions also with domain and value range ... PRID: from the explanations I can't relate the reinforcement learning strategy to a Markov decision process as stated I the paper. Minor things  . The number of discarded instances seems to be a free parameter of the system. How shall it be chosen in practice? Isn't it data dependent?  . The term ""follow-up"" on page 1 might not be used in the right way as ""follow-up"" most often refers to examinations carried out days or months later.  . ""the coronary artery""? I'd rather speak of ""the coronary arteries"".  . I'd recommend introducing the abbreviations again in the main text not only in the abstract.  . ""SOTA performance"" for ""state-of-the-art performance"": I never heard this abbreviation.  . Sentences shouldn't be started with abbreviations.  . Abbreviations are use inconsistently: e.g., ""Fig."" vs. ""Figure"".  . Word repetition ""MIL aggregators in MIL methods""  . The abstract speaks of ""above two modules"" while there's been only one introduced before. I don't understand this."	The proposed method classify the quality of the vessel by first removing the instances that not relevant for determining the quality of the vessels, then using the remaining instances to decide the quality of the vessel. The authors did some augmentation on the training dataset, could you please provide the volume of the dataset after augmentation? Different coronary arteries have different size and different length, the proposed method used the same number of cubes (n=19). Could you please add the details on how do you deal with the different lengths? Table l compares the results of the proposed method with state-of-art methods which also includes the results with/without PRID. Table 2 compares the results with different discarding numbers. It seems T-MIL without PRID has higher accuracy than the PRID with less amount of discarding number of instances. Could you please help me understand this? The authors provided supplementary materials for the propose method but lack of reference or description of the contents in the supplementary materials. Please consider add one or more reference sentences to the Figures, especially the Figure1. And Based on Figure1, it will be difficult to claim that the remaining examples are mainly concentrated in the front of coronary artery since index 2,4,7 has low frequency and, only 1,3,5,12,17 has a frequency more than 0.5. So, I would like to suggest the user to rephrase the claim/conclusion on Figure1. Instead of saying that the main concentration was on the front of the coronary, the conclusion could be that the assumption that only limited instances play important role in the VIQA is correct.	please address the concerns and questions above.	Mathematical formulation not clear enough Experiments only on inhouse data and with inhouse implementations of alternative approaches Conceptual flaws  . Fully automatic centerline tracing as preprocessing step, whose result may as well be impacted by poor image quality  . Are really all conceivable kinds of artifacts handled by the method and properly represented in the data collection?	The paper is well-written and well-organized. The proposed method formulate the image quality problem as a multi-instance learning problem, and combined with reinforcement learning to remove irrelevant features which makes the network focus more on important features. And the authors did ablation study and comparison experiments to show the effectiveness of the proposed method.	The whole idea of transformer based multi-instance-learning +  RL based instance discarding model is novel.
433-Paper0279	S3R: Self-supervised Spectral Regression for Hyperspectral Histopathology Image Classification	This paper presents a self-supervised spectral regression (S3R) to address the problem of self-supervised pre-training for hyperspectral histopathology image classification. Specifically, S3R consists of two pretext tasks (BR and CR) to learn a general representation for down-stream tasks. Experimental results on PDAC and PLGC datasets evaluated the effectiveness of S3R.	The authors have conceptualized an efficient and effective self-supervised spectral regression method which proposes to understand the inherent structures of hyperspectral images and pathological characteristics of different morphologies. The paper is well-motivated, has novelty, and is clinically relevant.	This paper added prior knowledge that each band of the hyperspectral images (HSI) could be represented by a linear mixture of the remaining bands in self-supervised learning to learn the features efficiently and effectively. The proposed Coefficients Regression (S3R-CR) encourages the deep learning models to regress the linear coefficients among multiple bands, while the Band Regression (S3R-BR) converges the pixel-wise differences of the selected band by re-weighting the remaining bands. Masked image modeling was used during self-supervise learning. By exploring the low rankness in the spectral domain of an HSI, The proposed methods achieved better downstream classification accuracy compared with other contrastive learning approaches.	a. They designed a self-supervised method tailored for microscopy HSI classification, which had not been concerned before. It's a novel idea to utilize the properties of hyperspectral images that one band can be represented as a linear combination of the remaining bands.  b. The experiments results show up to 14% improvements.	The idea of characterizing spatiospectral information from hyperspectral images using self-supervised spectral regression of histopathology images is a novel application. Hyperspectral images contain rich information and processing them for accurate diagnosis is challenging. The authors have done a good job in providing sufficient background information, related work, and motivated the study in the right direction. Further, the authors have compared their strategy to the emergent contrastive learning approaches with greater performance and faster convergence which is interesting.	This paper used the intrinsic relationship among different bands of HSI in self-supervised pre-training to extract the low-ranking contexts in the spectral domain. The design of loss is straightforward and concise to the characteristic of HSI. 3.The proposed methods provide an efficient and effective pre-training strategy for unlabeled HSIs with high spatial-spectral dimensions.	The author's introduction and visualization of the data structure of HSI images is not very clear. Compared with regular RGB pathological images, is it just a few more channels? What are the specific challenges for pathologists to analyze HSI images in human vision? For S3R-CR, the necessity of band dropping and is debatable, since the regression targets is the coefficients of the bands that were not dropped. Similarly, for S3R-CR, the necessity of spatial masking is debatable, since the regression target is the missing band. Overall, the masking operation is far fetched. The relationship between CR and BR is not direct. Moreover, the ablation study is insufficient. Image masking and band dropping are not the premise of CR. Image masking is not the premise of BR. So what would the performance of the model be without image masking? Missing standard deviation of ACC.	The authors highlight that S3R method can help explore the morphological characteristics of tissue images. However, no illustration or intuitive explanation is provided to address this contribution. The results from all the experiments having missing error bars to understand the stability of proposed algorithm.	"This paper lacks the literature reviews on unsupervised / self-supervised methods for HSIs[1]. The baseline models are insufficient to demonstrate the improvement. This paper did not evaluate the performance from classical unsupervised-learning methods since the linear coefficients among different bands from HSI can be closed-form solutions, and the features could be mathematically formed. For example: [2]. Meanwhile, this paper did not compare the SOTA deep learning based models for spectral-spatial feature learning of hyperspectral images. For example: [3,4,5]. This paper did not provide the 3-band results of the proposed methods, which is not fair to the baseline contrastive learning models since the selection of bands might highly influence the performance. [1] Ortega, Samuel, Martin Halicek, Himar Fabelo, Gustavo M. Callico, and Baowei Fei. ""Hyperspectral and multispectral imaging in digital and computational pathology: a systematic review."" Biomedical Optics Express 11, no. 6 (2020): 3195-3233. [2] Li, Kun, Yao Qin, Qiang Ling, Yingqian Wang, Zaiping Lin, and Wei An. ""Self-supervised deep subspace clustering for hyperspectral images with adaptive self-expressive coefficient matrix initialization."" IEEE Journal of Selected Topics in Applied Earth Observations and Remote Sensing 14 (2021): 3215-3227. [3] Mou, Lichao, Pedram Ghamisi, and Xiao Xiang Zhu. ""Unsupervised spectral-spatial feature learning via deep residual Conv-Deconv network for hyperspectral image classification."" IEEE Transactions on Geoscience and Remote Sensing 56, no. 1 (2017): 391-406. [4] Yue, Jun, Leyuan Fang, Hossein Rahmani, and Pedram Ghamisi. ""Self-supervised learning with adaptive distillation for hyperspectral image classification."" IEEE Transactions on Geoscience and Remote Sensing 60 (2021): 1-13. [5] Hong, Danfeng, Lianru Gao, Jing Yao, Naoto Yokoya, Jocelyn Chanussot, Uta Heiden, and Bing Zhang. ""Endmember-guided unmixing network (EGU-Net): A general deep learning framework for self-supervised hyperspectral unmixing."" IEEE Transactions on Neural Networks and Learning Systems (2021)."	The obtained results can, in principle, be reproduced if given access to the missing resources (code, data).	The authors have provided sufficient implementation details for reproducibility of the paper.	Regarding the reproducibility of this work, the description in the method section is sufficient to understand. However, some of the detailed implementations are missing for reproduction.	BR and CR are two reasonable pretext tasks for HSI image. However, the relationship between image masking and these two tasks is not direct, so the introduction of MIM model is abrupt. More theoretical analysis and experiments on the necessity of MIM for BR and CR are welcome.	Could the authors provide qualitative assessment of the proposed method? For example, how does S3R method help in exploiting the morphological characteristics in a sample tissue image. Highlighting the spectral bands and showing the result of this approach can help better understand the learning strategy. Next, the authors claim that S3R forces the network to understand the inherent structures of HSIs. Could the authors expand on this idea? Can the authors provide an insight into the failure modes and if it could be expanded to datasets from other modalities?	"In the implementation details, some parameters, such as batch sizes of all methods, are missing, and the details of the fine-tuning stage are missing (freezing parts of parameters, the structures of the classifier, etc.). In Fig. 3, it would be nice to see the ""ground-truth"" coefficient Visualizations and then compare the similarities between the two proposed methods. It would be interesting to see the quantitative results and illustration about ""S3R converges at least 3 times faster"" from the abstract."	The novelty and the completeness of experiments.	The paper presents a novel approach that is shown to provide a superior classification performance and training efficiency compared to contrastive learning. However, the paper has some limitations detailed above which needs to be addressed.	The proposed strategies from this paper seem interesting in Spectral-Spatial Feature Learning for HSIs, but more evaluation results need to be provided to demonstrate the improvement.
434-Paper0610	S5CL: Unifying Fully-Supervised, Self-Supervised, and Semi-Supervised Learning Through Hierarchical Contrastive Learning	This paper presents a new self-supervised framework, S5CL, by devising three contrastive losses defined for labeled, unlabeled, and pseudo-labeled images. Specifically, it devised supervised contrastive losses (SupConLoss) to unlabeled data and integrate it in training with lableled data. Experiments demonstrate the effectiveness of the proposed method and ablation study shows the effectiveness of each component.	To relieve the Pixel-wise annotation workload of histopathological data, this paper  proposes a novel framework, called S5CL, that unifies fully-supervised, self-supervised, and semi-supervised learning through hierarchical contrastive learning. With three contrastive losses defined for labeled, unlabeled, and pseudo-labeled images, S5CL can learn feature representations that reflect the hierarchy of distance relationships between images with respect to their class labels and consistency with different degrees of augmentations. Also, the resulting framework is easy to use and highly flexible.	This paper present a deep learning framework that is trained using a combination of supervised, semi-supervised and self-supervised losses. The proposed methodology borrows ideas from different SOTA approaches and combines them in a comprehensive way. Specifically, it utilizes two paths that correspond to labeled and unlabeled examples, for each of them appropriate losses are utilized (i.e. cross-entropy and supervised contrastive loss in different configurations). The authors show results on two medical tasks (multi-class H&E tile classification, multi-class single cell blood cytology images classification)	Well organized and written.  This work analyzes the problem of self-supervised learning in medical image field and introduces the method logically. Motivation is clear and the method is novel.  It extend the SupConLoss to devise loss in supervised, semi-supervised, and unsupervised levels. Improvement seems okay.  Compared with three most relevant baselines, the improvement is about 1~2% with very limited training samples. And ablation study shows the effectiveness of different loss terms.	(1) This paper proposes a novel framework, called S5CL, that unifies fully-supervised, self-supervised, and semi-supervised learning through hierarchical contrastive learning. Experiments show the effectiveness of this method: for a H&E-stained colorectal cancer dataset, the accuracy increases by up to 9% compared to supervised cross-entropy loss; for a highly imbalanced dataset of single white blood cells from leukemia patient blood smears, the F1-score increases by up to 6%. (2) The resulting framework is easy to use and highly flexible: one can omit unlabeled images and train fully-supervised; also can set the weights of the supervised and semi-supervised loss to zero and train self-supervised; or train with both labeled and unlabeled images in a semi-supervised way.	The paper is very well written and organized, the figures are clear and one can easily follow it. The combination of supervised together with semi/self-supervised losses is well motivated in the field of computational pathology. The experimental configuration is convincing and well presented.	"Novelty Authors should more clearly differentiate the proposed method from most relevant methods, such as BYOL, SupCon. Experiments and results (1) In experiments, authors choose three methods (i.e., CE; CE+SupCon; semi-sup MPL). However, pure self-supervised baselines are not includes. I strongly suggest authors include SOTA self-supervised methods as well. (2) The results on NCT-CRC-HE-100K dataset are very high while the results of Munich AML Morphology dataset are very low. Is it because the second task more challenging? Why? How is the performance influenced by the difficulty of dataset?  (3) On Munich AML Morphology dataset, the results of MPL are strange (i.e., the F1 score is not increasing with more labeled data).  (4) Authors say that the learned features of self-supervised methods and supervised methods are different. But in Fig. 3, we can see that different clusters are clearly separated. Although ""weakly augmented images and similar images are embedded the closest to their origins, then comes strong augmentations as well as different looking images from the same class"". How do it contribute to final classification performance? Why is the embedding meaningful?  Authors also claim that ""it also makes the feature embedding space more compact and explicable"". I don't see why it is more explicable. For example, given an unknown image (suppose we do not know the label in advance), the trained model may categorize it into a certain cluster, but how do we know the relationship between this image and all other points in the feature space? And what can we learn from it?"	(1) This work is related to fully-supervised, self-supervised, and semi-supervised learning. But in the paper, the relationships and differences among the three are not clearly stated, which may lead to confusion for readers. It is suggested to improve this problem in the Introduction. (2) In the experiment, all models use the same encoder ResNet18. But there are many other advanced CNNs, and how about other backbone networks as encoders?	The proposed work is quite similar to some of the referenced studies (i.e., [25]) There are some details that are missing and can further enhance the quality of the paper (refer to details comments).	Authors provide enough information on method details and experimental settings.	Good. All methods used for the proposed framework are depicted clearly and noted with appropriate references.	Code release promise after acceptance. Public datasets utilized	First, address my major concerns above.  Second, please proofread the paper and correct typos and minor issues.	(1) After a fixed number t of epochs e, the classifier can be applied to Z1U , yielding pseudo-labels. It may not be a good way , it is suggested to use the predicted confidence level to determine whether pseudo tags are acceptable or not, referring to FixMatch. (2) Generally speaking, in the combined loss function, adjusting the weight of the hyperparameters is very important, has a significant impact on the model. It is suggested that the value of parameters should be more fully explained or ablation experiments should be provided. (3) Figure 4d shows the ablation study of pseudo-labels. It can be seen that the effects of  pseudo-labels are not always positive. Especially, pseudo-labels in Lc and LL are even worse than no pseudo-labels. So, a more detailed analysis is recommended. (4) In the experiment for Munich AML Morphology, there are only comparison result of fully-supervised and self-supervised methods, but no self-supervised algorithm. It is suggested to provide the results of a self-supervised algorithm for comparison on this dataset.	How is the proposed different from the related work (e.g. [25])? Why different there were different batch sized for the labeled vs the unlabeled paths? How the batch size was tuned? Not clear how the error bars on Fig 2 were calculated. Details are missing from the text. More information on the classification task could help the reader (e.g. 9-way and 11-way classification). There are not so much details in the text. a),b),c) are missing from Fig. 2. Some references are missing (HED augmentation, Macenko's method)	Overall, the main contribution is extending the SupConLoss to multiple levels (i.e., supervised, semi-supervised, and unsupervised). Although the novelty seems somehow incremental, extensive experiments demonstrate the effectiveness of the proposed method and it is beneficial to the community. The major concerns are on experiments.	I make the decision mainly according to the novelty of the proposed framework and the organization of this paper.	I am recommending a weak accept mainly due to some missing details in the text and the fact that the proposed method heavily borrows from already published literature. However, I believe it is still has a merit for the community.
435-Paper0710	Sample hardness based gradient loss for long-tailed cervical cell detection	To deal with existing problems in datasets with a long-tailed distribution, a novel concept of sample hardness is introduced in order to help improve the performance of gradient loss. The hardness is calibrated at sample level with the gradient. Set in a cancer cells detection task, a novel gradient loss is presented where samples are re-weighted according to their hardness, achieving better gradient balance and much higher mAP than cross-entropy loss, surpassing other SOTA methods.	"In this work, the authors propose a method to take the ""hardness"" of each sample into account and rebalance the gradients of positive and negative samples. Specifically, Grad-Libra Loss is proposed to help put more emphasis on hard samples in both head and tail categories. Experiments show that 7.8% mAP improvement is achieved on TCT WSI image dataset using the proposed loss function."	1.This paper provides a new loss re-weighting method for tackle long-tailed problem by utilizing gradient information of each sample. 2.This paper proposes to use gradient information to measure the hardness of each sample in long-tailed learning problem. 3.The method of this paper achieves better results than previous methods for tackling long-tailed problems.	An interesting method to dealing with long-tailed datasets. A complete derivation for the loss function is given and the basic idea behind it is well explained. A proper dataset is created to imitate the real-life situations, which is large-scaled and has a strong data imbalance. The impact of the hyper-parameters in the loss function is separately studied and a proper analysis explained their significant contribution. The Grad-Libra loss is applied to other detectors, showing its generalization ability.	(1) The proposed Grad-Libra loss is relatively simple and intuitive. (2) The paper is well written and easy to follow with.  (3) Extensive experiments are performed to demonstrate the effectiveness of the proposed loss function on different detectors, both head and tail classes etc.	1.The experiments results are comprehensive and the qualitive analysis precisely illustrate the improvement. 2.Author uses gradient information of each sample to re-weight the cross-entropy of each class so that the method can solve long-tailed at sample level. This is an innovative loss re-weighting method. 3.This paper firstly summarizes the existing scheme for solving long-tailed problem and then this paper clearly claims what sets this work apart from other works. 4.Illustrated explanation about how the method works.	The rule of choosing hyper-parameters of the loss is vague. Lack of axis labels in Fig.4. The scale of the dataset is missing, hence it's doubtful that the quantity of rare samples is large enough to make the experiment results convincing. More detail should be provided about how the variable g is gained in Eq.3. Where do the output logits come from? Lack of insightful analysis. The idea is somewhat similar to Focal Loss, how's the nature different from that of Focal Loss?	(1) There is a lack of systematic methods to determine the parameters \alpha+ and \alpha-. As discussed by the author, \alpha+ and \alpha- are important parameters to tune in order to achieve satisfactory effect, it would be useful to see how these parameters can be determined in a systematic way. (2) Minor: isn't equation (3) a monotonically increasing function depending on g?	1.The performance with different value of modulating factor  alpha has obvious fluctuations. The value selection of modulating factor  alpha requires more detailed discussions and verification; 2.The method seems almost irrelevant to medicine. This method doesn't utilize any medical prior or clinical knowledge; 3.Although this method provides an amazing performance boost and shows comprehensive benchmark results, this method is only benchmark on private dataset. The experiment results may be a little bit incredible;	The dataset didn't go public, but the implementation details of the main detector are described, so it would be reproduced should the dataset be accessible. The details of other detectors (FCOS, ATSS, YOLOF) are not provided.	The experimental dataset is not public available neither with the code. It would be preferable if the author can publish their dataset and the code upon the acceptance of the paper.	1.The codebase of this paper is based on mmdetection. So, the code is well-structured and the results can be easily reproduced; 2.The collection of long-tailed dataset used by this paper is time-consuming. As far as I know, there is not a public dataset about long-tailed cell detection. However, the experiment results are totally dependent on the private dataset. If this dataset will not be open-sourced, the reproducibility of this paper will be greatly reduced.	See 5.	(1) It would be great if the author can provide systematic way to determine the hyper-parameters used in their method. (2)  It would be preferable if the author can publish their dataset and the code upon the acceptance of the paper, so the work can be fully reproducible.	1.Fig2 provide a visualization example about how the grad-libra loss works. But the weights after F are illustrated unclear by colorful circle. Expressing the weights after  F as concrete math number may be better. 2.It is better to provide visualization results of cell detection which can make your paper more impressive.	The method is novel and the experiment results prove good. But the rationality of gradients being related to sample hardness needs insightful explaining. And the scale of the dataset needs to be clarified. I'd consider raising my score on this paper should proper explanation be provided.	There is few work focusing on the hardness of the samples in the classification task. This work provides a unified framework to take the hardness into account for both head and tail classes. The extensive experiments show the effectiveness of their methods especially for the rare classes.	1.Clear description about how the method works; 2.Clear states about how it differs from existing methods; 3.Amazing performance improvements;
436-Paper0559	SAPJNet: Sequence-Adaptive Prototype-Joint Network for Small Sample Multi-Sequence MRI Diagnosis	this paper proposed to use Transformer and additive-angular-margin loss for small sample multi-sequence MR image classification. However, this paper has several major flaws and fails to illustrate their point: The first component, Transformer model, is claimed to filter intra-sequence features and aggregate inter-sequence features, based on attention mechanism. But I didn't see any details about how to achieve these 2 goals in section 2.1. The overall writing quality is poor and difficult to follow. Similarly, neither the section 2.2, prototype optimization strategy, illustrates how to approximate the intra-class prototype and alienate the inter-class prototype. For example, the query sample q^a and support sample s^b corresponding to which modalities, the loss_2 should be explicitly expressed like loss_1, and why you choose the additive-angular-margin loss instead of the ordinary cross-entropy loss for classification. Fig 2 is very ambiguous and lacks sufficient explanation.	This paper proposes a new deep learning method for optimizing disease diagnoses that rely on small sample multi-sequence magnetic resonance imaging (MRI) data. Specifically, using a common neural network known as ResNet50 as the backbone, the study has developed a SAPJNet approach that: 1) behaves like a sequence-adaptive transformer to generate joint feature representations of disease prototypes, and 2) constrains the prototype distribution through a prototype optimization strategy. Experiments using MRI of heart and knee show that the SAPJNet is 6-10% more accurate than four other related neural networks.	The authors introduce a sequence adaptive transformer based architecture that generates joint representations according to disease prototype. The paper deals with how to generalize disease classification from MR images despite the small sample size and presence of multiple sequences.	"this paper proposed to use Transformer and additive-angular-margin loss for small sample multi-sequence MR image classification. However, this paper has several major flaws and fails to illustrate their point: The first component, Transformer model, is claimed to filter intra-sequence features and aggregate inter-sequence features, based on attention mechanism. But I didn't see any details about how to achieve these 2 goals in section 2.1. The overall writing quality is poor and difficult to follow. Similarly, neither the section 2.2, prototype optimization strategy, illustrates how to approximate the intra-class prototype and alienate the inter-class prototype. For example, the query sample q^a and support sample s^b corresponding to which modalities, the loss_2 should be explicitly expressed like loss_1, and why you choose the additive-angular-margin loss instead of the ordinary cross-entropy loss for classification. Fig 2 is very ambiguous and lacks sufficient explanation. E.g., what are the vectors besides the local significance block, how to aggregate into global correlation (through concatenation, pooling, or others), where does the support prototype come from, where are the 2 stages in Prototype optimization strategy, and how to approximate intra-class prototypes and alienate inter-class prototypes. The experiment section lacks the explanation of 3 modalities of data (SAX, LAX, and LGE). Some minor issues: a. P2, 1st paragraph, the hyperparameter p's explanation is ambiguous. b. Section 2.2 mentions robust classification. So what kind of attack methods you are dealing with? c. Section 2.2 2nd paragraph says ""two outputs of the SAT ..."" what does the two outputs referring to? d. Still in Section 2.2 2nd paragraph, ""The former is reserved for ..."". After this, where is the latter one? What's the purposes of two stages here? e. Table 3 didn't explain what's VOT."	1) Developed a new neural network approach to deal with common clinical MRI issues 2) Compared the proposed method with several other methods in the literature 3) Added a brief 'ablation study' to gain insight of the proposed technique	Different sequences of MR images depicting structure, motion are utilized effectively for the classification purpose. Redundant intra-sequence features have been filtered using self-attention while inter-sequence feature correlations have been explored. Instead of simple fusion, multiple sequence features have been fused effectively. The authors introduce a contrastive learning inspired loss function to better classification performance even in sparse data scenarios. The paper groups like and unlike pairs according to disease category and alienates similar class prototypes. The results section is nicely presented.	"this paper proposed to use Transformer and additive-angular-margin loss for small sample multi-sequence MR image classification. However, this paper has several major flaws and fails to illustrate their point: The first component, Transformer model, is claimed to filter intra-sequence features and aggregate inter-sequence features, based on attention mechanism. But I didn't see any details about how to achieve these 2 goals in section 2.1. The overall writing quality is poor and difficult to follow. Similarly, neither the section 2.2, prototype optimization strategy, illustrates how to approximate the intra-class prototype and alienate the inter-class prototype. For example, the query sample q^a and support sample s^b corresponding to which modalities, the loss_2 should be explicitly expressed like loss_1, and why you choose the additive-angular-margin loss instead of the ordinary cross-entropy loss for classification. Fig 2 is very ambiguous and lacks sufficient explanation. E.g., what are the vectors besides the local significance block, how to aggregate into global correlation (through concatenation, pooling, or others), where does the support prototype come from, where are the 2 stages in Prototype optimization strategy, and how to approximate intra-class prototypes and alienate inter-class prototypes. The experiment section lacks the explanation of 3 modalities of data (SAX, LAX, and LGE). Some minor issues: a. P2, 1st paragraph, the hyperparameter p's explanation is ambiguous. b. Section 2.2 mentions robust classification. So what kind of attack methods you are dealing with? c. Section 2.2 2nd paragraph says ""two outputs of the SAT ..."" what does the two outputs referring to? d. Still in Section 2.2 2nd paragraph, ""The former is reserved for ..."". After this, where is the latter one? What's the purposes of two stages here? e. Table 3 didn't explain what's VOT."	1) Lack of details in Method 2) The output of the networks is unclear 3) Lack of justification of the steps chosen in image preprocessing, and in some parameters chosen in Method 4) A few grammar issues	1) Why did the authors choose cosine similarity as a correlation method? Also it would be great to have some ablation in terms of losses employed in alienating features. Please compare against other naive similarity losses - like modified Barlow twins, SimSiam etc 2) The authors came up with a transformer-based learning paradigm. Can you please explain 2.1 in more details - especially how the self-attention is modeled to filter intra-sequence features. A few mathematical equations would be great. The local significance section is vague. 3) Since the authors called it a Sequence adaptive transformer, what will happen if one or two sequences are missing for few patients but available for others? Like motion is available for one and not for another. It's a common and practical scenario for MRI input. 4) I get how the authors 'approximate intra-class prototypes' and 'alienate inter-class prototypes' through the loss function. But it would be good if they can give a more clear explanation regarding it. Also, only their mention without any context in Fig 2 makes it a bit hard to understand.	"this paper proposed to use Transformer and additive-angular-margin loss for small sample multi-sequence MR image classification. However, this paper has several major flaws and fails to illustrate their point: The first component, Transformer model, is claimed to filter intra-sequence features and aggregate inter-sequence features, based on attention mechanism. But I didn't see any details about how to achieve these 2 goals in section 2.1. The overall writing quality is poor and difficult to follow. Similarly, neither the section 2.2, prototype optimization strategy, illustrates how to approximate the intra-class prototype and alienate the inter-class prototype. For example, the query sample q^a and support sample s^b corresponding to which modalities, the loss_2 should be explicitly expressed like loss_1, and why you choose the additive-angular-margin loss instead of the ordinary cross-entropy loss for classification. Fig 2 is very ambiguous and lacks sufficient explanation. E.g., what are the vectors besides the local significance block, how to aggregate into global correlation (through concatenation, pooling, or others), where does the support prototype come from, where are the 2 stages in Prototype optimization strategy, and how to approximate intra-class prototypes and alienate inter-class prototypes. The experiment section lacks the explanation of 3 modalities of data (SAX, LAX, and LGE). Some minor issues: a. P2, 1st paragraph, the hyperparameter p's explanation is ambiguous. b. Section 2.2 mentions robust classification. So what kind of attack methods you are dealing with? c. Section 2.2 2nd paragraph says ""two outputs of the SAT ..."" what does the two outputs referring to? d. Still in Section 2.2 2nd paragraph, ""The former is reserved for ..."". After this, where is the latter one? What's the purposes of two stages here? e. Table 3 didn't explain what's VOT."	No mentioning about it, except listing of sources for the comparison methods	The authors have provided necessary parameter deatils. They intend to make the code public.	"this paper proposed to use Transformer and additive-angular-margin loss for small sample multi-sequence MR image classification. However, this paper has several major flaws and fails to illustrate their point: The first component, Transformer model, is claimed to filter intra-sequence features and aggregate inter-sequence features, based on attention mechanism. But I didn't see any details about how to achieve these 2 goals in section 2.1. The overall writing quality is poor and difficult to follow. Similarly, neither the section 2.2, prototype optimization strategy, illustrates how to approximate the intra-class prototype and alienate the inter-class prototype. For example, the query sample q^a and support sample s^b corresponding to which modalities, the loss_2 should be explicitly expressed like loss_1, and why you choose the additive-angular-margin loss instead of the ordinary cross-entropy loss for classification. Fig 2 is very ambiguous and lacks sufficient explanation. E.g., what are the vectors besides the local significance block, how to aggregate into global correlation (through concatenation, pooling, or others), where does the support prototype come from, where are the 2 stages in Prototype optimization strategy, and how to approximate intra-class prototypes and alienate inter-class prototypes. The experiment section lacks the explanation of 3 modalities of data (SAX, LAX, and LGE). Some minor issues: a. P2, 1st paragraph, the hyperparameter p's explanation is ambiguous. b. Section 2.2 mentions robust classification. So what kind of attack methods you are dealing with? c. Section 2.2 2nd paragraph says ""two outputs of the SAT ..."" what does the two outputs referring to? d. Still in Section 2.2 2nd paragraph, ""The former is reserved for ..."". After this, where is the latter one? What's the purposes of two stages here? e. Table 3 didn't explain what's VOT."	This is an interesting study addressing important questions associated with deep learning using real-world medical imaging data, such as MRI. The proposed method appears to be able to overcome 2 critical limitations of clinical MRI: small sample size, and multiple sequence acquisitions. However, there are a few questions about the paper that deserves further attention as seen below. 1) The method is largely unclear. For example, in section 2.1, how 'the SAT accurately extracts features and generates prototypes that mimic a doctor's overall assessment of ...'? How are 'these representations aggregated and translated into new semantics'? Further, how are the parameters 'p' and 'delta' set and what do they mean? 2) In section 2.2, how are the positive and negative sample pairs calculated; what does the 'supervisory information' refer to; and what is the relationship between 'pre-prototype' and 'prototype'? In addition, while there is an equation (#2), adding additional explanation for the 'additive-angular-margin loss' would help. 3) In Fig. 2, how was the 'filter intra-sequence features' step done? What's the usage of 'padding' here - expand the cropped images to 90x90? How to 'approximate intra-class prototypes' and 'alienate inter-class prototypes? 4) In Experiments and Results (section 3), the output of the proposed and comparison networks are not defined; multiple abbreviations, 'PAH', 'IIM', 'LGE' et al, are not defined at first use. 5) Also in the above section, it says that 'different sequences of the same patient, although ..., were not spatially aligned'. Is this beneficial or harmful, and why? Related to this point, in image preprocessing, the MR images do not seem to be normalized, in any dataset. How would that impact network performance, and why is that preferred? 6) In Fig. 3, it is unclear what 'abnormalities' are supposed to be seen despite the use of arrows in the panels. Including an example of normal image would also help. 7) In section 3.3 (page 8), it is unclear what this part means: 'As shown in Fig. 4, compared with the baseline method, it can be seen that with the reduction of training samples, the performance of the SAPJNet is better, and its performance loss is smaller in the five training'. In Fig. 4 (bottom plots), it appears that the sample size is increasing from 20 to 40 instead of 'decreasing' as the network performance increases. Please verify.	Please address the issues listed in weakness.	this paper proposed to use Transformer and additive-angular-margin loss for small sample multi-sequence MR image classification. However, this paper has several major flaws and fails to illustrate their point: The first component, Transformer model, is claimed to filter intra-sequence features and aggregate inter-sequence features, based on attention mechanism. But I didn't see any details about how to achieve these 2 goals in section 2.1. The overall writing quality is poor and difficult to follow. Similarly, neither the section 2.2, prototype optimization strategy, illustrates how to approximate the intra-class prototype and alienate the inter-class prototype. For example, the query sample q^a and support sample s^b corresponding to which modalities, the loss_2 should be explicitly expressed like loss_1, and why you choose the additive-angular-margin loss instead of the ordinary cross-entropy loss for classification. Fig 2 is very ambiguous and lacks sufficient explanation.	This paper addresses 2 common and important questions facing the use of clinical MRI in deep learning. The proposed method shows reasonable result compared to several other approaches. However, the study has several limitations as mentioned above.	The paper is well written. The methodology proposed to handle sparse distribution and mylti-sequence images is novel. They utilize SOTA attention mechanism and contrastive learning in different stages of their architecture. The results show a justifiable improvement when compared against other methods.
437-Paper0153	SATr: Slice Attention with Transformer for Universal Lesion Detection	This paper presents an approach to integrate inter-slice information for universal lesion detection. Results show consistent improvements over various baseline approaches.	This paper propose a novel Slice Attention Transformer (SATr) block that could be easily integrated with various CNN models. Experiments show that the proposed method improves the detection performance on Universal Lesion Detection (ULD) task under the settings of both full or less training data.	This paper proposes a novel Slice Attention Transformer block (SATr) for universal lesion detection. The proposed SATr can extract features from both individual and multiple slices. It can also be integrated into multiple network structures. Experiments are conducted on DeepLesion. Promising results are achieved.	"The idea of the paper is simple but solid. This paper presents a natural way to combine transformers into the multi-slice fusion problem. The paper is well-written and easy to follow. The figures are illustrative. The results are promising. The authors compare various baselines and show their innovation is a ""plug-and-play"" module with consistent improvements over the baseline methods."	A novel transformer block for multi-slice-input ULD backbone is proposed. Compared to the naive version, the proposed method has clear motivation: (1) enhancing the key-slice feature, as the key-slice is where the supervision applied; (2) using the adjacent slices as query and key to further strengthen the key-slice representation; Ablation study also clearly demonstrates the contribution of each modification; Extensive evaluation on multiple networks to show the effectiveness of the proposed SATr block.	Strength: 1.This paper is well written and organized. It is easy to follow and read. 2.The proposed SATr is easy to understand and implement. It is easy to transfer to other network structures. 3.The comparison in Table 1 between the vanilla network and the version with SATr shows that the proposed SATr does help with the lesion detection.	The equations in the method section is messy. List the whole workflow as equations is unnecessary and not essentially a mathematical contribution. A better way is just show an algorithm flow or pseudo codes, which would be much better and clearer. My recommendation is only keep the most important innovation as mathematical equations.	The final design of SATr block, while has strong motivation, still needs more justification; for example, why we should use adjacent slices as query and key? can we simply use key-slice feature as query and all-slice feature as value? The analysis of model size and flops are not provided; as the proposed block will introduce additional computation overhead, it is important to see the trade-off between accuracy improvements and computational requirements;	Weakness: 1.For the ablation study in Table 3, the authors ablate the few components. How about different hyper-parameters in the SATr? Would it be sensitive to the hyper-parameters. 2.I think the CAM figure is interesting. Could the authors kindly offer some comments or constructive explanation based on their understanding please?	Good reproducibility if code can be provided.	The authors claim that code will be published. Also, sufficient implementation is provided in the paper.	Can be reproduced	Please refer to my weakness part.	"The writing needs improvement and there are a few typo/grammar errors, e.g. Page 4 ""overfittin"", Page 5 ""Kernal"", Sec 3.2. full training results should be in Table 1; Please provide more justification on the design of SATr block as discussed in the above weakness section; and/or provide ablation result on, e.g. using key-slice as query and all-slice as key and value; Provide analysis on model size and flops to see the computational overhead introduced by the SATr block;"	Overall, I think this is a very good paper. If the authors could offer some useful feedback, that would be lovely.	This is a solid paper with minor issues. The idea is not too novel but is solid and so are the experimental results.	This paper propose a novel way to enhance the key-slice feature on ULD task, by using the self-attention block and modifying the query, key and value component in a reasonable way. However, the final design needs more justification. On the other hand, the paper lack of analysis on the introduced additional computation overhead and the performance improvement is not significant.	See the strength
438-Paper2686	Scale-Equivariant Unrolled Neural Networks for Data-Efficient Accelerated MRI Reconstruction	This paper proposed scale-equivariant unrolled neural networks by modeling the proximal operators of the networks with scale-equivariant CNNs to improve the data-efficiency and robustness to variants for reconstructing MR images from undersampled k-space data.	(1) This paper proposed a scale-equivariant unrolled network for MRI reconstruction. (2) The experiments demonstrate that the proposed approach outperforms state-of-the-art unrolled neural networks	The paper introduces the use of scale-equivariant (scale and translation) networks for learned unrolled reconstruction methods, where the proximal operator is replaced by a neural network. The geometric constraint on the network offers a better performance without the need for data augmentation.	This paper newly combined scale-equivariant CNNs with unrolled nerual networks for reconstructing undersampled MR images which could be different from conventional deep-learning-based MR image reconstruction methods.	(1) The paper is very well written and the presentation is clear. (2) As far as my knowledge goes, inserting a scale-equivariant proximal network for MRI reconstruction is a novel and interesting idea. (3) Good evaluation: comparison to state-of-the-art unrolled neural networks, generalization validation.	The proposed network architecture provides better reconstruction results even under constraints on the network. The use of scale equivariance is well motivated for practice.	1) The proposed model lacks novelty. It seems that the proposed model was built by simply changing the existing CNNs with pre-existed scale-equivariant CNNs. To highlight the novelty and demonstrate the effectiveness of the proposed model, more explicit explanations and rigorous experiments would be needed. 2) Although quantitative results showed better performance than the baseline (Table 1, 2), it is difficult to see the performance increment in the presented figures (Fig. 1).	(1) Some important references are missing, for example, unrolled neural networks [1][2], equivariant network for inverse problem [3]. [1] Deep ADMM-Net for compressive sensing MRI, 2016 [2] A deep cascade of convolutional neural networks for MR image reconstruction, 2017 [3] Equivariant Imaging: Learning Beyond the Range Space, 2021	It is said that the proposed changes provide data efficient training options. Unfortunately, the only comparison that is done in the study is based on data augmentation vs none. I would have hoped for a stronger support of that claim by reducing the training data size. Reconstruction results are all in supplementary, some results would have been also good in the main body. A limitation to only 90 degree rotations does not take slight rotations of patients into account.	The authors provided details about the proposed models, datasets, and evaluation. As the authors stated, the reference code seems be released after the review process.	The dataset used in this paper is publicly available. This paper has the reproducibility.	Data is publicly available and codes will be provided	1) The specific reasons for the increase in performance by combining the scale-equivariant CNNs and an unrolled architecture is unclear. More explicit explanations and rigorous experiments would be needed. In particular, it seems that the proposed model was built by simply changing the existing CNNs with pre-existed scale-equivariant CNNs. 2) The experiemtns were conducted only with vanilla, scale-eq, and rotation-eq models and lacked comparison with other deep learning-based MR image reconstruction methods. To show the effectiveness of the scale-equivariant CNNs, more rigorous experiments would be needed by applying them to other deep learning-based MR reconstruction methods and compared with other methods. 3) Although the quantitative results of the proposed network showed better performance than the baseline in Table 1 and 2, it is difficult to see the performance increment in the presented Fig. 1. Especially, the difference between Vanilla+ and Scale-Equivariant+ seems to be very minor. Please provide figures that can show the effectiveness of the proposed model.	(1) Please fully investigate the relevant literatures. (2) Give more qualitative comparison results.	The topic of designing networks that work better for smaller amount of data is important, especially when considering limited possibilities to obtain ground-truth references in medical imaging. A curious result is that the authors achieve better performance with the geometrically constrained network than the vanilla version. This is curious, usually slightly worse performance is observed under additional constraints of the network (here it is by including a scale invariance) I would have welcomed a short note on this in section 5. As mentioned above, one of the aims of the authors is to provide data efficient learning. Sadly a study on the influence of data size is missing. The argument is only underlined by eliminating the need for data augmentation, but that is done from the same data set anyways, so it does not reduce the amount of data.	"Even though the experiments showed better performance with the proposed method, the overall opinion is ""weak reject"" because of the mentioned weaknesses and execution of the idea."	The motivation of this work is clear, the method adopted is intuitive, and the experimental verification is sufficient, so I recommend to accept.	It is well motivated and results are good.  Novelty lies mainly in transferring the use of scale-invariant networks to MRI reconstructions, which is limited in novelty.
439-Paper1966	Screening of Dementia on OCTA Images via Multi-projection Consistency and Complementarity	In order to predict dementia from OCTA images, authors proposed CsCp module to abstract the consistent and complement representations from multiple projections of OCTA. The proposed CVF is connected after that for the feature fusion. The experiments conducted on a private dataset show its superior performance over SOTA multi-view fusion frameworks.	This paper proposes a multi-projection consensus and complementarity learning network (MUCO-Net) for dementia screening on Optical Coherence Tomography Angiography images. The proposed method performs very well on their private dataset and open OCTA-500 Dataset.	This paper proposes MUCO-Net to explore OCTA-based dementia diagnosis. MUCO-Net includes a consistency and complementarity attention module and a cross-view fusion module for understanding projective relationships and combining features. The method achieves the best results on two datasets.	To predict dementia from OCTA is attractive, less-explored and of great clinical value. Authors adapted the recently popular scale dot-product attention machnism to fuse the multi-view features of OCTA, which is a good and novel application of deep learning techniques. Experimental results show the proposed method outperforms SOTA multi-view fusion models.	The method proposed in this paper specifically solves the problem of dementia screening on Optical Coherence Tomography Angiography images. The complementarity and multi-projection fusion proposed in this paper is rarely studied in this problem and is an advantage. This article is well organized. In this paper, the background of the problem and the deficiencies of the previous work are clearly explained, and the corresponding modules are designed. There are certain comparative experiments and ablation experiments to illustrate the effectiveness of the method.	(1) The idea about Consistency and Complementarity in Screening of Dementia is interesting. (2) The experimental results in Table 1 are very good. (3) The structure of this paper is clear.	My main concern is about the experiments. First, authors conducted the dementia prediction experiments on a private dataset. It may cause the over-estimation of the proposed method, since the hyper-parameters, training settings and data preprocessing can be specifically desinged for the dataset.  Second, there is no ablation study in the experiments. I think a detailed ablation study is needed for this paper. For example, the effectiveness of consistency attention, complementarity attention, CVF, three different supervisions should be verified.  In addition, authors should also show the effeciency of the model. For example, FLOPS/model size/memory-usage information are recommended to be reported.	There are many hyperparameters in the loss function. The value of the hyperparameters is directly given in the paper, and there is no experiment to illustrate the influence of different hyperparameters. And the settings of the hyperparameters are not explained in the second set of experiments. The attention mechanism designed in the paper is not novel enough. The consistent and complementary attention module is similar to the previous positive and negative attention mechanism, and the cross-view fusion mechanism is similar to the co-attention mechanism.	(1) Novelty may be limited. Residual self-attention in Cross-view fusion module is similar to [1][2][3].  [1]. Exploring Self-attention for Image Recognition [2]. An Explainable 3D Residual Self-Attention Deep Neural Network For Joint Atrophy Localization and Alzheimer's Disease Diagnosis using Structural MRI [3]. Studying the Effects of Self-Attention for Medical Image Analysis (2) The author said that CsCp was gradually added after each feature extraction stage, and it was found that the classification accuracy became better. But why is the result of MUCO-stage3 much lower than other stages? Why is the effect of MUCO-stage1 better than other stages? (3) In Table 2, the result improvement is not very large. Besides, are the model size and test speed inferior to the SOTA approachs?	The method seems reproducible. However, the main experiments are conducted on a private dataset, which undermines the reproducibility.	The reproducibility of the paper is credible.	The authors will provide code after the paper is accepted.	I recommend to replace section 3.2 by ablation study, authors can put extented experiment to supplyment materials Authors should provide some examples of different projections of OCTA. Better to provide FLOPS/model size/memory-usage information It would be more persuasive if the dataset can be publicly avaliable	The authors should provide experiments on hyperparameters to demonstrate the reliability of the selected parameters.	Some minor typos should be noted, e.g., epochs is 200-> The number of epochs is 200. The experimental content is not rich enough, and the analysis should be further strengthened.	The article discussed an interesting topic and provided a well-designed framework to show the feasibility. The proposed method adapted the popular dot-product attention to combine multiple projection features of OCTA, which I think is a good application. However, the experiments are not sufficient to prove the effectiveness of the method.(see point 5 for details)	The paper is well organized, and the proposed method is innovative in dementia screening on Optical Coherence Tomography Angiography images. However, the proposed method is not novel enough.	Idea and results are good, and the task makes sense
440-Paper1416	Scribble2D5: Weakly-Supervised Volumetric Image Segmentation via Scribble Annotations	The paper presents scribble2D5, a weekly supervised deep learning approach to segment volumetric medical images  based on scribbles. The paper proposes an augmentation of a 2.5 attention UNet with a label propagation module to improve boundary predictions. Additionally, they extend an active boundary loss formulation to act in 3D. The method is evaluated on three datasets and results show that the proposed method outperforms the state of the art on two of the datasets.	This paper presented Scribble2D5, a method for 3D anisotropic image segmentation using scribble annotations (a type of weak supervision). A label propagation module and an active boundary loss were proposed to improve performance in terms of Dice score and overall boundary smoothness. Extensive experiments were carried out on multiple datasets to validate the effectiveness of Scribble2D5.	Image annotations sometimes are not easy to obtain in practice because annotating at the image pixel-/voxel-level is time-consuming and needs medical expertise to provide high-quality annotations. The proposed method tries to address these challenges by presenting a scribble-based volumetric image segmentation, Scribble2D5, which tackles 3D anisotropic image segmentation and improves boundary prediction.	This method achieves SOTA performance in 3D using weekly supervised scribble annotations, reducing the gap between fully supervised methods.  The use of scribble annotations provides practical utility as it greatly reduces the workload of manual segmentation for 3D images.  The authors successfully extend previous 2D methods to 3D, in which few alternative options exists. The authors propose label propagation module that uses a combination of existing methods to generate both pseudo masks and pseudo boundaries. Inclusion of these existing methods may provide additional somewhat orthogonal signals to help generate more accurate pseudo labels.	Scribble annotation is a promising type of supervision signal in terms of expense and performance. The paper proposed a scribble-based method and tested on three different medical image analysis datasets, which is of certain clinical feasibility. The experimental protocols were overall clear and results were extensive. In terms of technical contribution, the paper integrated several methods such as SLIC, 2.5D attention UNet, HED, ASPP, into the context of scribble supervised learning. A 3D active boundary loss was proposed based on its 2D ancestor. The ablation studies were well conducted.	It proposes a scribble2D5 network for segmenting medical image volumes with sparse scribbles for training only. It proposes a label propagation module for 3D pseudo mask generation and an active boundary loss to regularize 3D segmentation results. It provides some reasonable results on three public datasets.	"Much of the work is extending [1] to 3D with a few modifications, including: 3D backbone network (2.5 attention UNet) different edge detector use of attention blocks additional static pseudo mask It appears that the previous SOTA scribble methods were trained and designed for 2D images instead of 3D.  The method INExtremeIS, which was designed for volumetric segmentation shows similar performance to the proposed methods using extreme points, which is arguably a less informative signal. It would be better to compare with methods designed for 3D such as [2]. Scribbles are generated for VS and CHAOS datasets through ""erosion"". This process is not cited or further explained.  It should be clearly stated to ensure the these generated scribbles are representative of real scribbles. Additionally, scribbles could be generated on the ACDC dataset and compared to scribbles provided by experts. Why did the authors introduce an extra hyperparameter lambda_2 for active boundary loss that is not present in the 2D formulation (Chen et al., 2019)? [1] J. Zhang, X. Yu, A. Li, P. Song, B. Liu, and Y. Dai, ""Weakly-Supervised Salient Object Detection via Scribble Annotations,"" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2020. [2] H. Kervadec, J. Dolz, M. Tang, E. Granger, Y. Boykov, kai I. Ben Ayed, 'Constrained-CNN losses for weakly supervised segmentation', Medical Image Analysis, t. 54, ss. 88-99, 2019. [3] X. Chen, B. M. Williams, S. R. Vallabhaneni, G. Czanner, R. Williams, and Y. Zheng, ""Learning Active Contour Models for Medical Image Segmentation,"" in Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2019."	The scribble generation protocol of VS and CHAOS datasets may need elaborating. If it is a well-known pipeline, then the paper should cite it. Otherwise, the paper should at least state the parameters that affected the scribble quality, such as length, thickness, distance to boundary, etc.	It misses some related references in the proposed topic. The evaluation and comparison are not sufficient. Some scribble generations are not clinically realistic. More detailed comments are given in the following Sec. 8.	Datasets are publicly available and sufficient information is provided in the paper to reproduce the results with modest hardware.  The abstract states that code is available online.	The paper can be fairly straight-forward to reproduce. Some key modules of the proposed method are already open-sourced, such as SLIC, HED, etc.	Seems okay. The code is/will be available online.	"Please check spelling and grammar page 3 - ""In this way, we have a Label Propagation Module (LPM) to generate 3D pseudo labels from scribbles and images for ROI segmentation and static boundary prediction, respectively"" - This sentence is confusing. page 4 - ""Specifically, At the"" - At is capitalized page 8 - ""... methods and reduce the performance gap..."" - reduce should be ""reduces"" Text in figure 1 is too small to read. The caption should also be more descriptive, describing the overall architecture. In equation 3, Volume_out should be (1-u) instead of u Equation 4 has duplicate L_seg term. One of these terms should specify that the mask is refined"	The paper was technically sound and easy to follow overall. The authors exploited low-level boundary evidence, mid-level super-voxel evidence to achieve high-level segmentation.	There are some concerns of this paper: It misses some related references: Zhao, T., & Yin, Z. (2020). Weakly supervised cell segmentation by point annotation. IEEE Transactions on Medical Imaging, 40(10), 2736-2747. Luo, X., Hu, M., Liao, W., Zhai, S., Song, T., Wang, G., & Zhang, S. (2022). Scribble-Supervised Medical Image Segmentation via Dual-Branch Network and Dynamically Mixed Pseudo Labels Supervision. arXiv preprint arXiv:2203.02106. Zhang, K., & Zhuang, X. (2022). CycleMix: A Holistic Strategy for Medical Image Segmentation from Scribble Supervision. arXiv preprint arXiv:2203.01475. It is unclear what is the major advantage of the proposed method compared with the above related work. In the comparison, they only include two scribble-based segmentation methods. More state-of-the-art weakly-supervised methods need to compare, such as: [UNetD] Valvano, G., Leo, A., & Tsaftaris, S. A. (2021). Learning to segment from scribbles using multi-scale adversarial attention gates. IEEE Transactions on Medical Imaging, 40(8), 1990-2001. Zhang, K., & Zhuang, X. (2022). CycleMix: A Holistic Strategy for Medical Image Segmentation from Scribble Supervision. arXiv preprint arXiv:2203.01475. As for the mask-based segmentation comparison, they only include two U-Net methods. More recent methods should be included, such as: UNetD [Valvano et al., 2021] PostDAE [Larrazabal et al., 2020] ACCL: Adversarial Constrained-CNN Loss ... [Zhang et al., 2020] For the VS and CHAOS datasets, the scribble generation is not real from doctors or clinicians. So the evaluation results on these two datasets are not very convincing. More visualization results and qualitative evaluation should be provided.	The paper produces SOTA results that provides practical utility by reducing the requirements on manual segmentations.  Much of the contribution is adapting other methods to work in 3D.	The experiments were comprehensive and showed promising results.	This paper proposes a weakly-supervised volumetric image segmentation network, Scribble2D5. This method tries to reduce the performance gap between weakly-supervised and full-supervised segmentation methods. Meanwhile, there are some concerns on missing related references, insufficient comparison and evaluation.
441-Paper0453	Scribble-Supervised Medical Image Segmentation via Dual-Branch Network and Dynamically Mixed Pseudo Labels Supervision	In this paper, the authors present a technique for scribble-based medical image segmentation. Their approach features a dual branch network that implements a perturbation-consistency strategy, encouraging the network to produce sensible segmentations despite having only scribbles as supervisory signal. Experiments and ablation studies suggest that the proposed technique is robust and accurate w.r.t several baselines.	This paper proposes a simple yet efficient dual-branch network with one encoder and two slightly different decoders for image segmentation, which combines the scribble supervision and auxiliary pseudo labels supervision and performs better than current scribble-supervised segmentation methods.		The paper addresses a very relevant problem, i.e. the elevated cost of pixel-wise annotations for medical image segmentation. The methodology is novel, sound and interesting. The literature review and comparisons with other available approaches, both in the realm of weakly supervised and semi-supervised methods, are detailed and sensible. The experimental set-up is (mostly) well-devised and the results are convincing. Different settings, ablation studies and comparisons against a rather ample set of baselines allow a rather good assessment of the robustness of the proposed approach. The paper is very well-written and generally easy to follow.	The paper is well-written and easy to follow. The proposed method adopts dual-branch network and a dynamically mixed pseudo labeling strategy to train segmentation models with scribble annotations, which reduces annotation costs and makes good use of a small amount of supervision information. Experiment results demonstrates the effectiveness of the propose method.		Single-dataset experimental set-up: the authors limited their experiments to a single dataset (ACDC). While this is a very well-known dataset, experiments with additional datasets are required to better assess the real performance of the described technique. This being said, I don't think additional experiments are strongly necessary for a MICCAI submission, also considering the ample comparisons with different types of baselines that are available in the paper. Lack of discussion: the authors reported their results against many baselines in Table 1. However, there is no critical discussion of the achieved results. Specifically, do the authors have any idea why their method outperforms RLoss in DSC rather substantially, but do much worse in HD? Also, how can a method that produces rather poor DSC scores like RW provide instead such remarkable HD results? These are important aspects that need to be clarified.	To increase persuasiveness of the proposed method, the experimental settings should be illustrated more detailed, include comparison methods. The authors did not mention how much supervision information used of each comparison WSL methods. The proposed method adopts a dual-branch network, include main decoder, and an auxiliary decoder, the authors should give a comparative experiment between w/o auxiliary decoder to illustrate the effectiveness of auxiliary decoder. The network architecture is critical, it could influence the final results significantly. However,  main decoder+auxiliary decoder has been used in previous work(UDC-Net: Dual-Consistency Semi-Supervised Learning with Uncertainty Quantification for COVID-19 Lesion Segmentation from CT Images, MICCAI'21), which is not a new idea, and the authors should make some analysis between UDC-Net and the proposed method in the experiment.		The reproducibility of the paper is very high. Methods, implementations and settings are quite clear. Codebase is released. Dataset used is public.	The authors released the source code of this paper, and it is not hard to reproduce the experiment.		"Page 6: Table 1 is barely legible. Please improve the layout. There are a few parts of the paper that I find unclear. Page 6-7: ""we trained networks with partially supervised and semi- supervised fashions, respectively. We used a 10% training set (8 patients) as labeled data and the remaining as unlabelled data, as the scribble annotation also takes similar annotation costs [29]"". Page 8: ""3) the proposed approach dynamically mixes two outputs to generate hard pseudo labels for two decoders training separately"". Please re-phrase/clarify these parts. Please consider adding supplementary materials with more qualitative results (as Fig.3)."	The proposed dual-branch network is similar to the mean-teacher architecture, I prefer more discussions about the differences and strengths of the proposed algorithm compared to the mean-teacher architecture. Besides, the authors should add more experiments on other datasets with scribble annotation(such as PASCAL-Scribble Dataset) to improve the persuasiveness.		The paper is highly interesting. It presents a novel technique for scribble-based segmentation, which favourably compares to several baselines. The experimental design is restricted to a single dataset, but thanks to ablation studies and further analyses the results are convincing.	This paper proposes a simple yet efficient dual-branch network with one encoder and two slightly different decoders for image segmentation. And the paper is well-written and easy to understand. However, the proposed method is not a new idea, and the experiment is insufficient. There are several concerns above in the current version of the paper that addressing them will increase the quality of this paper.	
442-Paper1998	SD-LayerNet: Semi-supervised retinal layer segmentation in OCT using disentangled representation with anatomical priors	This work introduced a novel SD-LayerNet as a semi-supervised paradigm into the retinal layer segmentation task that makes use of the information present in large-scale unlabeled datasets as well as anatomical priors.	The paper proposes a novel medel SD-LayerNet to do semi-supervised retinal layer segmentation in OCT. SD-LayerNet makes use of the information in the large unlabeled datasets as well as anatomical priors. The model use both 1D surface and 2D layer information to train. And this network can also work on nested anatomy and where the thickness of a tissue is measured.	This paper focuses on the retinal layer segmentation task under the semi-supervised setting. The authors propose Spatial Decomposition Layer Segmentation Network (SD-LayerNet) with two novel contributions: one is a fully differentiable topological engine which facilitates the disentangled representation learning, another is a set of tailored anatomical priors encoded as self-supervised tasks for unlabeled data. The experiments show that the method is able to achieve state-of-the-arts results under the low-data regime.	Contributions: 1). It is very interesting to propose a semi-supervised paradigm into the retinal layer segmentation task, using of the information present in large-scale unlabeled datasets. 2). In this work, it is promising to notice a variety of reconstruction loss such as Eq (1) and the others proposed in Section 2.2. 3). A comprehensive methodological validation has been included in this work.	This paper solves a very interesting and practical problem, which makes use of unlabeled datasets for semi-supervised learning in retinal layer segmentation task. This paper is well organized, the idea of innovation is concise and reasonable, and it is easy for readers to follow. This paper also points out the potential application for other tasks, e.g. inner and outer vessel lumen wall, cardiac wall, knee cartilage, etc.	The authors propose a fully differentiable topological engine, which converts the surface positions to pixel-wise structured segmentations for anatomical representation learning. The authors propose a series of self-supervised tasks tailored for retinal layer segmentation based on several anatomical priors. The proposed method shows the promise of semi-supervised learning in retina layer segmentation.	Major Concerns: 1). Manually tuning parameters The reconstruction loss proposed in this work involves more parameters tuning. The reviewers are afraid that the parameters tuning would influence the performance of proposed method. 2). Further validations In addition, it is more interesting to show the validation of time-consuming of proposed SD-LayerNet with other peer methods.	Lack of ablation study. If change some of the settings in e.g. anatomical priors, what will happen. The author can do more ablation studies to show the effect of each module of their network. More analysis. Need to show more analysis about the experiment results. The author can discuss more in the results part why this design is better than baseline models. Typos. E.g. Fig 3 caption includes a colored bracket	The role of the textual factor branch in the anatomy encoder is unclear. Too many hyperparameters are involved in the algorithm, including eight loss balance weights and several empirical parameters related to the anatomical priors.	The reproducibility is good. The authors provide the link of source code.	They could reproduce the results with some difficulty.	good	1). Manually tuning parameters Although the reconstruction loss proposed in Section 2.2 is very promising, it would be challenging to determine all lambda values in reconstruction loss for self-supervising. If all lambda values for reconstruction loss are manually designed, the promising segmentation results of proposed SD-LayerNet should be arbitrary. 2). Further validations In Table.1, the authors provided a methodological validation of proposed method with other two peer methods. These results demonstrate that the performance of proposed methods is better than other peer methods, given the reported segmentation errors and standard deviation. In addition, it is more interesting to show the validation of time-consuming of proposed SD-LayerNet with other peer methods.	As shown in the weaknesses.	Overall, the paper provides some insightful contributions for semi-supervised retinal layer segmentations. The proposed method provides a possible way for reducing the annotation cost in layer segmentation tasks. The design of the self-supervised tasks in this paper could also inspire future researches in how to incorporate anatomy priors. The main drawbacks lie in that the intuitions behind some design choices are unclear, e.g., the textual factors, and the final algorithm involves too many hypermeters, which may be tricky for tuning. Some mistakes: The notation of the textual factor generation branch seems to be wrong in Figure 1, i.e., conv-t or conv-m?	This paper proposed a novel reconstruction loss that has been proved efficient in the experiments, and a strong experimental validation is provided in this work. The minor concern is that the proposed reconstruction loss requires more parameter tuning. It would be interesting if the authors could explain or set up more experiments to validate the lambda values.	As shown in the weaknesses.	The problem concerned by the paper is meaningful, the proposed method is also technically novel.
443-Paper1826	SeATrans: Learning Segmentation-Assisted diagnosis model via Transformer	This paper presents a transformer-based architecture called SeATrans by utilizing the segmentation information to boost the diagnosis task. The key techniques include: asymmetric multi-scale interaction and SeA-block for the segmentation-diagnosis interaction. The experimental results improve a large margin compared with other SOTA methods.	The author proposes a new transformer model for medical image diagnosis, which is named SeATrans.  SeATrans is a transformer model equipped with multi-scale feature integration architecture that achieves promising results on three different tasks.	The authors proposed a general framework for segmentation-assisted disease diagnosis. Their method consists of two jointly trained networks, one UNet to extract multi-scale segmentation-related features and one ResNet50 that performs the classification without the use of segmentation masks. They have proposed to combine the coarse and fine segmentation features of UNet to the first layers of the Resnet50 and use a transformer-based encoder-decoder block to learn the combined space. They validate their method on three public datasets from three different domains, outperforming the baselines.	The motivation is good by segmentation task to boost the classification task. Especillay with the non-regional information. The experiment results are very good. The method includes the Transformer to deal with segmentation-diagnosis feature interaction.	The author proposes asymmetric multi-scale interaction to correlate each low-level diagnosis feature with multi-scale segmentation features.  The author supplies a variety of experiments to validate the effectiveness of different components in SeATrans. The experiments show that SeATrans can achieve state-of-the-art performance on three publicly available datasets.	Generalizability - Their method doesn't make any prior assumptions about the medical images it is being used on. So it can be used on any kind of images.  Novelty - Although the authors have used the well-known networks of UNet and ResNet, they have joined the multi-scale segmentation features and the high-level features extracted by the first layers of ResNet, in a novel way to account for the non-regional feature dependencies missed by the convolutional layers.  Extensive validation - The authors have validated their performance by comparing to many existing methods that make the disease diagnosis based on the segmentation annotations.	Could you explain more (or give an example) on the non-regional relationship between the segmentation and diagnosis task? Visualization of the relationship between segmentation and diagnsosis tasks may help to understand the reason behind the proposed method for performance improvement. Missing comparison with some papers which designed for the down-stream task learning, like MoCo, SimCLR, etc.	The state-of-the-art methods compared in Table 2 lack the reference citation.  The author failed to analyze the efficiency of proposed methods with recent works.	Not well-written - The authors repeat their exact words many times. An example is the last two paragraphs of the introduction that are written very much similarly.  Or the methodology starts with what the authors propose. This is stated in the introduction and now they should start explaining their methods.	No code is provided now. Some training strategies have been described in the Experiemnetal part but not support for the reproducibility of the paper. Since this work is mainly based on the network design.  The code release would benefit other researchers.	The three datasets are available. The code is not available.	The authors have used public data and they will make their code public, according to their answers in the checklist. So I believe their work is reproducible.	See above.	The author is suggested adding reference citation for those state-of-the-art methods compared in Table 2 and supplying some analysis of the efficiency difference between proposed methods and recent transformer models. Meanwhile, If the authors can provide some visual comparisons of the results from proposed method and recent models, the quality of this paper can be further improved	"I believe the authors should provide references for those ""commonly used segmentation-assisted diagnosis techniques"" in the second paragraph of ""Experimental Settings"". I believe the authors should add some info about the distribution of data. The sensitivities reported are the lowest among all the metric and I wonder if it is related to the imbalancy in the data. I believe the authors should perform some statistical analysis to establish whether the improvement reported by their method is in fact statistically significant."	The results.	The idea is interesting.	The authors have used an innovative approach to fuse the segmentation masks into the classification network. They have validated their work by comparing to a large number of existing methods. And their method outperforms these baselines in terms of AUC. They have also investigated the effectiveness of adding each part of their proposed method by an ablation study.
444-Paper2257	Segmentation of Whole-brain Tractography: A Deep Learning Algorithm Based on 3D Raw Curve Points	The author proposed a novel 3D deep model to classify brain white fiber tracts. Two channel-spatial attention modules are proposed and added to the backbone network architecture (PointNet) and leverage the model performance in the detection of 10 major fiber bundles.	This work proposed a network and a new data preprocess pipeline to category the white matter tensor into 11 groups including a non-major group. The model included a T-Net and two spatial attention layers. The proposed method was validated using a manually label dataset of 25 subjects.	"This paper proposed point cloud based 1d-CNN deep architecture to conduct whole-brain tractography segmentation. 3D raw curve points are used to represent the curves of fibers. The whole model is trained based on a classification task: the fiber are classified into 10 major fiber bundles and the ""other fibers"" types. The proposed method get a 98.80% average accuracy."	First, a 3D image data with high-quality labels of the major white fiber tracts is introduced in this study, which carries extensive manual efforts. This large data containing 25 individuals might benefit future studies in the related area. Second, an automatic fiber tract classification pipeline is proposed and its technic novelty sounds solid.	As the author claimed in the introduction, the main contribution of this work is the model that can take the raw 3d curve data. It may provide an end-to-end structure and more raw information to the model. This is also partly confirmed by the results section, table 1. However, since the dataset are different, it is hard to evaluate directly.	Deep models cannot directly consume 3D curves in their raw data format, therefore additional preprocessing steps are needed, which adds  the complexity of the segmentation task. This work uses 3D raw curve points to represent the curves of fibers and can solve the above-mentioned problem.	"I would doubt the name of the ""segmentation model"" in the framework. It is actually a classification model which categorizes a set of point curves of brain white fibers into 10 major bundles. The proposed framework requires identified fiber curves as network inputs, whose pathways are already decided and aligned in the process of whole-brain tractography. Hence, the novelty of this work is downgraded due to this setting. Back to the classification task in the experiments, the comparison with baseline approaches seems unfair to me. Because the testing data are not aligned, e.g. different sizes of subjects, various quality, and even different input data formats (the most critical factor), the classification difficulty of baseline approaches are not at the same level and shall not be compared. In the ablation study, I suggest the author report the averaged performance under cross-validation. We are unable to draw statistical confidence about the reliability and effectiveness of the proposed modules based on the given numbers."	It may be not the weakness, but something needs to be clarified. what is the input of the model. The model took 256x3. I suppose that is a curve with 256 points and each point with 3d coordinates. The major fiber bundle can be selected 256 points randomly.  However, did it also work on the non-major bundles, which may have less than 256 points? In the curve sampling section, it is not clear what is the mid points and how they were selected. Is that possible to use interpolation instead of adding points randomly to achieve the same data size? Ablation studies may be required to confirm the performance of the proposed model. Were data from different subjects mixed? Was there any difference among subjects?	"One of the challenges of fiber segmentation is the anatomical individual variability. An effectively method should have the capability to handle the individual variances and provide robust results. In this paper, the method was evaluated using 10 subjects. However, the results part focuses on the accuracy value but the individual variability is overlooked. As the accuracy values in table1 is the reported values in each work obtained using different datasets, therefore this results cannot effectively compare different methods. As a major part, the motivation of Spatial attention module is not clear enough. From table2, we can see the performance of Baseline Network + Spatial attention module is worse than single Baseline Network. Some sentences are not correct, such as ""...processes using FSL [17] was used"", and ""...which parts are the informative in ..."""	Codes and training process are revealed in the supplementary materials. The reproducibility sounds plausible.	good. It is not clear whether the author will distribute the manual labels.	The author provides code in the supplementary file.	Please find my questions and concerns in the weaknesses section.	According to the above major questions: Perhaps the authors can provide more details of the data sampling. especially the non-major bundle and give examples of the long vs short fibers, the single vs branched fibers. It would be great to compared with other methods using the same dataset. Ablation studies can be performed for example by removing the attention layers, or using interpolation other than randomly selected points.	Q5	Though there is a data contribution, the technic contribution of this study is very limited. It is basically a fiber classification model rather than a segmentation approach and the experiment settings (e.g. preprocessing and input format) lead to a relatively simple task hence bringing few impact/benefit to the area.	It is an interesting work of classification with location and was performed on the raw data, although blind classification may be more interesting. But the performance was not fully validated and some details need to be provided.	I have read the whole paper carefully and understand all the parts. Based on the strengths and weakness in Q4 and Q5, I give the overall score for this paper.
445-Paper1676	Self-Ensembling Vision Transformer (SEViT) for Robust Medical Image Classification	This paper works on improving the robustness of a ViT to adversarial attacks. The authors propose a self-ensembling technique to learn multiple classifiers based on the intermediate feature representations. Experiments are conducted on the Chest X-ray dataset.	The authors propose a self-ensembling transformer for adversarial robust medical image classification. The proposed SEViT is validated on two public datasets.	This paper targets a good topic on the robustness of transformer-based models. By studying adversarial attacks on the transformer model, the paper evaluates the effect when the perturbation exists in the Transformer model. In general, this paper is well written and has a potential impact on natural vision problems but limited innovation and interest in the medical imaging community.	Strength: 1.This paper is well organized and easy to follow. 2.The idea of using ensembling learning of intermediate features and novel and seems work well. 3.The work allows us to know more about the combination of adversarial learning and transformer	The paper is well-written and easy to understand. Extensive experiment results show the superiority of the proposed model. Different modalities are used for the experiments.	original evaluation of the robustness of Transformer models.	Weakness: 1.Can the authors specify what is the motivation or application of adversarial learning in medical area? I can not think of an example where the perturbation happens and the adversarial learning would help with. 2.The conducted experiments in Table 1 is unfair. During the training of ViT, different level of adversarial attacks should be added during the training iteration to improve the ViT's robustness to perturbation, like what is did in the PGD serial paper. 3.Other adversarial learning techniques should be compared, for example the YOPO (https://proceedings.neurips.cc/paper/2019/hash/812b4ba287f5ee0bc9d43bbf5bbe87fb-Abstract.html), FOSC (https://arxiv.org/abs/2112.08304) and so on.	The motivations are not clear. The proposed method seems to lack novelty. Experiment needs further comparisons with state-of-the-arts.	The introduction describes the good background of adversarial attacks and the robustness of the transformer, however, the adversarial attack is more introduced in the natural images processing. The motivation for conducting the same analysis in the medical image analysis is not well discussed. The paper proposes a simple but effective method of handling adversarial attacks. The self-ensemble approach is clean and easy to follow. Is the method adaptive to natural images such as benchmarking on ImageNet? It would be better to discuss this as the proposed method is designed for generic transformer models not for medical image analysis. According to Table 1, the proposed method is effective when an adversarial attack exists. However, when comes to robustness as the paper claimed, the performance decreases a lot. Will the community accept the conclusion that a large decrease is observed as adversarial attacks are not commonly considered in the deployment?	can be reproduced	The paper can be reproduced.	As listed reproducibility checklist, the work should be in good reproducibility.	My main concern is the the comparison between ViT and SEViT is not fair. If the authors can supply a fair comparison, that would be helpful.	Please see the weaknesses. Other detailed comments are as follows: In Section 1, the authors state [11], [20], [22], etc, that enhances the robustness of ViTs. However, none of these approaches are compared in the experiments. It seems the idea is from [22]. More explanations about the improvements would be helpful. The contribution 3 can be removed since it belongs to experimental results. Many parameters do not be well defined, such as threshold for KL-matrix. Some visualization results are suggested to be added to support the results. The reasons for the noticeable performance drop in MLP number = 12 for X-ray in Fig. 3 (a) should be explained. It is recommended to compare the proposed model with other state-of-the-art methods.	The paper demonstrates a good aspect of the evaluation of Transformer model. To further improve the impact on the MICCAI community, the authors can discuss the motivation, challenges, and potential application of how the adversarial attacks are influencing medical image analysis. Detailed constructive comments are listed along with the above.	performance is good and motivation is novel	The main factors are insufficient experiments and the lack of novelty.	Well-written context and new aspect of the evaluation study.
446-Paper1504	Self-learning and One-shot Learning based Single-slice Annotation for 3D Medical Image Segmentation	The paper proposes a new method for 3D image segmentation that requires one slice annotations, addressing the problem of training data availability.	"The authors propose to learn image features that allow both (1) a ""representative"" slice to extracted from a 3D volume and (2) propagate manual annotations from it to the rest of the volume. First, to find the single slice to label, the method clusters the slices (K-means clustering), and finds the most representative slice from each cluster as that with the maximum summed cosine similarity between its learned features and the other slices in the cluster. Second, to propagate labels from slice to slice, the method weights the contribution of each pixel in the already labeled slice by the similarity of its features to the pixel in the unlabeled slice. The features are used for both steps, and are learned by the ability to reconstruct one slice from another when the weighting is again based on similarity in the feature space."	The authors present an approach to automatically select the best 2D slice in a 3D image that needs to be manually annotated. This annotation is propagated to other slices. This yields very good results with few effort.	The method is very interesting, addressing an important problem in medical image segmentation. The work is well motivated, and the empirical results are convincing. A good number of baseline / alternative methods have been used for comparison and an ablation study provides insights about the importance of individual components. Four datasets/applications have been used for evaluation.	Very useful strategy to allow users to annotate a single slice - this is a very user-friendly interaction strategy. Good comparison to lots of alternative methods for dealing with small annotations and good improvement as measured by Dice score, ASSD, HD, etc. Investigation into domain shift from one dataset to another.	clearly written good visualisation appropriate references convincing results good ablation study methodology well explained comparison to relevant work usage of good data sets	The paper could be a little bit more clear at times, where terms such as one-shot are used in a possibly unusual way without exactly defining what is meant here.	Novelty - in my opinion, most of the proposed method is certainly a sensible solution to the problem, but not particularly novel or thought provoking. Binary segmentation only - this paper appears largely limited to binary segmentations, as the single segmented slice must contain all labels in order for them to be propagated and this is unlikely in many multiclass segmentation problems. In particular, this significant limitation is never discussed. Depends on the interaction between the slice orientations and the anatomy's shape - the proposed method propagates segmentations from slice to slice. In particular, it learns pixelwise features that are used for each pixel in the unsegmented slice to find the most similar pixels in the previously segmented slice. Such methods can fail if the shape of the object's cross-section changes rapidly from slice to slice. In addition, the authors also restrict the matching to pixels within a 13x13 window. For objects that rapidly shrink or expand from slice to slice, it's possible that there aren't any pixel with the correct label in this small window size. Evaluation on liver/spleen only - Given the constraints above, I would need more convincing that these segmentation tasks are sufficiently non-trivial with respect to both image appearance and object shape to truly demonstrate the utility of the proposed method.	The values of the weights is not discussed Relying on one slide with ground truth may indeed influence the results. The scheduled sampling should solve this, but some examples for this would have been nice (supplementary material is missing)	Not assessed.	Satisfactory - even though these are checked off on the reproducibility response, there are actually no details of how baseline methods were implemented and used, no variations reported (error bars or standard deviation), and no statistical significance analyses.	seems to be ok	It would be good to clearly define terms such as one-shot, as this is not very clear and the use of the term seems slightly different from other works. I would suggest to replace 'inference' with 'test-time' throughout the paper, as inference is something slightly different than what the authors meant here. The use of the term 'inference sets' is misleading, and should be changed to 'testing sets' or 'evaluation sets'. The introduction may need a reference or sentence to explain what 'human-machine disharmony' means. Unclear what the authors mean by 'enormous semantics' in Method section. From the definition, it is unclear whether the authors assume all volumes to have the same number of slices D. I would suggest to rename 'Featuring Module' to 'Feature Extraction Module', and 'Reconstructing Module' to 'Reconstruction Module'.	"Alternative methods in Table 1 - do the results for experiments (3) to (5) come from the papers [40, 16, 37], or did the authors reimplement or run those methods themselves? In particular, I'm a bit wary of the results for the pseudo annotation results for the CT liver dataset, as this doesn't seem to be an extremely hard problem and the reported results (Dice 0.63) seems very low. I would have expected this method to do better. 3D vs 2D U-Net in Table 1 - Both of these use a single annotated slice, and it's unexpected to me that the 3D U-Net does so much worse, given that it could provide additional contextual information. Is there an explanation, e.g., does it overfit more than the 2D U-Net? CHAOS vs LiTS liver CT scans - I would think that they would be quite similar given that they are both CTs, but the experiments in Table 2 in which the training and testing datasets are different indicate that it is difficult to transfer from one to the other. Is the difference in field of view, contrast enhancement, etc? Additional small comments: Abstract - ""our new framework achieves better performance with less than 1% annotated data"" - to me this phrasing implies that the proposed framework with less than 1% annotated data outperforms fully annotated 3D U-Net training, which is not what the authors are actually trying to say. Consider rephrasing to something like, ""when less than 1% annotated data is available, our new framework achieves better performance than several baselines"". Related work - This method is quite similar to conventional patch-based segmentation along with a learned distance function - the authors could consider adding this to their related work. Introduction - I didn't understand the motivation to avoid human-machine iterations, as I'd think this would be fine as long as any computations were fast enough. Consider editing the self-learning loss in equation L_sche to incorporate the representative slice pairs from the screening module training stage (eqns. 2-4). As written, the loss reads as though the self learning operates on all pairs of neighboring slices and that the representative slice-pairs are unused."	given the limited amount of space, the paper is self-containing and clear enough. Some more evidence, as written above, would have been nice.	This is a good paper with an interesting method and thorough evaluation.	The main factor for my overall score is the lack of novelty plus the lack of explanation convincing me that the baseline methods were implemented correctly and still perform so badly. But, the method is still addressing a good problem and has nice results, so could still fit in at MICCAI.	no missing parts, well written, clear results, best paper in my stack.
447-Paper1341	SelfMix: A Self-adaptive Data Augmentation Method for Lesion Segmentation	SelfMix tried to solve the challenges that the generated tumor images are facing the problem of distortion, by adaptively adjusting the fusing weights of each lesion voxels based on the geometry and size information from the tumor itself	The authors proposed 'SelfMix' for image segmentation which extends existing 'CutMix' and 'CarveMix'. It is tumor-aware and considers background information.  It allows more realistic images for training the segmentation model and improves the  results overs baselines.	An effective lesion generation method is urgently needed to boost the performance of lesion segmentation. This paper proposes a novel data augmentation framework through better utilizing the lesion and non-tumor region information.	1)It is the first one that notices non-tumor information among data augmentation methods for lesion segmentation. It may improve the accuracy of lesion segmentation when the training data set is small. 2) Good evaluation. The author compared this method to trandiational methods and recent related papers.	The proposed 'SelfMix' extends existing 'CutMix' and 'CarveMix', which is tumor-aware and consider background information. More realistic images for training. Improved numbers on baselines.	Motivation is strong. Method is easy to understand. Experiments on two public lesion segmentation datasets show that the designed method improves the lesion segmentation accuracy compared with other data augmentation.	1) The authors did not explain why this method is distortion free. This method computed voxelwise weights with distance map and generate new images by combining two images with the weights.  As organs in different images may have different distribution, distortion may still happen. 2)  Symbols were not used in a good way in equations.  In Eq.1 and 3, the same symbol appeared in the both sides of the equations	Clarification on the selection of non-tumor regions.  For liver lesion, I guess the non-tumor regions should only be liver tissue. Otherwise, the synthetic image would not realistic. However, I didn't see a clarification on this. Demonstration of sample synthetic images.  Unfortunately, there is no more synthetic images to give the reader more ideas how good the proposed SelfMix is compared to cutMix and CarveMix. Fig 1 is not clear enough. Maybe find another case for demonstration. Results  It would improve the quality of this work if the SelfMix is done in 3D and training the segmentation models in 3D.  How cutMix is implemented in Table 2? Are the tumor allocated on random positions? Presentaions.  Fig. 1 is nice but it can be improved. I could not see the difference between CutMix and CarveMix.  Too many typos.	"The abstract pays several sentences to describe their motivation, which is of course vital for a paper. However, the talk about method is short, lack of details. And they present their advances similar with the sentences in contribution part(end of introduction). Maybe description part should be reconsidered to be brief and bullet point about method itself should be given. Some descriptions are not true. i.e. ""Compared with normal organizes, the lesion has a very small amount and meanwhile only occupies a small region in the whole image, which leads to less information can be proved to CNNs"" Actually, some head-neck organs are very small while lesions like glioma maybe very large. I understand why the authors talk like that, but accurate description is important in a research paper. From Fig 2, it seems that when fusing tumor information with non-tumor region, there is a mix both inside or outside tumor lesion. However, the mixed tumor is still labeled with the fused tumor contour. How to keep the label accurate? Section 2.2 ""Relationship with Mixup, CutMix and CarveMix"" is not method part. Normally it should be talked about this in discussion. Some typos. i.e., in Fig 3, TAD should be TDA, is that right? There is a constrain in augmentation of medical images, how to make sure the augmentation is clinical available. i.e. how do you make sure to locate a simulated tumor in probable position? Do you manually choose non tumor region? The performance of Vnet without data augmentation should be presented."	"Good.  More details of the ""random selected non-tumor regions"" should be given."	details look good to me.	The description is clear. Reproducible.	"1) Better writing is needed, especially for the equations. 2) More details are needed for the ""fusion"" step.  3) It should be explained more clearly why it is distortion free."	Clarification on the selection of non-tumor regions.  For liver lesion and kidney, I guess the non-tumor regions should be the liver or kidney tissues. Otherwise, if the synthetic images use completely background pathes, they would not realistic. However, I didn't see a clarification on the selection of non-tumor regions. Demonstration of sample synthetic images.  Unfortunately, there is no more synthetic images to give the reader more ideas how good the proposed SelfMix is compared to cutMix and CarveMix. Good and bad samples are all helpful. Results  It would improve the quality of this work if the SelfMix is done in 3D and training the segmentation models in 3D.  In Table 3, the number of using 100% data for UAD drops compared to the one achieved by 75%, why?  How cutMix is implemented in Table 2? Are the tumor allocated on random positions? Presentaions.  Fig. 1 is nice but it can be improved. I could not see the difference between CutMix and CarveMix. Image content is a bit different. Please crop the image carefully. Too many typos.  x. in Sec. 2.1, Fig 2 -> Fig. 2; multi-steps -> multiple steps ; date -> data	see the  main weaknesses part	It should be explained more clearly why it is distortion free.	Nice technical contirbution on an effective method for data augmentation in image segmentation.  But the presentation, clarification and results can be greatly improved.	Method
448-Paper2822	Self-Rating Curriculum Learning for Localization and Segmentation of Tuberculosis on Chest Radiograph	This paper presents a self-rating curriculum learning (SRCL) method for localization and segmentation of Tuberculosis on chest X-ray images. Experiments were conducted to compare the performance of the proposed method with that of the teacher model, Resnet50-FPN with Mask R-CNN, and the experimental results show the proposed method outperforms the teacher model.	Proposed an automatic method to rank image difficulty in order to perform curriculum learning by gradually adding more difficult images into the training set;	The study proposes a model training approach called self-rating curriculum learning. The idea of curriculum learning is starting the training process with relatively easier to predict dataset and gradually increase the difficulty level of the data. According to authors, one challenge for this approach is building difficulty measurer, which includes human expertise prior knowledge and effort. The study proposes a self-rating approach for difficulty measurer part, which does not require human participation. A teacher model is trained to classify the data into categories. The data is then gradually used to train to localize and segment the TB affected areas on CXRs. The authors also curated a large number of TB patient data from multi-center hospitals to develop and test the model. It is not clear that authors will share the dataset.	The paper proposed an effective ranking function using self-ranking scores instead of using prior knowledge of human experts. The experimental results show the SRCL has improved mAP although AUC has not been improved.	The proposed difficulty ranking algorithm was fully automatic, therefore making it more efficient to perform curriculum learning without human intervention.	The idea is simple and applicable to all types of medical images and most of the medical image analysis tasks.	The proposed ranking function has some novel aspect, however, the idea of SRCL is not new. The self-ranking curriculum learning or self paced curriculum learning or automated curriculum learning has been proposed and reported in papers in the past.	1) The problem being solved (TB segmentation/classification) was not very challenging given very high and almost identical AUCs using the proposed method and the teacher network (see Table 2); 2) There was not a lot of novelty in the deep learning network. Mask-RCNN with ResNet was used.	"The paper could have been written in a better way with better formulations, more organized way of parameter listing, and comparison outcomes. I think the method is not compared to ""without SRLC approach"". What would be the results of the Mask-RCNN+Resnet50 backbone architecture trained with the same training CXR dataset without SRLC approach, and tested on the same test set. Then, we would have a better understanding how much SRLC have contributed to the learning process. Is these results somewhere in the text (paper needs a better organization - especially experimental section - while providing the outcome of the test results)."	The dataset download link is not provided. The training code and model are not provided.	It should be reproducible.	Although the authors curated a large size TB-CXR dataset, my understanding is that they are not sharing the data with the manuscript. The authors also did not share codes. The experimental parameters are provided in the text. The idea is applicable to most medical image analysis problems.	"This paper presented an interesting research topic and proposed an effective ranking function for scoring images. The experimental design and evaluation of the data are satisfactory and the conclusions are justified. The manuscript is written in clear and concise English. However, the paper can be further improved by (1) referencing the latest state-of-the-art curriculum learning papers published in the last three years and compare your proposed method with the state-of-the-art methods; (2) using multiple datasets to validate your proposed method; (3) using more evaluation metrics such as sensitivity and specificity; and (4) rectifying some typos such as ""6000 case"" should be ""6,000 cases"" (Page 3), ""9600 samples"" should be ""9,600 samples"" (Page 5), ""achieve 4943"" should be ""achieve ""4,943"" (Page 7)."	1) In the paper, AP50/mAP50 was mentioned several times but I couldn't find their definitions. Please add one or two sentences of definition in the text when it was first mentioned. 2) Page 4 design of the ranking function: this was probably the most important part of the paper. However, there was no justification on why it was designed this way. Please at least provide high-level intuitive justification as to why this would be the optimal design. 3) In Table 2, the results using SRCL was only slightly better than straight-forward training in the AP metrics but not in AUCs, which made me wonder how much the proposed SRCL really helped improving the results. One additional experiment that could be helpful would be to compare the SRCL in the paper to a different self-ranking algorithm (for example a very naive algorithm that only looked at classifier output probability and its difference to ground truth labels). If it could be shown that the proposed self-ranking algorithm was superior to a naive self-ranking algorithm, I think the results would be a little stronger.	"The authors mentioned the TB studies in the literature, and one is the lack of radiologists' annotation. The mentioned study - Chexpert - is indeed one of the largest CXR datasets, and annotations are automatically extracted from radiology reports using an NLP approach - which contains mistakes. Then, a group of radiologists at RSNA went over some portion of the dataset for manual checking and used this subset as ground truth. But, as far as I remember, this large CXR dataset does not contain TB patient data. I would suggest authors go over the manuscript to provide a better-organized manuscript. I think the method is not compared to the ""without SRLC approach"". What would be the results of the Mask-RCNN+Resnet50 backbone architecture trained with the same training CXR dataset without the SRLC approach and tested on the same test set? Then, we would have a better understanding of how much SRLC has contributed to the learning process."	The proposed ranking function design may be of interests of broad audiences, and may inspire other researchers in designing their ranking functions when using curriculum learning methods.	Even though the proposed self-rating of image difficulty for curriculum learning seemed useful and somewhat novel, I wasn't convinced that it couldn't be replaced by a straight-forward method such as comparing how much ground truth differed from classifier output probability.	Nice and simple solution to increase the model training performance, which can be applicable most of the medical image analysis problems. However, the proposed approach did not provide a comparison with the base solution.
449-Paper0442	Self-supervised 3D anatomy segmentation using self-distilled masked image transformer (SMIT)	This paper introduced knowledge distillation into masked autoencoders (MAE), wherein the teacher network takes all patches of a 3D volume, and the student network takes the visible patches only. Apart from the reconstruction loss in the original MAE, the authors proposed to distill [CLS] token and patch token from teacher to student networks, ensuring both global (volume-level) and local (voxel-level) constraints, respectively. The Vision Transformer (ViT) was pre-trained on 3,643 CT scans from a variety of body regions. The efficacy of the pre-trained ViT was evaluated on two datasets, but the description and results of the MRI upper abdominal organs segmentation were unclear in the paper. The results on the BTCV dataset showed the proposed pre-training approach outperformed existing self-supervised methods developed for CNNs and Transformers.	This paper presents a new self-supervised learning method, SMIT, for 3D multi-organ segmentation. Specifically, they use ViT, masked image modeling (MIM), to learn dense patch feature and use MT self-distillation to train the model. Extensive experiments demonstrate the effectiveness of the proposed method.	This paper proposed a self-supervised learning method for 3D multi-organ segmentation. They presented a self-distilled masked image transformer to pre-train the segmentation network and employed the well-trained model to initialize the model for better segmentation performance. The method has been validated on two public datasets with nice experiment results.	Open dataset: Most results were obtained from public BTCV dataset. Recent top solutions on BTCV benchmark were reported and compared. Clear illustration: The description and illustration of the proposed knowledge distillation approach (both local and global) are clear and easy to implement. Sufficient comparison: The proposed method is compared with several up-to-date self-supervised methods under both CNNs and Transformer backbones.	The method is novel. It combines MIM and MT to train a ViT. Experiments and ablation study are thorough.	It is interesting to exploit powerful self-supervised learning techniques to improve the performance of target downstream tasks. The improvements over other SSL methods on two datasets look nice.	The relationship between [CLS] token and global image embedding is unclear. The conclusion of 1-layer vs. multi-layer decoder needs to be clarified.	Missing one ablation study: What is the performance of using MIM only?  This would help us to understand what role MIM and self-ditillation play respectively in the pre-training process.	As a matter of fact, I am more curious about the performance of applying the CT pre-trained model on MR segmentation tasks. Please also provide the segmentation performance of all methods trained from scratch, and the results of SMIT using the proposed SSL method (basically the MR version of Table 1). When finetuning SWIT for target downstream tasks, what are the computational time and memory consumption used to achieve a satisfying segmentation performance? How is the situation compared to other SSL methods? When finetuning SWIT on the MR segmentation task, will it require more samples and more training time to obtain good results in comparison with finetuning it on the CT segmentation task?	It is easy to implement the idea based on the method description.	Authors provide enough information on method details and experimental settings.	It would be easier to reproduce this work, if the code could be released.	I'm willing to increase the rating if the authors could clarify the following two aspects. What is the reason that [CLS] token in the image reconstruction task carries global information? It makes sense for image classification task (as stated in [24]), but it is not appropriate to directly borrow this assumption to image reconstruction task (pixel-wise task) without justification. What is role of [CLR] token in an image reconstruction task? Why does global image embedding matter for an image reconstruction task? It remains unclear the conclusion of Fig. 5. 1-layer seems to produce better reconstructed images than multi-layer decoder, but does a lower MSE loss mean better representation? Results for fine-tuning 1-layer vs. multi-layer decoders should be presented along with the reconstruction quality. Here are some suggestions for improving the paper: The authors assembled several CT datasets for pre-training by image reconstruction. One possible issue is the image difference across these CT datasets, such as contrast enhancement, because restoring pixel intensity can be deeply influenced by imaging protocols. This domain gap might make image reconstruction more challenging to accomplish. Please comment on this. For target tasks, have the authors applied the same pre-processing to Dataset I (CT scans) as the one used in pre-training? What about pre-processing for Dataset II (MRI scans)? How to address the domain difference (in terms of data) between pre-training and fine-tuning? The pre-processing steps for the target tasks should be included in the paper.	See the major weakness above.	It would be better to provide more descriptions of the proposed method in the caption of Fig.1. I would suggest briefly discussing the finetune difficulties among different downstream tasks (e.g., MR organ segmentation and CT organ segmentation). Please provide more details on the split of the dataset, such as the ratio of train, validation, and test.	Overall, it is a great study that incorporates local and global information into masked autoencoders, yielding performance improvement on two datasets. The authors also benchmarked with several representative self-supervised methods on CNNs and Transformers. However, several aspects need to be clarified, such as (1) the relationship between [CLS] token and global information and (2) the conclusion of 1-layer vs. multi-layer decoders.	The method is well presented and sound. Extensive experiments demonstrate the effectiveness of the proposed method. It would be better if one more ablation study (sse above) is added.	The improvements over other SSL methods on two public datasets are significant.
450-Paper1766	Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion	This paper presents a method for 3D patient body modeling. To this end, the authors propose a framework to localize 2D keypoints with two branches for RGB and depth images and to estimate 3D mesh from the 2D keypoints. The proposed system using RGBD data shows a mean per joint position error (MPJPE) of 115 (mm) for 3D mesh regression, which is 22 mm lower than RDF [1]. [1] Yang, F., Li, R., Georgakis, G., Karanam, S., Chen, T., Ling, H., Wu, Z.: Robust multi-modal 3d patient body modeling. In: International Conference on Medical Image Computing and Computer-Assisted Intervention (2020)	The manuscript presents an automatic approach to estimating the 3D mesh of a patient from a given RGB and depth image by proposing attentive fusion to fuse the RGB and depth image heatmaps to calculate 2D keypoints and heatmaps. The estimated 2D heatmaps and 2D keypoints are further passed to a regressor to estimate the SMPL parameter of the body mesh.	This paper describes a CNN-based approach for 3D patient modelling from RGB-D acquisitions under challenging conditions, i.e. different clinical scenarios. The main contribution is a more efficient supervision of the CNN, achieved by splitting the 3D model generation in two steps, which can be supervised individually: First, joint keypoints are detected from RGB, D or RGB-D inputs using an existing 2D keypoint detector and a fusion module, and the authors show how this can be trained using unsupervised pretraining and a relatively small number of labelled training data. For 3D mesh regressing, they also use an existing architecture, and describe an approach for generating synthetic 2D joint and mesh parameter pairs for training the mesh regressor in a self-supervised fashion. They evaluate their approach by comparing against other 2D joint and 3D mesh regression methods, showing good generalizability of their method.	1) There is a novelty in the network combining RGB and depth image features/heatmaps with intra-modal attention and inter-modal attention. 2) The application shown in Section 3.3 is appropriate and demonstrates the usefulness of the proposed framework.	The manuscript presents a medically relevant problem of estimating 3D mesh from the multi-modality depth images. The manuscript is well written and presented. The authors have proposed the attentive fusion approach that accurately fuses multi-modality heatmaps helping in achieving the final 3D mesh reconstruction.	* The training strategy proposed in the paper is very clever. To the best of my knowledge, the split supervision of 2D keypoint detection and 3D model generation, the latter of which can be supervised with synthetic training data only, is a novel concept. Aside from the proposed clinical setting, where it is used to overcome challenges associated with variability in patient positioning for different imaging modalities and coverage of the patient with sheets, the same strategy could also be applied for other, non-medical but non-generic scenarios, in which current state-of-the-art methods perform poorly. * The paper is well motivated and timely. Scanning automation is an increasingly important topic, especially considering the current situation, where clinical staff is typically short, and it can be advantageous to avoid too much close contact between staff and patient. Although many pose and human shape estimation methods exist, generalizing them to challenging clinical conditions seems to be an open problem. * The paper is well written and pleasant to read. I could not spot significant grammatical, spelling errors or typos.	1) The 3D mesh estimation description is difficult to understand, the reproducibility is poor, and there is no particular novelty in that part. 2) The credibility of the results is limited because the cross-validation is not performed on the data with a small number of patients.	A fair comparison with the current state-of-the-art is missing	* I think the experiments part in this paper could be stronger. The entire section is not as well written and structured as the remainder of the paper. Some choices and methods are not entirely clear from the description (see detailed points).  * The clinical significance and impact of the achieved results is not discussed. It remains unclear whether the method satisfies clinical demands, and if not, which limitations remain. * Although the number of required labelled training data can be significantly reduced by the approach, still, a considerable amount of labelled training pairs from clinical, in-bed pose images are needed to train the network. Aside from a public dataset, the authors use a large collection of proprietary data. For most researchers aiming on using or building upon the presented approach, such a dataset will be difficult to obtain.	The RGBD keypoint detection module does not seem challenging to implement, but its parameters are not fully explained. The 3D mesh estimation is challenging to implement.	Datasets and code: The authors used a combination of public and the private dataset. The authors have neither provided nor mention the availability of the private dataset, models, training/evaluation code upon acceptance. Experimental results: No result on the different hyperparameters setting or on the sensitivity of hyperparameters on the results. The authors used fixed hyperparameters.	Reproducibility of the method is overall given. Although the description of methods lacks many details in the main paper, they are provided in the supplementary material. The results in the paper could likely not be reproduced, since the authors use proprietary training and testing data. It would be very valuable for the field if this data would be made publicly available.	It does not seem easy to see that the experiment compared to OpenPose has a significant meaning. It is recommended to compare with more recent pose estimation approaches or to train state-of-the-art approaches with the given data. While training the RGBD keypoint detection framework, the magnitude of the difference between RGB-based and depth-based errors is ignored, which has room for improvement. The training data was composed of only a small number of patients (3 patients), but it would be good if the performance change could be shown according to the amount of training data. Several typos need to be corrected. Page 3 Given a RGB -> Given an RGB Page 5 (i.e., AMASS[25], ... -> missing the close paranthesis Page 6. 3.1(1) inconsistent double quotation marks	"The authors should consider addressing the following points. The proposed methodology of using attentive fusion is novel and appropriately combines the RGB and depth heatmaps for multi-modality 2D human pose estimation. The authors obtained better results than the state-of-the-art RDF model. However, The RDF model uses the Resnet-50 backbone features from various modalities, whereas the authors have used a more accurate and high-capacity HRNet model. So whether the improvement is coming from a better backbone or the proposed attentive fusion is currently unclear. For a fair comparison with the RDF model, authors should consider either using the same backbone model or using their proposed fusion method in the RDF model. The authors have used the ""Depth Keypoint Detection branch"" to estimate the 2D keypoints from the depth image. However, a depth image provides a better 3d representation encoded in the depth values. As the final aim is to estimate the full 3D mesh, using only the spatial 2d heatmaps from the depth might not fully utilize the full potential of the depth image. The authors should consider exploiting more appropriate 3D features from the depth image for the 3D mesh predictions."	Methods * The loss function is shown in Fig. 3, however, it not explained anywhere in the main paper. Maybe it's definition and explanation can be moved to supplementary entirely, as going back and forth between supplementary and main paper is tedious for the reader. Experiments and Results * It is not clear on which dataset the baseline methods were trained. To they use SLP, SLP + proprietary data, or are off-the-shelf models used? None of the baseline methods except RDF use the SLP dataset in their original models, so the comparison in terms of methodology would not be entirely fair for off-the-shelf models.  * Following up on the previous point, general purpose pose estimation methods, such as OpenPose, need to generalize to a much greater variability of poses, compared to only lying, in-bed poses in SLP and the proprietary dataset. Maybe other works focusing on in-bed poses [1,2] could be considered for a comparison. * The work in Ref. [36] in the paper was followed up by the same group with Ref. [15] in the paper. Why was [36] chosen as comparison? Particularly considering that [15] also uses RGB-D modalities and shows very encouraging results.  * It seems like the PVE-T-SC metric is missing from Table 1. * In Table 4, why are only results for the MI dataset presented, what about CT or MRI? Why was the head pose omitted from the evaluation? Conclusion * An interpretation of the results in terms of clinical significance is missing. For someone unfamiliar with the topic, the presented metrics show superior performance of the approach compared to other methods, but their clinical interpretation is not clear. It would be good if authors could comment on the clinical significance of the results. Does the approach already satisfy the demands of a clinical application? Which error ranges would be considered acceptable?  While I like the clinical evaluation in section 3.3, this is especially true for these metrics.  * Following up to the previous point, if the results are not yet satisfactory for a clinical application, limitations and areas for future research should be discussed as well. [1] Yin, Y., Robinson, J. P., & Fu, Y. (2020). Multimodal in-bed pose and shape estimation under the blankets. arXiv preprint arXiv:2012.06735. [2] Clever, H. M., Erickson, Z., Kapusta, A., Turk, G., Liu, K., & Kemp, C. C. (2020). Bodies at rest: 3d human pose and shape estimation from a pressure image using synthetic data. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 6215-6224).	Most of the manuscript except for 3D mesh estimation is clear and easy to follow, and the results are plausible.	The paper is very well written and addresses the challenging problem of 3D mesh estimation from multi-modality images. The only concern in the proposed fusion approach is a fair comparison against the state-of-the-art RDF model. The authors should consider addressing the issues described above.	The paper presents a solid contribution to the field and the results are encouraging. The flaws in the evaluation and description thereof, and the lack of critical discussion of the results, in particular in terms of clinical significance, prevent me from giving the paper a higher score.
451-Paper0450	Self-Supervised Depth Estimation in Laparoscopic Image using 3D Geometric Consistency	This paper, proposes a  self-supervised laparoscopic image depth estimation approach that in addition to left-right consistency, also invokes the inherent geometric structural consistency of real-world objects, as well as optimizing  mutual information between stereo pairs. The authors demonstrate their approach using public and locally- acquired datasets that show the ability of this approach to generalize across different instruments and imaging environments.	The authors propose to use stereo images to train a depth from mono method. They propose to generate depth images from monocular depth estimation from each eye and use a consistency loss to ensure these depth maps are consistent.  The method is validated on a well known benchmark dataset and compared with some previously published mainstream computer vision methods.	This work proposed a framework for laparoscopic image depth estimation. The estimator was trained self-supervised with stereo images, with 3D ICP loss and blinding mask, and achieved good performance on two datasets.	The paper is well written and presents a convincing demonstration that the proposed approach out-performs a number of other techniques, as shown by Fig 2. Although the quantitative results are presented in Table 1,  Fig 2 would be more informative of a) a colour scale was provided, and if the colour showed the deviation from ground truth instead of absolute depth. Some more details of the collection of the LATTE dataset (perhaps provided in supplementary material), would be appreciated, along with some indication of its quality. The 3D Geometric Consistency loss and  Blind masking approaches are quite novel, but it would be helpful to understand their impact more clearly. For example, how does  left & right 3D model consistency contribute to a more accurate disparity map? In experiments, is it possible to  show a comparison with and without 3D consistency loss to demonstrate its effectiveness?	The paper method is well written, clear and easy to follow. The validation is clear and shows a good improvement over some previous methods. The authors are also using a benchmark dataset which makes comparison more interpretable.	Writing is clear and easy to follow. Applying geometric constraints to endoscopic depth estimation is worth studying. This work can inspire future work to further explore this direction. The framework yields significantly better results compared to the listed previous work. A new dataset with ground-truth depth is collected, which will certainly benefit future research.	"There was a very similar paper published in MedIA early this year: Bardozzo et al.  ""A stacked and siamese disparity estimation network for depth reconstruction in modern 3D laparoscopy"" , which is also a self-supervised network minimizing a 2D reconstruction loss using SSIM as the metric. Can the authors refer to this paper and describe how the submitted work differs from it? It is not clear from the text what the primary motivation for this work is. The authors mention the acknowledged problem of providing ""ground truth"" for laparoscopic 3D reconstruction, against which various reconstruction algorithms can be evaluated, and the way the paper is presented, the reader could understand that this was its objective.  However, further reading reveals a structured light based generation of ground truth used to validate their new approach.  Perhaps the intro could be revised to make the objectives clearer - perhaps along with clinically-oriented statement of the ""un-met need'  that is being addressed. The training/testing scheme in this paper is somewhat problematic. ""Hence, only key-frame ground truth depth maps were used from this test dataset while the remainder of the RGB data formed the training set."" If I understood this correctly, the author used n-1 frames for training and 1 frame for testing for each sub-dataset. Would  this not cause bias for overfitting, since all images in the sub-dataset are very similar and have similar depth. Would it not be more appropriate to use several subsets for training and 1 or 2 for testing as was the case for the challenge?"	The main weaknesses of this paper are the utility of this method given a) many laparoscopes are stereo and b) 3D information is not really obtainable from mono depth methods the motivations are not clear whether this is the right community for this paper I elaborate more on this in the constructive comments.	"Major concerns: Novelty. The two claimed methodologic contributions of this paper, 3D geometric consistency loss from ICP and blinding mask, have already been well-studied by previous depth estimation work. The ICP loss for monocular depth estimation was proposed in [13], and the idea of using geometric constraints in endoscopy image depth estimation is not new [a]. The blinding mask is a widely-applied technique/trick; for example, it was applied in [13] and previous medical implementations such as [b]. Although this work re-implemented the approaches in a new application scenario, laparoscopic stereo images, the approaches are not novel. Necessity of 3GC. The proposed framework has three differences from Mono1, i.e., the additional ICP loss (3GC), blinding mask (BM), and decoder structure (FD). The selected results in the ablation study Table 3 only show combinations that are in favor of this paper, but the full ablation in supplementary raises concerns about the efficiency of the main contribution of 3GC. As shown in Supp. Table 1, modifying the decoder (Mono1 w/ FD) can already bring significant improvement comparing baseline Mono1, but further adding ICP loss (Mono1 w/ 3GC, FD) decreases the performance greatly (the second-worst result in the table). This is an important finding, showing the ICP is not as useful as claimed for a decent baseline. The authors should have pointed it out. Minors: The three baselines compared in Table 1&2 are all photometric-based methods. It would be more convincing if some geometric-based baselines were compared, such as general depth estimation baseline [c,d] and medical baseline [a]. An error in Eq.2 [a] Liu, Xingtong, et al. ""Dense depth estimation in monocular endoscopy with self-supervised learning methods."" IEEE transactions on medical imaging 39.5 (2019): 1438-1447. [b] Ma, Ruibin, et al. ""RNNSLAM: Reconstructing the 3D colon to visualize missing regions during a colonoscopy."" Medical image analysis 72 (2021): 102100. [c] Yang, Zhenheng, et al. ""Lego: Learning edge with geometry all at once by watching videos."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2018. [d] Bian, Jiawang, et al. ""Unsupervised scale-consistent depth and ego-motion learning from monocular video."" Advances in neural information processing systems 32 (2019)."	Good	The authors claim they will release data + code so this should be reproducible.	Implementation details were provided. The data collection process was described.	Overall a well written paper that contributes to the field of 3D reconstruction stereo endoscopic images. The paper would benefit by having the points made above addressed concisely	A weakness for me is that this method is quite general and not really specific to computer assisted surgery/MICCAI community. I feel like it could have been submitted to a mainstream computer vision conference and validated on the larger datasets those communities use, can the authors explain why they have submitted this paper to MICCAI? The motivation for depth-from-mono is not well established, many laproscopes now are stereo- the authors should improve this motivation in the introduction. The authors also do not really explain how they propose to use a depth from mono method to solve the type of applications they suggest in the introduction. Without a known object, a mono system cannot predict true depth. The authors should explain how they see depth from mono being used. I have concerns about the accuracy of the proposed method. Although it is clearly better than the previous monocular methods from mainstream computer vision, the error is still very large. Perhaps too high to be useful? Is this simply off by a scale factor since it's using a mono method? Can the authors comment on this? The authors could/should have compared with some previous medical depth from mono papers for a stronger comparison. For example 'Self-supervised Learning for Dense Depth Estimation in Monocular Endoscopy', Liu et al, MICCAI 2018, this paper has code available and would be a stronger comparison. Is this statement actually correct:  'However, all of these methodologies employed left-right consistency and smoothness constraints in 2D, e.g. [3],[5], ignored the important 3D geometric structural consistency from the stereo images.' I see the self-supervised methods as implicitly using the 3D whereas this is more explicit about it. The authors should clarify this point unless they disagree with my assessment.	Please refer to the weakness section.	The paper adds useful information to the standard methods of stereo reconstruction from endoscopes, and I believe makes a valuable contribution to the literature.	I feel this was one of the stronger papers in my stack, it has its limitations but at least proposes something fairly novel and validates on a well known dataset. I would definitely not argue strongly for it to be included, but it was at least interesting and the idea makes sense.	This work focused on the important direction of using geometric consistency to improve laparoscopic image depth estimation. Yet, the scientific novelty of the paper is limited, and the experiment results are not able to prove the merit of one of the major contributions.
452-Paper2634	Self-Supervised Learning of Morphological Representation for 3D EM Segments with Cluster-Instance Correlations	Submission 2634 proposes a novel method to learn a representation of the morphology of 3D objects segmented from EM volumes. The method is self-supervised, validation is performed on a large manually annotated dataset of neurite fragments. Comparison to state-of-the-art shows the advantages of the method.	This paper proposes a new self-supervision method for point cloud representation with the application for neuron subcompartment classification. The proposed method combines existing self-supervised methods for images, BYOL and SwAV, with minor modification. On its own dataset, the proposed method outperforms baseline methods.	The authors present a self-supervised approach for learning 3D morphology representations from ultra-scale EM segments. The proposed method leverage contrastive learning at both instance level and cluster level to learn the representation. Experiments on the over-segmented results FAFB-FFN1 outperforms other contrastive learning methods.	Strengths: As far as I'm aware, the idea of class-instance contrast is novel Excellent validation results, proper comparison to state-of-the-art The paper is well written and well illustrated The method itself is not specific to EM and is likely to be applicable to other problems which require a representation of morphology. A new dataset will be made available to the community Failure cases are demonstrated, limitations are discussed	The technical writing is clear. Contribution of a new dataset. The proposed method outperforms previous baselines.	The paper is well-written and easy to follow.  The first self-supervised method for 3D EM segments. The motivation is clear. The motivation is clear to integrate the feature correlations at the instance level and cluster level.	Weaknesses: Only one (although very big) dataset is used. Will the learned representations transfer to another dataset without retraining? The classification problem of soma/neurite/glia is not that difficult. It would interesting to test the method on the more common axon/dendrite/soma classification problem in mammalian brain data.	Lack of novelty: The proposed method is a direct combination of BYOL and SwAV with minor modification. It'll be good if the paper can directly re-use the terminology from the previous work and put things in context. Otherwise, the paper reads as if it proposes all these new modules. Lack of reference and comparison with state-of-the-art point cloud self-supervision methods [A]. BYOL and SwAV were designed for image input, while [A] is more proper for comparison. Lack of comparison with prior work. The proposed method can be more convincing if compared with prior work [15]. (1) Although the input field-of-view of [15] is much smaller, it can still be trained on the proposed dataset. (2) Also, it'll be great to test out the proposed method on [15] dataset. Unclear how hard the proposed benchmark is. It'll be good to use handcrafted 3D point cloud features as a comparison. For example, the scale of the soma is vastly different from the shown neurites and glias, which makes it a simple task with even handcrafted features. [A] Xie et al. PointContrast: Unsupervised Pre-training for 3D Point Cloud Understanding. ECCV 2020	Missing the quantitative ablation study for the cluster/instance level contrastive learning.  Missing the description of the choice for the hyper-parameters.	New dataset will be made available!	The supplementary material provides enough details for reproduction.	The method part is clear. But it lacks the discussion for the choice of hyper-parameters.	I realize these are impossible to achieve within the rebuttal timeline and would still recommend acceptance. However, if the authors want to improve the next extension of the method, I would recommend the following: As noted above, I think it would be important to evaluate if the learned representations have to be retrained when a different dataset is used for input. Besides the evaluation on a different dataset, preferably a mammalian one with more difficult classes, I think it would be interesting to compare more directly to the methods of [15] and [20].	Writing: The paper can be clearer if it can directly attribute modules to previous works and emphasize on the proposed new pseudo cluster label supervision. In its current form, it's unclear to novice readers which is this paper's contribution. Experiment: The paper only provides ablation study comparisons. It's unclear how it compares with previous neuron subcompartment classification methods or state-of-the-art point cloud contrastive learning methods.	In general, the paper is well written and easy to follow. The motivation is clear. The motivation is clear to integrate the feature correlations at the instance level and cluster level. However, it misses the quantitative ablation study for the cluster/instance level contrastive learning. Although BYOL and SwAV can be seen as the instance-level and cluster-level methods, the training set may be different from the proposed method. It is better to add a comparison of the proposed method w/o L_{CIL}/ L_{IIL}. Others: For the cluster initialization methods. Do KMEANs and DBSCAN have a large difference? Why choose K-means instead of the Sinkhorn-Knopp algorithm? Does the choose of /beta affect the performance? How to set K? How to choose the size of the memory bank?	A new method, full evaluation, very good results.	I really like the topic of this paper, which applies contrastive leaning approach to learn point cloud features for 3D neuron morphology. However, the writing does not make it clear which part is from the previous work and which is new contribution. In addition, the experiment section lacks comparison with prior work on either the same task or the point cloud contrastive learning methods.	The motivation is clear. The motivation is clear to integrate the feature correlations at the instance level and cluster level.
453-Paper2043	Self-Supervised Pre-Training for Nuclei Segmentation	The paper addressess the problem of nuclei segmentation in whole slide images (WSI), and how vision transformers (VT) can be used for this. VT are data-hungry and therefore usually used pre-trained on ImageNet, but this is not so useful for nuclei segmentation. The paper presents a pre-training strategy, which makes VT preform better on WSI.	The author utilized the self-supervised learning for unlabelled data to tackle the need of large quantity of dataset in transformer pre-training for medical application. The paper proposed a unique framework to pre-train by combining triplet loss, scale loss with a learnable background foreground criterion.	This paper introduces a new self-supervised learning method to pre-train Vision Transformers for nuclei segmentation. The proposed method drives the network to predict the patch features from the surrounding neighboring patches, thereby encouraging the network to learn meaningful nuclei features. This method is motivated by an observation that it is more difficult to predict non-background patches than the background ones. After fine-tuning the pretrained weights on the nuclei segmentation network, in practice it is shown that the proposed method achieves better performance than previous works.	The problem is relevant and well motivated. The approach is sound. Results are promising.	The idea of using SSL for transformer pre-training is feasible for a wide range of medical application. The idea of quantifying patch prediction difficulty to distinguish between nuclei and background is novel. The proposed-region triplet learning, combined with scale loss is well designed and showed satisfactory improvement.	Overall the proposed method is novel. The pretext task of predicting the patch features is interesting. The employment of region-level triplet loss is reasonable. Compared to previous approaches, the proposed method achieves better performance on two public datasets. This shows the benefits of the proposed method. This paper is well written, with clear organization and motivation. The method is easy to follow.	In places, the presentation is unclear. Especially section 3.1, which is a pitty, as this is the main contribution of the paper. For example, you define a matrix Hard, elements of which indicate how hard it is to predict a non-boundary patch. This seems to be only used element-wise to define sets of feature vectors (2). Why not define (2) directly from prediction difficulty. Also, the argumentation for calling this sets foreground and background patches is unclear - this is only an assumption, or...? To me it seems that we only can say that sets contain feature vectors of patches that are difficult/easy to predict.  Mathematical notation is difficult to follow due to many variables, and the use of italics (math) font for multi-letter variables. For example $An_{i,j}$ would normally be $A$ multiplied with $n_{i,j}$. The name Hard is also not a good name for a matrix, but if you insist, type it in roman. (On a similar note, for subscripts which are not variables, it should be $L_\mathrm{region}$. And functions like max and softmax should also be typeset in roman.)  In Figure 2, the text is very small. In Figure 3, the images are very small (and in the printed version, blue and yellow arrows provide poor contrast to black and white images).  Conclusion is blant. It's much better to make conclusions which are refutable. For example something that: Our results show that VT may be pre-trained to ...  Tiny thing: in 1 you say k=32, but later it seems you divide into 16x16 patches. What is the explanation?	The comparison seems to lack the setting with TransUNet (segmentation baseline) pre-trained with MoNuSegWSI. The evaluation of the methods did not include an ablation study. Although the performance comparison table does prove that the pre-training on MoNuSeg and the fine-tuning with supervision enhanced the performance. The paper didn't provide a compelling analysis of how much the triplet loss contributes to the improvements, or whether the pre-training on MoNuSegWSI is responsible for the performance boost. The idea could be useful, however, the layout and writing can still be further improved, and some more revisions might be helpful and necessary. Overall, the submitted version looks an incomplete version completed in a hurry, especially the experiments section.	The triplet loss and scale loss are not new. They are from previous works. But I think this is not a big issue. In the paper, it is unclear why the proposed self-supervised training method does not fall into trivial solutions, e.g., the network simply outputs a feature map with constant values. For me, I think this could be due to the scale loss. Some explanations would be necessary in the paper. In the experiment, no ablation studies are provided to show the impact of scale loss and triplet loss. It is unclear if the triplet loss only contributes marginally to the overall performance. In other words, it is possible that the good performance of TransNuSS is mainly due to the scale loss. The proposed method utilizes Vision Transformer, which can be more effective than ResNet in many cases. In the experiment, it is demonstrated that the proposed method outperforms InstSSL, which employs ResNet. Therefore, such a superior performance could be due to the Vision Transformer. To make a fair comparison, it is suggested to compare with InstSSL (and other methods) which also employs Vision Transformer. This paper claims that applying the proposed pre-training technique to a Vision Transformer is a contribution. However, I think this contribution is trivial because Vision Transformer can also be replaced with convolutional networks here. In the experiment, the paper does not compare with other general self-supervised methods which can also be applied to medical image processing, such as MoCo, BYOL, SimCLR, etc. It is suggested to also compare some of these methods to show the advantages of the proposed method. References: [1] Bootstrap Your Own Latent - A New Approach to Self-Supervised Learning. NeurIPS 2020. [2] A Simple Framework for Contrastive Learning of Visual Representations. ICML 2020. [3] Momentum Contrast for Unsupervised Visual Representation Learning. CVPR 2020.	Level of detail is high, so it should be possible to reproduce the work. It seems authors will not provide the code.	The complete architecture is shown in the manuscript, and some implementation details reveal hidden and unclear. The author could consider open sourcing their project, which might be helpful to improve its reproducibility.	The paper provides some implementation details about the proposed method. It is unclear if the paper will provide the official code. So potentially there could be some issues in reproducing the method.	Please address the comments listed under weaknesses of the paper.	The setup of ablation studies in this work is pale and weak. I don't find its current version convincing to justify the contribution of each individual design. A more comprehensive ablative setup is suggested, together with a more insightful ablation discussions. It might be more preferable by the community to denote Vision Transformer as ViT.	(1) The paper could add some explanations for illustrating why the proposed method will not fall into trivial solutions. (2) Additional ablation studies regarding the losses are necessary. (3) To make a fair comparison, it is better to also adopt Vision Transformer in InstSSL and some other methods. (4) It would be better to also compare with other generic self-supervised learning methods, such as MoCo, BYOL, SimCLR, etc. (5) It seems like the font of the paper does not follow the MICCAI paper template. It is suggested to fix this.	The work is relevant, sound and results a promising. Presentation may be improved.	Please see my comments above.	Overall I think the idea of predicting the patch features from the neighboring patches is interesting. This kind of idea has also been adopted by other concurrent papers, but from a different perspective. From the experimental results, it can be observed that the proposed method has some advantages over previous approaches. But it is unclear if such advantages still hold after employing Vision Transformers in the baseline methods. Also, this paper lacks important ablation studies; also it lacks some comparisons with other generic self-supervised learning methods. To summarize, the major factors that led me to the overall score are: novelty and experimental validation.
454-Paper0106	Semi-supervised histological image segmentation via hierarchical consistency enforcement	The authors propose a novel semi-supervised segmentation method applied to cells and glands in regions of histopathology images. The method follows a teacher-student model where the teacher is regularised with stochastically altered intermediate features.The results on two datasets of nuclei and glands segmentation show competitive, and in some cases better, results than state of the art SSL methods.	The paper describes a framework to utilize both labeled and unlabeled data for nuclear and gland segmentation in a hierarchical fashion. The method uses a teacher-student network setup where labeled data is fed to a student network for supervised optimization, while training with unlabeled data is based on consistency between multiple outputs from student network and a single output from the teacher network. Specifically, latent space perturbations are applied at multiple layers of the student decoder with prediction certainty maximized with self-supervised mechanisms and hierarchical losses using the Teacher model outputs. Results on existing public benchmark datasets show competitive results, including ablations on the proposed losses.	In this paper, the authors propose a semi-supervised histological image segmentation method based on the Mean-Teacher model with a hierarchical feature consistency loss. In experiments, the proposed method outperforms previous methods, including a SOTA method using transformation consistency.	Writing quality is good. The experimental setup is solid and design decisions of each stage are well-motivated with the aim of improving the standard semi-supervised mean teacher. The method technicalities are well explained. The method is evaluated on two state-of-the-art datasets for histopathological structures segmentation. The comparison with strong SSL baselines makes the paper relevant and the results promising. Is remarkable that with such a limited amount of data  (1 labeled image region), the proposed method reaches a dice >0.7 for nuclei segmentation.	* This work introduces several novel losses tailored to the hierarchical setting i.e., hierarchical consistency loss and self-supervised enforcement.  * The model is able to increase segmentation performance when with a limited amount of data e.g., 50% labeled data reports results comparable to fully-supervised baselines.  * The human based labeling process of histopathology images is an intensive task that also requires special expertise making it costly and difficult to scale. Therefore, the development of methods that leverage unlabeled data are important for larger scale medical studies and applications. * The framework presented does not depend on a special network architecture making flexible and perhaps easy to adapt in other models.	1) The problem is important and should be interesting to the MICCAI community. 2) The idea and method presented in the paper are intuitive and easy to follow, and the organization is clear. 3) Experiments are conducted on two datasets, and comparisons include several previous semi-supervised methods. Compared with semi-supervised methods, the proposed HCE generally shows better performance; It is encouraging to see that HCE achieves comparable or better performance even when compared with the full-supervised method, indicating the great potential of semi-supervised segmentation methods.	"The paper novelty is to support the idea that applying perturbations in the feature space is preferred than doing them in the input or in the output space. Nevertheless, the authors do not compare or explain properly how it improves against such SSL methods, which weakens the paper. Details and discussion on pixel-segmentation results are lacking. No statistical analysis of the results, or average performance over several repetitions are reported, which makes difficult to evaluate the robustness of the methods. The qualitative results show that the method struggles to separate the boundary of touching structures, would have been interesting to discuss deeper and propose further enhancements for the method to cope with this. State-of-the-art methods such as [1] make this the central innovation in their methods, I think HCE could be further improved if you take this type of domain knowledge into account. The qualitative results show that the method struggles to separate the boundary of touching structures, would have been interesting to discuss deeper and propose further enhancements for the method to cope with this. [1]: He, Hongliang, et al. ""CDNet: Centripetal Direction Network for Nuclear Instance Segmentation."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021."	* The paper is based on the idea that decoders at different layers of the network can reconstruct the exact same output as the last decoder. However, there is no clear proof that this consistency enforcement does not result on identity functions for the last layers of the encoder leading to a network that under-fits the already small training data. * The current reported results employ 3 hierarchical layers in the student model. It is unclear if the improvement stems solely from the use of multiple layers. Including ablations/evaluations for the number of layers may further support the need for hierarchical learning.	1) Even though I agree with the authors that the proposed HCE is simple, the novelty is limited. It's more of a marginal improvement over the current perturbation-based Mean-Teacher method such as [8]. More importantly, I think it is necessary to introduce more details about the perturbations, including both types and degrees, as I believe the essence of the method is to learn perturbation-invariant features for a more efficient learning process. It would be interesting to see more discussions on how would different feature-level perturbations affect the HCE performances. Could authors also comment on why transformations as in [6] or prediction-level perturbations as in [8] are not used in the paper? If possible, an ablation study on perturbations would be constructive. 2) Authros could consider including more recent related works, in the discussion and/or comparisons, such as Extreme Consistency: Overcoming Annotation Scarcity and Domain Shifts by Fotedar et al. and SimCVD: Simple Contrastive Voxel-Wise Representation Distillation for Semi-Supervised Medical Image Segmentation by You et al. 3) In Fig. 2, it would be helpful to better locate the visual differences if authors use the same or similar colormaps for different methods.	Methods: Highly reproducible as all the technical details are well-explained.  Datasets: Both datasets are open-access which	The presented ideas are clear and can be reproduced.	Authors claimed that the code will be released, and results shown in the paper seem reproducible.	"The datasets used in the experiments are relatively small regions of whole slide images. Provide the computation time (inference and training) and the average computing time for the instances (glands, cells). Providing the computation times can help evaluate the feasibility to apply this method directly on whole slide images in the routine work of pathology laboratories and researchers in computational pathology. Please elaborate why the learnable HCL of equation (4) is ""capable of providing a more reliable prediction as guidance for the student model"". Did you test this experimentally? Such statements should be accompanied with evidence to some degree at least. Is this the first row of the ablation results table? If so, please refer to it properly in the sentence. The writing can be improved in some parts: In section 2, the last paragraph that introduces the notation, should be the first, I think. Figure 1. Is quite cluttered and difficult to understand at a first glance, making it simpler probably would draw the attention of more readers."	Part of the optimization process is based on the measurement of uncertainty from the teacher's network output, but the explanation is limited and there are no additional experiments that validate its importance. It will be advisable to give more information to clarify and validate the design choice. In addition, I am concerned that the model is sensitive to the number of layers employed for HCE and learnable HC losses.	Please refer to the weaknesses above.	Few incremental novelties that fit together well. Strong baselines. State-of-the-art open access datasets. Relevant problem for computational pathology. Well written manuscript. The only drawback of the paper is that do not discuss the boundary separation problem, which is important in this segmentation task.	The paper presents a simple framework to exploit unlabeled data alongside limited labeled with HCE modules and novel learnable HC losses. I appreciate the extensive experiments and ablations with recent works, to both validate the proposed losses and their contributions in learning. According to results the model has the most improvement when the amount of data is very small, but it is not proved that the model has performance gains due to the proposed HCE module or it is also due to under-fitting.	Although the novelty is limited, the results are encouraging. I may raise my rating if authors could resolve my concerns regarding the perturbations.
455-Paper0597	Semi-supervised Learning for Nerve Segmentation in Corneal Confocal Microscope Photography	The manuscript presents a semi-supervised learning framework for corneal nerve fibers segmentation in confocal images.  The method is well described and characterized by many steps. Some of these steps are innovative and the overall method is a novelty. Results are well evaluated and show an improvement with respect to other methods for corneal nerve segmentation.	The authors proposed a semi-supervised segmentation framework that could work with only a few labeled data samples. They used masked image modeling (MIM) to pretrain a U-Net with unlabeled data and then fine-tune the model with labeled data and combined unlabeled data with pseudo-labels and actual labeled data to retrain the model. The performance is good and convincing.	The submission propose a semi-supvervised learninig framework for nerve segmentation in CCM, which contains three main components: pretraining on large unlabeled datasets and refining on small labelled datasets, generating pseudo labels, and retraining with the full datasets. The stage of pretraining utilize a masked image modeling based GAN structure, including coarse repring network, refine reparing network, and disciminator, among which the encoder of refine reparing network is used in the whole semi-supervised framework.	The paper is well written, methods and results are clear. Some parts of the proposed method are a novelty, and consequently the overall method is a novelty applied to corneal nerves images. The proposed method could be applied to other types of images.	This is a novel application for segmentation to Corneal Confocal Microscope images. The framework they proposed is solid and strong to deal with the small labelled data size problem and they tested the model both in public and private datasets to show it effectiveness.  To overcome the loss of pixel details, they used Refine Reparing Network to help refine the learning results of Coarse Repring Network which is excellent.	(1) The organization of the whole manuscript is quite good with clear illustrations of the implementing pipeline. (2) The whole framework is reasonable to deal with the annotation problem in microscope images.	Many papers in literature describe methods for corneal nerve segmentation from confocal images. The proposed method achieves a limited improvement in the segmentation performance, and the clinical relevance of this small improvement is not demonstrated.	The whole framework is solid but widely used in self-training, unsupervised training computer vision papers, so the novelty is not strong for a methodology paper for example, Kaiming He etc, Masked Autoencoders Are Scalable Vision Learners, used similar approach to do self-learning problems.	"(1) Some statements are too strong to be demonstrated. (2) Computational efficiency is not analyzed although it's mentioned in the last sentence of Section 1 that ""our method is data efficient and more friendly to computing resource."""	The method is made by many step. The overall method is clear but the reproducibility of each single step is not, probably due to the limited number of available pages.	Good	No code sharing was mentioned in the submission. The datasets are publicly avaible.	The proposed method is interesting and achieves an improvement in corneal nerve segmentation. However, this improvement is limited with respect to other methods for corneal nerve segmentation. I suggest to the authors to investigate about the clinical relevance of this small improvement and about the generalisation of the proposed method to other types of images.	The framework is a very complex framework e.g., three stages, pertaining with two networks type GAN, fine-tuning, retraining etc., but the Ablation Study is very short, and the table is not convincing enough. For example, what is the influence of plain model with using pseudo-labels, dropping any training stages, what about the other combinations. Based on my understanding, in this complex framework both model design and dataset mixing shall play a role and Ablation Study shall distinguish them clearly.	"(1) It's unclear why gated convolution is used in coarse repairing network in the perspective of its difference to refine repairing network. (2) It may be improper to use the term ""supervised "" in Table 2, as the ground truth are simulated from existing centerline, which are not real careful annotations and may be even worse than pseudo labels. Therefore, this manner is not able to demonstrate the proposed method is better than supervised method. Please use a weaker statement or provide stronger evidence. (3) Please provide analysis of computational considerations such as data efficiency and time efficiency. (4) Please provide description about what knid of preprocessing steps are needed to deal with noise and difference in background brightness. (5) Is there any reason to propose this framework for nerve segmentation? Is there any special design for this narrow object? Is it also suitable to solid objects?"	The paper is well written, methods and results are clear. The proposed methods is interesting. Some parts of the proposed method are a novelty, and consequently the overall method is a novelty applied to corneal nerves images. The improvement in the segmentation of corneal nerves is limited.	The paper is good at implemenation but the novelty of the paper is not strong for MICCAI.	Good work but some important descriptions need to clarify.
456-Paper1900	Semi-supervised learning with data harmonisation for biomarker discovery from resting state fMRI	This paper proposes a deep-learning framework that includes data harmonisation and semi-supervised representation learning for disease classification and biomarker discovery by using data from multiple sites. Their reported performance on two datasets is impressive compared to the existing work in the literature.	combined harmonization and classification framework demonstrates the ability to determine generalizable biomarkers	This paper presents a semi-supervised learning (SSL) method to harmonize data across imaging sites while simultaneously learning a classification task. The proposed variational autoencoder (VAE) model has a data harmonization encoder and decoder, with the latent representation used to learn the target classification task. The model is trained in semi-supervised way, where unlabelled data from multiple other sites are used to learn the data harmonization encoding/decoding, while labelled data is additionally used to learn the classification portion of the model. A two-step harmonization approach was also proposed, where the ComBat method was used for initial harmonization, followed by the proposed method. The method was tested against supervised learning on single sites and SSL without harmonization using the public ABIDE dataset.	The paper is well written and eacy to follow by making their contributions clear. The performances on two public datasets are improved by large margin.	Large multi-site cohorts Multiple diseases	This paper aims to solve a very important problem in neuroimaging studies - data harmonization across imaging sites, which would allow for combining different datasets for more generalizable analysis and thus learning neuroimaging biomarkers that truly represent the disorder/disease under study and not specific to a single imaging site/study. The novelty in this work lies in the incorporation of a linear data harmonization module in the encoder and decoder of a VAE model. The authors provide a link to the code and test on a public dataset, enhancing reproducibility. The experimental validation methodology is thorough, with 5-fold cross-validation performed with 10 random starts, and hyperparameters optimized using a separate validation dataset and set before running all the test experiments. The paper is generally well-organized and easy to follow.	The technical improvement is marginal by exploiting the existing mechanisms. It needs to survey recent work on multi-site brain disease diagnosis thoroughly. The reported performance is not persuasive by showing big difference from the existing work.	Effect of augmenting datasets through others not fully investigated	While the motivation for the proposed semi-supervised learning approach was that the there may be label inconsistency issues across sites. However, I'm not sure that this is a real concern in the presented ASD classification case - the diagnosis of ASD is largely objective and follows clinically well defined parameters. Still, there should certainly be other applications where label uncertainty could be an issue. The proposed data harmonization model does not consider the target label (ASD/typical control). I am wondering if there is a concern then that the two neurologically different groups are being harmonized to one set of parameters? For example, perhaps activation in social motivation areas is greatly reduced in ASD subjects compared to controls, but the same alpha_v and beta_v are learned for both groups. The experimental methods do not include any comparisons to other data harmonization approaches. At a minimum, I would have expected to see a comparison to a method that first runs ComBat on all the data and then uses the harmonized output to perform supervised learning for each site individually. Other potential approaches for the general domain shift problem is something like a generative adversarial network that tries to learn a representation invariant to site and other factors (e.g., [1-3]). [1] Bashyam et al., Deep Generative Medical Image Harmonization for Improving Cross-Site Generalization in Deep Learning Predictors, 2021 [2] Gao et al., A universal intensity standardization method based on a many-to-one weak-paired cycle generative adversarial network for magnetic resonance images, 2019 [3] Liu et al, Style Transfer Using Generative Adversarial Networks for Multi-site MRI Harmonization, 2021	NA	Reproducibility is good	The authors have checked off all relevant items on the reproducibility checklist, including sharing of code, and the reporting in the paper matches the checklist, so should be highly reproducible.	The paper missed many recent studies on multi-site brain disease diagnosis, including population-based graph neural networks, domain adaptation, and domain generalization methods. The proposed method applies the harmonization operation over the FC values, processed through a Pearson correlation function, rather than the raw BOLD signals. It is wondering how the performance would change when the harmonization is applied to the BOLD signals first and then to construct their respective FCs, feeding into the EDC. It would be interesting to compare them from different viewpoints. Regarding model training, no loss term involves the site-related parameters of $\gamma_{iv}$ and $\delta_{iv}$. How can those parameters be optimized? The authors raised an issue of inconsistency in diagnostic criteria among sites. However, the proposed method doesn't handle the issue at all. A critical concern about the performance is that the reported performance values are too high compared to the existing work on the same dataset, e.g., ABIDE, in the literature. In particular, the accuracy of the competing method of ASD-SAENet is approximately 10% higher than the one reported in the original paper. How could the authors explain this? The authors raised an issue of inconsistency in diagnostic criteria among sites. However, the proposed method doesn't handle the problem at all.	With the goal of helping smaller studies to be able to augment their data set, I think it is imperative to show the stability of the determined site-invariant biomarkers. A simple leave N-Sites out approach should be able to do this? It would also be good to see how much training data was actually going into the individual models. I also spotted a wrong highlight in Table 1 for USM. Maybe use only one number after the decimal point, as this table was pretty hard to read in some areas. I would not call putting the code online, although extremely important, a major contribution in the introduction.	"Comments are listed in order of appearance in the paper. The most major concerns are marked with (M). The paper states ""Unlike more complicated alternatives like ComBat, our approach of removing site differences allows biomarkers to be easily derived via computing saliency scores [10] since the implementation is based on linear layers."" The proposed generalized linear model for harmonization is the same as that used in ComBat, but ComBat estimates the parameters in a different way. Thus I am not sure what is meant by the quoted statement, as after the resting-state fMRI data is normalized in ComBat and then applied to say a classification task, feature importance could be attributed to the proper ROIs using the normalized data. In the data harmonization definition in eq 1, should M_{jv} be M_{iv}? The design matrix for covariates of interest (eg. gender, age) should only depend on the specific subject (i.e. subject j at site i). In eqs. 4-6, it seems that the subscript i has disappeared, but is needed to denote all the subjects from different sites. In Sec 2.4 motivation for SHRED-II, where ComBat is applied before using the proposed model, the sentence ""For very small datasets (< 50), we propose a two-step variant of SHRED"" makes it sound like ComBat is only applied in the small dataset cases in the experiments. However, I think that the intended meaning is that the two-step variant is proposed to further improve prediction in small dataset cases. Please clarify/reword to make this clearer. (M) For training of all the deep learning models, how many epochs were run / what criteria was used to determine when to stop training? The loss appears to be largely dominated by the harmonization reconstruction term, with a hyperparameter that is many orders of magnitude higher than the other loss hyperparameters. I'm wondering how much of an effect the VAE parts of the loss have then, i.e., how important is the VAE modeling compared to simply the shared representation and included data harmonization? (M) To reiterate the point in question 5. it would be helpful to see comparisons to other data harmonization approaches, e.g., applying ComBat and then single site training.  I am also curious how the results would look if the proposed model were applied to single site data, so that there would be harmonization of age and gender factors. This could further demonstrate the advantage of being able to include more data in the semi-supervised learning approach (if this performs better). For the supervised and semi-supervised (without harmonization) methods, it appears that the extra data that is used for harmonization (age, gender, site) are not included in the model. This then is not exactly a fair comparison, and goes back to the point above about needing to include other harmonization comparisons or at least compare other methods that also consider age, gender, and site (e.g., as inputs to the DNN). For the ASD-SAENet results, how might the authors explain the seemingly much higher performance reported here than in the original paper proposing the approach, which also used the ABIDE individual sites for testing? Table 1 is a bit hard to read - consider adding more space between columns or vertical lines to better separate values. Some noted typos: p. 5 ""constraints"" -> constrains"	Lack of survey on the related recent work No loss term for the site-related parameters of $\gamma_{iv}$ and $\delta_{iv}$ Inconsistency for the performance of the competing method to the original work	While I asked for some additional highlights, I think this is a great paper to be discussed by the community. Data harmonization is very important and it is good to see a combined approach including classifiers.	To help solve the important problem of data harmonization across imaging sites, the proposed method nicely incorporates the harmonization into a VAE model that also jointly learns the target classification task. However, I have concerns regarding the assumptions of the harmonization model for handling neurologically different groups, and very importantly the paper does not present any comparisons to other data harmonization techniques.
457-Paper0358	Semi-Supervised Medical Image Classification with Temporal Knowledge-Aware Regularization	This work proposes a new framework TEAR for semi-supervised medical image classification, which contains an AdaPL module for relaxing hard pseudo labels to soft-form ones and an IPH module for aligning feature prototypes across different training iterations.	This paper presents a semi-supervised image classification method with adaptive pseudo labelling and iterative prototype consistency. The adaptive pseudo labelling uses a loss estimating function to soften and calibrate hard pseudo labels while the iterative prototype consistency aligns the cluttered class centroids across model training iterations to reduce dependency of pseudo labels. The author conducts experiment on three datasets and the proposed method outperforms existing state-of-the-arts algorithms. Also, the author provides detailed ablation results to verify the effectiveness of their method.	This paper proposes a TEmporal knowledge-Aware Regularization (TEAR) for semi-supervised medical image classification. The upper bound of the loss of unlabeled samples was theoretically proved and it was used to relax pseudo labels (soft pseudo labels). In addition, the iterative prototype harmonizing (IPH) is proposed to maintain the harmonization of clustered prototypes across different iterations. In the experiments, the proposed method outperformed the state-of-the-art methods and sufficient ablation studies were conducted.	The designed AdaPL to mitigate confirmation bias is based on a theoretically derived loss estimator. The proposed IPH to encourage the harmonious clustered prototypes across different training iterations works in an unsupervised way. The paper is well written.	the idea of using loss estimating function across training iterations to calibrate pseudo labels is interesting also reasonable. The author provides theoretical proof of the feasibility of the loss estimating function. the proposed method achieves SOTA results on three datasets. And the author provides sufficient ablation results to demonstrate each components and effect of hyper-parameters.	*The theoretical analysis for an upper bound of a loss of unlabeled sample was conducted and it is used to soften the pseudo labels. This is interesting. *To match the feature prototypes across different training iterations, IPH is proposed, which takes the advantage of the knowledge from different training iterations and provides the coherent optimization. *The evaluation is sufficient. It contains the comparison with a sufficient number of compared methods, ablation studies, and the hyper-parameter sensitivity. This shows the effectiveness of the proposed method.	The proposed IPH exploits cluster-aware information that conducts an unsupervised clustering method (e.g., K-Means) within a mini-batch, so the batch size is vital to IPH. I guess the experimental results are sensitive to batch size, which, however, is not discussed in the paper. For ISIC dataset, the authors seem to copy the comparison results of GLM [36] and NM [22] from NM, which is an unfair comparison because the proposed method adopts a strong augmentation for training data.	the IPH compute clustered feature centroids within each mini-batch which makes it a little bit unconvincing. Because the batch class mean is likely to be a poor approximation of the real mean. Also, the batch size may affect the clustering result. when not all classes are present in a batch, the clustered features cannot reflect the correct class distribution and thus may bring new problems. the author did not show how the metric which measures the consistency of clustered feature centroids changes over the training iterations. For instance, a figure show the distance of centroids across iteration before and after using IPH might help illustrating the mechanism of IPH.	*Some of the explanations are unclear. in Eq.5, \sigma is the normalization function. However, $u$ is a single sample (LE(u;t) is a scalar). How to normalize it? It indicates using all the unlabeled data? The current description is a bit ambiguous. In the comparison and ablation study, what value of 'b' in Eq.5 was used? *The comparison is O.K. but, to show the effectiveness of the proposed method, it is better to compare with the semi-supervised methods that use training iteration information, such as mean-teacher algorithm. *Once carefully read the process of the method, the reason why the method is effective is understandable. However, it is not easy to understand it by reading the introduction. The reviewer recommends the authors to more clarify it, i.e., show the simple examples what case the relaxation is large or small and why.	The implementation details are enough for the reproducibility of the paper.	the author provides sufficient implementation details which helps reproduce the results.	The reproducibility is fine.	An ablation analysis on batch size is needed to verify whether the results are sensitive to it. It's better to visualize the cluster across different training iterations. This will help the authors understand how the proposed IPH module works. A fair comparison between the proposed method and other medical image classification methods [22, 36] should be conducted.	I appreciate the thoroughness of the analysis (different datasets, several state of the art methods, ablation study).  The organization and presentation of the paper is easy to follow. But the paper lacks qualitative analysis to demonstrate the mechanism behind the components (IPH) of proposed method. It would be excellent if the author can provides additional analysis on this point in their future work.	please see above comments.	The experiments lack of key ablation analysis and the comparison experiment is somewhat unfair.	good technique contribution good performance on three datasets the paper is well-organized and easy to follow	As described in strength, the paper organization is good, and the proposed method is well designed, and the evaluation is basically sufficient for MICCAI. Therefore, my rating tends to 'acceptance'.
458-Paper0427	Semi-Supervised Medical Image Segmentation Using Cross-Model Pseudo-Supervision with Shape Awareness and Local Context Constraints	This study proposed a semi-supervised segmentation framework consisting of two Unet networks that generate pseudo-labels for each other. The study also proposed a loss function, local context loss, as an extension of the dice loss. The framework was evaluated using two public datasets and showed superior results compared to other approaches with semi-supervision (n=7,14) while underperformed the baseline method with full supervision.	This paper introduces a semi-supervised segmentation approach. It extends a prior art of cross-modal supervision by incorporating two things. First is the share awareness by co-teaching a shape agnostic network and a shape aware network. The second is to adopt local context constraints using patch level dice loss. The approach is applied on two public datasets in a semi-supervised setting, and demonstrates superiority over other methods	This manuscript is about incorporating shape priors in the context of segmentation with neural networks. For this purpose two networks are used. One network is shape agnostic. The second network is shape aware and receives as input the original image as well as the output of the first agnostic network. The second network also encodes local information since it considers different parts of a shape. The method is validated with two datasets.	The proposed local context loss is new and showed improved segmentation performance.	The design of the framework is clear and reasonable The results look positive compared to other methods	It addresses a valid problem, namely incorporating shape information in neural networks segmentation. It is motivated by the publications referenced in [4][9], that are interesting.	The novelty of the paper is limited. Although with a handful of adaptions, the overall architecture of using two Unet networks and feeding the prediction to the other is not new. The gains of using the proposed architecture are marginal. Overall, the proposed method underperforms the baseline method with full supervision.	The author does not seem to be clear on what exactly are novel on top of their citations [4] and [9] The ablation studies can be improved.	It uses two different networks rather than one. Why not one? In the unsupervised loss training function (page 5) for unlabeled data the authors claim that the labels provided by the shape network are more accurate because of the shape information. This is not explained and substantiated. In general the cost terms and their terminology are explained after the whole cost is introduced. They should state any pre-processing necessary for the shapes whether the prior shapes must be aligned and the robustness of the method after rotations.	The authors will provide the code after acceptance. Public dataset is used.	The method is clearly described, so the paper should be reproducible in terms of implementation.	Precise reproducibility is not a particular strength of neural networks. The networks are described in the implementation section and can be reimplemented.	More theoretical explanations for the framework architecture or how this design sufficiently addresses the lacking of dataset issue, i.e. overcoming variability and complexity issue of medical images, is needed. For the comparison experiments, the choice for the number of training samples, 7 or 14, needs to be justified. Would this be clinically relevant, as a larger number of training sets could be reasonably obtained for the segmentation tasks targeted? Overall, the proposed experiments showed inferior results compared with the baseline ones using full supervision. When the local context loss is used, the DSC increases substantially for RV, while only slightly for Myo ad LV. An explanation for this difference is helpful.	"The paper is well written, the idea is simple and clear, the key is how much the author's contribution helps on performance gain. It will be nice if the authors can elaborate a bit what exactly is new compared to citations [4] and [9]. Also elaborate a bit on the ablation studies, specifically in Table 1,  what exactly is the ""Baseline""? what is ""UE"" (the authors mentions only U1 and U2 for the unsupervised loss), what is the impact if no threshold is applied to the entropy? There is also a conflict when the author describes the implementation and experiments. The author claims that they use only 7 labeled samples to generate the results in Table 2. But in section 3.2, the author states that during training a batch size of 24 is used, 12 labeled, 12 unlabeled.12 is more than 7. This is confusing"	The unsupervised loss function should be better justified. Could state the robustness of the method with respect to rotations or whether they have to be aligned and in the same pose. The preprocessing necessary for shapes.	Please refer to the strength(4), weaknesses (5), comments (8) for justification.	The paper presents a simple but effective semi-supervised approach. It is not exceptional in terms of novelty, but it is solid given the benchmark results.	A good method to incorporate prior shape to segmentation with neural networks that is currently not sufficiently addressed. Could be more forthcoming for pre-processing conditions.
459-Paper1441	Semi-Supervised PR Virtual Staining for Breast Histopathological Images	the authors designed a Pos/neg classification module  for consistency. Also they achieved the transformation of H&E staining to PR staining for breast cancer for the first time.	The paper describes a method of virtual staining that transfers between H&E stain and progesteron stain in breast cancer histopathological slides. The training is based on consecutive slides with different staining. Te stain transfer of un-paired patches is achieved based on the cycle-consistency framework.	The authors propose a smart system for IHC staining out of a H&E staining.	Well-written, use of significant metrics for evaluating achieved results	interesting idea with pos/neg classifier to improve consistency	The paper is well written about a smart architecture to virtually stain histopathological tissue in IHC (PRC) from H&E slides taking into account classification into PRC+ and PRC- consistency in the the architecture.	There is no clear if the dataset is free and public	more qualitative than quantitative results the metod works on 5x magnification (big FOV) with such magnification clinical usefulness of such processing is questionable	The image processing part about labelling the PR+ patches based on color analysis is tricky as brown is a difficult color to model. The post analysis with pathologists that could be interesting to explain why there is discordance is not done.	As said before, the dataset not seems to be free and public	the model and additional classifiers are vaguely described, without source code the study is not reproducible	Reproducible	As said before, the dataset not seems to be free and public	"Comments: (p.3) ""It is a pity for traditional unsupervised method to discard the information of consecutive slides completely"" - this is unclear, please explain/rephrase the description of generator on p.4 is vague. If the specific architecture is not crucial and the metod was tested with several different architectures yielding similar results it should be clearly stated in the paper. Alternatively, the specific architecture used should be give in supplementary materials. Nash equilibrium (p.5) is not properly introduced of referenced - it could be misleading for non-expert readers - please provide short explanation or reference (p.5) ""[...] auxiliary classifiers are used to introduce attention for finding the focus region [...]"" - the auxiliary classifiers are not fully described, please provide further explanation in either mian text or supplement (p.5) I believe the Nvidia model 3090 should be RTX instead of GTX, please double check on that Results presented in Table 1 are not cross-validated and based on overall only 8 WSI slides, this poses a question about reliability of these numbers slide-level performance and ablation test has only qualitative results, that might be biased the metod works on 5x magnification (big FOV) with such magnification clinical usefulness of such processing is questionable, please provide some confirmation or reference/citations"	"Dear authors,  very interesting work. I would go further on the ""brown"" color extraction for PR+ annotation and will add an explanation by pathologist about their perception error in the brownish grading. Proof read again ; ""Refenence"" instead of ""Reference"" and Paragraph 2.1 and 2.2 are not always very clear at first reading."	It is a very good scientific contribution in the field. The experimental framework is robust and denotes a lot of experimental work and analysis	All in all it is a well structured paper providing an interesting idea in the field of digital pathology. The reliability of the results might be questionable and majority of evaluation is qualitative (hence subjective), but nevertheless the performed study seems to be improving the knowledge in the domain.	The paper is interesting and well written but deserve to be slightly improved (English and explanation) and perhaps more technical details about color analysis in part 2.1 as well as pathologist perception of brown color in the virtual stained slides.
460-Paper0482	Semi-Supervised Spatial Temporal Attention Network for Video Polyp Segmentation	The authors propose possibly the first method for semi-supervised video-based polyp segmentation. To accomplish this, they annotate 60 videos of a video polyp detection dataset. Their technique uses two transformers to exploit both spatial and temporal information effectively. The proposed approach beats state of the art techniques.	A novel approach is proposed for polyp segmentation in clinical videos. This method consists of two main modules that shape a semi-supervised polyp segmentation architecture.	This paper proposed a semi-supervised polyp video segmentation by introducing Temporal Local Context Attention (TLCA) module and Proximity Frame Time-Space Attention (PFTSA) module to improve the video polyp segmentation.	Motivation is well founded. There is a need for methods which can segment polyps from video, and even creating datasets for such a task has been a shortcoming in the community. 2.The use of transformers to exploit both spatial and temporal information (although not novel to computer vision in general) is novel for this application. The authors achieve state of the art results, especially a fairly significant bump in terms of mIoU score. Further the authors get a good bump over the other transformer based PVT technique which shows the importance of the extra data for the data hungry transformers.	The paper is proposing a novel framework for a well-known clinical problem and represents a stronge evaluation.	The paper introduced the recent two main modules and proposed a Semi-Supervised Spatial Temporal Attention Network (SSTAN) for the polyp video and showed the higher performance in the segmentation problem.	There is nothing particularly novel about this work in terms of technical innovations. Vision transformers have been used both spatially and temporally in the literature for natural images. However, there is good enough application novelty in the reviewer's opinion to justify publication in this venue. There is a significant need for a semi-supervised video-based polyp segmentation method, and further for the data annotations. There is no discussion about how the training, validation, and testing images were split, no cross validation, no error bars, and no discussion of potential bias introduced. This can be mildly excused due to the cost of video data annotation and only having a limited number of videos fully annotated, but a discussion of how videos were chosen for the splits should at the very least be included.	There is no information regarding the computation time. It is difficult to evalute the paper based on the feasibility of implementing the proposed approach for real-time clinical application.	Two main modules improved the higher accuracy of segmentation performance but the modules are not originally proposed in this paper.  Segmentation task is important but I feel automatic classification task of benign or malignant of polyp is more important these days. Some product has been sold these days.	It's unclear from the reproducibility questions whether the authors plan to release their masks they produced. The authors do not include any complexity information about their approach, where it's known that transformers tend to be parameter heavy and slow. I do not see any discussion of hyperparameter sensitivity in the supplemental materials like the authors claim.	A detailed explanation of the proposed approach is presented and public available dataset is used for the evaluation step.	The paper shows source code is available and it is acceptable.	At one point the authors say they annotate every 11 frames. But in the supplemental materials the figure says every 5 frames. This should be clarified.	Figure 4: if possible, please mark the ROI in the original images in order to present a better understanding of the part that should be segmented for the readers. Section 4, Concluion: A stronge evaluation over the proposed approach is presented; however, the main advantages of the method over the state-of-the-art in not discussed. I recommend to highlight this point in the manuscript. Section 4, Conclusion: There are no significant evidence (such as computation time) in the manuscript to evalute the feasibility of the method for the real-time clinical application. I recommend you to add this point along with more explanation on the clinical need for such a system.	Qualitative results are shown with different models and the segmentation performance of the proposed approach is good.  It is better that some failure examples of this approach is shown with the reason.	Overall this is a decent paper worthy of acceptance for it's application novelty. There are no major issues with the paper, but also nothing particularly novel in terms of technical innovation.	The manuscript present a new approach for semi-supervised polyp segmentation using video data. It is very well-written and needs some adjustments and improvements to highlight the advantages and the feasibility of clinical applications.	Segmentation performance is good and basic improvement was done by adding the two recent modules.
461-Paper0543	Sensor Geometry Generalization to Untrained Conditions in Quantitative Ultrasound Imaging	This paper describes a framework to generate attenuation coefficient (AC) images from ultrasound (US) that is able to generalize to different probe geometries, in the context of quantitative US. The main contribution of the proposed method is a Deformable Sensor Adaptation (DSA) module, which takes the radiofrequency signal and the B-mode image as input, and learns to adjust the sensory data to a common sensory representation. The DSA module is trained leveraging on a meta-learning scheme, which improves generalization to unseen probe conditions.  Both numerical and in-vivo validation using probe configurations not seen at training time is performed. Comparison with other domain randomization approaches is present.	They propose a deformable sensor generalizable network to calibrate probe conditions. -They assessed the proposed method through numerical simulation and in-vivo studies.	The authors present a deep learning approach for AC reconstruction which is robust to the changes of the probe.	The proposed framework presents an original approach to synthetize AC images starting from sensory data, even when acquired with US probe geometries different from the ones considered at training time. The proposed approach provides an interesting contribution in the context of US-guided procedures in general. In fact, learning-based methods are generally trained on images or methods acquired with a single fixed probe geometry, and they thus fail to generalize to images acquired with different US probes or systems. This paper presents a promising idea to improve the generalization capabilities of such methods. Experimental evaluation is performed to assess the performance of the method both on numeric examples and in in-vivo settings. An ablation study is also reported to show the contribution of each of the presented modules.	They improved the accuracy of the AC image by 9.7%. In comparison with other methods, their method demonstrates 11.8% reconstruction improvement of the unseen data. Their proposed method demonstrates 26% enhancement for the reconstruction of out-of-distribution sensor geometry.	The work has good novelties It is highly required for ultrasound imaging Very good abalation experiments convncing improvements	The main weakness of the paper lies in the description of the conducted experiments and the obtained results. While the presented evaluations look reasonable, further details should be provided to better understand how the method assessment has been performed, why conducted experiments are relevant, and why the obtained results represent a proof of framework accuracy (more details below). Moreover, description of the methods is sometimes difficult to follow due to the many details, parameters and acronyms. Used parameters and notation are not always properly defined.	The biomechanical properties used for simulation are not described.	Some of the parts should be refined so that the readers can have a better undrestanding of the approach (exp: Meta learning, DSA ) In simulation results, there is no experiment for CNN trained on signle imaging setting (I think Baseline without those modules still trained on multiple imaging settings?, if not, please clarify) There is no comparision with non-deep learning approches	"In the reproducibility checklist, authors claim that they share a sample test dataset and their code. The current version of the manuscript has no links to open source repositories, but I expect authors would make data and code available upon acceptance. I see it very difficult to replicate the method and reproduce the results by only following the description provided in the paper, due to the complexity of the proposed architecture. Moreover, authors declare that ethical approval is ""not applicable"" to their work. Actually, since their evaluation include real patient data, ethical approval should be present."	The authors agree to make public the, Training code, Evaluation code,  (Pre-)trained model(s), Dataset or link to the dataset needed to run the code.	good	"The description of the introduced symbols should be improved. Some notation is defined but never used (e.g., W_probe), other is never defined (e.g., R and omega in equation 1). Then, do y and I_s represent the same thing (i.e., output image)? In 2.4, authors say that y represents the ground truth, while in 2.1 y_i is defined as ""organs and lesions"". Definitions should be more consistent. The implementation details at the end of 2.4 are very difficult to follow, since the notation is quite heavy. In particular, it is not straightforward to me what the apices p, l and r associated to the inputs x indicate. Authors claim that the method provides ""accurate AC image"", but more details should be provided to support this claim. How do PSNR and MNAE allow to assess the method accuracy? Equations provided in the supplementary material B use a different notation than the one in the paper, thus are not really helping with the understanding. Moreover, measurement units should be reported. In 3.1, authors refer to the ""aggregated dataset"" but they never define what it represents. It is not clear how the Deep-All baseline method is exploited for evaluation. Are the proposed modules used together with Deep-All in the ablation study? If so, how are the different modules interfaced? When authors refer to ""the NN"" in section 3.1, do they mean the Deep-All model? The description of in-vivo experiments should be improved. In particular: 1) How do authors define which is the AC related to lesions in in-vivo experiments? Do they extract lesion area from the generated images via segmentation?  2) To which approach do results related to the ""ablated baseline"" refer to (among those in Table 2)? 3) Since the paper reports an evaluation using in-vivo data, ethical approval must be mentioned. 4) I would suggest moving the details about the geometry of the probes used in in-vivo trials to section 3.2, to group there all the information related to such experiments. As for the data-augmentation part, it would be interesting to report the data distribution after VSG-aug is used and compare it with the one in Table 1. This would allow to understand if the tested conditions fall within the data distribution obtained when using the augmented dataset. Images can be improved for better clarity. Fig.1 should include the acronyms relative to each ""module"" to facilitate understanding of what is what. The font size in Fig.2 is too small, making it impossible to read, but it is important since it is the only way to know the dimensionality of the different parts of the architecture. Typos: Euclidian -> Euclidean Each resolution subnetworks -> Each resolution  subnetwork adjust -> adjusts"	"The authors mentioned they used ""biomechanical property that is set to cover general soft tissue characteristic"" with  a reference. It would help better clarification of the paper to mention these mechanical properties and how they were implemented in the simulations."	There is a recent highly related paper that the authors have probably missed and I recommend to cite: A K Z Tehrani I Rosado-Mendez, H Rivaz, (2022). Robust Scatterer Number Density Segmentation of Ultrasound Images, IEEE Trans. UFFC (TUFFC) The authors should simplify the paper. The DSA module should be explained in more details. Why using B-mode in DSA and why B-mode of one imaging setting is used. Overally, some part of this paper is not clear.	Generalization to multiple probe geometries is a very interesting and novel contribution, with high potential impact to other US-guided applications. The paper is well-written and the evaluation looks robust, even if poorly described. I think the paper can be considered for publication, provided that authors improve the description of the conducted experimental evaluation and the obtained results.	Network structure and the proposed method are explained well	The paper has the merit of acceptance in MICCIA. Some of the modules such as meta-learning and DSA should be explained clearer.
462-Paper0415	SETMIL: Spatial Encoding Transformer-based Multiple Instance Learning for Pathological Image Analysis	This paper presents a novel spatial encoding multiple instances learning method for pathological image analysis. It releases an attention-based pyramid multi-scale fusion module, which is a novelty for aggregating the local information of the patches. In addition, the joint relative and absolute position encoding module simulates the diagnosis process of pathologists. Presented the two modules, the transformer-based model gain improved performance in pathological image analysis.	This paper proposes SET-MIL, a transformer-based framework for WSI representation learning which incorporates spatial information (neighbouring instances and globally correlated instances) to obtain representations that capture more semantic information. Based on the experiments, the proposed method outperforms recent MIL-based methods for WSI representation learning task.	The presented paper introduces a MIL approach for WSI level feature embedding and classifier with position preserving embedding followed by Transformer-based Pyramid Multi-Scale Fusion. Results were verified in two datasets in the paper and TCGA experiments in the supplementary materials.	"1.The pyramid multi scale fusion is interesting, combining the T2T(tokens to tokens) and MSA(multi-head self attention) enhances the features map's local representation and this process is trainable. 2.In SET module, the relative encoding leverage the MSA ""pay more attention"" to local information of neighbouring instances. 3.The network has some advantages in dealing with the tasks, whose local information of the neighbouring patches is vital for the pathological feature."	The paper is technically sound. The idea of aggregating representations of both neighbour instances and globally correlated instances simultaneously, which mimick the clinical practices, seem to be novel and improve results. -Experiments are comprehensive.	In general, the work seems novel, and the results are promising. I would support this work as it addresses the main computational pathology concerns. 1-   Providing slide level description for WSIs is what people need with current datasets like TCGA. MIL approaches are an excellent way to use this slide-level information.  2-   The idea of preserving all patches coordination is new to the best of my knowledge. This is important as tissue morphology is meaningful with looking at its neighbours.  3-   Multi-level view is necessary when dealing with WSIs, and this work uses the multi-scale approach smartly. 4- Experiments are well designed and multiple datasets have been explored, however, I have some concerns regarding the experiments comparison.	Trans-MIL has already adapted the transformer to solve the MIL problem in pathological image analysis. Essentially, This paper seems similar to Tran-MIL[17]. The differences with Trans-MIL [17] should be clearly descripted. 2.In fact, The SET module is the relative encoding module, it's not clearly claimed in the paper.  3.This paper displays that the local information of neighbouring instances is vital for the pathological image analysis. This view is not mentioned in the paper. The pyramid multi scale fusion module and the SET both enhance the local representation of the neighbouring instances, the purpose of this design and the difference of these modules should be described clearly.	"I am concerned about the practical application of the current framework as its memory usage is converging to a setting where we feed WSIs directly to the model. This is because that current approach is employing all patches from WSI for representation learning which leads to the famous memory bottleneck that exists for this task. The fact that all the patches are being used makes the experiments against many MIL methods unfair as they only employ a small number of patches for training the model. Comparing against a framework like:  ""Neural Image Compression for Gigapixel Histopathology Image Analysis""  David Tellez*, Geert Litjens, Jeroen van der Laak, Francesco Ciompi That uses all patches seem to be a fair comparison. The training here is not end-to-end, which may lead to sub-optimal solutions. Recently there have been efforts to develop end-end MIL methods for WSI representation learning. This is necessary to discuss your approach against methods that use a small proportion of patches but trained in an end-to-end manner. Which approach is preferred and Why? ""CNN and Deep Sets for End-to-End Whole Slide Image Representation Learning"" Sobhan Hemati, Shivam Kalra, Cameron Meaney, Morteza Babaie, Ali Ghodsi, Hamid Tizhoosh"	1- The main consern is the fairness between experiments. Most MIL methods are using small amount of WSI whiledeveloped method is using all patches in one WSI. So it might not be fair to compare light algorithm with bruteforce method.  2- The other weakness is connected with the first one. As feeding whole WSI to the model (even with L'' and W'') still model is large. Such that just 4 of them fitted to V100. As a result, the work might not be repruducable with regular GPUs. 3- The last weakness is feeding the blank space to the model. Even if model remove them after training still they will  be fed to the model.  4- I wish more pathology related point of view like multi-magnification (5x, 10x, 20x) was used rather multi-level	the paper provided sufficient details, the reproducibility is good.	The source code has been  provided.	Sharing code is one of the best practices for reproducibility, and I would say the work is reproducible.	see 5.	"I believe authors should discuss the memory usage of the current approach and also the fairness of the experiments against methods that use a small proportion of patches. To this end, they can compare their method against the following work, which also uses all the patches. Here is an example: ""Neural Image Compression for Gigapixel Histopathology Image Analysis""  David Tellez*, Geert Litjens, Jeroen van der Laak, Francesco Ciompi Also, this is necessary to benchmark their work against end-to-end methods. Here is an example: ""CNN and Deep Sets for End-to-End Whole Slide Image Representation Learning"" Sobhan Hemati, Shivam Kalra, Cameron Meaney, Morteza Babaie, Ali Ghodsi, Hamid Tizhoosh"	"Fig. 2. Sub-figure (A) - Color meanings is not considered. For a while I was confused with yellow green blue color code for 3x3, 5x5, and 7x7 windows. However, right below that the same colors have been used for showing unfold without any relevance to the windows size. = Concat(T2Tk=3(Ei), T2Tk=5(Ei), T2Tk=7(Ei)), it needs period (.) after this equation. ""achieve inferior performance compared to other methods [1, 9, 8]."" should be other MIL methods Table 1.needs patch percentage from each WSI"	The design of the network is effective. The novelty is not described clearly.	Although the practical application of the current framework is questionable, the ideas in SETMIL for mimicking the clinical practice by aggregating representations of both neighbour instances and globally correlated instances are interesting.	Enough innovation and extensive experimentation plus acceptable results are main reasons for the acceptance.
463-Paper2204	SGT: Scene Graph-Guided Transformer for Surgical Report Generation	The paper proposes to leverage scene graph and Transformer to generate surgical report, where DPP is used to obtain the prototype for encoding global relation, and a homogeneous graph is constructed to encode local relation. Extensive experiments on EndoVis18 is performed to validate the effectiveness of method.	This work proposes a Transformer-based architecture guided by scene graphs to approach the surgical report generation task. This paper uses scene graphs representing visual objects and relationships encoded using a Transformer encoder. In the attention layer, the key and value are expanded with a sample memory obtained using a k-Determinant Point Process [12, 15]. This strategy allows the use of global and local attention. Finally, it uses a meshed decoder [5] to generate the report from the resulting encoder rendering. This paper presents results in one benchmark dataset: Endovis2018.	This paper introduces a novel surgical report generation model vis scene graph-guided transformer (SGT). The scene graph could extract the relation graph between tissues and instruments, which could accurately guide the report generation process. A novel relation memory augmented attention is introduced to better interact between input videos and generated reports. Graph neural network is adopted to better model the relations between tissues and instruments.	The novel method which leverages the scene graph to generate surgical report Promising results achieved with thorough ablation study	The task of surgical report generation is relevant to the medical image analysis community. The method introduces technical novelties that improve the empirical results of the task. This work significantly outperforms the state-of-the-art in this task.	The proposed approach is novel, which first generates a scene graph for each frame, and then uses the scene graph to guide the report generation. To solve the redundancy issue for the entire video, a determinant point process (DPP) is adopted to sample the most important frames. Besides, the graph neural network is used to encode the scene graphs. The experimental results on the MICCAI 2018 challenge show that the proposed approach could significantly outperform the baselines.	Some clarification should be made, see the detailed comments The evaluated dataset is quite small, therefore, cross validation is better to be conducted	In general, the clarity and organization of the paper could be improved. The method section could describe the model from input to output. These modifications will make it clearer for the audience. Additionally, the mathematical notation should always support the text. The ablation study does not allow an assessment of the model generalization capacity. Optimizing the architecture over the test sets might result in overfitting in the benchmark dataset. The ablation study should be performed in the validation set.	"Some components are not explained clearly. For example, in equation (1) what is the matrix Z? How to obtain Z? There are some typos. (1) On page 3, section 2.2, line 3, either ""the"" or ""a"" should be used. (2) On page 4, in line 5, what does p mean in \mathcal{E}_{he}^{p}? There are some missing references. This paper has mentioned several recent chest x-ray report generation papers as related works. However, please also consider citing the following two easiest works on chest x-ray report generation. [1] Jing, Baoyu, Pengtao Xie, and Eric Xing. ""On the Automatic Generation of Medical Imaging Reports."" In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pp. 2577-2586. 2018. [2] Wang, Xiaosong, Yifan Peng, Le Lu, Zhiyong Lu, and Ronald M. Summers. ""Tienet: Text-image embedding network for common thorax disease classification and reporting in chest x-rays."" In Proceedings of the IEEE conference on computer vision and pattern recognition, pp. 9049-9058. 2018."	The reproducibility is good, as the code is already released.	According to the reproducibility checklist, the source code and pretrained models will be made publicly available, which is essential to guarantee the reproducibility of the results. Additionally, the method was developed using a public benchmark dataset for surgical report generation, which promotes research in the area.	Reproducibility looks good.	How to define the Interaction Representation X^r, i.e. how to extract it from the raw image? Why only use X^r to calculate the relation memory M, instead of including X^v? The more intuitive explanation about why extract 'interaction' can make the graph change from heterogeneous one to homogeneous one? Meanwhile, why graph induced attention can capture the 'local' attention? If the scene graph is first generated by [9], I am wondering whether it shall require the extra annotation information. Is the 'interaction' annotated by the experts, as the original dataset does not contain such information? For the dataset, EndoVis18 actually show limited variety within each sequence, therefore 3 sequences for testing are relatively insufficient. The cross validation is better to be conducted when the dataset is small.	It is recommended to use self-contained tables and figures' captions to transmit the information more clearly.	In general, the proposed approach is novel and interesting, and the experimental results could demonstrate the effectiveness of the proposed method. However, there are some unexplained parts and typos (as listed in the weaknesses).	Interesting method with promising performance achieved	Although the flaws in clarity and organization, the task of surgical report generation is relevant for the medical image analysis community. Additionally, the paper's main contributions correspond to the technical novelty of the model supported in the empirical results that outperform the state-of-the-art. This work could be improved with some modifications, but its contributions support its acceptance.	The proposed method is novel and interesting. The experimental results could demonstrate the effectiveness of the proposed method. The strengths outweigh the weaknesses listed in above.
464-Paper0193	Shape-Aware Weakly/Semi-Supervised Optic Disc and Cup Segmentation with Regional/Marginal Consistency	This paper discusses the segmentation of glaucoma with an hybrid supervision setting, mixing weak labels and no labels. The idea is to regularize the training with regularization that taps into anatomical priors about the problem, such as the shape and diameter relationship of the two circles to be segmented.	The paper proposes a semi-supervised framework for optic disc and cup segmentation via a dual-task level of geometric consistency between pixel-wise segmentation mask with distance map.	The authors proposed a weakly/semisupervised framework with the benefits of geometric associations and specific domain knowledge between pixel-wise segmentation probability map (PM ), geometry-aware modified signed distance function representations (mSDF), and local boundary region of interest characteristics (B-ROI ). Experiments on six large-scale datasets demonstrated that their method outperform state-of-the-art semi-supervised approaches for segmentation of the optic disc and optic cup, and estimation of vCDR for glaucoma assessment in color fundus images, respectively.	The paper is overall well written and well motivated, and the topic is of interest. The dual prediction and regularization is interesting.	The paper proposes a dual task which injects geometric consistency between the pixel-wise segmentation and distance map; It proposes a differentiable estimation layer to predict the vCDR directly without offline post-processing.	The research is of practical significance and can solve the problem of less labeling in medical image segmentation. In addition, the manuscript is well organized and fluent in language.	Adding a clear example of the weak labels would help the reader to understand exactly the supervision setting. Though the handling of the multi-classes is not clear and hamper the comprehension of the paper	The motivation and the contribution of the proposed mSDF is not clear. In the previous work, e.g., [15], SDF is adopt to the represent the target mask. In this paper, it proposes a modified mSDF. What's the difference between the proposed mSDF with the previous work? And what's the advantage of the proposed mSDF compared to the previous work? The experimental setting is not clear. Did it train a single model using both the labeled SEG dataset and unlabeled UKBB dataset or train separate models? The way to utilize the labeled images and unlabeled images is not clear. It mentioned that in one batch, it has both labeled images and unlabeled images. Does it have the same training loss for both the labeled and unlabeled images?	I don't think the manuscript has any major weaknesses.	The code is already shared and the data downloadable. The documentation seems enough to be able to re-run their experiments.	Meet the requirement.	I think the method can be replicated, and the author has provided the code.	"How would the method handle multi-class setting? I have troubles to understand the motivation of Eq (2), and I absolutely do not see how it could be generalized to multi-class. Which is a problem here since we have three classes to segment, so I am a bit lost. For equation (6), would it be possible to use ""default"" or ""safe"" values for unlabeled samples?"	See the weaknesses.	It can be added in the abstract that how much the performance of the proposed method is better than that of SOTA. Why is the data size set to approximately 1:2 for the weak supervised set and the test set allocated in the UKBB data set? In each batch of the training data, must the proportion of labeled data and unlabeled data be 1:1? In the experiment, what is lambda in Eq.(9) set to? Why?	Despite the shortcomings (which are fixable in the camera ready), I believe that the paper (both method and its application) has value and is of interest to the community.	Even though the paper brings some new ideas in semi-supervised segmentation and vCDR prediction, the technique contribution is limited.	The research has a positive effect on computer assisted diagnosis, and the method is novel, the results are promising, and the writing is fluent.  Vertical cup to disc ratio (vCDR) is an important observation factor in the diagnosis of glaucoma, the usual method in the computer-aided diagnosis is to segment the optic cup and disc region first and then calculate the vCDR. The optic disc and cup segmentation via deep learning can obtain positive results, however, these deep learning methods need a large size of training data. This paper designed a framework which can train a segmentation model by using small size data of optic disc and cup labels and large size data of vCDR labels. This is an effective weakly/semi-supervised approach and is reasonably interpretable. In my opinion, this study is more enlightening to the field of optic cup optic disc segmentation.
465-Paper1627	Shape-based features of white matter fiber-tracts associated with outcome in Major Depression Disorder	The authors present an association of biomarkers created from shape of white matter fiber tracts with outcome in major depression in a longitudinal study of 63 individuals. The authors compute the shape metrics of the fibers from an MRI taken at start of the study and try to predict the outcome after 6 months, using their biomarkers and other relevant covariates.	The paper is about major depressive disorder and white matter imaging. The methods describe a technique to predict clinical improvement at six months.	The article describes a two levels shape analysis (global and local) of fiber bundles based on the large diffeomorphic deformation metric to discriminate treatment resistant depression.	The paper describes a study, where the authors have defined a proper experiment. The description of methodology is mostly clear and the paper is generally well written. Technical aspects are described, all the way from getting ethical permissions for the study, image acquisition, preprocessing of data, major extraction of relevant features and statistical analysis. The authors furthermore describe their findings.	The method is good, according to accepted standards	Very few studies are focused on tractography as a biomarker for treatment resistant depression.  As we still don't have widely accepted imaging biomarkers for depression, this is study is a relevant contribution to the field.  It is appreciable the cure into the neuroanatomy details of the results.	"The paper has some weaknesses that could be addressed. 1) The paper assumes a lot of technical knowledge from the reader, and could sometimes benefit from being a bit more clear in explaining what exactly is meant by specific words. E.g. in the abstract the authors state ""Shapes are characterised via the deformation of their center line from a centroid shape."" It is not clear at this point what is meant by ""a shape"". A shape could be a path in 3d space, a surface, a volume, this needs to be clarified. If it is a path, then it can be a closed loop, non-closed, or something that branches, this needs to be a bit more clear, since the word shape is very generic. 2) The novelty seems to be in using these shape features for the following analysis. It is not exactly clear what has been done prior. I would also have wanted to see a comparison to some baseline, where other kind of information is extracted from the bundles. 3) The paper could explain the dataset better. E.g. depression is more common in women, is this reflected in the dataset? What was the depression severity of the patients at the start of the experiment? What is the age distribution? This should all be in a supplementary table, along with all the other covariates used for the linear models. Each covariate should also be described. 4) A figure could aid sections 3.1 and 3.2. This could also be used to clearly define what is meant by a shape. It seems to be a path defined by a smooth mapping from the unit interval [0,1] to R^3. This figure should ideally clearly explain the local and global features. 5) The analysis could be improved (see detailed comments). 6) I think it is very strong wording to call the findings a biomarker. 7) The size of the dataset (number of individuals) should be mentioned in the abstract and the introduction."	The voxelsize is anisotropic; 2 x 1 x 1, which causes issues on the tensor model fitting. A specific result was found on the right hemisphere. There was no correction for handedness in the study. In order to be correct, this needs to be added to the model.	The approach seems novel, to the knowledge of the review not many serious concerns. Mostly text improvements and capitalization in the bibliography.	This report is consistent with the paper.	Should be reproducible based on what has been reported. Though, code is missing on how the methods were executed.	They used publicly available tools. However, the dataset is not public, hopefully it will be.  Used parameters are reported but they specific code is not available or not clear whether will be. I was not able to find the the reference foot note 1 to what is referred (the github link).	"Specifics about analysis: There are two major issues with the analysis. These both stem from the fact that this is a relatively small sample with many covariates. You do not have much statistical power to detect any associations, except for very strong effects. There are two things that you do that I believe are inflating the association results, and this is not a standard way to do association analysis. 1) I do not understand the point of adding noise to the bundles in section 3.3, it sounds like an attempt to do some regularization because of the excessive number of covariates compared to the number of samples. This is fine in machine learning and statistics, where you are solving a prediction problem, but this does not seem correct in the setting of doing association, it actually might result in double dipping and inflated association statistics. 2) I understand the point of doing leave one out cross-validation, but it makes more sense for prediction. In this setting, for the each of the 1000 models run, I would have wanted to see two models. One with only standard covariates and the other with the standard covariates and then also the ones specific to the particular bundle (local or global). Then the authors should perform a likelihood ratio test whether the model with the shape data is significantly better than the model that has only the standard covariates. This is a standard approach for this kind of analysis. The results seem to be the adjusted R squared on the case that is left out in cross-validation. This R squared is using the standard covariates with the shape data, so it doesn't reflect only the contribution of the shape data, but also the standard covariates, and is thus very likely inflated and not truly representative of the association the researchers seem to be after. Comments on English language: 1) This sentence in section 3.3 is wrong w.r.t. English language: ""We did not added more landmarks..."". 2) It sounds weird at the start of section 3.3 to say that you ""performed"" two linear models. I would say that you either fit two models, or that you defined two models. Defined sounds more correct here, since you furthermore fit a bunch of times for cross-validation."	Interesting findings in the splenium of the corpus callosum, the right optic radiation and the right thalamo-occipital fascicule. How would this fit with major depressive disorder?	The first sentence is a bit misleading. Depression is a wide spectrum of mental illness, some minor, some mild and some dangerous. Please specify that you are referring to major depression, or treatment resistance earlier. Riemannian is upper case Riemannian Diffusion image processing: please report which tools you used.  Capitalization in bibliography: 7-t in bibliography should be 7-T or 7T, mri should be MRI, etc	This paper is generally very good, but I am not convinced by the approach used for doing the association analysis. If this is fixed, then I think this should be accepted, but I am concerned that this might be too much to ask for at this stage.	The clinical application is interesting. Not only a novel technical application, but also working towards a biomarker.	I have not been able to find critical issues.
466-Paper0537	ShapePU: A New PU Learning Framework Regularized by Global Consistency for Scribble Supervised Cardiac Segmentation	The authors propose a weakly supervised segmentation approach utilizing scribble-guided annotation, a positive-unlabeled learning framework and shape consistency regularization. The proportion of each segmentation class in the unlabeled pixels is estimated via EM. The approach is evaluated on two open data sets and reportedly outperforms other supervised as well as weakly supervised approaches.	Authors propose a new scribbled method for cardiac segmentation with weak supervision based on the positive unlabeled framework and shape regularization, that penalizes inconsistent segmentation results. Their method makes use of an Expectation-Maximization (EM) algorithm to estimate the proportion of each class in the unlabeled pixels.	In this work, authors have proposed a weakly supervised framework where they use scribbles to train the method for segmenting LV, RV and MYO on cardiac MR images. The method involves an EM algorithm for estimating the mixture proportion, PU learning to identify the classes of unlabelled pixels by maximising marginal probability and by leveraging the consistency of features across shapes by cutting out specific regions and finding cutout equivalence. They also perform comparative analysis with prior methods on two different datasets.	* Novel ways to ease the time and labor-intensive annotation work are very important especially in the medical domain, to allow doctors to focus their time directly on patients. * The paper introduces a novel multi-class PU learning framework with a interesting integration of shape information in the loss function * Results are demonstrated on publicly available data sets, are accompanied by an ablation study and are compared to different levels of contender methods with superior performance (even on par with fully supervised approach)	They propose a new learning framework that can use unlabeled data to use the Expectation-Maximization (EM) algorithm to estimate the proportion of each class in the unlabeled pixels. Their background is strong and well posed. And their experiments demonstrate advantage compared against other methods.	Clearly written introduction and motivation Novelty of the method: Using a probabilistic PU learning framework combined with shape/texture information for weak supervision is promising for this application. It would be interesting to see if introducing weighted version of PU loss would be more helpful (as a future work). Evaluation on multiple datasets: The evaluation is done on multiple datasets and the results look good. While the main statistically significant improvement seem to be with respect to the UnetF method.	I'm missing the quantification of the time savings compared to fully supervised methods, which is a major motivation for this work. Is the tradeoff annotation time/accuracy arguable? In the end doctors want the most accurate model, even if this means tedious annotation work.	It is lacking a clinical test, to show real robustness in any case.	Errors in the equations: There seem to be errors in the condition used for getting the main equation (1) in the paper: sum_{j=1}^m pl(cj x) = 1 should be replaced by  sum_ {j=1}^m pu(cj x) = 1 for the equation (1) to be valid. Also check equations 3 and 5 in the supplementary material (numerator and denominator seems exchanged and the pu is replaced by pl). Also, given that we do not know what classes the unlabelled pixels belong to, the authors should explain how the assumption for eqn (1) can hold good.  Also, in the loss function authors determine the positive and unlabelled pixels as \omega and \omega_bar, but they haven't used unlabelled voxels in the loss equation? They seem to have calculated the marginal probability only for the positive pixels? Can they explain why? Lack of details regarding differences between datasets: While it is good that authors evaluated on multiple datasets, more information should be provided on how they different and what measures were taken to reduce domain shift? is ACDC also post-gad? how is it handled (any normalisation done)? differences due to pathological conditions (eg. myocardiopathy)?	The work was evaluated on a publicly available dataset. Following the checklist, I assume that the code will be available after acceptance.	The paper can be reproduced and data can be collected to compare against new algorithms	The implementation details are mentioned and the authors have used a standard Unet model for this framework. A few more details such as how long it took and how many iteration for EM (was the EM algorithm performed for each epoch?) would be helpful.	"* As stated above my major concerns are about the quantification of the time savings possible using the proposed approach. Is the time/accuracy trade of arguable?  * Besides of being a steady standard for medical images segmentation, is there a reason to stick with U-Net? There are several modifications and/or alternatives published with demonstrated superior segmentation performance (e.g. nnUnet).  * How do you explain the drop in performance when combining cutout and pu without including shape in your ablation study? Please add a short discussion on this to the respective section. Minor: * There seems to be a typo here: ""We randomly divided the 45 images into 25 training images, 5 validation images, and 20 test images."" Either you used 50 images or a different train/val/test split. * Table 1: I assume the significance is always given in comparison to the previous model? A clarification in the caption or text would be appreciated. * Figure 3: The images are pretty small. If there is enough space, larger images would be appreciated * Table 2: I assume HD is HAUSSDORFF DISTANCE. Please add it to the caption.  * Is there a reason for not highlighting statistically significant differences in Table 2 and 3? For consistency I'd suggest adding these indications here too. * Figure 4: It would be interesting to also in include the corresponding scribble annotations"	Authors provide with enough insights of their methods and a good number of experiments with comparison against other methods. Perhaps when they open their code they will be completely understood. An explainability analysis is documented so, the reader can understand the advantages of their methods	With reference to my above comments regarding weaknesses, here are my comments: Please clarify the equations for the EM section and the PU loss section. Why should it be shape and not texture? When the cut-out is performed the model could perceive it as change in the shape or in texture since the texture information is missing in the cut-out region - hence the term 'shape' is quite misleading. Also, how helpful is cutout? Would be helpful to see the results in extreme cases? e.g. distortion or non-circular MYO? Provide more details regarding the shift in characteristics between the two datasets and a brief disc on how they were reduced. Samples from the datasets could be included in the suppl. material. If possible, it would be good to see the cases where the method fails or gives lower performance. The results report greater improvement in HD values (btw, average or 95th percentile of HD?) than dice - why is this? was the boundary smooth with the proposed method that led to lower HD? Authors should discuss the reason behind this given that the improvement in HD is much better than Dice).	A novel approach in reducing the annotation workload, a sound presentation of results as well as ablation studiy and comparison to other sota methods.	Authors explain their paper, show all the experiments and work done, and compare and quantify themselves against other existent methods. Reproducibility is important if the authors provide the code.	The method proposed in the paper has potential but is weighed down by the lack of clarity in the equations and assumptions. Nevertheless the results look promising and seem to work on multiple dataset outperforming existing methods.
467-Paper2725	Show, Attend and Detect: Towards Fine-grained Assessment of Abdominal Aortic Calcification on Vertebral Fracture Assessment Scans	This paper presents a framework for automated assessment of AAC (Abdominal Aortic Calcification).  There are a few methods which predict an overall AAC-24 score, however they have some shortcomings. To address them, an effective framework is proposed to generate fine-grained scores of images in a sequential manner. It utilizes an attention based encoder-decoder network to mimic  the human-like AAC-24 scoring method. According to the authors, this is the first time such a methodology is used to address the AAC-24 scoring problem.	The authors present a model for automatically assessing aortic calcification on DXA scans using an LSTM with attention. Their model has the benefit of providing individual scores for regions of the aorta, which is potentially useful for diagnosis as well as understanding model output. They compared against their implementation of a previous model and showed higher performance metrics.	In this paper, a framework for automated fine scoring of Abdominal Aortic Calcification using L1-L4 vertebral X-ray scans is described. The authors propose the use of a convolutional encoder to obtain a latent representation of the scans, and then train an attention mechanism to focus separately on the anterior and posterior segments of the abdominal aorta. The resulting output provides a fine-grained scoring, based on the AAC-24 scale, which is an improvement over previous efforts that only provided a global score for a given X-ray scan.	Cardiovascular Disease (CVD) is the main cause of death globally, and it contributes to disabilities significantly, too. When calcium deposits in arteries, Vascular calcification happens, resulting in heart attacks or strokes. The abdominal aorta is one of the first vascular beds where calcification is seen. Since AAC happens well before clinical events, there is a chance to identify people at risk and intervene in a timely manner before they suffer cardiovascular events. The main strength of this paper is the usage of an attention based encoder-decoder network to mimic the hAAC-24 scoring. Compared to the 3 previous works found in literature, the proposed methodology outperforms them. Further, it can classify patients into the three risk categories (low, medium and high), with certain levels of accuracy, sensitivity, and specificity. The positive aspect of this work is that it has achieved satisfactory outcomes for a small dataset of 1,916 scan. If a considerably larger dataset (with labels) can be found, the results could be justified better.	The approach of using an LSTM to treat the problem as a sequence is interesting and generally follows clinical practice. Having score breakdowns can be a major advantage for model explainability, which is necessary for eventual clinical implementation.	Improvement over previous methods, which only provide a global AAC-24 scoring classification per exam/image Very thorough explanation of the clinical significance of the research	Nothing to be specific as weaknesses, however, some strategies to achieve better classification outcomes with some more enhanced results for the evaluation metrics (accuracy, sensitivity, and specificity) could have been suggested. It says that the AAC-24 scores are highly correlated with expert assessments with an accuracy above 80%. How can this be improved, too?	One significant weakness is that the results presented here are significantly lower than the previous publication of the state-of-the-art model they are training against, despite using the same model. The authors suggest that the previous publication may not have performed an appropriate evaluation, but the explanation here is lacking. It would be helpful to show a replication of the previous results and clearly demonstrate why those results are not a realistic representation of the model's performance. The authors also weaken the justification of their sequential approach (the main innovation in their model) when they state that the individual scores are rigid and random. It seems that the authors were just trying to contrast their problem with language since, in later descriptions, it sounds like their is structure in the sequences, so it does seem like there is value in this approach. It would just be good to be descriptive of how the structure of these scores can be modeled using a sequential approach and to stay consistent. Finally, there are several mistakes and inconsistencies in the results that make things a bit hard to follow or appreciate.	The paper needs to be proofread again, as there are a lot of typos and mistakes Results validation could be improved and the reported results are not very convincing	None	The evaluation uses a public dataset and the model is clearly described.	The authors will publish the trained model and their code upon acceptance. However, some details should be disclosed in the paper, such as the number of training epochs. Also, a citation was provided where the same dataset was used, which does not appear to be published.	A good piece of work which has some future work left. Applying the proposed algorithm on a larger dataset. Testing another option rather than using a pre-trained network. Explore how the difference between expert assessment and the outcome of the proposed method can be reduced.	"First, I'll elaborate on my first point in the weaknesses. The authors note the reported results in the previous publication, but write them off by saying they ""report their best results."" It is unclear what is meant by this statement; in most cases, we want to present our best results in a publication. If the authors are suggesting that the previous paper might have cherrypicked a beneficial test/train split or random seed, then it would be helpful for the authors to replicate this result (e.g., demonstrate the range of possible results using the previous publications methodology and show that the reported value is the top of the range). This would demonstrate the limitations in the previous paper and show that the baseline model is implemented correctly. I'll also list the issues/errors I have found with the evaluation here. The metrics reported in table 2 are computed one versus rest and then averaged, which can be deceptive in a three-class problem. The accuracies here are substantially lower than the actual three-class accuracy, which can be directly calculated from the confusion matrices in figure 3 (about 73% for the proposed model and 56% for the baseline). There aren't any confidence intervals or statistical tests showing the stability of the results or the significance of and differences. The second example AAC24 score in figure 1 is calculated incorrectly. The value should be 2, not 0. The confusion matrices in figure 3 are transposed. As displayed, the counts for the ground truths do not match the breakdown in the body. The metrics for the proposed model in table 2 do not match the confusion matrix in figure 3. Most are close, but do not match exactly (for example, the Moderate PPV should be 37.5 not 40)."	"Section 4: A few important details of the implementation were not explained, which hinders a full comprehension of the model and of the obtained results. For example:     - how many training epochs were performed?     - what was the threshold correlation value used in early stopping?     - which layers used dropout regularization and what was the dropout alpha? Section 4.2: Data augmentation transformations should be applied carefully to medical images. Shear transformations are not always desirable, as they may distort spatial relations between anatomical structures. Also, the authors should clarify further the data augmentation ranges, e.g., ""[-10, 10] pixels"" What was the reasoning behind the choice of ResNet152v2? Did the authors test other backbone architectures? The results analysis could be improved with a comparison of different CNN encoders. The paper need to be thoroughly proofread. Some minor issues include:   - page 1, line 16: add a comma after ""out of these""   - page 2: define AAC-24 before in the text (e.g., after ""Kaaupila 24-point scoring method)"". There is a definition later on section 2.1.   - section 2.1: no need for ""rd"" after 1/3 and 2/3. Also, if the authors find the space for it, please rewrite the third phrase in 2.1, using ""more than""/""less than"".   - section 2.1, 2nd paragraph: repeated ""the"" after ""Furthermore,""   - In Fig.1b, there seems to be a typo. There is no calcification (as described in the text), but the left column of AAC-24 reports a 2 for L3 Ant, and the score is 0.   - section 2.2: after [3] -> ""followed"" and consider removing ""as [4]""   - section 2.2, line 4: add comma after ""as a whole""   - ROI acronym is not defined. Also, it can be used in section 4.2   - near the end of section 2.2: ""scores AAC-24"" -> ""AAC-24 scores""      - section 3, line 3: ""prepossessed"" -> ""pre-processed""      - section 3, line 6: ""conventional"" -> ""convolutional""      - section 4, line 1: remove ""of"" after ""comprises""      - section 4.3, line 17: start sentence with ""In each fold,""      - section 4.4, line 2: ""as compared to"" -> "", instead of a""      - section 4.4, line 4: ""sum up all"" -> ""sum of all"" / remove comma after ""Since""      - section 4.4, line 7: ""comprises of"" -> ""consists of""      - section 4.4, line 15: remove ""one each""      - ""vertebra"" is the single form, ""vertebrae"" is the plural form"	This paper has explored a proposal on how automated assessment of AAC (Abdominal Aortic Calcification) could be done.  The usage of an attention based encoder-decoder network is something novel when compared to the 3 related works highlighted in the paper. Further, it assists severity classification as well. Tough the accuracy is not that high, the idea has the potential for future expansion.	I think the way the problem is framed is interesting and producing individual scores can provide clinical value. The limitations in the evaluation as well as the discordance with the publication for the baseline model lower my enthusiasm.	The proposed method is quite interesting, with a fair degree of novelty, and its application purpose is of great importance. However, some of the presented results are not convincing enough (e.g., correlations with human scores in table 3 are very low). Moreover, result validation could be a bit more extensive and the paper suffers from a lack of clarity at some points.
468-Paper0060	Siamese Encoder-based Spatial-Temporal Mixer for Growth Trend Prediction of Lung Nodules on CT Scans	Main contributions are:  (1) Authors derived the prediction of lung nodule evolution by organizing a new temporal CT dataset called NLSTt by combing automatic annotation and manual review. (2) Authors proposed a spatial-temporal mixer (STM) to leverage both temporal and spatial information involved in the global and lesion features generated from 3D ROI pairs. (3) Authors conducted extensive experiments on the NLSTt dataset to evaluate the performance of their proposed method and confirmed the effectiveness of their model on an in-house dataset from the perspective of clinical practice.	To advance the prediction of lung nodule evolution, the authors construct a new temporal CT dataset NLSTt, and propose a spatial-temporal mixer (STM) to extract the temporal and spatial information involved in the global and lesion features of CT images information.	This paper focuses on the prediction of lung nodule growth trend in CT scans.  Authors organized a new temporal CT dataset from NLST dataset and proposed a growth trend prediction framework including a Siamese encoder, a spatial-temporal mixer and a hierarchical loss. Authors validated the framework on the organized dataset as well as on an in-house dataset to demonstrate its feasibility.	Authors proposed a Siamese Encoder-based Spatial-Temporal Mixer for Growth Trend Prediction of Lung Nodules on CT scans. Their method can help predict the growth of the nodule, to facilitate the diagnosis and treatment of theses nodules, which is a very targeted task in now a days medical field application. The framework is simple, though authors succeeded to achieve reasonable classification results. This work can serve as a starting point of formulating a much complex problem as: whether a treatment is efficient or no in the treatment of lung cancer or whether a nodule will grow into a malignant nodule.	The authors of this paper construct a CT dataset NLSTt for predicting the evolution of lung nodules based on the NLST dataset, which combines automatic inference and manual review to obtain reliable labels for nodule texture types and evolution classes. In our proposed method for pulmonary nodule growth trend prediction, a s spatial-temporal mixer (STM) module is proposed to exploit both spatial and temporal information. A two-layer H-loss is also proposed to pay more attention to the nodule growth (dilatation class) related to possible cancerogenesis in clinical practice.	Clinical feasibility: The test with in-house datasets demonstrates the feasibility of the proposed framework.Extendibility: this work can be extended in lesion trend prediction in other diseases with time series images, such as cervical cancer lesion trend in colposcopy.	It is better to add some examples of images where the algorithm fails / succeed to classify correctly the output, a lake of images make the quality of the discussion section poor. The mathematical formulation of the loss functions and model in general is poor. It would improve the quality of the paper to improve the mathematical formulation. The discussion section is poor, and authors did not specify why their algorithm classify correctly or misclassify samples. The proposed method is not innovative, and components of the framework have been used before for similar tasks. This paper would have been more suitable for miccai if a more innovative approach was used.	The characteristics of pulmonary nodules considered in this paper are only diameter and texture, and do not utilize other sign information and clinical information of each patient. Some important formulas are not listed in the Methods section. There are few methods compared in the experimental part, and the results of the comparison experiments are not visualized. A description of the limitations of the method and future work is missing.	"Limited comparison to state-of-the-art: 1) In the subsection of NLSTt dataset acquisition, it would be better if authors could explain clearly how they pair nodules whose locations change a lot between two Ti and T0, and what is the accuracy of pairing correctly nodules. 2)Authors compared between the CNN and ViT encoder, as well as between different mixers, whereas the proposed method is not compared with existing methods, such as [17,19] and [R1,R2]. It would be more interesting and convinced to compare between such methods. [R1]. Rafael-Palou, X., Aubanell, A., Bonavita, I., Ceresa, M., Piella, G., Ribas, V.,Ballester, M. A. G.: Re-identification and growth detection of pulmonary nodules without image registration using 3d siamese neural networks. Medical Image Analysis, 67, 101823 (2021) [R2] Tao, Guangyu et al. ""Prediction of future imagery of lung nodule as growth modeling with follow-up computed tomography scans using deep learning: a retrospective cohort study."" Translational lung cancer research vol. 11,2 (2022): 250-262. doi:10.21037/tlcr-22-59"	reproducible	This paper does not specify the software framework and version used, the description of the model is not clear enough, the experimental code and other experimental details are not provided, and the reproducibility of the paper is poor.	Good reproducibility: Authors provided clear description about the pipeline of their methods, clear description about the datasets they used for experiments, as well as the parameters and environments used for training models.	8- It is better to add some examples of images where the algorithm fails / succeed to classify correctly the output, a lake of images makes the quality of the discussion section poor. Illustrations improve the quality of the paper. The discussion section is poor, and it would add to the quality of the paper to study more the strength/weaknesses of such method. The framework is simple and seemed to give good results though the components are not innovative and have been used before which make this paper more suitable for other conferences rather than miccai.	It is suggested that other signs of pulmonary nodules and clinical information of the case be used in this paper, which is closer to the doctor's clinical diagnosis process. In the experimental part, more comparative experiments and ablation experiments should be done to show the superiority and effectiveness of the method more comprehensively and accurately.	Lack of clarity: Better explanation of the figures: it would be helpful if authors could make the figure captions, especially Fig.3, more clearly. Abbreviations in the figure should be explained in the caption, for example, MLP, FG1,etc. For future work, I would recommend to apply the proposed framework on more diseases and more image modalities, such as lesion variation in cine MRI, cervical cancer lesion variation in time serie colposcopy, etc.	The idea is of the paper interesting and very useful in for the medical field. The prediction of the growth of lung nodules can assist in multiple tasks like future treatment orientation or overall survival estimation of patients. It would be good to show some examples where the algorithm failed and explain why The components of the framework are not innovative and have been used before.	The CT dataset NLSTt constructed in this paper is a good work to predict the evolution of lung nodules, and it is a good idea to construct a spatial-temporal mixer (STM) module to effectively utilize the spatial and temporal information of the ROI. However, the detailed description of the method is not clear enough, and the experimental part is not perfect.	Authors proposed a novel framework for nodule trend prediction but comparisons with the state-art-the-art is limited. I would suggest accepting it.
469-Paper1539	Simultaneous Bone and Shadow Segmentation Network using Task Correspondence Consistency	This paper presents a incremental innovation for bone segmentation from US images. The main contribution is the cross task transfer block that improve the performance of the overall segmentation for both bone and shadow.	Multi-task segmentation with the use of a new cross-task feature transfer block Proposed a task correspondence consistency loss to improve multi-task learning Validation on images of different anatomies and different ultrasound scanners.	Bone segmention from US images, taking into account the bone surface and shaow artiffact. The study is presente in the context of image-guided orthopaedic intervention, but it really is yet another segmentation paper.	This paper is well-written, clear and follow a proper methodology. As far as I can understand, the implementation is consistent and the results good after a proper comparison with previous algorithms.	Novel framework for multi-task learning Thorough validation and ablation studies	The method presented explicitly considers bone surface and shadow artifact, in the segmenation of bone structure in US images. The study involved IRB-cerfitified data collection, plus three additional virgin data not included in the network development. The wriging is clear and purposefully highlights the key items for sucessfull MICCAI submission.	Unfortunately, with my experience I cannot evaluate the novelty of this work in terms of programming. The CTFT looks for me similar to a GAN approach but a bit different. The rest of the proposed method is based on well-known previous studies.	Small number of subjects Ground truths segmentations for curvilinear probes appear problematic	The method is argued to be inteded for CAOS, but not specific condition, and strategy for clinical implementation is given. The subjects are all healthy subjects, not particulary represeting the image features of US images taken in CAOS procedures.	No comments.	Network architecture and training information are clear and helpful for reproducibility.	OK. NO particular comment.	No additional recommendations. US segmentation is a challenging task and this proposal will increase (not so much, but at least a bit) the current performance.	"The number of subjects is small. This can result in low anatomical variability, and it is unclear if the subjects are healthy controls or patients. It is not clear how Section 2.1 is related to the segmentation ground truth. For the curvilinear probe, the shadows should not be in the vertical direction parallel to the length of the image. Instead, they should follow a fan-shape along the transmission direction of the ultrasound beams. This can affect the validity of the trained network. Some discussion regarding the requirement of accuracy vs. the impact of the application will be appreciated. The term ""mutual information"" used in the article is not appropriate since it does not refer to the more commonly used information-theory-based metric. Figure 5: (d) and (e) are not consistent between the first and second rows regarding the surface and shadow segmentation. It will be good to mark which anatomical structures are shown in the figures. For the Ablation study in Table 2 and 3, it is unclear if the improvement with CTFT is statistically significant."	The paper can be improved by collecting data from disease subject.	Please, keep my recommendation under the others reviewers comments regarding to novelty. The paper is clear and the methodology consistent. An overall (small) improvement on the results are presented. All these together compile a good paper.	While the framework is novel and interesting. The problematic ground truths can affect the validity of the work.	The clarify of the paper is outstandig.
470-Paper0924	Skin Lesion Recognition with Class-Hierarchy Regularized Hyperbolic Embeddings	In this paper, a method for skin lesions embedding and classification based on hierarchical class relations encoding and hiperbolic embedding is presented. The two main contribuions are: They use hyperbolic geometry, instead of Euclidean, for the image embedding.  . They incorporate to the loss a distance based on the hiearchical relations between classes.	This work proposes to use class hierarchy with deep learning algorithms for more accurate and reliable skin lesion recognition. A hyperbolic network is proposed to learn image embeddings and class prototypes. Validation was carried out on an in-house skin lesion database which consisted of ~230k dermoscopic images on 65 skin diseases.	This paper proposes a skin lesion classification approach that leverages a  hyperbolic embedding in a class hierarchy-aware setting. The authors' goal is to increase classification performance on the task by projecting the deep features to a hyperbolic geometric embedding and performing classification on it. Their method also models and enforces the problem's class hierarchy by incorporating a novel loss into the training process.	The paper strengths of the paper are: 1) The introduction of hieralchical relations between classes and the introduction of this informataion in the loss function. 2) The use of hyperbolic geometry for image embedding, more suitable for hierarchical relations then Euclidean according to [5] and [11] 3) They include an ablation study in which they evaluate the improvements of each of the proposed contributions. 4) They include two evaluations metrics of great interest to analyse the contribution that they propose: mistake severity and hierarchical distance of he top k (HD-k).	Elegant problem formulation. Clear theoretical background introduction. Thorough literature review on related work. Extensive validation experiments.	The paper has novel contributions because it incorporates recent developments in hyperbolic embedding learning. It also presents a novel loss function that uses the class hierarchy of the underlying problem in their architecture. The paper is technically sound, mathematically well-funded, and enjoyable to read.	The weaknesses are reduced: 1) I consider questionable the data augmentation that they employ. In an application where color is essential for the diagnosis, it does not seem adequate to incorporate color transformations in the augmentation. 2)  An explanation of the dynamic range and justification of the values of the different metrics in Table 1 would be desiderable. For example, they do not justify that 50% accuracy is reasonable for 65 classes or the maximum possible value of the mistake severity  or the maximum possible value of the mistake severity or if 1.X is a good value for HD-k	The logic of the hyperbolic hierarchy structure of the skin class labels is debatable. What specific values does such formulation brings? In hyperbolic space, the distance becomes longer close to the boundary areas. However, the final classes, i.e., the leaf nodes, are all there. The reviewer does not fully understand the full motivation. Why are the distances between two nodes that share the same parent nodes are formulated with the same distance between two nodes that do not share the same parent nodes? It is exciting to see a dataset with ~230k images from 65 disease subtypes. A natural question is whether the image numbers in each class are balanced. What are the maximum image numbers and minimum images numbers for a disease type? Any performance evaluation methods to address them? The performance improvement was relatively incremental. The training details are not provided in Appendix.	Despite the interesting formulation of the method described in the paper, the results (mainly table 1 ) look relatively closer to the baselines. The performance improvements look marginal compared to the rest of included methods. In particular, it does not look like the hyperbolic approach contributes much to the performance as it does the hierarchy aware variants.	They scarcely describe the database. They mention the number of images but they do not mention how many images of each class or if it is balanced. They cited a previous work of the authors, perhaps in this reference the database is better described. They do not provide implementation details. They state ithat it  is included in an Appendix, but in the available appendix only a figure with the structure of the skin taxonomy is contained.	Excellent.	Upon release of the dataset employed in the study, the paper contains the necessary information to reproduce the paper's results. However, I encourage the authors to share more details about the classification network and clarify whether multiple layers were used and if there is any type of batch normalization or non-linearity.	In this paper, a method for skin lesions embedding and classification based on hierarchical class relations encodng and  hiperbolic embedding is presented. The two main contribuions are: They use hyperbolic geometry, instead of Euclidean, for the image embedding.  . They incorporate to class distance a distance based on the hierarchical relations between classes. Five comments that may improve the paper: 1) I consider questionable the data augmentation that they employ. In an application where color is essential for the diagnosis, it does not seem adequate to incorporate color transformations in the augmentation. 2)  An explanation of the dynamic range and justification of the values of the different metrics in Table 1 would be desiderable. For example, they do not justify that 50% accuracy is reasonable for 65 classes or the maximum possible value of the mistake severity or if 1.X is a good value for HD-k 3) It would be interesting to highlight in the text the main contributions of the paper with respect to the literature. 4) Perhaps the theoretical explation 2.1 could be substituted by more details in the method. 5) Figure 4 is impossible to visualize.	It would be nice if a concrete contribute list is provided in the paper.	"The introduction succeeds at stating the problem and the related work. It is also well-motivated and clarifies the paper's contributions. It would be helpful to see the standard deviation on all the values in table 1 to evaluate the proposed method's performance further. The authors mention more implementation details in the Appendix, but they seem to be missing. Can the authors further clarify the default ""0"" parameter used in Eq. 3. It seems with 0, Eq. 2 simplifies a lot, and it is not clear how this affects, for instance, the expressivity of the network. I believe there is a typo on the abstract on the word ""provably."" It seems that the authors meant ""probably."""	The proposed research contributes to solve an important medical problem. It also include methodological contributions.	It is an inspiring work with solid clinical importance. It will promote the society to pay attention to advanced geometry research.	My decision is primarly based on the marginal increments in performance of the proposed method with the baselines in table 1. I would be willing to change my rating if more evidence of the benefits of the proposed approach is shown.
471-Paper1806	SLAM-TKA: Real-time Intra-operative Measurement of Tibial Resection Plane in Conventional Total Knee Arthroplasty	The authors propose a method to evaluate the 3D alignment of the tibia resection block during a total knee arthroplasty procedure based on a series of intraoperative fluoroscopy images and a preoperative CT scan of the tibia. The author(s) tested the proposed method in a simulation and clinical pilot study.	This paper presents a real-time algorithm for reliably and intraoperatively estimating the tibial resection plane for CON-TKAs. It solves a SLAM problem using a patient-specific pre-operative tibia CT scans, a trocar pin mesh model and two intra-operative X-ray images. Simulation experiments demonstrate the robustness and accuracy of their proposed algorithm.	This paper proposed a SLAM based approach to accurately estimate the proximal tibial resection plane intra-operatively, using 2 X-ray radiographs, 3D tibia mesh model, and trocar pin 3D mesh model. Simulation and in-vivo experiments demonstrated its good performance.	According to the reported results, the proposed method outperforms previous methods with regards to robustness and accuracy, while ensuring a short runtime.	The authors novelly formulate intraoperative tibial resection plane estimation as a SLAM problem, and their simulation results show the possibility to apply their method in the clinical settings.	Clinical significance was well illustrated. The authors clearly showed the impact of using this technique on TKA. The problem has been re-modeled mathematically to be solved by SLAM. The definition of different data term were well formulated. Validation on simulation and in-vivo experiments showed promising results.	I'm doubtful about the clinical applicability of the proposed method. Although the authors tested the clinical feasibility of the method in a pilot clinical study, in my opinion, the advantage of using such system in clinical practice was not sufficiently discussed.	There is no clinical evaluation in the current study.	Proof-reading is required.	Author(s) response to reproducibility indicate that data and code will be shared, which makes this paper full reproducible.	The implementation details have been well addressed.	The reproducibility depends on whether the code is to be open-sourced. Otherwise, making a SLAM pipeline working takes a lot of efforts in fine-tuning. It is good that the authors promise to share the code after acceptance.	"Firstly, I would like to comment that the accuracy of the results you have presented in the manuscript is impressive. However, I have a few questions regarding the method used to create these results: From my understanding, the algorithm requires as input an initial state, comprising of an estimated C-Arm pose for each fluoroscopy image, and initial 3D pin poses. According to equation (3) the state also contains a set of 3D back-projection points for each contour edge point and each fluoroscopy image. I'm unsure how these 3D points are initialized? As mentioned in equation (1) and (2) the relationship between 2d contour points and 3D back-projected point is only proportional. Would the algorithm not also require the tibia and pin contour points from the fluoroscopy images? I believe in figure 3, these are named contour observations? If so, how are these contour points determined? How would the accuracy of the contour detection influence the overall results? In your simulation experiment (section 4.1) what was the resolution of the fluoroscopy images? The differences in your results compared to the other methods are noteworthy. Could you discuss your results and why you think that your method outperforms the comparative methods? The colors-dots in the legends of the images in Fig 3 are too small to identify According to the supplemental material the tests seem to be performed all with N=2 (number of fluoroscopy images). This information should be added to the manuscript. Also, how would change in N effect the results, i.e. N=1, N=10? Despite the good results, I have some hesitation about the clinical feasibility and applicability of the proposed method. For example, the following sentence in the manuscript ""... the tibial resection plane estimation can be completed without much influence on the CON-TKA procedure."" gives the impression that the method can be applied at the cost of the runtime of the optimization. To the best of my knowledge, fluoroscopy imaging is not standard of care during a conventional TKA procedure in many clinics. Therefore, setup and obtaining the images should be considered part of the proposed method and clinical feasibility and applicability should be considered including these extra steps and additional radiation exposure.  In the introduction you mentioned that your proposed method can be used as replacement of systems which are actively navigating the tibia resection block and/or tibia resection. Arguments against the computer-assisted navigation system includes: extra procedure steps, time, complexity and instruments. Although I agree that these are well established and published disadvantages of many computer-assisted TKA systems, I believe it would need to be evaluated if your proposed method can in fact eliminate these concerns. As mentioned above, procedure time and complexity might be influences by the use of fluoroscopy. Furthermore, while computer-assisted system are designed as a ""one-step"" navigation, your proposed method would need a repeat alignment of the tibial block when an error is detected. Lastly, you mentioned also that no significant improvement of mid-to-long term functional outcome was detected using navigation. However, in the publication you refer to, the authors did measure a significant improvement of tibial component alignment compared to conventional method. Your method also aims to improve the tibial component alignment. I would therefore be interested to get your thoughts on if and how your system might have the potential to overcome this mid-to-long term problem."	"The authors present an important method for improving the CON-TKAs by formulating the cutting plane estimation method as a SLAM method, and demonstrate the effectiveness of their method by the in-vivo experiments. Some suggestions are give below. The authors should add the clinical evaluation or add the corresponding discussion in the manuscript, and should carefully proofread their manuscript to correct the existing typos, e.g., ""gold standar""."	"More proofreading is needed to correct the typos, e.g. ""gold standar(d)"". Enlarge the legend font size in Fig.3"	the remaining open questions to the methods make it difficult for me to fully assess the merit of the proposed method	The formulation method is interesting and critical for improving the intra-operative TKA surgeries.	The paper is very well written. I also like the elegant solution: SLAM is not novel but the application of SLAM on TKA seems novel and the paper demonstrated promising accuracy and robustness in terms of resection plane estimation (<3 degree).
472-Paper2190	SMESwin Unet: Merging CNN and Transformer for Medical Image Segmentation	The authors proposed an image segmentation method that combines CNN with Transformer. As part of the network architecture, they introduced superpixel to reduce redundancy and noise in the images. It is fed into CNN to generate features of input images that are later combined with multi-scale features from Transformer. The proposed method is evaluated on two datasets, demonstrating superior performance in comparison to other related models.	his paper presents a new network architecture, SMESwin Unit, that merged CNN and transformer for medical image segmentation. It fuses multi-scale semantic features and attention maps, then introduces superpixels to avoid the interference of meaningless parts of the image, and finally uses external attention to consider the correlations among all data samples. The proposed network achieved better results than CNNs and other Transformer-based architectures on three medical image segmentation datasets.	Authors proposed one deep learning segmentation system based on hybrid swin-transformer and Channel-wise Cross fusion Transformer (CCT). They replaced one skip connection from CCT with one CNN branch processing super-pixeled raw images and name it MCCT. External attention mechanism was deployed to further refine the features. The system was tested in GlaS, MoNuSeg, and WBC for gland, nuclei, and cell segmentation, respectively and achieved promising performance.	The authors proposed an improved design of transformer-based method for image segmentation. In particular, it combines CNN with Transformer with the help of superpixel and a multi-scale fusion module. As the results show, the addition of superpixel aids in improving the performance, which is effective and useful since superpixel is simple yet generic methodology that can be applied to similar problems. The multi-scale fusion module incorporates multiple features from both CNN and Transformer, which would be one of the key aspects of the proposed work since the combination of CNN and Transformer happens in this module.  The authors obtained good results for two datasets and conducted a full ablation study on one dataset. These show the superiority of the proposed work and the effect of different design components of the proposed work.	1: The proposed architecture seems to be general to another segmentation tasks. I think it will work for many applications. 2: The experiments results are enough for the evaluation and give good evidence for the claims of the authors. 3: The paper is also well written; most of the parts are easy to follow.	The manuscript is well-written and well-organized. It is easy to follow and also should be easy to reimplement the system. The system was comprehensively tested in three datasets with different segmentation target.	Although the proposed work shows good results and proposes a new design of a Transformer-based segmentation method, the work is built based upon several pre-existing methods. Hence, there is limited technical novelty. Simple adoption and combination of swin unet and CCT as well as addition of a CNN layer. The authors evaluated the proposed work on three datasets. For MoNuSeg, the results were inferior to UNet++ for both metrics.	1: The technical contribution is the main concern. I feel that it is not certainly enough for a MICCAI paper. 2: I think the usage of the superpixel may hurts the feasibility of the method. If this procedure could be removed, and the performance just degrades slightly, then I will vote for publication. 3: It lacks of the evaluation in the 3D datasets; for a general network, such an evaluation may be required.	As the system was based on CCT, it is kind of a must to compare swin-transformer with CCT in the final performance to prove the CNN branch added is useful. If CNN branch can be proven truly useful, this can have a bigger impact to the research field for better deep learning design. The system is a hybrid of swin-transformer, CCT, and ET. So relatively the work lacks novelty in technique.	likely to be reproducible	Possible to reproduce.	As the codes will be public after acceptance and dataset is already public, the reimplementation and repeat of the results should be possible and easy.	The authors examined their work on three datasets. They obtained the best results for two datasets (GlaS and WBCs). However, the results for MoNuSeg was inferior to UNet++. And, there is no explanation for this result. The authors may provide an extended discussion on their results.  Such difference may arise from the different characteristics of the datasets. All three are similar in a sense that the method needs to segment objects that are circular or elliptical, in general. The ones in MoNuSeg may be the smallest. This may affect the performance of the method. If so, it indicates that the method has difficulty in dealing with small objects. The authors may investigate their results from this perspective.  Also, the comparative methods are not the state-of-the-art methods for the three datasets. The authors may compare their method to the current SOTA methods for these datasets. For these datasets, DICE and mIoU may not be the optimal choice for evaluation metric. PQ would be an alternative, in particular for nuclei segmentation, which has been widely used these days.	1: I feel that there are too much text used for the superpixels generation. Current version has not give enough arguments and motivations for using it. 2: In MCCT, it is a bit mathematically intensive; providing more conceptual description would be helpful for the readers to understand it. 3: Pointing out the limitation and future directions for the further development of the method could help some readers sometimes.	As the T1, T2, T3, T4 all have different number of channels and number of patches/tokens, how are they concatenated together? In section 2.3, please double check the dimension of all matrices mentioned. The Ti should be in the shape of dCi; if W_Qi is in the shape of Cid, the shape of Qi should be d*d. Could authors provide more insight illustration on the reason why they think CNN analysis on raw images cannot deal with its 'influence of constructed defect and noise'. In Eq 5, as Fi are in different size, how should they share the same Mk? The CNN generated features are only used as 64/(64+128+256+512)=0.07 of the T_sum in attention mechanism. I am wondering if this can be a significant contributor to the final results. And also, swin transformer with CCT should be compared with MCCT to prove the CNN branch is useful. Also, in original CCT paper, they used four skip connections, while authors of this paper used three and one CNN branch. It is better to provide more experiments and illustrations on this: replace one or add extra one.	Simple combination of the existing methods that limits the technical novelty of the method.  Limited discussion on the experimental results.  No comparison to SOTA on these datasets used here.	I. think that the knowledge contribution may be not enough for a MICCAI paper and the presentation quality is also a concer for publication.	It is one interesting and convincing manuscript. The only main concern for me is in my opinion, experiments of swin transformer with CCT should be included to prove the MCCT, the added CNN branch, is useful as mentioned in point #5.
473-Paper2715	Sparse Interpretation of Graph Convolutional Networks for Multi-Modal Diagnosis of Alzheimer's Disease	This paper presents a (GCN) Graph Neural Network-based approach for classification and detection of Alzheimer's disease from multi-modalities of brain images. The authors propose the sparsity to capture subgraph representations attending on most important features for better discrimination between AD and healthy groups. Ablations and experiments in the paper show that the SGCN method provides better classification than related work when using multi-model data.	In this paper, a sparse interpretable GCN framework (SGCN) for the iden- tification and classification of Alzheimer's disease (AD) is proposed using brain imag- ing data with multiple modalities.	This paper proposed a sparse interpretable GCN framework (SGCN) for the identification and classification of Alzheimer's disease (AD) using brain imaging data with multiple modalities.	Applying Graph convolution networks (GCN) on multi-modal neural-images is an interesting idea as it shows evidence that a single modality of images is not sufficient. Imposing sparsity in GCNs (SGCN) as attention shows that the method is better than the conventional GCN. The experiments show performance metrics higher than related works.	Proposed method applies an attention mechanism with sparsity to identify the most discriminative subgraph structure and important node features for the detection of AD. The model learns the sparse importance probabilities for each node feature and edge with entropy, l1 , and mutual information regularization.	This paper proposed a novel  sparse interpretable GCN framework (SGCN), which applies an attention mechanism with sparsity to identify the most discriminative subgraph structure and important node features for the detection of AD.	"Sparse representation in GCN is not a novel contribution. The SGCN method has been used widely on other computer vision applications, which makes the novelty of the approach moderate. Few examples are: Renjie Wu et al. 2018 ""k3-Sparse Graph Convolutional Networks for Face Recognition"" and Liushuai Shi et al. 2021 ""SGCN:Sparse Graph Convolution Network for Pedestrian Trajectory Prediction"". See also the survey in David Ahmedt-Aristizabal et al. 2021. ""Graph-Based Deep Learning for Medical"" Diagnosis and Analysis: Past, Present and Future Some items of the loss function and mathematical notions are not clear. There is no explanation of the architecture and implementations."	Comparison of proposed method need to be done with existing state of the art methods for multimodal disease diagnosis	Correlation analysis between multimodal data is slightly lacking.	The paper includes analysis of results and performance and the dataset used are cited. However, the paper lacks implementation details for reproducibility.	Publically available data is being used for evaluation.	The ideas and experiments described in this paper are quite clear, should be repeatable.	It is unclear why the loss in  EQ. 3. is called mutual information. In my knowledge, the mutual information describes divergence between two probabilities. EQ. 3 looks more like a conditional probability based on joint attention matrices. Could the authors clarify more this ambiguity? The regularization/cross-entropy loss in EQ. 5 need to be checked. What are the ground truth and predicted variables? Please check the mathematical notation in section 2.2. n and N look referring to the same thing. The authors have chosen K=10 but they didn't justify this choice or discuss the effect of K smaller or larger on the performance of the GCN model. The authors might also compare their method with existing Graph attention-based convolution networks which are close to SGCNs. The presentation has some English typos to be checked in (e.g., section 2.3).	Dataset sample size in each of the modality is less in number. How to make sure the convergence of proposed method? Edges in functionality connectivity are estimated based on weighting the Gaussian similarity function of Euclidean distance. What is the intuition of selecting this metric for evaluation. More detail about mutual information loss is needed. Why the interpretability is being called as sparse. How the proposed method make sure that the sparsity is being imposed on interpretable coefficients? Interpretation results need to be discussed in more details. Grammetical errors need to be checked throughout.	The research work in this paper is innovative and of practical significance. It will be better if the following questions can be considered. Since the work in this paper is based on multimodal data, it would be better to analyze or learn the relationship between multimodal data more deeply. Several more datasets can be used to test for greater persuasiveness.	The contribution in terms of originality of the approach and its significance are moderate. Sparsity in GCNs is not a new framework/approach. I am not confident about the efficiency and robustness of the approach since the paper doesn't show clear evidence about time complexity of the SGCN computations and the impact of K (in KNN) on sparse matrices. My rating could change if the authors clarify my concernes.	This paper is proposing interpretable model for Multi-Modal Diagnosis of Alzheimer's Disease. Proposed method utilizes Graph Convolutional Network for doing the classification task along with obtaining interpretation on input features.	The work of this paper has good innovation and practical significance, and it is recommended to accept.
474-Paper1168	Spatial-hierarchical Graph Neural Network with Dynamic Structure Learning for Histological Image Classification	This work proposes a new Graph Neural Network (GNN) method for the classification of histology images. The two contributions of the proposed method include firstly the dynamic graph structure that is learnt as part of the learning stage. Secondly, the employed vision transformer mechanism improves the feature extraction using the deploying the multi-level structure of the graph.	The paper presents a spatial-hierarchical GNN framework including a dynamic structure learning module to explore the spatial topology and hierarchical dependency of the multi-level biological entities in order to improve histological image classification. The proposed framework was evaluated on two datasets and results demonstrate that it outperformed the state-of-the-art methods on both datasets.	In the paper a method for the classification of histopathological images is presented. The method is based on a Graph Neural Network (GNN) approach, introducing a dynamic learning. Moreover, an approach visual transformer-based is used for the final classification.	The proposed GNN method dynamically learns the graph structure that employs feature representations and position attributes to connect nodes, i.e. entities. The feature extraction and aggregation for classification have been improved using a vision transformer that improves method performance. The evaluation has been done on two datasets and the proposed method has been compared to its relevant competitors. Selected methods have been compared based on quantitative metrics. Use of the proposed method can result in the generation of more explainable graphs that can better relate to the medical explanation of the problem and can therefore result in more reliable / accurate results.	The paper has a clear structure and well-written, it is overall easy to follow. The method is described with details, and enables interpretability of prediction outcomes. The code is open sourced and one of the datasets is publicly available.	the concept of dynamic learning in GNN is new, with respect to my knowledge the use of a vision transformer in GNN is a novel approach and it is particularly interesting	Explaining the multi-level structure of the proposed structure, it would be informative to mention the number of levels used in the experiments and to explain the effect of that as a hyper-parameter. The literature review could have been enriched on the use of vision transformers in previous work.	Despite the detailed description of the graph-based metrics, the authors do not explain how the training strategy was designed. They mentioned that they used a 5 repeated 3-fold cross-validations but they did not explain if it is an end-to-end learning fashion or no. There are several spatial graph convolution methods existing in the literature. Why choosing GraphSAGE in particular? The rationality of choosing transformer is not clear. Why not using GAT network which is based on attention mechanism? The limitations of this framework needs to be discussed.	results are not very convincing, above all on BRACS dataset, further results should be added	The code has been provided to regenerate the results.  Part of the experiments has been done on BRACS dataset which is publicly available.	The checked points in the reproducibilty checklist match perfectly the information provided in the paper.	Beyond the presence or absence of the software, further details in terms of reproducibility could be given, for example the stop criterion for the choice of the best model.	Even though the flow of the text is good, there is a relatively large number of grammatical mistakes, mostly in the use of articles such as 'a', 'an', 'the'.	In the first paragraph, citations started with [6,28] it is better to see an order in the references [1, 2] instead. Please see the weakness section.	"The paper is well written and well organized. The method is clear and the comparisons are sufficient. However, I have some suggestions: please, could the authors clarify how dinamically change the structure of graphs? what is meant by dynamic change? do it change during epoch? In subsection 2.3 it is written "" Specifically, each hierarchical sequence is tokenized and attached with positional embedding as the input of a Transformer encoder consisting of Multi-Headed Self-Attention [26], layer normalization (LN) [4] and MLP blocks."" How the tokenization of graph in ViT happens? Please, clarify further information on reproducibility should be added, for example the early stop criterion for the choice of the best model with particular reference to BRACS dataset, in the cited paper, as HACT, the results are given in terms of F1 measure, why did the authors enter the results only in terms of AUC? AUC as Accuracy do not take into account the imbalance of the dataset, which is particularly evident in BRACS. Instead, F1 measure is adapted in the case of imbalanced dataset and I suggest to added also this measure the comparisons were performed on the same dataset for all methods? HACT used a previous version of the dataset, thus I would make sure of this. the reference to CRCS dataset is missing. Moreover, the reference to BRACS is not [22], but the following: Brancati, N., Anniciello, A. M., Pati, P., Riccio, D., Scognamiglio, G., Jaume, G., ... & Frucci, M. (2021). BRACS: A Dataset for BReAst Carcinoma Subtyping in H&E Histology Images. arXiv preprint arXiv:2111.04740."	This paper well explains the problem it is addressing that is of clinical importance in the disease diagnosis through histology image classification. Most relevant prior works have been addressed and their shortcomings have been explained. The proposed method has been designed to address the initially presented problems. Claims have been addressed through the experiments and the story makes a good sense.	"The paper is well-written and it shows a good validation on multiple datasets, however there are some concerns regarding the details of GraphSAGE and transformer. For this reason I picked ""accept"" rate."	The results are not very convincing. It would be appreciated if F1 measures are added for the experiments.
475-Paper0557	Spatiotemporal Attention for Early Prediction of Hepatocellular Carcinoma based on Longitudinal Ultrasound Images	This paper focuses on early prediction of hapatocellular carcinoma (HCC) based on longitudinal ultrasound images. The authors propose a spatiotemporal attention network that adopts a convolutional-neural-network-transformer framework. Their method achieves state-of-the-art performance compared with other popular sequence models.	The paper presents a CNN-transformer network for hepatocellular carcinoma (HCC) diagnosis from longitudinal ultrasound. The authors incorporate an ROI attention block into the CNN to perform feature extraction. They also use an age-based positional encoding to process longitudinal imaging of arbitrary time intervals.	This paper presents a new early HCC prediction method using a spatiotemporal attention network (STA-HCC) based on longitudinal US images. The topic is especially important in the case of 1) defining regions of interest (ROIs) in longitudinal US images and 2) sequential images and irregular temporal components. The proposal uses non- longitudinal US images to predict HCC. The motivation of the paper is clearly defined, and a brief state-of-the-art presented. Paper contributions: 1) ROI attention block, 2) age-based position embedding in transformers.	This paper focuses on early prediction of HCC based on longitudinal ultrasound images. The authors claim that few studies have focused this topic and I also could not find similar studies. The authors made a large sized dataset of longitudinal US examination for HCC including 619 subjects (although the dataset is not public). The proposed method achieves better performances compared with popular sequence deep learning models such as LSTM, BiLSTM, GRU and vanilla Transformer.	The ROI attention block and age-based encoding are simple and sensibly designed. Good baselines and ablations. Other researchers working on classification tasks from longitudinal imaging data can draw inspiration from this work.	The task investigated in the paper is clinically important. It is interesting to try to include age-information features in the position embedding. The figures are illustrative.	"The proposed method lacks novelty in some technical aspects. The proposed method is almost a combination of existing technologies. The core idea of the ROI attention block is the same as the method originally proposed in [1]. In addition, the transformer encoder used in the proposed method has the same architecture as the original transformer [2]. The age-based position embedding in the proposed method has some originality (and experimental results show that it can improve performance), but it is not novel from a technical point of view. [1] Wang et al., ""Non-local neural networks"", CVPR2018 [2] Vaswan et al., ""Attention Is All You Need"", NIPS2017"	No glaring weaknesses, although comparison to radiologist performance and analysis of failure cases would help demonstrate clinical utility. I am also curious how much radiologists actually rely on previous scans to perform a diagnosis, and how much this model does. Is it sufficient to just show the latest + current scan to get good accuracy? What about the current scan alone?	The experimental evaluation should be clearer. The writing quality of this paper is not satisfactory. There are sentences which require some references.	The experimental conditions in terms of reproducibility are well described. The authors provide source codes for training and evaluation. The dataset is not public.	Private dataset and code.	Paper is clear enough that an expert could confidently reproduce	"[Major comments] The main drawback of the proposed method is its novelty in technical aspects. The authors mention the difference between the proposed method and nonlocal attention in section 2.3, but the difference might be marginal. Please provide clear and additional explanation on the novelty of the proposed method. Some experimental conditions are not clear. (1) Why do the authors set N to 3 ? What does it mean from the clinical benefit point of view ? (2) Were the ultrasound images captured using the same ultrasound diagnosis machine ? If not, are there any impact on the performance ? (3) The ablation studies include ""(iii) STA_HCC without the age-based PE (w/o age-based PE)"". How do the authors define PE in this case ?  I think the authors do not use the PE used in the vanilla transformer since the experimental results shown in Fig. 3 and Table 1 are different. [Minor comments] The term ""longitudinal"" is confusing. In medical ultrasound images, the term ""longitudinal"" uses to indicate the direction of the image plane in general. It might be good to provide a note about the definition of the term."	"Usually I understand ROI to mean bounding box. Consider renaming ""ROI attention"" to ""spatial attention"" My understanding is the ""Transformer [18]"" baseline does not use age-based PE? It seems that your model w/o age-based PE does worse than [18], which suggests that maybe [18] could outperform your model if you just add age-based PE. Would be a little stronger with multisite data."	"There are major weaknesses in the evaluation of the method: The information of and hepatocellular carcinoma is kind of localized in MRI slice. Compared with reference [22], why do you think taking advantage of the gating mechanism would help the localize focal areas? Could you add more experiment to show that the necessity of ROI attention in this task? For examples, adding results without sigmoid or ROI attention in Fig.4 to intuitively show the improvement of location. There are sentences which require some references. -In page 2, .... (US) is currently the most common imaging modality for diagnosing and monitoring HCC "" requires a reference. -In page 8, "" bidirectional LSTM (BiLSTM) "" requires a reference.  -In page 6, ""We chose SE-ResNet50"".... requires a reference. Typos: The dataset collected from...--->> The dataset was collected from... -.. track and focus the same lesion...--->> track and focus on the same lesion... -..used to establish a appropriate positional...--->> used to establish an appropriate positional..."	Although the technical novelty of the proposed method is marginal, the method has some impact on early prediction of HCC based on longitudinal ultrasound images, which has clinical benefit and is a new application in MIC.	The paper presents a simple yet incremental approach that could be useful for a wide range of longitudinal image classification tasks (although the paper only demonstrates it on the task of HCC diagnosis from ultrasound). The experiments are clear and fairly convincing.	This paper applied transformers to the clinical task, which looks interesting. However, the ROI attention block proposed in the paper has not been effectively verified. To be precise, the experiment can not prove the superiority of ROI attention block compared to nonlocal attention [20].
476-Paper1715	Spatio-temporal motion correction and iterative reconstruction of in-utero fetal fMRI	The authors attempt to address the motion1. challenges in fetal MRI by looking at the spatio-temporal dimensions of the 4D fMRI data set as a whole rather than as a set of 3D volumes over time.	As a general preprocessing step of fetal fMRI analysis, this manuscript proposes a four dimensional (4D) iterative reconstruction method to correct the motion scattered fMRI slices, while existing methods processing 3D images at each time frame individually.	The authors propose a spatial-temporal iterative reconstruction method for motion correction of in-utero fetal fMRI. Experimental results on a cohort of real clinical fetal fMRI data indicate improvement of reconstruction quality compared to the conventional interpolation approaches.	The spatio-temporal formulation is intuitive and makes sense The use of well-established regularizer such as TV makes it easy to implement and reproduce	Motivation of the proposed is well illustrated. The presented method is well presented and easy to follow. Experiments were performed on real clinical fetal fMRI data. Down-steam tasks were used to measure the effect of motion compensation (Section 3.4).	Novel reconstruction of a 4D image from a single sequence acquired over time by employing the spatial-temporal similarity Solid evaluation in real clinical data	The computational effort of this algorithm needs to be detailed - specifically memory and time per iteration The choice of the Lagrange parameter (Alphas in the current implementation) needs to be demonstrated based on L-curves or other methods - Tikhonov for example. It will be useful to see the original SSIM values instead of just the difference	Comparison is not fair enough from my point of view.  In Section 3.3, baseline methods are simple interpolation of independently re-aligned 3D images of different time frames.  Firstly, comparing the proposed method with linear, cubic, and sinc interpolation is redundant. There is not a significant gap between different interpolation methods, thus it is not very necessary to compare all this interpolation method. Also, better baseline methods, rather than simple interpolation, would make the proposed method more convincing. Secondly, for fMRI analysis, is it necessary and important to interpolate between different time frames? It seems interpolation is not used in experiment in Section 3.4 (Fig. 4). In Section 3.4, it seems that the proposed method is compare with observed image without any re-alignment, which is not fair enough. It is better to perform basic 3D registration before fMRI analysis. If I misunderstand something in Section 3.4, this should be clarified.	The iterative spatial-temporal reconstruction may be computationally expensive; the computation time should be clarified The motion is estimated prior to the iterative reconstruction. It is unclear whether the motion parameters can also be iteratively updated during the reconstruction.	The authors meet the reproducibility criteria. The reviewer was unable to find any code or a commitment to share later.	The proposed method seems to be  reproduceable. The method is clearly presented and there is an algorithm summarizing the workflow.	The algorithm is clearly explained, though the data and code are not made available.	The authors have pursued an important problem - motion mitigation in fetal fMRI.  The strengths and weaknesses are outlined above.	My major concern is the comparison of baseline methods. Authors only compare their proposed method with some simple interpolation-based method, which can be much improved in my point view. Fair comparison with baselines would make this manuscript more convincing. Check the ``main weakness'' for more details. While This manuscript is well organized and easy to follow, there are still several things unclear to me. What does  the metric ''L'' mean in Fig. 3 (a)? I do not find any definition all through the manuscript. Also, SSIM metrics require a reference image, but obvious there is no such a ''ground-truth'' in this dataset.  Fig. 2. shows a Reference volume, but the main text does not have any description of the reference volume. Also, the ''s2v'' in Fig.2 is not properly abbreviated. Language-related errors that can be easily fixed. ''..., whereas the proposed 4D iterative reconstruction did recover the entire brain.''	1: Iterative reconstruction of the fMRI data with 96 volumes using the low-rank and total variation constraints can be computationally demanding. Please comment on the computation complexity. 2: How the downsampling and blurring operators in the reconstruction equation are designed should be clarified. The motion operator is missing in Eq. [7]. In the current algorithm, the motion is estimated prior to the iterative reconstruction. The curiosity is whether the motion parameters can also be iteratively updated during the reconstruction.	The problem of fetal fMRI motion is well motivated. Looking at the spatio-temporal data as a whole is an intuitive approach. The use of TV is simple to implement and reproduce. The details related to computation - time, effort and convergence - needs to be discussed.	The motivation, method part are very well, but the experiment part can be much improved.	Novel motion-corrected reconstruction of 4D in-utero fetal fMRI by employing the spatial-temporal similarity, which results in much better image quality than non-iterative interpolation methods.
477-Paper0361	Stabilize, Decompose, and Denoise: Self-Supervised Fluoroscopy Denoising	The paper proposed a self supervised method for fluoroscopy denoising. In their method they have first stabilize the the frames to compensate the non stationary background effect induced by the motion of the x-ray detector, than decompose the background and foreground using RPCA(a variant of principle component analysis) and then denoised the background and foreground separately.	They proposed a pipeline to stabilize and denoise fluoroscopy video where severe noise and motion exist. First find global motion between adjacent frame, decompose foreground and background using proposed mask-based RPCA, denoise them and composed. They showed the better denoising performance compared to other approaches in numerical measure and expert evaluations.	Proposed a three stage framework for denoising including stabilizing using optical flow, decomposing by proposing masked Robust Principle Component Analysis (RPCA), and denoising by using a simple self-supervised network.	Proposed framework utlized the knowledge of fluroscopy imaging physics to design the method. It is a self supervised method, so paired data is not required for training. The first two stage, i.e., stabilizing and decomposition has contributed significantly to improve the denoising performance of existing self supervised denoising methods.	They built a framework which does stabilization, decomposition, and denoise which is novel. They provided mathematical proofs that support their claim.	Please see below	"Literature review of the paper is very poor. Although it has discussed different method for self supervised denoising, RPCA but no previous study regarding fluoroscopy denoising is described. Ambiguity in clinical study. It is not clear what the author mean by ""In addition, for each group, the five images are permuted randomly"". If this permitted images are used as input to denoising network, then for some case the already denoised image may have been used as input to the network. What is necessity of this step is not clear, and how the author ensure the above did not happen mistakenly. No comparison with existing methods for fluoroscopy denoising. The paper only considered baseline denoising method without first two stage and complete method. However did not compare their method with exsinting fluoroscopy denoising methods. a. Matviychuk, Yevgen, et al. ""Learning a multiscale patch-based representation for image denoising in X-ray fluoroscopy."" 2016 IEEE International Conference on Image Processing (ICIP). IEEE, 2016. b. Amiot, Carole, et al. ""Spatio-temporal multiscale denoising of fluoroscopic sequence."" IEEE transactions on medical imaging 35.6 (2016): 1565-1574. Also other SOTA self supervised video denoising method should be used as baseline."	Please see the detailed comments below.	Please see below	The paper is reproducible.	Implementation details and experimental settings were well-explained. It could be reproducible.	Yes	Literature needs to more comprehensive about fluoroscopy denoising. Do comparison with other SOTA methods.	"Robust Alignment[1, 2], which combined RPCA and image registration, has been studied to deal with stabilizing the video where sparse corruptions exist. Robust image alignment aligns the batch of images together, and simultaneously doing RPCA so that the alignment is robust to the sparse corruptions. Using robust alignment, we could acquire aligned images, and corresponding low-rank background and sparse foreground. It can be viewed as the combination of Stage 1 Stabilize and Stage 2 Decompose in the proposed paper. It should be clarified about the difference between the proposed method and robust alignment, and also the reason why the proposed method is a better formulation. Also, robust image alignment handles all image data at once, where the Stabilize step of the proposed method finds the transformation parameter for each frame. The robust image alignment can find a better registration parameter based on better accurate low-rank subspace. Training a smaller student network using a large teacher network to reduce prediction time is a widely used technique, but I can't find any information about speed improvement or execution time w/ and w/o student-teacher network. Optical flow is estimated from the network implemented by PWC-Net. Was it newly trained on this fluoroscopy video, or was it a pre-trained network? In the example videos (in supp.), I observed the blood vessels are moving with the heartbeats, whereas the background body does not. Is it okay since we are finding a representative translation parameters using KDE (where the pixels for blood vessels are more sparse compared to the pixels of the backgrounds)? It is difficult to investigate better performance in Figure 4. Consider adding arrows to emphasize. Consider adding the following recent RPCA paper in the Introduction: Han, Seungjae, et al. ""Efficient neural network approximation of robust pca for automated analysis of calcium imaging data."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021. Followings are minor comments. Are other denoising methods (Noise2Void, Noise2Self, ... ) trained in XCA dataset the same as Self2Self? In the citation of RPCA methods in the introduction, need to change the order of the last two papers (currently [... 32, 34, 33]). Typo in the Table 1 caption. denoiseor -> denoiser In Table 1, there are Ours+N2V, Ours+N2S, Ours+S2S. Isn't the proposed framework include Denoise step? Consider denoting as 'Ours w/ N2V'. Not sure only one proficient radiologist is sufficient, without consensus, for qualitative validation of the methods. References: [1] Peng, Yigang, et al. ""RASL: Robust alignment by sparse and low-rank decomposition for linearly correlated images."" IEEE transactions on pattern analysis and machine intelligence 34.11 (2012): 2233-2246. [2] Zhang, Xiaoqin, et al. ""Robust low-rank tensor recovery with rectification and alignment."" IEEE Transactions on Pattern Analysis and Machine Intelligence 43.1 (2019): 238-255."	This paper proposed a three stage framework for denoising including stabilizing using optical flow, decomposing by proposing masked Robust Principle Component Analysis (RPCA), and denoising by using a simple self-supervised network. Evaluated on one private dataset and one clinical dataset (a radiologist to rate denoised image quality). The paper is easy to follow and understand, the writing and formation could be improved (such as adding numbers to Fig. 4). The novelty is limited except a contribution to RPCA (based on Inc-PCP algorithm) to solve non-overlapped areas, the first two stages are data processing stage and the denoising stage leveraged an available self-supervising network, overall it does not have a significant contribution. The experiments are not strong, suggest to use more publicly available benchmark datasets and avoid to use private dataset. The comparisons to other methods are unfair since the proposed method used optical flow to stabilize inputs for motion data, but other methods such as Self2self proposed for the additive white Gaussian noise (AWGN) data. More recent works suggest to discuss after the year of 2020 and compare if possible.	The method is interesting, but in my opinion validation is not enough.	Good paper from an overall perspective, but there are some curious points as in comments. Also, the differenece and comparison to the robust image alignment method need to be clarified.	Novelty and Experiment
478-Paper1623	Stay focused - Enhancing model interpretability through guided feature training	The authors propose a mask input-based training scenario that separates the training target from the background for the design of explanatory AI models. As a target task for explainable AI, a multi-class classification problem was targeted, and an instance segmentation model was utilized for mask generation. SmoothGrad [10] was applied to visualize the feature output of the model. The authors propose the eCDF-Area method to evaluate the explanatory power of a model, and at the same time show that it is possible to train with mask-based synthesized images to minimize the burden of labeling in guided feature training.	This work proposes a data augmentation technique, named Guided Feature Training (GFT) for improving the interpretability of a deep learning model for the task of surgical instrument detection. The augmentation technique proposes to blur the background (everything that is not a surgical instrument) to guide the feature training towards the instruments and disregard other less useful information for the task (e.g. the background). The proposed augmentation requires a binary segmentation model (or binary segmentation annotations) that segment surgical instruments.	Improving the interpretability/generaisability of NN's using data augmentation and guided feature training for a surgical application, utilising numerical evaluaton of a model with a proposed area metric called eCDF.	The authors claim that the explanatory power of the model is improved when the proposed guided feature training is performed. Authors also propose the eCDF-Area method to quantify the explanatory power of enhanced artificial intelligence. For quantification of heat map-based explanatory methods such as GradSmooth, the authors utilize eCDF-Area methods with target RoI information to be recognized. At the same time, guided feature training increases the generalization performance of the model, which can be seen in SmoothGrad visualization.	The idea of blurring the background for guiding the feature learning of the features is simple and interesting for improving the interpretability. The authors propose a metric (eCDF) to measure to quantify the interpretability or focus of the model. A simple figure could help the reader to better understand the eCDF metric. The proposed method seem to improve the 'focus' of the model on to the surgical instruments in the samples shown by the authors and in the performed experiments The paper is well presented, written, and easy to understand	The eCDF-Area metric is an interesting measure introuced for numerical evaluation with heat maps and the mask for the region of interest. Using the features to guide improvements is a nice approach and made for an interesting read.	Overall, it was difficult to know the purpose of the proposed study. The proposed guided feature training looks like a variant of the general feature fusion-based training method. The fused training with additional labeling information can improve recognition performance and enhance feature visualization such as GradSmooth in the model. In conclusion, I am not sure if guided feature training can be viewed as a branch of XAI research.	"The authors evaluate the proposed approach in three settings. The third one as stated by the authors is ""investigating the effects of GFT in its realistic application"". Table 1 shows the results of these three settings. The one in realistic applications, in which segmentation is automatically inferred, shows that a conventional training ('none') obtains an average F1 of 76.7%, using blurred backgrounds ('blurred') obtains 74%, and using non-blurred and blurred images ('combo') obtains a 77.3%. This suggests that using only blurred images damages the performance (though improving the interpretability). The experiment with 'combo' seems to slightly improve the performance (+0.6%). However, it seems that there is no experiment that shows wether this small improvement comes from using non-blurred + blurred images or simply by training on double amount of (repeated) data. Authors should evaluate the model in a fourth setting 'naive combo' where they use double amount of (repeated) training data without any blurring to obtain a fair evaluation. It is difficult to assess a paper for which the data is not public at the time of submission."	Interesting results, just thinking that if it performed well on a small dataset, perhaps we don't have a clear indication of performance but a start of a particular trend. My concern is probabably the different combinations of original/modified datasets - what/how is an optimal vlaue chosen?  This is not clearly defined/explained (page 3). On page 3, perhas a more technical word/wording for 'decent approach', as it is a formal paper. I would like to see more statstical testing performed to compare model performances.	Authors will publish all relevant code, datasets, and models.	Authors will provide the code and dataset upon acceptance. However, these were not available at revision time so that they could not be assessed.	Details in terms of calculations are clear, code will be provided. The data is publicly available but the details re images might not be as clear/shared with and in this way one could face a bit of problems replicating the work.	If the proposed study focuses on the XAI point of view, it is an opinion that it may be better to focus on the proposal of eCDF-Area metrics for visualization methods that try to explain the output of models such as Grad-CAM, SmoothGrad, SmoothGrad-CAM++, etc. The training scenario in Figure 1 should be expressed more concretely in terms of the input and output of the model.	"Perform further experiments as (or similarly) than the proposed in the point above Further justify (and or add a reference) to certain statements. For instance ""As preliminary experiments have shown, common models that determine if a certain instrument type is currently visible (instrument presence detection) lack focus on the instruments themselves and often decide based on features in the background."" Where has this been shown? Improve figure captioning. For instance, add description of what the graph shows in Figure 3, specially on the horizontal axis; and improve the caption of figures including what the reader should extract from them."	Thank you for the work performed for this paper, I have some points to kindly raise: 1.1 Table 1: 0% for the clipper, for automated masks? Why is that, do you know? 1.2 Table 1: I notice the Scissor performed less optimally compared to other instruments, any thoughts around this? 1.3 Thoughts about the combo providing the best outcome? Do you think this is because a model needs to see more images not similar to the ideal in order to train more generaisable?  1.4 Why only train with false negatives for guided feature training? 1.5 Figure 1 - I assume you first train the model then retrain? Sorry this needs to be a little clearer unless I have misinterpreted this. 1.6 Protocol for the video/cine stack - not sure this is clear  1.7 My concern is probabably re the different combinations of original/modified datasets - what/how is an optimal value chosen - see previous comment - I don't see the ratio/size given for each?	It may be correct to view guided specific training as an improvement in the training model due to multiple supervision rather than XAI research.	The idea of blurring the background for improving the interpretability seems to be novel The idea for quantifying the increase in 'interpretability' trough focus is interesting The performance (F1-Score) seems to slightly hurt or at most obtain comparable results than traditional training (without the idea proposed by the authors) Additional experiments to support that the proposed idea improve the performance (on top of increasing the interpretability) will strength the paper	I think overall, a well layed out paper, some methodologies to be given in more detail, but a nice approach that is interesting to read about.
479-Paper1049	Stepwise Feature Fusion: Local Guides Global	This paper proposes a transformer based segmentation architecture SSFormer. The main novelty of SSFormer is the local emphasis operator, which forces the attention to put more weights on nearby patches. SSFormer achieves good empirical performance on the polyp segmentation task.	In order to segment polyp more accurately, this work designs a new framework, SSFormer, which exploits pyramid transformer architecture as encoder, and proposes a multi-stage aggregate decoder (PLD) to progressively fuse information at different stages. It shows a better performance on different benchmarks.	The paper provide a pyramid transformer based model for 2D medical image segmentation, with a novel progressive locality decoder from multi-stage feature aggregation, in order to improve the generalizability/robustness and better capture local features in challenge segmentation task (polyp segmentation). Comprehensive experiments with promising performance are reported.	Local emphasis operator (LE). Although there are similar techniques to emphasize locality, e.g. swin transformer and NesT, the LE module is somewhat different. The experimental results are good.	The paper is well organized A new pipeline is designed for polyp segmentation. The experiments show it has a better learning ability and generalization ability Ablation study on different combinations of encoder/decoder is conducted to show the effectiveness of PLD.	The paper has a clear motivation for its method. Pyramid transformer encoder is selected for better generalizability of the model, especially for challenged polyp segmentation with various size and shape. And progressive locality decoder (PLD) is designed to emphasize local features of small structures. The model is evaluated on both in-domain testing set as well as unseen dataset, and the results demonstrate the robust generalizability of the model performance. The model is further trained and tested on other segmentation tasks, i.e. skin lesions, nucleus segmentation, which shows the model' potential for common medical segmentation tasks. Comprehensive horizontal and ablation studies are reported in Results, which further convinced the improvements of the model. The paper provides attention maps of each scale and method to better visualize better local feature extraction ability of PLD.	"The main claim that the LE module works becuase it can reduce ""attention dispersion"" (I guess it's more commonly known as ""feature oversmoothing""?) But the intuition why this can reduce ""attention dispersion"" is not clearly explained. The authors only support this claim with visualizations (Fig. 2). Important baselines are missing in the experiments, esp. ViT based models, such as Segtran (IJCAI 2021), TransUNet (arXiv:2102.04306), Swin-UNet (arxiv:2105.05537) and SETR. In addition, PraNet should be compared in Table 1. Citations are missing for Segtran, TransUNet and Swin-UNet. Various writing problems (see Section 8)."	To verify the effectiveness of the LE module, it would be better to have an ablation study in which conv layers are removed. In Fig 1(b), does it mean that the sizes of output are always H(W)/4? The framework exploits the transformer encoder to have enhanced capability. The whole pipeline is similar to Unet. Overall, the technical novelty is marginal. For results, compared to SegFormer, the improvement is marginal.	"The paper's novel PLD contains two parts: LE and SFA. Although the results show that this PLD decoder improves the segmentation mDice score on multiple datasets, there's no ablation study to explore and demonstrate the effect of LE or SFA separately, i.e. how is the performance gain with only LE combined with traditional parallel feature fusion? and how is the performance gain with sole SFA paired with simple upsampling without precedent two conv layers? Without these ablation study, it's hard to verify whether the performance gain of the whole model benefits from LE or SFA or both. The scores of other horizontal methods in Table 2 and 3 (generalization tests) are directly refer to [9, 14, 16, 25], instead of reproduce those papers' methods using the same experimental settings, especially data augmentation in this paper. This is because the data augmentation itself is able to improve the generalizability/robustness of the model on unseen dataset, which has been proved in both robust learning[A1] and domain generalization[A2]. Therefore, if the scores are refer to the original SOTA papers, people cannot know whether the improvements of generalizability is from the proposed model or the different data augmentation. The paper uses mDice and mIOU as evaluation metrics, but these two metrics are actually evaluating the same aspect of segmentation by calculating overlapping area, so only keep mDice is enough, which is widely accepted in medical segmentation. In addition, in order to better demonstrate the segmentation performance, Hausdorf-Distance, another commonly used segmentation metric, should be used to evaluate the predicted mask shape accuracy. The novelty of the method is slightly limited: the pyramid transformer encoder design is from [20,22], and for the two novelties in PLD, compared to Segformer[22], the Local Emphasis simply adds two convolution layers before upsampling feature maps of each scale, and the Stepwise Feature Aggregation, instead of parallel fusion, inserts a linear layer between each aggregation level and progressively fuses multi-scale features from deep to shallow, where this progressive feature fusing scheme is also used in many other segmentation decoders, even widely-used Unet is using a complicated version of this scheme. Thus, the method novelty of this paper is more like some small increments of modules from previous works, which somewhat limits the method novelty of the paper. Minor problems: The convolution kernel size of LE module is missed. The size of C in PLD is missed. The description of Fig.2 has no explanation of (c). The description of Table.4 has no definition of CvT. Eq1 doesn't contain the upsampling operation in LE. Ref:  [A1] Hendrycks, Dan, et al. ""Augmix: A simple data processing method to improve robustness and uncertainty."" ICLR 2020. [A2] Volpi, Riccardo, et al. ""Continual adaptation of visual representations via domain randomization and meta-learning."" CVPR 2021."	I didn't spot issues about reproducibility.	I think this paper has provided enough details for models and experiments settings.	The authors indicate that the code and model will be released after being accepted. The datasets used in Experiments are all open accessed. The paper also provides the data splits, training parameters and model structure, although some of the model parameters are missed (convolution kernel size in LE module), the results of the paper should be able to reproduce.	"Writing issues: 1) Don't capitalize State-Of-The-Art. 2) ""attention dispersion"" is invented in this paper. If you propose a new term, please define/explain it in detail. Otherwise, please follow common terms. 3) In the caption of Fig.1., ""(a) is the ..."" => ""(a) the"" 4) In the caption of Fig.1., ""emphasized features"" is an awkward term.  5) In Section 2.2, (such as... etc.) please don't use ""such as"" and ""etc."" at the same time. 6) In Equation (1), what's Ci, C and i? Please define them. 7) In ""Stepwise Featgure Aggregation"", ""information interacted by .."" is an awkward expression. Fig. 2 is not clearly explained. Esp., as transformer attention is pairwise, you have to select a query point and visualize the attention of all pixels with the query point. What are the query points used for the attention maps in Fig.2?"	"For equation (1) and (2), it would be better to explain the meaning of notations, like C and F_i. In table 4, it seems the last sentence is not complete. (""the CVT is"")"	"For the Experiments: In order to better explore and present the impact of LE and SFA separately, I'd suggest the author adding another group of ablation study for several settings: LE + parallel fusion, simple upsampling + SFA, LE + SFA and baseline without LE or SFA. Keep mDice scores and remove mIOU scores, and add Hausdorf Distance as an extra evaluation metric for a more comprehensive segmentation evaluation. Double check the data augmentation methods in papers [9, 14, 16, 25] from which the scores in Table 2 and 3 are refer. As I mentioned above, the different data augmentation will affect the robustness and generalizability of the model. It's even better to reproduce the models in those papers using exactly the same experiment settings, especially data augmentation, and compare the final scores in Table 2 and 3. Other issues in paper writing: From the paper, it's not clear how the author generate the attention maps at different scales. Please briefly introduce how the attention maps in Fig.2 is generated. Check through the paper and fix the minor writing issues mentioned in ""main weakness"", as well as correcting some other spelling and grammar typos."	"The empirical performance of SSFormer seems good. Therefore I lean towards acceptance. However a few important baselines are missing, and there are many issues on writing. The intuition of LE against ""attention dispersion"" is not clearly explained. I can only recommend ""weak accept"". The authors should heavily revise the manuscript to address these issues."	This work proposes a new framework for polyp segmentation, and achieves competitive results. However, the technical novelty is marginal. I prefer to have weak reject at the current stage.	Considering the clear motivation and reasonable contributions, the comprehensive experiments and the promising results, I think the paper has potential to be accepted. Even though there are a few deficiencies in the experiments and results, which cause some weakness of the paper, the overall rate of the paper should be above borderline.
480-Paper2108	Stereo Depth Estimation via Self-Supervised Contrastive Representation Learning	This paper addresses the problem of stereo depth estimation. Authors introduce a self-supervised contrastive representation learning method for two-stage stereo depth estimation.	This paper proposes the first contrastive representation learning (CRL) method for stereo depth estimation based on a momentum pseudo-supervsied contrastive loss, which achieves state-of-the-art performance.	This paper proposes an approach for stereo depth estimation. This paper proposes a two-phase training procedure including contrastive representation for feature learning and self-supervised disparity learning.	Methods are novel and interesting. Authors formulate CRL as a dictionary look-up problem as the contrastive learning for self-supervision. Methods are pretty well presented and easy to understand. Results are convincing. Following Figure 1, we notice that such a method can show the differences between different categories. Fig 5 also shows that such a method can be extended to general images. Authors have promised to release codes used in the paper. This paper is well written and easy to follow.	The paper is well written and easy to follow in general. The authors proposed a novel contrastive learning algorithm tailored for the stereo depth estimation problem. Self-supervised stereo depth estimation is an important problem given the lack of ground-truth labels, especially in medical domain. The proposed method is evaluated on two surgical datasets and one non-surgical dataset and demonstrates state-of-the-art performance.	This paper focus on the self-supervised stereo depth estimation. This is important as it is hard to collect abundant ground truth clinical data for supervised leaning.	I do not have major concerns for this paper.	Despite the novel application of CRL on stereo depth estimation, the proposed method seems to be a combination/modification of multiple prior work, which limits the technical novelty. For example, the proposed momentum supervised contrastive loss is a combination of the MOCO loss [8] and supervised contrastive loss [9]. And the decoder is modified from DispNet and PWCNet [14]	It is unclear what is the major difference between the proposed method and existing methods. It seems the two stage framework with the first stage using the contrastive representation learning is the main difference. If so, it is necessary to study other approaches to learn the representation at the first stage as the first stage only requires class labels rather than the ground truth disparity maps. The class labels are easy to get. There are other options to learn/initialize the encoder such as finetuning a pretrained model. Both the memory dictionary and the momentum based key encoder appear in the literature of contrastive representation learning. It is hard to identify any new method/contribution in the proposed stereo contrastive representation learning. It is unclear what is the difference of the proposed decoder and the decoder in DispNet as the details of DispNet is missing. It is unclear what image data, what feature representation, what model etc are used to obtain the t-sne visualisations in Fig.1. Thus, it is impossible to draw any conclusion based on the figure. It is unclear how to train the Encoder when using only the Lpe. Does the Encoder use weights pretrained on another dataset? Is the Encoder initialized randomly and jointly trained with the decoder?	Authors promise to release their code as well as the datasets used in this paper. Thus I believe the results should be able to reproduce following the descriptions and the released repo.	The authors provide implementaion details in the paper and also agree to release codes and pretrained models in the reproducibility checklist.	Details to train the encoder is missing.	Please refer to 4 and 5.	More ablation studies may be needed to understand the contribution of each components of the proposed method. For example, how does the proposed momentum supervised contrastive loss compared to vanillar CRL loss/ MOCO loss? It is unclear how the hyper-parameters are chosen such as the temperature \tau, and the weights for L_pe. Can the authors explain more about multi-scale disparity estimation in the decoder?	The legend in Fig.1 is too small.  Reference is expected to be added for DispNet. The equation (3) is confusing. I understand losses, Lmo and Lpe are applied at different stages of the model training. But the equation suggests both losses are applied at the same time.	Novelty, performance and reproductibility.	The application is novel and the performance is good, but the technical novelty is somewhat limited.	The contribution of this paper is limited. And there are much details unclear to understand the proposed method (see weakensses).
481-Paper0252	Stroke lesion segmentation from low-quality and few-shot MRIs via similarity-weighted self-ensembling framework	This work approach the problem of Ischemic stroke lesion segmentation when the training dataset is small and the MRI images have low resolution. To accomplish this task, the authors propose a framework that simultaneously train a neural network to segment ischemic stroke lesion and brain tumor, such that the update from the ischemic stroke lesion is stronger than the update provided by the larger dataset on brain tumors. According to the authors, the key elements of their approach are 1) a module that identifies the lesions and iteratively refine the prediction (IDN), and 2) another module that transfers optimization direction from brain tumor problem to facilitate the learning to segment the stroke lesion (SDU). The authors compares the proposed method with three other methods proposed in the literature for few-shot learning in terms of Dice, Accuracy and Hausdorff distance, surpassing with a large margin in all metrics.	The authors propose a novel approach to automatically segment stroke lesions on low-quality and few-shots MRI. The method exploits attention mechanisms to first identify lesions from a global perspective and then progressively refine their segmentation. A new strategy is also proposed to overcome the few-shots challenge.	The paper presents a framework for stroke lesion segmentation from low-quality and few-shot MRI. The authors present the Identify-to-Discern Network, which combines a pyramidal structure with attention layers and a multiscale loss. They also propose a Soft Distribution-aware Updating strategy (SDU) as a more effective alternative to pretraining on a related task. These techniques achieve good performance when co-training on glioma segmentation.	The capacity to exploit a large dataset to help training with a small dataset in a different problem could help to approach problems that has not been properly explored due to the lack of data. So this work could have an important impact. Based on the tests, both proposed modules (IDN and SDU) had an clear impact in improving the performance of the baseline, surpassing the other three methods.	-The new contributions are explained well are in details. Both the attention mechanism and the soft-distribution aware-updating are novel ideas  seamlessly integrated in the framework.  -This work has potentially a real-world applicability, aiming to improve stroke lesion segmentation in low and middle-income countries.  -The method proposed was compared with and outperformed different state-of-the-art approaches on publicly available datasets.	The two innovations are interesting and fairly novel. The Identify-to-Discern Network is well-designed and sophisticated without being overly-complicated. SDU with co-training is a simple and effective approach that successfully eliminates poor outliers compared to a pretrained baseline.	Lack of intuition on the process. Brain tumor is irregular with a complex structure (oedema, core and enhanced regions) represented by four labels, while the ischemic stroke don't present the same complexity in structure, being represented by a single region (one label). This mismatch in complexity is not considered in the paper and the authors don't provide a rationale for the soundness of the approach. Lack of clarity on the architecture. The architecture in Fig 2 is explained briefly, omitting the definition of some modules, such as, background and foreground attentive inception, channel and spatial attention, number of feature maps, places where upsampling are performed and the use of some symbols, etc. So it is difficult to understand how the architecture were engineered. Lack of detail on the training. How the authors deal with the fact that both problems use a different number of input sequences and the output stage are different? Also, the trained process is not adequately explained. We do not know when the SDU interacts with the network. Every batch? Every epoch? And the loss function? How is it composed? Lack of clarity on the tests. The authors don't explain how the three methods (MLDG, PROTO and Reptile) were trained. Also, it is not explained why those were chosen and if they could be applied in this context. For instance in MLDG [15], the common factor was the bone, while the variation was the place, acquisition protocol, orientation, field of view, or surgical implant. So, the rationale of including methods whose assumptions are quite different has to be properly explained. Size of the test set. Although the metrics improve over the methods used for comparison: 1) the test set is small, so the metrics could be due to the random choice of cases and the inadequacy of the compared methods due to the mismatch on their assumptions. 2) Also, we don't know the architecture of the baseline. Since, the authors only provides the boxplot for the baseline, we can't compare the baseline with the proposed method and methods used for comparison at the same time.	-The authors describe in details the shortage of radiologists and the low quality MRI available in developing countries, and this can be of interest to the reader. In this work, however, high-quality datasets are degraded to simulate low-quality data. It would have been extremely interesting to evaluate the proposed method also on an acquired low-quality MRI dataset. -Statistical tests to support the results obtained are missing.	"Meta-learning baselines are not really fair. The meta-learning algorithms are typically applied when you have multiple training tasks (more than 1!) that are drawn from the same task distribution. It seems a little different from choosing a related task to pretrain/co-train on. It also would be helpful to have an attention-based baseline to compare IDN against. Maybe a segmentation transformer like UNETR. Ablations are not sufficient to understand what parts of the innovations matter, or why IDN/SDU synergize. For IDN, multi-layered supervision should be its own ablation vs. the attention layers. ""SDU only"" should have its own results. Pretraining-then-finetuning is a well-proven approach for a lot of other applications, help us understand why SDU makes sense / performs better in this application."	The authors indicate that they will provide access to the implementation. This will help to reproduce the results, but at the cost of a careful reading of the code, since the reader has to fill the gaps in the description found in the article.	Code and data are publicly available.	Code will be made available. Datasets are available.	The motivation of the article in investigating ways of using a large dataset to train a method on a small dataset on a different problem is interesting and may have impact. This could justify an article only on this problem without the goal of learning to apply in a situation with images with a lower resolution. Although there is potential in the objective, the idea that by learning to distinguish between necrotic and enhanced tissue based on T1, T2, FLAIR and specially T1C may help to distinguish ischemic tissue, using a set of MRI sequences that differ partially is not obvious. So providing the rationale of the adequacy of the method is a must. Also, it is important to show that it is also consistent. This could be argued by more compelling tests. For instance, 1) the authors could have opted to present a test in the blind test set of SISS; this would allow comparing with state of the art methods on this dataset. An improvement over these would give indication on the strength of the proposal. This could be accomplished by not reducing the resolution of the images. 2) To give evidence on the consistency of the approach, the authors could test in another problem, for instance in SPES that distinguish between the core and the penumbra of the ischemic lesion. An improvement here could allow arguing the methods had potential to transfer information about the complexity of the brain tumor to another less complex problem but more complex than SISS. The authors should improve the description of the method. Figure 2 should have all components defined and explained, or referenced to the respective article for detail. Also, the authors should indicate the number of feature maps, the input and output stages and, the places where the upsampling is performed. The training process is not adequately explained. The authors should explain when SDU updates the parameters. It is not explained if it is at every batch or every epoch. Also, when the samples of the glioma dataset are used and using which proportion? This is not explained in the article. As referred in the weaker aspects above, the authors should review the tests. The rationale for the selection of the methods should be clear. Also, a comparison with state of the art methods on the problem would strength the paper. The training of each method should be described. Also, the baseline should be included in table 1.	"-Rather than splitting the dataset in training, validation and testing once, a nested cross-validation could be considered to evaluate the method over the entire dataset. -Fig1. B has a typo. To be employed BY our method. -Throughout the manuscript, the authors mention the voxel spacing without indicating the unit. I assume this is mm and it should be specified.  -Fig.2 could be improved. Instead of repeating ResBlock and DiscernBlock, a legend showing the color-block correspondence could be added. The text is quite small and difficult to read.  -I understand the limited number of pages available, but the Introduction is rather long and there is no Discussion section. Rather than discussing the results in the Experiment section, I would do that in a separate Discussion section. -It would be interesting to add a time comparison between a manual annotator and the automated method proposed. This would strengthen the contribution of this work and its applicability. -Evaluation. ""Four matrices"" is probably a typos. This should be ""Three metrics"" as only three metrics are listed. -Conclusions: ""Our further work will improve the lesion segmentation accuracy and quantify the lesion volume"". This is a bit vague, the authors should rather mention how they think the lesion segmentation accuracy can be further improved."	"""pyramidal structure"" is better than ""coarse-to-fine"". ""coarse-to-fine"" usually describes an architecture that first explicitly processes a coarse version of the image and then fills in finer details. a little better to rename ""accuracy"" to ""precision"" is stroke lesion segmentation really needed for ""rapid stroke diagnosis""? seems this is more of a tool for clinical researchers. maybe can be used for radiological reporting but not for diagnosis. no related work section"	The rationale for the validity of the approach is not evident and is not presented in the paper, which raises question on the reproducibility of the results in a slightly different setup. This is exacerbate by: 1) the small size of the test dataset; 2) the lack of comparison of state of the art methods on the problem; 3) the choice of methods that were not designed to work in a scenario as described in the paper - use of datasets from different domains and semantic labels.	The authors propose a novel framework to segment stroke lesions in low-quality MRI. The contributions are explained in details and the method proposed could be useful in low-resources settings with a real-world application. Although it could be improved in some aspects, the manuscript is well-written and would be of interest to researchers in the field.	The proposed approach is interesting and novel, but does not provide enough comparisons/ablations to help readers understand how the different components work together to produce the more robust model that the authors achieve.
482-Paper0050	Structure-consistent Restoration Network for Cataract Fundus Image Enhancement	In this paper, the authors proposed a restoration network (SCR-Net) for cataract fundus images enhancement. They designed a synthesis model of cataract images to generate synthesized training data (SCS), a restoration model to get cataract fundus images from synthesized data, and a module for HFC alignment. These three parts were integrated into the backbone of SCR-Net. Their approach was evaluated on two public datasets and two private datasets respectively and test results showed good performance.	This paper aims at enhancing the quality of fundus images to improve the certainty of fundus examination, proposes a structure-consistent restoration network (SCR-Net) to enhance cataract fundus images in the absence of supervised data. The authors generate synthesized cataract set (SAS) by synthesizing cataract fundus images sharing identical structures according to the imaging principle. To boost the training stage and structure preservation in SCR-Net, the high-frequency components (HFCs) are extracted from network to constrain structure consistency. Following comparison experiments and ablation studies prove that the proposed algorithm has achieved state-of-the-art.	The authors proposed a SCR-Net, which utlizes the high frequency structure consistency of fundus image, to enhance degraded fundus image. Moreover, a synthesis model for generating cataract images is proposed following the principle of fundus image.	(1) In order to overcome the difficulty of collecting paired cataract images, the authors designed a cataract simulation model to generate synthesized cataract sets as training data (2) To enforce the structure preservation in the restoration, they designed a model to extract the HFCs from the SCS to boost the model training.	The article focuses on capturing cataract-invariant features of retinal structures in image restoration, generates a synthesized cataract set by a simulation model with several parameters. Moreover, a low-pass Gaussian filter is adopted to extract the low -frequency components and maintain structure consistency. The authors expound the design of the algorithm clearly with detailed formulas and experiments. The comparison of several algorithms is also intuitively showed and demonstrated with restoration image results.	The way of generating SCS is well explained and easy to follow. Additionaly, the SCR-Net is technically sound.The evaluation is not limited to the restoration but to various clinical applications such as segmentation and classification.	"(1) Compared with other studies, the model proposed by the author has a certain improvement in evaluation metrics.  But the samples of public data sets used for evaluation and comparison are few, public data sets are also few, so the model's validity is limited. (2) IoU was not validated in the Kaggle dataset. (3) The authors said intuitively SCS generated by cataract simulation models with Eq. 2 share the same identical structures. They should use metrics for quantitative and qualitative evaluation. (4) In abstract part, the authors said the SCS were generated from cataract fundus images.  However, in other parts, such as in Fig.1, SCS were generated from a clear image.  They should be unified. (5) Were the clear images from post-operation eyes or normal eyes?If the clear images are from post-operation eyes, would they also suffer from the short of data. (6) The authors should not augment their conclusions. E.g., they demonstrate that their approach was effectiveness in the follow-up clinical applications and the existing algorithms ignored the performance improvement of clinical applications from the enhancement. But there was no corresponding evidence.  (7) In conclusion part, the authors said ""Thanks to its independence from annotations and test data, the proposed algorithm is convenient to deploy in clinics"". Please explain ""independence"" and the convenience on clinical applications."	"The value settings of specific parameters in most formulas are not clear, the authors need to show the actual parameter values in paper and explain the reasons for choosing these values. Moreover, some revelant experimental data in parameter settings need to be introduced, which means it would be better if following ablation studies of parameters are shown in the experiment section. In introduction section, the article summarizes the challenges and shortcomings of cataract restoration algorithms, however, ""To address these problems"" in page 2 is not suitable. The point four, ""ignoring the performance improvement of clinical applications from the enhancement"", seems not clearly illustrated how it was solved. Besides, several experiments to evaluate clinical applications need to be done. Though the authors have done a solid work, the novelty of the work is limited. The designment of the whole restoration process is too complicated, it's hard to say whether the final results are benefit from the structure consistency idea or just many additional detailed local designments."	1.The discussion of three loss function in Eq.7 is not sufficent, conducting ablation studies on them would make the proposed method more convincing. 2.It would be better to present more qualitative results of cataract restoration results on appendix, considering Fig.3 only contains one test sample.	Upon release of the code, the reproducibility can be verified.	Two private datasets are adopted in the training and evaluation stage. Besides, several value settings of parameters are lost in the article. Adding the instructions of these parts in the paper would contribute to the improvement of reproducibilily.	It is likely to be reproducible.	"(1) The text lacks clarity and needs important English editing. The authors also need to be more precise, to do not overinterpret their results and to dampen their conclusions. E.g., ""intuitively"" lacks clarity and not scientific. (2) Other comments mentioned clearly in the strength and the weakness section of the papers."	The parameters showed in the figures should be explained. In Fig 1, three kinds of loss fuctions, LR, LH, Lcyc, their meanings of symbol are not found in the instruction. A detailed description of quantitative metrics of restoration and segmentation in Table 2 is required. It's better to clarify the reasons and advantages of choosing those metrics.	The author may consider discuss more about the loss function, as mentioned above.	The study did provide several interesting results, the model proposed by the author has a certain improvement in evaluation metrics. The study overcomes the difficulty of collecting paired cataract images and enforces the structure preservation. But the samples of public data sets are few, so as to the public data sets. Besides IoU was not validated in the Kaggle dataset. the text also lacks clarity and not scientific, needs important English editing. Although the relevant weakness and comments are mentioned before, we still think the paper can be accepted weakly given that the comments given in the weakness are addressed properly.	The novelty of the paper is limited, but the contribution is solid.	This paper is technical sound, easy to understand, and satisfying in its novelty. Specifically, the authors analyzed the characteristics of fundus imaging and designed a cataract simulation model. In addition, the authors designed a feature alignment module for learning the correspondence between high frequency features and origin features, which is reasonable. However, the experiments are not sufficient, for example, some ablation studies are missing. Considering the above issues, I recommend to accept this paper.
483-Paper1454	Super-Focus: Domain Adaptation for Embryo Imaging via Self-Supervised Focal Plane Regression	This manuscripts presents a method for standardization and super-resolution (in the slice direction) of the human embryonic data. The authors present a method for simulating realistic-looking focal planes, that can be used both for generating missing planes as well as for upsampling the data via super-resolution. The methodology is simple but efficient and the validation is convincing.	Authors are suggesting a generative model approach for generating the missing focal planes of different domains in human embryo microscopy. 3 different generators receive two consecutive planes and generate the third one (one for each case, up, down and middle). An autoencoder extracts the features and reconstructs the input. A discriminator helps with adversarial loss and finally latent space feature representations from the generated image and its ground-truth are aligned with a self supervised loss.  Results are reported i) qualitatively ii) on embryo grading iii) on single cell segmentation	The authors propose a method for predicting missing slices in embryo imaging data sets. The model is trained in a self-supervised manner and evaluated on a large data set including some tests with four human raters. Multiple generator models allow predicting missing slices either below, above or between two existing other slices. While the methods don't seem novel per se, the application to this problem are reasonable and the results indicate benefits of using this additional super-resolution approach.	* Strong and complete conference submission: various data, clear methodology and presentation, convincing validation. * Simple but efficient methodology resulting in improved performance.	Relevant problem solved with an interesting combination of different methods. Results studied from various aspects (Embryo grading, cell segmentation, expert qualitative assessment) impact of the training dataset is shortly discussed	Nicely written and well-understandable paper that tries to solve an important problem. Self-supervised , i.e., no manual annotations required. Large training / test data set Several interesting ablation studies including the fact that FID apparently is more suitable to judge the realism of generated images.	* Some parts of this manuscript can benefit from more detailed explanation. For example, it is unclear how the decision on about which focal planes are missing (and need to be generated) is made.	Is Frobenius norm the best for L_per? Does a different loss such as cosine similarity make any difference? It was not studied if there is any domain shift between the datasets from different centers due to different settings of microscope. The paper only talks about the number of focal planes assuming different domain and centers produce identical images.	There's not much to criticize from my side but I had a few questions that may be worth answering in the paper: You only use D_A for training of your model. In Table 2 it seems that the model consistently performs worse on data sets C-E. Why not including a few images of these clinics as well to increase the variability seen during training? As it's a self-supervised approach it should be straightforward to include other training images as well. In Table 2, it also would have been interesting how human raters assess the unaltered images without any additional slices. Do complete images also obtain a score of 5 as would be expected? You mention several changes you made to the original U-Net architecture but don't explain why those changes were made. The same applies to the training: you state you train for 30 epochs but don't mention any criterion for stopping the training. Please comment. I did not fully understand how you know when to apply which of the generators. Are slices systematically missing, such that you could use the same generator for all data of a particular clinic? Or is there any other sophisticated way of identifying the missing slices? I do understand that predicting a slice between two existing ones might be reasonable and doable (similar to interpolating between the slices). However, I have some doubts that the predictions at the border regions (above / below the acquired stack) are guaranteed to reflect the reality. Is there any way how to assess the validity of the slices in these border regions?	All the methods developed in this paper are clear and valid. Also majority of the implementation details are properly described and values of all the parameters are reported.	Since all datasets are private, reproducing the reported results is not possible. Pretrained models are also not released. One can only get ideas from reading the text on similar datasets/problems.	Seems reproducible, given that the authors indeed provide the code in case of acceptance.	"It is unclear how the decision on about which focal planes are missing (and need to be generated) is made. This requires some sort of data alignment. From what I could deduce, this step was performed manually. In either case (if it was automated), this needs to be mentioned explicitly. It would be interesting to obtain more information about the acquisition hardware: same/different manufacturer, model, etc. Section 4.1. ""... 4 (uniformly) randomly selected planes ... "" I find somewhat difficult to interpret how selection can be random and uniform at the same time. Please clarify. From the description in Section 4.2 the reader can get an impression that the stacks were shown to the experts in this particular order: 50 real, followed by 50 simulated, and then 20 copies. Was this the case? Or were the stacks shuffled before being shown?"	Probably UMAP of the embedded features from the autoencoder can help us to have a better understanding if there is any domain shift or not.	In addition to the suggestions mentioned earlier, some minor comments: Table 4 appears before Table 3. Consider swapping the labels. Fig. 3: mention that within one panel the pairs are real/generated, respectively. Otherwise, one could think the two left pairs are real and the two right pairs are fake. Please carefully go through the references again. Some of them are incomplete or lacking page numbers and the like (e.g., 9, 12, 13, 32).	This is a good conference paper, with simple but efficient methodology, clear presentation and convincing validation.	They are suggesting a relatively simple method addressing dataset standardisation across different domains.	Solid conference contribution that solves an interesting problem, nicely written paper, reasonable selection of methods that are not new per se but used in a clever way. Quite some validations including multiple expert assessments and relatively large data sets.
484-Paper1684	SUPER-IVIM-DC: Intra-voxel incoherent motion based Fetal lung maturity assessment from limited DWI data using supervised learning coupled with data-consistency	The authors propose a deep learning based method for the estimation of IVIM parameters. The proposed approach extends the state if the art by basically combining two previous approaches, where one works with a supervised loss measuring the difference between the ground truth IVIM parameters of a forward model and the parameters predicted by the DNN on the basis of the signal simulated using said model, and the other one works with an unsupervised loss measuring the difference between the measured/simulated signal and the signal generated by the DNN. The authors perform experiments that are supposed to show that their extended approach yields more robust and accurate results as compared to a state of the art model.	Intravoxel incoherent motion imaging is an emerging MRI modality for the characterization of tissue microvascular perfusion and diffusion. Its clinical value is undoubtedly the ability to describe tissue viability and vascularity without the need for contrast agents, however, such measurements are often compromised by low SNR. The authors propose a DNN coupled with data-consistency term that may provide more reliable parameter estimates in low SNR settings. They test their hypothesis by undersampling a volunteer dataset. Furthermore, they evaluate the method in a challenging fetal MRI setting to characterize lung maturation. Compared to DNN based parameter estimation methods from the literature, the authors propose a supervised loss function coupled with a data-consistency term, which they hope to achieve more robust estimates in case of new data and in case of low SNR (fewer b-values). Their method yields lower standard error for the IVIM parameter estimates compared to the IVIM	This paper introduced SUPER-IVIM-DC, a DNN approach for the estimation of the IVIM model parameters from DWI data acquired with limited number of b-values. Their numerical simulations and healthy volunteer study show that SUPER-IVIM-DC estimates of the IVIM model parameters from limited DWI data had lower normalized root meansquared error compared to previous DNN-based approaches.	The method is sound and the experiments are suitable for showing the improved performance of the approach. The method has the potential of reducing required acquisition times for the MR signal, which is clinically relevant.	Novel concept for IVIM parameter estimation that would be applicable for the clinical routine since it needs fewer images to be acquired, saving some time	This paper proposed a data-consistency term that enables the analysis of diffusion and pseudo-diffusion biomarkers based on DNN. Their work demonstrated the added-value of SUPER-IVIM-DC over both classical and recent DNN approaches for IVIM analysis through numerical simulations, healthy volunteer study, and IVIM analysis of fetal lung maturation from fetal DWI data. This paper has clinical significance in fetal lung maturity analysis.	The authors provide quantitative results to show the superior performance of their approach but they do not perform a statistical analysis of any kind to prove the significance of this performance gain. As it stands, the presented results only show that their approach might be better but this can only be confirmed with a thorough statistical analysis. The correlation between f and the gestational age is pretty tenuous. I doubt that this relation is a useful indicator of clinical impact of the method. Maybe there are better measures relating the lung development to the signal/IVIM model parameters?	clinical scenario: the authors removed cases with motion, however, it is sure that the retained cases had some residual motion. It is also not clear what motion correction strategy they used. The low SNR of fetal MRI, particularly for motion-sensitive sequences like EPI, is often a result of spin dephasing. Therefore it is important to characterize residual motion and perhaps describe the reliability of the method as a function of motion or other artifacts the authors tested their study on one participant only. It is not clear how the network would behave on new data Fig 2. Is a bit misleading since the Y axis has been scaled and shifted in a way to exaggerate the magnitude of improvement: the NMSE of SUPER-IVIM-DC ranges from 0.26 to 0.3 while for IVIMnet is 0.25, however, in the plot, it looks like it is orders of magnitude better. The improvement for D is actually quite marginal.	DNN-based methods have formalized the IVIM model parameters estimation problem as a prediction problem, such as [3][4][11]. The problem formulation is not novel. The clarity of the paper is not very clear. There are many mistakes in expression.	The authors claim that their code will be made publicly available upon publication. The simulated data the authors are using is probably easy to reproduce. The authors do not mention that they are planning to publish the in vivo data used in their experiments, which probably makes the exact reproduction of their results impossible. AT least the experiments using healthy volunteers should probably be reproducible using own acquisitions though.	The authors provide sufficient details for reproducing their methodology, including a detailed description of the implementation of their code. While they pledged to make code available, they did not rely on open data, therefore a complete reproduction is not possible.	Their code and trained models will be made publicly available upon publication.	Apart from the points described above, it might be interesting to perform more extensive experiments and comparisons to other models, also classical non-DL methods. The authors only focus on one unsupervised approach.	"Thank you very much for this high quality submission. The authors find my comments in the ""strengths"" and ""weaknesses"" section."	"This paper proposed a SUPER-IVIM-DC to alleviate the need to acquire DWI data with a large number of ""b-values"" by constraining the DNN training process through a supervised loss function coupled with a data consistency term. However, the SUPER-IVIM-DC is similar to the published work IVIM-NET [3] [11]. Compared with IVIM-NET, the correlation results of SUPER-IVIM-DC has no significantly improvements (0.239 vs 0.242)."	I like the paper and the approach but without a proper statistical analysis it is hard to judge its real value. The paper would also benefit from including more baseline methods. This should also not be difficult to realize. The presented measure for clinical impact also seems not really useful.	The paper shows clinical value and it would be relatively easy for other groups to implement the method.	This paper focus on fetal lung maturity analysis based on DWI data. It has clinical significance in fetal lung maturity analysis.This paper proposed a data-consistency term that enables the analysis of diffusion and pseudo-diffusion biomarkers based on DNN. Their work demonstrated the added-value of SUPER-IVIM-DC over both classical and recent DNN approaches for IVIM analysis through numerical simulations, healthy volunteer study, and IVIM analysis of fetal lung maturation from fetal DWI data. However, the proposed model lack of novelty. It is an improvement of IVIM-Net. The improvement of comparison experiments is not significant.
485-Paper2448	Supervised Contrastive Learning to Classify Paranasal Anomalies in the Maxillary Sinus	The paper proposes to augment a cross-entropy based classification task with an adapted contrastive SimCLR loss which uses samples from the same class as positive pairs.	The authors in this work propose to combine the supervised contrastive loss with a cross-entropy loss for classifying paranasal anomalies in the maxillary sinus.	The authors propose a self-supervised SimCLR method to maxillary sinus classification in MRI images. They also conduct a population study - experiments on large number of patients with wide distribution statistics - a clinically important but rather rare contribution in MICCAI.	Nice addaption of supervised contrastive training for paranasal anomalies and combination with a CE-loss. Experiments and evaluation seem to be done well.	The paper is mostly well-written and clear. The method achieves good performance on the authors' private dataset. Most of the aspects are described in sufficient detail to enable the reproduction of results.	Self-supervised learning methods such as the SimCLR used in the paper is a high interest to the readers in the field. The paper does a good job in introducing the method to the clinical problem with good experimental results. The experiments are thorough and sufficiently backs the contribution. The visual examples are well illustrated. Limitations are clearly mentioned.	"Contribution is not major, more application of an existing method to a medical dataset. The presentation and structure can be improved and the Figures 2&3 are terrible. Minor: For [7] (citation/introduction of contrastive learning) please use the first original paper from  van den Oord and not a survey paper. For t-SNE please always report the parameters, otherwise the results are hard to trust (basically t-SNE is not a good way to show separability, since no separability for one t-SNE setting does not mean there is no separability, it's nice 'anecdotally' but does not show anything). Pg 5. ""F())"" there is a typo. I can just assume (knowing contrastive learning) the there also is a Proj_1 which is used for the contrastive task (Proj_1 is only briefly mentioned in the architecture but not what its used for)."	The novelty of the paper is not enough, which basically combines the supervised contrastive loss and binary cross-entropy loss. The experimental comparison lacks the current SOTA classification models. Thus, it is hard to justify the paper's contribution. It would be better if the authors can provide some results on public medical classification benchmarks	Experiments could have been better - it is possible that other methods could be comparable or even better than the proposed method. Showing scenarios where the proposed method really is more beneficial than the existing methods would be good. Fig. 3. could be better - more explanations on what are shown - are the hyper-parameters fixed as much as possible? What about the hyper-parameters for t-SNE? What are the number of samples for the visualization and how are they chosen?	Overall, most hyperparamters are given and the methods themselves are not novel. The dataset is not provided so its hard so reproduce the exact results, but extending them to different datasets should be easy.	The authors will not release their code.	The paper seems fairly reproducible - no code nor data is available but readers could try the suggested methods on their dataset of choice.	I think its a decent paper with a minor novelty and application to a new dataset. However I think the presentation is still lacking and can be improved. In particular IMO the introduction is a bit all over the place and the general structure/ clarity can be improved. Furthermore the Figures 2&3 should be revised (especially Figure 3).	Although the paper is well-written, the major novelty is trival.	It is probably early to conclude and generalize that contrastive loss + cross entropy loss is better for medical images and those with limited amount of labels. It'd be helpful to include one or more experiments with similar characteristics/problems for more generalizable conclusion. More examples and visualizations to help understand the benefit of the proposed method would be good. For example, in which cases do the methods fail and succeed compared to the others? Can we get some insights into why?	"With a top presentation I think the application & minor novelty would make me see this as a ""weak accept"" - ""accept"". However, given the current presentation I think it does not make the cut."	I appreciate the well-organised paper. I believe this paper can have good applicability potentials. But the novelty of this work is not good enough for MICCAI given that its a combination between existing works (See weakness). Also, the authors claimed that they will not release the code, which may cause reproducibility problems. Hence, I will suggest a reject score.	The paper is clearly written and well structured. The claimed contribution is clear and experiments are done well. Nonetheless, the claimed contribution does not stand out strongly - neither based on the conducted experiments nor by the theoretical analysis.
486-Paper1659	Supervised Deep Learning for Head Motion Correction in PET	In this paper, the authors proposed a new deep learning-based method to correct the head motion and diminish the artifacts and quantification errors in PET imaging. This is supposed to be a good start on algorithm-based motion correction in PET imaging. However, the algorithms can't be perfect. The experimental results also confirmed this. I was wondering if the inaccurate motion correction is acceptable in clinical use? The results showed that the predicted correction sometimes even caused higher errors.	This work attempts to do a very challenging task of learning 3D rigid head motion from highly noisy data. A motion tracking system (Vicra) is used as the learning target. To achieve this, the authors designed a neural network based model which uses encoders on two input images at different time points, and a FWT unit that operates on the difference of these two images.	The paper introduces a deep learning approach to real-time motion tracking based on PET data. The motion tracking results are validated in a single-subject and multi subject experiment and against an external tracking device, the Polaris Vicra camera. This aproach to motion measurements is very interesting since it is a big problem in PET acquisitions due to their much longer acquisition times than e.g. CT or MRI.	In this paper, the authors proposed a new deep learning-based method to correct the head motion and diminish the artifacts and quantification errors in PET imaging. This is supposed to be a good start on algorithm-based motion correction in PET imaging.	This paper aims to address a very challenging task, i.e. to estimate motion on a second-by-second basis from noisy low resolution PET data. I have not found previous work which this paper makes incremental extension from. The transparency of including all kinds of results, like in Fig2 is valued. The authors discussed the limitations of the current results thoroughly.	The approach is methodologically interesting in 2 ways 1) Instead of using the PET image sin image space and trying to register them, the 1-second frames are back projected along the LOR and used to construct a point cloud. This seems to make it much easier to register, since a regular 1 second PET image, not even FDG, would have enough contrast to else register it. 2) The motion tracking is properly evaluated against an external tracking device, the Polaris Vicra. This is very nice and the proper way to do it instea dof only assessing motion improvements visually or image based as is often the case. Furthermore, the paper addresses an important clinical issue, motion degradation due to motion in PET acquisitions and provides a solution that is feasible to implement without acquiring an external tracking system.	"Language problems need to be checked, many long sentences are not clearly expressed and are confusing. The experiments section needs more clarifications: The subject group contains patients with normal cognition, cocaine dependence, and cognitive diseases. I would like to know if any strategy was used in splitting the training and test sets? For example, keeping a diversity of patient types in each subset. In the ablation study (Table 1), the test MSE value for ""more data, FWT, Deep Encoder and Normal sampling"", which is located in the second row, has a much higher value than the others. Can the authors explain this? The results In fig. 2(a) are for subject 1. Also, there is another subject 1 in Table S1. I assume they are different subjects since one is for single-subject experiments and one is for multi-subject experiments. But referring to them the same name still causes confusion. In section 3.2, the authors claimed that subject 2 has a mean MSE of 0.02. But Table S1 shows the mean MSE of subject 2 is 1.114. They are contradictory. I assume they are also different objects as I mentioned above. The confusions need to be addressed. ""The results for Subject 2 (mean MSE 0.02) show that the network is capable of accurately predicting motion from training subjects even though the motion relative to the reference frame was never used for training."" Does this mean that in the experiment in figure 2, the moving images of subject 2 have been resampled and different from the images that are used during training? The detail of the experiment settings needs to be clarified."	"The definition of the ""3D cloud representations of the PET data"" is not very clear. From the paper it looks like a back projection image, but why is it called 3D cloud data? Does it have anything to do with point clouds? Also not clear if the 3D cloud images are attenuation corrected? For head motion, the non attenuation corrected data can be beneficial because there are quite strong signals near the skull. It is not clear to me why the outputs of the encoders and FWT were multiplied, why use this way to combine the two sets of features? The results don't seem very convincing for demonstrating the performance of motion estimation using the proposed method, as shown in Fig2. It is indeed a very challenging task, and it will be very helpful to know what the DL-HMC network is doing. The ablation analysis included is appreciated."	The clearest drawback is that the experimental tests are not so extensive, but considering the novelty of the approach, I would say that is acceptable. There are some minor inconsistencies, e.g. wrt acquisition time, in the methods which I will go into in detail below.	"Language problems need to be checked, many long sentences are not clearly expressed and are confusing. The experiments section needs more clarifications: The subject group contains patients with normal cognition, cocaine dependence, and cognitive diseases. I would like to know if any strategy was used in splitting the training and test sets? For example, keeping a diversity of patient types in each subset. In the ablation study (Table 1), the test MSE value for ""more data, FWT, Deep Encoder and Normal sampling"", which is located in the second row, has a much higher value than the others. Can the authors explain this? The results In fig. 2(a) are for subject 1. Also, there is another subject 1 in Table S1. I assume they are different subjects since one is for single-subject experiments and one is for multi-subject experiments. But referring to them the same name still causes confusion. In section 3.2, the authors claimed that subject 2 has a mean MSE of 0.02. But Table S1 shows the mean MSE of subject 2 is 1.114. They are contradictory. I assume they are also different objects as I mentioned above. The confusions need to be addressed. ""The results for Subject 2 (mean MSE 0.02) show that the network is capable of accurately predicting motion from training subjects even though the motion relative to the reference frame was never used for training."" Does this mean that in the experiment in figure 2, the moving images of subject 2 have been resampled and different from the images that are used during training? The detail of the experiment settings needs to be clarified."	The authors have given enough information to reproduce the work.	So there is no code that is shared, at least I cannot see a link to the GitHub repo. The MOLAR reconstriction is accessible in principle, but is not easy to get to run, not even if one also has an HRRT scanner. Hence, the reconstruction of images is probably hard to redo. The data can probably not be shared, since it is clinical data.	"Language problems need to be checked, many long sentences are not clearly expressed and are confusing. The experiments section needs more clarifications: The subject group contains patients with normal cognition, cocaine dependence, and cognitive diseases. I would like to know if any strategy was used in splitting the training and test sets? For example, keeping a diversity of patient types in each subset. In the ablation study (Table 1), the test MSE value for ""more data, FWT, Deep Encoder and Normal sampling"", which is located in the second row, has a much higher value than the others. Can the authors explain this? The results In fig. 2(a) are for subject 1. Also, there is another subject 1 in Table S1. I assume they are different subjects since one is for single-subject experiments and one is for multi-subject experiments. But referring to them the same name still causes confusion. In section 3.2, the authors claimed that subject 2 has a mean MSE of 0.02. But Table S1 shows the mean MSE of subject 2 is 1.114. They are contradictory. I assume they are also different objects as I mentioned above. The confusions need to be addressed. ""The results for Subject 2 (mean MSE 0.02) show that the network is capable of accurately predicting motion from training subjects even though the motion relative to the reference frame was never used for training."" Does this mean that in the experiment in figure 2, the moving images of subject 2 have been resampled and different from the images that are used during training? The detail of the experiment settings needs to be clarified."	I wonder if the authors would like to explore the low number of degrees of freedom in the rigid body motion problem. The proposed network uses a lot of parameters and dense connections, which are useful for dense prediction problems. But the 3D rigid motion has 6 parameters after all. For example, the 3 translation parameters can be estimated from the difference in the centre of mass, if the centre of mass can be properly estimated by a neural network. Also I wonder if the performance of the proposed model can be easily improved by doing like one MLEM step to replace the 3D cloud images in Fig1 as the input. As far as I remember, the first few iterations of EM give more or less what looks like the brain without the details. This is of course at the cost of time in preparing the input data, but it may be a worth trade-off. Typo: Page 3: 'algorithn'	"Abstract: The authors state ""However, to date, there is no approach that can track head motion continuously without using an external device."" Well this depends on how you define continuously and if you only speak for PET. In MRI, 3D Navigators can be placed in a regular MPRage sequence every TR, so giving you a head position approx. every 2 sec, hence you should rephrase this You use the Polaris Vicra, but throuhgout the paper there is no comment regarding if the Polaris has been validated wrt attenuation issues in the field of view of the detectors, so to speak are you acquiring worse image swiht the tracker on? You explain already in the abstract that ""the output [of your DL method] is the prediction of six rigid transformation motion parameters."". Also later on this is stated, but in whcih coordinate system are these motion parameters reported? In scanner coordinates or in Polaris position coordinates? I am assuming in scanner coordinates, but that information is completely missing. Intorduction: I really like the introduction and the emphasis on the clinical application. You state "" average head motion can vary from 7 mm [1] in clinical scans to triple this amount for longer research scans."". As someone who regularly works with 90 or 120 minute PET research scans, I find especially the seond statement hard to believe. I would tone that down. Regarding the usage of tracking devices the drawbacks you mention are ""HMT is not generally accepted in clinical use, since it usually requires attaching a tracking device to the patient and additional setup time."" Well, by now there are also several mrtkerless tracking solutions on the market even MR compatible ones and setup times are not an issue anymore. So I would rather put the emphasis on extra cost of acquisition or on the capabilities of actually running event-based reconstruction methods based on high frequency external tracking, especially in the HRRT setup that due to its geometry restricts which reconstruction algorithms can be used. I also disagree wiht the statement ""Other systems like markerless motion tracking [11] are still under development and have not been validated for PET use"". That's simply not true anymore, see e.g. https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0215524 Methods: Section 2.1 I have a comment on the choice of tracer. While FDG is by far one of the most clinically used tracer, it is also rather stable and one cannot compare kinetics to e.g. C11-based tracers for dopamin or serotonin. So while your population is interesting and diverse, their FDG images will still look very similar, I guess. I ma a bit confused about the measure of motion. You state ""The average intra-frame motion of eight points forming the vertices of a cube [7] was used to summarize (mean+-SD) the overall motion of the brain throughout the entire scan to be 12.07+-7.12 mm."". Could you add an illustration of this? Is the cube located in the center of the head or center fo the field of view, I am a bit confused about this. Also an average of 12 mm over the acquisition across 25 subjects seems to be really large to me. I think there is a typo in the description of the acquisition. You write ""All PET imaging data is 30 minutes acquired 60 minutes post injection."" This I would understand as you having a 30 minute acquistion, aka 1800 1-second frames. But below you keep using 3600 frames. So I am assuming that you switched the two numbers around and that it should say ""data is acquired for 60 minutes 30 minutes post injection"". Also what is your regular framing for the 60 minute scan? 5 or 10 minute frames? Section 2.2 You state ""The encoders effectively reduce the 3D image data volumes down to a vector of size 128."". That reduction seems really extreme considering the 6 degress of freedom you are trying to measure. Did you try with a larger input, e.g. by using a different network for pre-training? Can you confirm that t_ref is the last second/1-second image in the whole scan? As stated already in the intro, how was the HMT tracking data transformed into the 3D cloud, aka PET scanner frame of reference coordinate system? Is that cross-calibration provided by the HMT system, so all tracking is already provided relative to scanner coordinates or was this transform performed by you? Section 2.3 This section confused me even more regarding to what the reference time is, is it now always the last timepoint or does it vary? ""we calculate the relative motion transformation matrix from the Vicra data"" I guess, here I would like more details and maybe a reference to understand it Section 2.4 Here you introduce t_ref = 3600 which does not match with an image acquisition of 30 minutes Results I guess what you call theta is often referred to as RMS in the MRI motion correction community, see e.g. https://onlinelibrary.wiley.com/doi/full/10.1002/mrm.27705, but that is calculated wrt a point of reference, e.g. a point cloud center when using a markerless tracker or center of the marker when using a marker-based tracker. What is your point of reference? Just as a side note, when you are the only PET site in the world with an HRRT and using the MOLAR reconstruction, then anonymization kind of goes out the window if the reviewer has decent PET expertise. Regarding FreeSurfer by now you probably want to replace reference 4 by Fischl B. FreeSurfer. Neuroimage. 2012 Aug 15;62(2):774-81. Section 3.1 Your results on motion prediction where you state that ""accurate motion prediction performance error to be 0.035+-0.073 (mean+-SD of corresponding dataset)."" is quite impressive. But what was the overlal range of motion for that single subject? Can you add that? Section 3.2 Figure 2, these plots are way too small, try and make axes descriptions only once and enlarge the plots. Also please comment in the caption on my question in which coordinate system this is shown. Again when you state results for the other subjects such as "" Mean MSE for Subject 3 is 0.74 and for the failure case in Subject 4 is 6.33."" Please also give the overall range of motion for these subjects over the scan as reference. Discussion You write ""While our initial model results indicate capabilities of predicting motion of magnitude ~1mm, our current pre-processing reduces input image resolution to ~10mm3, which may limit the model's ability to detect motion with smaller magnitudes."" But as a common cutoff for motion when deciding whether to do motion correction of the PET data is 2-3 mm, this is not really useful in practice. can you comment on that?"	"Language problems need to be checked, many long sentences are not clearly expressed and are confusing. The experiments section needs more clarifications: The subject group contains patients with normal cognition, cocaine dependence, and cognitive diseases. I would like to know if any strategy was used in splitting the training and test sets? For example, keeping a diversity of patient types in each subset. In the ablation study (Table 1), the test MSE value for ""more data, FWT, Deep Encoder and Normal sampling"", which is located in the second row, has a much higher value than the others. Can the authors explain this? The results In fig. 2(a) are for subject 1. Also, there is another subject 1 in Table S1. I assume they are different subjects since one is for single-subject experiments and one is for multi-subject experiments. But referring to them the same name still causes confusion."	This work aims to learn the rigid head motion second-by-second from dynamic PET image data, which is very challenging given the low quality image input. The authors demonstrated the results with transparency. There could be a few things the authors could try to improve the performance, such as finding computationally efficient ways to improve the input image quality, or exploring ways to reduce the data complexity for the low number of parameters in this problem.	This paper covers a very important clinicla issue and comes up with a nice methodologicla solution using state of the art computational approaches both wrt the learning framework as well as in general wrt the reconstruction framework and is teste don the state-of-the-art scanner, an HRRT.
487-Paper1418	Suppressing Poisoning Attacks on Federated Learning for Medical Imaging	"In this paper the authors propose a new method for federated learning that aims to address ""poisoning attacks"", ie. considers that some of the nodes contributing to the federated learning network are malicious. The proposed method is based on computing distances between the parameters that are communicated by each node and then weights them according to a copula-based outlier detection method. Empirical results illustrate the usefulness of the proposed method in two datasets."	The paper propose a general aggregation rule of federated learning. The proposed method is technically sound and simple. The experimental results show the effectiveness of DOS against several types of attacks.	This paper proposes a Federated Learning framework with Distance-based Outlier Suppression (DOS) based on Euclidean and Cosine distances and a softmax operation with temperature for tackling client poisoning attacks. The proposed method is evaluated on two medical imaging datasets on classification tasks and achieve improved performance over previous methods.	Well motivated and presented paper Places work in context to other works and compares against them in empirical results	The key strength of this paper is developing a robust aggregation rule for FL against attacks from malicious clients. The aggregation rule employs a parameter-free outlier detection algorithm, namely COPOD, to detect abnormal values among the distance metrics of model parameters between the clients and the global model.	The proposed method achieves improved results over previous methods indicating the effectiveness of the method. The method leverage two distance metrics to achieve robust distance and outlier measurements and could be extended for other distance metrics. Rich experiments are conducted. Visualizations shows clear model performance under different situations.	The limitations of the proposed framework are not sufficiently discussed. Why is it necessary that the proportion of clients experiencing byzantine-failures is less than 50%? There is not sufficient discussion, or proof, to justify this. The discussion about limitations should also include distribution of data/classes into the different nodes. ie. in the non-iid data distribution case, what are the limitations depending on how the data and classes are distributed? E and C groups (referenced in some parts of the paper, like Algorithm 1) should be more clearly defined through the text.	Some key references and baselines are missing, e.g, RFA (Robust Aggregation for Federated Learning) and SparseFed (SparseFed: Mitigating Model Poisoning Attacks in Federated Learning with Sparsification). Two differences between the proposed method and existing methods are: 1) using COPOD to score the clients, and 2) performing scoring on a distance space instead of the parameter space. The implementation details are well-described but lack theoretical analysis. Despite the variety, the experiment settings are not comprehensive and the evaluations are less insightful.	The authors use -1 as temperature parameter in softmax computation to reduce client weights with higher outlier scores and increase client weights with lower outlier scores. Though -1 is a feasible approach, it changed the original relative distribution of the learned outlier scores from the exp function. It may be better to keep the relativity for easier learning of the network. The fonts in figures should be increased. Now it is so hard to see clearly. Why the authors assume 40% of the clients are malicious instead of, say, 50%, 60%, 100%?	It seems feasible to reproduce the results with the provided information.	The implementation detail of the proposed method is well presented, as well as the experimental settings.	N/A	This is a well motivated and presented paper that proposes a new method for federated learning that aims to address poisoning attacks. The work is discussed in context to related work and empirical evidence is presented to support the usefulness of the proposed method. The limitations of the proposed framework are not discussed. There is a claim that the proposed method works when the proportion of clients experiencing byzantine-failures is less than 50% but this is not sufficiently justified. Moreover, there is no discussion about the proposed methods limitations with respect to the data/class distribution among the nodes that are participating in the federated learning network. These issues make it hard to assess the practical impact of the proposed method.	Federated learning is indeed a good solution to enable a multi-center collaborative training process. However, in this paper, the only connection between medical images and FL is the dataset employed in the experiments. My suggestion is to change the motivation, making it closely related to the clinical aspect. For example, the authors can provide some specific attacks that commonly appeared in FL with medical images, then analyze why existing techniques will possibly fail in such cases. Compared with SOTA works in this area, a major issue of this work is the lack of theoretical analysis. It would be better to discuss why the parameter-free algorithm is robust in abnormal detection, and why the distance space is more appropriate for detecting malicious clients. To make the experiments more comprehensive, it would be better to analyze the performance under the different percentages of malicious clients (e.g., 10% to 40%) and different numbers of clients (e.g., up to 30 clients). Moreover, methods like RFA or SparseFed should be included as two strong baselines. The current version of Section 4.4 Results and Discussion is not very insightful, which only demonstrates the good performance without discussing why baseline methods fail and why the proposed method works well.	Please refer to the comments.	Well motivated and presented paper with empirical results comparing to other approaches. Very little discussion about the limitations of the proposed method.	The paper proposes an effective aggregation rule for FL learning, and the experimental results support some of the authors' claims. However, this work may bring limited contributions to the MICCAI community. The proposed method is developed on the existing FL framework and changes the client scoring function by using COPOD. Although the performance is good, the methodology lacks theoretical analysis. this work focus on an important research topic, Federated learning, but has a relatively limited impact on medical images and may be interested in a narrow group of MICCAI audiences. It would be more interesting and impact to enhance the connection between the proposed method and medical images. Therefore, I think the weakness of this paper slightly weighs over its merits, and I suggest a weak reject.	Overall, I think this is an interesting paper with clear presentation. I hope the authors could address my questions.
489-Paper2739	Surgical Scene Segmentation Using Semantic Image Synthesis with a Virtual Surgery Environment	The paper performs extensive sets of experiments for surgical scene segmentation with real and synthetic images. Synthetics images are generated from an advanced surgery scene simulator. A large-scale segmentation dataset is also released.	This paper propose a surgical image sythesis pipeline, which contains a complex virtual surgery environment, class-balanced frame sampling, domain randomization and semantic image sythesis. The authors contribute a large-scale surgical image segmentation dataset with both real and sythetic images, which can be used for visual object recognition and image-to-image translation research for gastrectomy with the dVSS. The effects of synthetic data between tasks, models, and data are analyzed with extensive experiments.	The authors propose a synthetic data generation framework for generating synthetic data for minimally invasive surgical scene segmentation. The scene is rendered in the Unity engine as segmentation mask from which photorealistic images are generated using two different semantic image synthesis GAN-based methods. All related datasets will be published. A comparison of state-of-the-art segmentation models is performed.	Extensive experiments on instance and semantic segmentation with different models and different combinations of real/synthetic data; More sophisticated surgery simulator with more organs and tools; The first large-scale dataset for surgical scene segmentation, also tackling class imbalance, is released.	This work demonstrate the feasibility and effectiveness of surgical image sythesis based on complex virtual surgery simulation, which is valuable and inspiring for the surgical vision field. Systematical experiments are carried out with various configurations and different models. The dataset will be released to public, and can be used for both semantic and instance segmentation.	The datasets and synthetic data generation pipeline could be valuable for the community. The presented approach for advanced training of surgical scene segmentation models is interesting and promising and the implementation and considerations of the authors for dataset generation are thorough. The authors compare state-of-the-art models (ResNeSt, Swin Transformer,...) on the proposed  datasets and perform an extensive evaluation.	I don't have any complaints.	The paper is technical and experimental. The scientific insights and methodological innovation are kind of lacking. The paper is wordy and a little disorganized. Better figures and clear layout are expected. Section 3 could be extended for more informations.	It's not clear where the authors see the novelty of the presented work. Is it the datasets, the synthetic generation method, or both? The authors use out-of-the-box methods for semantic image synthesis and segmentation. The novelty of the work is limited to the dataset and the synthetic pipeline which could be valuable for further research and other clinical applications. There, the authors should publish the synthetic data generation method which could be transferred to other applications. For models that perform already very well, the generated synthetic data does not improve the results.	It looks reproducible.	The dataset will be released to public on their website.	The authors promise to publish the real and synthetic datasets, as well as the baseline segmentation models for reproducibility. However, a fully functioning published version of the synthetic data generation pipeline would be valuable for the community to transfer the method to other domains and clinical applications	Plots could be more interpretable than the tables but I guess it might be a bit tricky to convert them into plots.	The first figure on https://sisvse.github.io/ is much more informative than Fig. 1 in the paper. A table or paragraph introducing the main features of the dataset could be added, for example, total image numbers, organ types, instrument types, min/max instrument number in a frame, etc. Thus, the readers can understand the value of your contribution.	What does X and O mean in table 1? A better notation could improve the clarity of the table. Why do the authors choose to do class-balanced frame sampling? Surgical scene segmentation is an imbalanced problem by nature and data imbalance can be handled by the framework (focal loss, data augmentation, ...). Class-balanced frame sampling throws away a significant part of the data that could be used to improve the results. This should be at least discussed in the paper. The authors should not only publish the datasets but also the synthetic data generation pipeline which could be transferred to other (medical / surgical) applications. Were the medical professionals trained before creating the manual virtual synthetic data in unity? How do the authors make sure that the simulated execution resembles reality? The tables on page 7 are cluttered, maybe it would be helpful to only present the most relevant results in the paper.	The merits of the paper include introducing a more complex surgery simulation, generated synthetic images from it and real dataset with more classes of organs and surgical tools. Extensive experiments with different combinations of dataset, e.g. real, synthetic, real + synthetic, etc, were performed, comparing against several previous methods for instance and semantic segmentation. Moreover, the new public dataset, which also addresses class imbalance problem, made the paper quite strong.	This work is practical and solid. A novel large dataset is built and will be released to public. My major concerns include: What are the most important insights from surgical synthetic data generation and application? The authors should emphasize this. This paper lacks theoretical contents. So the experimental insights are required. In Table 2, the improvements given by synthetic data seem not significant. In Table 3, the relative mIOU increments are very significant. How do the authors explain the differences between the benifits for overall performance and class-wise performance.	The novelty and contribution of the presented work mainly lies in the published dataset which might be not as competitive in comparison with other MICCAI submissions.
490-Paper0951	Surgical Skill Assessment via Video Semantic Aggregation	The paper describes a method for analysis of videos to assess surgical skill. The network architecture includes a semantic grouping module that uses clustering of local semantic features.	This paper proposes a new framework called ViSA to predict the skill of surgical videos by discovering and aggregating different semantic parts. The framework has been compared to previous work and gets competitive performance on two datasets: JIGSAWS and HeiChole.	The authors propose a novel framework called ViSA which predicts addresses the problem of automated skill assessment in surgical videos. The authors state that the state-of-the-art models often do not capture semantic information as they often employ CNNs for short-term feature extraction and temporal aggregation networks (e.g., LSTMs) for long-term relationship modeling. They claim that global pooling over the spatial dimension on CNN features ignores semantic variance of different features. They propose to instead discover and aggregate different semantic parts of the surgical setting across spatiotemporal dimensions.	The method includes a data-driven approach to emphasize relevant semantic information for skill assessment. While the experiments illustrate instrument motion as the semantic information, the introduction claims that the method can isolate information beyond the instrument. The findings are expected, reiterate current understanding of relevance of information in video images for surgical skill assessment. The method is evaluated on two datasets.	The idea of this work is interesting. Without supervision it can find different sementic parts in the surgical videos. The visualization results confirm this. This is helpful to discover structures from complicated surgery scenes.	The authors' motivation to address the shortcoming of state-of-the-art models by discovering and aggregating different semantic parts of the surgical setting across spatiotemporal dimensions is an interesting approach. As the authors cluster spatial features, no supervision is required to discover these semantic features which are expected to belong to different surgical elements such as tools, the tissue, and the background. However, a supervised version is also proposed. This supervision is achieved with kinematic data. They experiment on the commonly used JIGSAWS, and another dataset HeiChole, which is in-vivo, therefore a more realistic one. The method proposed is able to achieve an improvement over the state-of-the-art models. The organization and the flow of the paper is good, as well as the technical writing.	The evaluation metrics are not acceptable, despite citing previous works that used correlation as a metric. Correlation is perhaps one of the less relevant measures when evaluating model predictions. Mean absolute error is ok, but not sufficient. I don't mean to undermine prior work on the JIGSAWS dataset that is listed in Table 1, but the sub-par choice of evaluation metrics by the community was likely because of insufficient input from collaborators with statistical expertise. There are no measures of variance, which makes the claims made in the paper unclear to me. Many of the claimed improvements may be within what is expected from sampling variance.	"(1) The writing needs improvement. For example, Fig. 3 in Sec. 3.3 mentions that ""SGM facilitates the concentration on the task-related regions such as tools and discarding the unrelated background regions."" However, it is not clear how the conclusion was obtained. Does it mean that the authos conduct an comparision experiment that Grad-CAM combined with and w/o SGM? (2) As for temproal context modeling, have the authors considered other models besides LSTM and BiLSTM? (3) As for the number K, besides the ablation study on K=2,3,4, is there any more analysis and consideration for the choice of K? For example, does it depends on the scene complexity of the surgical videos? or sugery types?"	The inverse kinematics are used as supervision in the Semantic Grouping Module (SGM), however, inverse kinematics are not sensitive enough to guide this process, and it can in fact be observed that the tool positions are not accurate, nor precise in the Fig. 4. What I understand is that the authors use inverse kinematics as to provide somewhat close to accurate supervision, and it seems to improve performance. No supervision is required to discover semantic features which are expected to belong to different surgical elements such as tools, the tissue, and the background. While this is interesting, it is achieved by clustering, and the number of the clusters is chosen somewhat arbitrarily with the belief that they will corralte to the surgical elements. In Fig 4, for example, we see that there is only a loose correlation.	Measures of variance are missing.	The reproducibility of the paper looks fine.	The authors state that they will release the source code upon publication. They experiment on publicly available datasets: one of them is the commonly used JIGSAWS with predefined experimental setups and the other one is HeiChole, for which the authors include some details to their experimental setup.	i) What is the architecture of the transformer for which findings are reported in Table 3? ii) What do we learn about the semantic groups in the HeiChole dataset? Do they emphasize only the instruments, how consistent are the groups based on visual examination?	Please see weakness.	A discussion on using inverse kinematics for supervision while they do not provide accurate or sensitive enough supervision could be discussed. It is interesting that the proposed model uses clustering to aggregate semantic features, therefore does not need supervision. However, a note on how the K is somewhat arbitrarily chosen (with a belief that these features will relate to tools, tissue, and the background) should be added and discussed in the light of the resulting clusters which only loosely relate to these elements (Fig 4).	Measures of performance are insufficient; measures of variance of estimates are missing. The method is interesting and a variant of existing methods that illustrate the relevance of known information.	Overall this is an interesting paper. Both the idea and experimental results are attractive. However, there is still concern about the writing and analysis of experimental resutls.	The authors' motivation to address the shortcoming of state-of-the-art models by discovering and aggregating different semantic parts of the surgical setting across spatiotemporal dimensions is an interesting approach. As the authors cluster spatial features, no supervision is required to discover these semantic features which are expected to belong to different surgical elements such as tools, the tissue, and the background.
491-Paper0916	Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer	This paper introduces a VisualBERT and ResMLP based model for Surgical-VQA. It is evaluated on answer classification and sentence generation tasks on three datasets and achieve improved results over previous methods and the baseline method. Visualizations on the generated words/sentences are presented. Ablative studies are presented.	This work proposes to use VisualBERT[15] together with ResMLP[24] to approach the task of surgical VQA. This paper presents results in three datasets: Med-VQA, Endovis18-VQA, and Cholec80-VQA. Furthermore, it reports an ablation study on the proposed architecture.	The authors propose to use a deep learning method to answer the potential questions from student conditioned on the surgery video. It utilizes the Bert-based model to take visual tokens and text tokens in the feature extraction process, and apply conventional transformer-based decoder to predict the output answer. On this paper the RESMLP is proposed to enhance the representational ability of the transformer encoder. It also proposed an extension version of dataset based the EndoVis and Cholec80, including the questions and the answers. Finally, it has compared to the state-of-the-art model in Cholech 80 dataset.	The introduced model is evaluated on three datasets and achieves promising results. Rich experiments are conducted on two tasks as well as module ablations. Leveraging VisualBERT and ResMLP on Surgical-VQA is novel and is shown effective over the previous methods. This helps promote related work with similar data scale and requirements. Ablative studies on temporal visual features are also given which is helpful for related video tasks.	The task of surgical VQA is relevant to the medical image analysis community. The paper is easy to read. This work reports a complete ablation study over the model hyperparameters.	The main strength of the paper is the novel application, visual questioning answering to release the burden of clinical experts. It introduces the Visual Bert and its variation ResMLP to achieve cross-token and cross-channel fusion during the text-image feature extraction. Secondly is the newly proposed dataset in Cholec80 with classification-based and sentence-based VQA dataset. Finally, the proposed method is compared with state-of-the-art methods and achieves the promising results.	The novelty of the proposed VisualBert ResMLP is limited considering it as a combination of VisualBert and ResMLP modules. VisualBert ResMLP seems to achieve very close performance to VisualBert alone in Table 1 and Table 2. This can also be seen from Figure 4 (c). This makes me wonder the effectiveness of combining the ResMLP module. Table 1, the Acc performance of MedFuse is different from the results in the original MedFuse paper. Can the authors explain more on this? How long does it take and what are the GPU requirements to conduct this experiment? The font size should be increased for better visibility in Figure 2, 3, 4.	The principal weaknesses of the paper are listed below: This work does not present technical novelty. All the individual components used in this work already exist and are already implemented. This paper presents just a compilation without almost any modification. Specifically, the main architecture was taken from [15], and it uses ResMLP from [24]. The improvements by incorporating the ResMLP into the main architecture are marginal. The reported results do not demonstrate the required empirical contribution of the paper. The experimental setup does not allow an assessment of the model generalization capacity. Optimizing the architecture over the test sets might result in overfitting in the three benchmark datasets. Without results in an independent set of data, it is not possible to discard that possibility. The data used is not public. According to the reproducibility checklist, the data will be released upon acceptance. However, within the text, there is no clear intention to make it publicly available or as a relevant contribution to the paper. If the data were not released, it would limit the reproducibility of the results and the progress in this task. Considering that the paper provides ablation experiments, these should be performed on the validation set to guarantee its generalization.	-For the ResMLP, the figure illustration and the equation (1) (2) are so general that the input shape, output shape and the processing is not well explained. -Computational resource, the transformer structure is a high space-complexity method, it has the fundamental disadvantage to handle long sequences. It should be discussed the computational resources used to train & test the model. Also, length of input sequence should be clarified. -ResMLP: The effect of ResMLP seems to be minor according to the tables 1.	The authors provide codes and documents.	According to the reproducibility checklist, the source code and pretrained models will be made publicly available, which is essential to guarantee the reproducibility of the results. Additionally, the method was developed using a public benchmark dataset for surgical VQA, which promotes research in the area. Despite not including any intention to publicly release the data in the main text, the reproducibility checklist mention that they will be released upon acceptance. Furthermore, there should be included more statistics about the new data. How were these annotations generated? This information will be valuable for new research on this task.	According to the materials, the paper is reproducible.	Please refer to previous comments.	The submission instructions must be followed correctly. The paper ID must be added at the beginning of the submission. The legends in Figure 4 are not consistent with the reported results in Tables 1 and 2. How is the annotation generation process performed? Is it a completely automatic process? Do the questions and sentences follow natural language templates? It is not clear why the performance drops when using temporal features. Are there any additional insights?	-Except for the weakness, I think the author can give more details about the motivation behind this VQA task as well as its clinical benefits. For now, the system can only act as a second option to roughly clarify student's confusion. I am wondering if there is any intra-operative applications that could use this system. -The effect of the decoder is unknown. As far as I am concerned, the transformer decoder, unlike the auto-regressive decoders can predict sentence at once. Can you give more explanation about the decoder's choice and can we replace this decoder with the other methods, such as LSTM, GRU? -Also, will you public the dataset? -Can you compared to the other captioning methods?	Overall, I think this paper is an interesting paper proposing a BERT based framework on Surgical-VQA task. I hope the authors can address my concerns especially question 3 in the 'Weakness' and I'm willing to raise my score.	Although the task of surgical VQA is relevant for the medical image analysis community, the lack of technical novelty and the marginal improvement of the final model significantly affect the paper's main contributions. Additionally, it is unclear whether the data will be made publicly available upon acceptance. This work requires further modifications before being accepted.	High performance compared to the baseline model. The new caption dataset.
492-Paper1119	Survival Prediction of Brain Cancer with Incomplete Radiology, Pathology, Genomic, and Demographic Data	This work introduces a two-stage deep learning approach that integrates multiple data modalities in the presence of missing data. In addition, the authors performed a detailed ablation and comparison study to justify the importance of each module, and the improvements over baselines, respectively.	The manuscript introduces an approach to predict patient survival based on multi-modality data which includes MRI scans from 4 sequences, histology images, genomics data, as well as demographic data. Three different multi-modality data fusion methods are compared, in which modality dropout was introduced to simulate the scenarios when certain data modalities were missing. Also assessed were whether reconstructing the missing modality could improve the prediction or not. Experiments were carried out on a combined public dataset. An ablation study was conducted to evaluate the effect of all the features introduced to the model.	This paper tackles the problem of effectively utilizing multi-modality data in the presence of missing modalities. In particular, the authors ask the question of how to effectively predict survival in glioma patients when some modalities might be missing for patients. This is a common setting in real life. To solve this problem, this work proposes to aggregate individual modality feature embeddings using the mean vector which can be decoded to obtain individual modalities. The resulting mean vectors is used for predicting survival.	The significant contribution of this work is that it provides a comprehensive study of multimodal fusion for survival prediction. It gives a detailed guideline for the readers to integrate multiple data modalities in the presence of missing data. In addition, the modeling components are straightforward and well established in the field, which makes this model generalizable for solving other problems. The model contains two key components, the first one is a uni-modal feature extractor module, and the second one is a fusion module. The unimodal feature extractor module learns a lower-dimensional representation of the input data, and the fusion module combines the learned features and extract discriminative patterns which improve survival prediction. The authors use well-established models to build the unimodal feature extractor module. This allows them to use pre-trained weight, which reduces computational complexity. In addition, due to this choice, the authors can combine the power of multiple state-of-the-art models already proven to be robust for representing imaging, genomics and demographic data. The fusion module contains a two-layer MLP that further projects the output of the unimodal feature to a latent space where a mean vector is calculated for data fusion. The authors take a unique approach to dropout modalities, enabling the model to learn discriminative patterns from the rest of the data when a modality is absent. This approach boosts their performance, and they can take advantage of a larger dataset. The authors have provided a detailed study explaining their choice of optimization strategy. They compared their model by training on complete data. They showed that the model could improve performance by extracting discriminative patterns from multiple data modalities in the presence of missing data. In an ablations study they show the importance of data fusion compare to uni-modal approaches. Finally, the model shows improved performance compared to the baselines.	Survival prediction based on multi-modality dataset is a meaningful problem in clinical research, and handling dataset with missing data in different modalities is a very realistic problem. The comparison between end-to-end and two-stage training strategies and the ablation study on data fusion strategies are conducted systematically.	The problem tackled here is both interesting and challenging. Many of the recent methods which try to work with multi-modality data tend to assume that the information of all the modalities of interest are available for all the patients. This makes the task easier, but not applicable to all patients in clinical settings. When ground truth information of some modalities are missing, it is an interesting problem to somehow use the information from the patient to extract relevant information for the training. The paper also summarizes the previous works neatly, enabling us to clearly understand the contribution of this work.	The authors use a two-layer decoder to reconstruct the embedding learned by unimodal feature extractors. Performing that in two separate stages makes sense; however, during training in an end-to-end fashion, making the decoder learn projection of a previous layer is a non-trivial choice. In such scenarios, the model could learn random projection that does not contain the input's formation. Usually, decoders are used to reconstruct back the whole data. It would be helpful if the authors could provide further clarification on this. This could be the reason for the poor performance of the end-to-end approach. The authors fix the hyperparameter \lambda to 1. They argued that it is empirically selected to balance the cox loss and the reconstruction loss. Does this mean L_cox and L_recon lie in the same range? If not, this choice is not natural and should be fixed using cross-validation.	More details are needed for the extraction of radiology and histology image features. More discussions could be conducted on the relationship between information provided by different modalities. The introduction of embedding reconstruction seems to bring marginal value to the prediction.	The paper claims to completely solve 4 important questions in multi-modality fusion with incomplete data. This is a very large claim that needs to be stated with caution, with assumptions and limitations. This paper only works on a limited glioma dataset. Thus, any claims should be restricted to this setting. The optimal setting of the multi-modality fusion network discovered by the authors is for a particular set of setups which the authors consider - i.e. where mean vector fusion is used, along with autoencoders for helping with generating informative embeddings.	The authors have provided necessary information for reproducibility.	The authors specify that code and data will be publicly available	The authors provide some information about the dataset used, and the experimental settings. The exact details of the split generation are missing. Overall the work looks to be reproducible.	This work introduces a multimodal fusion model built on top of a state-of-the-art deep learning model for improving survival predictions. The model architecture and the optimization strategy are well motivated. The authors have provided all the necessary justification for each component. It will be very helpful for the reader if the authors could explain their reasoning behind reconstructing the embedding of the data instead of the data itself. The authors could perform cross-validation to select the best hyperparameter. This could improve their performance.	The authors are providing a solution to a highly meaningful and practical problem in clinical research, which is learning features from multi-modality dataset under the condition that data points from each modality could be missing on subjects due to various reasons. Providing solution to such a problem would help researchers take advantage of data that used to be discarded. Since many features are added into the comparison, more analysis should be needed to identify which features are the most important for improving the performance of the prediction model. For example, training with modality dropout seems to bring substantial difference when compared with the corresponding models without dropout. Such factors need to be highlighted and supported with statistical tests. The extraction of radiology image features is described with limited details. For instance, it is unknown how the segmentation of the 3D tumor volume was conducted. Also the extraction of 2D features seems arbitrary. Overall this step could result in over-parameterized model that is prone to over-fitting. The extraction of pathology image features is described without necessary details. Since the cited method would generated three types of features which are from CNN, GCN, or the combination of the two networks, the readers would need more information to know which way to follow if they would like to replicate the work. The introduction of the reconstruction following modality dropout seems to bring very limited value in the prediction, which to some degree is expected since the reconstruction is based on existing features from other modalities which doesn't bring in additional information to the combined model. More work may be needed to justify the introduction of this feature.	"There are some method descriptions which can be made clear What is modality dropout? Why was it used in the previous works? How are the modalities dropped? In what way is your method ""optimal""? Why do you use radiomics features in addition to the ResNet features? How does the performance change if you do not use the radiomics features? What is the source of variation in the results in Table 1? Are there different splits being considered here? The table description can be made clearer. How are the single modalities reconstructed from the mean vector embeddings? What is the architectural design of the model used for this? What is the decoder structure? Do the experiments clearly demonstrate your answer for Q3? To me, it looks like both strategies perform somewhat similarly and more experimental validation is necessary to clearly answer the question. There are some typos/grammatical mistakes. All the experiments were ran on a -> All the experiments were run on a Chen et al chen2020pathomic should be a reference Some prior studies already shown -> Some prior studies ""have"" already shown"	This work introduces a deep learning model for fusing multimodal data along with a comprehensive guideline to train it. The simplistic architecture and the easy training strategy may prove helpful for other researchers to train this model. In addition, the authors did a commendable job quantifying the importance of each of the modules.	The paper addresses the issue of handling missing data that is a very realistic problem faced in clinical applications	The paper is overall well-written, with a good setup and a good level of experimentation. The claims of the paper seem to be overstated.
493-Paper0015	SVoRT: Iterative Transformer for Slice-to-Volume Registration in Fetal Brain MRI	The authors proposed a novel slice to volume registration method using transformers in the context of fetal brain MRI reconstruction from multiple-stacks. The proposed framework not only provides the slide to volume estimation but also an estimation of the 3D volume as to assist the motion estimation process. Results are performed on synthetic data and compared to two other slice-to -volume approaches. Qualitative results on two real acquisitions are also presented.	This paper proposes a Slice-to-Volume Registration Transformer (SVoRT) to map multiple stacks of fetal MR slices into a canonical 3D space and to further initialize slice to volume registration and 3D reconstruction. 1) construct a Transformer-based network that models multiple stacks of slices acquired in one scan as a sequence of images and predicts rigid transformations of all the slices simultaneously by sharing information across the slices. 2) The model also estimates the underlying 3D volume to provide context for localizing slices in 3D space. 3) In the proposed model, slice transformations are updated in an iterative manner to progressively improve accuracy.	The authors propose a novel method for fetal brain slice-to-volume registration where they use a synthetic dataset to train a neural network based on the new transformers architecture along with an inverse problem formulation to reconstruct the final volume. They also applied their method to real clinical dataset and the result were visually more accurate. Comparisons were performed with respect to two baselines.	To my knowledge, transformers have never been used before in the context of slice-to-volume registration, as such the proposed method is novel Comparison with existing techniques Provided results illustrate overperformance on synthetic data as regards SOA	This work produces a novel solution to a challenging clinical problem. Deeplearning methods [11,23] have been proposed to predict transformation, which process each slice independently, ignoring the dependencies between slices.  This work process the stacks of slices as a sequence, SVoRT registers each slice by utilizing context from other slices, resulting in lower registration error and better reconstruction quality. Instead of predicting the transformations alone, this work also estimate a volume from the input slices, so the estimated volume provide 3D context to improve the accuracy of transformation. Specially, during volume estimation, this paper consider some wrong slices, resulting in artifacts in the reconstructed volume. They proposed addition SVT to predict weight of slice, where represent the image quality of the slice.	The paper elegantly merges strengths of CNNs through ResNet by extracting meaningful features from original stacks, transformers strengths to know where to attend to the most and a weighted inverse problem formulation to construct the final volume using the transformations and the weights learned. They apply the method to two datasets They performed ablation experiments to support some components of their model	Quantitative validation is limited to 12 subjects only, though x4 are generated from those Overclaims on the impact of the proposed method to final 3D reconstruction methods in practice (authors limit up to 3 stacks only in this paper) Lack of details on SOTA parameters or how SVoRT parameters are set (lambda)	Singh A, Salehi S S M, Gholipour A. Deep predictive motion tracking in magnetic resonance imaging: application to fetal imaging[J]. IEEE Transactions on Medical Imaging, 2020, 39(11): 3523-3534. The related work is missed. This paper also predicted motion parameters from sequences of slices based on RNN. It also utilize spatio-temporal information.	No baseline comparison to fetal SVR non DL methods Some choices are not backed up by explanations or experiments (please refer to point 8) It would have been great to see more real world examples such as in supplementary materials Minor comments (point 8)	Authors state on the reproducibility statement that code will be made available, nothing is mentioned to this sense in the paper. Still the different generated/simulated motion levels on the FeTA dataset should be made available also as to ensure reproducibility.	The reference implementation of SVoRT will be available on github.	Reproducible for the publicly available FETA dataset. The clinical dataset is not available as the authors answered not applicable to << A link to a downloadable version of the dataset (if public) >>, the code is/will be however put in Github.	The authors present a novel and interesting approach using transformers for the slice to volume motion estimation in the reconstruction of fetal brain MRI. I've found the paper interesting but many aspects appear as preliminary and I have major concerns with the choice of the experiments. My general comment is that as a work of slice-to-volume registration it would have been valuable to illustrate experiments according to the level of motion. This is lacking to me as to understand the value of the simulated images. It is unclear to this reviewer wether this method aims at provide a 3D reconstructed image or only an initialization as for other classical inverse problem reconstruction. Formulation in equation 1 solves the data term inverse problem, but it is unclear to this reviwer why no regularization is included then. If the final goal is really to provide also that SR image, classical inverse problem could have been used as for SOTA methods also. But if the goal is to use this approach as to further initiaiza a more classical SVR it would have been good to illustrate then how the different initializations (SVoRT, Planet, SVRnet) influence a classical SVR recon (inverse problem+regularization). So, how SVoRT potentially improves SVR is not proven. It is unclear to this reviewer the experimental setup. Authors mention FeTA dataset, and then registration to a brain atlas and resampling to 0.8 mm isotropic. Which is the rationale behind this step? Was this done by FeTA or this is something needed/specific for this study? Authors mention simulation of 3 stacks in random orientation, do they mean orthogonal? Or really random? Often in a real fetal acquisitions orthogonal views are generated (sometimes not perfectly). Please clarify the definition of random views. From the 12 left testing cases (which GA? Were they normal or pathological?) it says 4 different samples were generated for each. What does it means? 4 different levels of motion? This is a crucial point to understand the indudec motion for generating testing examples. Would have been interesting to see results vs level of motion for instance. It seems then in the experimental setup that yes a SVR is used further for real cases. Which SVR method is applied with which regularization technique? This reviwers wonder also the parameter setting of the SOTA methods, if any, how this was choosen? Do they also have outlier rejection scheme? Would the authors compare with manual initialization or classical motion estimation methods slice to volume for comparison purposes? At least for the two illustrated real cases. Certainly those methods might be more time consuming but would be interesting to illustrate the added value. I am not sure I understand what does it means the study with one stack only. There is then no super-resolution in that case. In practice, due to the in-plane through plane resolution differences, multiple stacks are acquired for fetal MRI. I would have found the assessment more meaningful if starting from 3 stack up to 6 for instance. Is real data at 3T? A gap/slice thickness of 2mm is the smallest one often seen in fetal acquisition that may go up to 4 mm often (also depending on the field strength and in plane resolution). Did the experiments with real data with SVRonly use some classical slice to volume or multi-scale slice registration as often used? Could the authors evocate hypothesis on why SVROnly seems even to work better that with the two SOTA initialization? I've found this weird overall. Would have been interesting to see the type of low-resolution stacks acquired as to illustrate the level of motion.	This work produces a novel solution to a challenging clinical problem. The SVoRT construct architecture to predict transformation and volume simultaneously. The estimated volume as auxiliary task provide 3D context to improve the accuracy of predicted transformation. This work utilize transformer to encode spatial correlation of the input sequence. Specially, during volume estimation, this paper consider some wrong slices, resulting in artifacts in the reconstructed volume. They proposed addition SVT to predict weight of slice, where represent the image quality of the slice.  I am curious about the necessity of transformer module in this framework. What are the benefits of transformer compared to other RNN (lstm)? Can the estimated volume of the network output be used as a preliminary reconstruction result? How about quality of the estimated volume? If the quality of estimated volume is good,  the estimated volume as a reference volume for reconstruction.	Unclear why using both y_hat and y (concatenated) and not only y_hat as the input of ResNet, please provide an explanation (or experiment) on the why Please mention dataset type (synthetic v.s. real) in Figure 3 Minor comments: sec 2.3. to b*ridge, sec 2.3: << Previous works >> have demonstrated, please cite, sec3.1: << Learning rate of 2 x 10 4 and linear decay for 2x 105 iterations >> unclear please give decay rate for how many iterations and please mention total number of iterations(or epochs)	Despite the clear novelty and scientific value, i've found the evaluation of this paper unclear and weak. Overall it jeopardises my evaluation of the real added value of the proposed strategy.	The paper is well-written and proposes a solution to a challenging clinical problem. The paper propose a new formulation to model slice-to-volume registration. They demonstrate significant improvements over the state-of-the-art, particularly in accuracy of transformation. Furthermore, they conduct an ablation study and evaluate the significance of their volume estimation and positional embedding, justifying the reasons for their model design. Their qualitative figures demonstrate the improvements brought by their network, and this work represents an important contribution in fetal brain image reconstruction.	The innovative merger of three paradigms (CNNs, Transformers and inverse problems) to solve SVR + the realistic look of the real dataset made my decision clear (although I would have preferred more than two examples on the real dataset, either as textual results or as images)
494-Paper1401	Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI	This paper proposed a SDAUT network that combines Swin Transformer and deformable attention for fast MRI which also provides explainability	The authors solve the problem of accelerated MRI reconstruction from an undersampled k-space. They propose a UNet based on Shifted Windows (Swin) Transformer and deformable attention derived from deformable convolution networks.	The authors integrate Swin deformable transformer [22] with U-Net to propose SDAUT, which is applied to undersampled MRI reconstruction. The authors attempt to provide explainability of the proposed methods on its superiority to the comparison algorithms by showing the deformation fields and attention score in the inference stage. The proposed SDAUT outperforms nPIDD-GAN and SwinMR with lower computational cost. SDAUT betas DAGAN in SSIM, PSNR and FFID but fails MACs.	The paper is well-written and the idea is easy to follow overall The combination of Swin Transformer and deformable attention is a novel design and can help to reduce computation while maintaining/improving performance SDAUT achieves state-of-the-art performance and can provide explainability The authors provide an extensive ablation study to validate the design of the proposed method	The article is clearly written, the drawings are informative, the motivation is sound. The resulting architecture is much more computationally efficient than classical feedforward neural networks, including those based on transformers.	The deformable field and the attention score in Fig.4 attempted to provide insights of how the model works for the undersampled reconstruction task. The swin deformable transformer is somehow novel in undersampled MRI reconstruction application.	Supplementary material is not provided though mentioned in the main paper The values of model parameters are missing, e.g. L in RSDTB/RDTB blocks, r in deformable attention, the number of feature channels, and the window sizes for Swin attention computation In the ablation study, it is unclear how DDDDDD-O models perform, i.e. models using dense deformable attention only. I understand it might be computationally costly, but will it bring significant performance gain?	"The novelty is limited, given the fact that the authors combined the UNet with Transformers (SWIN) and deformable convolutions. As an example, U-Net on Transformers already exists:  1) Petit, O., Thome, N., Rambour, C., & Soler, L. (2021). U-Net Transformer: Self and Cross Attention for Medical Image Segmentation. ArXiv, abs/2103.06104. 2) Unet based on SWIN: Cao, Hu et al. ""Swin-Unet: Unet-like Pure Transformer for Medical Image Segmentation."" ArXiv abs/2105.05537 (2021) 3) Gao, Yunhe et al. ""UTNet: A Hybrid Transformer Architecture for Medical Image Segmentation."" ArXiv abs/2107.00781 (2021): 4) Yeonghyeon Gu, Zhegao P., Seong J. Y. ""STHarDNet: Swin Transformer with HarDNet for MRI Segmentation"", Applied Sciences 12 (1)468, 2022. DOI:10.3390/app12010468 Here is a nice collection of models for MRI acceleration using transformers: https://github.com/junyuchen245/Transformer_for_medical_image_analysis There is no comparison with any of these methods in the experiments. In general, the authors compare only to the three basic approaches and SwinMR, with respect to which there is almost no improvement, except the reduction of the number of parameters. It would be important to compare with performance to, e.g., SwinUnet (second reference on the list) - also because of its efficient configuration. Also, the manuscript presents only 1 dataset, on the topic of fast MRI where the the top benchmarks of Fast MRI challenge (https://fastmri.org/) are not considered."	While the proposed SDAUT outperforms the competing algorithms in the paper, it lacks the comparison with state-of-the-art algorithms. The authors claimed the proposed SDAUT provide explainability. It is not clearly written how the proposed methods/which mechanisms provide explainability. While the discussion & conclusion section shows an example of such claim, it lacks of explanation of 'explainable fastMRI' . The authors could define and clearly mention how does SDAUT is explainable. The supplementary material (which has been mentioned multiple times in the manuscript) is missing. Data consistency (DC) module plays an important role in undersampled MRI reconstruction. The proposed module does not contain DC layer, potentially leading to lower fidelity reconstructed images. Nor did the authors compare with the models that contain DC blocks, such as D5C5, variational network, MoDL etc. While the writing is easy to follow, the motivation is not well-explained. e.g. How the proposed methods reduce computational cost? Why/How does it provide explainablity? Although the results section confirmed this, the introduction/methods part did not mention it, which makes the manuscript less convincing. The data used in the manuscript is 12-channel, how does the authors combine the multi-coil images? Did you use sensitivity map?	Some model specifications are missing in the paper. But the authors have agreed to release codes and models in the checklist.	Hard to assess.	The reproducibility checklist shows the authors will upload their code and implementation details online. The experimental settings are listed in the manuscript, but the hyper-parameters for the proposed method are missing.	Please refer to the weakness section. My main comments are: 1) How do DDDDDD-O models perform? This is important for understanding/ validating the necessity of combining  Swin Transformer and deformable attention.  2) Attention score based explainability is not unique to this method but a property for all Transformer-based models. Have the authors checked how it compares to gradient-based model explanation methods?	To strengthen the article, the experimental part needs to be refined by including proposed benchmarks and other transformer-based models. Explainability claims need to be elaborated too (or removed from the title).	"I suggest the authors to write the motivations behind their designs more clear, especially in the methods parts. Why and how the design leads to performance gain? Which design reduces the computation cost? I can see the experimental results confirm the claims (good performance with limited computational cost + explainable fastMRI), but more insights of the design will make this paper more solid. The most interesting part of this paper is the claim of ""explainable fastMRI"", but it is not convincing enough about how the proposed method provides explainability. Detail explainations/motivations on this should be mentioned in the introduction & methods parts. More comparison with SOTA methods, especially those including DC layers should be demonstrated. Also, it is interesting to see if the performance improves when including DC layers."	The method design is novel and effective, and explainability is important for medical applications. However, there are some weakness as described in weakness and comment sections.	The manuscript is of an engineering nature where two already developed approaches (Swin + deformable attentions) were combined. The experimental part is not strong, with the main claimed achievement of the article being the computational efficiency. However, there is no comparison with the other transformer-based U-Nets which also have much fewer parameters than the SwinMR. Also, only 1 dataset is considered in the article; the single-coil vs multi-coil setup is not addressed.  Given these issues, the score is below the borderline threshold.	The manuscript is clear and easy to follow, but the motivation of the design is not well-explained. The performance improvement is quite marginal. Nevertheless, the application of deformable swin transformer to fastMRI and the results on deformation fields/attention score for explainble fastMRI are the merits.
495-Paper0819	Swin-VoxelMorph: A Symmetric Unsupervised Learning Model for Deformable Medical Image Registration Using Swin Transformer	In this paper, the authors propose to a framework to do image registration usin swin transformer. The architecture is the swin unet: a unet-like architecture with convolution blocks replaced by swin transformer blocks. The output of the network are the geometric transformation from each input image to the other one. The (symmetric) loss minimize: image dissimilarity and inverse consistency between the transformation and penalize irregular transforms and negative Jacobians. The method is compared to state of the art methods on brain MRI images and evaluated using Dice on cerebrale structure and percentage of negative Jacobian.	The authors present a deformable image registration network based on the Swin Transformer - Swin-VoxelMorph. They use the ADNI and PPMI datasets to evaluate the model. They achieve an average Dice Similarity Coefficient (DSC) of 0.775.	This paper presents a method for deformable medical image registration using Swin Transformer.  The technical novelty of the method is to explicitly exploit Swin Transformer for deformable medical image registration, and utilize orientation and inverse consistency constraint to guarantee the topology-preservation and inverse consistency of the predicted transformations.	- as far as I known, this is the first implementation of swin unet for image registration - the method compared favorably to state of the art w.r. dice, negative Jacobian percentage and computational time. - the resulting registration method have all the good expected properties of a registration method: regularity, symmetry, invertibility.	The main strengths of the paper lie in the fact that they use a transformer based network for deformable image registration task. The paper is well written and easy to follow.	It constructs a objective functions including orientation and inverse consistency constraint to guarantee the topology-preservation and inverse consistency of the predicted transformations.	-  only minor weakness (see below)	The main weakness of the paper is with the figures. Figure 3 is hard to follow. Are the warp field and Jacobian determinant corresponding to the top images or the bottom images? The colorbar is also very hard to see.	The The structure is similar to reference [6].  The main difference is that swin transformer replaces fully revolutionary networks. The technical novelty of the method is limited and the performance increment is incremental.	very good reproducibility, with code and dataset available.	The paper seems reproducible. Although, the authors do not mention in the text whether they plan to make the code publicly available.	The paper is easy to follow, though some of the details are not clearly described.	The dice evaluation has been done using Freesurfer segmentation has ground truth. In an extended version, it would be interesting to see also the results on a dataset with manual annotations (such as the IBSR or the MICCAI multi atlas 2012 challenge dataset). Using these data only in the test would also help reinforcing the results regarding the generalization.	The authors should improve the figures and make Figure 3 easy to understand.	"One of my main concerns with this work is a lack of precision in the explanation of steps which are essential for the whole apparatus to work well and be credible. For example, the authors state in the 'symmetric loss terms' 2.2 subsection: ""where phMF and phFM are difffferentiable and invertible in a bidirectional fashion"". How is this guaranteed?This can  be precisely formulated in mathematical terms."	The paper is clear, the proposed pipeline is, as far as I know, the first implementation of swin unet for image registration. The resulting registration method include all good properties expected: regularity, invertibility, symmetry. Runtime and quality metric compare favorably to state of the art.	The paper is very well written and easy to follow and the only weakness is the clarity and description of the figures.	The problems to be solved are challenging, and it is worth encouraging to try to find a solutions with the latest methods. And this paper takes into account the topological consistency and differentiability for registration.
496-Paper0194	Tagged-MRI Sequence to Audio Synthesis via Self Residual Attention Guided Heterogeneous Translator	This paper propsoed a novel deep-learning based method to synthesize spectrograms (audio) from tagged-MRI sequences (imaging).	To our knowledge, this is the first attempt at translating tagged-MRI sequences to audio waveforms. They proposed a novel self residual attention guided heterogeneous translator to achieve efficient tagged-MRI-to-spectrogram synthesis. The utterance and subject factors disentanglement and adversarial training are further explored to improve synthesis performance.	The authors propose an encoder-decoder (translator) model which is trained in GAN-fashion on pairs of input data to synthesize acoustic mel spectrograms from an MRI sequences of oropharyngeal muscles movement (which corresponds to tongue movement). To only exploit the information that lies in the muscle movement, an additional encoder-decoder FCNN network (residual attention) is trained alongside to filter out static regions in the input frames. The latent space of the encoder-decoder-model is disentangled into utterance-specific and subject-specific latent features, where the utterance-specific part is learned by enforcing prior knowledge via KL-divergence on utterance-matched sample pairs. All modifications seem to improve the performance of the model.	Authors raised an interesting research topic: transfer imaging data (tagged MRI) to audio data (spectrograms). Specific self-residual attention guided heterogenous translator and utterance disentanglement were designed for this specific task. The proposed method outperformed the available method for similar task.	They are the first team to try to translate tagged-MRI sequences to audio waveforms and they proposed  an efficient fully convolutional asymmetry translator with help of a self residual attention scheme to specifically focus on the moving muscular structures for speech production. And they used a pairwise correlation of the samples with the same utterances with a latent space representation disentanglement scheme. Furthermore, we incorporated an adversarial training approach with GAN to yield improved results on our generated spectrograms. The topic and results are very interesting.	The proposed architecture is sophisticated and tailored to the target application and data format, and the authors present an ablation study which shows the benefits of each part of the proposed model. The approach is novel and interesting, the results seem consistent, and the paper is well written.	Details or justifications were missing for operations used in pair-wise disentangle training. The sample size for the experiments was small. There were only two words investigated in this study, and the number of the data sample was imbalanced. The performance gain of adding GAN is marginal compared to attention and pair-wise disentangle, taking the computational cost of GAN into account, the involvement of GAN is questionable for such a model. Loss with/without GAN was missing.	It's hard to reproduce, because they did not share the source codes.	The proposed model is trained with a total of 63 tagged-MRI sequences which is a very limited sample size. It would be interesting to have a discussion how this method can scale up. The differences to the competing architecture, Lip2AudSpect, could be explained better. It could also be explained better how Lip2AudSpect was adapted to MRI data. The authors did not mention the research field of speech generation from real-time ultrasound which is very related.	Authors have provided fair information to reproduce the method.	It is negative for the reproducibility of the paper, because they did not share the source codes.	The authors provide sufficient information to reimplement the proposed architecture.	For pair-wise disentangle training, the authors selected a few channels for the feature to denote the mean and variance. Please explain how these channels were selected and why. I suggest the authors should also justify the necessity of GAN. I believe it may be hard to enlarge the dataset used and explore more word samples, even sentences. But it would be more interesting if more samples can be included for this study.	It is difficult to reappear, but the results are promising for applications.	"The authors should include a discussion how the model would scale up with an increased dataset size. The reconstructed samples are very distorted which is probably (partly) caused by the use of the Griffin-Lim algorithm for waveform reconstruction. Maybe the authors can discuss more recent approaches, e.g. MelGAN [1]. The authors should the implementation of the baseline method, Lip2AudioSpec, in more detail. Basic information about training, e.g. batch size, should be included in the manuscript. The ""self-trained attention network"" seems to only generate masks by blurring the residual frames. Why is this complicated approach chosen, and could a simple technique maybe even improve the performance (e.g. filling the area of white pixels in the binary residual frame with simple computer vision techniques)? The authors should discuss the very related field of generating speech from real-time ultrasound images, e.g. [2]. [1] Kundan Kumar, Rithesh Kumar, Thibault de Boissiere, Lucas Gestin, Wei Zhen Teoh, Jose Sotelo, Alexandre de Brebisson, Yoshua Bengio, Aaron C. Courville, MelGAN: Generative Adversarial Networks for Conditional Waveform Synthesis, Advances in Neural Information Processing Systems 32 (NeurIPS 2019), 2019 [2] Jing-Xuan Zhang, Korin Richmond, Zhen-Hua Ling, Li-Rong Dai, TaLNet: Voice reconstruction from tongue and lip articulation with transfer learning from text-to-speech synthesis, Title of host publicationProceedings of the AAAI Conference on Artificial Intelligence, 2021"	This paper raised an interesting research topic and the authors provided a fair solution to this problem. Some details were missing but overall a fair paper to be accepted.	This paper proposed a novel method, and this is the first attempt at translating tagged-MRI sequences to audio waveforms. The results are promising for applications.	The paper presents a novel and creative approach which could be used to understand the relationship between tongue muscle movement and speech.
497-Paper0832	Task-oriented Self-supervised Learning for Anomaly Detection in Electroencephalography	his paper proposed a task-oriented self-supervised learning approach to train a feature extractor based on normal EEG data and key properties of the abnormal EEGs. Two-branch of CNN with larger kernels is designed for effective extraction of both small-scale and large-scale features.  The method is tested on two public and one internal EEG datasets with reasonable good performance.	In this paper, the authors demonstrate the use of converting domain knowledge into SSL transformation rules to augment the data, so that the anomaly detection result is improved. The authors demonstrate the effectiveness through two simple transformations.	A simulated based anomaly detection method for EEG Data.	Self-learning for anomaly detection of EEG based on CNN may have some novelties The way of generating abnormal EEG signals is interesting even if there exist deficiencies.	The idea is simple and natrual. In other words, domain experts may add as many SSL transformation rules based on their understanding of the problem.	The experimental results are quite good !  It is great to see the simulated based anomaly detection is such effectiveness in EEG modality. The ablation study is well designed. Using Mahalanobis distance as anomaly score is reasonable.	The simulated abnormal EEGs are generally different from real abnormal EEGs, e.g., more irregular and include combinations of various anomalies.  It is expected that more realistic amplitude-abnormal and frequency-abnormal EEG data can be generated for anomaly detection.  Hence, it is in doubt whether or not the proposed method is capable of detecting more complex noisy abnormalities in EEG signals.	I worry that how deep the knowledge human experts need to have to create valuable but not detrimental SSL rules. For instance, the proposed method works in these three datasets due to the simulated anomalies make some sense under the context. However, do this approach generalize when we inject incompatible simulation rules? The baseline are not SOTA. For instance, OCSVM, KDE, and AE are very basic methods. There are a large number of SOTA sequence-based anomaly detection methods in https://github.com/datamllab/tods [1]. [1] Lai, K.H., Zha, D., Wang, G., Xu, J., Zhao, Y., Kumar, D., Chen, Y., Zumkhawaka, P., Wan, M., Martinez, D. and Hu, X., 2021, May. TODS: An Automated Time Series Outlier Detection System. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 18, pp. 16060-16062).	""" Impirical evaluations show that the above-mentioned simple transformations are sufficient to help train an anomaly detector for EEGs. "" The comparsion with fully-supervised method is required to support this claim. This claim may mislead the readers !   Intuitively, more complex anomaly data can improve the performance. Also, why not choose more complex anomaly similation combining frequency and amplitude abnormal ?"	Codes not open, should have some issues in reproducibility.	GIven the submission, it is possible to reproduce the results even without the code.	Good.	"1) What's the criterion in defining the range of amplitude scaling factor, e.g., \alpha_l and \alpha_h in the second paragraph of page 3, ""Generation of self-labeled abnormal EEG data""? Also applies to the scaling factors for frequency scalar factor. Is this range different for different types of anomalies? 2) Please compare with method [25], and indicate the major advancement of the present method in comparison to [25]. Noticed that there are some results presented in Fig. 3.  3) In general, what's the dimension for the shortcut branch from the output of the 1st convolutional layers to the penultimate layer?  Does this design need to change for the detection of different anomalies of EEG disease? 4) Regarding the methodology discussed in page 5, what's the performance if you retain the classifier head instead of using Mahalanobis distance on the extracted features?? And what's the criteria of determining the threshold for anomaly detection? 5) As for the experiments, please indicate the disease present in these datasets."	I think the authors need to justify how to and when should one to use the task oriented SSL rules? When do they apply? I would highly recommend improve the baseline. The current unsupervised anomaly detection baselines are too old to be effective.	Choose and discuss more complex anomaly similation combining frequency and amplitude abnormal. It is suggested to provide 2D t-SNE to visualize the features of normal, simulated abnormal and abnormal features. Comparision to supervised ( with fully and partial training label ) methods. As a classifier is trained, it is interesting to see the performance of directly using the classifier to detect abnormaly EEG data. Another simulated based method in medical imagning is highly related to this paper but has not been cited: (NormNet:) Label-free segmentation of COVID-19 lesions in lung CT, TMI, 2021.	The moderate novelties of self-supervised learning for anomaly detection of EEG signals and the way of generating abnormal EEG signals.   However, the abnormalities generated may not be practical abnormal EEG signals, hence limit its applicability of application.	I am mainly concerned due to is applicability and experiment setting.	Although the idea is similar to CutPaste and NormNet, it is valueable to see such a simulated-based anomaly detection method is effectiveness in EEG modality.  The experiments are well designed, especially the ablation study. Also, there is a major concern : lack of comparision with supervised methods. So, I recommend weak accpet.
498-Paper0887	Task-relevant Feature Replenishment for Cross-centre Polyp Segmentation	A method to segment Colonscopy polyp images is presented. In particular, the paper focuses on the domain shift problem when data belongs to different centres. The paper proposes a domain invariant feature decomposition  (DIFD) module that aims to reduce style variations, then a task-relevant feature replenishment module tries to disentangle informative context from the residual domain specific features of DIFD, and finally, an adversarial learning strategy is applied to bridge the domain gap by aligning features in output space.  The architecture is an encoder-decoder framework. The DIFD modules are placed after each encoding block. It decomposes the features into domain-invariant portion and side-out domain-specific portion. The domain-specific ones serve as input for TRFR module, which uses them to produce task-relevant features. Both domain-invariant features and task-relevant features are combined to predict four segmentation maps through a decoder network. Finally, features in the	This paper propose an UDA method that effectively combines multiple techniques, including style transfer, adversarial learning and self-attention, for polyp segmentation. Extensive experiments demonstrate the superiority of the method over prior works.	The authors proposed  a Task-relevant Feature Re-plenishment based Network (TRFR-Net) to tackle existing problems in UDA, for eliminating domain shifts in multi-centre colonoscopy images while retaining sufficient discrimination capability.	The paper faces a very important problem in the medical imaging field, the domain-shift problem. The idea to join domain-invariant features and task-specific features is interesting. The paper is well written and simple to follow.  Extensive quantitative comparisons are conducted, and all the methods are trained using the same data splits.	The paper is generally well-written and easy-to-follow; The paper explore multiple techniques in computer vision and effectively combine them into a single framework; also, each proposed module are modified to tailor the segmentation task; Extensive experiments on multiple datasets demonstrate superior performance of the proposed methods;	The paper is well organized and clearly written. Tables and some of the figures are well presented. The authors present a new method for unsupervised domain adaption which is really a challenging problem.	"It is not clear how attention is used in the DIFD module. In section 2.1: the paper states that h() and g() are the attention functions, but no details are provided about them. An overview of the DIFD module is provided in Fig.2 but there is not a clear explanation of the design of the module in the paper. Sec 2.3 How the w_bg is calculated? What the ""significance of predicted background region alignment"" represents? What is the real contribution of adversarial and TCLoss? The lamda parameters used are very lower. Why? Have these two losses different orders of magnitude w.r.t. the segmentation loss?"	Lack of qualitative analysis of each module: while each component (e.g. DIFD, TRFR) has clear objective, whether in the end they realize the goal or not is hard to tell from the current presentation; Some design decisions are lack of careful justification (TCLoss, PAAL and TRFR).	Figure 2 needs to be redrawn again. In the current form, It is difficult to follow. For instance, From the reader's point of view, it is hard to understand where X(s) is being used. The authors also need to mention which is their final output (prediction masks)? From the Figure, it is unclear. The author uses SE-block. CBAM has already shown improvement over squeeze and excitation block. Is there any specific region behind choosing the block? An ablation on this would be helpful. From the reader's point of view, the authors confuse domain adaptation and domain generalization. Please make it clear. The authors only test on still images, consisting of tiny images samples. Currently there are many public video polyp segmentation datasets available with large number of samples and are public. Please validate your results on larger datasets such as ASU-Mayo, CVC-videoClinicDB, and SUN datasets). The authors could site some relevant important domain generalization and recent multi-centre work in the literature.	The hyperparameters are clearly listed. Although is not clear how the model is evaluated. The paper states that the maximum training epoch is set to 150. Is the model evaluated at the end of the 150 epoch? Or maybe some strategy is used to select the best epoch in which the model is evaluated?	Sufficient implementation details are provided in the paper.	Although the author does not provide source code, they mention about the hyperparamters. But i do not think the method is reproducible at current form.	Some aspects need to be clarified as stated in the weakness section.	The formulation of TCLoss is not intuitive; it is understandable that I(p+) should have lower entropy than I(p_di), but TCLoss essentially encourages the network to be uncertain on I(p_di); how is that benefit the overall method and what if we only penalize I(p+)? For PAAL module, why pixel-level adversarial learning is applied rather than image-level? Domain is usually defined on the image-level, how does the network can differentiate the domain of a pixel? The TRFR module is very similar to a feature pyramid network (FPN). Does TRFR really replenish the task-relevant feature, or is just simply aggregating feature from different scales? Please discuss the different between TRFR and FPN and consider citing relevant works.	The authors should redraw Figure 2 and make it easier for the audience. Through literature search is missing currently. The clinical motivation of the paper could be improved. I feel that CBAM can further improve your model performance. Please give it a try.	The paper is easy to follow, and tackles a real problem in medical image field. The validity of this approach is supported by the improved performance w.r.t. the other methods. Although some aspects need to be clarified.	This paper explore and exploit ideas from multiple CV tasks and achieve considerable improvement on the UDA tasks; on the other hand, some designs lack in-depth justification and discussion, making the overall story not sound and logically coherent.	The experiment is limited to still iamges that consist very few images. There are more few publically available video polyp segmentation dataset and also still image that have decent number of images (for example, Neopolyp (BKAI)) and few more. Experimental results on the larger dataset may help to justify the current improvement of the method and usability of the models in the clinic.
499-Paper1318	TBraTS: Trusted Brain Tumor Segmentation	This paper brings a new method to be able to provide uncertainty in addition to the segmentation. They use Dirichlet distribution in a very nice way for this aim. Furthermore, the robustness of the network is improved.	This paper proposed an end-to-end trusted model for brain tumor segmentation by quantifying the voxel-wise uncertainty  and introduced the confidence level for the image segmentation in disease diagnosis. A series experiments were conducted and  the results verity the reliability of the model.	This paper proposes a trusted brain tumor segmentation relied on the evidential deep learning method. The proposed method estimates uncertainty without excessive computational burden and modification of the backbone networks. Experiments show improved performance.	* This paper provides a smart way to be able to predict uncertainty related to the resulting segmentation * comparisons with existing methods are detailed	The idea of using Dempster-Shafer theory for uncertainty quantification is attracting and the proposed  methods show good performance on model's reliability with Entropy, ECM and UEO.	The application of evidential deep learning on medical image segmentation is an interesting use case and can be helpful for the community. The experimental results demonstrate the effectiveness of the proposed method over other methods.	I did not detect any real weakness in this pape except the lack of references in matter of U-Net variants.	"The experiment section is not well organized and the results are not well explained. For example, Fig2 a) is complex and hard to understand.  In paragraph""Differences from similar methods."", the authors should compare with other similar Dempster-Shafer-based methods. See detailed comments below."	Lack of novelty. The proposed method is a direct application of the existing evidential deep learning method [22] to the brain tumor segmentation task. Although [22] is 2D image classification-oriented, and this paper focuses on 3D medical image segmentation, the overall difference is small. Generalizing techniques from image-level classification to pixel-level classification (segmentation) is straightforward. The authors are suggested to elaborate more on their contributions. Is there any other paper that also relies on evidential deep learning [22] in medical images? If yes, the authors should discuss them as well. The writing should be improved. The authors are suggested to proofread carefully and enhance the writing. Minor issues: Scale Fig. 2 to a proper range. It is hard for readers who prefer to print the paper and read.	This paper seems to be reproducible on any segmentation architecture.	The paper is reproducible.	No special issue here.	The only remarks that I have is that some references relative to U-Net variants are missing, and some sentences are not well constructed; please reformulate them.	"Besides subjective logic theory, many methods have been proposed to assign belief of mass, for example, Shafer's model [1], Evidential KNN [2], and Evidential neural classifier [3]. Why did the authors choose subjective logic theory instead of other methods to assign belief of mass? The discussion and comparison between different belief assignment methods should be interesting and meaningful.  [1]. Shafer, Glenn. A mathematical theory of evidence. Princeton university press, 1976. [2].Denoeux, Thierry. ""A k-nearest neighbor classification rule based on Dempster-Shafer theory."" Classic works of the Dempster-Shafer theory of belief functions. Springer, Berlin, Heidelberg, 2008. 737-760. [3]. Denoeux, Thierry. ""A neural network classifier based on Dempster-Shafer theory."" IEEE Transactions on Systems, Man, and Cybernetics-Part A: Systems and Humans 30.2 (2000): 131-150. why did the authors only choose two modality data as the experiment dataset? And the Gaussian noise was added to which modality? Why only add noise on one modality but not modality together? Fig 2 a) is hard to follow. The performance of Dice is not as good as others. The authors should give some explanation for it. In Fig 2 a), the most stable method is UE. The authors should give some explanation for it. The results of Fig2 are obtained during training. Where are the results of the test set? Fig 4 only visualize the uncertainty comparison with the slice containing enhanced tumor (class 4) results. The brain2019 dataset is a three-class segmentation task. The authors should offer an example slice with three class information included to show the model's reliability better. There are some methods that use the Dempster-Shafer theory for uncertainty quantification in medical image segmentation tasks. Some discussion beyond those methods is necessary, i.e., [1]Huang, Ling, et al. ""Evidential segmentation of 3D PET/CT images."" International Conference on Belief Functions. Springer, Cham, 2021. [2]Huang, Ling, Su Ruan, and Thierry Denoeux. ""Belief function-based semi-supervised learning for brain tumor segmentation."" 2021 IEEE 18th International Symposium on Biomedical Imaging (ISBI). IEEE, 2021."	Considering elaborating why the proposed/used uncertainty estimation method is more effective, in medical image segmentation (e.g., brain tumor), than the existing ones. In your experience, is there a conscious difference between different estimation methods? Such kinds of discussions can provide insights to the community and increase the contributions and impact of this paper.	I really appreciated the way uncertainty is computed, I think this approach promising.	This paper proposed a novel direction for uncertainty quantification by assigning the belief of mass with the Dempster-Shafer theory.  The proposal was evaluated with four metrics, Dice score, Entropy, ECE and UEO.  The authors also compare their proposal with the popular uncertainty quantification method, i.e.,MC dropout,  model ensemble.	See the weakness section.
500-Paper1738	Test Time Transform Prediction for Open Set Histopathological Image Recognition	The focus of this paper is to develop a method that can identify clinically relevant patches that are present in the train set (closed set) while ignoring the irrelevant ones, not present in the train set (open set). To this end, a model is trained with two tasks, one to predict the class and the other to predict the colour transform applied to the input image. During inference, the input patch without any transform is processed through the model. The confidence score for predicting the transform is used to distinguish between open and closed sets since it tends to be lower for the open set. The proposed model is evaluated on two colorectal datasets and is shown to perform better as compared to other techniques.	In this study, the authors proposed a new approach for Open Set histopathological image recognition based on training a model to accurately identify image categories and simultaneously predict which decoupled color-appearance data augmentation has been applied.	"The paper presents a methodology to detect Out-of-Distribution data in addition to classifying ""known"" regions of histopathology images"	Proposed a simple yet novel self-supervised way of filtering out the irrelevant patches. The source code will be made public with the publication.	The authors proposed a decoupled color-appearance data augmentation strategy and a test-time transform prediction model.	The paper is clearly written and easy to follow. It offers a good depth of background information, rationale for approach, and contains valuable ablation studies and comparisons.	There are some points that needs clarification.	This work lacks of the performance investigation of appearance transformation stage.	While the paper is meant to provide output of detailed classification of known regions, this specific performance is not well analyzed. In addition, the performance improvements with regards to the other evaluated methods seem minimal. Therefore, the added value of the approach (in technical terms) is unclear.	The source code will be made public with the publication. The authors have used public dataset for their experiements.	The reproducibility of this work seem to be reasonable, but should be further validated on more datasets.	I feel the authors present enough information on the datasets and algorithm to allow for reproducibility of the paper. If they indeed release a working software, the impact of the paper will increase.	Ablation study: Train a simple model with the same data split as in the paper. The train data would comprise all classes in the closed set in addition to the open set as a single class. Comparison with this experiment would show the benefit of the proposed approach. Tables 1 and 2: Add columns for average ACC and AUC over all three splits. It is not clear which loss function was used for classification and transform prediction. What is the form of ground truth labels and expected network output for the transform prediction task? Was thresholding applied to softmax probability to decide whether the input image belongs to an open set or closed set? If so, what was the value and how was it obtained? Do you think the trained model will generalize well to the unseen dataset from another domain? What do values in parentheses mean in Tables 1 and 2?	"1.The Abstract should address more novelties of the proposed model and detailed results for attracting the readers. 2.With respect to the transform space in T3PO, there are seven appearance (i.e., Identity, Brightness, Contrast, Saturation, Hue, Gamma, Sharpness) generated by appearance transform. How does this different appearance affect the performance of T3PO? Does generating more appearances improve the performance of T3PO?  3.Reference 3 citation format should delete ""conference Name: IEEE Transactions on Pattern Analysis and Machine Intelligence""."	"I would have liked to see a more detailed analysis of the classification outputs for the ""known"" image regions, especially in comparison with the state of the art. The reported metrics are a good aggregate value of performance, but in detailed images such as the histopathology ones, it's usually the rare classes / limited pixels that contribute the most in semantic understanding."	I would recommend accepting this paper if ablation study is provided to justify the benefit of using color transform.	This research work seem to be reasonable, but should be further validated on by additional experiments.	Please see strengths and weaknesses above.
501-Paper0841	Test-time Adaptation with Calibration of Medical Image Classification Nets for Label Distribution Shift	This paper proposes a new method, named Test-time Adaptation with Calibration (TTADC), to handle arbitrary unknown test label distribution shifts, which deal with the practical issue of test class proportion being different from that of the training set. Specifically, TTADC first trains K representative one-dominating-class classifiers during training, followed by adaptive Test-time aggregation of that K classifiers exploiting augmentation consistency. Experiments on real-world medical diagnosis tasks demonstrate the effectiveness of the proposed TTADC.	This paper presents the first method to tackle label shift for medical image classification, which effectively adapt the model learned from a single training label distribution to arbitrary unknown test label distribution. Note that the label distribution means the proportion of each class. The contributions can be summarized into aspects: C1: This paper innovates distribution calibration to learn multiple representative classifiers, which are capable of handling different one-dominating-class distributions. C2: When given a test image, the diverse classifiers are dynamically aggregated via the consistency-driven test-time adaptation, to deal with the unknown test label distribution. C3: The authors validate our method on two medical image classification tasks including liver fibrosis staging and COVID-19 severity prediction.	The authors propose a framework to apply test time adaptation to solve label distribution shift in the context of CNN for medical images.  The method is composed by two phases: 1) training different models that are able to handle different labels distributions 2) aggregating these classifiers during test time in order to solve the labels shift problem	A novel method for test-time adaptation, i.e., TTADC, is proposed, with strong and convincing empirical evaluations. Experiments on real-world clinical data demonstrate clinical feasibility. The paper is well written with clear logic.	S1: This paper presents the first work to effectively tackle the label distribution shift in medical image classification. S2: The paper is overall clearly structured. S3: Authors have tested the proposed method on two public datasets to demonstrate the effectiveness of the proposed method and have conducted extensive ablation studies.	Well-conducted research. The paper is well written and the research is well conducted. every choice is well documented and results are convincing and presented in a proper way (mean and std) different tasks are evaluated (Liver fibrosis staging and Covid19 severity prediction) Ablations showed that their proposed components are actually improving results Results. They compared with different SOTA models (even if these SOTA methods were not built with a medical setting in mind) and achieved the best results	The clarity associated with the derivations, especially on Eqs 1-3, should be significantly improved.  The underlying assumption on multiple binary classifcations with sigmoid function is not discussed in detail.	W1: Although the authors have elaborated the significance of the tackled problem: label distribution shift, I cannot grasp the significance of solving the label distribution shift issue if there is no long-tail problem in training dataset or there is no domain gap between training and test datasets. Since we usually conduct inference for test image one by one, the final classification performance is not sensitive to the label distribution of test data. W2: As illustrated in the experimental section, the training and test data are from different centers with domain gap. What is the difference between the tackled label distribution shift problem and widely investigated domain adaptation tasks, such as domain adaptation and test-time adaptation?	"The difference with [34]. Please add information on how much and in what this method differs from the cited works as the provided explanation (1 line with no clear info) is no enough in order to compare them 2.3 ""...with the implicit knowledge of label distribution on the test set"" can give more insights about this information? I see from the test time adaptation loss that label information is not present (and I suppose that this information cannot be used) but this sentence leaves me with doubts The advantages of test time adaptation are not clear to me in the medical setting. If the authors can clarify this aspect in the paper in the rebuttal would be beneficial. Why do I need to adapt at test time? How your model works when trained on a balanced train and test sets and compared to a classical training procedure? I think that one of the major barriers to applying Ai in medicine is that models are not still effectively explainable. I don't know how a model that adapts itself at inference time could be helpful in the medical community."	Code is not available yet. But the TTADC is expected to be readily reproducible.	Authors claim that code will be available after review.	everything is ok	"Please highlight the assumption that all test data samples are accessible simultaneously. It's assumed that, after converting multi-class classification into multiple binary classifications, the resulting binary classifications are conditionally independent. This is a strong assumption that need careful justification. Please elaborate on that. The proof of Eq.(1) is not convincing, because the connections/assumptions between y_i and y_{-i} (given x) are completely missing. Will Eq (1) still hold? Under what assumptions? The clarity on Eqs 1-3 should be significantly improved. For example, the main idea is not clearly stated; the terms related to ""expected"" or ""training"" can be confusing. Eqs (1) and (3) are not easy to understand without the supplementary materials."	It is suggested that the authors should provide the comparison between the tackled label distribution shift problem and long-tail problem. Moreover, the comparison between the tackled problem and test-time adaptation task should also be discussed.	I find the paper valuable and well written, but I can't see the advantage of test time adaptation in a medical setting. Let me explain better with a simple example. COVID detection or severity like the example in the paper: I would like to have a model that, given a single CT scan image, can detect if covid is present, the severity and the areas that lead to that decision without influencing how many other positive cases. I suppose that the goal is to train models that can extract features related to the pathology. Why my model will be influenced at test time if I trained it in a balanced manner?. Maybe you can explain better in the paper or in response to that review what is the advantage.	The presented techniques are novel and likely useful to many practical applications.	My recommendation is based on my concerns in 4 and 5. I am willing to upgrade my score if authors can address my concerns in 5, that is, clarify the significance of addressing the label distribution shift issue.	My rank is primarly based on doubts on the value for the medical setting of this work. Anyway the work is valuable from all the others point of views and so I decided to leave a weak accept in order to wait for a clarification
502-Paper2231	Test-Time Adaptation with Shape Moments for Image Segmentation	The authors propose a method to adapt a segmentation DNN using just the test data during inference. The adaptation is using a single subject. A UNet based DNN is first trained on the source domain using standard cross entropy loss. During test time, only the network's batchnorm parameters are adapted on the target subject using a combination of shannon entropy and shape descriptor losses (Shannon entropy for high confidence predictions, KL div between class ratios, Soft penalties on centroid and distance-to-centroid descriptors). The method is applied to two different scenarios:MRI to CT adaptation for cardiac segmentation (MMWHS data 20each), and cross-site adaptation for MRI prostate segmentation(NCI-ISBI Challenge T2-weighted MRI 30 each). The metrics used in results are 3D DSC and ASD. The method is compared against TTA, DA and SFDA methods. The proposed methd performs better than other methods for the DSC metric in both the cardiac segmentation and prostate segmentation scenarios.	The authors present a single subject test-time domain adaptation approach for image segmentation. The method is based on the entropy minimization of the target image's softmax prediction, introduced by Wang et al. and further extended by the authors to consider three different shape moments (class-ratios, class-centroids and class-centroid-distances) into the adaptation process. Related work implementations and results with ablations are presented on an MRI-CT multi organ cardiac and a cross-site prostate segmentation dataset. Overall, the authors could show significant performance increases by incorporating the class-ratio and class-centroid moments regarding the Dice score on both datasets and regarding the average surface distance on the MRI-CT task.	This paper proposed a simple formulation for source-free and single-subject test-time adaptation of segmentation networks, and demonstrated its performance in MRI-to-CT adaptation and cross-site adaptation.	The paper is well written and easy to understand. The authors use publicly available datasets to validate their approach. The validation benchmark includes the entire range of methods : From NoAdap (lower bound) to Oracle (Upper bound). The proposed approach has been compared against methods that include TTA, DA and SFDA.	The major strength of the presented test-time domain adaptation method is the increase in performance over classical and source free domain adaptation methods, which is shown to be achieved by the addition of the introduced shape moments.	This work proposed a shape-guided entropy minimization objective with shape moments to achieve the task of test-time single-subject adaptation. From the experiment section, the proposed method exhibits better performance compared with other methods. The chosen datasets in section 3.1 are appropriate to exhibit the performance of the proposed method, by dealing with MRI-to-CT adaptation and cross-site adaptation.	-Minor spelling mistake - 3.1 Adaption instead of Adaptation. -The Shannon entropy expression in page 4: Did the authors mean to put the class weights inside the summation over 'k'? -Compared to ref [2] and [20], the novelty is limited to inference on the test subject and use of soft constraints on centroid and distance-to-centroid shape descriptors. -How does the approach do when the DNN is adapted using the proposed loss function on the entire target domain data?  -There is a large gap between the oracle and the proposed method.	There are two major concerns regarding the work. First, the presentation of the mathematical aspects of the approach is inconsistent and accordingly at parts very hard to follow. There are no major issues, however multiple inconsistencies sum up and make it hard to build an intuition of the general idea of the work (see section with detailed constructive comments). Second, it seems that the chosen datasets are well suited for the incorporation of the defined shape priors, especially the centroid. It might be a good idea to optimize the center of mass of a class within a slice of the image towards the estimated center of mass defined by the whole 3D image for roundish classes (as they appear in the two datasets), however, this is not necessarily true if there are i.e., classes with elongated shapes diagonally covering the whole image domain. Intuitively, the matching of the distance to the centroid from one slice to the whole stack of slices is probably more robust in that regard, the authors could probably comment on the choice of Moments in their discussion, why and where they are expected to work well?	It is hard to understand the main idea of this work for the first time. Compositions of Figures 1 and 2 need to be improved.	"-Details about the data preprocessing and the data splits are provided in the paper. -Details on training are provided in ""Training and implementation details"" section."	The datasets are publicly available, the code as well, with the correction of a few definitions in the description of the method, the work should be reproducible. However, as the method is running an optimization process at test time it would be good to include the details of the used hardware and the measured runtime in order to know what the expected inference time for a sample of the target domain is.	The authors provide source codes for this work and the main datasets used are from challenges. Therefore, the reproducibility of this paper can be achieved to a certain extent. But, it is better to provide more information, like the description of system dependencies, instructions to train and test models, and data preprocessing.	The paper uses shape descriptor based losses to perform TTA of a segmentation model that is trained only on the source domain. This is a good way to add shape priors to the model, while spending significant training time only on the source domain dataset. The benchmarks are good and show the improvements due to the method. One thing that is missing here is - How does the approach do when the DNN is adapted using the proposed loss function on the entire target domain data? This would show how much better the proposed method is compared to [2]. Why was this not done?	We found the following mathematical inconsistencies: The index variable n is used first at page 3 in the Method section to identify an image I_n, whereas it is only defined later (page 4) as the index variable identifying a slice of the target image, therefore in equation 2 it is unclear what the domain omega_n should be (which by itself is also never defined). Equation page 3 bottom: u and v prime are not defined, are they the components of the centroid as defined at the top of page 4? Table 1, Class-Ratio: what is the domain omega_T in this equation, should it be the lower-case t (target domain) defined a little later? The weights ny_k in the formulation of the weighted shannon entropy should probably be after the summation symbol, not before? Generally, the notation of scalars, vectors and matrices is not consistent. First, bold fonts are used, later i.e., for the mu after it is stated to be vectorized not anymore. Result presentation: Table 2+3: The Dice score is unitless. The ASD should be converted to mm which allows a much better comparability over different datasets. It is unclear why TTAS_RC and _RD are proposed methods but the TTAS_R under the heading of ablation study? The ablation with all three moments would be interesting as well? Minor linguistic and structural inconsistencies are:  [..] variations in image modalities ... without in  [..] Standard DA methods, such as [18,17,5,22,18] ... duplicated reference and order  [..] is unavailable during training  ... available during training  Page 4, subsection Test-Time adaptation and inference ... it is described how the centroid and distance to centroid moments are estimated but at this point it is unclear how the class ratio is estimated. Page 6, estimating the shape descriptors ... it would probably be much cleared to shortly describe how R prime is created instead of referencing to [1] Other:  I wonder how the results would change if a Dice score-based loss is added to the CE loss in the pre-training on the source domain and the oracle as it is standard in state-of-the-art image segmentation to use a combination of both.	"It is hard to understand the main idea of this work for the first time. The authors should use more detailed and organized descriptions to state what are shape moments and how to use them. By the way, I am not sure whether the citation of [14] (section 2) is correct or not. I did not find the explanation of ""Shape moments"" in [14]."	The paper addresses a relevant problem of TTA using shape descriptors for adaptation. The amount of data available in the target domain could be low, and such methods are useful in those scenarios. The authors add minor changes to what is already proposed in ref [2] and [20]. Although the results perform better compared to other methods on the DSC metric, it is still significantly below the Oracle. It also does not do that well on the ASD metric on most of the cardiac classes and the prostate.	The major increase of performance using the introduced shape moments, even over domain adaptation methods which are specifically trained on datasets of the target domain outweighs the weaknesses of the work. The mathematical inaccuracies can be addressed and comments regarding the performance on specific datasets and the selection of the given shape moments can be included.	This work proposed a shape-guided entropy minimization objective to improve the segmentation performance in target domains. The experiments show the proposed work exhibits better performance.
503-Paper0733	Test-time image-to-image translation ensembling improves out-of-distribution generalization in histopathology	This paper proposed a new test-time data augmentation based on multi-domain image-to-image translation,  which is achieved based on a StarGanV2.  The proposed network achieves better performances on 3 public datasets compared with other SOTA methods.	The focus of this study is to improve the model generalization. To this end, test time image augmentation is performed based on multi-domain image translation. During test time, the proposed pipeline transforms an input image into a number of invariants with a multiple domain-specific styles. All these invariants are then passed through a classifier to obtain their respective class predictions. The final prediction to the input image is decided based on the ensembling strategy.	The authors propose a test-time augmentation (TTA) strategy based on StarGAN-V2 to specifically improve the domain generalization ability for histology images. This method is intended to overcome the stain variations across different data centers, which is a very common challenge in histology analysis. Three kinds of ensembling methods have been proposed to enhance the TTA.	Very intuitive idea and the proposed algorithm gets a good boost Experiments were conducted using multiple seeds on  large enough datasets while comparing many mainstream ood generalization methods	The application of generative models at test time is a novel idea for improving the model's performance. The proposed approach is evaluated for two different tasks and compared with a number of other approaches.	Domain gap is very practical and challenge challenges for almost any multi-center histology, due to stain variations. This paper provides a new strategy (i.e., test-time augmentation) in addition to the previous stain normalization/augmentation for addressing this issue, where the previous methods are training-time based or prior to the training time.	Although there is a good improvement, however, the idea's novelty is still somewhat lacking. A similar idea can be found in https://proceedings.neurips.cc/paper/2021/file/a8f12d9486cbcc2fe0cfc5352011ad35-Paper.pdf.  They achieved 94.8% on camelyon17. When comparing with other TTA methods, can the authors provide a forward count/test time comparison? The performance increases while the time required by the authors' method may increase many times, which is fatal for WSI images	The manuscript could be improved further and needs clarity and details on various points (see point 8). The proposed approach may not scale well with an increase in a number of domains. Getting predictions against a large number of invariants in order to get a final prediction for an input image would be time consuming. It is not clear if separate discriminator and classifier are trained for each domain style. If so, it could be a bottleneck when there are many domains.	1) Efficiency is also important for medical image especially for the inference stage. Yet, execution time/computational cost are not compared. 2) No comparisons with any stain normalization approaches, which are mostly used in histology to overcome the stain variations. 3) To make the comparison fair for evaluating the performance of StarGAN-V2 (which is a stain augmentation approach), comparing with (i) TTA + Stain Normalization; (ii) TTA + Stain Augmentation [ref. 21], which are much more efficient  histology-specific TTA approach, are missing. These approaches needs no extra computational cost to train a StarGAN but shows very good empirical results from my experiments (which is a very effective and efficient techniques for competition).	Very clear description, can be reproduced	No links or references for the source code and dataset, however the authors have stated in the reproducibility form that they will release it upon acceptance of the paper.	Probably fair. But it would be better if the authors could release their codes.	The author should compare with https://proceedings.neurips.cc/paper/2021/file/a8f12d9486cbcc2fe0cfc5352011ad35-Paper.pdf, and discuss their difference. The authors should provide any compared metrics like forwarding counts or test time when compared with other top methods.	"I would suggest adding details of how mapping between different domains was learned with a simultaneous classification task. The inference pipeline is clear, however training is not. Page 2 ""UDA methods needs unlabeled data from the target domain"". Do you mean availability of domain labels? Please add reference in the last sentence of the last para of Introduction section. Section 2.1: a random latent latent code -> redundant latent Add details on training the model. Due to page limits, this could be added to the supplementary doc. Give reference to Supplemenaty Figure 2 and Table 1 in the main manuscript. Add details of baseline method. Also, it is not clear what ""Base + XYZ method"" (Table 1) really means. What are those values in Table 1."	Execution time comparison, and performance comparison with (i) TTA + Stain Normalization; (ii) TTA + Stain Augmentation [ref. 21] are suggested (please see weakness for more details as I am not going to repeat.)	The novelty of the paper	There are various points that need clarity. The proposed approach is not scalebale.	Lack of proper experimental comparisons with simple but efficient competitive methods (see weakness #3)
504-Paper2524	TGANet: Text-guided attention for improved polyp segmentation	The authors focus on polyp segmentation and propose TGANet to use auxiliary classification task to improve the final performance. TGANet predicts the number and size of polyps and embeds them as the weight in channel attention. Experiments show that the proposed method can improve the final performance on multiple datasets.	The main contributions of the work include - 1) text guided attention, 2) feature enhancement module, 3) multi-scale feature aggregation, which improve the polyp segmentation performance. The proposed TGANet surpassed the recent related works on four public benchmark datasets.	The authors propose an auxiliary classification framework to weight the text-based embedding that allows network to learn additional feature representations. Also, they propose some attention modules to futher improve the performance.	The proposed label attention can further utilized classification information to constrain the feature maps for segmentation. The experimental results demonstrate improved performance using four datasets.	The motivation is clear and the idea is reasonable. The network is expected to be aware of the context attributes and the authors realize it in terms of size&number. The text guided attention module is portable for different networks. The text labels of polyp attributes are easy to generate automatically, so that the training does not need extra manual annotations. The experiments are comprehensive. The authors evaluated the proposed TGANet on four publicly available polyp datasets and compared it with five recent medical image segmentation methods.	The authors exploit size-related and polyp number-related features in the form of text attention during training. It helps the network to improve the performance  for the 'small', 'medium' and 'many cases'. Also, they conduct lots of experiments to show the improvement on four different datasets.	TGANet uses many channel and spatial attention modules in the baseline, such as FEM. From Table 3, it seems the FEM is more important which improves the performance by 2.4%. However, simply adding such channel and spatial attention in FEM is a well-known trick for DNNs and lacks of the novelty. To better evaluate the effectiveness of label attention, the ablation study lacks of the comparison that only using label attention (without FEM and MSFA)	The most important innovation, text guided attention mechanism, is not fully explained and investigated. The feature enhancement module and multi-scale feature aggregation are just new re-designs but not novel ideas. The authors should emphasize more on text guided attention mechanism. The so-called FEM and MSFA might be useful but not fresh at all. The Label attention module should be introduced in detail, with more sentences. For example, byte-pair encoding (BPE) is proposed in NLP area. The authors should describe why and how BPE is used. Or the readers would be confused. More experiments about text guided attention should be conducted and presented. For example, the classification performance, the visualization&discussion on the generated attentions.	"As for me, I think this paper does not bring many new insights. They add some attention-based modules (""Feature enhancement module"") and multi-scale feature learning modules to improve the performance. However, I think these similar components have been proved many times to be effective for network training in recent works, e.g. CBAM, Non-local, U-Net... Also, the text-based embedding method is more like a multi-task framework. I think the multi-task framework also has been used in many recent works."	The key details are sufficient to reproduce the main results.	The paper introduces the network architecture clearly. The experiments are conducted on four public datasets. Code is not released.	The authors plan to release the codes.	As shown in weaknesses part, the authors may need to add the ablation studies only using the label attention for further discussion. Since channel and spatial attention introduce additional parameters and flops, the authors may need to report the influence on FPS when adding different components in Table 3. The authors may need to show how to determine the label of size classification for the cases with one or more polyps.	"Fig. 1(C) looks very twisted. The layout and arrow lines can be better designed. There should be a section number ""2.6"" before ""Joint loss optimization"""	"More comparisons with recent works. The proposed ""Feature enhancement module"" should be compared with recent attention modules, e.g., CBAM, Non-local. What is the definition of ""small"", ""medium"", and ""large""? Please show the specific measurement for these different groups. Showing the performance if removing the labeling attention module but just keep the ""Num polyps"" and ""Polyp size"" tasks. I'd like to see the improvement of the labeling attention module besides the improvement from the multitasks of ""Num polyps"" and ""Polyp size""."	As shown in the strengths and weaknesses parts, TGANet proposes the label attention which can use the classification results to guide the segmentation features directly by byte-pair encoding and channel attention. Although TGANet contains many existing attention modules, the ablation study shows the necessity of the label attention. Besides, experiments show that the TGANet achieves better performance than other advanced models.	The idea of text-guided attention is innovative and impressive. The experiments are comprehensive. However, the authors did not adequately focus on text-guided attention. The insights and evidences about text-guided attention are lacking.	They conduct lots of experiments to show the improvement on four different datasets. As for me, I think this paper does not bring many new insights.
505-Paper2266	The (de)biasing effect of GAN-based augmentation methods on skin lesion images	This paper discusses (de)biasing effect of using GAN-based data augmentation. and  introduce the dataset with manual annotations of biasing artifacts in six thou- sands synthetic and real skin lesion images, which can serve as a benchmark for further studies.	The paper investigates the tendency for conditional and unconditional GANs to exacerbate biases in their training databases, specifically looking at artifacts (e.g. dermatological markings, frames, etc...) and natural features (e.g. hair). The authors have found that for their GANs, strong correlations, spurious or otherwise, tend to be amplified and rare events suppressed. Interestingly, the authors suggest that unconditional GANs (trained separately on the two data classes) are less biased than conditional GANs (one GAN to generate both classes).	In this paper, the authors analyse the impact of data augmentation and bias inheritance on melanoma classification from skin lesion images. They experiment different settings such as manual annotation GAN augmentation, and artifacts such as hair, gel, ruler, frame, ... Results give specific insights on the bias generated with each method.	(de)biasing effect of using GAN-based data augmentation is discussed in great detail.	The second experiment on counterfactual artifact insertion is the defining strength which strongly supports their conclusions regarding the exascerbation of certain biases in the data caused by artifacts present in both conditional and unconditional GANs. The first experiment demonstrates some of the effect that the authors claim, although not to the same degree. The cautionary motivation for the paper is really quite strong, and it provides a measured response to the argument that generative models can completely solve the problem of data availability.	The authors explore a large and relevant range of possible bias in skin lesion images, sharing these annotations is acknowledgeable. Besides classical metrics, counterfactual bias insertion metrics provide complementary information to the bias effect.	Novelity of the paper need to be mentioned clearly in an introduction section. GANs are being used widely for data augmentation. How the (de) biasing will change the overall workflow?	"The authors in the introduction clearly state that they are interested in understanding algorithmic bias, setting aside questions of racial, gender, etc... bias. This is very odd considering that the domain they are investigating, dermatological image processing, is definitely very affected by ethnicity and racial considerations. There is a clarity issue for Section 3.3, notably the difference between the aug. GANs and the synth. GANs. One assumes it appears that artifacts were inserted into the evaluation dataset (in order to calculate the number of ""switches"") and also into the training set for the aug. GANs (and not the real data nor the synth. GANs) but this should be made explicit."	The quality of the annotation is not evaluated; for instance no information are given on the expertise of the annotator (practitioner, naive, ...). This have a direct impact on the final evaluation results.	Data is available online	The paper is conceptually reproducible, using well-known methods and well-defined techniques.	Implementation details are given. Manual annotations will be share publicly.	Cross validation result need to be reported rather than considered testing on some fixed data. More details are needed for unconditional and conditional GANs. Results should be evaluated on other bigger dataset. Discussion section need to be added as currently results section looks week from discussion point of view. Please mention about parameter tuning inside GANs fully. Compare the results of classification with other state of the art work on same dataset. In Table 2, mention standard deviation along with mean value. Grammatical errors need to be corrected throughout the manuscript.	"Overall, the authors seem to confuse artifact removal with bias removal, which are not necessarily the same thing. The authors appear to use the term ""debiasing"" to refer to the removal of particular artifacts such as short hairs, but this isn't really debiasing the data. One would assume that debiasing would somehow be equilibrating the frequency of certain features causally known to be separate from the task at hand between the two classes."	It would be interesting to provide some significance metrics when comparing different parameters. Also, the title can be misleading as it seems generic. It would have been better to mention that it is about bias on skin lesion images.	Paper seem to be raising interesting topic of debiasing in GANs.	The paper is well written and motivated and I think it would pose a good contrast to the GAN papers that normally appear at MICCAI that propose new models. The main weakness I see is the lack of expansion of the method to sources of bias that are also very important in the area (namely race) which are harder to address using simulations in the same way that adding frames can be addressed.	The experimental setting is intersting. However, the conclusions are 'local' and may not be generalized to other medical applications.
506-Paper2284	The Dice loss in the context of missing or empty labels: introducing Ph and 	The paper analyses the commonly used Dice loss in the context of missing or empty labels. It provides a formulation for the loss in terms of reduction dimensions 'phi' and smoothing term 'epsilon', and demonstrates how their choice influences segmentation results.	This paper investigates the underlying mechanism of Dice loss by checking its derivative during model training. Compared to some existing works that study the Dice loss, this paper provides more details about the derivative as well as the reasoning about the choice of $\Phi$ and $\epsilon$. Based on some theoretical analysis, this paper proposes heuristic combinations of $\Phi$ and $\epsilon$ that help train the segmentation network under missing or empty labels.	The authors study the Dice loss in the frequent context of missing or empty labels. They provide a useful formulation of the loss and clearly present the different possible subsets used for the reduction dimensions (image, batch, class), i.e. dimensions on which to aggregate the intersections and unions. They show the importance of these dimensions and of the smoothing term when dealing with missing or empty labels and propose well motivated heuristics for setting these parameters. Segmentation experiments on two public datasets (BRATS, ACDC) are performed to illustrate the effect of the heuristics, with quantitative and qualitative results.	The paper provides a formulation that generalizes several loss functions in the context of missing or empty labels. It demonstrates how to tune the parameters 'phi' and 'epsilon' based on the problem at hand (correctly handling missing labels or empty labels.	(1) This paper provides a theoretical analysis on the derivative of Dice loss, as well as the effects of $\Phi$ and $\epsilon$. Such an analysis provides a deeper understanding of Dice loss. (2) The experimental setups are reasonably designed, and the evaluation results verify the claims made in the paper to some extent.	The paper is well written and structured. Missing and empty labels are common problems in medical image segmentation that will benefit the clear formulations and proposed heuristics. Beside the limitations mentioned in the weaknesses, the experiments are well designed and described, the results support the hypotheses.	The introduction section is not clear and not helpful for understanding the context of the paper. The topics of missing and empty labels are barely mentioned there.	"(1) I would like to point out that this paper is not easy to follow. Some descriptions in the paper are confusing and unclear. For example, for the BRATS dataset, what are partially and fully labeled data? In addition, the heuristic in Section 2.3 is not easy to understand. For example, what does it mean for the sentence ""A very simple strategy would be to let ...""? (2) In the experiment, it is mentioned that the marginal Dice loss and the leaf Dice loss are compared. However, I did not see the results for these two losses in the experiment. Do these two losses correspond to some certain B values in Table 1? This is confusing. (3) To demonstrate the heuristic regarding the choice of $\epsilon$, this paper sets $\epsilon$ to the expected volume of the groundtruth, which turns out to work well. To better illustrate this heuristic, it is necessary to conduct experiments where other $\epsilon$ values are chosen. This would provide a more effective comparison for the impact of $\epsilon$ values."	An experiment with real empty labels would be needed (cases or patches with only background, instead). Here the results show that the model starts learning when to predict empty labels when psi and epsilon are designed to do so, but only as a negative result (lower DSC). In the presence of real empty labels, we should see an increase of performance by reducing false positives in the empty maps.	The authors stated that they will release the code and the related material for the paper.	The paper mentions that the code will be released upon acceptance. This would be helpful in reproducing the results. In addition, this paper also offers some training details for reproducing the results. But these details may not be completely sufficient to obtain the experimental results.	Mention to release the code upon acceptance, public datasets	"The article talks about the dice loss in the context of missing and empty labels. However, the introduction doesn't explain the need behind it and the relevant use cases for each one of them. The introduction talks about general papers about the dice loss, but mentions only a handful of works about the topic of missing or empty labels. The relevant papers are cited much later in the methods section. This makes it difficult to follow through the article. In the introduction the reduction dimension ""phi"" is mentioned without any background or formula to understand what it is referring to. It becomes clearer only later. DL_CI and DL_BCI are mentioned in section 2.1 but are not used or compared to in the experiments. Their function is therefore not clear."	(1) The writing of this paper should be improved to make it easier to understand. In particular, it would be better to additionally use plain language to explain the insights and intuition. (2) Additional experimental results are required. For example, it would be helpful to provide the results by other $\epsilon$ values. Also, it is necessary to provide results of marginal Dice loss and leaf Dice loss.	"As mentioned above, I would recommend experiments with real empty labels, to show that it reduces false positives in cases with only background, e.g. patches that do not contain a tumor, cases with and without a lesion etc. Other segmentation losses could be briefly mentioned in the introduction The main difference between missing and empty labels and their implications should be discussed. False positives to take into account in the case of empty labels? It comes later in 2.2 and 3, but it would be good to explicitly mention it in the introduction. In 2. I would remove ""most general case"", as it is in the case of segmentation. The intermediate part of eq. (1) could be removed."	The paper has a significant contribution to the understanding of dice loss in the context of missing and empty labels. It provides a generalization of the loss and shows the impact of the parameters on segmentation performance in this context. However, its organization and clarity are lacking.	Major factors: the paper writing and experimental verification. Many descriptions in the paper are not clear. This prevents the understanding of this paper. For the experimental verification, some additional experimental results are needed to further verify the proposed idea.	The formulation is important. I agree with the authors that it could help other authors to clearly present their Dice loss implementation. The heuristics are well motivated and results seem promising. The experiments are not sufficient to fully illustrate the approach
507-Paper1636	The Intrinsic Manifolds of Radiological Images and their Role in Deep Learning	This paper analyses intrinsic dataset manifolds empirically, and shows that medical imaging datasets have a lower number of intrinsic dimensions than natural images, but still medical image recognition tasks are not easy. The authors indicate that their analysis highlights the importance of developing tailored medical image recognition approaches, and not just purely relying on the advances in the natural image recognition.	This paper computes the intrinsic dimension of the dataset using the method in [18] for natural images and medical images. The experimental results show that the medical images normally have smaller intrinsic dimension as well as worse test accuracy than natural images. The results empirically show that there is difference between natural images and medical images, and thus indicate more careful thoughts are needed to transfer from natural images to medical images.		Great topic Great organization of the paper Very lean approach Large enough benchmark	Empirical results are sound by applying the ID estimation on multiple medical image dataset and natural image dataset. The different choice of neural network shows similar results. The finding of negative dependence between test accuracy and ID is foreseeable, but the difference of scope between natural image and medical image is novel and interesting.		I could not identify any. However, code release that allows to reproduce the findings would be nice.	"Theoretical contribution of this work is weak. The ID computation is based on the existing method by [17,18], and directly used on medical and natural images. There's another concern is that maybe the ID computation for natural images is different from medical images. For example, the natural images are bounded by 0-255 and use 3 channels, while radiological images are not bounded and normally only contain 1 channel.  Also, although Normalization layer might help to train the neural network, the distribution of radiological image pixels are very different from natural image pixels. So the current existing neural networks like resnet etc. may not be a good method to measure the test accuracy. The ID computation is based on the assumption of Poisson process and MLE. This is slightly concerning as lacking of evidence. For example, one can show that auto encoder can/ cannot learn a reasonable reconstruction error for different dataset to show the empirical ID for each dataset. This paper directly uses the existing method to this specific domain and shows no toy data example to support the ID estimation. Therefore, the overall estimation of ID might not be exact and the finding of different scope of negative rely may come from very different aspects other than ""ID difference and domain difference""."		The results are reproducible, but code release would be nice.	It should be reproducible.		As I have mentioned before, I am happy with the paper as is, but it lacks the openly available code.	Show some evidence about the ID estimation is correct/ exact. Rule out all other potential possibilities to affect the relation between test accuracy and intrinsic dimension and domain to finally make a strong and clear statement. Fig. 4 right is pretty, but the 3D version is hard to view which color is above another. Maybe reduce the \alpha or show the lapping information in a better way.		Good idea, experimental validation, and valuable insights	Even though the empirical finding has some flaws and theoretical contribution is not enough, the current finding of different scope of negative relation between test accuracy and ID is a fresh aspect to think the difference between radiological images and natural images.	
508-Paper2523	The Semi-constrained Network-Based Statistic (scNBS): integrating local and global information for brain network inference	The manuscript presents an extension to the Network Based Statistic (NBS) and constrained NBS (cNBS), by using a data driven policy to refine the large-scale constraints used in cNBS. The method, called semi-constrained NBS (scNBS), is based on four steps: network partition, marginal ranking, cut-off selection, and network-level inference. On semi-synthetic data, the Authors compare scNBS with multiple alternative solutions, like cNBS and NBS, claiming increased specificity, power and consistency.	This paper proposed a connectome-based inference that integrates both local and global information, called semi-constrained network-based statistic (scNBS). In experiments, the proposed method was applied to synthetic and true brain imaging data. The results showed that the proposed method increased statistical power and validity in synthetic data, and showed consistency for repeated measurements in resting state brain imaging data.	In this paper the authors propose a novel method, called Semi-constrained Network-Based Statistic or scNBS, that uses a data-driven selection procedure to pool individual edges bounded by predefined large-scale networks to compare functional connectivity matrices and tries to overcome the main issues raised by conventional edge-wise approaches.	The strong aspects of this work are: the detailed description of the scNBS, corroborated by excellent figures; the very interesting experimental design and data, which mixes Human Connectome Project resting-state fMRI data with somewhat-realistic synthetic phenotypic data.	The proposed method could account for both local and network-level interactions that might have clinical significance.	* Novel approach grounding on solid statistical basis * Important impact in the field, with possible applications even in clinical scenarios (e.g., comparing patient data)	"It is not entirely clear to me if there is some potential issue of circularity (a.k.a. double-dipping, bias) in the proposed methodology. For example, in Section 2.c, where the cut-off selection is described, the Authors explain that ""the final threshold is the one that gives the largest effect among all possible cut-offs"". Typically, steps like this one, requires a careful nested cross-validation implementation to avoid that selection of parameters is operated in a biased way. Unfortunately, the manuscript is not clear on this (and other) implementation details but they claim to disclose the code of their experiments. A second source of concern is the experiment's section, which is entirely based on ""synthetic"" data (I'd say semi-synthetic, since it heavily relies on actual fMRI data). The noise models are quite elementary and may not properly represent physiological situations."	The algorithm performed after subnetworks were predefined. If the subnetworks were not known, how could the proposed method work? The cut-off values, t_{g}^{+} and t_{g}^{-} were obtained by the correlation with y, and the final inference was also estimated by the relationship between V_{g}^{+}(t_{g}^{+}) and y, and between V_{g}^{-}(t_{g}^{-}) and y. The results obtained during the process of the proposed method were reused for the final results of the proposed method. It would be better if the results of the consistency analysis of the other methods was also shown in Fig. 6.	* I do not see major weaknesses in the paper	As said above, the method is described in detail but it may be prone to circularity issues in the implementation. In the reproducibility statement, the Authors claim to disclose the code of all experiments, which should enable meta-reviewers and - if accepted - future readers to verify important technical aspects. I invite the Authors to clearly mention in the manuscript the complete availability of the source code.	The order of the paper was a little unfamiliar. It would be better for Sec. 2 scNBS to be included in Sec. 3 method, and for the subsection 'Synthetic Data for Benchmarking' to be included in Sec. 4 Results. The words in the figures and the figures were too small to read. In Fig. 3, R is not defined. In the second line on page 4, the parentheses are missed. There is no explanation of how S_{g}^{+} and S_{g}^{-} were used. t_{g}^{+} was selected when the correlation between V_{g}^{+}(c) and y was maximized, however, t_{g}^{-} was selected when the correlation between V_{g}^{-}(c) and y was minimized. Why was the negative effect in t_{g}^{-} selected when the correlation between V_{g}^{-}(c) and y was minimized? The cut-off values, t_{g}^{+} and t_{g}^{-} were obtained by the correlation with y, and the final inference was also estimated by the relationship between V_{g}^{+}(t_{g}^{+}) and y, and between V_{g}^{-}(t_{g}^{-}) and y. It would be better if the results of the consistency analysis of the other methods was also shown in Fig. 6.	Good. I would recommend the authors to release their code upon acceptance of their manuscript.	"My main constructive comment is about the experiment's section. The Authors should at least provide more ground to the choice of presenting only ""synthetic"" experiments and should at least provide perspectives for future experiments. My second main constructive comment is about discussing the limitations of the proposed method. The Authors should spend more effort on this side and characterize within which limits the method is expected to perform and underperform."	It would be nice to make the picture a little bigger and add performance comparison with the other inference methods to the result part.	In this paper the authors present a novel method (scNBS) to statistically compare the functional connectivity matrices from different conditions/populations trying to increase the statistical power which is greatly reduced when conventional edge-based methods are used. The paper is interesting and well-written, I enjoyed reading it, the figures are appropriate to convey the message and help the readers to follow the pipeline. I only have some minor points and suggestions: I would try to give more details and revise the section related to the generation of the synthetic data as at the moment it lacks a bit of clearness; Would it be possible to adapt this method to structural connectivity matrices? This would make the method even more applicable and general and I would mention this in the discussion; Please carefully proofread the manuscript as there are several typos across the different sections; I would suggest the authors to make their code freely available upon acceptance of the manuscript, as this would allow other interested researchers to explore and use this novel method.	The manuscript present an interesting extension of NBS and cNBS that overcome some limitations of those methods. The topic is surely of interest to this community. The proposed method is pretty interesting and the experiments provide support to the claims of increased specificity, power and consistency. The experiments are only on (semi) synthetic data and some steps of the description raise some doubts on circularity issues.	The proposed method could do statistical inference by accounting for both local and network-level interactions that might have clinical significance.	Interesting and clear paper, the methodology is sound and robust.
509-Paper0158	Thoracic Lymph Node Segmentation in CT imaging via Lymph Node Station Stratification and Size Encoding	This paper proposes a new approach based on LN-station-specific and size-aware LN segmentation framework.  As a result, high performance LN segmentation could be achieved with good generalizability.	"This paper describes a novel way to segment thoracic lymph nodes (LN).  By first combining LN-station to form 3 ""super-stations"", then differentiating between large and small LNs, and finally going through a post-fusion module to generate the final prediction.  This is an interesting framework made to guide the learning, and it succeeded in improving the performance comparing to other methods."	To overcome the difficulties of segmenting visible lymph node (LN) from CT images, a novel LN-station-specific and size-aware LN segmentation framework is proposed, which can explicit utilize the LN-station priors and learn the LN size variance. Two-stage learning process is proposed, thoracic LN-stations are segment and then grouped into 3 super lymph node stations firstly. A multi-encoder deep network is designed to learn LN-station-specific LN features; secondly, to learn LN's size variance, two decoding branches are proposed to concentrate on learning the small and large LNs, respectively. Validated on the public NIH dataset and further tested on the external esophageal dataset, the proposed framework demonstrates high LN segmentation performance while preserving good generalizability.	Stratified LN-statin and LN size encoded segmentation are validated based on the LN size variations.  Three super LN stations and learning framework were proposed and showed the high performance in metrics compared with the previous approaches.	"The formulation of the framework is interesting.  It seems intuitive to group the LN-stations to form ""super-stations"" to try to help the learning.  Similarly with the large/small differentiation. These ""pre-processing"" of the data seemed to have helped the learning. The use of an external dataset to validate the result is also interesting, as they have different resolution comparing to the training data."	Thoracic LN-stations are segment and then grouped into 3 super lymph node stations firstly. A multi-encoder deep network is designed to learn LN-station-specific LN features To learn LN's size variance, two decoding branches are proposed to concentrate on learning the small and large LNs, respectively. Validated on the public NIH dataset and further tested on the external esophageal dataset, the proposed framework demonstrates high LN segmentation performance while preserving good generalizability.	Three super LN stations are used in this paper but it is not clear how many super stations are optimal in general.	One would be tempted to think why a CNN would not be able to figure out by itself the grouping method for the super-stations, nor the large/small differentiation. While the criteria for the super-station grouping is intuitive, perhaps the authors should try different criteria and see the change of performance.  Also with the large/small differentiation, in which values other than 10mm could be tried. The datasets used in external testing are substantially different in terms of slice spacing, as these are 5mm vs 1.2mm in training data.  Also, these are all CT scans from patients with esophageal cancer, and I am not sure if there is any bias because of this.	In a word, this paper is not well organized, and the writing is needed improved thoroughly. And the English of the manuscript should be significantly polished before further consideration for accept. The architectural settings are rarely discussed. The reason of choice of nnunet blocks should be explained. And the structure of nnunet block should be described simply. The images offered in the manuscript are low-quality. It is recommended to improve with high-quality images. Moreover, it is better to repaint some illustrations with unclear intentions. For example, such Fig 2, the contents are too small.	It is better that code is available to confirm the whole procedures proposed in the paper although the paper is well written and reproducibility can be recognized.	Adequate.	The code of this work were not provided. The reproducibility is slightly worse.	The proposed approach improves the LN segmentation performance for the corrected test data of actual esophageal cancer patients.  The effectiveness of the original idea is shown in the experimental evaluations.  It is better that some failure examples are shown with the reasons as far as the segmentation performance is not perfect.	For the CT scans, the slice thickness may have an effect.  It is unknown if these were contiguous slices or overlapping slices.	To overcome the difficulties of segmenting visible lymph node (LN) from CT images, a novel LN-station-specific and size-aware LN segmentation framework is proposed, which can explicit utilize the LN-station priors and learn the LN size variance. Two-stage learning process is proposed, thoracic LN-stations are segment and then grouped into 3 super lymph node stations firstly. A multi-encoder deep network is designed to learn LN-station-specific LN features; secondly, to learn LN's size variance, two decoding branches are proposed to concentrate on learning the small and large LNs, respectively. Validated on the public NIH dataset and further tested on the external esophageal dataset, the proposed framework demonstrates high LN segmentation performance while preserving good generalizability. This study provided a meaningful approach, but there are several weaknesses: In a word, this paper is not well organized, and the writing is needed improved thoroughly. And the English of the manuscript should be significantly polished before further consideration for accept. The architectural settings are rarely discussed. The reason of choice of nnunet blocks should be explained. And the structure of nnunet block should be described simply. The images offered in the manuscript are low-quality. It is recommended to improve with high-quality images. Moreover, it is better to repaint some illustrations with unclear intentions. For example, such Fig 2, the contents are too small.	Three super LN stations and learning framework are most important idea of the paper.  Proposed model consisting of super station based stratified encoders, size-aware decoder branches and a post fusion module performs high segmentation performance.  It is shown new approach improves the segmentation performance further.	There does not seem to have enough contribution for an acceptance.  The method, however, has good results comparing to others.	To overcome the difficulties of segmenting visible lymph node (LN) from CT images, a novel LN-station-specific and size-aware LN segmentation framework is proposed, which can explicit utilize the LN-station priors and learn the LN size variance Validated on the public NIH dataset and further tested on the external esophageal dataset, the proposed framework demonstrates high LN segmentation performance while preserving good generalizability. Although there is no novel structure or theories proposed in this study, the proposed method demonstrates high LN segmentation performance while preserving good generalizability. Thus, I suggest receiving it after major revision.
510-Paper1449	TINC: Temporally Informed Non-Contrastive Learning for Disease Progression Modeling in Retinal OCT Volumes	The authors presents a modification of the recent VICReg self-supervised method for longitudinal imaging data. The modification is (1) using pairs of images that are randomly sampled from different time point of a patient, and (2) changing the first loss, which corresponds to the invariance of related presentations, to depend on the time between the two images, such that for smaller time difference will lead to stronger enforcement of similarity between the two representations.	This is an interesting paper. The authors propose a  modified loss function which they call TINC (Temporally Informed Non-Contrastive Loss) to be used with VICReg to predict whether an eye is going to convert to wet-AMD within 6 months time-frame.	This paper introduces a new loss function (TINC) to exploit existing temporal information in longitudinal context of OCT data. The proposed model outperforms the compared methods in terms of AUROC and PRAUC.	The proposed method is a simple yet interesting extension/adaptation of VICReg to longitudinal medical images. To the best of my knowledge the method is novel, and I'm not familiar with other demonstrations of the VICReg method to medical images. The experiments and results are satisfactory, in terms of showing that the proposed method can have superior performance over other self-supervised  non-contrastive methods: including vanilla VICReg, and simpler adaptations of it (which can be viewed as ablation study).	Modified the VICReg's invariance term by constraining it with the normalized absolute time difference between the input images	This paper addresses a interesting clinical question, i.e., Disease progression in the longitudinal contexts. This problem is not only existed in Eye, but also other organs. Superior performance has been achieved when comparing with some baselines.	The paper is not self-contained. I had to go to the original VICReg paper to understand the technical details of the method, and the intuition behind it. Also, there are additional unclear issues, which I detailed in Q8 below. The evaluation is done on a single dataset.	discussion around misclassifications and possible ways to improve missing	Only one dataset has been evaluated in this paper, which may limit its generability evaluation. If the authors only has one dataset available, cross-validation would be more appreciated. The written can be improved. The author states that they have exploited the meta-information. However, what kind of meta information has included, and how they are included is not clear.	Apart of few technical issues that I detailed below, and after reading the original VICReg paper, I think that the method is clear and the results can be reproduced.	Not reproducible	The authors state that they will release training code, pre-trained model and evaluation code. I believe the reproducibility of this paper is good.	"Th presentation of VICReg is incomplete and lacking a clear motivation for the three losses. I had to read the original VICReg paper in order to understand this paper.  Specifically std(z, epsilon) and ""d"" are not defined.    I suggest to provide a short introduction for the three loss terms, for example: the first loss is for reducing the distance between related presentations (e.g. invariance to data transformations),  second loss is for increasing the variance of the different elements in the presentation to prevent norm collapse, and third loss is for de-correlating the different elements in the representation, to reduce redundancy. In the original VICReg method, the batch of n patients is composed of n images, which undergo two separate transformation, t1 and t2.  The following sentence was unclear to me: ""Given a batch of n patients with multiple visits, let visits v1 and v2 be the n tuple of time points randomly sampled from each available patients' visit dates within a certain time interval.""   Does this mean that for each patient we randomly sample a pair of images from two different points? In other words, does tuple=pair, and each sample contains a single image? In Table 1, the vanilla VICReg had ""representational collapse"". The authors mention that in medical images a larger area-ratio crop should be taken. It would be interesting to see the results of VICReg with larger area-ratio crop but without the two visits input. Was there still a representation collapse? I suggest the authors add this result, to demonstrate the contribution of two-visits input. To assess whether the difference in the performance (AUROC, PRAUC) is statistically significant, I suggest to add p-values based on bootstrapping. I did not understand this sentence ""This can be explained by the fact that in Barlow Twins, the fine-tuning reached the peak validation score within 10 epochs, same as the number of linear training epochs."" Did you try increasing the number of epochs, maybe the fine-tuning takes longer to converge to a better model?"	This is an interesting paper. The authors propose a  modified loss function which they call TINC (Temporally Informed Non-Contrastive Loss) to be used with VICReg to predict whether an eye is going to convert to wet-AMD within 6 months time-frame Comments are below:  Include and discuss in a section to explain misclassifications. Are there images which were classified correctly by other methods and not by the proposed method. If yes, why? And also vice versa, why the proposed methods works better based on images as example.	I would suggest a more comprehensive study, e.g., more datasets if possible, cross-validation on one dataset. Consider the time is limited during rebuttal. I will not say this is a mandatory requirement, but the authors may consider it. Improve the writing to make the readers easier to follow. As the weakness 2, the readers may be confused about that. So other parts including:  Is the evaluation metrics (AUROC, PRAUC) computed based on volumes level or eye-level? Without this information, it is not friendly to readers who is not a expert on OCT.  2)	The novel and interesting extension of the recent VICReg method to longitudinal medical images and the demonstration of its utility.	novelty and few other weaknesses as given above	A new loss function to address a clinical-interested task. The authors also show promising results
511-Paper2727	TMSS: An End-to-End Transformer-based Multimodal Network for Segmentation and Survival Prediction	The paper proposes a transformer network for head & neck tumor segmentation and outcome prediction challenge using PET-CT and EHR data.	An End-to-End Transformer-based Multimodal Network for Segmentation and Survival Prediction of Head and Neck Cancer. The problem is of clinical importance.	The paper proposes the use of a multimodal transformer network to combine the tasks segmentation and survival prediction in order to achieve an improved performance for the survival prediction and a competitive performance on the segmentation task. The model is evaluated on the HECKTOR dataset and achieves superior performance in comparison to state of the art algorithms.	SOTA results on survival prediction Complementary segmentation which can potentially be used for RECIST computation etc. Combining multimodal data.	The problem is of clinical importance. Although Transformer-based Multimodal Network has been used to solve other problems, it is used for the first time for head and neck cancer segmentaion and survival prediction	Overall the paper is very well-written, easy to follow and soundly evaluated. The manuscript clearly states its contributions and has a strong focus, adequately leading through the overall concept. The authors provide a good overview on the current state of the art and demonstrate a profound knowledge in the field. The authors have agreed to disclose their code on acceptance. Finally, the authors achieve state of the art performance.	Images are cropped to 80x80x48. This potentially helps as a sort of attention/mask mechanism where the tumor region is now in focus. Using the entire image downsampled to the smaller size (for easier training) would have been fairer. No ablation studies on the patch size, contribution of modalities etc has been done.	It would good to provide more results. The hypothesis behind using the network is not clear	The authors have chosen to use a cross-validation and additionally conducted a metaparameter optimization. In this combination, however, the authors might have partly done a circular analysis.	Reproducible	"""An analysis of situations in which the method failed"" is missing"	"The reproducibility checklist well matches the contribution. I would like to argue, however, that significance analyses (""Not applicable"") are well applicable, and thus, that the depiction of measures of confidence would have been adequate."	Hausdorff distance can also be included to evaluate the secondary output of segmentation. Ablation studies to show the effect of patch size. Ablation to test the effect of using only PET-CT image (minus EHR). Range for intensity normalization can be included i.e. was the whole CT HU window used to rescale to 0-1 or similar specifics. Were any other EHR records identified (besides smoking and alcohol) to have missing data and was any imputation used for the same?	See the wekness section for details	Evaluation As to my understanding, the authors have chosen to use a 5-fold cross-validation and have further optimized their parameters using the OPTUNA framework. - In this combination, the authors might have partly done a circular analysis, namely they might have found parameters which perform well in this specific setting. - I would recommend to instead initially split off a hold-out set, which then the authors conduct their final analysis on, as only this would rule out the opportunity of statistical dependence of the model training and the later testing phase, makes the results more reliable and thus creates a clearer incentive for others to use the work later on, which would be in the authors' as well as in the community's interest. Further, I was not able to find any measure of confidence: - As the authors have conducted a cross validation, depicting the variances across different folds would have created a first impression on the confidence of the achieved results. - If the authors aim for a statistically more sound evaluation, I would recommend to also employ methods such as bootstrapping [1] and statistical testing, which would make it easier for the community to understand the value of the proposed method. Minor I would like to recommend to make the text in Fig. 1 a bit larger to facilitate reading. The table formatting in Tab. 1 felt somewhat old-fashioned. I would like to recommend [2] for this. References [1] Efron, B. (1987). Better bootstrap confidence intervals. Journal of the American statistical Association, 82(397), 171-185. [2] https://people.inf.ethz.ch/markusp/teaching/guides/guide-tables.pdf	The cropping approach which appears to use some domain knowledge that has not been clearly defined. Multimodal approach that achieves SOTA performance.	The application has clinical impact. The method outperformed existing works. Analysis is elaborate	The combination of transformer networks for simultaneously predicting lesion segmentations and survival estimates in order to achieve an improved estimate, and under the combination with EHR data to the best of my knowledge is unprecedented and has a large potential for a strong impact. Further, the paper is very well-written and easy to understand.
512-Paper2295	Toward Clinically Assisted Colorectal Polyp Recognition via Structured Cross-modal Representation Consistency	The manuscript represents an approach for authomatic colorectal polyp classification via structured cross-modal representation consistency on WL and NBI images.	The manuscript presents an algorithm based on Structured Cross-modal Representation Consistency for polyp detection in colonoscopy images.	This manuscript proposed a novel method to directly achieve accurate white-light colonoscopy image classification by conducting structured cross-modal representation consistency.	The paper is well-written and address a clinical issue. Also, it presents a novel designed Spatial Attention Module to calculate similarities.	Well written paper The addressed topic is of interest for the surgical data science community Experiments and comparison with the literature are performed	This manuscript has detailed mathematics, designs and implementations for each module, it is easy for people to follow even for those readers who are not familiar with attention model.	The proposed method is evaluated on a relatively small datasets, and no information regarding the computation costs as well as no discussion on the real-time application in clinical settings is provided.	The methodological innovation should be stated clearly The research hypotheses of the work should be stated (e.g., which is the hypothesis behind using SAM?) Limits in the literature should be highlighted. The discussion of the results can be improved to give more insights to the readers.	Need more visual representation for result section.	The technical implementation process is well described and the method is tested on public datasets.	Good.	It is very easy to follow the network design, with the information provided by the authors, it should be able to duplicate the network without major issues.	Did you perform any image augmentation in order to increase the number of images and avoid possible overfitting? Please clarify if the presented results on the Table 1 and Table 2 are on the same datasets or not. I highly recommend you to add an explanation regarding the real-time clinical application of the proposed approach.	"The introduction is a bit verbose (avoid using the same sentences for both abstract and introduction) The survey of the state of the art should be improved. Limits in the state of the art have not been clearly highlighted. Open challenges the authors want to tackle are not listed. The listed contributions are not clear to me. The authors state that they introduce a general framework of multi-modal learning for medical image analysis, but only one dataset is considered for the experiments. Point (3) is not a contribution. The authors should try to clarify why the introduced contributions allow them to overcome the state of the art (without just presenting the results). The clinical rationale behind the work is not clear. The authors write ""Despite the enhanced imaging, endoscopists rely on WL images before they change the light mode to detect the possible polyps, which means that the WL images may fail and lead to missing detection"". What does this sentence mean? If the clinicians have the possibility to exploit NBI, I don't see the point of developing algorithms that work with WL. The issue can be that some centers do not have NBI endoscopy."	Please provide more attention and vision transformer information in medical image domain, rather than general computer vision domain.	The paper proposes an intresting approach that can be a solution for a clinical problem. However, better evaluation on a bigger dataset, better representation of the advantages over the state-of-the-art, and better discussion on the real-time clinical use need to be addressed in the manuscript.	Nice paper but there are some weaknesses that have to be solved.	This manuscript used the currently popular deep learning architecture, attention based vision model, to solve an important cross-model Colorectal Polyp recognition problem. Also, it improved the baseline model with its newly designed module. Similar method should be able to be applied to other similar problems.
513-Paper2769	Towards Confident Detection of Prostate Cancer using High Resolution Micro-ultrasound	This study proposes a learning model for PCa detection using micro-ultrasound that can provide an estimate of its predictive confidence and is robust to weak labels and OOD data. The proposed model uses a co-teaching paradigm to handle noise in labels, together with an evidential deep learning method for uncertainty estimation.	The authors presented the development of a deep learning model with a focus on micro-ultrasound-guided prostate cancer biopsy. This framework was tested leveraging a dataset of micro-ultrasound from 194 patients. The results from approaches assessed (evidential deep learning (EDL) and EDL + co-teaching) show promise in aiding the detection of prostate cancer.	The authors present a framework for training a classification model to detect prostate cancer from micro-ultrasound images which accounts for limitations in the quality of ground truth-specifically weak and out of distribution labels. Furthermore, the inference component of the framework provides uncertainty estimates for predictions with adjustable levels of confidence.	The main strength of the paper is that the faced problem is clinically directly related, and the evaluation experiments also consider the clinical concerns.	"The development of a predictive algorithm to assist in the detection of prostate cancer using micro-ultrasound, without the need of additional MRs, which can be expensive and prone to co-registration error, is a welcoming direction. The paper is well organized and presented. The ""Methodology"" section, with the aid of Figure 1, which is well done, is especially easy to follow. Given the available data, the authors presented a thoughtful and well-designed validation experiment."	The main strengths are providing methods to adapt to sub-optimal training data, which reflects a challenge with most real-world machine learning tasks in medical imaging; and a means of interpretability by utilizing adjustable confidence thresholds which better integrate with an operator's decision making.	The main weaknesses of the paper is that the comparison with other studies should be added, especially the final heatmap.	A major component of the study is to compare different uncertainty methods. While this reviewer appreciates the trend and comparisons of different methods presented in Figure 2, the lack of data in tabular form makes it challenging to assess and gauge a full performance profile. In particular, the upper bound of confidence threshold for Evidential approach in Figure 2(a) is interesting in comparison to its counterparts. There appears to be lack of discussions on the performance difference between EDL and EDL + Co-teaching for sensitivity and specificity, and whether such differences are clinical relevant, especially given the impact of co-teaching is a particular interest of the study. For results presented in Figure 3, it would provide additional information and discussion if the heatmaps are compared with confidence threshold via quantification such as Jaccard or Dice against a baseline configuration.	Performance comparison with conventional ultrasound would be an appropriate gold standard, if data were available in a similar patient population. While performance in terms of confidence and accuracy are compared across model variations a more clinically relevant performance assessment should include actual decision making across a group of operators. For instance, if an operator is given control to adjust the confidence threshold for given images in a test set 1) would they take a biopsy and 2) where would they take the biopsy.	Satisfactory	Reproducibility would be a challenge that requires significant effort.	The authors have provided code and details on parameter selection that may allow others to reproduce performance, however, only if similar data and ground truth information is found elsewhere.	(1) This paper proposes a micro-ultrasound PCa detection learning model that is robust to weak labels and OOD samples. The weak labels and OOD samples are common problems in medical classification tasks. Therefore, it is better to provide the comparison with the DNN model that are proposed for solving the problems of weak labels and OOD samples. (2) The used clinical evaluation metrics are important parts of this manuscript. Please add some references. (3) About the accuracy and calibration error, please add the standard deviation. (4) In section 3.3, the heatmap can provide good biopsy targets, which is critical for the adoption of precision biopsy targeting using TRUS. This found is important, but maybe it is better to add the comparison of heatmaps by using different models.	This reviewer would encourage the authors to minimize the use of acronyms, which would enhance the overall readability of the paper. Figure 2(c), comparing to its (a) and (b) counterparts, lacks a legend.	The paper is very clearly written. The motivation for adopting their framework appears well informed by the clinical problem and data available for training. I eagerly look forward to performance comparisons with standard of care TRUS and mpMRI-fusion biopsies.	This paper is clincally interesting.	The paper provides an interesting methodology with strong validation design and moderately strong validation results.	The major factors in my recommendation were the focus on an excellent clinical problem and appreciation for the advantages of confidence thresholding in order guide operators when using this model in clinical practice. This level of pragmatism baked in to the framework, coupled with the encouraging performance, motivated my high score for this paper.
514-Paper1162	Towards Holistic Surgical Scene Understanding	"The authors present a novel dataset and method for benchmarking in the domain of surgical data science. The dataset, called ""Phase, Step, Instrument, and Atomic Visual Action (PSI-AVA)"", contains eight instances of radical prostatectomy surgeries (performed with the Da Vinci SI3000 Surgical System). The annotations in the dataset go beyond currently available datasets, comprising hierarchical annotations from atomic actions and surgical tool bounding boxes, to surgical phase detection. The proposed method (TAPIR) creates a strong baseline performance on this method and is shown to be able to make use of hierarchical annotations on PSI-AVA data."	The paper introduces a new data-set (PSI-AVA) with annotations for phase and step recognition,instrument detection, and the novel task of atomic action recognition in surgical. The dataset is novel and unique and can serve as a new benchmark for multiple tasks in surgical video understanding. The paper also proposes TAPIR, a transformer-based method that leverages the multi-level annotations of PSI-AVA dataset. The authors show superior performance of their method compared to other baselines.	This paper introduces and validates the PSI-AVA dataset with annotations for phase and step recognition, instrument detection, and the novel task of atomic action recognition in surgical scenes. Besides, this paper proposes TAPIR, a transformer-based method that leverages the multi-level annotations of PSI-AVA dataset and establish a stepping stone for future work in our holistic benchmark.	"PSI-AVA: Novel dataset and benchmark code, shared publicly (license unclear, please indicate in paper) Hierarchical/multi-level annotation and learning targets: Compared to other dataset, a very comprehensive set of target labels (see Table 1 - incl. phase/step/instrument recognition, instrument detection, action/task annotaiton, spatial annotations ) Novel method: ""Transformers for Action, Phase, Instrument, and steps Recognition (TAPIR)"" able to leverage multi-level annotations, from tool localization over task classification to surgical phase recognition. Experiments underline TAPIR's ability to make use of hierarchical knowledge. This is shown in Table 3, where TAPIR performs better than e.g. a SOTA model ""SlowFast"", both on the (non-hierarchical-labels) EndoVis challenge data and disproportionally better on the PSI-AVA dataset with hierarchical labels. PSI-AVA intrinsic validation: the dataset is also briefly validated by comparing two non-TAPIR models (Faster R-CNN vs. Def. DETR), which yield more consistent performances on PSI-AVA (at similar FLOPs/#Param characteristics). This probably indicates a high quality and consistency of annotations in PSI-AVA. Appropriate and informative supplementary material."	The first strength of the paper is introduction of a new data-set (PSI-AVA) with annotations for phase and step recognition,instrument detection, and the novel task of atomic action recognition in surgical. The dataset is novel and unique and can serve as a new benchmark for multiple tasks in surgical video understanding. The paper is well written and with enough level of detail on the dataset, methods and validation strategy. The authors also propose a new transformer-based model for feature extraction in spatio-temporal domain from surgical videos. Given the evidence in the computer vision community related to merit of vision transformer models, the paper and it's validation can benefit the miccai community. Experimental validations are sufficient to prove the claims made by the authors and can serve as a guideline for other to follow.	There are two main advantages of this paper. First, the paper proposes a new dataset for phase and step recognition, instrument detection, and the novel task of atomic action recognition in surgical scenes. This dataset is adapted to a variety of tasks and is publicly available. It is very helpful for researchers to follow-up work in this field. Second, the authors propose a transformer-based framework.	Dataset only contains only 8 instances of surgeries (but total runtime of 19.1 hours, which is average compared to other datasets, see Table 1.). The cross-validation is 2-fold, 4 surgeries for training, 4 for validation, which even further reduces the amount of training data.	TimeSformer and Swin transformers are two of the state-of-the-art models recently introduced in the computer vision community for video analysis tasks. For completeness, authors need to compare TAPIR with such methods to show their method is actually the state of the art. It would be interesting to see how TAPIR performs on other publicly available datasets like Cholec80.	The structure of the transformer is not shown in Figure 2. This is not conducive to the reproduction of the model. All tables have no bottom border. It's not pretty. The authors used too few methods for comparison to well verify the advanced performance of the proposed method.	"The Reproducibility statement is filled out correctly, to my impression. Very high reproducibility overall, as the authors state: ""we will make publicly available the PSI-AVA dataset, annotations, and evaluation code, as well as pre-trained models and source code for TAPIR"", which is fully transparent. The only missing information was the chosen license for dataset+annotations, TAPIR code, and pre-trained weights."	The paper is reproducible if authors release the dataset.	The structure of the transformer is not shown in Figure 2. This is not conducive to the reproduction of the model.	The paper is very strong, both on PSI-AVA and TAPIR aspects, in combination even stronger. Needless to say, 8 radical prostatectomies is not much data, and it is arguable whether this covers a large range of variability, especially on the highest annotation level (surgical worflow deviations, anatomical anomalies causing backup/recovery workflow steps etc.). Annotation effort on PSI-AVA level must be enormous - nonetheless, if any way possible, it would be great to further increase this number, e.g. towards a journal extension, or maybe make this a yearly growing challenge, similar to how BRATS dataset size has increased over the years.	TimeSformer and Swin transformers are two of the state-of-the-art models recently introduced in the computer vision community for video analysis tasks. For completeness, authors need to compare TAPIR with such methods to show their method is actually the state of the art. It would be interesting to see how TAPIR performs on other publicly available datasets like Cholec80.	The structure of the transformer is not shown in Figure 2. This is not conducive to the reproduction of the model. All tables have no bottom border. It's not pretty. The authors used too few methods for comparison to well verify the advanced performance of the proposed method.	Very complex dataset with timely challenges to teams participating in benchmarking (hierarchical annotations, spatio-temporal inference from video etc). Very strong reference model TAPIR, to set a solid baseline performance for competing teams once the dataset goes public. Overall, an extremely valuable contribution to the community.	The paper has strong contributions both on the novelty of their dataset and methods. The miccai community can benefit from such dataset for various video analysis tasks.	The dataset presented in this paper is valuable.
515-Paper1782	Towards performant and reliable undersampled MR reconstruction via diffusion model sampling	In this paper, the authors proposed a novel diffusion model-based MR reconstruction method (DiffuseRecon), which is robust to the sampling pattern and acceleration factors. The proposed DiffuseRecon achieves SoTA performances in two large public datasets: fastMRI and SKM-TEA.	This paper proposes a novel MR reconstruction framework based on the diffusion model. Experiments show that the model can outperform other baselines consistently.	The paper talks about how to achieve a robust and reliable model to reconstruct MRI images from undersampled acquisitions. The DiffuseRecon model consists of two components is proposed: 1) a k-space guidance module incorporates observation into the denoising process and allows for stochastic sampling from the marginal distribution of E[q(y_full x_obs)]; 2) a coarse-to-fine sampling module accelerates the selection of reconstructed images and gives an approximation of the noise on reconstructed samples. The goal is to generate all candidate reconstruction samples of MR images and accelerate the reconstruction process.	The authors leverage the recent diffusion model-based generative methods to learn the distribution of fully-sampled images. Therefore, the proposed method doesn't rely on a certain sampling pattern or acceleration factor. The authors did thorough studies on ablation studies, and different sampling factors, and provide enough details on the training and inference, Fig.3 shows the robustness of the proposed method. Solid work.	This paper provides the first data point, to my knowledge,  where the diffusion model is introduced to MR reconstruction framework. Experiments show that the model can outperform other baselines consistently, especially the proposed method can  adapt different under-sample rates.	The k-space guidance module uses $x_obs$ added by a zero-mean Gaussian noise as the condition and mixes it with the reconstructed image in k-space, allowing for stochastic sampling.  The coarse-to-fine sampling module accelerates the speed of selecting reconstructed images by averaging samples generated for the estimation of E[q(y_full x_obs)].  The ablation study supports the benefit of using each module. The evaluation experiment shows the improvement with the proposed method in both increasing performance and less time-cost.	One work that the authors should have compared or at least cited is this: robust compressed sensing mri with deep generative priors, which use a GAN to learn the distribution p(y_full) in the paper, and use Langevin dynamics to perform the reconstruction. Please comment on the advantages of using diffusion model compared to GAN model. If possible, please add the std statistics to Table 1. As mentioned in the paper, the inference time is quite long for DiffuseRecon, can you provide some potential ways to accelerate it?	"This paper is unclear about some implementation details:  As the authors claimed ""We follow [1] with some modifications to train a diffusion model that generates MR images.""  What are the detailed modifications? Since this step is the fundenmental of this work, I can not understand the training process from current version. The authors also set all the medthods to produce 2 consecutive slices which is not similar to the original MR reconstruction baselines, why? All the figures in current paper are drawn to generate 1 slice along with the formulations. Do other models (D5C5 and KIKI) are also modified to  produce 2 consecutive slices? The diffusion model is computationally expensive [1, 2]. The comparison of running time and parameters should be included in table 1. Although the authors disscussed the problem in page 8, excessive computational cost can also limit the applicability of the proposed method. [1] Nichol A Q, Dhariwal P. Improved denoising diffusion probabilistic models[C]//International Conference on Machine Learning. PMLR, 2021: 8162-8171. [2] Ho J, Jain A, Abbeel P. Denoising diffusion probabilistic models[J]. Advances in Neural Information Processing Systems, 2020, 33: 6840-6851."	The description of the refinement in the coarse-to-fine module is not very clear. The implementation detail of training DiffuseRecon is not elaborated.	The authors provide code in the supplementary material.	This paper is unclear about some implementation details about model training and settings.	The author were able to make the code and data public. That would benefit the community in recognizing this problem. Also, the evaluation metric is clearly described so that the result would be convincing.	Please add the std statistics to table 1. If possible, would be nice to compare with the work or at least cite: compressed sensing mri with deep generative priors. They have their code published.	The detailed pretraining details and motivation of generating 2 consecutive slices should be provided. Since the current method is bulit on U-net, the authors can discuss if designing more advanced backbone via NAS [1] [2] can help to improve the inference efficiency and performance. [1] Huang Q, Xian Y, Wu P, et al. Enhanced mri reconstruction network using neural architecture search[C]//International Workshop on Machine Learning in Medical Imaging. Springer, Cham, 2020: 634-643. [2] Yan J, Chen S, Zhang Y, et al. Neural Architecture Search for compressed sensing Magnetic Resonance image reconstruction[J]. Computerized Medical Imaging and Graphics, 2020, 85: 101784.	The detail of the refinement step in the coarse-to-fine module could be more elaborated: Is $epsilon_theta$ the same as the one used in the k-space guidance module and the coarse sampling step? How does $x_obs$ involve in the conditioning? How to choose $T_refine$? The author should provide the objective function of training the neural network of $epsilon_theta$. The author should elaborate on 1) how many samples are used to train the network $epsilon_theta$ and 2) how much time it takes to train the network.	This paper proposed a novel diffusion-model-based MR reconstruction method, which is robust to the sampling patterns and the acceleration factors, the authors gave thougrough anylasis, great work!	Though the  combination of the diffusion model  and MR reconstruction framework may interest MICCAIers, the lack of important details make this paper difficult to follow.	This work combines a conditional reconstruction module and an accelerated refining module to improve the reconstruction of MR images with higher efficiency. Sufficient experiments have been done to support the benefit.
516-Paper0313	Towards Unsupervised Ultrasound Video Clinical Quality Assessment with Multi-Modality Data	The authors developed a 3D encoder and decoder pair bi-directional model that identifies ultrasound images of the fetal head that are suitable for making measurement. The model does this by learning a spatio-temporal representation between the video space and the feature space.	In this paper, the authors describe an objective task-based approach to the quality assessment of clinical ultrasound video, which relies on learning a latent spatio-temporal representation from two modalities (ultrasound video and optical flow). A third modality is also included, i.e. gaze, which helps the model focus on regions of interest in high-quality videos. The proposed model aims at automatically assessing the diagnostic quality of fetal ultrasound exams. Videos containing the transventricular plane (TVP) were considered as the high-quality references for data-driven/unsupervised learning. The method uses the feature reconstruction error to discriminate between low and high-quality videos.	The authors presents an unsupervised approach for the quality assessment of ultrasound fetal head images.	The model appears novel and the method may make the measurements of the fetus more reproducible and less dependent on the operator. The analysis appears sound and the results are convincing.	Research motivations and its clinical importance are clearly stated Scalability of the method The method does not require anatomical annotations for training, which is an important improvement over state-of-the-art methods A strong validation of the results is provided, with extensive result comparison and ablation studies, which support the proposed methodology	The paper is well written and easy to follow. The proposed approach for unsupervised training seems interesting. The experimental results are acceptable.	Some information about the dataset is missing Statistical analysis of the results is missing	Some important details about the proposed model, e.g. parameters, and the used dataset are not given Code/datasets were not provided. Thus, given the missing information above, reproducibility of these results is hindered	The size of training dataset is quite small. Details regarding dataset is missing.  It is not clear how robust the method is to input domain shifts. The clinical application of the proposed method seems to be minor.	Appears to be complete	Although the authors state that the code is provided, this is not the case. The used dataset is not public and thus is not provided. However, the authors should provide a bit more information on image acquisition/origin.	As the method includes too many details and the model has several parts, the results would only be reproducible if the code and data were available.	It is not clear what makes the three types of images to be of low quality. Is it because they are the wrong planes for making the measurements? It is not clear how you partitioned the images in the training and testing datasets, i.e., did images from a given subject appear in both the training and testing dataset? Figure 2: Can you add meaning of the labels of the columns to the caption. Figure 2: It is not clear in this figure what makes the images to have high-quality and low-quality. Perhaps a short description in the caption would help. You described the difference between the TVP and TCP, but that is not obvious. Perhaps arrows on the image to relate to the description. Was the study performed with Institutional Ethical Review Board approval? Since the labeling of the images is dependent on the frozen frame, who did that? You had a total of 611 video clips. Was each clip from a different subject, i.e., did you image 611 fetuses? Tables and figure 3: The differences between the reported performances is clear, but it is not clear if they are statistically significant different.	"In section 2, the authors state: ""The definition of quality assessment in ultrasound is different in that it needs to factor in clinical context"". This is not entirely true, as quality assessment for ultrasound content may also focus on image clarity and definition, for example, to provide insight in equipment/technology development. Consider rewriting this sentence to accommodate this. The final sentence in section 3 (before 3.1) and the one immediately before the ""Spatial zoom-in module"" subsection could be written just once. They are repeating the same idea. Also, in section 4, the second and third sentences could be condensed into one. The parameters of the Farneback algorithm and the median filter should be described. The same applies to the fully connected layers in D_v In section 4, the loss weights should be defined as w_adv, w_rec, and w_gaze, following the notation in Eq. (1). How exactly was the image-based approach implemented? Was this done with the model proposed by the authors? If that is case, it is not clear how a single image input would be processed. This should be further explained. - The paper needs to be thoroughly proofread. Some minor writing issues include: Abstract, line 8: end sentence after ""temporal information"". Begin new sentence with ""The model..."" Abstract, line 9: ""anatomy-specific annotations, which makes..."" Introduction, line 2: ""free radiation"" -> ""acquisition process, which does not use radiation"". Introduction, line 9: add comma after ""labour-intensive"" Page 2, line 1: ""spaces"" Page 2, line 3: add comma after ""error"" Section 2, line 2: add comma after ""proposed"" Section 2, line 15: ""considers"" Section 2, line 16: add comma after ""gain)"" Section 2, line 22: ""checks (if) images"" Section 2, line 23: add comma after ""[16]"" Page 2, last line: ""limit"" -> ""limits"" Consider renaming section 3 as ""Methods"" or ""Methodology"" Subsection ""Spatial zoom-in module"", line 5: ""on (the) overall"" Subsection ""Bi-directional reconstruction"", last sentence: ""The structure of the discriminator DV is similar to that of encoder"" Page 6, line 8: ""exemplar"" -> ""example"" Subsection ""Quantitative results"", line 5: add comma before ""thus"" Avoid repetitions such as ""clinical quality for clinical tasks"" Page 6, last two lines: add comma after ""single-modality video reconstruction"" Subsection ""Ablation study"", line 4: ""achieved by (the) inclusion"" Subsection ""Gaze prediction"", line 2: remove ""can"" Subsection ""Gaze prediction"", line 7: ""approximate"" -> ""approximately"""	There are two major issues with the current submission as follows: 1) The size of the dataset is very small. While the details of dataset and imaging settings have not been mentioned, I think only 300 video clips for training a 3D model with several components is quite insufficient.  Please explain the detail of your stopping criterion because your method includes adversarial training. In Fig. 1 (b), it is also unclear how the label is created to calculate quantitative indexes. Please also explain whether cross-validation and data augmentation is used in the training step or not. 2) The clinical application of the proposed method seems to be minor because the method is computationally expensive. I would think that the clinicians can easily look at the images to see whether TVP exists or not. It is not something hard to distinguish or user-dependent or even the user does not need to be very experienced. I would think the task is not challenging enough.	The usefulness of the model, which makes fetal ultrasound more reproducible and less operator dependent.	The authors propose an interesting method, which achieved good results on this particular case, and has great potential for a more widespread application across different medical imaging modalities. The overall score suffered with the lack of clarity on some important details.	The robustness of the proposed method as well as clinical applications are questionable.
517-Paper0500	Tracking by weakly-supervised learning and graph optimization for whole-embryo C. elegans lineages	In this paper, the authors propose an incremental extension of Linajea [14], a state-of-the-art tracker of cells in embryonic image data, to improve its tracking performance by employing a ResNet18-based cell state classifier [21] and to facilitate its hyper-parameter fine-tuning via structured SVM [8]. The proposed method has been quantitatively evaluated using three different sets of C.elegans embryo recordings and achieved insignificant improvements in the DET and TRA scores (third and fourth decimal digits as reported in Tables 1 and 2) when compared to Linajea.	This manuscripts presents a method for improved lineage tracing from whole-embryo C. elegans data. The authors build two additional modules: a cell-state detector and a weight estimator, on top of the earlier-developed algorithm. This results in improved performance, in particular to detection of cell divisions. The proposed method is leading the scoreboard of the dedicated challenge in this particular data category.	The authors present an extension to the previously published tool linajea to perform lineage reconstruction in whole embryo recordings of C. elegans. The extensions include cell state/polar body classification that are incorporated to the learning-based approach. Moreover, an automatic hyperparameter identification module using a structured SVM approach is used. The extensions are validated on three different data sets and show (slightly) superior results to previous methods.	Despite its incremental methodological novelty over Linajea, the idea of hyper-parameter fune-tuning to alleviate the need for identifying a suboptimal hyper-parameter configuration using grid-search seems to be promising. However, it is unclear whether such a strategy is viable for other types of embryonic image data, or time-lapse cell image data in general. The authors also promised to make two fully annotated datasets publicly available along with the paper, with the aim of accelerating further algorithmic developments in this area.	* Developed methods improve the performance, both in terms of higher validation scores and ease of use. * Top-ranked performance on the challenge data.	The achieved tracking quality of the proposed method is a strength of the paper (although the scores of the prior methods linajea are already impressive as well). Although not invented by the authors, an interesting way of automatically identifying hyperparameters via a structured SVM is applied to eradicate the need for manual hyperparameter tuning / grid searches. The additions to the ILP formulation are reasonable extensions and it's generally a great approach to incorporate biological constraints directly to the optimization problem. Last but not least the paper is nicely written, comprehensible and I didn't even spot a typo.	The main shortcomings of this work are (i) incremental methodological novelty of the proposed method that only combines previous concepts ([14], [21] and [8]) into a single framework; (ii) insignificant superiority of the tracking performance in terms of DET and TRA compared to Linajea [14] that is named as JAN-US in the Cell Tracking Challenge; and (iii) very limited performance comparison with the state-of-the-art methods. Based on the scores listed in Tables 1 and 2 and available on the Cell Tracking Challenge website for the JAN-US (i.e., original Linajea) method (http://celltrackingchallenge.net/participants/JAN-US/), it does not seem the integrated cell state classifier would improve the tracking performance by any statistically or practically significant margin. Furthermore, it is unclear why the top-performing methods for the Fluo-N3DH-CE datasets (i.e., KTH-SE (1) and KIT-Sch-GE (1) publicly available on the Cell Tracking Challenge website) were not taken as the baselines for the mskcc-confocal and nih-ls recordings.	* Improvement in terms of two established validation measures, related to detection (DET) and tracking (TRA), respectively, looks marginal. * Modified version of the algorithm does not results in consistent performance on all the validation measures: while it improves the scores in some categories, the results with respect to other validation measures actually deteriorate.	I'm wondering why the additional metrics were not computed for the cell tracking challenge data set and why it is mentioned separately in the text, rather than combining it to Table 1. There was a recent submission to the cell tracking challenge that outperforms the proposed approach. While this is to be expected for an ongoing challenge, it would still be good to correct the statements in the results section accordingly. The improvements over the previous methods seem to be quite modest. It would be good if the authors could describe the implications of the improved scores. Can this be somehow quantified? For instance: How many manual corrections are saved by these minor improvements? How many more complete lineages are present? I'm aware that it would unveil the identity of the authors, but for the final manuscript it would be great to include a link to the annotated data set as the authors propose in the abstract. This would indeed be a valuable addition for the community.	It is a pity that the mskcc-confocal and nih-ls datasets and the results achieved over them were not made privately available for the peer-review purposes. Furthermore, This reviewer did not find any relevant information about the computational aspects of the proposed method (i.e., hardware requirements, execution time, memory footprint, etc.) in the paper.	The methodology is valid and clearly presented. The most convincing results were achieved on the challenge data, with the submission being evaluated by the organizers.	Seems to be reproducible to me. However, as mentioned above it would be good for the final publication to include links to data and repositories to be used for benchmarking and reproduction the community.	Besides the comments given in the box #5, there are other points that deserve additional attention: It seems that the proposed method is not listed on the Cell Tracking Challenge website. It is therefore questionable whether it really ranked first at the time of paper submission, especially when its predecessor (i.e., Linajea or JAN-US in the Cell Tracking Challenge terminology) achieved practically the same scores as those reported in Section 3 (2nd paragraph). It is unclear whether the cell state classifier can deal with temporal gaps in cell detections, and thus correct imperfections of the cell detector employed. The description of the evaluated performance measures is unclear, which makes the numbers listed in Tables 1 and 2 difficult to interpret. For example, the last reported value of FPdiv in Table 2 (i.e., 0.046) does not give any integer number after being multiplied by 18 (i.e., by the number of evaluated configurations). The formal definitions of individual measures need to be provided. Furthermore, please clarify where the 16% reduction of manual curation comes from (Section 3, 3rd paragraph) when comparing Elephant and the proposed method. There is no information about the annotation protocol followed for the nih-ls dataset. Furthermore, it is unclear whether multiple manual curations of the Starrynite results for the mskcc-confocal dataset were prepared and subsequently fused to reduce the subjectivity and error-proneness of the final reference annotations. The numbers listed in Tables 1 and 2 are mostly similar across the evaluated approaches. Are the differences statistically or practically significant? As the proposed method shall be used a baseline for the nih-ls dataset, it is unfortunate that the cell state classifier did not account for apoptotic celss. How much would the performance of the proposed method improve when training the ResNet18 backbone for that class too? Other remarks: Table 2: There is a typo (cls -> csc) in the table caption. To this reviewer's understanding of the Cell Tracking Challenge format, two test sequences per dataset can be downloaded from the Cell Tracking Challenge website (i.e., the test data is public). The FP decrease from 3.7 to 2.5 in Table 2 does not seem to be dramatic. Please tone this claim down. Check the spelling of Linajea across the paper.	"The main result of this work for me is that the proposed method achieved top-raking performance on the Cell Tracking Challenge data. Please, by the way, specify this explicitly in the corresponding paragraph of the Results section. StarryNite, while being often used for benchmarking, is known to be rather sensitive to the input data; meaning that its performance on data that differ from their own data tends to be significantly worse. It is interesting to notice that the modified method and the original one seem to prioritize different validation measures. The same holds for ablation study presented in Table 2. This is something that needs to be discussed by the authors. Judging from the two established measures of lineage tracing (DET and TRA), the improvement provided by the modified method is very marginal. I do realize that it is impossible to assess its statistical significance due to low number of data samples. But this observation does somewhat undermine the value of this work. An alternative way to show the worth of the improved performance would be to demonstrate its positive impact on the downstream analysis; which is missing in this manuscript. ""In this regard, our improvement in TRA over Elephant should mean that our method entails a 16% reduction in manual curation effort as compared to Elephant."" I find it difficult to understand where the 16% are coming from; taking into account that the corresponding tracking scores are 0.979 and 0.975. Please explain the meaning behind the underscored entry in Table 2. ""By adapting the cost function D one should be able to modulate this depending on respective application-specific needs."" I find this sentence somewhat speculative. Otherwise the authors have to explain how this can be achieved. Please report the value of the l parameter for completeness."	See list of weaknesses.	The work presented in this paper suffers from (i) incremental methodological novelty; (ii) insignificant superiority of the tracking performance compared to the baseline (Linajea); and (iii) very limited performance comparison with the state-of-the-art methods.	This work presents methodology for making the algorithm for lineage tracing in C. elegans data more accurate and easy to use. The methods are valid and clearly presented. However, the added value of the developed methodology is somewhat limited as the 1) It was not possible to assess statistical significance; 2) It is inconsistent with respect to different validation measures; and 3) Improvement on the most-commonly used validation measures (DET and TRA) is marginal. On the other hand, this was enough to climb on the leading position of the dedicated  challenge.	Overall a nicely written paper proposing reasonable extensions to a previously published method. Performance is assessed on three data sets and demonstrates the validity of the performed modifications.
518-Paper2282	TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers	The key contribution is to construct a representation of whole-brain tractograms with consistent structure enabling applications of machine learning algorithms that take tractograms as input.	The paper approaches the problem of parcellation-free WBT analysis by the proposed TractoFormer. The proposed method first map the tractography information at the level of individual fiber streamlines to 2D representation then leverages ViT to performance classification.	This paper proposed a TractoEmbedding method to encode 3D fiber spatial relationships by 2D image. This representation method can represent fibers of different regions by different channels and by performing random downsampling, multiple TractoEmbedding images can be generated. Based on the generated 2D TractoEmbedding image,  a ViT-based TractoFormer was proposed to conduct HC/SCZ classification.	The formulation of the representation is novel, interesting and sensible. It offers genuine advantages over more ad-hoc solutions one might imagine. Simulation results are promising and show the approach works as expected in one example scenario. Real-data results are reasonably compelling showing good discrimination between psychiatric groups and highlighting reasonable features of the tractogram as salient features.	The topic is interesting and clinically significant. The paper is well organized and easy to follow. The idea of embedding tractography information into 2D representation seems effective and interesting.	The proposed TractoEmbedding method can encode the 3D fiber spatial relationships by 2D image, which allows image-based models such as CNNs and ViT to leverage fiber spatial similarity information. The figures of this paper are clear, which is very helpful to understand the main idea of the proposed method.	Writing is a bit hard to penetrate. I had to read it a few times.  Intro could be more focussed and define more clearly what the actual task is - it takes until we get to the experiments before that becomes clear. Could do with some ad-hoc baselines in the experiments e.g. a simple tract-density image.	Choosing the first two eigenvectors of the affinity matrix as the coordinates of the 2D embedding grid might lead to information loss. The interpretability of CNN can also be achieved by Class Activation Maps (CAM). Authors does not provide experiments to show superiority of the proposed method against it.	"The introduction of TractoEmbedding method is not clear. First, spectral embedding as an important step, even citation is given, should give some basic introduction. Second, in the second step, only first two dimensions for each point was used to generate the  2D embedding grid. There is no experiments to discuss if the two dimensions can effectively represent the spatial information of the 3D fibers. Third, the paper mentioned ""multiple fibers that are spatially proximate are mapped to the same voxel"",  based on this kind of representation, the differences of multiple fibers which are mapped to the same pixel are ignored. It is better to conduct experiments to measure similarity and differences of the fibers at the same pixel. Exp 1: Synthetic data. To evaluate if the proposed method can identify the fibers with group differences in the WBT data for interpretation, this paper generated two synthetic groups of G1 and G2, compared G1, the mean FA of each CST fiber in G2 was decreased by 20%. This kind of group difference is too simple compared the complex change caused by SCZ. The results that the proposed model can get 100% acc for G1/G2 classification but a much lower acc for HC/SCZ (table1) can also illustrate this problem. Therefore, the interpretability of the results may be not very accurate."	Seems fine.	Authors mentioned that the code will be made available upon request. The availability of the pre-trained model is unclear.	The code and data will be released according to the reproducibility checklist.	Not much more to add.  I like the idea and I can see it being useful in some applications. As I note under weaknesses, the most obvious way to feed a whole-brain tractogram into CNNs etc would be to create a tract-density image (streamline count at each voxel); an experiment with this as a baseline would highlight the benefits of the proposed approach better.	Authors may explore the embedding in higher dimensions. The proposed workflow should be easily extended to that. It is interesting to see how emending affect the performance and will be a good ablation study. Comparison between CAM and ViT's attention seems necessary to demonstrate the interpretability of TractoFormer. Since the proposed method is able to obtain 2d grid representation, it is interesting to see using other kinds of conventional data augmentation techniques in computer vison.	Q5	Novel and useful method even if not the most clearly written paper.	The rating reflects innovation of the approach for of parcellation-free WBT analysis. The merits outweigh the weaknesses.	I have carefully read all part of the paper. Based on the strength and weakness elaborated in Q5, I gave the score.
519-Paper1253	TransEM: Residual Swin-Transformer based regularized PET image reconstruction	The main contribution of this work is the introduction of swin transformer into PET reconstruction. The image reconstruction is done in the ML-EM iterative style, following exactly the framework in [11],  with the key update equation 9 being the same as the equation 6 in [11]. The reconstruction regularisation operates in the image domain, and is done by applying the swin transformer, a newly (relatively) developed model in the NLP-> computer vision field, which can efficiently capture long-range dependencies in an image. This is expected to improve the PET image reconstruction results in the proposed work.	The work proposes a novel TransEM method (an image reconstruction method based on MLEM and residual swin-transformer) for PET image reconstruction. Compared to the traditional convolutional neural networks-based methods, TransEM owns the strong ability in modeling long-range dependencies of the measurement data. It is able to reduce noise without compromising on image details. This is validated with simulated human brain data, and the robustness analysis is also well performed.	The paper describes an already presented approach combining an EM update and a neural network to solve the PET reconstruction inverse problem. The authors change the CNN network to a swin-transformer, which is the main contribution of the paper. The presented method is the best over other studied methods (some classical PET reconstruction algorithms and Deep learning based methods).	This work assesses the performance of incorporating swin transformers into PET image reconstruction.	The swim-transformer is relatively new in the computer vision field. It is a good attempt to apply swim-transformer to the PET reconstruction field. The article is well organized. The authors analyze the robustness of the proposed TransEM on down sampled cases and perform the experiments to investigate the generalization ability of different models, which validates the excellent performance of the proposed method. The experiments demonstrate the claims set out by the authors.	The introduction well steers the proposed method, which is well divided into three parts. We understand why the authors want to use transformers in PET reconstruction. Moreover, starting from a recent state of the art idea (FBSEM) and combining with recently proposed transformers instead of CNN is a relevant combination of recent methods.	This work uses the swin transformer as the image regularisation model in an existing reconstruction framework. Demonstration of the impact of incorporating the swin transformer needs to be stronger. It is hard to see the advantage given the low image resolution involved in the work. The swin transformer is supposed to capture long-range dependencies in the image, however this benefit is not clearly shown in the results. Also, with the swin transformer as an image quality improving tool, the authors did not justify the need to incorporate it into the reconstruction, given that it can be used post reconstruction as well. This work is done in 2D, where PET image reconstruction is a 3D problem. Only simulated data were used to train and assess the proposed model. As the authors mentioned in the discussion, clinical evaluation will greatly strengthen the impact of this work.	1) Limited novelty: the proposed work is a special case of the swin-transformer, the current work has no obvious innovation in network architecture, except the additional shortcut added before and after the convolutional layer. 2) Without clinical evaluations: All the experiments are performed with simulated data. It is hard to check whether the proposed method is applicable in real applications. In addition, for practical use, TransEM may encounter difficulties in collecting abundant clinical raw data for the network training.	"The main weaknesses are more about clarity and organization rather than scientific aspect. There are lots of mistakes which catch the eye when reading: some misprints (""reconstrution"", page 3, ""Possion"", page 3, equation (1)), ""the"" missing several times, capital letters in the middle of sentences, commas missing to better understand the structure of the sentence, spaces missing between words and their acronyms or after colons. There are lots of imprecision in the chosen values (cf detailed constructive comments) and it seems that there is confusion between training set and validation set when the authors talk about hyperparameters. The analysis of the results is light, sometimes unclear (the authors said TransEM does not perform so good, and then, it is the best in addition to DeepPET), not argued and not scientific enough: ""relative not so good results"", or subjective: ""lots of excellent works"". The different number of counts in the experiment is unclear. The experiment seems to be a ""low count"" simulation. The network is trained with ""high dose"" images, which seems to correspond to the ""high count"" description from the authors. A second level of count is presented in the ""robustness analysis"" part, but a table presenting those results is shown above. In this part, it is not clear whether the number of data is the same as in the low count simulation, or if the data are ""downsampled"" or not. Moreover, the PSF of the high count simulation is different from the low count one, and not specified for the robustness analysis, whereas the scanner is always the same, which defines a unique PSF. All of this should be clarified."	The authors have given enough details to reproduce this work.	NA	The authors made an effort to make their paper reproductible, which lots of setup values. I did not find in the paper or in supplementary material a way to access the code, whereas it seems to be done according to the reproducibility checklist filled in by the authors. Maybe this is not possible at this stage. The authors detailed the hyperparameters values of the neural network, of the method, and the initialization of the image, which is very important. However, were  several runs make ? Or was the initialization of the neural network weights fixed ? About the PET simulation, some simulation setup is missing, like the system matrix modelling, the number of line of responses.	"How is the variance-bias analysis? Fig2. looks like the bias is really high from the ground truth Fig4. why does DeepPET show a different slice? Typos: Page 4: ""and The LayerNorm"", ""The Whole process"""	1) The authors should present a concise explanation about Patch Embedding layer. 2) The authors should state the choice of the hyper-parameters more clearly, such as the choice of patch size M.	"The introduction structure is good, but be sure to make more link between the different methods you talk about. You still can refine a bit the state of the art / the explanation of each pros and cons for each category to be more accurate : ""represented by filtered back-projection(FBP [2]) and maximum-likelihood expectation maximization(ML-EM [3])"". Why ""represented"" by these 2 methods ? What does it mean ? ""solve this problem well"": what problem ? I think you meant the modeling of physical properties, but it is not clear whether it is that or the noise problem. Examples on post processing: talk about the more common one : gaussian filter. I don't know this paper : ""Machine learning in PET: from photon detection to quantitative image reconstruction"". You may only cite [8] from Reader et al., which is a wide overview of deep learning in PET reconstruction. Gong et al. [10] is not a unrolled network from what I know, maybe I am wrong. Maybe you should skip the part with historical methods and focus more on the DIP learning methods, or be sure to be very clear and accurate. The equation in the method part requires slight modifications to be very clear : I was a bit confused about equations (4) and (5). I did not know the FBS algorithm, but when reading it for the first time, I understood that theoretically, it was equivalent to the optimization problem (2) because you say ""used to split the objective function"". I did not understand directly that it corresponds to the equations of an iterative algorithm. ""where the goal is to perform the pixel to pixel fusion"" : you should replace it by something like ""can be viewed as a pixel to pixel fusion. ""The hyper-parameter a was learned from training data"". Did you mean from the validation set ? Otherwise I do not understand how you can learn it. The fact that the number of unrolled blocks is hand-crafted should be said at the same time the other hyperparameters are learned from training data. Ambiguities which need to be rephrased : Prior is ambiguous : you can talk about prior information as additional or anatomical information, used in kernel methods. But prior in the Bayesian point of view can be used for maximum a posteriori, which is a category of Penalized Log-Likelihood (PLL) methods. Please do not merge these 2 different methods as one called ""prior-incorporative methods"". Anatomical information enable to improve the image quality by adding information. Penalized Log-Likelihood (PLL) methods are used to decrease the noise, directly in the optimization process. What do you mean by ""ablation study"" ? Removing RC ? ""ablation"" is a medical term which should not be used in your case. Number of counts in your table is smaller than 1. Figures 2 and 4 show normalized images, which does not enable the reader to do a fair comparison between the different methods. You should show them with the same contrast."	The impact of introducing the swin transformer into PET image reconstruction needs to be strengthened in this work. The authors' intention of trying a 'SOTA' model from another field is understandable, and it can be convincing with appropriate demonstration.	It is the first attempt to apply the swim-transformer in PET image reconstruction. Authors combine MLEM and vision transformer together to reduce the noise which is not compromised on image details.	The method is a very good idea, based on two recent state of the art methods. This is totally relevant to try combining them as both are at the cutting edge of PET reconstruction (unrolled methods) and feature extraction (transformers). The work just needs to be more thorough, with a clean way of writing and no mistakes, and more detailed discussion on the results. It lacks a bit of work which is noticeable when reading, but this can be improved to the conference.
520-Paper1473	Transformer based feature fusion for left ventricle segmentation in 4D flow MRI	The paper describes a novel approach for segmenting the left ventricle from MRI 4D flow images. Using deep learning based on a combination of U-Net architecture with Transformer components and feature fusion layers, the algorithm tackles the challenges of poor anatomical appearance in 4D flow images.	This paper proposes a Transformer based segmentation model for 4D flow MRI. 4D flow MRI is a recent blood flow velocity diagnostic on which automatic assessment has not been fully investigated. The authors design two self- and cross-attention-based methods to fuse the information from different modalities in 4D flow, and perform evaluations on a large in-house dataset.	The authors propose a novel method to take the velocity and magnitude image into a unified segmentation framework to achieve the automatic segmentation of the LV directly from 4D Flow. The experiment demonstrates that their method outperforms the best previously published results for this task.	generally an interesting problem, as it brings cardiac segmentation to a novel imaging method with potentially brings new diagnostic and therapeutic benefits novel approach to concurrently learn anatomical appearance from multiple images strong evaluation both in terms of technical (DICE & surface distance) and clinical measures (ejection fraction, volumes etc. - both absolute errs as well as statistical metrics)	Application novelty. The automatic assessment on 4D flow MRI has been little investigated before. As the authors states, it may be the first work for this application. The employed techniques are sound. The authors introduce different attention modules for intra- and inter-modality feature extraction and fusion. The results also support the performance improvement thanks to the attention mechanism. The choice of evaluation metrics. Since the most significant of the paper is the application novelty, the authors employ both geometrical and clinical metrics.	They propose the first study to segment the LV directly from 4D Flow MRI data. They propose a Transformer based cross- and self-fusion layer to explore the inter-relationship from two modalities and model the intra-relationship in the same modality.	Overall a pretty good paper, I am not sure what to criticize ;-)	Minor concern: since this is a relatively recent application, the work has little related work for comparison. Moreover, the compared methods are more considered baselines, hence, the evaluation against SOTA is not available.	The writing is not rigorous enough in some sections of this paper.	What can I say, the code and models are available on Github - as good as it gets! :-)	The experiments are performed on a private dataset while the code is avaible on git.	The description of the proposed method is clear and the paper is reproducible.	Please clarify whether the LV was annotated for all the 30 frames, or only for a subset?	The authors have well proved the superiority of the proposed model. However, I would suggest adding the results on a single modality (2D MRI without velocities) with U-Net. These experiments can prove the effectiveness of 4D flow MRI against conventional CMR.	"The figures in the paper is not clear enough, the font in all figures is inconsistent with the text, and there is some ambiguity. It is recommended to revise all figures carefully. ""dividing feature maps into patches leads to loss of spatial information"" should be given detailed evidence to prove this point through ablation experiments. The paper mentions ""a learnable positional encoding sequence"", however the detailed descriptions are not given. The typesetting is not neat enough. It is recommended to use ""latex"" for typesetting. what the meaning of ""in 91 182 annotated pairs""? is it a writing error because of the large space in one number? Please check it. The sentance ""KE was normalized to EDV as recommended by other researchers"" is too casual and not rigorous, please explicate this point by citation who recommend or which paper inspired you to do like this. ""we did not employ any data augmentation methods to enlarge the dataset"", in future I suggest you to conduct some data augmentation like nnU-net to improve the performance of the proposed method. ""All of those Pvalues are larger than 0.05, which confirmed that there is no significant different..."" is different from the ""Pvalue was computed using Wilcoxon-signed-rank test. P<0.05 indicate a significant difference between two variables"". Please check which one is wrong."	See main strengths above: it brings a seemingly novel approach (transformer based multimodality learning) to a challenging problem (low quality imaging) to generate convincing results quantified with relevant metrics	Two major factors affect my recommendation: The novelty of this application. The quality of evaluation.	The novelty from application is relatively good.
521-Paper0965	Transformer based multiple instance learning for weakly supervised histopathology image segmentation	This paper proposes a transformer-based multiple instance learning (MIL) for weakly supervised histopathology image segmentation. The motivation behind this paper is modeling dependencies of MIL instances via multi-head self-attention in the transformer. In addition, the authors propose deep supervision to overcome the limitation of annotations in weakly supervised scenarios and make the better utilization of hierarchical information from the Swin transformer. The experimental results demonstrate the method's superiority compared with other weakly supervised methods for the segmentation task.	This paper proposed a Transformer based method for semantic segmentation in histopathology image. Swin Transformer is introduced to this specific task to consider related information between instances in MIL. The method was evaluated on public colon cancer dataset in comparison with a number of MIL methods and reached SOTA results. Besides, the Ablation study explored the effect of backbones, stages, and deep supervision. The idea seems to be promising and valuable for research in this field, however, the paper needs to be further improved.	In this paper, a transformer-based MIL framework is proposed to overcome the limitation of segmentation performance caused by the lack of correlation between instances. In addition, deep supervision is introduced to strengthen the constraints. The experimental results show that the weakly supervised segmentation method proposed in this paper is effective.	-Overall, the paper is very well-written and presents a successful combination of current methods for achieving a successful weakly-supervised segmentation method on histopathology images. -The state-of-the-art segmentation results are achieved on the colon cancer dataset. -The authors carry out enough ablation studies on the components of their method, proving the efficacy and contribution of each of these parts.	This paper introduced Swin Transformer to MIL and semantic segmentation of pathology image. Many comparison and ablation study proved the effectiveness of the proposed method.	1.From the perspective of the characteristics of the semantic segmentation task, this paper proposes to improve segmentation performance by overcoming the problem of independent instances in MIL. 2.Weakly supervised methods are relatively sufficient in the experiment, which can prove the effectiveness of Swin Transformer Based MIL.	-The paper does not present a significant novel concept but rather combines existing approaches in an efficient manner to the given task. It builds upon the Swin Transformer and deep supervision for the side outputs. The latter has already been used for the CNN-based segmentation approaches. Addressing the correlation of MIL instances was the theme of a few recent papers, e.g., the TransMIL method. However, since they are mostly addressing the classification task rather than segmentation, I still believe the minor contribution is valuable. -Since there is no separate validation set, I wonder how the authors determine optimal values for the hyperparameters, e.g., the parameter r of the generalized mean function and the weights of the three side-output layers. It is also unclear how much performance improvements are due to hyper-parameter tuning.	The introduction of Swin Transformer to pathological image segmentation is novel, which considers the relations between instances in MIL. However, the paper lack further explanation or visualization about how Swin Transformer model the relations and benefit segmentation. Deep supervision is also an existing method in reference paper. Most parts of the method is described in detail. However, some details are still unclear and need to be further clarified. The method is compared with some MIL methods, but only one FSL method, U-Net. Also, the experiments are conducted on a single dataset with 2 metrics. It might be better to provide experiments on more datasets, such as camelyon16, with more metrics like dice coefficient.	1.The use of deep supervision to better use image-level annotations and strengthen constraints is already prevalent in other works and cannot be described as a contribution proposed in this paper. 2.The interpretability of the fusion strategy is insufficient and the exact fusion of the side-outputs is unknown,which means the definition of is not clear. In other words, the features of the side-output of each layers are different, which means that different fusion strategies bring different effect. 3.There is an ambiguity between the visualization results and the experimental results in this paper. The visualization results of UNET are very close to the results obtained by Swin-MIL , and it seems that the results of the Swin Transformer Based MIL outperform those of the UNET, which is inconsistent with the comparison of the performance on F1-scores and Hausdorff Distance between them.	Probably. The authors mention that all code and models will be made available upon acceptance.	The paper gives detailed explanation of its method and experimented on a public dataset and prepares to release the code, thus the reproducibility is good.	This paper is reproductive.	I suggest adding the discussion about chosen hyperparameters, discussion of the weakness of the approach, and possible failure cases.	"It would be better if the authors further explain how Swin Transformer consider relations between instances. E.g. in Method, the statement ""Similar features get high attention weights while dissimilar ones get low attention weights, which leads to an improvement in distinguishing foreground and background"" need further explanation. The image size and patch size needs further to be clarified. I wonder whether the H&E refers to original size(3000) or downsampled size(256). Also, is it necessary to downsample the images from 3000 to 256 since only 4*4 patches are fed into Swin Transformer. Some details of the method need further explanation or correction. I wonder how multi-scale features are fused through fuse layer. In Fig.1, the ""Swin Transformer Block"" is actually two successive Swin Transformer blocks. Also, the Decoder is mixed with the ""structure of decoder"" so the clarity can be improved. The paper uses F1 and HD as metrics. But there are many other popular metrics such as dice coefficient for semantic segmentation and it would be better if these metrics are adopted."	1.In page 3, the author defined Y_n \in {0,1}, and in the last line of page 4, the author defined, where the author confused this confusion between Y_n with \hat(Y_n). 2.The author did not specify whether as the probability of the pixel  in the th image being positive or negative. 3.The full name of CAM was not given when it first appeared in the paper.	Although a weak novelty, the proposed model is motivated by reasonable ideas of modeling correlation of instances of MIL approach for the segmentation task, and experimental results show the effectiveness of the proposed method. Considering the possibility of application in a wide range of medical image segmentation, I lean toward accepting this paper.	The method is novel in introducing Swin Transformer with deep supervision to pathologic image segmentation. Some comparison and ablation study proved the effectiveness of the method.	The weakly supervised semantic segmentation is interesting. This paper is recommended as acceptance due to the contributions and motivation claimed in the manuscript.
522-Paper1805	Transformer Based Multi-task Deep Learning with Intravoxel Incoherent Motion Model Fitting for Microvascular Invasion Prediction of Hepatocellular Carcinoma	This paper focuses on prediction of MVI of HCC. The authors used a transformer approach in the network to obtain deep features of the images and, furthermore, they proposed a cross-attention block to improve the performance.	In this work, the author proposed a multi-task learning method based on transformer to perform (1) MVI prediction and (2) IVIM parameter fitting at the same time, leveraging the fact that those 2 tasks are related to each other. The method is compared to 2 other multi-task learning methods and single task methods for each of the 2 tasks.	This paper aimed to perform simultaneously IVIM parameter model fitting and MVI prediction using a multi-task learning method based on transformer. The originality of the work is to combine the advantages of convolution network and transformer to jointly achieve IVIM model parameter fitting and MVI prediction, which allowed the authors to obtain better results than those obtained when performing one single task (fitting or prediction).	Altought this paper uses well-known techniques and algorithms, the authors compile a robust and (from my point of view) innovative solution. They provide proper methodology and implementation details and compare its results with previous studies.	The use of transformer and multi-task learning to leverage information from 2 tasks that are linked is interesting. the comparison to other methods is extensive.	Design a transformer-based multi-task learning model Perform simultaneous IVIM parameter model fitting and MVI prediction Demonstrate better results with multi-task strategy.	Maybe, the most significant problem could be the very reduced number of samples. 114 HCC are a really small dataset and I cannot be sure of the performance of this proposal with a dataset of proper dimension for this kind of technique.	This paper is an incremental improvement of the method described  in [21]. The additional aspects are not clearly stated in the paper which makes it difficult to assess its novelty. The paper is also not very well written and not easy to read, follow. It would benefit from a careful English proof reading.	Not appropriately justify the methodological motivation of the work No IVIM parameter maps have been shown.	No comments. Everything is clear	source code seems to be available.	No elements on paper reproducibility could be found in the paper.	If possible, I strongly suggest to increase the samples to test the proposed method with a proper dataset. Although, cross validation is really useful for this small dataset (together with removing of outliers or selection of relevant features) I am a bit concern about possible overfitting or other problems related with this kind of very reduded dataset.	Some points in the method are not really clear: The author claim that Compact Convolutional Transformer (CCT) enable the transformer to work with small dataset of medical images but this is not proven in any experiment. The IVIM Model Parameter Fitting Task not clear: How is it self-supervised? The Ground Truth assessment is also not clear What would be the results if the method from [21] will not be modified to use ResNet-18? The training, evaluation, testing data split are not explained. No results are shown in the paper. The results shown in the supplementary materials should appear in the paper. I know that space is limited but table 1 and 2 could be merged into 1 single table for example. Minor points: Table 1 and 2: best results should be highlighted for better clarity. ref 22: typo: gd-eob-dtpa-enhanced?? [19,21] are 2 studies so in intro: a recent study -> recent studies page 4: both two task-specific embeddings is passed to the task shared transformer block -> both task-specific embeddings are passed to the task shared transformer block What is ViT? Parameter is Eq 1 are not introduced. p7 spacial => spatial	"This paper aimed to perform simultaneously IVIM parameter model fitting and MVI prediction using a multi-task learning method based on transformer. The originality of the work is to combine the advantages of convolution network and transformer to jointly achieve IVIM model parameter fitting and MVI prediction, which allowed the authors to obtain better results than those obtained when performing one single task (fitting or prediction). However, methodological motivation of the work was not appropriately justified. The authors motivated ""the parameter fitting of the IVIM model based on the typical nonlinear least squares method has a large amount of computation, and its accuracy is disturbed by noise."" But, the paper didn't deal with these two aspects. On the other hand, the authors claimed that their method performs better IVIM parameter fitting; but throughout the paper, the authors have never shown IVIM parameter maps, which is an obscure point. A few typos:: ""embeddings is passed"": are. Fig. 1, ""Average polling"": pooling."	The proposed method, the implementation and the compared results of this papers are good. My only concern is related with the very small dataset.	incremental work from [21].	Design a transformer-based multi-task learning model Perform simultaneous IVIM parameter model fitting and MVI prediction Demonstrate better results with multi-task strategy No IVIM parameter maps have been shown, which are essential in IVIM applications.
523-Paper1238	Transformer Based Multi-View Network for Mammographic Image Classification	"This paper designed a multi-view network based entirely on transformer architecture. The used ""cross view attention block"" can work better in a pure transformer style. This paper introduced a learnable ""classification token"" into the network. This token can gather all useful information to make better prediction. This paper designed ""(Shifted) Window based Cross View Attention Block"". This structure can fuse cross view information anywhere in the network with low computational cost."	"This paper introduces several cross-view attention mechanisms to learn representations for multi-view images (i.e., mammographic images).  This paper introduces: (1) a ""cross-view attention"" to aggregate information over multiple views. (2) a learnable ""classification token"" to make better predictions. (3) a ""shifted window-based cross-view attention block"" for saving computational cost."	This paper proposes to use transformer architecture for multi-view mammographic classification, to more effectively use the cross view attention mechanism. A classification token is introduced, and experiments show that the proposed approach outperforms feature concatenation based approaches and CNN based cross view attention approaches, on malignancy classification task.	This paper proposes a new method for classification of mammography images.	Cross-view attention is interesting. The proposed model performs the best on the DDSM dataset (Table 2). This paper provides empirical studies on the fusing stages (Table 1).	The proposed pure transformer based multi-view network seems to be novel. Intuitively, it can better utilize cross view attention mechanism, which is verified by the experiments. Previous multi-view approaches rely on hand crafted attention blocks, which are kind of ad hoc.	1.Literature research is insufficient. The novelty of the proposed method needs to be explained. 2.The network process description is not clear enough. How do the three sub-networks evaluate the results as a whole? 3.In the experimental part, the method in this paper should be compared with the methods mentioned in the introduction to demonstrate the effectiveness of the method. 4.Figure 2 does not have acceptable image quality. 5.The English expression of the article is very poor.	Key details are not explained well. For example, this paper does not provide details (e.g., equations or figures) for the proposed shifted window-based cross-view attention block. Ablation study. This paper does not provide the experimental results of Swin-T with feature concatenation. Therefore, we are unable to directly compare the proposed cross-view attention with the simple concatenation. As a result, we are unable to know how much improvement does the proposed cross-view attention bring. On page 5, there are some issues with the fonts. For example, Q, K, V, \alpha, \beta in the last paragraph.	It would be interesting to compare the performance on other tasks as well, such as lesion detection tasks.	The method proposed in the article has good reproducibility, and the referenced modules are described in more detail in the article.	Reproducibility looks good.	No code is given, but the description should be clear enough to reproduce.	1.The English expression of the article needs to be improved. 2.The abstract should state the novelty of our method and why it is useful. 3.The article needs to strengthen the logical ordering, and clearly describe the algorithm process and working principle. 4.The article needs to add relevant experiments to prove the superiority of this method.	In general, the idea is interesting. However, more detailed descriptions should be provided for the shifted window based cross view attention block. More ablation studies should be provided, such as (1) the experimental results for Swin-T (feature concatenation) should be provided; (2) shifted cross-view attention vs cross-view attention.	Adding experiments on other tasks can further strengthen the results.	1.The literature survey is not sufficient to verify the novelty of the method proposed in the article. 2.The logic of the method part is not clear, and the experimental part is not perfect. 3.The English expression of the article needs to be improved.	My major concerns are (1) the description of the shifted window based cross-view attention block is not quite clear, which is a major component of the model; (2) without the results of Swin-T (feature concatenation), we are unable to know how much improvement does the proposed cross-view attention could bring.	Using transformer for multi-view mammographic analysis is natural and seems to be novel.
524-Paper0423	Transformer Lesion Tracker	A transformer based lesion tracker is proposed. In feature selection stage, a sparse selection strategy is chosen for cost reduction. Then a registration augmented cross attention transformer is used to predict the location, followed by a global regression module.	The authors present a novel approach using Transformers to propagate the center of a lesion from the baseline to the follow-up scan.		Significant improvement in performance, from 79.5 to 87.7 in CPM@10mm. Clear ablation study, including SSS, RAMM-CAT, global regressor	The paper addresses the clinically relevant problem of lesion tracking over time. In general, I enjoyed reading the manuscript. It is well structured, has a clear motivation, and is well written. the authors propose a novel approach using Transformer to predict the center of the propagated lesion in the follow-up scan The experiments were conducted on the publically available DeepLesion dataset indicating good performance. code will be available on Git		"The registration benchmark is somewhat out-dated and some new reference could be added. The lesion tracking is similar to video motion tracking, and the searching/template image pairs is similar to 2 frames in the video. Some references in video motion tracking/optical flow:  Teed, Zachary, and Jia Deng. ""Raft: Recurrent all-pairs field transforms for optical flow."" European conference on computer vision. Springer, Cham, 2020. Qin, Chen, et al. ""Joint learning of motion estimation and segmentation for cardiac MR image sequences."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2018. Motivation/results mismatch. It seems the largest gain is achieved from SSS, but it seems the motivation for SSS is reducing computation cost. Further discussion why SSS improve performance is appreciated. Some reasoning is missing. It is not clear why affine transformation is used. For lesion deformation, it is more like a non-rigid registration task. Adding reasoning on why affine is used is appreciated."	The paper misses a discussion about weaknesses or limitations of the presented work and give more insights into the method (e.g. for which lesion types does the method work well or in which cases does the method fail) Overall, in my opinion, there is an imbalance between the sections. The introduction and related work section are relatively long, but the results are only briefly presented and not discussed at all. some information is missing: The output is the predicted center coordinate and a classification result. However, after reading the paper, I don't know what is classified (maybe I missed it?!) some related works are missing:    Hering, A., Peisen, F., Amaral, T., Gatidis, S., Eigentler, T., Othman, A., & Moltz, J. H. (2021, August). Whole-Body Soft-Tissue Lesion Tracking and Segmentation in Longitudinal CT Imaging Studies. In Medical Imaging with Deep Learning (pp. 312-326). PMLR.  Moltz, J. H., D'Anastasi, M., Kiessling, A., Pinto dos Santos, D., Schulke, C., & Peitgen, H. O. (2012). Workflow-centred evaluation of an automatic lesion tracking software for chemotherapy monitoring by CT. European radiology, 22(12), 2759-2767.		In the abstract the author mentioned the code would be published. The dataset is public available as well.	An analysis of situations in which the method failed. [Not Applicable]  -> It is applicable but not done. code will be available on Git		It seems the largest gain is achieved from SSS, but it seems the motivation for SSS is reducing computation cost. Further discussion why SSS improve performance is appreciated. It is not clear why affine transformation is used. For lesion deformation, it is more like a non-rigid registration task. Adding reasoning on why affine is used is appreciated. Add some discussion about dense tracking/object tracking is appreciated.	please don't introduce abbreviations in the abstract The Learn2Reg image registration challenge paper is a good reference for several registration methods! Questions: Why are the images resampled to 2mm for the deeds algorithm? It can also handle larger images and maybe the registration quality is better with a 1mm resolution.		The novelty and performance is good while got some concerns about the motivation/missing discussion and references.	The paper presents an interesting method to track lesions over time. The evaluation is based on similar work and shows good results. Overall, it's good work, but nothing totally groundbreaking either. A discussion is missing and should be added!	
525-Paper1461	Transforming the Interactive Segmentation for Medical Imaging	This paper describes a method to incorporate user interaction into the segmentation process. The method is validated using datasets containing lungs, colons, and pancreas. An ablation study is also provided.	Authors propose a framework based on transformers networks and user clicks to segment any structures and tries it on challenging ones, such as those from organ cancer. It refines automatic segmentations through the addition of click annotations, and it is denominated as Transformer-based architecture for Interactive Segmentation (TIS). They demonstrate their results on three different datasets	"This paper proposes TIS (transformers interactive segmentation), a new network architecture to allow for the refinement of segmentation inferences from a (multi-class) segmentation network (\Phi_ENC \Theta_e) using clicks (i.e. xy positions and labels) encoded through a click encoding network (\Phi_REF \Theta_r). In these experiments the segmenter is a U-Net but can in principle be any type of encoder decoder. The click encoder (the main contribution) is transformers based and takes as inputs both the click coordinates and the (vectorized) encoder output. This step is followed by  a ""label assignment"" mechanism whose purpose is to learn to balance the contribution of the clicks wrt similarity to ground truth labels."	The results of the proposed method are better than five previous methods. The results improves fast with more user interaction. Also, ablation study shows that the new components (click encoding and label assignment) both contribute to the improvement of the results.	The major strength is that most of segmentation methods, in order to have less error, they have to be trained for the specific application. This method ensures they can segment most cases of cancer organs, doing a refinement through clicks given by the users. This method is natural for the clinicians for example.  Where they are used to delineate manually the structures and lesions.  So usability is a must on this paper.	The paper is well written, the proposed architecture is is quite innovative (although I am no expert in interactive segmentation), and results are very convincing, with consistent improvement improved by both the click encoding mechanism as well as the label assignment step.	"The descriptions of the method is not specific enough and sometimes confusing, see detailed & constructive comments"" below."	-The paper defines a method with only tests on challenging datasets Users did not try their method on real applications or in collaboration or supported by medical personnel of a final user. -The assumption of clicks on working progress is not new and other studies have used them before with simpler method.  -Perhaps the authors could elaborate why their results are significant against previous methods.	I  am missing a technical aspect. I fail to understand why the label assignment brings such an addition to the click encoding only, and why click encoding only  perform so poorly. As I understand the label assignment includes the click encoding step. However based on the results illustrated in table 2, label assignment leads to a stunning +8% dice in cancer segmentation compared to click encoding only, while results using click encoding do not seem to benefit at all of increased number of clicks. (-1% Dice between 5 and 10 clicks). Maybe this balance between the two steps should have been clarified better for non proficient readers.	Some important details are missing.	The reproducibility of the paper is possible if authors submit the code to the project . Databases are available for tests . Perhaps a deeper explanation of the software modules would be necessary in order to reproduce it correctly. But it can be done if the code is well documented.	handling UI is always a bit tricky so open sourcing would be welcome.	"There are several points which should be more clear and consistent. (1) The descriptions of click encoding and label assignment are not specific enough: The authors state ""$\phi_{index}$ refers to an indexing function that simply extract the features from the dense feature map"", but it is unclear what feature extraction means here. Also, in the statement ""$\phi_{CE}$ refers to a projection from the category labels to high-dimensional embeddings"" it is unclear how the projection between the category label and the high-dimensional embeddings works. (2) It is confusing in which steps the user interaction (clicks) is used. On the one hand, in Sec. 2.3 it reads ""After stacking 6 layers of click encoding and label assignment modules, we adopt a linear layer to read out the segmentation labels, and train it with pixelwise cross-entropy loss."", so it seems that clicks are involved during training. But on the other hand, according to Sec. 2.1 (""allow end users to refine its own output by incorporating feedback during inference""), the clicks are only used during inference, but not during training. (3) In Fig. 2, subfigure D1 (Lung Cancer), D3-1 (Pancreas), and D3-2 (Pancreas Cancer), there are several curves which almost do not change with increasing number of clicks. In subfigure D2 (Colon Cancer), the green curve even drops when there are more clicks. This is strange since all compared methods are interactive methods, so their results are supposed to improve if more clicks are used. Is there an explanation for this phenomenon?"	The paper has a good organization and the method is explained with enough detail.  Now the next step is to use real data that the system can test. And also a usability evaluation with final users.	"This paper proposes TIS (transformers interactive segmentation), a new network architecture to allow for the refinement of segmentation inferences from a (multi-class) segmentation network (\Phi_ENC \Theta_e) using clicks (i.e. xy positions and labels) encoded through a click encoding network (\Phi_REF \Theta_r). In these experiments the segmenter is a U-Net but can in principle be any type of encoder decoder. The click encoder (the main contribution) is transformers based and takes as inputs both the click coordinates and the (vectorized) encoder output. This step is followed by  a ""label assignment"" mechanism whose purpose is to learn to balance the contribution of the clicks wrt similarity to ground truth labels. Validation is performed by simulating automatic clicks in regions where the encoder failed based on the ground truth, and evaluating the improvement with increasing numbers of clicks. The authors provide a comparison to many other interactive segmentation methods (which I am not familiar with) and demonstrate significant and consistent improvement over SOTA. The paper is well written, the proposed architecture is is quite innovative (although I am no expert in interactive segmentation), and results are very convincing, with consistent improvement improved by both the click encoding mechanism as well as the label assignment. I  am missing a technical aspect. I fail to understand why the label assignment brings such an addition to the click encoding only, and why click encoding only  perform so poorly. As I understand the label assignment includes the click encoding step. However based on the results illustrated in table 2, label assignment leads to a +8% dice in cancer segmentation. Results using click encoding do not seem to benefit at all of increased number of clicks. (-1% Dice between 5 and 10 clicks), while impressive results are achieved using I however strongly recommend to accept this very interesting paper."	Overall results are good, but important details are missing. Also, there are unexpected observations in the comparison with previous methods.	Over all the paper is innovative, but the technical approach needs perhaps a sequence diagram in order to completely understand each part.	This is a very original contribution in the rather niche field of interactive segmentation. The architecture is very generic and it is therefore likely to have a large range of potential applications in the field of medical image segmentation.
526-Paper1860	TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation with Transformers	The paper has presented a transformer architecture to combine cross-view information in medical images that can help in segmentation. The TransFusion approach proposed by the authors is able to capture long range dependencies between different scales and views.	In this work, the authors have proposed TranFusion algorithm for the segmentation of Right Ventricles from cardiac-MRI. The proposed algorithm use to merge the divergent information from multiple views and scale. using attention mechanism.	This manuscript proposes TransFu-sion, a Transformer-based architecture to merge divergent multi-view imaging information using convolutional layers and powerful attention mechanisms for automated medical image segmentation.	The problem is well-formulated.  The authors claim that the size, modality and even dimension can be different among views. This is an interesting take.	Following are the strength of the paper: The paper is well written and the work flow is explained properly. The novelty of the work is satisfactory. The algorithm was tested on multi-centric, multi-View and Multi-disease data.	The proposed method is crucial to improve the performance and robustness of automated methods for disease diagnosis, which combines information from multi-view images. The authors propose a divergent fusion attention (DiFA) module to handle the multi-view inputs. A Multi-Scale Attention (MSA) is used to collect global correspondence of multi-scale feature representations.	The average runtime for each result, or estimated energy cost is missing.  The motivation for choosing transformers based architecture for the task is not well elaborated. Since the model is evaluated on cardiac MRI only, it is not appropriate to claim that it is a medical image segmentation model. Accordingly, the title and text should be updated and made specific.	Following are the weakness: It would be great, if the authors would provide the computation details like the training time of all the methods use to compare with TranFusion. Different types of U-Net (TranU-Net or Res-U-Net or vanilla U-Net) were compared. It would be nice if it is compared with U-Net having attention block. Though, Trans U-Net has the transformer component but  still it would be interesting to see the comparision with Attention U-Net.	The Fig.2 misses some detailed explanations, and it is hard to understand. The proposed DiFA and MSA modules seems to be the increment from the single input to the multi-view inputs.	The authors have chosen YES in response to all the questions but I am not sure if they address the questions. For example, the scenarios where the approach may fail are not clearly reported. The average runtime for each result, or estimated energy cost is missing.	The paper looks reproducable from the response. The proposed algorithm is validated on the multi-disease and -centric data, hence the paper is reproducable.	Reproducibility of the paper is good.	In last para of Introduction, I would suggest to link the terms with the figure. For example, the terms such DIFA and MSA are mentioned with no background on where these terms come from (though they are later described in the overall method).  The authors should also highlight the limitations of the approach with examples where the proposed method fails to work. Similarly, information on required training resources and how these compare with other methods should be included.	The proposed method looks interesting and the validation is also proper. Kindly, include the computation details of the proposed methods as well as the methods used for comparision including the hardware details.	The proposed attention modules seem to be much heavier than the baselines. The model computations and time consuming may be considered for comparison experiments.	I think the problem is nicely described. The method seems novel. I am of the opinion that it will generate good discussion. There are some limitations in the way the paper is presented that I have highlighted in my comments for the authors (discuss limitations and provide comparison for computational resources).	The novelty and validation of the proposed method looks good to me. The  paper is well written and structured.	The proposed modules are effective for multi-view inputs and the conducted experiments are solid.
527-Paper0417	TranSQ: Transformer-based Semantic Query for Medical Report Generation	The authors propose a Transformer-based Semantic Query (TranSQ) model to generate medical reports. The approach considers report generation as a sentence set prediction and selection problem. So, it learns visual embeddings and semantic queries for the sentence candidate set. It is apparently the first work to consider medical report generation as a candidate set prediction and selection problem. The authors conducted experiments to show that TranSQ achieves good performance against existing methods based on NLP metrics and clinical efficiency metrics. They also provide a sentence-level interpretation of the report to illustrate the approach's explainability.	Proposes a system to generate reports from images by proceeding sentence by sentence.  A set of visual semantic queries are created.  Each query is then used to probe one specific aspect of the image, and the report is constructed from the collection of results	This work proposes to treat the medical report generation task as a sentence candidate set prediction and selection problem, which is novel and interesting. To solve this task, this work proposes a novel Transformer-based Semantic Query (TranSQ) model, which incorporates the well-known and powerful Vision Transformer (ViT) to significantly boost the performance. The visualization shows that the proposed approach can achieve better interpretability than previous works.	Motivation for the paper is clearly explained. Methods used in the paper are interesting and distinct from the current state of the art work in the medical report generation field. First approach to treat report generation as a sentence set prediction and selection problem. Methods section has been clearly explained with the motivations for each aspect of their work (visual feature extractor, semantic encoder, and report generator). Comparison against other methods has been shown and results surpass previous state of the art methods.	The paper advances the SOTA on the report generation problem on two widely used datasets. The comparison to previous work is thorough and clear.	The paper is well-written and easy to follow. The proposed approach is interesting and novel. The experiments on two benchmark datasets show that the proposed approach can achieve state-of-the-art performances.	Failure modes of the approach have not been discussed. Current NLP metrics (e.g. ROUGE-L and BLEU-1) will fail when there is finding uncertainty or absence. Perhaps using CIDEr is a better metric to compare against to detect the presence of abnormalities (Pino 2022 AIIM-D, SSRN). Grammatical mistakes are seen in the paper (e.g. Sec 2.3, paragraph 1 - don't start a sentence with a conjunction like 'and'). But, overall the paper is clear and easy to follow.	"The use of this part based approach leads immediately to questions about overall ordering of the sentences in the result.  The method for ordering is defined, but does this affect the score? Some of the example sentences in Fig 2 seem to refer to priors e.g. ""stable"" ""have increased""- but there are no priors fed to the system, so the system has picked up this language from the training data.  Would be good to discuss."	Please see the Q8 for details. Some very important analyses are missing, e.g., the ablation study. Some important implementation details are missing.	I think the paper can be reproduced sufficiently. Hopefully, the authors will provide their code and make it public as it will be of use to the medical imaging community.	Reproducibility claims software released and documented, but there is no reference to this in the paper (expecting an anonymized footnote.)    The reproducibility statement refers to the supplementary material, but it does not seem to have been submitted.	I believe that the obtained results can, in principle, be reproduced. Even though key resources (code) are unavailable at this point, the key details (e.g., proof sketches, experimental setup) are sufficiently well described for an expert to confidently reproduce the main results, if given access to the missing resources.	How long does the model take to train on the dataset? Were there instances when the model did not perform adequately? What were the main reasons for failure and your best guess as to why? Bipartite set matching with a vision transformer runs into issues when detecting small objects (see DETR Carion 2020). Does your model have similar issues as well? What is the smallest finding that it did not do well on?	It could be interesting to explore the stability under small jittering of the image which would cause the VIT to differently tokenize it.	"Strengths: The paper is well-written and easy to follow. The proposed approach is interesting and novel. The experiments on two benchmark datasets show that the proposed approach can achieve state-of-the-art performances. Weaknesses: Some important implementation details are missing. Although the authors report a very good performance, it is not clear to me which part of their method is responsible for it. In particular, the proposed model incorporates the powerful Vision Transformer (ViT), which is pre-trained on large-scale datasets, and there is no experiment to show how much improvement is brought by the existing ViT and how much improvement is brought by your proposed approach. So I wonder if previous works adopt the ViT as the image feature extractor, can they achieve better performances than your approach? The novelty of the idea is limited. Although the authors claim that ""they make the first attempt to address the medical report generation in a candidate set prediction and selection manner"", in my opinion, this idea is very similar to the existing retrieval module in medical report generation [1]. What are the main differences between this work and previous retrieval modules? Some implementation details are missing. How to initialize the semantic queries, and how to ensure that these K queries have different latent topic definitions? Did you visualize them to prove it? How to conduct the retrieval process in the proposed approach? How to obtain/construct the database used for retrieval? I recommend the authors add a related work section to better discuss the difference between this work and previous works. Meanwhile, it can help the readers better understand the contributions of this work. Although some newest methods, e.g., [2][3], perform worse than this paper, I still recommend the authors quote them in the Tables. typos: 'To solve the problem, We consider' -> 'To solve the problem, we consider'; [1] Hybrid retrieval-generation reinforced agent for medical image report generation. In NeurIPS, 2018. [2] Competence-based Multimodal Curriculum Learning for Medical Report Generation. In ACL, 2021. [3] Writing by Memorizing: Hierarchical Retrieval-based Medical Report Generation. In ACL, 2021."	The paper has a reasonable motivating factor and the proposed approach seems to work very well on the MIMIC-CXR dataset as evidenced by the results compared to the current state of the art. The authors have conducted an experiment where they provide visual attention maps for each sentence, which is useful as it enables the model to indicate the image regions that lead to its confidence in providing targeted sentence candidates.	A new SOTA on the report generation problem is reached, with thorough comparison to prior work.	The proposed approach is novel and interesting, but some important experiments and implementation details are missing. In particular, the paper did not conduct the ablation study to analyze the contribution of incorporating the existing powerful Vision Transformer (ViT), which may bring significant improvements to performance. So it's unclear how much improvement is brought by the existing ViT and how much improvement is brought by the proposed approach.
528-Paper0837	Trichomonas Vaginalis Segmentation in Microscope Images	In this paper, the authors proposed a TVNet for Trichomonas Vaginalis segmentation in microscopy images. The proposed method is constructed by a high-resolution fusion (HRF) module and foreground-background attention (FBA) module. Extensive experiments on a private TVM13K dataset have shown the effectiveness of the proposed method by outperforming several image segmentation methods.	The authors present a new large-scale annotated dataset for the segmentation of Trichomonas vaginalis on microscopy images named TVMI3K, together with a novel deep neural network called TVNet used as baseline. TVNet is a Res2Net-like architecture with five levels of features, high-resolution fusion modules, a neighbor connection decoder and foreground-background attention modules. The proposed baseline performs favorably with respect to nine state-of-the-art image segmentation models.	The contribution of this paper is two-fold. First, a large dataset is created with annotations at different levels. Second, a new method is proposed for TVS.	The proposed HRF and FBA modules are indicated to be effective via ablation studies. The proposed method is making an early attempt on the Trichomonas Vaginalis segmentation in microscopy images. The overall paper is clearly presented and easy to follow.	The publication of a new large-scale annotated dataset is of interest to many biomedical computer vision developers and poses new challenges to the existing methods. The proposed baseline architecture combines very recent modules in order to improve the segmentation results. Such modules are justified using an ablation study.	"Both contributions (dataset and segmentation method) are significant: The dataset is very large and extensively annotated, including common annotations like object-level segmentation masks, but also more detailed attributes at image level (labels like ""multiple objects"", ""out-of-view"") and object level (labels like ""complex shape"", ""out-of-focus""). The segmentation method outperforms latest methods."	"There lack experiments on public datasets, which limits the reproducibility of the proposed method. The HRF module is similar to the channel and spatial wise attention mechanisms, which have been proposed in the following publications: L. Chen, et al, ""SCA-CNN: Spatial and Channel-wise Attention in Convolutional Networks for Image Captioning"", CVPR 2017, pp5660-5667. S. Woo, et al, ""CBAM: Convolutional Block Attention Module"", ECCV 2018, pp3-19. The paper lacks computational complexity analysis on the proposed method in comparison with other image segmentation methods."	"References to previous work on Trichomonas Vaginalis segmentation/detection are missing. For instance, check ""Wang, X., Du, X., Liu, L., Ni, G., Zhang, J., Liu, J. and Liu, Y., 2021. Trichomonas vaginalis Detection Using Two Convolutional Neural Networks with Encoder-Decoder Architecture. Applied Sciences, 11(6), p.2738."" and its corresponding dataset: https://github.com/wxz92/Trichomonas-Vaginalis-Detection In the comparison with the state of the art, the number of execution trials and hyperparameter exploration is unclear. In the ablation study, the Dice and IoU values are missing."	"The description of the segmentation method should be improved (see ""detailed & constructive comments"" along with some minor issues)."	Experimental settings are presented in the paper. However, all experiments are conducted on a private dataset, which is not available during review.	The authors do not mention the range of hyper-parameters considered nor the method to select the best hyper-parameter configuration. They only specify some of the hyper-parameters used to generate results. The exact number of training and evaluation runs (iterations or epochs) is not provided. A description of the hardware infrastructure used is provided but nothing is mentioned about the deep learning framework used nor the code availability. There is no analysis of situations in which the method failed. There is no description of the memory footprint nor an average runtime for each result, or estimated energy cost. There is no analysis of statistical significance of reported differences in performance between methods. The results are not described with central tendency (e.g. mean) & variation (e.g. error bars). The specific evaluation metrics and/or statistics used to report results are correctly referenced. There are no details of train / validation / test splits nor details on how baseline methods were implemented and tuned.	The paper contains enough details for the reproduction of results, although the segmentation method should be described in more detail.	"In Table 1, is there any specific policy to set the definition of the small object, i.e., ratio < 0.1 There is a related work (see below) also focusing on the Trichomonas vaginalis analysis using deep learning methods, please include the discussions and comparisons with it: X. Wang, et al, ""Trichomonas vaginalis Detection Using Two Convolutional Neural Networks with Encoder-Decoder Architecture"", Applied Sciences, 11(6), 2738, 2021."	"An effort should be made with regards to reproducibility and evaluation. More specifically, the authors should provide a better description of the range of hyper-parameters considered for every method (including those of the state of the art), the number of training and evaluation runs, validation split and validation results, etc. In that sense, I recommend to follow the code of good practices proposed by Dodge et al. (""Show your work: Improved reporting of experimental results"", 2019). In page 2 there is a typo: ""deep learning techniques has not yet been well-studied"" should read ""deep learning techniques have not yet been well-studied"""	"Major issues: In the description of the segmentation method (Sec. 3), details about NCD are missing. There should be a high-level summary about how NCD [4] works. A comparison with the partial decoder component [31] should be made in the introduction, not here. The overall presentation of the method should also be improved. For example, in Sec. 3.1 and Fig. 2 it is unclear what $P_1, ... P_6$ are, since they are only mentioned later in Sec. 3.3. In Fig. 2 the arrows for $P_3$ to $P_6$ are also confusing since it is unclear where they originate, and where the arrows for $P_1$ and $P_2$ are (or, whether $P_1$ and $P_2$ exist). Minor issues: Sec. 3.3: Why does the weak foreground region $F^2_{i+1}$ contain boundary information? Does ""weak"" mean that the foreground feature is not strong enough? If so, then the boundary information there might be inaccurate."	Overall, the paper is clearly written. However, the reproducibility is limited since there is no experiments on the public datasets. In addition, the overall methods lack novelty.	The novelty of the proposed dataset is clear and the baseline results are promising, although they should be confirmed with a proper description of the hyperparameters used for every method.	Both dataset and the segmentation method are very convincing.
529-Paper0983	UASSR:Unsupervised Arbitrary Scale Super-resolution Reconstruction of Single Anisotropic 3D images via Disentangled Representation Learning?	The paper presents a method to increase the resolution of a 3D medical image (an anisotropic set of slices). The method relies on a generative adversarial network that learns how to augment resolution without requiring a huge number of samples as other machine learning approaches. The method is compared with several other methods of the literature using well known metrics. Quantitative results show that the presented method achieves higher scores on most of the comparisons. Qualitative results show visually pleasant results that are similar to the ground truth.	This paper addresses the problem of recovering high-resolution images from low-resolution images. Compared with other methods requiring the paired high and low-resolution images as input, this paper introduces an unsupervised arbitrary scale super-resolution reconstruction (UASSR) method to solve this problem and achieve good performance without pairing images between two resolutions.	This paper presented an unsupervised super-resolution methods via disentangled representation learning. The proposed method split images into content space and resolution specific space. The evaluation on MRI and CT images demonstrates the effectiveness of the proposed method.	Using a GAN for single image SR is novel in medical imaging, as far as I know. The results presented are comparable and potentially superior to the SOTA methods for SR.	Interesting idea with novel method: this paper introduces using an unsupervised arbitrary scale super-resolution reconstruction (UASSR) based on disentangled representation learning to eliminate the requirement of paired images for training. Good writing: this paper is well written. The whole method is clearly presented and, overall, easy to understand. Good results. Authors compare their methods with some existing SOTA methods and show better performances.	The proposed method focused on unsupervised learning, which gets rid of paired training data. Splitting the images into content space and resolution specific space seems reasonable. Eventuating on two different modalities is a strong point.	"It is unclear in the experiment what ""507 MR images"" mean. These are 507 slices of the same exam? Or 507 exams of different knees? The same for the spine. The quantitative assessment is input dependent. It is uncertain if the improvements will repeat with other data. The differences in the comparison with SOTA methods are small. The qualitative evaluation is limited to visualizing one sample image and only in the authors' opinion. There is no independent assessment by a population of experts. It is also uncertain how the method will perform on natively low resolution images or on increasing resolution of natively high resolution images. The fusion part is unclear. Interpolation and SR can be applied in different orders to obtain arguably different results. It seems that for lack of space the authors did not detail that part. The paragraph ""ablation study"" does not make sense for me. I do not see ablation there as my perception is that fusion is an extension and not part of the method. It is unclear in table 2 how the metrics are applied to ""with fusion"" and what ""arbitrary"" means in that context. Limitations and applicability are not discussed."	Some of the experiment details are not clearly shown, such as the train/val/test split for the datasets that are used in the experiment is not clearly presented. Since the inhouse dataset of spine CT scans is a new dataset, we can better assess the model better and apply other methods on this dataset if train/val/test splits are shown in the paper.	Simply modeling resolution space as a Guassian distribution seems rather simple. The big picture Fig. 1 could be improved with more information. The comparison between the proposed and SMORE should be discussed.	Part of the data used is publicly available. The methods are thoroughly explained in the paper, except for the fusion part. Several items for which the authors responded yes are not included: A clear declaration of what software framework and version you used. A link to a downloadable version of the dataset (if public). Whether ethics approval was necessary for the data. Information on sensitivity regarding parameter changes. Details on how baseline methods were implemented and tuned. The details of train / validation / test splits. A description of results with central tendency (e.g. mean) and variation (e.g. error bars). -An analysis of statistical significance of reported differences in performance between methods. The average runtime for each result, or estimated energy cost. A description of the memory footprint. An analysis of situations in which the method failed. A description of the computing infrastructure used (hardware and software). Discussion of clinical significance.	Authors promise to release the code as well as the dataset if the paper is accepted. Thus I think reproducibility will not be a problem for this paper.	The proposed method is complicated. Without released sourcecode, the reproduciblity of this work could be an issue.	"The paper is interesting and well presented. I will detail here the weaknesses to help authors understand what is not ok and why. The dataset is not clear from the begining. At the end, ""507 MR images"" seem to be 507 complete exams of different knees, but in the beginning I though it was a single knee exam with 507 slices. The same for the spine. The quantitative assessment should have at least a mean and sd so we understand that the data in table 1 is issued from a population of exams. Otherwise it looks like a single slice is being tested. If this is the case, the result is sample dependent and the contribution is very small. Supposing that the experiment has a large population of exams and the data in table 1 is the mean, the differences in the comparison with SOTA methods are small. An analysis of variance would be necessary even to determine if the differences of the means are sgnificant. The qualitative evaluation presented is limited to visualizing one sample image for each dataset and only in the authors' opinion. There should be an independent assessment by a population of experts. Here, it would be also interesting to see how the method will perform on natively low resolution images or on increasing resolution of natively high resolution images. Finally, even if just for curiosity, it would be interesting to see how the method performs on general photographs. The fusion part was the most unclear for me. After reading the paper again, I could understand that the initial goal is to increase the resolution of the whole volume, but that should be made clearer form the beginning. While understand that new slices can be made from interpolation and an axial stack can be computed from a sagittal stack, I would not call that ""different views"". The term was misleading. Moreover, interpolation and SR can be applied in different orders to obtain arguably different results. The pipeline in fig 2 indicates that the HR sagittal is built from the fusion of HR axial and HR coronal. To do so, LR axial and LR coronal are resampled from LR sagittal, then LR axial and LR coronal pass independently through the UASSR before being fused. In such a way, the original LR sagittal never passes through the UASSR. It seems that for lack of space the authors did not detail that part. Finally, the paragraph ""ablation study"" does not make sense for me. I do not see ablation there, as fusion is not part of the method (at least I understood it as an extension). It is unclear in table 2 how the metrics are applied to ""with fusion"" and what ""arbitrary"" means in that context."	Please check 4 and 5 for details.	The arbitrary scale is not clearly explained. If the proposed method can deal with three different scales in a single model, the resolution distribution would be Gaussian mixture model instead of a single Gaussian distribution. If the proposed method can deal one with one model, this is no difference with existing methods as existing methods can also retrain the model for different scales. The investigation of resolution space is not enough. How this could interpret the resolution information? Fig. 1 could be redraw to include more information about the training flow. Resolution space is independent to the content. Can resolution space be transferred from MRI to CT?	The method presented seems to be novel and valuable. However, the experimental evaluation lacks rigor. The size and importance of the contribution depends on the statistical validity of the output data. The results on table 1 must be clearly explained. If they come from the average of the metrics applied to a population of exams, the authors can add the missing statistical analysis. However, if they come from the application of the method to one single exam, the results can be just a coincidence.  As this is not clear in the paper, I put it below the line.	Novelty Performance Writing	The proposed method is interesting and seems useful. Evaluation is a strong plus.
530-Paper0805	ULTRA: Uncertainty-aware Label Distribution Learning for Breast Tumor Cellularity Assessment	This paper proposes a method of predicting Tumar Cellularity in breast cancer in a way to leverage label uncertainty in the deep learning process. The network optimizes the distance between a predicted distribution with target distribution (from GT labels)., as well as the MSE between a predicted mean with target TC mean.	Authors propose an uncertainty-aware label distribution learning (ULTRA) framework for tumor cellularity (TC) estimation. In details, apart from directly regressing the TC score with MSE-Loss, authors model the uncertainty of TC score using normal distribution and train a multi-branch DNN to minimize the KL-divergence between the output and the normal distribution. Authors validate the proposed method on the public TC estimation dataset BreastPathQ and achieve the SOTA.	The goal of the paper is to present a novel method for estimating tumor cellularity (TC) in breast cancer on histopathology images. TC assessment by experts suffers from variability and quantifying uncertainty is key for building better evaluation tools. For this matter, in this work, the regression problem on TC value is translated to TC distributions learning, multi-rater process is reproduced by a multi-branch fusion module (each branch is feed with an augmented version of input data). TC score is still included as an additional loss term. The public BreastPathQ dataset was used for evaluation the approaches and results are improved compared to SOTA.	The claimed contributions are original. The concept of learning distribution of labels along with fusing multiple augmentations to mimic clinical uncertainty is very powerful.	The main strength of this work goes to the novel idea. Considering the uncertainty of TC score, authors transfer the regression problem to a label distribution learning problem. They use normal distribution to model the uncertainty of the given TC score and train a multi-branch DNN to fit the normal distribution for TC score prediction.	interesting clinical problem simplicity of the method	Learning the distributions can be time consuming compared to single value labels.	The main weakness of this paper is the limited novelty of the method. The implementation of ULTRA is very similar to the method proposed by Tang et al. [1]: (1) regarding the regression problem as a label distribution problem; (2) utilize normal distribution to model the uncertainty.  It feels like that this article has just slightly changed [1] and used it on the TC score estimation task. [1] Tang, Y., Ni, Z., Zhou, J., Zhang, D., Lu, J., Wu, Y., Zhou, J.: Uncertainty- aware score distribution learning for action quality assessment. In: Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. (2020) 9839-9848	statistical analysis is rather weak for evaluating generalization important parameters (sharpness of the Gaussian distribution, importance of branches, relative loss of KL and MSE losses)  are set empirically and their influences on the outcome are not discussed or investigated. the rationale for multiple branch is rather weak: augmented version do not really reproduce the kind of variability that would change the prediction of an expert.	Dataset is public Implementation / code is not provided	Authors claim they will release the code. If so, the study could be reproduced.	The study is based on a public dataset and code will be provided. More detail could be given in the paper (e.g. exact augmentation strategy, exact architecture of MLP and FC layers).	This paper presents a novel and innovative method to learn tumor cellularity (TC) using distribution instead of deterministic label. The input undergoes multiple augmentations that each is fed to a separate network branch, from which the TC score distribution is predicted. The loss is evaluated by comparing the distance (Kullback-Leibler (KL) divergence) between distributions as well as the MSE between the distribution mean.  The paper includes detailed description for the methodology. It also shows a comprehensive ablation studies and comparisons to other methods.  I have some minor comments: How does the standard deviation is reflected into the Target Distribution in Figure 2. I mean you have a fully connected network to predict the mean of the TC distribution. Do you need to deploy another network to predict the standard deviation? In Equation 5, what can be interpreted is that you have a fully connected network (MBFF) that predicts the pins of the distribution immediately. I feel that there should be some sort of conditional learning between pin in the MBFF? I get that you used KL divergence to constraints the network learning for this purpose, However I would like to check if you explored other methods at this stage, for instance adding vision attention layer or so perhaps? In the ablation study (Table 1), I would recommend adding further augmentations more than 3 to address the performance of this system given this parameter.	Authors can give more introduction about label distribution learning and emphasize the advantages of transferring a regression problem as a label distribution learning problem. Authors should give more details on the generation of heatmaps shown in Fig. 3. If possible, please plot the ground truth heatmaps for comparison. There should be more innovation in the method.	The paper is interesting and well written. Yet the approach should be more justified to be completely convincing. The results are satisfactory but the rather weak statistical validation probably makes some comparison not statistically significant. Uncertainty is considered by replacing labels by distributions but the sharpness of this distribution is set empirically (how?). More medical insight (e.g. prior estimation of inter-operator variability) would be very interesting. Using augmented version of input data to reproduce variability could work in principle but here the augmented transforms (horizontal, vertical flips and elastic transforms) are probably not the ones that would dramatically change an expert assessment. Perhaps more transforms (on contrast) could be considered or samples with high discordance TC analyzed. Many hyper-parameters are set empirically as well as the procedure for predicting TC score from regression and label distribution branches.  These should have been set e.g. with a CV selection on the training set and then evaluated on the validations set.	Innovative idea - novel architecture with enhanced results compared to state-of-the-art models.	A novel idea to transfer TC score regression to a label distribution learning problem. But the novelty of the proposed method is limited.	Although the idea and clinical importance is interesting, there are too many weakness in the paper for it to be suitable for publication. In particular, statistical analysis should be improved and importance of hyper-parameters that are empirically set investigated.
531-Paper2324	Uncertainty Aware Sampling Framework of Weak-Label Learning for Histology Image Classification	The authors present a novel methodology for addressing one of the main challenges when analyzing WSIs in computer-aided diagnosis systems, which is focusing on the right patches to classify.	This paper tackles the weakly supervised histology image classification. Authors proposed a two-stage training framework to train a tile-level classifier with whole slide image labels. The main idea is to use an uncertainty-aware CNN (UACNN) trained with noisy labels to sample the most diagnostically relevant tiles for each WSI, and then to train another CNN based on the sampled tiles for better tile classification performance. The motivation is clear, and the technique is sound.	This paper presents an uncertainty-guided sampling approach for efficient training of  tissue classification in whole-slide imaging. The proposed method uses weak-labeling for tiles with low uncertainty to improve classification accuracy.	The methodology seems correct and it's interesting. The paper is clear and easy to follow.	-Simplify. The models for training are all basic convolutional neural networks. It is easy to implement.  -The improvement is significant. The two-stage training strategy delivers significant accuracy improvement for histology image classification to the baseline the authors defined.	Uncertainty estimation with Bayesian neural networks is a powerful tool in medical image analysis, which this paper uses to select highly informative samples from otherwise weakly labeled WSI. The method is straightforward and the experimental results suggest its effectiveness. I appreciate the use of statistical tests to show the significance of the results. This should become the standard in medical imaging with deep learning.	The authors should compare their approach to other current state-of-the-art techniques, such as applying the Blue Ratio to extract only the most relevant patches from each WSI. Apart from that, the amount of WSIs used is very small, and should be improved.	-The novelty may be overstated. Uncertainty is already used for histopathology image segmentation, i.e. [1],  -The dataset is small and incomplete. There are in total 85 WSIs used in the experiments. The 85 WSIs are categorized into 3 grades but the cancer-free slides, which are usually important for weakly supervised WSI analysis, are absent. This absence affects a lot to the models trained with noisy labels, i.e., the baseline methods. -Lacks comparison with related works. There are quite a few studies in the domain aim to solve the problem of weakly supervised histopathology image classification, e.g. [2-4]. The goal of these studies is very similar to this paper but none of them is compared. [1] Thiagarajan P, Khairnar P, Ghosh S. Explanation and Use of Uncertainty Obtained by Bayesian Neural Network Classifiers for Breast Histopathology Images. IEEE Transactions on Medical Imaging, 2021. [2] Li J, Chen W, Huang X, et al. Hybrid Supervision Learning for Pathology Whole Slide Image Classification[C]//International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021: 309-318. [3] Lerousseau M, Vakalopoulou M, Classe M, et al. Weakly supervised multiple instance learning histopathological tumor segmentation[C]//International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2020: 470-479. [4] Lerousseau M, Classe M, Battistella E, et al. Weakly supervised pan-cancer segmentation tool[C]//International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021: 248-256.	No major weaknesses, only minor comments (see below).	The paper seems to be reproducible.	The histopathology images are obtained from TCGA database. The authors say the code will be made public.	The paper uses a publicly available dataset and the methods are described sufficiently well.	Currenly. there are many publicly-available datasets with thousands on WSIs, such as TCGA-PRAD and PANDA. I would recommend the authors to increase the amount of WSIs used in other to evaluate their methodology in a most robust and realistic scenario.	-Evaluate the proposed method with larger dataset and compared it with SOTA weakly supervised tile classification/WSI segmentation methods. -The Resnet18 trained with noisy labels (WSI labels) for tile classification was used as the baseline. It can be regarded as the lower bound of the proposed method. Besides, the Resnet18 trained with tile-level labels, which can be regarded as the upper bound of the weakly supervised method, should also be evaluated and compared in Table 1. The experiments are not difficult to conduct, as for the slides seem to be fine annotated by pathologists.	"The ""1"" in the second paragraph, Section 1 seems to be a typo. All figure axis labels are very small and can only be read with digital zoom. Try to increase the label size to at least 8 pt.  I assume that the prediction probability is referred to as $ max P(y* x, D) $ but could not find this stated in the paper.  What do the values in Tab.1 represent? Mean +- std? Please state. I would appreciate some sentences or a formula on the loss functions SCE and OR. This would make the paper more self-contained. Beyond the scope of this paper: I wonder how well the uncertainties are calibrated, i.e., how well they correlate with the predictive error."	Although some flaws are present in the paper and some improvements have been detailed, the current state of the paper is good enough. The results may not be as good as they could be (mainly due to the amount of WSIs used), but future work could address that and provide an improvement over current alternatives for detecting relevant areas of the WSI.	-The scale of dataset is small and incomplete for the study. -Lack comparison with important related works.	The paper uses a well-known method from variational inference to increase efficiency and accuracy in WSI classification. It does not propose a novel method at its core, but presents a solid empirical evaluation of Bayesian methods with a nice application in digital pathology. I vote for accept.
532-Paper1024	Uncertainty-aware Cascade Network for Ultrasound Image Segmentation with Ambiguous Boundary	The uncertainty of each pixel from ultrasound images is leveraged to improve segmentation performance. A new uncertainty-aware network with AFM, UAM, and RECM is proposed. The experiment results are shown to be competitive compared to SOTA on three public US datasets.	For the task of ultrasound image segmentation, this paper proposed  an uncertainty-aware cascade framework. The confidence map-guided uncertainty map is adopt for feature fusion, feature attention and edge correction.	This paper proposes  an uncertainty-aware framework with multiple uncertainty-aware modules to improve the segmentation accuracy on ultrasound images.	The motivation and innovation of this work are good. A novel recurrent edge correction module (RECM) adjusts the low-confidence pixels based on the neighboring high-confidence pixels to decrease the weights of the indistinguishable features.	1) The proposed components seem reasonable and relevant to clinical practice. 2) Extensive experiments were conducted to show the effectiveness.	The overall structure is clear and informative. The experiment is comprehensive and the compared methods are new and related. The performance is good with in-depth discussions. The method is novel which utilizes multiple forms of uncertainty to enhance feature sharing and embedded in a cascade network.	The UAM idea is confusing (see detailed comments). Some descriptions, figures, and equations need clearing up (see detailed comments). The authors lack analysis of design ideas or experimental results.	1) Some technical details are not clear. e.g., what is 'ultrasound feature' in Fig.2 and Sec.2.1? 2) There are no qualitative results to show the effectiveness of proposed method in boundary issues, especially in recurrent edge correction.	Some training details are missing, e.g., the number of layers and channels in the network.	since some necessary details are missing, the Reviewer thinks the reproducibility is modest.	The reproducibility is acceptable.	Easy to reproduce.	"The authors apply the UAM structure before each encoder layer of the second U-Net. According to Eq. 2, the UAM requires uncertainty confidence maps, decoder features, and high-scale features. It is puzzling that the size of the uncertain confidence map is 1wh for each layer, which seems to be incorrect. How do you obtain the uncertain confidence map for non-top layers? Also, for the top layer, what are the high-scale features? In Fig. 2, it is best to label the UAM in the U-Net and show more details of the RECM. The operator symbols are difficult to understand. Does the ""x"" symbol in Eq. 2 reflect element-wise multiplication? Is the ""/"" symbol in Eq. 3 representing element-wise division? Does element-wise multiply M_conf and X in Eq. 3? And M_ca should be M_conf in Eq. 2. The authors divide the network training into two phases. Can the whole network be trained jointly to achieve better performance? The RECM and boundary loss actually do not contribute much to the improvement. The first sample in Figure 3 on TN-SCUI shows that the segmentation result of the network differs significantly from GT, and even the borders appear curled. For the results in Table 1&2&3, it is better to add the standard deviation. The authors lack analysis of design ideas or experimental results."	"1) In Fig.1, how the uncertainty map obtained? Considering the proposed method rely highly on confidence map, it is better to list confidence map for better motivation introduction. 2) What is 'ultrasound feature' in Fig.2 and Sec.2.1? 3) Authors mentioned that the method improves segmentation by ""decreasing the weights of features causing the uncertain predictions"". Why ""decreasing""? Will ""increasing"" work? Considering ""increasing"" will make the model pay more attention to the hard regions. I'd like to hear about the discussion. 4) There are no qualitative results to show the effectiveness of proposed method in boundary issues, especially in recurrent edge correction. (Fig.3 failed in this issue.) 5) Considering AFM and UAM are time consuming components, the time efficiency analysis should be added. 6) Typos. e.g., ""duo"" in abstract."	The writing is clear and well organized with clear motivation The experiment is relatively comprehensive (3 datasets and compared with 3 relevant important MICCAI'21 works),  and does a good job on ablation study The results show superior performance to well support the claims The investigation of related work is sufficient The code will be published also There is no discussion of the failure cases, which may due to the length limit There is no standard deviations for the results The figure quality is not very good. Suggest to improve the image resolution. The novelty of each module may be limited or incremental. However, I think it's ok to propose a framework with a good performance.	There are still several errors in the proposed network, and some experimental results cast doubt on the network's applicability. Analysis of design ideas or experimental results is lacking.	The proposed components and corresponding experiments seem to be reasonable. Some minor issues should be revised or discussed as mentioned above.	Good and clear introduction, and comprehensive experiment with good performance.
533-Paper0136	Uncertainty-Guided Lung Nodule Segmentation with Feature-Aware Attention	The paper proposes to adapt a FCN architecture to the segmentation of lung nodules while taking into account uncertainty. The uncertainty stems from the fact that different radiologist contour the same region differently, due to the subjectivity of the manual segmentation process. The authors argue that these differences are not random, and model them in their proposed UGS-Net. This is an enhanced UNet augmented with a Uncertainty Aware Module and a Feature-Aware attention model. The content of the paper is technically sound and the results appear superior to other comparing methods.	This paper proposed a method for lung nodule segmentation on the LIDC-IDRI dataset.	In this manuscript, the author proposes an uncertainty-guided segmentation network to fully utilized all the annotations. A multi-confidence mask is proposed to obtain the uncertainty levels by including the intersection and union among all annotations. Then, the uncertainty-aware module and feature-aware attention module are applied to learn the uncertainty from the ambiguous regions, especially on the boundaries. The experiments on the LIDC dataset show state-of-the-art results, and the ablation study demonstrates each component's effectiveness.	The is technically sound. The observation that areas of uncertainty and disagreement between radiologist correspond to different tissue density when compared to the areas where agreement can be found is clever. The statement that disagreements are not random and therefore can be modelled is interesting The authors provide ablation studies The number of other approaches used for comparison is sufficient	The paper is in a good format and is easy to understand.	The paper is well organized and easy to read. The paper has interestingly found that uncertainty is associated with certain HU distributions in ambiguous regions. As the HU value relates to the tissue's density, the authors take advantage of the uncertainty in the HU value and guide the segmentation network to learn from the uncertainty to generate better results on the edges and ambiguous regions. The attention module by integrating the multiple annotations is novel. The proposed network takes all annotations in training as well as includes intersection and union to enrich the attention on the uncertainty regions. As the state-of-the-art methods focus more on automatically learning the variance with VAE, this paper provides an alternative method to focus on the uncertainty region. 	It is not clear what is the formulation of the Gabor and Otsu operators and how this impacts backprop. It looks like the network is trained end-to-end with the loss L = LBCE (S, GTS ) + LUAM. is that correct? The learning objective LUAM = LBCE((GT)',(GT)) + LBCE((GT)',(GT)) + LBCE(GTS' ,GTS) contains terms that are in contrast with each other, therefore a balance may be found by the optimisation algorithm which takes into account all the different GTs without necessarily modelling the effect of different ground truths. Of course, since the authors seem to use different prediction heads for the computation of the three components of LUAM, this effect might only be mild. In the supplementary materials the authors show ablation studies which seems to confirm that the presented version of the network is actually better. they mention two other versions V1 and V2, without really specifying their respective configuration. I suggest to at least mention the result of ablation studies of 1) FCN trained with LUAM without separate prediction heads, 2) FCN + UAM only and 3) FCN + UAM + FAAM It is unclear to me how certain experiments are performed. For example, the standard UNet used for comparison is trained on single ground truths. It achieves different results on single annotations, intersection and union between annotations. These results are usually inferior compared to UGS-Net. But, was the normal UNet trained on unions and intersections when performing experiments relating to unions and intersections (resp)?	"As described in Sec. 1, line 5, the LIDC-IDRI dataset provides multiple annotations. If a method conducts experiments on this dataset and only chooses a single annotation as the learning target, the method is not a conventional method or a traditional method, it is a wrong method. The motivation of this paper is confusing; Figures in this paper are relatively small and are hard to see. Nodules that are smaller than 3mm in the LIDC-IDRI dataset are commonly not used for experiments. It seems that the authors don't know the dataset enough; There are many grammar and spelling errors. For example, in the Abstract, line 13, ""LIDC-IDRI"" should be ""the LIDC-IDRI""."	"""UGS-Net's input is the lung nodule CT images, and its final learning target keeps consistent with the current mainstream methods, which is a single annotation GTs."" While the multiple annotations are provided, how to choose which annotation will be conducted in the uncertainty aware module and the final output? As the intersection and union provided, did the author consider calculating the average of the GTs, as the intensity of the average mask may include the agreement probability of the nodule region? No comparison with other uncertainty based methods ref[ 8- 10]. In table 1, how to implement the baseline U-Net with multiple annotations?  Minor: Some fonts in Fig. 1(B), Fig. 2, Fig. 4(B) are too small to read"	Some details are in my opinion missing. Reproducing the paper would require quite some effort.	N/A	The author claims to release the source code and the paper provides enough details to reproduce the paper.	Radical changes are not necessary. It is important to explicitly and throughly state why the method is better than a normal FCN (Eg. approaches used for comparison). Without going in too much detail about the dataset you have selected, you want to show that your approach yields better results. So, supposedly, you have a dataset with multiple annotations for each lung nodule. You propose experiments done on union, intersection and single GT annotations. When testing the performance on union you need to train the approaches used for comparison (which do not have the luxury of creating union masks as a byproduct of their execution) on unions. Same thing about intersections, etc.  Revision of ablation studies in supplementary material is suggested.	Please refer to 5. for more information.	Please provide more details for the questions mentioned in limitations. The state-of-the-art methods are only conducted on a single annotation. Please also compare the state-of-the-art methods by adding the uncertainty - references [8-10] mentioned in this paper.	I believe there is merit to this idea and the results seem to confirm that. Clarifications about the experimental evaluation are appreciated though.	Please refer to 5. for more information.	The paper provides a novel solution to integrate all annotations to improve semantic segmentation accuracy. Instead of using VAE to encode the variance, the authors provide an alternative way to implement the uncertainty for annotation via a self-attention model - which is novel. The results show the effectiveness of the proposed methods.
534-Paper0010	Undersampled MRI Reconstruction with Side Information-Guided Normalisation	This paper investigates the use of MRI acquisition protocols, views, scanning parameters/manufacturers as embedding to improve image reconstruction quality in both knee and brain datasets.	This paper uses side information, which is normally accessible but overlooked, as the normalisation parameter to improve undersampled MRI reconstruction. The proposed SIGN encodes MRI acquisition information, attributes/imaging regions into a vector, which are mapped to a parameter space in the normalisation layers. The simple and effective design of SIGN improved performance considerably on two backbones: D5C5 and OUCR.	The authors propose a Side Information-Guided Normalisation (SIGN) module to encode the different acquisition parameters in a heterogenous dataset to generate the normalization parameters for the feature maps in the CNN reconstruction network. The SIGN module is tested in two popular CNN reconstruction networks and is demonstrated to be effective in improving the reconstruction performance.	This paper is novel and addresses an important area of research in the field to further improve image reconstruction accuracy. The proposed formulation is sound and experiments are solid.	(1) The idea of using side information to improve undersampled MRI reconstruction is novel and interesting. The motivation of the design is also clearly presented.  (2) The proposed SIGN is simple and effective. The SIGN module only contains a few embedding and fully-connected layers. They can be inserted in the reconstruction backbones easily.  (3) The improvement is significant, as demonstrated in the Table1 and Fig.5.  (4) The paper is really clear and easy to follow.	The SIGN module is a novel way of providing prior information to the network to improve the learning performance Solid investigation of the SIGN module by inputting the wrong side information	One major weakness is that the evalution of the methods should include comparison with other training mechanisms.	The author should mention the ways of how to insert SIGN in other backbones.	If the reconstruction network has enough capacity, it should be able to model the heterogeneity in the dataset Possible redundancy in the selected side information	I encourage the authors provide open access to their code.	This paper uses public dataset and reports the details of implementations. It would relatively easy to reproduce the results.	The proposed method is clearly described though the code is not made available	As per weakness point.	(1) For the OUCR backbone, the parameters of convolutional layers are shared in the recurrent design. Are the SIGN modules shared as well?  (2) The author could mention a more general case of using SIGN. i.e., How to insert the SIGN modules in other backbone networks. Is it after the convolutional layer as well?	1: The image contrast (T1, T2 or the proton-density) is determined by the key imaging parameters of TR and TE. The question is if the scanning parameters are already encoded by the SIGN, the categorial variables related to the different contrasts are still necessary? 2: It seems the simpler network of D5C5 benefit much more from the SIGN module than the OUCR network. Does it mean that if the capacity of the reconstruction network is high enough, the network itself can model the heterogeneity in the dataset without explicitly inputting the side information? 3: For the baseline methods, even without the SIGN module, is the instance normalization kept? If not, the comparisons are not fair.	The paper is strong with novelty of the methods and comprehensive datasets for testing.	Overall it is an novel and interesting paper, with a simple but effective design leading to superior results. I would like to see it presenting in the conference.	If the reconstruction network has enough capacity, the benefit of adding the side information seems marginal.
535-Paper0077	UNeXt: MLP-based Rapid Medical Image Segmentation Network	This work proposes a UNet-like architecture that is very efficient in computational complexity (and inference time), without sacrificing performance on segmentation tasks. This is achieved by (1) reducing the number of convolution filters used, (2) presumbably by using a summing long skip connection (encoder to decoder) instead of concatenation, and (3) replacing the convolutions at the lowest two resolutions with depth-wise convolutions where mixing across channels is achieved by MLP. Horizontal and vertial shifting of the feature maps is also proposed in thse depth conv + MLP blocks.	The authors propose a method of modifying U-Net with state of the art MLP-Mixer and Token Mixup inspired techniques. The techniques allow the authors method to achieve state of the art results on 4 datasets while using only a small fraction of the parameters and runtime.	The authors propose to adopt MLP-based network and combine it with popular ConvNet to achieve faster medical image segmentation tasks with less computation burdens on mobile devices.	UNext is computationally efficient and performs well for segmentation. Shifting of feature maps appears novel but is poorly explained and, more importantly, is not well motivated and is poorly validated. It remains unclear what is done, why it is done, and whether it is really a good replacement of positional encoding. It may be a regularizer. Or it may not do much at all.	The method appears to be novel, pulling in state-of-the-art papers and techniques from ViT and MLP-Mixer which have previously mostly focused on classification techniques. The motivation is well founded, being able to learn maintain or slightly improve while dramatically reducing the runtime and memory is essential in many applications. The authors proposed approach achieves slightly improved results over state of the art, but more impressively, it does so with a fraction of the complexity (space and time). Not only are their practical benefits to this, it shows potential theoretical benefits where strong features are able to be learned without needing the massive parameter counts of similarly performing networks. The experiments and ablations appear to be thorough and show the contribution of each proposed novelty. Further, the authors splitting of the data and inclusion of error bars was very appreciated.	The efficiency improvement is satisfactory. The idea of combining ConvNet and MLP-based Network is interesting. The application is of broad interests (i.e., point-of-care applications).	The method is not sufficiently or clearly described. I had to search for UNext code online to understand it. There is no comparison to, or even mention of, other works that reduce the computational complexity of UNet-like models.	"The authors state that ""It is worthy to note that we experimented with MLP- Mixer as encoder and a normal convolutional decoder. The performance was not optimal for segmentation and it was still heavy with around 11 M parameters."" Although this work is not a segmentation based work, there is enough similarity that the authors felt it justified to add an entire paragraph ""Difference from MLP-Mixer"". Given that, the authors should include these results in the main table rather than a vague statement that its ""performance was not optimal."" It would be worthwhile to move the two additional dataset experiments from the supplementary to the main paper. The introduction can be cut down some, there is a bit more space spent on motivation than is really necessary."	"Some similar ideas of adopting MLP-based networks are recently proposed under heated discussion, thus, the unique novelty or technical contribution (made in this work) should be strengthened and emphasised. Comparison with the counterparts from MobilNet family is missing, which is an important milestone established in the domain of edge/mobile computing. Adding a comparison with them, in terms of both efficiency and effectiveness, would be more convincing. (Some available resource and hope it helps: https://coral.ai/models/semantic-segmentation/) Cross-validation (i.e., 3-fold, 5-fold or etc) is suggested. The current experimental setup is specified as ""We perform a 80-20 random split thrice across the dataset and report the mean and variance"". However, the variance of the proposed method (within Tbl.1) looks a bit unstable among the others, especially when comparing with the competitive ones. There is no doubt regarding the efforts paid in reducing the computation burden, but it would be very useful and practical to the community if a statistically solid benchmark can be built. Qualitative analysis on more popular medical image segmentation datasets is preferred, such as MoNuSeg. The qualitative analysis in Fig. 5 looks meaningless and relatively pale. Honestly, the claimed superiority of the proposed method cannot be told from a personal perspective, which brings me the idea that whether more popular and more competitive medical image segmentation datasets are suggested."	While the code is not included or linked in the paper, it is easy enough to find. Unfortunately, it was necessary to look at the code to understand the paper since he method was insufficiently described in the paper. Still, the code looks like it should allow full reproducibility, which is great.	The reviewer agrees with the checklist and is happy with the reproducibility of the paper overall.	The authors promised to release the code and pre-trained models after review process. The dataset split (80:20 split thrice) is also necessary for reproducibility, however, N-fold cross validation is still preferred.	"Clarity Writing and grammar are generally fine. Poor clarity is mainly a result of missing details or descriptions. eq 4: is T actually T_W? identify 'PE' as 'positional embedding' in table 2 caption ""We split the features to h different partitions and shift them by j locations according to the specified axis. This helps us create random windows introducing locality along an axis."" What does this mean? How do you determine j? is j different for each partition? Don't you lose much of the info when shifting by a big j? [looking at the code, it seems you roll the map instead of shifting it (and presumably zero-filling it then)] ""To tokenize, we first use a kernel size of 3 and change the number of channels to E, where E is the embedding dimension (number of tokens) which is a hyperparameter."" What does this mean? What is the full tokenization? is it just a single conv layer? [looking at the code, it seems to be so - this must be clear in the paper] What kind of skip connections do you use? This is not mentioned in the paper; concat requires many more parameters than sum [looks like it's sum in the code]. Positional encoding, shifting, and tokenization [25] encodes position info with 3x3 conv from a 4x4 patch by relying on the zero-padding; there is no such padding nor any patches here so why would conv give a positional encoding? How does shifting give a positional encoding? Is shifting of feature maps just having a regularization effect? It's not clear that shifting helps or how it helps. The performance improvement is marginal so shifting should be tested on more, larger public segmentation tasks. The benefit of regularization also decreases with training set size so a large training set would be useful. The 'tokenization' method is never defined (the explanation is unclear and insufficient). While words like 'transformers' and 'tokens' are in vogue, they seem ill-applied here where there are no image patches extracted; rather, performance improvements here seem to come from decoupling convolution from feature mixing: conv is depth-wise and feature channels are mixed by a fully connected layer. Validation of results How does performance vary across repetitions of model training? How was statistical significance evaluated? This is not described. There is no mention of or comparison to other fast segmentation methods: ENet, FSSNet, FastSCNN, Squeeze U-Net, C-Unet, etc, etc. It's strange that all methods achieve almost the same performance on MoNuSeg and RITE - what about datasets where recent methods impr"	"Typo at end of section 2. ""...across the embedding dimension H... In our experiments, we set H to 768"". H is used for height. Earlier in the section it was stated that ""E"" is the embedding dimension."	"Typo in Page 5: ""more smoother"""	A high performance segmentation method with low computational complexity is a useful contribution; however, this work is not validated against other low computational complexity segmentation methods. Furthermore, many details of the method are not described and the feature map shifting innovation, while novel, is not well motivated or well tested.	State of the art performance at a fraction of the cost. Some novelty, although mostly pulling in state of the art methods from vision classification works (ViT and MLP-Mixup), the authors do use these in new ways and even have some added novelties.	See main weaknesses above.
536-Paper1888	Uni4Eye: Unified 2D and 3D Self-supervised Pre-training via Masked Image Modeling Transformer for Ophthalmic Image Classification	This paper proposes a self-supervised architecture allowing to extract latent representations for both 2D and 3D heterogeneous ophthalmic imaging data. These latent features can then be fine-tuned on different classification tasks. A first step allows extracting a fixed number of patchs (2D or 3D) from the input images based on random masking of the images. These patchs are then fed to a pretrained visual transformer block. Two decoder blocks acting on all patches (non masked ones with visual attention, and masked ones) allows reconstructing the original images as well as the gradient images. Performance of this self-supervised latent space learning is evaluated on six classification tasks based on a dataset concatenating more than 95 000 samples aggregated from different public datasets.	Authors propose Uni4Eye, a self-supervised pre-training approach using Masked Image Modeling (MIM) and vision transformers to learn universal and relevant features of various 2D and 3D ophthalmic imaging modalities. The approach resides in proposing a Unified Patch Embedding module to handle both 2D and 3D data with MIM and multitask learning with the reconstruction of both the input images and its gradient map. Authors introduce and use mmOphth-v1, a large dataset of 2D and 3D ophthalmic image with numerous modalities, which will be made available publicly. The relevance of the proposed pertaining is showed on multiple downstream classification tasks (4 in 2D, 2 in 3D) in comparison to state of the art methods, as well as ablation studies.	The paper proposed a self-supervised method named Uni4Eye that was based on masked autoencoder with a Unified Patch Embedding (UPE) module to enable the model takes both 2D and 3D images as input, and two-branches decoder to enhance sharp edges in the reconstruction. The authors also collected the largest ophthalmic image dataset, and evaluated six downstream tasks to show the superiority of the method.	The general framework design to adapt to different types (fundus and OCT, 2D or 3D) of ophthalmic data -Implementation of mask autoencoder is original, as far as I know and efficient  -Comparison with state-of-the art method is performed -The authors perform some ablation study, to evaluate the impact of the different components of their architecture (mixing 2D and 3D data, use of two different decoders..)	Universality of features: the dataset collected include numerous ophthalmic images modalites and the pretrained model is able to achieve state-of-the-art results on several classification tasks (proposed by different challenges) by fine-tuning, which could make their model a universal pretrained backbone for ophthalmic images features generation and a variety of downstream tasks.  New large public ophthalmic images dataset: authors collected and created a dataset that will be made publicly available containing a wide variety of ophthalmic images modalities as well as 2D and 3D images (although missing some description on the contribution to the data collection).	The paper is well-orgnaized. The evaluation is complete and thorough. The authors contributed what they claimed to be the  largest ophthalmic image dataset.	-The dataset contains very heterogeneous data types . Although the authors perform different types of ablation studies, it is sometime hard to disentangle the impact of the different patterns of the heterogeneous datasets, eg does performance on some type of images (eg fundus from GAMMA) benefits only from the latent representations learned on additionitional images from the same type (eg fundus from EyePACS) or also from additional images of different patterns (eg 3D OCT). Might be interesting to add some ablation study training the SSL network on the different subtypes of images. (Similarly to what was done in Table 2 of the Appendix when considering a separate training on 2D or 3D data). At least, discussion should be enlarged to address this point.	Slight lack of clarity in the evaluation: authors denote the downstream by the related challenge or dataset, which do not necessarily describe what is the actual classification task. The 3D results only report 1 comparison method (which is not state-of-the-art) whereas the 2D comparison methods may be easily expanded to 3D. Moreover the ablation studies results are reported each for 1 different downstream task (certainly for time / resources consideration), authors could explain further their choices.  Although it is a proxy task for the self-pretraining of relevant features, the reconstruction task results (only qualitative) seem quite moderate on visualization on Fig.4, even with the lowest mask ratio (25%).	Some parts of the method are not well-explained. The novelty of the method is limited. The method is based on masked autoencoder (MAE). The novel components are Unified Patch Embedding (UPE), which does not make complete sense based on their current description, and the dual-branch decoder, which is not quite novel.	The authors answered positively to all questions, which does not exactly match the paper content, but the authors mention that they will make the code available which is good!	The code will be made available. Still the model and training hyperparameters are thoroughly described. I made a little comment for the authors on the ViT architecture details and the setting/tuning of the loss weights. The created / collected dataset is described in the appendix and will be made publicly available (although missing some description on the contribution to the data collection).	The code is provided. Not sure whether the dataset is open source.	Short descriptions of the mmOphth-v1 ophthalmic dataset as well as the evaluation strategy (split into train, test, validation ..) should be provided in the main paper. -Details concerning the backbone architectures of the encoder and decoder should be provided. As an example, the authors refers to Vit-large and ViT-base which is not clear for non expert readers. -OCT and fundus images have very different texture patterns. It would be interesting to provide hypothesis on the ViT module adapts and benefits from such a different data types.  -3D OCT images should be better described, eg OCT enface?? Illustrations are provided in the Appendix but no explanation regarding the difference between the different 2D and 3D image types. Would be nice to add some sentences on the differences and clinical practice regarding these different images.	"Major comments: Table 1 of supplementary materials show some details on the collected and created dataset mmOpthth-v1. The table mostly show that mmOpthth-v1 is the concatenation of already existing public dataset (OCTA-500, GAMME, EyePACS, PRIME-FP20 and Synthesized FFA). Some further description / details on the contribution of authors to the collection of the data would be welcome in the main paper. To further investigate the relevance of the self-pretraining and generated features, authors could show downstream task results while freezing the pretrained model and only training a classification head (e.g. linear layer, MLP) on top of it. Authors decided not to describe (briefly) the ViT architecture, still two ViT sizes are used (-base and -large). A detail on the important architecture hyperparameters could be added for both sizes. Authors propose to only feed the visible patches to the ViT encoder. Could this hamper the spatial information between the patches ? Is there any strategy to account for that ? Authors could explain / discuss further this point. For 2D inputs, the proposed UPE module outputs 2D square patches, whereas for 3D inputs, UPE module outputs 3D cubic patches, how does the ViT handle both possibilities ? As training batch sizes are different for 2D and 3D, authors could further explain their strategy to train over the whole 2D + 3D data sets. For example, maybe 1 training epoch = 1 epoch over the 2D data set + 1 epoch over the 3D data set ? Reconstruction results are assessed only qualitatively on Fig. 4, it would be interesting to see quantitative results (e.g. MAE, RMSE, SSIM ...) on the whole dataset and independantly for the different modalities. Minor comments: Authors could explain more on the use of the gradient map with the Sobel filters for the self-supervision task. The gradient map with such filters appear as a rough segmentation of the image and as the collected public datasets seem to sometime also provide the segmentation maps, it could be used alternatively and authors could compare to their proposed approach. authors state that the loss weights were set equals to ""make the network concentrate equally on global intensity information and local edge information"", further investigation could be performed on those hyperparameters."	Fig.1 is a bit confusing. In the input, fundus, OCT en-face, and OCT, are there any relationships between them? or it's just showing the method can support one of the three inputs? Fig. 2 and Sec 2.1 are also unclear to me. It seems that 2D and 3D branches are unrelated, thus it can basically considered as two models? Also, it seems that the blue and orange vectors are combined together to achieve f^d, but it seems to be contradict to the part that 2D and 3D are unrelated? More explanation is needed. Since the method is based on masked auto-encoder (MAE), a direct comparison with MAE  seems to be a natural choice to show the contribution of the proposed components, but it's not compared. Any reason?	The paper contains interesting ideas, experimentation is well conducted, the ablation study might be improved to disentangle the impact of the different image types.	The universality of the approach for ophthalmic images using numerous imaging modalities, as well as both 2D and 3D is very appealing, considering the outperforming results of the model with fine-tuning on several 2D and 3D downstream task, mainly for disease classification. Nevertheless, the lack of methodological novelty in the straightforward self-supervision task using multi-reconstruction with MIM, producing visually moderate results and no quantitative evaluation appears as the main weakness.	The method needs to be better explained.
537-Paper2587	Unified Embeddings of Structural and Functional Connectome via a Function-Constrained Structural Graph Variational Auto-Encoder	This paper proposes a novel method, the Functionally Constrained Structural Graph Variational Autoencoder (FCS-GVAE), capable of combining information from the functional and structural connectomes in unsupervised learning. The method is evaluated on OASIS-3 Alzheimer's disease (AD) dataset. The results show that this method has a better performance than the baseline model.	The author proposed a function-constrained structural graph variational autoencoder model to learn the joint embedding of both structural and functional information. To further evaluate the proposed model, they used the joint embeddings to classify different populations.	This work proposes a function-constrained structural graph variational autoencoder (FCS-GVAE) capable of incorporating information from both functional and structural connectomes in an unsupervised fashion.	-The idea is very interesting, which combines the functional and structural connectomes of individuals to inferences about their differences. -The methodology is explained beautifully, albeit it could be improved.	The author proposed a novel application of graph variational autoencoder model in incoporating the functional and structural connections and identifying joint embeddings.	Contributions: 1). It is very interesting to employ an approach of incorporating function-constrained structural graphs from both functional and structural connectome in an unsupervised fashion to generate a variational graph autoencoder. 2). In this work, the authors provide a comprehensive experimental study and hyperparameter tuning.	-Presentation and Writing Quality: The quality of the paper needs to be improved: figures/ diagrams, and the overall presentation need significant modification before publication. -The Experiments (datasets\ ablation experiment) of the proposed methods can be further validated. -The discussion (weakness and limitation) of the proposed methods can be further revealed.	While the author claims that their model can establish a unified spatial coordinate system for comparing across different subjects, their analyses still rely much on the registration process in preprocessing stage. In addtion, the necessity of including an AE model is questionable and needs further clarification.	Major Concerns: 1). Methodological validation The reviewers are curious about validating proposed methods with other peer deep neural networks. 2). More details of the sampling technique in this paper need to be included. 3). More explanation that orthogonality cannot allow for better compression is needed.	-Reproducibility: Code is not provided.	The reproducibility of the paper is limited.	The reproducibility of this paper is limited. There is no source code released in this work.	This paper proposed a new idea in brain networks. However, in my opinion, there lacks of insightful thinking about the results and some minor comments need to be addressed. -Figure 3 is not explained further in the article, what is the role of Figure 3? -The classification methods ( SVM, MLP, and RFC)of AD need a detailed description. And why not compare with CNN for the AD classification tasks? In Table 1, what mean is the RFC? Why not do an ablation experiment? How much does this node feature(fMRI) affect the downstream tasks? How do the proposed methods and tuning strategies generalize to other datasets?	It would be necessary to discuss the necessity of including an AE model in the proposed framework. Did author use separate training and testing dataset for classification task? Please add more details. Lack the visualization of joint embeddings of structural and functional connectome, which makes it difficult to understand the specific correspondence between them.	Major Concerns: 1). Methodological validation The authors provide the validation based on two embedding techniques for AD classification in this work. In addition, the reviewers are interested in comparisons of other peer deep models. For instance, the authors can evaluate Deep Boltzmann Machine (DBM) to replace the AE in Fig. 1. Here are some references for methodological validation: Salakhutdinov, R., & Hinton, G. (2009, April). Deep Boltzmann machines. In Artificial intelligence and statistics (pp. 448-455). PMLR. Furthermore, did the authors perform cross-validation for classification validation? Unfortunately, there have not been more details of classification validation included in this work. Moreover, reviewers are curious about the comprehensive comparisons. In detail, can authors validate the proposed GVAE with other peer methods in terms of time-consuming and reconstruction accuracy? This validation would further benefit the clinical translational application in the future. 2). Sampling Technique Issues In Fig.1, the authors described a vital computational pipeline. However, the authors do not discuss the sampling technique in detail. Which sampling technique do the authors utilize in this work? Can authors validate their sampling techniques with Gibb's sampling? Or some dimensionality reduction techniques can be thought of as an alternative way to replace the sampling techniques? 3). Why is orthogonality better? In Section 2.5, the authors emphasized, 'Note that, unlike PCA, a single layer AE with no-nonlinearity does not impose orthogonality, allowing for better compression. Originally, orthogonality could maintain the lowest dimensionality of feature space. From the reviewer's perspective, there is probably extensive overlapping existing in feature spaces generated by AE. Alternatively, can the authors provide references to support their conclusion?	-More experiments are needed to prove that the combination of functional and structural connectomes can make the downstream task better. The Presentation and Writing Quality are poor. Not good enough. Limited experiments. Needs more work and/or revisions. I believe it should be rejected.	The novel application of graph VAE model.	The proposed approach comprehensively employs the functional and structural information is promising. Further validating the proposed GVAE with the other peer deep neural networks is required; more details and an explanation of the technique should be provided. Moreover, clarify the terminology such as no-nolinearity.
538-Paper1638	Unsupervised Contrastive Learning of Image Representations from Ultrasound Videos with Hard Negative Mining	This paper presents a contrastive learning based method for USG videos, mining both intra-video and cross-video negatives in a hardness-sensitive negative mining curriculum. The effectiveness of proposed strategy is evaluated on two downstream tasks: GB malignancy classification and COVID detection. And the paper construct a large-scale USG video dataset.	The paper includes two main contributions. One is a USG video dataset of 64 videos and 15800 frames; based on the dataset, an unsupervised representation learning method is proposed, with a simple but insightful hard example mining mechanism.	The authors propose representation learning from Ultrasound Videos with contrastive learning. They propose a hardness-sensitive negative mining curriculum from both intra-video and cross-video negatives. The authors also contribute USG image datasets related to GB malignancy.	The paper is well-motivated with good writing. They propose a novel hard negative mining strategy from the prior knowledge of USG videos, which is quite reasonable. The paper release a large-scale USG video dataset. The proposed contrastive learning method suppresses both ImageNet and SOTA contrastive learning based method.	The paper is clearly written, and easy to understand. The dataset is a contribution to the community. The intra-video hard example mining mechanism is interesting and looks effective.	I think the idea of hardness-sensitive negative mining curriculum learning is novel. The authors performs evaluation against a wide range of baseline methods, 10-fold cross-validation is reported. Ablation study is performed to demonstrate the contribution of proposed components. The USG image datasets related to GB malignancy can be useful for future study. Expert radiologists are involved in this study.	The sampling of positive and negative frames involves many hyper-parameters, which may have a significant impact on the results. The paper should explain how the parameters are selected. The proposed method resembling the classical idea of CPC[1] and DPC[2], which should be added in the related work. Some details: the format is not allowed, as the authors have used  a lot \vspace and parallel table. [1] Contrastive predictive coding for video representation  [2] Video Representation Learning by Dense Predictive Coding	My major concern is regarding the generalizibility of the algorithm. It is not clear to me whether the method can work in other medical modalities like 3D CT images. I will suggest to refer to the paper [1] for more insights on this problem. [1] Contrastive learning of global and local features for medical image segmentation with limited annotations, NeurIPS 2021.	The Butterfly video dataset only contains 1533 images, which is small for contrastive learning. Some details are missing. e.g. what is the size of the memory queue.	Easy to reproduce.	Code and pretrained models are provided, thus the work is of high reproducibility.	The authors release their code and dataset.	Clarify how hyper-parameters are chosen and the impact on pre-training. Add related work mentioned before	The work is based on an observation that in USG videos, there are no similarity between the temporally distant frames. However, I feel that the property also holds for many other medical images. Thus, I expect that the authors can provide an in-depth discussions regarding the generalisation of the method. The investigation of the cross-video sampling is very limited. How is the 'n' determined? Is it better to combine the negatives by their ranking weights against just contrasting all the negatives with equal weights? How is the memory bank maintained, e.g., as a queue? How will the memory size affect the performance?	The authors could use some embedding visualization methods (e.g. PCA, tSNE) to see if malignant images are separated from normal images. They could try their method on larger dataset. They could specify some details like the size of the memory queue.	Quality-wise, the paper is well-motivated, propose an effective method and demonstrate a good performance. And the paper release a dataset. However, according to the guideline, using \vspace in the paper is strictly prohibited, accepting it would seem unfair to the other paper submissions, I'm happy to change the score, if ACs agree that paper format is not a big deal.	The paper introduces a novel dataset and method for unsupervised representation learning from USG videos. The code is provided, making it reproducible. Even though there are some missing empirical studies, they are not related with the main motivation and can be easily studied. Thus, I will recommend the paper for acceptance.	I think the proposed method is novel. The experimental setting is sound. The released USG image datasets related to GB malignancy can be useful for future study. Lastly, expert radiologists are involved in this study.
539-Paper0316	Unsupervised Cross-Disease Domain Adaptation by Lesion Scale Matching	(1) Demonstrate the feasibility of cross-disease knowledge transfer, i.e., to transfer knowledge from thyroid lesions to breast lesions in an unsupervised cross-disease domain adaptation manner. (2) To address the lesion scale gap problem by proposing a lesion scale matching approach, i.e., search for bounding box size in latent space for rescaling, together with the Monte Carlo Expectation Maximization algorithm.  (3) Demonstrate the method with improved performance on one private thyroid US  dataset and one public breast US dataset.	This paper proposes an unsupervised approach to transfer knowledge across diseases i.e. from thyroid nodule to breast nodule classification in ultrasound imaging. It does not require labelled data in the target domain. To address the lesion scale gap across the two domains, they propose a lesion scale matching solution, which entails a framework of latent space search for bounding box size and a MC expectation maximization algorithm.	This paper propose a lesion scale matching approach to rescale the source domain image  and Monte Carlo Expectation Maximization algorithm is employed to match the lesion scale.  It shows noticeable improvement in average in 3 sets of ablation studies.	1) Identified the problem of knowledge transfer between two different diseases. 2) proposed a lesion scale matching approach by searching for bounding box size in latent space together with the Monte Carlo Expectation Maximization	The paper addresses a critical important problem that may be of help in other similar scenarios for cross-disease transfer of knowledge; The paper propose an original approach, with clear methodology and experimental results.	Due to the data limitation in medical domain,  this work holds a valid and practical motivation.  Extensive experiment studies have been conducted.  Especially how the scale disparity affects the estimation accuracy in target domain which address the key argument in this work.	"1) This method is proposed based on existing unsupervised domain adaptation By adding the lesion scale matching the performance can be improved. However, the authors did not discuss the scenario when the knowledge of target data is not available.  2) The authors need to provide a detailed ""algorithm"" in a table to show the whole process. 3) The authors haven't compared with existing ""batch normalization"" based method."	Perhaps the choice of ResNet50 needs to be motivated	However, I have to point out, the scale mismatching issue is a typical problem in the computer vision community.  Namely,  the classification /regression are based on samples from the bounding box.  Namely,  we will not use a city view street image for car classification but the car image patch crop instead. It is also a standard pipeline in most of the data augmentation pipeline to introduce the randomness in scale which is the latent variable z in this work. Though it shows nice theoretical insight about the random scale (LS) here from prior perspective and employs monte carlo EM. It at least missing a strong baseline based on the conventional data augmentation pipeline to random scale z instead using a fixed  z in the ablation study.	"The source codes are not provided.  One of the dataset is private dataset. There is no detailed ""Algorithm"" presented in a table, hence reproducibility may not be that straightforward."	It appears that authors meet the requirements.	code and dataset will be avaialbe according to the submission.	1) Please give some explanation on E1. (2). 2) Section 2.2 regarding how to determine the source and target size, this is totally based on prior knowledge for target domain? 3) Can the authors include an algorithm to show the whole process of the method to help understand.  4) Can the author compare the proposed method with existing batch normalization-based domain adaptation method? For example: * Revisiting Batch Normalization For Practical Domain Adaptation by Y. Li et al.  * Domain-Specific Batch Normalization for Unsupervised Domain Adaptation by W.-G. Chang et al. CVPR'19	Perhaps the choice of ResNet50 needs to be motivated	Due to the similarty of the standard data augmentaiton pipeline, the impact seems to be limited.	"The better results of using lesion scale matching.  The authors may need to give a detailed ""Algorithm"" for reproducibility of the work No comparison with existing domain adaptation methods on batch normalizaiton."	Crucial problem Original methodology Good experimental design	Please check weakness.
540-Paper2464	Unsupervised Cross-Domain Feature Extraction for Single Blood Cell Image Classification	The authors have proposed an approach of features extraction for white blood cell microscopy images such that the features may generalize well across samples collected from different sites. The paper defines the problem as a cross-domain learning task.	The paper presents a cross-domain methodology to extract features in an unsupervised manner from individual white blood cell image datasets.	This work proposed a representation learning method that can learn robust features which can work well on the unseen domains. The authors achieve this by manipulating the cell feature representation from the Mask R-CNN, using self-supervised learning and a domain adaptation approach.	The idea is interesting and might contribute towards building generalized models for wbc classification.	Novelty: although the authors employed some well known methods (Mask R-CNN), the proposal contains novelty characterisation, because it attempts to perform a cross-domain feature extraction based on instance features of a Mask RCNN. Experimental evaluation: The authors exploited three very different blood cell data sets and the two tables show precisely the results obtained, which I consider to be of great interest to the community in this specific task.	The workflow of the proposed method is interesting, which combines self-supervised learning and domain adaptation. Using feature from the Mask R-CNN is smart as the object detection algorithm is pretty robust. The writing is very clear.	There is no comparison with state of the art methods.  The authors does not report any insights on the computational requirements.  The motivation of the pipeline is not clear. Why choose what they choose (for the different components of the model.  Mask R CNN is a semantic segmentation model, not a detection model.	"Introduction and motivations to the work: There is a great deal of nuance involved in haematological classification. The authors chose as a use case a very narrow and highly simplified problem of distinguishing white blood cell types on images containing individual white blood cells. Although the results obtained are of considerable interest and realistically applicable on further domains, ideally I believe the introduction lacks detail and characterisation of the problem at hand. For example, what is the rationale for classification on images consisting of only white blood cells? To cite an example (please note that this is not a request for citation, if the authors do not consider it necessary): in some works, such as this one (https://www.mdpi.com/2076-3417/12/7/3269#) recently, it has been discussed how reliable CAD systems based on CNNs are for the analysis of globules on ""whole"" images, i.e. composed of a multitude of cells, and how it is basically impossible to have a reliable diagnosis on the basis of a direct classification carried out with CNNs. Therefore, I would suggest to the authors to improve the introduction so that it provides a deep overview of the study. In fact, I think it is unclear what unique challenges are associated with this task. In Section 2: from the figure, it seems that the Mask R-CNN training was performed on the three dataset merged. This aspect is not stated in the text. Could the authors be more precise in this sense? Section 2 again: why do you call it 'reconstruction'? It sounds more like image generation to me. If it were a reconstruction, I would expect to see the WBC clean and not something ""polished"". Still Section 2: ""In our experiments GN was effective in image generalization."" How and why? Section 2.1: What is the anchor dataset D0? Please explain. Section 3.2: why was the constant b in equation 3 set to 5? Section 3.2: the training procedure is not entirely clear to me. Did you train on the 80% of the original images? Or on the ""reconstructed"" ones? In reference to Table 1, the authors state ""AE RF: random forest classification of features extracted by a similar autoencoder trained on all datasets with no domain adaptation."". However, I think it should be useful for reader to give, even if brief, a detail about the similar autoencoder, also considering it reached best results in same-dataset experiments. With reference to the classification method used, Random Forest, I think the authors should better motivate his choice (Note: this is not a criticism of the method itself, just a request for clarification)"	"Major: The baselines are not strong enough. In my opinion, the main contribution of this work is in the domain adaptation area, where no label from the target dataset is given. Also this is the place where the proposed method shows better performance (Train on one dataset, and directly test on another without fineturning). This method should be compared with domain adaptation method. But we cannot find this kind of the baselines in the experiments. Inconsistant description for the ResNet-RF method. In Table 1, it seems it is trained on different datasets in each experiment. If so, where does the classification label come from? And in the main text, it said ""ResNet RF: random forest classification of the features extracted with a ResNet101 [9] architecture trained on ImageNet dataset"". The training set is inconsistent here. Minor: The application of the proposed method is limited as the accuracy on the cross domain seems far way from training supervisedly."	One of the three datasets used in the work is private and not shared by the authors.  No code or link for this or an intent for sharing of this is provided.	The authors gave a satisfactory quantity of details for reproducibility.	No code is provided, but the details of the training and network architecture are provided.	"While the idea might be interesting, the authors need to carefully consider the way they present it.  The writing requires some more clarity. I can only quote few examples.  Page 2, Section 1, ""that is analyzed by an autoencoder"", what is analyzed? how is autoencoder an analyzer? (Also, I am repeating comments from above section). There is no comparison with state of the art methods.  The authors does not report any insights on the computational requirements.  The motivation of the pipeline is not clear. Why choose what they choose (for the different components of the model.  Mask R CNN is a semantic segmentation model, not a detection model. Page 5, Section 3.2, ""we decided to use 50 as the bottleneck size"", 50 what? 50 images? not very clear. How does the different component effect the performance? What motivated the choice of these components?"	"Dear Authors, I read your manuscript with great interest and I found it of good quality. Also the results are quite impressive and opens the field for further improvements. However, I think that several clarifications are needed, as in some points I found it unclear. I list them as follows: Introduction and motivations to the work: There is a great deal of nuance involved in haematological classification. The authors chose as a use case a very narrow and highly simplified problem of distinguishing white blood cell types on images containing individual white blood cells. Although the results obtained are of considerable interest and realistically applicable on further domains, ideally I believe the introduction lacks detail and characterisation of the problem at hand. For example, what is the rationale for classification on images consisting of only white blood cells? To cite an example (please note that this is not a request for citation, if the authors do not consider it necessary): in some works, such as this one (https://www.mdpi.com/2076-3417/12/7/3269#) recently, it has been discussed how reliable CAD systems based on CNNs are for the analysis of globules on ""whole"" images, i.e. composed of a multitude of cells, and how it is basically impossible to have a reliable diagnosis on the basis of a direct classification carried out with CNNs. Therefore, I would suggest to the authors to improve the introduction so that it provides a deep overview of the study. In fact, I think it is unclear what unique challenges are associated with this task. In Section 2: from the figure, it seems that the Mask R-CNN training was performed on the three dataset merged. This aspect is not stated in the text. Could the authors be more precise in this sense? Section 2 again: why do you call it 'reconstruction'? It sounds more like image generation to me. If it were a reconstruction, I would expect to see the WBC clean and not something ""polished"". Still Section 2: ""In our experiments GN was effective in image generalization."" How and why? Section 2.1: What is the anchor dataset D0? Please explain. Section 3.2: why was the constant b in equation 3 set to 5? Section 3.2: the training procedure is not entirely clear to me. Did you train on the 80% of the original images? Or on the ""reconstructed"" ones? In reference to Table 1, the authors state ""AE RF: random forest classification of features extracted by a similar autoencoder trained on all datasets with no domain adaptation."". However, I think it should be useful for reader to give, even if brief, a detail about the similar autoencoder, also considering it reached best results in same-dataset experiments. With reference to the classification method used, Random Forest, I think the authors should better motivate his choice (Note: this is not a criticism of the method itself, just a request for clarification)"	As suggested in the main weaknesses section, the baselines are not very strong for comparison. The authors are suggested to use some domain adaptation methods as the baselines.	I believe the idea might be interesting, but the way it is presented and motivated is not very clear.	Novelty of the proposal, results obtained, quality of experimental evaluation.	The baselines are not strong enough, no domain adaptation method is tested as the baseline. The application of this method is limited as the performance on the cross domain are not satisfied. However, the writing of this paper is pretty good.
541-Paper0421	Unsupervised Deep Non-Rigid Alignment by Low-Rank Loss and Multi-Input Attention	This paper presents a non-rigid registration network with a low-rank loss for noisy image registration. The experiments were conducted on synthetic and real images.	The paper propose three subnetworks for learning-based non-rigid alignments of photoacoustic hand imaging, that is, noise decomposition network with noise loss, non-rigid deformation network with low-rank loss, and sparse error complement network with multi-input attention module. Then the authors evaluate their method on both synthetic data and photoacoustic data, and compare with several other SOTA methods.	They proposed a neural network for robust non-rigid image alignment. This method is especially powerful when the noise and corruptions exist in the images. It is based on the idea of low-rank and sparse decomposition, assuming that well-aligned images with corruptions and noise are removed should have a low rank, which is forced with a low-rank loss  It achieves highest score among several rigid/non-rigid alignment algorithms.	The main strength of this work is that inspired by robust alignment by sparse and low-rank decomposition (RASL), the authors introduce a low-rank loss in current registration network to deal with the images with noises or corruptions.	The paper is well organized and written. Considering the actual problems in photoacoustic microscopy imaging, a complete pipeline is designed for multi-scaning pam images alignments.	Main philosophy of the paper is novel and make sense. Compare to previous robust image alignment algorithms, proposed method has following advantages: (1) it could handle non-rigid transformations, (2) since it is NN, we could generalize for dataset. And compare to previous NN based image alignment methods, it is robust since it decomposes sparse noise and corruptions from the data. It could handle non-rigid alignment where severe noise exist, it could be potentially used in pratice.	The main weakness of this work is that the authors didn't give a good description of their methods, the whole method part is difficult to follow. In addition, the application of this method may be limited.	"The discussion in many places seems not objective enough and lacks supporting materials, such as the authors claim it is the first low-rank loss for alignment, but it seems that some articles for registration already exist: A low-rank representation for unsupervised registration of medical images."" arXiv preprint arXiv:2105.09548 (2021); although it is not stated in terms of loss. The author uses multi-attention to complement the sparse corruption, what is the motivation between attention and sparse. And the authors claim the deep-learning-based methods can not have a denoise function, but for the learning of high-frequency information (noises) and low-frequency information, NN can reduce noise very well: Deep image prior, CVPR2018; Training deep learning based denoisers without ground truth data. NeurIPS2018. The experiments can not effectively illustrate their method, I will list it in the comments. A more clear outline of the next steps in research would be appropriate."	There are some curious points for evaluation and comparison. Main concern is that whethere the comparison was fair.	Not so good.	Reproducibility is insufficient. The author's paper states that the codes will be provided, but there is no explanation of the selection and sensitivity of hyperparameter of losses, and the collection of the datasets.	They did not submit the code, but it will be made public according to the authors. Details including hyperparameters, network architecture, computation time, etc. are given in the manuscript. It could be reproduced as the paper.	"-I think the use of the low-rank loss for the registration of noisy images is good. -However, I do not think the authors give a good description of the proposed methods. The whole method part is difficult to follow, I can only try to understand the detailed design by using my knowledge on RASL and robust PCA. -The proposed network may require high computation resources and memory usage. The authors shall compare the parameter amount, floating point operations, and training time, etc. Moreover, too many hyper-parameters need to be tuned for different applications, which may reduce the generalization ability of the method. -The standard deviation of the results should be provided. -The RASL is a classical low-rank and sparse decomposition method, the authors could compare more methods focusing on fast and noisy robust decomposition, e.g.,  Wu, Yi, et al ""Online robust image alignment via iterative convex optimization."" 2012 IEEE Conference on Computer Vision and Pattern Recognition. Zheng, Qingqing, et al ""Online robust image alignment via subspace learning from gradient orientations."" 2017 International Conference on Computer Vision. -The registration accuracy seems not in a high level, with just 60% Dice. If there are other methods reported results on the same dataset, the authors shall mention them so let us know the current accuracy level for this application."	For equation 1 loss noise, how is b chosen or calculated? If the noise can be defaulted to Gaussian noise with a constant mean and small variance, why not limit it, such as with KL loss, etc? and t is also used here, so the convergence is also affected by the transformation? In this case, I think the loss declared by the authors will be more subjective about the penalty for foreground signals. When there is no noise or strong bias, the behavior of the denoising module or the corresponding loss? For the deformation loss, what is the reason why introducing the third regularization for sparsing displacement field. Sensitivity to the selection of the four hyperparameters for overall losses. Regarding the comparison with other methods, it doesn't seem fair. Because the three modules are responsible for part of things (denoise, aligning, sparse or inpainting...), the number of parameters will be more compared to other methods. The Ablation study is not sufficient. There is only a splicing and ablation of three sub-networks, and there is no way to prove the role of the innovation points discussed in each network. The contribution points in each network can not be demonstrated, such as loss noise, deformation...	"Figure 2 has some confusing points. Why outputs of noise decomposition network contains black rectangles? While \tau_2 and \tau_i are different, I_1, I_2, and I_M are same. How the images (S_*) from the sparse error complement network look like in the real data? Since the noise will be removed in the noise decomposition network, inputs for sparse error complement network will be noise-free images. Was there any meaningful sparse corruption in the real data? How the Dice score is calculated for RASL in the synthetic dataset in detail? How many the images are given to the algorithm? For robust alignment algorithms such as RASL, the number of images are important to acquire exact the low-rank space which drives the alignment.  If the 8 number of images were passed to the RASL, it seems a small number of images compare to the experiments in RASL paper (See Fig. 5 of RASL paper). Why the 6.4 pixels were used in the synthetic experiment? It would be better to calculate the metrics for different level of noise level or non-rigid transformation, such as phase diagram in RPCA and Robust alignment papers. In the Table 1, VMorph which is a non-rigid alignment method does not outperform even the RASL which is a rigid-alignment algorithm. Did you inspected the aligned images from RASL and VMorph? Please give us a simple answer about this situation. In the Table 1, what is the take home message for the time measurement? Is it just a result? And time for the NN approaches calculated only for inferencing without training? It is unfair to compare only evaluation time for NN to the optimization-based method such as RASL. Authors' method and VoxelMorph fix one image and do registration to make other images well-overlapped to the fixed image. RASL does not have this property and all images are registered together. In other words, first aligned image using RASL will different from the input first image. If the ground truth mask to calculate dice score was drawn based on the first image, it is natural that RASL achieves lower dice score. Then, it is not appropriate to calculate dice score to compare the performance. One way to do a fair comparison using dice score is that we should first find the transformation matrix of first image estimated using RASL, and apply inverse transformation for all images to achieve a same property that first image is the 'reference' and not be aligned. Followings are minor comments. ""Robust"" in the title is missing in the pdf submission, where it exists in the CMT submission. Is the gaussian noise was applied in creating a synthetic data? Then did the data have passed such as ReLU to enforce non-negativity where the image should be positive? In the Table 1, the word Dice Time seems mistakenly added in the third line. Explanation of Figure 4 is bit confused. The rightmost one is the image averaging without alignment. Seems that the left and middle images are also the average projection after the alignment using propose method and RCN, respectively. If they are, please clarify three of them are all average intensity projected image from the aligned result and raw data."	Please see the described weaknesses and detailed comments.	Despite the flaws in this paper, the overall method and experiments make some sense for the community, so I recommend weak accept.	The formulation makes sense and it is natural to achieve best performance. However, there are some points in the experiments to be clarified.
542-Paper0045	Unsupervised Deformable Image Registration with Absent Correspondences in Pre-operative and Post-Recurrence Brain Tumor MRI Scans	The authors propose a deep learning-based deformable registration method for the pre-operative and post-recurrence brain MR registration. They use forward-backward consistency and inverse consistency to identify regions with absent correspondences and exclude them in the similarity measure. The proposed method is validated using the BraTS-Reg dataset.	The authors of the paper propose a novel method for deep learning based image registration of pre-operative and post-recurrence brain tumor MRI scans. The method automatically identifies regions where no correspondences can be established (e.g. due to tumor resection) and incorporates the resulting segmentations directly into the algorithm for masking the similarity measure and other parts of the loss function. Additionally, the method includes a constraint enforcing inverse consistency and is compared to several other algorithms showing superior registration results especially in near tumor regions.	This paper estimated the bidirectional deformation fields and located regions with absent correspondence, such as the tumor mask.	1 Pre-operative and Post-Recurrence Brain Tumor registration is an important research area. 2 The proposed strategy is sound	The paper is very well structured and easy to follow The method is well classified into the state of the art The novel approach of estimating regions without correspondences is very interesting and well embedded into the overall approach No segmentations (e.g. of tumors) needed Strong evaluation with several compared algorithms (both conventional and deep learning based)	This work addressed the unsupervised deformable registration method for the preoperative and post-recurrence brain MR registration and tumor segmentation.	"1 1 The proposed method lacks novelty. It seems to be a combination of a few ""tricks"" from existing works. The forward-backward consistency is a widely used optical flow technique [19,21,28,29]. The inverse consistency is also a well-known constraint in the registration community. 2 Unfair comparison and incremental performance improvements. 1) Methods used in the comparisons are not designed for registering images with absent correspondence. MICCAI has a community dedicated to methodologies for pre-op and intra-op brain tumor images registration (CURIOUS'18, TMI), the authors should compare the proposed method to the best-performing methods in the CURIOUS challenge. 2) To the best of my knowledge, in Image-guided neurosurgeries, TRE (for landmarks near tumors) within 2mm is considered good. The TRE achieved by the proposed method is larger than 3mm, which might be too large to be clinically acceptable. 3 Lacks discussion potential. The proposed method doesn't show clear innovations and contributions over the state-of-the-art methodologies. I'm doubtful that the MICCAI audience would be interested in this work."	The quality of estimated regions with absent correspondence could be evaluated in more detail, providing evidence for the proposition 'The results demonstrate our method is capable of accurately locating the regions without valid correspondence': How do the masks compare to automatically computed masks for pathological regions (that were used for '-CM' baseline methods)? Can false-positive regions sometimes be a problem?	It would be helpful to discuss the extraction of accurate masks in the proposed unsupervised learning scheme, considering the smooth regularization of registration fields. There lack some descriptions of the regularization term (Eq. 7) and mask threshold.	Some important parameters, such as the threshold for the inverse consistency (in Sec2.2), might be data-dependent and have to be tuned.	The method is well described and the source code will be made publicly available after acceptance of the work. The data are not directly publicly available, but can be requested from the organizers of the competition. Therefore, the reproducibility is rated as good.	This paper has provided details about the models, datasets, and evaluation.	MICCAI audiences are likely not interested in papers with minor methodology novelty and incremental performance improvements. The authors may consider a workshop submission.	The last proposed method 'DIRAC-D' sounds promising, yielding slightly better results including additional MR modality as input. However, all other methods are only cmpared without this additional input It is still a bit unclear how the parameters alpha and p for the forward-backward consistency constraint are determined What are 'successfully registered landmarks'? If there is a threshold applied, how is it determined? Typo on page 6: '...voxels are marked in m_{bf} and m_{bf}', the second 'm_{bf}' should be 'm_{fb}'	It is interesting to use the bidirectional deformation field to define the mask of the tumor. However, it is suspicious to extract accurate masks in the proposed unsupervised learning scheme. It is unclear how to extract the exact boundary of the masks, considering the smoothness regularization of registration fields. Threshold \tau_{bf} is used to define the mask. It would be helpful to describe the threshold selection. The last term in Eq. 7 is used to regularize the mask to be small. Since the mask is represented by a vector or matrix, the appropriate norm is required. The proposed method was compared with those using cost function masking with the tumor core segmentation map. What did the cost function masking mean? Whether the prior mask of tumors was used in methods -CM. Table 1 showed that the proposed unsupervised approach outperformed the CM. It would be helpful to discuss the performance gain achieved compared with the CM. The proposed methods used invertible constraints for valid correspondence. Since the diffeomorphic registration provided invertible registration fields, using the existing diffeomorphic registration network as the backbone would be interesting.	The proposed method doesn't show clear innovations and contributions over the state-of-the-art methodologies. The performance improvement is incremental. It doesn't seem to have any discussion potential at the conference.	The authors of the paper explain the challenging task of registration of images before and after tumor resection and propose an interesting and well explained novel method facing this problem. The paper is well structured and easy to follow, including an extensive evaluation that lacks only minor clarifications. Overall it is a very well written paper.	This paper presented an unsupervised joint registration and segmentation framework, exploiting a forward-backward consistency constraint to estimate masks and registration fields. The proposed approach has been applied to the BraTS-Reg challenge dataset with performance gains over deep registration methods.
543-Paper2562	Unsupervised Domain Adaptation with Contrastive Learning for OCT Segmentation	This paper presents a contrastive learning-based method to deal with unsupervised domain adaptation problem on the OCT segmentation task. The key technology is the positive pair selection strategy. The authors not only utilize the augmentation samples but also the rounded slices in the 3D volumn. The results on their own collected dataset show its effectiveness on various ablation studies.	The authors propose semi-supervised contrastive learning approach for domain adaptation, including augmentation strategy for pair generation, and projection head for embedding generation from convolutional features. The model validated on Spectralis and Cirrus datasets emerged from clinical trials.	The authors propose an unsupervised domain adaptation method for the segmentation of 3D OCT images. They train a U-Net segmentation model with full supervision on the source domain, and explore the effect of an adapted contrastive loss function, a new augmentation strategy, and a new projection head channel-wise aggregation. They state that they outperform  other contrastive frameworks on the target domain, and achieve similar (or sometimes better) results as the supervised method on the source domain.	The paper is easy to follow and well organized. Utlizing the adjacent slice to be the positive sample is interesting and reasonable. The ablation studies is exhaustive and improve a lot compared with the baseline methods.	I find this paper interesting, especially application of the contrastive learning to the problem of the domain shift between OCT devices. The method seems to be well-motivated, novel to some extent, as the majority of the methods focus on adversarial domain adaptation, and validated.	The paper is well written and clearly structured. The method and the performed experiments are described in detail. The evaluations, comparison to SimCLR and SimSiam, as well as the ablation study are sound. The presented method  can outperform existing frameworks on the given dataset for the segmentation task. While the contrastive framework is well known, the novelty lies in the combination of the adapted contrastive loss, the augmentation strategy, and a new projection head. The supplementary material also includes a hyperparameter analysis.	"As said in the paper, the final pairing strategy is first generate P_slice and then apply augmentation on them. Actually, such operation will miss the original P_augm which only does the augmentation on the same slice. The results under P_augm + P_comb would be interesting. Actually, the improvement of updated Projection head did not bring too much improvement from 27.21 to 27.77 in Table 1. ""In Table 1 and Table S2, UpperBound results for a supervised model trained on labeled data from the target domain are also reported for comparison. This labeled data, used here as a reference, is ablated for all other models."" According to the description, the dice performance of SegCLR(Pcomb ,Cch ) should be 29.32 + 27.77=57.09, right? Isi it too low for a segmentation task? Only one dataset is utilized for testing. The generalization ability of the proposed may need to be verified on other datasets. Missing comparison with other unsupervised domain adaptation methods."	some formulas are not clear validation compares only with contrastive learning methods	-In Table 1, only the scores relative to the baseline are given. This is confusing, as the reader has no idea how good or bad the performance of the baseline method is. In the text, it is stated that the baseline method has a poor performance on D_t, but this performance is shown nowhere in the main paper. Looking it up in the supplementary for each class is a bit cumbersome. -It is unclear whether this approach is done for the 3D volumes or only 2D slices. In the abstract and introduction, it is stated that this is a 3D segmentation. However, the pairing is performed on the slices, and the cited U-Net is also originally implemented in 2D. Is the segmentation performance then computed in 2D or 3D? -In Figure 3, the relative scores are reported, which is somehow confusing in my perception. It would be nice to compare the real performance on D_s and D_t, instead of a relative loss of performance.  -In Section 3.4, the authors state that their method with unsupervised domain adaptation remarkably produces results within the ranges of experts' variability. However, I do not see that in Figure 4. If the authors want to make this statement, they should explain more about this in Section 3.4. The proposed projection head C should be included in Figure 1. -Some comment on the model complexity (e.g. number of parameters) should be given. -The authors only compared themselves against similar contrastive learning approaches. It would be interesting to see the performance for other UDA methods such as disentanglement- or adversarial-based methods.	No code is provided.  Some parameters have been provided in the paper.	The method is reproducible	The loss function and hyperparameters are described. The architecture is described. However, there is no link to code and no declaration of the authors to make the code publicly available. It is also unclear whether the dataset is publicly available or not.	See above.	some formulas are not clear tilda notation is typically applied to random variables. It's more suitable to use x \in D in the sum limit, or specify sum limits explicitly. Also in formula (1) for example, is the summation is performed over all samples of the dataset or over all pixels in y_i and F(x_i)? same applies to other formulas. What's the difference between d1 and d2? Both have L2 norm in denominator. Formula (5) uses \in notation instead of tilda. The formulas require revision. validation compares only with contrastive learning methods. The paper lack the comparison with the domain adaptation methods of another origin, CycleGANs for instance [1,2], or adversarial domain adaptation [3] or [4]. Classification methods can be adopted to segmentation. In table 1, please also mention baseline score in absolute numbers, for positive numbers it might be more intuitive to use + to highlight that these numbers are relative. [1] USING CYCLEGANS FOR EFFECTIVELY REDUCING IMAGE VARIABILITY ACROSS OCTDEVICES AND IMPROVING RETINAL FLUID SEGMENTATION https://arxiv.org/pdf/1901.08379.pdf [2] Domain Adaptation via CycleGAN for RetinaSegmentation in Optical Coherence Tomography https://arxiv.org/pdf/2107.02345.pdf [3] UNSUPERVISED DOMAIN ADAPTATION FOR CROSS-DEVICE OCT LESION DETECTIONVIA LEARNING ADAPTIVE FEATURES https://ieeexplore.ieee.org/document/9098380 [4] Unsupervised Domain Adaptation by Backpropagation https://arxiv.org/abs/1409.7495	"Please address all points listed under ""weaknesses"". Please refer in the main paper to the results given in the supplementary, as they help for the understanding."	No comparison with the state-of-the-art unsupervised domain adaptation methods. Only one evaluation dataset is utilized.	I tend to weak accept since from one hand it's an interesting method and interesting application, from the other hand, the paper slightly lacks in validation part.	This approach introduces three novel changes to an existing contrastive learning framework, which lead to an improvement of the segmentation performance in an unsupervised domain adaptation scenario. While the method is well presented and the results are sound, the presentation of the results should be improved. Moreover, comparison to more state-of-the-art methods would be interesting.
544-Paper0139	Unsupervised Domain Adaptive Fundus Image Segmentation with Category-level Regularization	This work proposed a category-level regularization approach for both intra and inter-domain categorization in unsupervised domain adaptation for segmentation. A prototype-guided discriminative loss was proposed to learn discriminative feature representations. The proposed method yielded a decent performance which is comparable to a supervised method that is deemed upper-bound, especially for Drishti-GS.	This paper focused on the UDA for Fundus Image Segmentation. The model majorly consists of three sections: inter-domain category regularization, source domain category regularization, and target domain category regularization. Experimental results showed that the proposed model outperforms several baseline methods.	This paper proposes an unsupervised domain adaptation framework based on category-level regularization to accurately segment the optic disc and cup from fundus images.	The global distribution matching via adversarial learning cannot align subtypes or subcategories efficiently, and thus the authors proposed category level regularization approaches to align categories. The authors proposed the novel formulations alongside the extensive experimental results including the ablation study.	The method section is well explained.	Different from previous global distribution alignment, category information is also considered in this work. It solves UDA from both intra-domain and inter-domain perspectives.	"A concept akin to the prototype-guided discriminative loss was proposed previously, including transferable prototypical networks and subtype aware UDA approach, e.g., [1] Pan et al. ""Transferrable prototypical networks for unsupervised domain adaptation"" CVPR, 2019 [2] Liu et al., ""Subtype-aware Unsupervised Domain Adaptation for Medical Diagnosis,"" AAAI, 2021 It is unclear how to achieve edge localization."	In section 2.1, what is segmentation model G? The segmentor? Eq (2) is just a simple Euclidean distance. Why not choose some domain adaptation distance functions, such as MMD and CORAL? In source domain category regularization, margin delta is the key parameter. It is better to show how to set the best value of 0.01. What are the effects of different values? There are too many loss functions in the training procedure. Without a detailed algorithm, it is difficult to know how to train the whole pipeline. In terms of the results, the paper did not compare with SOTA methods [1-2]. Some results are significantly better than the source-free model [1], in which no source domain labels are used. In RIM dataset, 0.905 VS 0.908 [2]. Also, the results of the Drishti dataset are exactly the same as the results as in [2]. These performances should be further explained, at least it is not better than [2]. There are many parameters in the models. It is better to add parameters analysis section to show how to get these optimal values. In the ablation study, there are many variants that are not compared. L_{inter} L_{dis} L_{aug} L_{inter} + L_{dis} L_{inter} + L_{aug} L_{dis}+ L_{aug} Without these results, it is difficult to know the effectiveness of different loss functions on the model. [1]. Chen, C., Liu, Q., Jin, Y., Dou, Q., & Heng, P. A. (2021, September). Source-Free Domain Adaptive Fundus Image Segmentation with Denoised Pseudo-Labeling. In International Conference on Medical Image Computing and Computer-Assisted Intervention (pp. 225-235). Springer, Cham. [2]. Lei, H., Liu, W., Xie, H., Zhao, B., Yue, G., & Lei, B. (2021). Unsupervised domain adaptation based image synthesis and feature alignment for joint optic disc and cup segmentation. IEEE Journal of Biomedical and Health Informatics.	Only a dice score is applied to evaluate the performance. Target domain category regularization is similar to strong and weak augmentations in semi-supervised learning.	The authors indicated sharing their code, which thus is highly reproducible.	Without a detailed algorithm, it is difficult to reproceduce the results in the paper.	Sufficient details for reproducibility.	While the method can be applied to datasets with >2 subcategories, the dataset used in this work has only two categories including object and background (binary). The authors presented only good results, and it would be good to provide the worst cases as well.	See weakness section	What is the dimension of prototypes and how do determine which layer to extract features for calculating prototypes. You need to specify these details. Edge discriminator is utilized in your work but without detailed illustration and ablation study. are all the results based on the single run? Since the test data size is small, the performance variance may be large. You would better apply cross-validation in your experiments.	Novel formulations for a category level regularization to deal with aligning subtypes or subcategories efficiently. The experiments are thorough, and the results are convincing.	The uncleared training process of the model and inadequate experiments	interesting idea but without detailed description and solid experiments.
545-Paper0928	Unsupervised Lesion-Aware Transfer Learning for Diabetic Retinopathy Grading in Ultra-Wide-Field Fundus Photography	The paper presents an unsupervised method for DR grading in ultra-widefield retinal images by utilizing additional narrow-field fundus photography. The paper consider an auxiliary task of lesion detection in narrow-field images because the DR grading is closely related to lesion	Ultra-Wide-Field Fundus Photography is an emerging novel imaging technique that can provide a broader field of view . This is useful for diabetic retinopathy disease screening and grading. However, due to the unavailability of a large public UWF dataset, it is hard to train an automated system with a UWF dataset. Here the author proposes a transfer learning-based method where originally, the well-labeled publicly available color fundus photography images were used to train the system. Subsequently, an unsupervised transfer learning based strategy was taken to assist the DR grading of  UWF images with the help of well-labeled color fundus photography image dataset.	This paper proposes an unsupervised framework (ULTRA) for the classification of diabetic retinopathy (DR) in ultra-widefield (UWF) images, by transfer learning from labeled (and more common) colour fundus photograph (CFP) images (including pixel annotations for CFP images). The CFP data is used to train an adversarial lesion generation model (segmentation model) for the UWF images, the feature maps of which are then combined by a lesion external attention module. Classification performance of ULTRA is claimed to be superior to supervised training with CFP labels, and a few other methods.	The proposed method does not require ground truth labeling of UWF images. Instead, it takes advantages of existing labeled dataset of narrow-field images. The ablation studies validate the effectiveness of the proposed modules.	The paper is well written. The paper performed a lot of experiments including relevant ablation studies. The results are better than the other methods.	Large number of CFP involved in training Usage of CFP pixel annotations, not just labels Attempts to generally adapt/transfer rarer (UWF) features into more common (CFP) features	The loss function for UWF lesion detection is quite weak. It is not very clear how accurate the lesion detections are (for UWF modality). The design of  LEAM is not fully justified.	The paper does not talk much about the benefit of using UWF over CFP. Is there any specific study that authors can cite showing the at broader field of view of peripheral retinal pathology of UWF resulted in better diagnosis or detection of specific biomarker of DR ? This can be cited along with the citation 3.	Most natural baseline (supervised UWF model with label) appears missing Performance improvement appears marginal (and dependant on metric); it is unclear whether the contribution of the method would diminish with more (labeled) UWF data (currently, only 904 UWF images available, of which about half are normal) No direct (if partial) evaluation of lesion segmentation for UWF data	The paper includes the implementation deatils for reproducibility.	The note on reproducibility seems fine but the authors used a private dataset where it will be hard to access.	The UWF dataset appears to be private. Exact details for the proposed adversarial lesion generation module and lesion external attention module (including hyperparameters) do not appear to be provided, only the general description.	The loss function for optimizing the multi-lesion generation task consists of the BCE loss between GT and predicted lesion mask and the adversarial loss to distinguish between source and target lesion. The supervision for UWF lesion detection is very weak (only the adversarial loss). The paper should include more analysis on the results of UWF lesion detection. In Fig.2, please explain the code coding. What do red/yellow/purple/green pixel mean? The design of the proposed LEAM is not fully justified. The attention map is obtained from features in lesion module. Why adding the attention in lesion features as oppose to features from grading module? How effective this module is compared with simple concatenation? Please provide analysis and discussion of the limitation of the proposed method (failure case).	Here are some comments along with strong and weak points mentioned before: It will be good if the authors can show some example images from different disease classes. The author resized the images 512 X 512. The UWF images were 3900x3072 which significantly higher. Can the authors comment about the information loss while downsizing these images. Can these affect the final outcome? The results in terms of numbers are not quite high. Is this a issue with UWF image in general? As the authors collected those from local hospital, is it possible to comment on the accuracy of diagnosis by clinicians with those images? In table 1, the precision of M(Ultra) is significantly higher than CycleGAN where as F1 score is higher in CycleGAN. This is a little confusing. Is this because of the Recall?	"The development of a framework for transferring knowledge from a more-common/better-annotated modality, to another (wider-coverage) modality, is desirable and likely has much application outside of ocular imaging. However, the manuscript in its current stage may be somewhat short on technical/implementation detail, and relevant comparisons. In the Methodology section, it is stated that ""...we trained a U-Net to mask out such artifacts [on UWF images]"". How was ground truth obtained for the UWF images, since it is also stated that the UWF images were provided with any pixel-level annotations (Section 2.2) The training of the models (as in Figure 1) might be clarified further. In particular, is the DR grading module trained jointly with the lesion generation module, or is the lesion generation module trained first and then frozen? It might be clarified as to how the source and target image inputs (as shown in Figure 1) are selected, for each pair of inputs. This is additionally since the CFP and UWF images should not be of the same patient/eye, being from different datasets. Then, particularly if the model is trained in an unsupervised manner with respect to UWF, is there any assurance that the images are compatible? Moreover, it is unclear as to whether the (arbitrary?) CFP image should be part of the input to the DR grading module (implied at Point C in Figure 1), if the objective is to obtain a DR class for the target UWF image. Does this imply that the DR class output for the UWF image might be different depending on which CFP image it is paired with? This might be clarified. For the lesion segmentation for CFP in the lesion generation module, it might be clarified whether different types of lesions (MA/HM/SE/HE) are annotated and classified separately. Moreover, the term for the relevant loss might be standardized (apparently L_Seg in Figure 1, L_CE in the text and Equation 1) The naming of the lesion generation module might be reconsidered, if its actual function is to provide pixel-level annotations of UWF images, and not to generate new lesions on the UWF images (as from Figure 2) Moreover, it might be considered to directly evaluate the performance of pixel-level annotations from UWF images, by transfer from CFP images; ground truth on a (small) subset of the UWF images would be sufficient. The most obvious baseline to compare against, would appear to be a supervised UWF model trained conventionally using the image-level DR labels, either with pretraining from ImageNet or CFP data. This does not appear to have been attempted, from the models/results in Table 1. It is unclear why increasing the number of available CFP images (from 8,000 to 15,000) would result in reduced performance (Table 2). It is also not clear how F1 and Kappa metrics would diverge so greatly, and what the interpretation might be (2,000 Images having F1 values of 66.53% but Kappa of only 33.00%, compared to say 8,000 Images having barely-higher F1 of 67.57%, but Kappa of 51.01%). Repeating the individual experiments to estimate the variance of the results might be appropriate. Implementation details (including training & hyperparameter search methodology) might be provided for the comparison methods. It might be briefly checked as to how frequently the trained model gets the domain (i.e. source CFP vs. target UWF) incorrect, if activated during testing. This should provide some context as to the extent to which the grading module is actually domain-independent. Some minor phrasing/spelling issues, e.g. (Abstract) ""newly imaging technique"" -> ""new imaging technique"" (Abstract) ""is practically challenge"" -> ""is practically challenging"" (Section 2.2, Section 3.1) ""This moudle consists of two parts/grading moudle"" -> ""module"" etc."	Overall the paper has merits and is of interest to the MICCAI community (unsupervised method/use of additional modality/under-explored modality).	The paper idea is relatively novel and also the paper is well-written. Other than some  comments and issues with the results, I believe that this can be a stepping stone for successful and further investigation of this novel imaging system.	The unsupervised transfer task is of great interest, but the manuscript could do with additional clarification, and experiments relating to the direct evaluation of UWF segmentation & against a supervised baseline.
546-Paper2097	Unsupervised Nuclei Segmentation using Spatial Organization Priors	The authors propose a novel unsupervised nuclei segmentation method for IHC images. It utilizes the consistency of nuclei distribution between HE and IHC images and imposes the spatial organization prior via generative adversarial learning. Experiments on three datasets demonstrate its effectiveness	This paper propose an unsupervised segmentation method for nuclei segmentation. The proposed approach train a Unet based segmentor in an adversarial training setting. The segmentor is trained to generate nuclei segmentation masks. The predicted masks and the real masks from some other nuclei dataset are used to train the discriminator. To avoid false negative segmentation, a reconstruction network is trained to reconstruct the input image based on the predicted segmentation mask. The proposed method is verified on three public datasets. The results seems promising. This paper conveys novel contribution and is well written. The reviewer has a little concern that how practical such system can be. Segmentation networks are usually used to extract the shape information of the nuclei. This requires the segmentation network to be accurate enough to capture the variance in the nuclei shapes. It is unclear if this segmentation method can achieve that level of accuracy.	This paper presents a novel unsupervised method for nuclei segmentation incorporating priors from public H&E datasets for use in IHC-stained images. The method exploits available information at the segmentation level by encoding and identifying the histological tissue characteristics that are independent of the staining.	The way to leverage spatial organization prior between HE and IHC images is interesting and shows good performance. The experimental details are clearly described. Ablation studies show the effectiveness of different components.	Unsupervised segmentation is a challenging problem. This paper present a working system for this task. The experiment is extensive and sufficient.	novel unsupervised method stain independence good results on 3 datasets in comparison with SOTA methods good results with nuclei and membrane staining images	"Missing the description of the HE database. Lack of comparison with other adversarial learning methods for nuclei segmentation[1][2]. [1] Liu, Dongnan, et al. ""Unsupervised instance segmentation in microscopy images via panoptic domain adaptation and task re-weighting."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2020. [2] Mahmood, Faisal, et al. ""Deep adversarial training for multi-organ nuclei segmentation in histopathology images."" IEEE transactions on medical imaging 39.11 (2019): 3257-3267."	The method proposed in this paper may be better termed as unsupervised nuclei detection. It may be hard to make a reliable segmentation method. Segmentation is usually used to extract the shape information of the cells or nuclei. If the segmentation is not accurate enough, it could extract misleading information for diagnosis. What are the training settings for the supervised and unsupervised in Table 1? Actually, the proposed method also makes use of the labeled data. What data and how many samples are used to train the supervised methods? What and how many labeled and unlabeled samples are used to train the proposed method? Is the proposed method using less labeled data than the Unet and Nuclick in Table 1? Precision and Recall may not be good metrics separately. Why not computing F1 score based on the precision and recall? How is the accuracy balanced? How is the object level segmentation achieved using Unet?	impact of postprocessing included in proposed method is not discussed lack of evaluation in relation to tissue with different compactness	The discription of training details is clear. The reproducibility will be good if the author add the details for HE database construction.	Some details of how the metrics are computed are not present. For example: How is the accuracy balanced? 2.How is the object level segmentation achieved using Unet? The proposed method uses some labeled masks to train the discriminator. How many such masks are used?	Loss functions and hyperparameters are present in the paper so the idea could be roughly reconstructed. But taking in consideration the amount of parameters and size of the model without actual code it is impossible to fully reproduce the results. There is no information about public release of the code so the reproducibility of the paper is limited.	In general, this paper is well written and easy to follow.  It utilizes the consistency of nuclei distribution between HE and IHC images and imposes the spatial organization prior via generative adversarial learning. The motivation is clear and experiments on 3 datasets show its effectiveness. Other questions: How to build the HE mask database? Cells in different organs may have large appearance variance. Will it affect the performance a lot? The main purpose of ICH images is to estimate the immunofluorescence correspondences. How can the method be generalized to multi-class nuclei segmentation? The consistency loss only considers the colour augmentation. Have you tried other types of perturbations such as the VAT? Will the processed Warwick HER2 dataset be published available?	The method proposed in this paper may be better termed as unsupervised nuclei detection. It may be hard to make a reliable segmentation method. Segmentation is usually used to extract the shape information of the cells or nuclei. If the segmentation is not accurate enough, it could extract misleading information for diagnosis. What are the training settings for the supervised and unsupervised in Table 1? Actually, the proposed method also makes use of the labeled data. What data and how many samples are used to train the supervised methods? What and how many labeled and unlabeled samples are used to train the proposed method? Is the proposed method using less labeled data than the Unet and Nuclick in Table 1? Precision and Recall may not be good metrics separately. Why not computing F1 score based on the precision and recall? How is the accuracy balanced? How is the object level segmentation achieved using Unet? It can be helpful to show some failure cases for the method to help understanding the limit of the proposed method.	The overall idea of the novel unsupervised method based on GANs is quite well described. I have but a few comments: the figure reference on p.3 should bo to fig. 1 instead of fig.2 I suggest a redesign of figure 1 to be more consistent with the in-text description (generator-segmentator) and to possibly include the other discriminator It would be also interesting to evaluate the influence of the included post-processing in the proposed metod. How much the evaluation metrics change without erosion and watershed.  Maybe Unet+watershed would be even better but since only proposed method has postprocessing it is unknown. Please provide further explanation/results in either main paper or supplementary material. Since this method is based on spatial organisation priors, it would be interesting to evaluate the consistency of the results in relation to tissue architecture compactness. (this is just a suggestion for future investigations)	The motivation of the method is clear. Results on 3 datasets demonstrates the effectiveness.	The paper bears some novelty. The experimental results has some gaps compared to supervised and semi-supervised method, but looks promising for unsupervised methods.	The idea is novel and well presented. The description is clear. The results are significantly better than reference SOTA methods, and based on 3 very different datasets. Overall, the presented novel unsupervised method for nuclei segmentation that can learn from H&E and then can be used for IHC-stained tissue images is very promising for the computational pathology.
547-Paper2242	Unsupervised Representation Learning of Cingulate Cortical Folding Patterns	Technically, the major contribution of this work is the introduction of topology-based augmentations in the SimCLR setting and adding a decoder to SimCLR and analyzing SimCLR and b-VAE reconstructions to recover folding patterns.	The author used unsupervised learning to learn latent representation and apply them in discovering folding patterns.	The paper develops an unsupervised learning framework based on b-VAE and SimCLR models, and affinity propagation clustering algorithm, to explore the folding patterns of cingulate cortex. The method is performed on HCP database with 550 subjects. The experiment results are interesting. Overall, the proposed method is reasonable, but the descriptions are not very clear.	This work tackles a challenging task: encoding cortical folding patterns and using the latent layer features to represent and cluster cortex in a unsupervised manner. Adding a decoder to the methods of SimCLR and b-VAE helps to recover folding patterns.	The topic is novel, important, and quiet significant in neuroscience. The paper is clear, easy  to follow.	The research topic of exploring cortical folding patterns of human brains is novel and of great importance.	Technical improvement is limited. Technical novelty can only be found in adding a decoder and proposing methods in the preprocessing steps. Interpretation of the results is relatively shallow. Only description of the morphology of clustered folding patterns and whether they are present in the reports are found. The choice of cluster number seems arbitrary. There are no clear conclusion by comparing SimCLR and b-VAE. Also, its application in clinical scenarios is not very clear.	The technique contribution is low. The authors seems to perform a contribution of two methods, in a special data form. This paper seems to be more like exploring folding patterns, rather than developing a new method.	Lack of validation experiments. The description is not very clear, especially for the comparison of patterns discovered by different models. Lack of a comprehensive comparison of cortical folding patterns discovered in this work and related papers.	The code was provided. Dataset used is publicly open.	Excellent, the author provided their code, which is a plus.	I believe that the obtained results can, in principle, be reproduced.	As an application, technical improvement to the existed approaches is acceptable. However, the authors may provide more solid conclusions on the findings, including the shape clusters and comparison across SimCLR and b-VAE. The authors may show the promise of this unsupervised clustering approach in some applications, such as abnormal shape detection.	The revealing of novel folding patterns is not so convincing, what will you find if you simply increase the cluster number? Also, what does these new folding patterns suggests? do they have any relationship with human cognition or disease? I feel it is not so complete by only exploring patterns.	In introduction, please cite papers for some sentences, e.g., 'Contrary to macaque...making it a fingerprint of each individual'. Papers about cortical folding features used as fingerprints for individual identification should be cited. The introduction section is not well-structured, its logic is not very clear, 8 paragraphs in this section are too much for MICCAI paper. In methods section, the hyperparameter bin 1st equation should be mentioned and described. In experiments and results section, please perform cross validation to validate that whether the models applied on different subsets of the dataset can obtain consistent cortical folding patterns, which helps to convince the readers that the result is reliable and reproducible. Please describe the relationship of the folding patterns discovered by two models in both fig. 2 and fig. 3. In my opinion, though the models are different, most of the latent folding patterns should be similar or consistent. But they are not very similar here, please explain why. Lack of a comprehensive comparison of cortical folding patterns discovered in this work and related papers.	The paper attempt to tackle a challenging task while the interpretation of results, conclusion and possible application are not well presented.	The topic is interesting. However, This paper is lack of completeness as I mentioned above.	The research topic is novel and of great importance, and the methods are reasonable.
548-Paper0547	Usable Region Estimate for Assessing Practical Usability of Medical Image Segmentation Models	The authors note that there is often a discordance between accuracy and usability in machine learning models. They propose two metrics to address this issue. First, Correctness-Confidence Rank Correlation measures the association between prediction correctness and confidence. Second, Usable Region Estimation measures the proportion of the population for which the model is clinically usable.	The paper propose a concrete and unified measure, called Usable Region Estimate (URE), to quantify both the correctness and reliability of the model, which is more clinically-usable and helpful for comparing and selecting models.	The paper proposes two new indeces : CCRC that measures the correlation between correctness scores and confidence estimates and URE that quantifies prediction correctness and reliability of confidence estimates. The authors tested the two indeces on 6 different clinical applications	The paper is clearly organized and easy to follow. The problem of model usability is a major concern in medical machine learning that is not adequately addressed in current literature. The idea of combining usability with accuracy in a single metric can potentially be highly useful in real implementation of machine learning models in healthcare.	(1) The paper is generally well-written and easy-to-follow; (2) The paper successfully identified the limitations of current model evaluation and selection pipeline, and propose a novel quantitative measure that is more concrete and practicable.	extended experiments on different  clinical  applications	The authors mention the main strength of using their metric over assessing accuracy and usability independently is that they are combined in a single metric. However, the combined metric is dependent on several tunable parameters (i.e., the URE metric depends on the choice of a threshold and choice of statistical metric). Since the ranking of models may depend on the choices of these parameters, it may not be clear which is actually superior for a task. The evaluation shows the relative performance of several models on different public datasets. This evaluation only shows that the ranking of these models changes based on the metric used, but does not demonstrate that the proposed metric's ranking is superior. An additional usability study with clinician feedback would be helpful here to show that the proposed metric is more aligned with how users evaluate model output.	(1) The advantage of CCRC over ECE is not clearly justified; (2) The limitations of URE is not identified and discussed;	No comparison with other model evaluation methods No justification on why these new index should perform better than existing ones	Datasets and models are all publicly available, so it should be easy to replicate.	Sufficient implementation details are provided.	No reproducible	In addition to the main weaknesses mentioned above, I have a couple of additional comments about the evaluation. First is that there are no error bounds or statistical tests showing significance of differences between the models. Second, it is unclear why different models were used in the sixth dataset evaluation. Third is that the bar graphs in figure 3 can be hard to read. I would suggest creating line graphs to better show the association between the values for different correctness requirements within each model.	(1) In Intro the paper claim that CCRC is more practical, as a high CCRC indicates better alignment between correctness and confidence; I believe this also applied to ECE. If so, what is the advantage of CCRC over ECE? (2) The computation of URE needs a pre-defined correctness constraint, this limits the flexibility of model evaluation; in fact, defining a proper correctness constraint may be hard in many applications. It would be good to see what the authors' thoughts are on this problem; (3) In Figure 3, we can see that a small change of correctness requirement can lead to dramatic change of UR, e.g. Fig.3-4, a change of 0.01 dice can lead to 0.15 UR change for U-Net. Does this mean URE is very sensitive to the correctness requirement, or in other words, no stable? Because changing the requirement by very little may lead to a different model recommendation; (4) How the conclusion (URE is stable on unseen samples) is drawn from Table 1? It's hard to judge whether a ~10% frequency violation is good or bad based on the current content.	The authors should justify why these two indeces should be used instead of the existing ones. They should add a comparison and show cases where the proposed  indeces are more representative of the model performance	The proposed metrics have potential and it is an important problem. However, the evaluation does not demonstrate the superiority of the proposed metrics over existing work. The results are also dependent upon parameter selection, which can make the results unstable and hard to interpret.	The paper propose a novel evaluation metric that quantify the model correctness and confidence in an unified way. The paper is generally well-written. Further discussion and justification on the limitation and experiment evaluation may help improve the paper.	It is not clear why these indices should give more information on the model performance than existing indices
549-Paper0871	USG-Net: Deep Learning-based Ultrasound Scanning-Guide for an Orthopedic Sonographer	This work proposes USG-Net, a method for identifying RCT in US images. A generator portion of the method generates a 3D portion prior to the classification module which helps guide the direction of the probe motion. In contrast to previous work, the proposed method does not rely on inertial measurement unit signals. While the authors don't specifically say this (which I think they should - if I'm correct) this could mean a simpler setup, or in other words use of existing clinical pipelines without additional equipment (i.e. added cost/complexity)	A deep learning-based scanning-guide algorithm is proposed to guide to the exact target diseased region without external motion-tracking sensors. A new scanningguide task that aims to search target disease regions using a corresponding network. They develop an automatic dataset construction method to train the deep learning-based scanningguide network.	In this paper, the authors extract feasible 2D US images from clinical 3D US volumes and use these to create a deep-learning approach for simultaneously detecting rotator cuff tears (RCT) in 2D US images and providing feedback to sonographers on US probe movements if RCT is not detected in the plane.	To attempt to remove the need for IMU signals simplifies the setup and allows use of this approach in existing clinical pipelines, with minimal changes.	The paper is well-writen following the structure of Dataset construction, USG-Net Architecture, Loss function. The contributions are clear: A deep learning-based scanning-guide algorithm that guides to the exact target diseased region. The Dataset is constructed and the automatic dataset construction method to train the deep learning-based scanningguide network is summarized.	Clinical Relevance: User dependence in the acquisition of 2D US images is a consistent and clinically relevant challenge. This also extends to sonographer training, which could be a potential application of this work. Experiments and Results Section: The results presented are quite thorough and clearly described, including an ablation study.	The main weakness of the work is lack of clarity in some areas. Some information needed for better understanding the method is given later in the paper, and at least one additional figure explaining some of the definitions (e.g. directions, tangential plane, etc) is needed. Also, there is no comparison in this work (with similar methods or clinical evaluation)	A formal mathematical formulation of the problem is recommended. The paper lacks the basic formulation, starting from the detailed dataset. More comparison with other methods is recommended, only the proposed method and its variants are compared. Training epoch in ablation studies can be more in the experiments parts.	Clarity: Some sections of the work are unclear, particularly in the Methodology where some variables have not been defined and hyperparameters are not stated. Novelty: As acknowledged in the literature review provided in the Introduction, the idea of providing US probe movement directions to sonographers is not novel and has been explored in other applications. Although this is acknowledged, the limitations of the existing work should be described further to justify the unmet need addressed by this contribution.	Dataset acquisition method, system specifications and dataset are all anonymous, hence, it is not possible to comment on reproducibility of the work.	The work is solid and easy to follow and reproduce. training and testing code will be released. Clear implementation details of hyperparamters in appendix. Clear dataset construction pipeline.	Although based on the reproducibility checklist, the authors demonstrate a commitment to publishing their code publicly, it is unclear whether the dataset will also me made available. The hyperparameters used appear to be missing in the manuscript.	"Abstract lacks quantitative information. E.g. results 2.1 pg 3: This section is quite confusing. What is the significance of the parallel lines? Assuming a cubic 3D volume, wouldn't any cross section contain parallel lines? And is there a specific plane/direction we look at the RCT (axial, coronal)? Or does it not matter. E.g. In fig 1 if we tilt the probe around the yellow line, we would get a pink parallel line which is a bit further down the bottom face. Would that still be an acceptable plane? As for distance, are the authors measuring the distance between a plane and points on a 3D RCT volume? I think re-writing this section to highlight why this is done, along with a figure showing how this distance is measured, can clarify things. When you talk about probe motion, would that be motion of the probe orthogonal to the surface only or could rotation/tilt of the probe (which could also eventually capture the RCT in the field of view) be a feasible solution? In LU, L, LD, etc what are the up/down/left/right directions? What direction is S? (from later in the text it appears to mean ""stop"" but it is not clear here) 2.2. What do the authors mean by thick and thin volume? 2.2. If D is depth, I'm assuming W and H are width and height (again, relative to what? Please show in a figure). What is ""p""? This is somehow shown in figure 2 but not clear from this section. In section 2.2. Do the authors mean to say that the direction of movement is calculated from the prediction of the location of the RCT, which is computed by predicting the 3D volume around a 2D slice? Do results depend on how close the starting probe position is to the RCT? Please provide information regarding the dataset (size, train/test split, how the data was collected, an relevant clinical information etc) Section 4: what is the significance of this method for dataset construction (e.g. why randomly slicing 3D US images lead to better results?) The authors state that since their  approach has never been studied in that the proposed model guides the probe to the target regions, they did not perform comparative studies. However, it is worth comparing the method to manual performance of experts. For example, how would using this method as a guide improve the performance of an expert compared to when they manually try to find the RCT. Would this improve the time needed? How would this help non-experts? The authors initially justify that this method would help non-experts. Would this improve the accuracy of them finding the RCT? By how much? Some clinical evaluation would strengthen the usefulness of this method. [The authors acknowledge this as a limitation, however ""some"" form of comparison is necessary in my opinion] I wouldn't consider limitation 2 a limitation, but a strength."	There is some points to improve: More comparioson with other methods in Table I, as only the proposed method and its variants are compared. More training epoch is recommended in the experiments. More formal mathematical problem formulation in the method chapter. An overall introduction of the experiments to provide general picture.	"As noted in the prior sections, the two major revisions are as follows: 1) Ensure that all variables used in the manuscript have been defined and provide hyperparameters where relevant. 2) Expand or modify the literature review provided in the Introduction to justify the need for the proposed method to overcome existing limitations. Minor revisions: Abstract: -Unclear what is meant by ""structural complexity"" -The tense used for the methodology is confusing. Please ensure that methods are written in past tense. Introduction: -Paragraph 2: I would be careful with the definition of CAD as it is currently stated (""...the deep learning network diagnoses diseases in the acquired images"". The deep-learning algorithms currently aid in the diagnosis but are not making independent diagnoses. -Paragraph 2: Good job acknowledging existing work in this area. I would like to see more justification of ""Despite the accurate guidance toward the standard scan plane by the proposed deep learning models, the unskilled sonographers have still difficulty in searching target disease regions"" to explain the need for the proposed approach Methodology: -The figures are very helpful for understand the approaches used. -2.1 Dataset Construction: In the last sentence, the meaning of a movement in the ""S"" direction is not defined until later in the experimental results. Please add it here. -2.2 Anatomical Representation Learning: Variables H, p, and W do not appear to be defined in the text. Experiments and Results: -Clearly described Discussion and Future Work: Limitations: For you second limitation, it sounds like this would be a straightforward transfer to another application but further work would need to be done for new applications to show the algorithm what to detect. Please clarify this in the text. Conclusions: -Unclear how the high evaluation performance demonstrates ""novelty"". Please rephrase."	Lack of clarity in many areas, lack of adequate comparison with other work or manual approaches.	Clear contributions in novel task, Well-written, but the problem formulation is lack and general design of experiments are missed. More methods should be compared, as only the proposed method and its variants are compared.	Overall, I think that the work presented will be of interest to the MICCAI community, providing a creative combination of deep-learning approaches to address a clinically relevant topic. The justification of the need for a new method and details of its implementation could be stronger.
550-Paper0962	Using Guided Self-Attention with Local Information for Polyp Segmentation	This paper proposes PPFormer for accurate polyp segmentation. It combines transformer and CNN to improve polyp segmentation accuracy.	The paper propose a novel transformer block, called L2G PPFormer block, to better model boundary information in Polyp segmentation. Experiments demonstrate that the proposed method advance the SOTA results.	"The authors present a set of attention techniques to perform polyp segmentation on colonoscopic images. The main specific contributions are: PPFormer, a neural network combining transformers and CNN's for global and local feature extraction respectively. PP-Guided self-attention, a technique used to guide the model to focus on regions that are difficult to classify. L2G (local to global), a mechanism designed to capture first local then global information in each transformer block. State-of-the-art results on multiple representative datasets. Note: ""PP"" refers to a dot product performed in the L2G blocks, P*P, where P is the flattened feature map from a level with smaller resolution."	The main strong point of this paper is combination of transformer and CNN/. For long capture of features, they utilize the transformer. The authors achieved very high scores in several datasets	(1) A novel self-attention block that encourages the model to focus on high-confidence region, which potentially improves the quality of attention; (2) A bottom-up two-stage self-attention strategy for better capturing local context. Both the two techniques are shown to improve the performance.	The method presented in the paper is well explained and the design decisions are well justified. It mixes modern techniques from other works that leverage both attention mechanisms and CNNs to design a novel architecture and blocks that achieve results that seem considerably improve the state of the art. The paper is well structured and complete, and the motivation and challenges are clearly described. The quantitative evaluation is thorough and suggests significant improvements over the state of the art for multiple relevant datasets.	Some rewriting process may be need to improve readability. Also, there is no explanation or presentation of polyp segmentation from the medical aspects. How about flat polyp detection rate? How about concave polyp? The paper is heavily depends on public dataset.	(1) Some details are missing in the paper, e.g. how does the transformer decoder work and how is the attention map in figure 1 generated? (2) Lack of comparison in terms of model size and FLOPs;	Limitations of the approach are not presented, nor are failure cases. Moreover, no statistical analyses have been performed, and the variance of results is also missing. This makes me skeptical of the reported results and the superiority of the presented methodology, although the differences in mean metrics indicate that this might not be a concern.	no problem	Some model details are missing. The authors do state in the reproducibility checklist that they will release the code.	For all models and algorithms, check if you include A clear declaration of what software framework and version you used. [Yes] No, the version is not mentioned. For all datasets used, check if you include: For all code related to this work that you have made available or will release if this work is accepted, check if you include: Specification of dependencies. [Yes]  Training code. [Yes]  Evaluation code. [Yes]  (Pre-)trained model(s). [Yes]  Dataset or link to the dataset needed to run the code. [Yes]  README file including a table of results accompanied by precise command to run to produce those results. [Yes] None of this is mentioned in the manuscript!! For all reported experimental results, check if you include: The range of hyper-parameters considered, method to select the best hyper-parameter configuration, and specification of all hyper-parameters used to generate results. [Yes] No; no; some. An analysis of situations in which the method failed. [Yes] Not found in the manuscript. A description of the memory footprint. [Yes] Not found in the manuscript. The average runtime for each result, or estimated energy cost. [Yes] Not found in the manuscript. An analysis of statistical significance of reported differences in performance between methods. [Yes] No statistical analyses have been performed! A description of results with central tendency (e.g. mean) & variation (e.g. error bars). [Yes] No variation of results is described! The details of train / validation / test splits. [Yes] Validation splits are not mentioned. The exact number of training and evaluation runs. [Yes] Not found in the manuscript. Information on sensitivity regarding parameter changes. [Yes] Not found in the manuscript.	In local-to-global approach, do you think more multi-scale layers will improve the segmentation accuracy?	(1) In terms of the PP-guided Self-attention:  a. how to properly determine the parameter alpha? Is the model performance sensitive to the choice of alpha? b. The vanilla attention matrix, i.e. M_{SA} in the paper, is normalized by softmax operation, but the proposed attention is not normalized; does this affect the model performance and training stability?  (2) In Table 1, some numbers are not consistent with prior works, e.g. TransFuse [24] achieves 0.942 mDice on ClinicDB in their paper, but the number in Table 1 is only 0.908. Why is the gap so large? Also, to ensure fair comparison with TransFuse and PraNet, please compare the model size and inference speed; (3) Where does the attention map from figure 1 comes from? The paper claim that the proposed method enhance model's perception of polyp boundary. It would be better if the paper can compute the similarity between the attention map and the groudtruth segmentation in the boundary region. (4) Most medical segmentation application are on 3D images. It would be good to see how the proposed model perform on 3D data, e.g. MRI, CT., and compare the results with nnUnet3D (https://github.com/MIC-DKFZ/nnUNet).	"I recommend adding measurements of dispersion in the quantitative results, and justify the choice of examples for qualitative analyses. Also, please check the language. Some examples that need reviewing are: ""six metrics, i.e. mean Dice, mean IoU and etc."" ""an architecture consists"" ""using prediction map"" ""decoder has two stage"" In the implementation details I see the authors employ a ""multi-scale strategy"", but it is not clear what this means. I would also like to know why vertical flip was not used."	This paper shows good example of transformer and CNN.	This paper propose a novel transformer block for polyp segmentation and advance the SOTA considerably. On the other hand, the paper lack analysis on model size and inference speed, which weaken the comparison results with prior models.	I found the paper was well written, all contributions were explicit and clear, and the motivation was also clearly presented. The structure is good, the figures are helpful. I would have liked to see a stronger effort on reproducibility, and statistical analysis of the reported metrics. I am not happy with many of the claims in the reproducibility checklist, which don't correspond to what can be found in the manuscript.
551-Paper1714	USPoint: Self-Supervised Interest Point Detection and Description for Ultrasound-Probe Motion Estimation during Fine-Adjustment Standard Fetal Plane Finding	This manuscript presents an end-to-end learning method to obtain the relative 3D pose between source and target ultrasound images. The proposed method first extracts interest points with descriptions from two images, finds the best matches with a graph neural network, and applies singular value decomposition to get the final relative 3D pose between the coordinate system where the IMU sensor is installed during data collection. The paper conducted experiments to demonstrate that their method has better feature matching performance than a hand-craft and a learning-based feature matching method. They also compared this proposed method with a regression-based pose estimation method for motion estimation and showed better performance.	In this work, the authors propose a method where they can estimate the relative probe motion from a pair of images, using self-supervised keypoint detection and attentional graph NN to match those keypoints between the source and target image pair. They apply this to fetal Ultrasound (US) images with the intention of being able to guide non-expert users to successfully acquire the desired US view-plane. Once probe motion is reliably estimated, relative to an ideally established probe position for that view-plane, instruction can potentially be given to remedy that deviation from the ideal position. In terms of results, they demonstrate that both their features, and the method, is superior to other methods they compare with.	A feature-based network is for motion estimation of US images is proposed.	The paper organization and writing are clear. The figures are also well made and helpful. I can well understand the proposed method. The end-to-end pipeline involving several stages is interesting and intuitively should be more generalizable to unseen cases compared with a regression-based pose estimation method. Additionally, the proposed pipeline is explainable, and developers could look into the different stages of the pipeline to figure out the source of errors if degraded performance were to be observed in practice.	The method proposed by the authors seems fairly new for US imaging. The direct regression methods (of motion parameters) perhaps don't work well for subtle motions as described here. Essentially the direct regression method is trying to equally optimize for all points in the image (including many noisy, non-informative points), whereas here, it'd learn to optimize only for a select key interesting points. So this sort of approach provides an alternative in these situations. The method doesn't require annotated keypoints/segmentations for interesting point generation, which is great. Visualizing the keypoints and their matches is a great way to interpret whether or not the method is working - so this is bonus explainability. The data used for training/testing is quite thorough.	Very good novelty in using graph network and SVD alongside the learned features. Good Novelty. Strong fondation compelling results	"The proposed method might not be able to be used in the actual fetus standard plane finding. For this method to work, the image of the standard plane of the scanned subject will need to be acquired in advance. This method is simply a pose estimation method between two ultrasound images and the models do not have a conception of what a standard plane image should look like in the learned weights. There seem to be some fundamental issues with the design of the method. In the motion estimation module, from my understanding, the authors want to first lift the matched 2D pixel pairs to 3D Euclidean space so that a closed-form point cloud registration method based on singular value decomposition can be used to obtain the 3D relative pose. However, the authors seem to use a learning-based network ""Transform Net"" to make the conversion from 2D pixel locations to 3D Euclidean locations. Wouldn't there be a non-learnable way to directly obtain the 3D location of the pixels with respect to the imaging source based on the imaging mechanism of ultrasound? In addition, what the authors present here is to estimate the motion of the point where IMU was installed during data collection instead of the actual ultrasound probe. Should there be a calibration process to bridge this gap? In addition, both problems above may suggest that, with the current design, the learned models may not be able to generalize across different types of ultrasound probes. The novelty is relatively limited. As the authors pointed out, the design of the local detector and descriptor and the pre-training method follow the design in citation 3. The graph matching method to find feature matching is based on the work named superglue. There are also methods discussing how to handle singular value decomposition differentiably. The main technical novelty seems to be combining these modules into an end-to-end pipeline for the task of relative pose estimation between two ultrasound images."	The main criticism is that this paper has too much content for a short conference paper - this leads to some parts being not very clearly explained. Although the keypoint detection part doesn't need any ground truth, you do ultimately use ground truth IMU or simulation based data, to indirectly train it right? Could you comment on how you would make it fully self-supervised? If you apply the transformation predicted on the source image, and calculate loss with the target image, and backpropagate that, would it work? It is clear that you're solving for subtle linear probe motion. What happens if there's a large motion and/or there are other compounding non-linear motion such as breathing, cardiac motion, etc? Won't SVD be an expensive operation to carry out each time? How long does training your algorithm take in the hardware you describe (which seems quite good)? How reliable are IMU signals? They are subject to some noise/drift issues too right?	The term self-supervised for this work is misleading, I think the method is unsupervised not self-supervised. Unsupervised is used when the network is trained by comparing the first image and the second image whereas, Self-supervised term in motion estimation is used when the network corrects by comparing with its own prediction in a teacher-student fashion. The loss function is poorly presented, the authors mentioned groundtruth in loss function equation while, as authors mentioned the training does not require any annotations. The authors violated the rule that there shouldent be any text in supplimentary materials.	The dataset will be not publicly available but the source code is promised to be available.	I didn't find reference to code/data etc. I suspect the code may be easily available later but not sure about the data since it seems more custom generated. The components of the method should be generally easy to replicate and test.	The details about training, hyper-parameters are not provided	"In equations (3) and (4), shouldn't there be no subscript ""j"" for the symbol ""M""? ""j"" is only used inside the summation. Consider using the special symbol instead of the italic letter ""T"" for matrix transpose for all your equations. Need more information on the motion estimation module. How do you use the Transform Net? Does it take all 2D point pairs and output a single 3x3 matrix ""T""? I find the process of predicting a 3x3 matrix to lift 2D pixels and then use a closed-form 3d point cloud registration solution based on SVD problematic. Can the authors explain more about why they choose this process for motion estimation? End of page 7, ""arccos"" and ""GT"" should not be italic. These are texts instead of symbols. It is important to know the performance of the local detector and descriptor coming out of the first two pre-training phases compared to the one out of the final phase of the proposed end-to-end pipeline. This is because the first two phases, mentioned in the supplementary material, come from other works and the authors need to show that their proposed end-to-end pipeline used in the final phase of the training process further boosts the feature matching performance. The deep regression method authors compared seems to be published in 2018. Are there no more recent ones with better performance that authors can compare their method against?"	"Page 5, attentional graph matching section -> My sense is that 'intra' and 'inter' are standard terms as opposed to 'intra' and 'extra' ? Not a big issue though. About the sinkhorn algorithm - how is it executed during training time? Is it a differentiable operation like the SVD? Or is just a callback or something? Could you explain how this works? You write U, V \in SO(3). SO(3) isn't described - may be beneficial to some readers to define it. The qauternion is also not defined. Your method also seems to suffer from the standard drift issue (larger error for frames that are farther away from the reference frame) right? Do you have any comments/ideas on how to improve that? There's some language issues here and there affecting clarity. Some obvious ones I caught: page 1, motivation: ""locate the probe position"" -> However, locating the probe position. page 2, ""the main limitation is naturally for lack of generalization ability"" -> the main limitation is naturally the lack of generalizability. page 3: ""as Fig. 1 shown"" -> As Fig 1. shows"	The title is very lengthy, please use a shorter title. Page 2, typo error method not methed. I think there is miss-undrestanding for self-supervised training.  The author should cite recent unsupervised motion estimation in US imaging including :   Tehrani AK, Sharifzadeh M, Boctor E, Rivaz H. Bi-Directional Semi-Supervised Training of Convolutional Neural Networks for Ultrasound Elastography Displacement Estimation. IEEE Transactions on Ultrasonics, Ferroelectrics, and Frequency Control. 2022 Jan 27. Delaunay R, Hu Y, Vercauteren T. An unsupervised learning approach to ultrasound strain elastography with spatio-temporal consistency. Physics in Medicine & Biology. 2021 Sep 3;66(17):175031. and semi-supervised methods: KZ Tehrani A, Mirzaei M, Rivaz H. Semi-supervised training of optical flow convolutional neural networks in ultrasound elastography. InInternational Conference on Medical Image Computing and Computer-Assisted Intervention 2020 Oct 4 (pp. 504-513). Springer, Cham.	The major factors is the seemingly limited practical value of the proposed method in the application of standard plane finding. Also the proposed method seems to have some fundamental design issues that should be addressed by the authors. The novelty is slightly limited also considering all of the individual modules exist in the previous works.	I think the work is quite good although the organization is a bit haphazard.	There is a good contribution in the paper but a few modification should be made including loss function, the self-supervised term.
552-Paper1036	Vector Quantisation for Robust Segmentation	The paper provides a novel method to make a standard U-Net segmentation model more robust by incorporating a discrete bottleneck using Vector Quantization. The paper claims to perform well within a certain degree of perturbation and domain shift under a certain set of assumptions and also provides a simple theoretical justification for it.	This paper proposes a quantisation block that learns a discrete representation in the embedding space in the bottleneck of UNet architecture to improve the robustness under domain shift and noise perturbations in segmentation models.	This manuscript describes an application of quantized low-dimensional space in medical image segmentation to achieve more robust segmentations.	Vector Quantization has been widely used in Speech Disentanglement and Image Generation. The paper provides a novel formulation of incorporating a discrete bottleneck (VQ) in a segmentation framework for increasing the robustness(noise perturbation of the input as well as domain shift in the datasets) of the trained model. Thorough experiments have been performed to justify the claim in both binary and multi-class settings. Perturbation study of the latent space corroborates the robustness claim. Provides a simple bound on the amount of perturbation needed to change the output of the quantization block.	This paper introduces vector quantisation, a method proposed in the generative model , i.e., VQ-VAE, into the U-Net architecture to improve the robustness of the segmentation model. The author shows that vector quantisation of latent features could effectively solve the domain shift and noise perturbations in the input space.	Vector quantization of the low dimensional space has been shown to be promising in many areas and I think it is not too popular in medical image segmentation. Strong evaluation: various datasets, two metrics, and two ways to measure the robustness against variance: 1) via adding noise (Section 3.3) and 2) by testing on data from a different domain (Section 3.2).	While the paper is successful in justifying the robustness claim, it is still not clear why there is an increase in performance from the baseline U-Net on a single domain? The paper says in the CONCLUSION section that a potential reason is codebook behaving as an atlas in the latent space for segmentation of anatomical parts(which are very structured), but unfortunately no experiments/visualization has been performed to claim this. The paper also says that because anatomy is structured, the quantization bottleneck helps in capturing this structure by limiting the latent space through discretization. The reasoning doesn't seem to be plausible, with no visualization of the codebook to support it. Working only on basic U-Net architecture doesn't prove its applicability on other sophisticated segmentation architectures. No results on datasets where the segmentation region is small which limits the evaluation of the technique. For instance, I would like to see some results on the ACDC dataset (Automated Cardiac Disease Diagnosis) where the number of pixels of interest are less as compared to background. The method particularly use Group Normalization and Swish Activation which is different from basic traditional U-Net architecture. I would like to know if the success of the method is contingent on using Group normalization and Swish Activation?	If the paper aims to claim that vector quantisation could be effective in the segmentation task universally, the author should validate this on a more general platform, such as nnUNet. Additionally, the author should try at least one more architecture except for U-Net to approve the generalization. As VQ-VAE has claimed, the discrete representations fit for discrete latent variables, like planning and language tasks. However, the author claims that the VQ is also an The author claims that the network could be robust by minimizing Ph(x+e)-Ph(x), this assumption could be true for the noise perturbations. However, for domain shift, the problem should become Ph(f(x))-Ph(x) instead of simply adding a small value of e. The function f() can range from a renormalization function to a non linear mapping, like in the paper shown in [1] [2]. There are many other works for solving the domain shift problem in medical images, such as [3][4]. While the author only compares VQ-Net against U-Net. So what is the advantage of VQ against domain adaptation methods? In the task of the domain shift experiment for lung segmentation, as shown in Figure 1, I didn't see there is an obvious difference between NIH and JRST. And in table 2, the segmentation performance improves from JRST to NIH encounting domain shift. Therefore this experiment is not convincing enough to prove the effectiveness of VQ for domain shift. Reference: [1] Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation [2] Synergistic Image and Feature Adaptation: Towards Cross-Modality Domain Adaptation for Medical Image Segmentation [3] The domain shift problem of medical image segmentation and vendor-adaptation by Unet-GAN [4] A Closer Look at Domain Shift for Deep Learning in Histopathology	Use of UNet as a baseline (from 2015) instead of a more updated approach, such as nnUNet or TransUNet. I found Section 3.1 a bit confusing.	Reproducible.	If the paper aims to claim that vector quantisation could be effective in the segmentation task universally, the author should validate this on a more general platform, such as nnUNet. Additionally, the author should try at least one more architecture except for U-Net to approve the generalization. Additionally, the author doesn't give enough provement to demonstrate the effectiveness of VQ in domain shift. Please see the below comments.	The reproducibility statements look reasonable. Authors mentioned in both the reproducibility statement and the paper that the code will be released.	"It would strengthen the paper's claim of capturing anatomical structure if we can see some visualization of the codebook vectors and how it relates to the increased performance over baseline in single domain. Results on ACDC Dataset will strengthen the paper. Some ablation studies are missing. E.g., an ablation study on number of vectors in the codebook. Does increasing the number of vectors leads to ""more complete"" codebook and hence improve performance in the domain shift experiments? Please refer the points in the ""weakness"" section."	"The author claims that the network could be robust by minimizing Ph(x+e)-Ph(x), this assumption could be true for the noise perturbations. However, for domain shift, the problem should become Ph(f(x))-Ph(x) instead of simply adding a small value of e. The function f() can range from a renormalization function to a non linear mapping, like in the paper shown in [1] [2]. Therefore, it is hard to believe that the VQ could solve the domain shift problem in general, as the domain shift could be brought by different reasons, like the setting of equipment, or the variance of imaging conditions. So I would suggest the author mainly validate the work on their second task to approve the robustness under noise or degradation. There are many other works for solving the domain shift problem in medical images, such as [3][4]. While the author only compares VQ-Net against U-Net. So what is the advantage of VQ against domain adaptation methods? Are there any further experiments to approve this? In the task of the domain shift experiment for lung segmentation, as shown in Figure 1, I didn't see there is an obvious difference between NIH and JRST. And in table 2, the segmentation performance improves from JRST to NIH encounting domain shift. Therefore this experiment is not convincing enough to prove the effectiveness of VQ for domain shift. Minor Comments: Maybe the name VQ-Unet is better than VQ-Net in this paper, since the network develops from U-Net. This follows the same naming rule from VAE to VQ-VAE. For Table 2, it would be better if the author could reorganize the data. For example, they can represent the change of dice score and HD95 in seperate tables. Therefore, it would be easier for the reader to compare the difference brought by the domain shift. Reference: [1] Zakazov, Ivan, et al. ""Anatomy of Domain Shift Impact on U-Net Layers in MRI Segmentation."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2021. [2] Chen, Cheng, et al. ""Synergistic image and feature adaptation: Towards cross-modality domain adaptation for medical image segmentation."" Proceedings of the AAAI conference on artificial intelligence. Vol. 33. No. 01. 2019. [3] Yan, Wenjun, et al. ""The domain shift problem of medical image segmentation and vendor-adaptation by Unet-GAN."" International Conference on Medical Image Computing and Computer-Assisted Intervention. Springer, Cham, 2019. [4] Stacke, Karin, et al. ""A closer look at domain shift for deep learning in histopathology."" arXiv preprint arXiv:1909.11575(2019)."	"I think that the manuscript would benefit in terms of clarity if ""sg"" from Eq. 2 would have a more detailed explanation, as in [19] (right below their Eq. 3). Related to this, above Eq. 2 we can read that the gradients from the encoder need to be copied to the decoder. At first, I thought that it was to ignore the codebook from \Phi_q, but I think that this codebook gets updated by backpropagation as well right? (Eq. 2, third term). I found Sections 2.3 and 3.1 a bit difficult to read/understand, in my opinion. First, in Section 2.3, ""r"" is defined (Eqs. 3-4) to later discover in 3.1 that ""r is obsolete"". Typo above Assumption 3: ""and thereby enforce"" -> ""and thereby enforces"""	In the case of a single domain, the increase in performance is not very clearly explained, which casts doubt on the technique. I am open to increase my rating if my questions and doubts are answered.	The author claims that by introduing VQ can effectively solve the problem of domain shift in  medical images. However, either the theorical assumptions or experiments could prove this claim.	This work shows that the quantized low-dimensional space in the encoder of autoencoder-like networks is beneficial to gain robustness. Authors show this via trustworthy experiments.
553-Paper2513	Video-based Surgical Skills Assessment using Long term Tool Tracking	The paper proposed a method to assess the surgeon's skills in minimally invasive surgery. The proposed method is video-based and has two main steps. The first one is creating motion trajectories of the surgical instruments by tracking the their motion in the video. The second step is using these trajectories to classify the skill into two classes; good and poor performance. The proposed method was tested on real surgery videos from the Cholec80 data set.	This paper provides some incremental contributions on algorithms for video-based tracking of surgical procedures.	The manuscript proposes an automated way of assessing a surgeon's skills based on the tool tracking method employed on the video feed. The idea is to track the instruments over a longer time span and use the trajectories to predict skill levels. The proposed tracking model uses a transformer architecture and is evaluated on the Cholec80 dataset with skill ratings provided on Calot Triangle Dissection. The manuscript further compares the transformer method with traditional ML methods like a random forest with the goal of reducing the number of identity switches. The manuscript provides results on both tracking and skills assessment where it outperforms a state-of-the-art tracking method.	The paper is generally well written. The fact that the proposed method only needs the surgery videos makes it applicable in a wide range of surgical settings. Testing the proposed methods on real surgery (not dry lab exercises for example) is another strength of this paper.	As far as my knowledge, this paper proposes new ways to track and assess surgical tools in a video sequence. At least in the field of surgical tracking and surgical workload the proposed algorihtms are innovative and provide good results (vs. bytetrack that is not specifically designed for this purpose).	The abstract is clear and well written. The motivation for the work is clear and matches the goal provided in the abstract. The requirement of automated skills assessment and what those typical skills consist of are clearly mentioned. The novelty lies in the new tracking algorithm for generating a cost function and track recovery method based on the Hungarian algorithm. There is a potential of extending the public Cholec80 dataset with spatial labels if the used annotations are made public.	It seems unfair to compare the proposed method with the GOALS assessment tool (or at least arguing that it can replace GOALS). The reason is that in GOALS one gets continuous value, representing the assessors' feedback, but the proposed method only considers two classes. In this context, from a trainee's prescriptive, GOALS gives more useful information compared with the proposed method.	"From my point of view, the main drawback of this paper is the lack of comparison of its results with other previous works that could be highly related with this topic. The authors used bytetrack such as baseline for performance comparison but I will suggest to include some others: doi: 10.1007/s10916-020-1525-9 doi: 10.1109/WACV.2018.00081 doi: 10.1007/s11548-016-1388-1 Additionally, the main contributions (cost function and track recovery) have not been discussed or compared with other approaches of the previous studies. Finally, I cannot clearly see the novel contribution stated by authors ""classify skill directly from the tool tracking"" in the discussion section. Maybe, I missed something but for me 'classification"" implies to group these skills in any way (novice, intermediate, experts... or something similar). As far as I read the paper I cannot directly understand that the algorithm can provide these groups."	The evaluation dataset is very small and limits the reliability of the obtained results. The rationales for architectural choices are omitted. Ablation studies is needed to justify lots of modeling in this paper.	I think that it would be better if the authors share more details on how they evaluated their proposed method to make it easier to reproduce their results.	Any additional comment on this.	The manuscript mentions the use of data augmentation strategies but did not provide details on different augmentation methods that may have been used. -The basic hyperparameters are not presented. The experiments performed should accompany the details of the training but there are no details found in the manuscript. The manuscript mentions Bayesian hyper-parameter search was used for learning-based models but did not include initial or any parameter details on which the search was carried out. This is missing from the experimental setup section.	"1- On using methods in refs 15 and 24 for longer videos: Would not the decomposition of longer videos into shorter ones make it possible to use these methods for long videos as well? This is instead of creating a brand new method to deal with this problem. How sensitive is the proposed method to the authors' choice of the 3.5 threshold? (in the Dataset Description section) It seems unfair to compare the proposed method with the GOALS assessment tool. The reason is that in GOALS one gets continuous value, representing the assessors' feedback, but the proposed method only considers two classes. In this context, from a trainee's prescriptive, GOALS gives more useful information compared with the proposed method. In the discussion section, the authors wrote: ""This indicates the classification created by the model is comparable to human level performance"" I am not sure if the ultimate goal should be having models comparable to human level performance because humans in the surgical skill assessment task can vary a lot and we do not want to reproduce this variability at all."	As said before, I will suggest to compare your proposal with other convolutional networks that were specifically developed for surgical tool tracking. There are some of them in the bibliography and at least in a qualitative way should be included in the discussion section. The real contribution of the proposed algorithms could be fairly compared after this analysis because an important part of the state of the art is currently missed.	"Title matching content: Yes, the manuscript describes a novel tool tracking method for skills assessment and thus matches with the title description. Abstract summarizing content: The abstract mentions the problem statement clearly along with the problems associated with prevalent methods for skills assessment. The abstract mentions the use of the self-attention transformer network but did not justify the rationale behind their choice. It would be nice to mention the percentage improvement obtained by the proposed model for skills assessment. Knowledge advancement: The method proposed a new cost function and uses an existing re-identification network with a transformer to perform tool tracking. However, the manuscript does not provide details on how it compares with other standard temporal models such as LSTM, TCN, etc., hence failing to provide a holistic picture of the tracking method. The method is evaluated on a small subset of the dissection phase amongst the entirety of other phases present in Cholec80 for skills assessment. This limits the novelty only to the specific Calot Triangle Dissection phase. Positioning with existing literature: Bulk of reviewed works on object tracking are outside the scope of medical domains. However, there are lots of published works on tool tracking in surgical domains such as [1-3]. The manuscript needs to be properly positioned. Related references are covered for the tool trajectory-metrics, transformer, and datasets are provided. References [1] Nwoye, Chinedu Innocent, et al. ""Weakly supervised convolutional LSTM approach for tool tracking in laparoscopic videos."" International journal of computer assisted radiology and surgery 14.6 (2019): 1059-1067. [2] Robu, Maria, et al. ""Towards real-time multiple surgical tool tracking."" Computer Methods in Biomechanics and Biomedical Engineering: Imaging & Visualization 9.3 (2021): 279-285. [3] Zhang, Lin, et al. ""Real-time surgical tool tracking and pose estimation using a hybrid cylindrical marker."" International journal of computer assisted radiology and surgery 12.6 (2017): 921-930. etc. Method description and rationales: The method is divided into three modules with sufficient details and provides enough purpose for all the modules. The three modules correspond to Tracking algorithm, feature-based and learning-based skills assessment. The method uses yolov5 for detecting tools in a scene for which yolov5 was evaluated on the ""last 5 videos of the dataset"". This part is confusing as it does not clearly mention which dataset is used for evaluation - is it the Cholec80 or the 15 videos annotated with a bounding box (section 2.1). It would be nice to clarify the dataset used for evaluation. It would be nice to justify the Kalman filter method used for tool tracking and what formulation for the Kalman filter is used for the same. This needs to be explained better as tracked locations are further used for the construction of a cost matrix, a novelty the manuscript claimed.  The description provided for the cost function under ""Cost Function Definition"" is not in proper order. The manuscript mentions the third term in equation 1 as the second term in the description (pg 5). Similarly, the second term in equation 1 is mentioned as the last term. These structural mistakes should be avoided to prevent confusion for readers. Even though the transformer works well in time series data, there are other temporal models like LSTM, TCN, etc. which work well, but the manuscript did not provide enough justification for choosing the transformer model beyond the standard ""best performing model"" claim. This leaves the readers to contemplate the performance of other temporal models with respect to the transformer. The manuscript should clarify more on this as a simple ablation with other temporal models would have been sufficient. The manuscript did not present a description of the transformer configuration or parameters.  The traditional machine learning model used for feature-based skill assessment is the random forest model, however, the manuscript did not provide any rationale behind it. What happens if other models such as ""xgboost"" or tree-based models are used? The manuscript should provide ablations justifying the use of the random forest model. Standalone figures and tables: The problem statement in Fig 1 is clear and summarizes the goal of the paper. The caption for Fig 1 should add short details of what the x,y,d, and w variable represents. Also, the intermediate cube mentioned with d and w/2 parameters should provide details on what it represents. However, the acronyms provided in Table 2 such as IDs, MOTA, FP, and FN, and their full names are missing from the paper. Data contribution/usage: There is no dataset contribution, However, the method is implemented/evaluated on the dissection phase of the Cholec80 dataset. The manuscript did not mention train/val/test splits used for training and evaluation of the model. The CholecT50 triplet dataset is annotated with bounding boxes (133k) for surgical instruments for detection during tracking. Results presentation: The results in the tables are clear but the best results must be highlighted in bold for Table 2. The results are not specified with mean and std across different runs which would have made it easy to comprehend the stability of the experiments. Discussion of results and method justification: The skill efficiency is separated into two classes - low and high based on the 3.5 threshold value. Is there a justification for choosing 3.5 for the threshold? The author should provide more details on this.  The manuscript points to the less number of Identity switches with the proposed method but it is important to know how the ID switch is computed; the detail which is missing in the paper. To avoid the class imbalance problem, the manuscript mentions the use of random oversampling but did not provide details on the number of samples for the two skill classes - low and high.However, the readers would not have an idea about the number of samples used. Comparative analysis with existing works: The results were compared with the baseline ByteTrack and Random Forest models as there is no existing work dedicated to GOALS-based skill assessment on Calot Triangle Dissection. Limitations are clearly specified and provided with reasoning. Conclusion  The manuscript presents insights for research continuation for extending skill assessment to other phases of the Cholec80. Arguable claims The manuscript claims the transformer to be the best performing model of tool tracking and yet did not provide ablations on other temporal models to position the importance of the transformer model. It raises serious questions on the nature of the experiments performed. Manuscript writing and typographical corrections Reference to Hungarian Algorithm paper is missing. (pg: 4 first row) [Aglorithm] in Algorithm 1: [Algorithm] [hungrian] in Algorithm 1: [Hungarian] [inacive] in Algorithm 1: [inactive] [cos t] in Track Recovery subsection: [cost] Acuuracy in Table 3: [Accuracy]"	It think the work in this paper is interesting. However, the points I outlined in my review above need to be addressed properly for the paper to be really useful for the readers in my view.	Lack of comparison with previous contributions in video-based tracking of surgical tools.	The contribution is novel and relevant to the community. There is a reported data annotation effort. However, comparison with the right baselines is missing.
554-Paper2365	Vision-Language Contrastive Learning Approach to Robust Automatic Placenta Analysis Using Photographic Images	The author improved current ConVIRT with NegLogCosh (Negative Logarithmic Hyperbolic Cosine ) and sub-feature comparison to address the feature suppression problem. Experiments verified the generalizability and robustness of their method on their placenta dataset.	The main contributions are two-fold: 1. introduces a vision-language contrastive (VLC) framework for pretraining for placenta analysis. 2. introduces a new loss function for training the VLC model and the proposed loss function seems to give better results.	The authors present a vision-language contrastive learning approach to classify placenta images. The method pretrain a generic image encoder using pathology reports and images. The contrastive-learning loss uses a negative logarithmic hyperbolic cosine similarity to avoid shortcut solutions in the model. The pre-training dataset consisted of 10K images and the fine tuning dataset of 2.8K images. The results show that the model outperforms the visual only baseline using resnet and the multimodal conVIRT models.	The authors introduce contrastive learning to reduce the data scarcity on placenta images. The authors build up the placenta datasets with different modalities, which will benefit the community in this research area. The authors address the feature suppression problem in VLC approaches to improve generalizability and robustness.	I think the VLC framework used here is reasonably novel, particularly as it is applied to placenta analysis. The new loss function introduced is also novel, particularly in the context of the VLC task The results are strong - and the new loss function seems to give better results.	The paper is easy to follow. The state-of-the-art in the problem is well-mentionated and the proposed method is well justified. The idea of stabilizing feature importance using the hyperbolic cosine is interesting and it deserves further exploration in various multimodal clinical scenarios. The evaluation in placental pathologies highlights the model's usefulness.	The comparied baselines is not enough and strong, i.e., the resnet50. No experiments on public datasets.  It would weaken the confidence in their improvements on the tasks of placenta image analysis.	The new loss function is not adequately motivated. For example, why not simply use something like x instead of log(cosh(x))?  There were no statistical tests performed to ascertain if the proposed model is significantly better than others. Also no confidence intervals are presented. The writing could be improved. There are a lot of notational inconsistencies as well as logical gaps (see detailed comments below)	The results table only reports one absolute result for each method, and not an average over several runs, or a k-fold training. Making it difficult to evaluate the robustness of the method to initialization/ augmentations used during training. Details and discussion of qualitative prediction results are lacking. There is a baseline missing: Clinical text reports only. It would have been interesting to show what are the more important areas for the classification using a saliency method such as Grad-cam (despite the well-known issues of it).	The reproducibility is challenging without the authors' dataset and training hyperparameters. The authors are suggested to open their datasets and algorithm that will positively impact placenta image analysis.	likely reproducible	The authors mention that the dataset comes from a large urban academic hospital but not mention it's availability, possibly the dataset is private and therefore it will be difficult to reproduce the results using the same data. The authors do not mention availability of their source code making it difficult to even do the same evaluation in other datasets. In the supplementary material there is extra information of the training hyperparameters, this might help in the reimplementation of their method.	Please use a table to improve the description clarity of the negative sample and positive sample in the dataset Section. Please optimize the writing and figures.	"The notation is not defined. For example, in equation 1, what is t_{-1}? It is also not consistent - in equation 1, x_i is a vector without bold fonts, but later in equation 3, bold fonts are used for vectors and in equation 4, normal font v_i is a scaler. Please ensure consistency in notation. There are many statements that are not clear to me. Please rephrase these: a. ""NegLogCosh have less emphasis when vi and ui are very different thus reducing the effect of dominant feature from either the text side or the image side."" - why is that? log(cosh(x)) looks like |x| when |x| is large. Not sure what you mean here. b. ""Because the similarity metric (5) compares two feature vectors element-wise, the result of the comparison should not change much if some features are missing."" - why is that? Equation (5) ultimately takes the mean, which can is easily affected by outliers. there are several places the equations/logic is incorrect/has typographical errors: a. in equation 9, what is c? and how do you get (9) from (5)? b. How does equation (10) imply (11)? It only holds if equation 10 is true for every possible realization of (i_1, i_2,...i_k) (all 2^k of them!) Instead of heuristically assuming that 1 or 2 percentage point difference is significant, it may be better to perform statistical tests to verify significance.  Instead of a computationally expensive new loss function log(cosh(x)), what would happen with a simpler loss function x ?"	"+In the right side of Figure 2: ""Stop gradient"" is not clear what it refers to, is it not just that the output for the non-relevant tasks is not used (equation 2?), or you used a multi-task objective and stopped the gradient each time for the tasks that were not the relevant at a given optimization step? This should be explained better in the text . In what images the multimodal model was better than the vision only? Can you devise a method to assign word importance to image regions?"	The authors provide a pilot work on multi-modalities (image and medical reports) contrastive learning for placenta image analysis.	i think the method is interesting, particularly with joint image-language contrastive learning. They also introduces a new loss function which seems to do better than the standard loss function on their dataset.	The method is interesting but the evaluation and results interesting could have been optimized (more runs, qualitative results, better discussion).
555-Paper2116	Visual deep learning-based explanation for neuritic plaques segmentation in Alzheimer's Disease using weakly annotated whole slide histopathological images	The authors propose a deep-learning based method for semantic segmentation of taupathies for AD patients. They present a baseline to generate a significant advantage for morphological analysis.	The authors of this paper describe neuritic plaque segmentation using 8 brain whole slide images. The authors try different patch size, different scanners, different stain normalization, and different models (UNet and attention UNet) to segment plaques and improve human annotations.	In this paper, a baseline for semantic segmentation of neuritic plaques in human WSI is proposed. Also, the release of a new expert annotated database as well as the code will be useful for the scientific community to accelerate the development of new pipelines for human WSI processing in AD.	The authors propose a deep learning framework for plaque segmentation in brain WSIs. The authors explore the interpretability and explainability of the deep features. Integrates domain knowledge in the segmentation method. The observations and the dataset provided can be helpful to improve the AD study wrt plaques. The proposed method shows interesting application for quantitative estimation of plaques.	This paper uses deep learning to analyze AD tauopathy which has been rarely explored.	A baseline for neuritic plaques is established; The expert annotated database is released as well as the code.	No major contribution in the method of the study but the application is interesting. The dataset size is small (8 whole slide images). The dataset is not publicly available so the merit of the method cannot be established and compared to any state of the art method. Quantitative analysis of different factors  like different modalities, context effect and attention maps are missing. A table showing the contribution of each of the factors will be helpful.	I am concerned for the technical novelty because the proposed method uses attention UNet for segmentation. Perhaps emphasizing more on why this problem is important and why others have not done this would highlight this work.	I could not see any technical novelty in terms of the approaches and methods used even though the methods used are solid and well accepted within the community; A bit more of explainability exploration could have helped to make the paper stronger, what exactly the model learned.	The training/testing code and the dataset for the study will be provided. This will help in reproducing the results and use the method as a baseline.	Code is provided as supplementary material.	good assuming the code and the data will be shared.	The quantitative estimation using the trivial software can be included to compare the results. More dataset could be included. Deep learning methods on limited dataset could be over fitting. Clinical value of the techniques is not clearly discussed.	Emphasizing more on why this problem is important and why others have not done this would highlight this work. Including results from UNet architecture (Section 3.1) for scanners and normalization in Table 1 would be helpful. According to Table 2, attention UNet provides better results than UNet. In addition, Figures 3-5 show results of attention UNet. Then what is the message from UNet? From Figure 5, the authors claim that attention map can improve human annotation. I think the changes in delineation is minor - what do the authors expect to see clinical improvements using this attention map for plaque segmentation? There are some typos to be fixed: and -> an (page 4), tunning -> tuning (page 5), diffult -> difficult (page 8), assitive -> assistive (page 8)	I could not see any technical novelty in terms of the approaches and methods used even though the methods used are solid and well accepted within the community; A bit more of explainability exploration could have helped to make the paper stronger, what exactly the model learned.	Deep learning models with explainability features have not yet been applied in tau segmentation from WSIs. The authors develops baseline Deep learning method for this application. The clinical significance of the method is not thoroughly discussed and compared with trivial methods.	I would like to see more description of why this work is important and why others have not previously done it.	I think it is important to facilitate data and code sharing and this paper can help to this.
556-Paper0718	Visual explanations for the detection of diabetic retinopathy from retinal fundus images	This submission attempts to maintain highly accurate and improved visual explanations simultaneously for the detection of diabetic retinopathy by combining a plain and an adversarially robust model. The combined model shows better prediction accuracy than the robust model, while with improved visual explanations compared with plain models using experimental results.	The authors propose an ensemble model (consisting of a baseline model and an adversarially robust model) to mitigate the trade-off between classification performance and quality of visual explanations. Experiments were performed in three Diabetic Retinopathy (DR) datasets, one of which had DR lesion annotations at pixel level. They compare the interpretability results obtained between a visual counterfactual explanation and two common saliency map techniques (Guided-Backprop and Integrated Gradients).	This paper suggest two contributions: the proposal to ensemble a tranditionally-learnt and an adversarially-learnt model together in order to improve interpretability, and an adversarial approach to generating saliency maps.	Authors attempt to address the dilemma: a plain model usually has better prediction accuracy but less reliable visual explanations; in contrast, an adversarially robust model has more reliable visual explanations but suffers from dropped accuracy. The authors propose a simple ensemble model (Equation 2) to maintain high accuracy and improved visual explanations simultaneously. The ensemble model can also generate a saliency map by the difference between visual counterfactual explanations and the original image, which can serve as a complementary method to existing saliency-based techniques.	The authors propose an ensemble model (including an adversarially robust model) to mitigate the trade-off between classification performance and quality of visual explanations. That makes sense since sometimes a good performance results from spurious correlations in the data, and is not driven by clinically-supported evidence. Improved saliency maps show precisely that models become more robust.	The strength of the model is the clear prediction regarding ensembling traditional and adversarially-learnt models and what the effect would be on accuracy and interpretability. This hypothesis is largely supported by the data, although with a lower robust accuracy than expected (i.e. the ensemble models seem to err on the side of the traditionally-learnt models).	From controlling the trade-off between adversarial and plain training schemes aspect, Equation 2 (adversarial training) and Equation 3 (ensembling) are similar. Therefore, the authors should provide more argument with more detailed discussion on why Equation 3 is effective.	The work lacks novelty. The only novelty element of this work is the use of an ensemble model. The two saliency map techniques used in the experimental setting are two of the most simple ones (Guided-backprop and Integrated Gradients). Introducing an ensemble increases complexity and makes the model less interpretable. It is not clear how the saliency maps were computed (as presented, the network has two branches, each one with the image as input, how were the gradients propagated?) It is also not clear how the oversampling of the minority class was performed.	The paper (correctly) notes that saliency maps are used to help justify a decision made by a neural network model to a clinician. However, this paper only shows something similar to a saliency map for two cases (Fig 1 and Fig 2) in which the diagnosis is very visually apparent and the classification is correct, which is not clinically relevant. What would be clinically relevant are cases that are either non-apparent (with a correct prediction) or ones in which the prediction is incorrect. Without those, it is hard to tell if their technique has value to a diagnostic workflow. From Figure 1, it is hard to see what the advantage of their VCE approach is compared to the other approaches. Qualitatively, it seems that Guided Back-Prop is more specific than the proposed method. From Table 2, it seems that both comparative methods on average outperform the proposed one, although that might not be stastistically significant. (The authors can easily check this with paired non-parametric tests such as signed-rank tests.)	Authors do not provide source code. But with the detailed description presented in the paper, results should be reproduced.	Reproducibility is guaranteed.	The paper is mostly clear about the techniques/data it used as well as open-source implementations and public databases it uses, meaning that it could conceptually be produced relatively easily.	The justification why Equation 3 (ensembling) is more effective than Equation 2 (adversarial training) should be given more detailed discussion. It is better to design experiments to justify why the values of tunning parameter beta (in Equation 2, adversarial train) could not help a robust model achieve high accuracy and good visual explanations simultaneously.	The authors should provide more details regarding the generation of the saliency maps and oversampling. It would be interesting to check the visual explanations generated by more recent saliency map techniques (e.g., LRP).	Major comments: The paper spends a lot of time motivating VCE's and their derived saliency maps, although no evidence (and some weak counter-evidence) that they are better than simple, openly available frameworks. Ultimately, the paper would be improved by focusing solely on the first contribution, which takes up less than half of the methods (about 1 page of methods+results versus approximately 2 for the same sections for VCE components) but produces meaningful results. What would be warranted here is more ablation studies, studies using different ensembling techniques, or 'pure' ensembles of tranditional or adversarially-learnt approaches as well as mixed.	Novelty and experimental results.	The work is interesting but lacks novelty.	The paper spends most of its effort on proposing a saliency map approach that doesn't outperform already available ones. Although this is not a problem in itself, the paper doesn't do extensive analysis on this which would make it of scientific (rather than technical) interest. The area of more technical interest (i.e. the ensembling approach) also doesn't receive enough attention (e.g. ablation studies, clinically meaningful visualisation, etc...) to be of sufficient interest to a MICCAI reader in its own right.
557-Paper0634	vMFNet: Compositionality Meets Domain-generalised Segmentation	In this paper, the authors developed a 2D semi-supervised vMF-kernel based model for domain generalization segmentation.  The authors also propose to leverage the unlabeled data using a reconstruction module.  The experiments on 2 datasets show ed the superiority of the vMFNet compared to SOTA methods.	The paper present a method for semi-supervised and test-time domain generalization based on the composition of von-Mises-Fisher (vMF) kernels. The proposed method models the distribution of features as a mixture of vMF components. This mixture (clustering) prior is used to regularize training with a loss minimizing the negative log likelihood. This loss is combined with an unsupervised reconstruction loss and a supervised segmentation to learn a representation for segmentation which is more robust to domain shifts. The proposed method is tested on two datasets for cardiac segmentation (M&M) and spinal grey matter segmentation (SCGM), and shows improved performance compared to different baselines.	A novel architecture and loss function is proposed for domain generalization in medical image segmentation. The novelties are inspired by recent progress in compositional CNNs. Experiments on multi-center cardiac and spine MRI datasets reveal improved robustness to domain shifts, as compared to several recent domain generalization methods. The performance is shown to improve further upon training some blocks of the pipeline at test time, using an unsupervised loss that was also used during the initial training.	The idea of using vMF kernels is novel and interesting, and has not been well explored in the fields of medical image processing. This provides an alternative perspective of decomposing image into style and content as in traditional disentanglement approaches. Besides, this paper conducted a strong evaluation by comparing against the state-of-the-art domain generalization methods in the semi-supervised setting.	The idea of modeling compositional components using vMF kernels has not been explored for domain generalization. Although inspired by [20], the application setting of the proposed method differs from this previous work. Experiment on M&M and SCGM show clear advantages compared to recent baselines for domain generalization. The extension of the method to test time adaptation adds depth to the paper.	This is the first time that I have seen use of concepts from the compositionality literature in medical image segmentation + domain generalization papers. In principle, I agree that the notion of compositionality might play an important role in helping us build more robust algorithms. Kodus to the authors from bringing up this topic - I am confident that this will of high interest to the community. Strong evaluation for domain generalization. Usage of multiple datasets and anatomies. Extremely clear writing and overall presentation.	Experiments on both datasets were all conducted in 2D networks. It would be interesting to understand the effectiveness of vMF modeling in 3D contexts. Tests of statistical significance are missing: for example, in Table 1, in SCGM 20% (Dice), the vMFNet achieved about 1.5% Dice improvement compared to DGNet but with a standard deviation of 8.8.	The theoretical motivation of the paper is somewhat unclear. The concept of compositional components is defined in vague terms. In practice, the proposed method seems like a regular clustering prior on features based on a mixture model. The advantage of employing vMF kernels instead of Gaussian components as in GMM is unclear. Some elements of the method could be better explained / motivated. For example, I am not sure that the definition of L_vMF achieves the desired goal of aligning mu_j to z_i (missing minus sign?). Also, I fail to see how the image can be reconstructed from such a low dimensional representation, or how this reconstruction can be useful. The proposed method is similar in essence to have a auto-encoder loss on the features as in [35]. I believe this approach should be included as baseline in the experiments to demonstrate the advantage of the vMF compositional model.	My main concern is that I find a lack of sufficient explanation / intuition as to why the proposed method should help for domain generalization. Despite the additional loss function L_{rec} and L_{vMF}, and despite the additional decomposing-composing pipeline, I do not exactly see why a neural network trained on a source distribution will provide correct predictions on a shifted target distribution. Specifically, the feature extractor trained on the source domains may not necessarily extract similar features from a test image. Thus, the fixed kernels may not necessarily correct highlight different regions in different likelihood channels. Indeed, the fact that test-time-training improves performance shows that there are some errors that TTT can alleviate. Nevertheless, the proposed training seems to provide improved robustness even without TTT. Can the authors provide any pointers that can help me better understand this?	The authors agree that the code will be made publicly available.	Except for the potential error in the L_vMF loss, the method and experiments should be reproducible. Sharing the source code would help.	The authors have agreed to make their code publicly available. Also public datasets have been used for experimentation. Sufficient implementation details have been provided in the paper.	As mentioned in the paper, 'the vMF likelihoods contain only spatial information of the image', it would be interesting to further discuss the relationship between modeling via vMF distribution and the traditional disentanglement. Especially in Figure 2, the visualization of the most informative vMF channels is very similar to that of the content code feature maps in disentanglement. In section 3.1, using 'D' as the channels is somewhat confusing. 'C' would be a better notation for channels. Typo at section 4.5: 'subjectc'.	I wish authors can explain in a more formal manner why the proposed model helps learn features that can generalize across domains. What is the difference/advantage of the method with respect to a simple clustering prior on the features? Why use vMF instead of a GMM ? The loss L_vMF seems incorrect. I believe it should be Sum_i min_i (1 - mu_j'*z_i) since the goal is to minimize the cosine distance, not cosine similarity. The reconstruction loss in Eq (2) is somewhat arbitrary. Why would the log-likelihood be used as weight ? How can the image be accurately reconstructed from such low dimensional space ? Shouldn't there be another weight in front of the reconstruction loss in Eq (3) ? See the main weakness section for other comments.	"The feature extractor is almost a 'full' U-Net, but without the last upsampling layer. This means that the features Z would be half the input images in terms of spatial dimensionality. The reconstruction and segmentation networks are quite shallow with only 1 upsampling layer. It would be useful to clarify these aspects in figure 1 - for instance, you could add a partial decoder to the orange feature extractor, and reduce the number of upsampling in the blue and green blocks. It is said that ""For data from different domains, the features of the same anatomical part of images from these domains will activate the same kernels."" This is a strong claim. Is this based on empirical results or can the authors support this claim theoretically? In the former case, can the authors provide an intuition why the learned kernels are robust? In the evaluation, two settings have been mixed - those of semi-supervised learning and domain generalization. While the constraints of both these settings (less labelled data and domain shifts between training and test images) may occur concurrently in practice, the methods that the proposed method has been compared with have all been developed primarily for the domain generalization problem. A fairer comparison with respect to semi-supervised learning would have been to include methods from that setting as well. I admit that this would call for too many comparisons - a leaner way could be to focus on one of the two problems at a time. Please provide details of which intensity and resolution augmentations are used and with what hyperparameters in the compared method SDNet + Aug. I am not sure how much can be read into the compositionality visualization in figure 2. I suspect that if one visualizes the different channels of a layer before the last upsampling layer of a normal U-Net trained using a supervised loss, one might also see similar segregation of structures into different channels. Perhaps the presence of the reconstruction loss also preserves background structures, which would not happen for a u-net trained only for segmentation. On the other hand, the background structures (e.g. those on the top-right) are not necessarily separated into different channels. How sensitive is the method to the choice of the number of kernels? Is it important to keep this number relatively low?"	The compositional network with vMF is novel and has not been used to address the domain generalization problems in the literature. The decomposition by vMF kernels can serve as a potential alternative to content-style disentanglement and may be applied to the field of image harmonization and etc. The paper is well-written and make reviewer easy to follow. The experiments are well designed and the visualization of the compositionality provides more insights to the readers about the vMF kernels.	The paper brings an interesting concept of component compositionality for domain generalization, and shows the advantage of the method compared to recent baselines. However, the motivation of the method and its loss terms needs clarification.	I believe that the compositionality idea is very interesting from the point of view of domain generalization, and the authors have developed, presented and evaluated the method very nicely. This paper will make a fantastic addition to the conference, in my opinion.
558-Paper0897	Vol2Flow: Segment 3D Volumes using a Sequence of Registration Flows	This paper presented a self-supervised algorithm for 3D image registration to find 2D displacement fields between all adjacent slices within a whole volume together. The output of Vol2Flow is employed to segment each arbitrary anatomical structure in a 3D medical image by gradually propagating the 2D segmentation mask provided by a user between other slices.	The paper presents an image segmentation method via slice-to-volume label propagation using image registration. It is overall an interesting and novel approach and it has shown to outperform a few other methods in the literature (e.g. fully supervised, other registration methods, etc.).	The paper proposes a method trained to create a 3D segmentation given a single slice manual 2D segmentation. The 2D segmentation is propagated sequentially to neighboring 2D slices using the inter-slice 2D displacement field (flow).  The Flow between all pair of 2D slices is generated by the network, which was trained (self supervised) to align neighboring slides of the train images.	The paper is well-written and organized. The experiments is sufficient.	The volume segmentation problem is addressed busing slice-wise image registration. Only one slice needs to be annotated to segment the whole volume in test time.	The paper presents an approach for semi-automatic segmentation, combining ideas from self-supervised learning and registration. It builds up upon Sli2vol algorithms to take advantage of a global 3D context.	Many important method and experiment details are missing in the paper.	The actual applicability in real scenario worth an in-depth discussion. Additionally, the method is suffered from severe error propagation problem due to slice-wise registration.	1) the displacement field between the neighboring slices may not exist.  For instance, Z-spacing of CT images could be 5mm, and the object boundary changes significantly from slice to slice.  it seems the algorithm most likely will fail in such cases 2) it unclear how the algorithm propagates the segmentation when the organ boundary is reached and it disappears from the next 2D slice. It seems to be another limitation.  3) Comparisons are insufficient.  Optical flow algorithm chosen is very basic and old, its results looks artificially weak. A better deformable image registration method (based on Mutual Information) should be tried for a fair comparison.  Another comparison lacking is to other semi-automatic segmentation methods, such as segmentation based on extreme points (e.g. in 3D) , at least a discussion about such methods would be nice.	I believe that the obtained results can be reproduced.	It is difficult to re-implement the exact method and reproduce the results due to missing implementation details.	seems reasonable to reproduce.	More details about the user-provided single slice annotation should be added for clarity. Additionally, the influence on the segmentation performance from the slice annotation should be investigated.	The paper presents an image segmentation method via slice-to-volume label propagation using image registration. It is overall an interesting approach and it has shown to outperform a few other methods in the literature (e.g. fully supervised, other registration methods, etc.). The actual applicability in real scenario worth an in-depth discussion. A few comments are provided as below. From Fig.1 and the corresponding description, it seems the warped slice is used as the input to produce the next warped slice. Error will propagate rapidly. The method requires a SVM training process to post-process the label, which seems quite inconvenient. In a multi-label scenario, do you need to train several SVM classifier? Please clarify. Different organs may require to annotate different slices, if they're not appeared in a single slice simultaneously. It is desirable to investigate the effect of the location of the annotated slice to the segmentation performance. Is it better to annotate the central slice of the target organ? How to select which slice to annotate? One drawback of registration based method is computational time. It is desirable to report the training time and inference time when comparing different methods. It is an interesting approach to propagate the label from slice to volume. However, it is still an intensive workload to annotate one slice (or potentially a few slices) for each unseen volume accurately, especially in a multiple organ case. How does it compare with few shot learning/semi-supervised learning? Could the author comment on how well the registration method work on slices with organ transitions or slices with larger thickness? When does the registration fail? Worth considering applying the method to an interactive image segmentation scenario, where the user can interactively improve the segmentation result.	the method is consequentially novel and interesting and it deviates from a common algorithms for the task.  Nevertheless the approach seems not very practical, and the comparisons are limited.	The idea is novel and useful to improve the effective utilization of medical datasets.	The paper has certain novelty and the performance is good and convincing.	The method is limited in applicability since it is based on the assumption that a displacement field exists between the 2D slices, which is rarely true. It also seems to over-complicate the task of producing registration if displacement field is desired.    A fair comparison with a better deformable image registration algorithms (non-deep learning) should be able to accomplish the same task without any training, which would be much simpler.
559-Paper1748	Warm Start Active Learning with Proxy Labels & Selection via Semi-Supervised Fine-Tuning	The manuscript deals with the problem of a cold start in active learning. The problem addressed here is which sample we shall start labeling given a set of unlabeled data.	This work applied self-supervised training for cold-start and semi-supervised training for active learning. The pseudo-labels in self-supervised task are collected by thresholding and the task is to perform binary segmentation. The pseudo-labels in the semi-supervised task are collected from the trained network. During the process, images need to be ranked based on network's uncertainty using MC. The proposed method outperformed the baselines on two datasets.	The paper presents a method to label 3D medical image volumes without any labelled data to begin with. A proxy segmentation task on pseudo-labelled data provide uncertainty measure to rank unlabelled data for initial annotation. Following this, a two stage combination of supervised segmentation model and a semi-supervised fine-tuning model take over selection of unlabelled data to be annotated next.	The manuscript deals with an interesting research topic, i.e., how to select samples for being labeled by the user. The manuscript is well written (primarily). The main idea is to use a self-supervised approach to pseudo-label the data. Then, a fully supervised training procedure takes place for further fine-tuning under a semi-supervised learning framework. That final model is used to select data for annotation. Pseudo labels are created by thresholding and other standard image operators. The proposed approach is good as well. Several and different concepts are used to compose the final methodology.	The proposed method is simple and straightforward. The results from the figures are strong, meaning at least one setting is outperforming the baselines.	Use of uncertainty from some proxy task to tackle cold-start problem is an interesting take of the problem. The build up of method from initial to no labels to using unlabelled data to select for annotation is applicable to many medical image data in clinical setting. Extendability to other tasks - Changing the pseudo label process and proxy task could help implement the same method to other medical image modalities.	The manuscript suffers from several weaknesses. First, the figures are small and have a poor resolution. Secondly, the experimental section does not compare the proposed approach against others in the literature. The authors could consider similar approaches that have been proposed for other applications and try them for 3D medical data.	"It is not clear what are the different settings compared in the experiments. Especially, how the variance and entropy methods are used together with semi-supervised method. In algorithm 1, line 6, 10, 12, there are three places where data needs to be selected and the comparison/ablation study setting is not clear. ""proxy ranking"" has also be introduced but the exact definition is not clear."	It seems that from figure 2, the improvement / performance gain of semisupervised method with proxy is tied to choice of data and acquisition function. For example, the ProxyRank-Semi performed well above supervised method and other semi-supervised method (e.g. entropy) for Liver & Tumor but for Hepatic Vessel data, the ProxyRank-Semi came close to ProxyEnt and at the end of fourth round was well below it as well as ProxyRank-EntSemi Limited discussions From figure 3 (eg. 3C and 3F), the combination of gain of semi-supervised method with proxy ranking was seen more on Hepatic dataset than liver and tumor which could have been explored and explained better. Semi-supervised added with proxy better only for early rounds (fig 3C)?	It seems reproducible.	The authors state that the code will be shared. The data is also public.	The authors have satisfied the reproducibility checklist.	I recommend the authors improve the paper presentation. Although they provided an algorithm to describe the proposed approach better, which is nice, they failed in providing a more robust experimental section.	Explaining the different setting better and uniform the setting name between figures and tables would largely increase the paper's clarity. In sec 2, it may be better to provide some arguments or related work for the claim that mean teacher and shape constraints are not ideal for AL. The figures 2 & 3 are not easy to read, may be better to increase font size of the legend.	The method is three-staged at this point i.e. proxy ranking, supervised, unsupervised. One future direction could be making the method end-to-end	Proper paper presentation and a more robust experimental section are lacking.	The method is good. The results are good. Explaining the experiment setting should be minor changes.	The presented method proposed a methodology that tackled the problem at hand i.e. cold-start annotation / active learning and exploitation of unlabelled data for better annotation candidates The paper is well written but some parts of the results could have had a better exploration and discussion
560-Paper1064	WavTrans: Synergizing Wavelet and Cross-Attention Transformer for Multi-Contrast MRI Super-resolution	This paper proposed a joint wavelet and residual cross-attention transformer network for multi-contrast SR reconstruction which tried to restore high-frequency information in the target image with the help of high-resolution reference image. This method is evaluated on two datasets derived from the in-house datasets brain and public dataset fastMRI knee. Comparisons with several previous methods also prove the effectiveness of proposed method.	The paper presents a novely way of synergizing wavelet with swin transformer to obtain super-resolution images from multi-contrast MRI. The ideas proposed are interesting and the obtained results have been compared to various recent methods with an improvement over all of them.	Autorhs propose a novel WavTrans to synergize wavelet transforms with a new cross-attention transformer to tackle the challenges in super-resolution. The proposed method outperforms state-of-the-art methods by a considerable margin with UFs of both 2-fold and 4-fold.	Technical contribution. While the proposed method builds on existing building blocks, I think it is sufficiently novel. Extensive comparison with chosen state-of-the-arts methods Extensive ablation study demonstrating the advantage of the proposed contributions Code has been provided in the supplementary material	The paper presents a novely way of synergizing wavelet with swin transformer to obtain super-resolution images from multi-contrast MRI. The ideas proposed are interesting and the obtained results have been compared to various recent methods with an improvement over all of them.	The proposed WavTrans method outperformed the state-of-the-art multi-contrast SR reconstruction methods.  This study provides a potential direction for further research into the processing between multi-contrast images for MRI super-resolution. The paper is well organized  and the experiment is credible.	No weakness, but a minor suggestion, I hope the authors can add another ablation study to discuss the importance of K-space data consistency loss.	The paper indicates to use wavelet packet decomposition but it seems that it hasn't benefited from WPD but instead the simple wavelet transform. It needs to be clarified which one out of the two it was and why. It also used simple Haar wavelet, the choice of which wasn't justified and may need to be explored.	The transformer model is difficult to optimize, the proposed is implemented with 4 V100 GPUs, which is unpalatable for clinical application.	This paper is highly likely to reproduce as the authors have provided the code in the supplementary material.	While the software details are provided, the paper uses in-house data and non-open source code which will likely make it hard/impossible to reproduce the results.	This is paper is well organized, I think it is with good reproducibility.	Detailed comments: What's the model size of proposed method? And what's the inference time compared with other methods. The architecture illustrated in Fig. 1 can be optimized, it's a little bit confusing as there are too many lines in the figure.	proofread and fix any grammatical errors e.g. section 1 line 2 'is able to provides'; page 2 line 3 'To the end'; page 2 line 6 'MR images quality' -> 'MR image quality'; 3rd last line before section 2 'a in-house dataset'; fig 1 shows the WPD and the text in above sections indicate use of wavelet packet decomposition but do you need wavelet packet decomposition or is it just wavelet transform which is being used? why wavelet packet and not simple wavelet transform? if wavelet packet, how is it being used since the paper says we only use 1-level decomposition but up to 1-level there is no difference among the WT and WPD? can you clairfy this in the text fig 2, not clear what each row represents; further, the columns need to be referenced with the relevant citation where the method is from another paper	Why authors choose Swin Transformer as backbone? Please compare it with other transformer model (e.g. DeiT, PVT) and state your reason. Please show the parameter quantity and computational complexity in Table 1.	The idea of this paper is technically sound and reasonable. Specifically, a joint wavelet and residual cross-attention swin transformer network is proposed for multi-contrast MR SR problem. Comprehensive experiments including comparison with other state-of-the-art methods and ablation study show the effectiveness of the proposed method. They also evaluate their method in 2 different datasets, one is in-house brain dataset, the other is public fastMRI knee dataset, which show the generalizability of the proposed method.	strong methodology, good results	This paper is well organized.
561-Paper1100	Weakly Supervised MR-TRUS Image Synthesis for Brachytherapy of Prostate Cancer	The authors propose a novel weakly supervised learning strategy to train DCLGAN, thus achieving the transition from input MRI images to output TRUS images. Prostate Contour Segmentation and MRI Pattern Exaction are proposed to realize the weakly supervised learning for DCLGAN.	To cope with low soft tissue contrast and tumor visualization in the prostate area of TRUS imaging, this paper proposes a method that synthesizes TRUS images from unpaired MR images. Anatomical structural constraints with weakly supervised modules are considered to emphasize the structural content of the synthesized TRUS image.	The paper employs DCLGAN to generate TRUS-styled MR images using weakly supervised learning. This can be applied to LDR prostate brachytherapy planning and treatment. The results show both image quality enhancement and precise segmentation.	The author collected a dataset and carried out detailed experiments on this basis.  Compared with the baselines, the performance of the proposed method is much improved. Prostate Contour Segmentation and MRI Pattern Exaction contribute to realize the weakly supervised learning for DCLGAN.	The method for synthesizing US images from MRI images is novel, the application has significant clinical value. Comparison and evaluation of the results are provided.	The proposed method creatively constructs the style transfer architecture upon a generative adversarial network based on cycle consistency. Particularly, the discriminator classifies the real and synthetic MR images, TRUS images, MR prostate contours, segmentation of tiny anatomical structures of prostate. Adopt weakly supervised learning to alleviate the requirement of paired data for training, which is usually too challenging to have segmentation masks on MR Images.	Qualitative results show that the proposed method still has a lot of room for improvement. Weakly supervised learning is emphasized in the title and contribution of the paper, but there is too little description about weakly supervised learning in the main body of the paper. The authors did not conduct ablation experiments. This is not a big problem, but the author uses too many modules, I am not sure whether all these modules could work.	-	Need to specify how to select the weights in the loss function.	This paper is clear enough that an expert could confidently reproduce.	I believe it could be reproduced from the paper.	The paper has included enough details to facilitate reproducibility of this work, except for the weights in the loss function. In addition, it would be a large contribution to our community if the authors consider opening the training data.	The authors did not conduct ablation experiments. This is not a big problem, but the author uses too many modules, I am not sure whether all these modules could work. For example, Contour, IDT loss and BCE loss are all used in Prostate Contour Segmentation, but I am not sure whether these settings are all necessary.	It is very helpful that the authors can provide additional explanation on the optimization objective for the generators G and H.	I have no suggestions here.	I think this thesis is probably below the threshold for acceptance. The main problem lies in the structure of the paper, the lack of necessary ablation experiment, and the qualitative effect is not particularly good.	Coping with low soft tissue contrast and visualization of the tumor in the prostate region is a difficult task. The paper presents a method that synthesizes TRUS images from unpaired MR images. No major revision is required but improvement could be on clarifying the MR image pattern extraction which is very technical.	The proposed method is novel with most of the specification provided. Using weakly supervised learning for prostate segmentation on MRI resolves the issue of lacking prostate labels.
562-Paper0890	Weakly Supervised Online Action Detection for Infant General Movements	This paper presents a method to classify and localize the fidgety movements of infants in videos. To this end, the authors propose a framework consisting of a local feature extraction module from human pose keypoints and two branches of clip-level pseudo labels generation and online action modeling. The proposed system shows an accuracy of 93.8 % in the collected dataset, which is 5.4 % higher than MS-G3D [1]. Liu, Z., Zhang, H., Chen, Z., Wang, Z., Ouyang, W.: Disentangling and unifying graph convolutions for skeleton-based action recognition. In: Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. pp. 143-152 (2020)	The authors propose an FM online action detector for automated GMA analysis for early medical intervention for cerebral palsy in infants. The proposed MIL loss-based action detector is divided into two branches to enable online learning of the vertex fusion features of the extracted key points. The authors appropriately borrowed the ideas of weakly supervised learning and online action detection to train successful FM detectors. The authors have built their large-scale datasets for FM detection, but they will not be released to the public.	In paper, the authors have proposed a weakly supervised online action detection framework for infant movements. The proposed framework had been evaluated on 757 videos and can achieve promising results with only clip-level supervision. In addition, the proposed framework is of great importance of clinical use. Only the first 20% duration of the video is needed to get classification results as good as fully observed, suggesting a significant cut of diagnosis time.	1) Detailed and accurate mathematical modeling of the three introduced modules (local feature extracting, clip-level pseudo labels generating, and online action modeling) 2) Learning without frame-level annotation 3) Showing the same performance by reading 20% of the video as if the system had read the whole video	As the authors argue, the development of automated FM action detectors for early medical interventions for cerebral palsy in infants is important. The authors designed a 2D key point estimation-based WO-GMA in a trainable form by appropriately borrowing the ideas of [7] and [19], which are used for natural vision. Although the authors did not evaluate the clinical effect of the FM action detector, the performance of the detector itself was adequately compared with other SOTA models.	The strengths of the paper are as follows. The application is novel. The applications for infant general movements are rare compared with segmentation of organs or image classifications. As a reviewer and a reader, I would like to read new applications which can broaden my horizon. The techniques used in this paper are reasonable. By using the weakly supervised method, the annotation cost can be cut and the whole diagnosis time could be accelerated. The paper is well-organized and easy to follow. The experimental results are convincing. The authors evaluated the proposed framework on a relatively large dataset. And the methods used for comparison are all recent works.	1) It relies heavily on Openpose performance. 2) The paper insists that inference with only 20% of the entire video is sufficient, but other methods also show similar results.  2) The dataset used is not disclosed, but detailed explanation is still lacking.	The consequences of ablation are unclear. In the case of the branch and local feature extraction module proposed by the author, it seems meaningful in video-level classification but does not affect action detection. It is encouraging that FM detection has been attempted from the point of view of action detection, but the overall performance is low despite the single class of action target. Even if a proposed dataset is constructed in a very limited environment, scaling to action detection does not appear to be effective.	"The paper also needs improvements. The weaknesses of the paper are listed below. The detailed information of the proposed method may need more elaboration. For example the size of the CNN used in clip-level pseudo labels generating branch. The proposed framework was only tested on an in-house dataset. The cross-dataset generalization would be an issue. If the proposed framework could be tested on more publicly available dataset it would be great. There are some typos.  For example, on page 4, ""in detial"" should be ""in detail"". How the clip-level annotations were generated and used should be presented."	The paper presents a detailed description of the proposed method, and the implementation details of the training strategy are expressed in detail. However, many parameters in the design are missing, so re-implementation is difficult.	Regarding the reproducibility of the research, the authors planned to release the code and models, but there is no plan to disclose the dataset.	"The reproducibility of the paper is not very high. Because the dataset used in this paper was not publicly available. Based on the ""No Free Lunch Theorem for Machine Learning"", the proposed framework may not be able to perform well on different datasets as it performed on the in-house dataset."	It would be nice if Figure 2 shows the intermediate result for the STAM. Furthermore, it would be good to match and align the measurement points as well. It will be helpful to explain why the proposed structure is appropriate for infant video data.	The ablation results do not sufficiently show the strength of the design of the proposed model. In particular, it seems that more diverse experimental designs and analyses are needed for action detection. In addition, a better evaluation would be possible if there was an in-depth analysis of the ablation results.	"The comments and suggestions from the reviewer are as follows. The detailed information of the proposed method should be provided. There are some typos. For instance, the authors used ""Table 1"" and ""table 1"" interchangeably in the manuscript. But the reviewer thinks they should be consistent with each other. How many male babies and female babies were in the dataset? The interpretation of the experimental results were too limited. The explained and analysis of the results should be enhanced. In the manuscript, the resolutions of the whole dataset are not the same. How did the authors solve this problem was not mentioned. The reason why the authors focus on F+ is not clear. The author may want to explain more."	The method is interesting, but the method description contains unclear parts, and the results are plausible except for the early classification (requiring shorter video).	It is the same as the answer to question 8. The ablation results do not sufficiently show the strength of the design of the proposed model. In particular, it seems that more diverse experimental designs and analyses are needed for action detection. In addition, if there was an in-depth analysis of the ablation result, a better evaluation would be possible.	The paper presented a framework based on weekly supervised learning method, which can solve the data scarcity problem in medical image processing.  The authors had demonstrated clinical feasibility.  The problem solved in this manuscript is of great clinical importance. The techniques used in this manuscript are reasonable. The manuscript is well-organized and easy to follow.
563-Paper1515	Weakly Supervised Segmentation by Tensor Graph Learning for Whole Slide Images	The authors are addressing the well-known challenge of annotating medical data, in this case, the annotation of large-scale Whole Slide Images (WSI) for semantic segmentation. For this, the authors combine the idea of superpixels with sparse point annotations and graph networks. They propose a new architecture with three paths including different approaches (Handcrafted features + graph network, learned features + DL-Classification, learned features + Graph Network). The overall network architecture is evaluated using two public datasets. They conduct a comparison with other fully-supervised and semi-supervised methods and include an ablation study to the data.	This work propose to 1) use superpixel classification to split patch into super pixels 2) extract features per super pixel, 3) define graphs based on which, multiple losses are defined in addition to segmentation cross entropy loss. One of the additional losses if node classification.	Proposed a weakly supervised network based on tensor graphs for segmentation of whole slide images.	* The authors proposed an interesting approach that combines different techniques and ideas, including such as learned and hand-crafted features, neuronal classifiers and graph classifiers.  * The paper is generally well-written and good to understand, I really enjoyed reading it. (Although there is still room for improvement) * The authors indicate that they will release the source code of the paper. This, and the fact that the data is public lead to reproducible results.	The method seems to be novel with a complexed setting. The results of the proposed method are better than the baselines.	Please see below	* While the evaluation is done on two datasets, which is a clear plus, there are some shortcomings in the evaluation which reduce its value of it. For example, the comparison methods are not in the current state of the art but were published in 2019 (with one exception); only fractions of the datasets have been used during training, and no measures for uncertainty are given.  * The overall idea of using superpixels and graph networks is not new. Also, the type of annotation has already been used. (Of course, the authors do not claim this as a novelty)  * The authors used only parts of the training data due to hardware limitations. In general, the method seems to be rather computational expensive compared to the benefit.  * It is unclear how the meta-parameters were obtained.	"The explanation of the proposed method is hard to understand. The paper is not easy to read. For the tensor graph learning, it is hard to follow the explanation without some clear definition. For instance, ""Graph adaptive and aggregation module (GAAM) learning combines and adjusts between different relations encoded in multi-relational graphs."" There's no equation to explain how different tensors interact. Also for the tensor graph, it is not clear how to define the edge? Regarding the loss, it is defined as a trace of a matrix, as it is not a traditional loss such as cross entropy, it would be better to explain the meaning of this loss, or cite a reference. For the node reweighing, what does \hat{y}_m represent in the loss l_3?"	Please see below	Of course, the paper itself is not reproducible, as too many details are not described (Likely to space limit). But the general ideas are well-reported and easy to get. The authors suggest that they will release the source code for the experiments and used public data. This should guarantee a high degree of reproducibility for the paper.	The authors state that the code will be released. The data is also public. However as the method is not easy to understand, it may be hard to reproduce the results.	Yes	"(Major + Minor) Points that should be addressed within the review: The proposed methods does have a significant number of hyper-parameters. How were they parametrized? Using the optimal value overall experiments? Please comment (and if possible, include this information in the paper) Please comment on the computational cost for the algorithm, especially as you haven't been able to use all training data. This is especially interesting for the additional cost by l1, which really adds only slightly to the overall performance. Please provide some measures of uncertainty for the experiments. For example, by calculating a confidence interval using Bootstrapping. In Table 1, the results of WSNTG are all bold, even if other results are higher. This should be changed, it is very misleading. There are too many variables defined. A significant number of all the variables defined in the paper are only used once, during their definition. This makes it really hard to remember the important variables. Please reduce the number of variables. The paper is generally well-written. However, there is still room for improving the language. For example, active voice is often used like ""WSNTG does ....."" which should be formulated in passive. Also, the use of ""the"" is often wrong. On page 5 is a type. I assume FAAM should be GAAM While this is most-likely too much for the revision, I would suggest addressing the following points before submitting this work again (to another conference or a journal): Using two datasets is already quite good, however, I would suggest including more of the publicly available datasets in this field. This would make the evaluation even stronger. Include more current state of the art methods, for example YAMU by Samanta et al."	The authors should improve the writing to explain the methods in more details or the reasoning of some choices.	"This paper proposed a weakly supervised network based on tensor graphs for segmentation of whole slide images. It efficiently segments WSIs by superpixel-wise classification and credible node reweighting using only sparse point annotations. The proposed network represents multiple hand-crafted features and hierarchical features yielded by a pretrained CNN to deal with the variability of WSIs. Experiments conducted on two benchmark datasets. This paper is easy to follow and to understand, the writing is well and feel like the idea is interesting, but it is not from my domain, so I could not provide some good insightful domain advice. I noticed there is no Related Work section which is very helpful for researchers to understand the context and background, some recent works even in a different domain such as, Liu, Xien, et al. ""Tensor graph convolutional networks for text classification."" Proceedings of the AAAI conference on artificial intelligence. Vol. 34. No. 05. 2020., could be discussed like intra-and inter-graph propagation. Experiments could consider more publicly available benchmark datasets and to compare with more state-of-the-art methods, and more analysis suggest in the ablation study."	The paper is interesting and well-written. While the main idea is not new, the proposed solution seems to be well-engineered with some additional tricks.  The evaluation is generally good, but has some limitations in terms of comparison methods and computational cost.	The results may be strong however the explanation of the methodology is very poor compare to other papers in the stack and many concepts remain unclear after multiple attempts of re-reading.	Novelty and Experiment
564-Paper1599	Weakly Supervised Volumetric Image Segmentation with Deformed Templates	The authors propose a weakly supervised segmentation method which uses user inputs, a deformable template governed by an active surface model (ASM) through a 3D neural network to segment structures in multiple imaging modalities. The user obtains an initial estimate of the segmentation by initialising the ASM via clicks, and obtains refined predictions by supplying more click. The neural network is used to create complete these updated shapes in response of the clicks. The method is interesting. Experiments highlight advantages.	The paper proposed a novel approach for weakly-supervised volumetric image segmentation, which requires only a few 3D points as training input. The main supervisions are obtained by comparing the segmentation results with a deformable template, and comparing a reconstructed image with the original image.	A method of training networks to segment volumetric images with using only minimal annotations - a sparse set of points.	Interactive approaches are undoubtedly the correct tool to get large amounts of data annotated. The paper experiments on multiple images from multiple modalities. Results seem good even though they limited in number The method is relatively quick considering it works in 3D	This paper is well written. The idea of using deformable templates and self-supervision together to address the weakly supervised segmentation is quite novel.	An interesting idea, proposing a method requiring only sparse point annotations to supervise the training.	I personally do not understand the purpose of the image reconstruction component. Certainly, forcing the network to reconstruct the original image from the segmentation mask may help learn some of the network parameters. But I was expecting this feature being used to create some sort of feedback loop. Using the mse loss for reconstruction of image content is also very risky in this context: a perfect reconstruction is only possible as a result of overfitting because the model won't really learn the distribution of plausible images but just to associate segmentation with image. This can at best generate some contours with plausible greyscale levels near the boundaries of the object but blurry details elsewhere. Since the authors do not seem to use this in a feedback loop, I still am puzzled about why it is part of the training.	Although effective in terms of labor cost, the performance seems not to be very outstanding compared to the baseline approach.	"* Doesn't mention how many images in each dataset, nor how the ground truth was obtained. * Experimental results are not explained properly. * Section 4.3 ""Full supervision"" does not seem to be clearly defined.  Do you mean you train a U-Net with fully annotated volumes? If so, how many were used for each set? * How exactly are the numbers in table 1 achieved?  Was a pair of networks trained on weakly supervised data, then tested on a different set of test images? If so, how many were in each set? * Is it the case that the U-Nets are only trained once? The user performs point annotations until the template is satisfactory, then the U-Nets are trained?  Or is it that the U-Net is trained as the user goes along, so that they can assess the performance and add points to poorly annotated regions? If the former, how will the user know that they have placed enough points? * Is the approach to train a pair of networks for each image to be segmented, or does one train a pair of networks for a set of images (each with its own sparse annotations)?"	The paper is reproducible as authors include the code (which I haven't personally tested).	Codes are promised to be released.	Fairly clearly described core approach which wouldn't be hard to reproduce, though the exact set up is not described (see notes above).  Experiments are not explained clearly though. It is claimed that code will be made available.	"The idea is valuable as the use of an ASM is probably a good idea in this context. I would suggest to develop an algorithm that can take both positive (""this region should be part of the foreground"") and negative (""this regions should not be part of the foreground"") feedback from the user for example using left and right clicks."	Weakly supservised volumetric image segmentation is very useful in practice. Although the performance is not much better than the baseline, the novelty of the propose method makes it a good attempt to address this challenging problem.	The paper should be revised to make it clear exactly what the set up is and how it is evaluated.  In particular, is this a method applied to a single image, or is one training a model from a set of images.  I suspect this is considered so obvious to the authors that they forgot to mention it in the text (unless I've missed it).	The paper is good but I am not convinced that the image reconstruction step is the right thing to do here.	This paper propose a complete and feasible pipeline with considerable technical novelty to the important problem of weakly supervised volumetric image segmentation.	This is an interesting idea, definitely worth exploring. Unfortunately the paper doesn't explain the actually set up clearly - is this something done one image at a time, or is a model trained on a set of images?    This is something that could easily be fixed by the authors.
565-Paper0507	Weakly-supervised Biomechanically-constrained CT/MRI Registration of the Spine	This paper introduces a weakly supervised anatomy aware method for registration of CT & MRI images of the spine.	The paper proposed a weakly-supervised deep learning framework that preserves the rigidity and the volume of each vertebra while maximizing the accuracy of the registration. Authors introduced anatomy-aware losses depended only on the CT vertebra segmentation for training the network.Results show that adding the anatomy-aware losses increases the plausibility of the inferred transformation while keeping the accuracy untouched.	The paper presents a deep learning-driven deformable registration method using vertebra contour-based DVF rigidity loss, to preserve the local rigidity of the DVF while allowing global deformation.	The paper is well written and organized. The results support the main hypothesis of the paper.	1, Authors introduced a new anatomy-aware losses only depended on the CT vertebra segmentation to improve the CT-MRI deformable registration. The idea is interesting to let the warped label be similar to its regid transform. Besides, using only one sgemtation map to calculates loss function is innovative since that segment vertebra from MRI is not as easy as segment that from CT.  2, Another loss that force the deformation fields to be regid in bone region is also a good try.	The introduction of the novel rigidity loss terms.	What is the likelihood this method will extend to other areas of the body, or general CT vs. MRI registration?	1, The notations used in this text are confusing. Many characters have no definition. 2, Experiment results are not promising. Fig. 2 and Table.1 do not show significant improvements with compared methods, and it is difficult to find the impacts of different functions from the results. 4, Writtings and layouts should be improved. For example, Table 1 was mentioned before Fig.2 and Fig.3, but it appears in last pages.	Although the introduced rigid dice loss and rigid field loss perform better than the baseline method w/o such losses, the relative advantage is small. The ablative studies with properness condition and orthonormal condition also show varying results, with different loss terms having their own benefits. It makes it hard to draw a conclusion from such a work.	the work appears reproducible (clear description of the methods and also a robust evaluation strategy), although the primary data set is unlikely to be accessible.	It is difficult to reproduce without open source codes.	NA	What is the likelihood this method will extend to other areas of the body, or general CT vs. MRI registration?	1, The regidly panelties L_{rigid} includes four different variants: rigid dice loss, rigid field loss, properness condition and orthonormal condition. What's the basis of selection for your different experiments?  2, The rigid transformations seemed to be calculated every iteration during the training which is inefficient. I prefer to get information about how you solve this. 3, You only showed the visualizations of the warped labels. However, the visualizations of the warped images and the DDFs are much more importent.	The paper will benefit from a more in-depth analysis of what loss term might be the most appropriate for the vertebrae registration. And although the title includes 'biomechanical', the actual implementation does not include much biomechanical component by only assuming the rigidity of the vertebrae structures. The work will be more exciting if some true biomechanical modeling can be introduced.	the proposed method and loss functions appear to outperform previous methods.	The idea is interesting but need more experiments.	A good work with comprehensive evaluation. But it can benefit from more methodology development and in-depth analysis.
566-Paper1283	Weakly-supervised High-fidelity Ultrasound Video Synthesis with Feature Decoupling	This paper proposes a framework to generate videos from static content Ultrasound images. This work is based on self- and supervised- learning approaches.  at the first key point detector network is trained in a weakly supervised manner and then a dense motion network is used to learn occlusion and heatmap, finally, an encoder with two outputs (content and texture) is designed to generate a final prediction. A quantitative and qualitative result was demonstrated and compared to other works.	In this paper, the authors proposed a weakly-supervised learning-based framework to synthesize high-fidelity ultrasound videos, by animating the static source images based on the motion of the driving videos.	In this paper, a conditional ultrasound video generation model is proposed. In training phase, the key point and motion module takes source and driving image as input and predict occlusion map, and optical flow. Then the optical flow and occlusion map are used for frame generation, supervised by GAN and reconstruction loss.	Generating video from static images for US procedures that can be used for training junior medics. Considering the effect of deformation and occlusion in the design and using a discriminator to increase the quality of the final video.	The paper is clear and well-organized. The embedded demo is helpful for understanding the task and observing the results. Unsupervised image animation is currently a popular topic in general cv domain, but it is indeed less-explored in the medical domain due to some realistic issues such as noises and varying sizes as mentioned in the paper. Accordingly, the authors resort to a weekly-supervised approach for motion transfer, which is more suitable for more complicated medical problems, and could be generalizable to other tasks. The proposed framework has a two-branch architecture to learn content and texture separately. I think it is an interesting idea and could have the potential to help better understand the motion transfer process.	Introduce the animation generation techniques to medical image analysis. The major framework is from existing methods but this is a good application. Achieve good video quality. The video quality is measured by quantitative, qualitative and user study.	How occlusion and deformation are obtained as ground truth for this work? Detecting sparse features as keypoints within US should be a challenging task, how robust is the proposed module to noise and outliers? What is the level of similarity between the source and driving image? What does it need to be for this framework to prevent any failure? Why a single module such as optical flow has not been used instead of keypoint and dense motion?	It is not quite fair enough to compare the proposed model with FOMM, given that FOMM learns features in a totally unsupervised manner. It could be more convincing if the authors conduct more extensive ablations on examining the contributions from each proposed component in the network. The novelty of this paper is somewhat limited theoretically, given that FOMM has been already well-explored in the cv domain.	Novelty seems to be limited. The major frameworks in fig 2 and fig 3 comes from [11], including the self-supervised keypoint detection, optical flow and occlusion prediction, and the occlusion aware conditional generation. The novelty parts are (a) introducing the manual label (b) introducing the GAN loss and VGG loss. Lack of ablation study. There are limited experiments showing the newly add components works. For example, compared with [11], it is unknown if the manual label and GAN loss helps to improve the final results. Lack some illustrations of acronyms. For Table. 1, it is not clear what is P, PT, and PTG.	According to the authors the code and data will be available, but it is not challenging to replicate this work.	The author checked all reproducibility questions. I think it is easy to implement the code given the description provided in this paper.	Data and code will be available, according to the response.	"Quantitative results for the keypoint detector are essential to provide a better understanding  of their effects on the final results, More clarification about VGG loss should be given; ""For texture part, due to unavailability of its ground truth, we adopt feature reconstruction VGG loss [5] to constrain the similarity of driving frame and the final prediction with texture information.""   It seems from eq 5 that this loss is directly applied to derived and source images with the hope that it can improve the texture output. Contrastive loss can also be explored in the future work"	There is no explicit definition for the model variants: ours-P, ours-PT, ours-PTG.	(1) Add more explanations to make the paper self-contained. (2) Considering more ablation studies in the future work	The paper proposes an interesting framework for US imaging that can be used in other domains as well.	The practical use of the proposed weekly-supervised approach in US synthesis could be impactful. The paper is well-written and the proposed model has achieved clear improvements over SOTAs.	The major concern lie in the novelty part.
567-Paper0130	Weighted Concordance Index Loss-based Multimodal Survival Modeling for Radiation Encephalopathy Assessment in Nasopharyngeal Carcinoma Radiotherapy	(1)To assess the rationality of nasopharyngeal carcinoma radiotherapy regimen, the author proposed a deep multimodal survival network (MSN) equipped with two feature extractors to learn discriminative features from image and non-image data; (2)A new WCI loss function is proposed for MSN training, which has temperature hyperparameters that can effectively utilize the REP samples of each batch to help the model converge.	This paper proposes a new multi modality model for predicting radiotherapy-induced radiation encephalopathy (REP). The author proposes a multimodal survival network and a new weighted CI loss function to improve the accuracy of the prediction.	This paper describes a model to predict the risk at 36 months for developing radiation encephalopathy (REP) in patients diagnosed with nasopharyngeal carcinoma (NPC) and treated with radiotherapy. The model was trained using image data (CT and RT) fed to a multimodal survival network and non-image diagnostic data fed to a multilayer perceptron model. The outputs from the two networks were weighted and summed to provide a final risk value. The data used to train, test, and validate the model was used from the time of treatment for a pool of n=4,816 patients. The model was implemented and tested using multiple different loss functions, including their own novel weighted concordance index loss function. The author's novel WCI loss function resulted in the trained model that had the highest measured CI and AUC values compared to the same model trained with 5 other loss functions.	For the first time, both image and non-image data were used in an NPC radiotherapy regimen to predict radiotherapy-induced REP. A deep multimodal survival network (MSN) with two feature extractors is designed to learn to identify features from multimodal data. One feature extractor performs feature selection on non-image data, and the other learns visual features from images. A new weighted CI (WCI) loss function is proposed, which allocates different weights to all representative samples through a double averaging operation, thus effectively utilizing all representative samples. A temperature hyperparameter is introduced to sharpen the risk differences in sample pairs and help the model converge.	The author propose way to use multi-modality data from both images and non-image data is new to this task. The proposed WCI loss is helpful to alleviate data imbalance issue and improve the accuracy.	The strengths of this paper include: Large number of subjects available for training and testing. Combined use of image and non-image data. Use of survival analysis methodology and loss functions to train model. Inclusion of ablation to examine empirically derived parameters such as weights for image versus non-image risk outputs to final output and temperature parameter used for WCI loss function.	(1) The author conducted ablation experiments on their own models on the two modal data and did not use other current models for comparison; (2) Regarding the processing of image data, I hope that the author will introduce more details, such as: how does the author obtain the ROI region, manual or automatic detection? What is the ROI area size? Is the number of CT, RS, and RD pictures the same per patient? Three kinds of picture information can be directly controlled by the operator? (3) The author's model is evaluated on a private dataset, is the public dataset used for experiments?	The dataset is highly imbalanced (1:10), a confusion metric or similar might be worth adding to show what kind of mistakes the model is more likely to make. I am not sure if these improvement on CI is statistically important. The authors may want to add some analysis on the statistic significance.	"The weaknesses of this paper include: Claim of ""best accuracy"" despite no statistical testing and/or analysis between ROC AUC or CI values for significant differences. The weights may have been made parameters of the model to train (w_nv, w_v)."	The author used the proposed method on the private data set, which is not conducive to the reproduction of the method.	The dataset is private, which I assume will not be made public. The authors answered the codes for this study will be released.	The authors state they will release the trained model if accepted. But the dataset will be private, so reproducing the same results would be impossible. And training a model implemented independently based on the description would also be impossible. Something similar could be done, but not exact.	(1) It is recommended to supplement and compare with other current models; (2) Regarding the processing of image data, I hope that the author will introduce more details; (3) Experimental comparison in public datasets using the model proposed by the authors.	Is the CE in Table 1 weighted or not? What's performance if simply use weighted cross entropy for the data imbalance?	"This is a very well written and organized paper. Some suggestions to make it even better: Replace ""synthesizing"" in section 3.2 with ""using all"" as you are not synthesizing data but combining actual data. Possibly add more information about how the image data is pre-processed before it is input to the model. Add methodology to test for significant differences between the measures for the different loss functions."	The author had too little comparison in the experimental part, which is not conducive to comparison with the current methods in this field at home and abroad.	The paper looks solid: proper review of related study and sound experiment result. I am not familiar with the task of radiation encephalopathy assessment and survival model. But based on the presentation of method and result, I vote for weak accept.	The authors presented implemented a model with to predict the future risk of an unwanted injury due to radiotherapy. Having knowledge of this risk prior to treatment would allow clinicians to modify their plans to reduce the risk. The paper was well organized and has many strengths as described above. The weaknesses are minor.
568-Paper1328	What can we learn about a generated image corrupting its latent representation?	This paper presents a noise injection technique that allows generating multiple outputs, thus quantifying the differences between these outputs and providing a confidence score that can be used to determine the uncertain parts of the generated image, the quality of the generated sample, and to some extent their impact on a downstream task.	Proposing noise injection technique for testing latent representation; Proposing two metrics to calculate uncertainty for synthesized images; Validating the hypothesis that robust latent representation leads to better quality of generated image. Small noise injections during the training phase lead to more robust representation and slightly higher image quality.	the authors investigated the hypothesis that we the image quality can be predicted based on its latent representation in the GANs bottleneck.The authors presented a method using latent representation corruption with noise as such that multiple outputs can be generated to obtain uncertainty. The results demonstrated in this paper show that the method has the ability to predict uncertain parts of synthesized images, and identify samples that may not be reliable for downstream tasks.	This paper is well written and easy to read. Sufficient experiments. The results are described and analyzed in details	Noise injection is easy to use in deep learning models, such as GANs; Conducting pre-experiment to observe that noise injection can identify uncertainty parts; Carried on adequate experiments including image translation and downstream tasks.	Easy to read. The idea presented in this paper is very clear and easy to follow. The experiments are solid The idea presented in this paper is somehow novel and can provide insights in medical imaging. Becasue the uncertainty estimation method is unsupervised, which make it value in medical imaging domain considering that most datasets cannot provide proper label.	- There is no need to do experiments, We can also be sure that the scores of these two indicators(variance and MI) will be very poor in Sec.3.2. The generator has a high generation freedom with a black patch. So the author claims 'The results suggest the possibility of finding a potential confidence threshold to eliminate uncertain synthesized images.' is also worth researching.highly reproducable	The validation experiment of the proposed confidence score correlates with the quality of downstream task, e.g., segmentation, on the synthesized image is not enough and straight forward. Segmentation tasks may not be enough, more tasks such as classification and detection can be implemented.	The experiements shown in Figure 3 are somehow vague. I feel hard to understand the relationship between segmentation mask and the generated confidence score maps. Better to overlap both images to show correlation.	highly reproducible	Authors will not make the code publicly available. But the method is easy to be implemented.	Implementing the method should not be a problem because the idea is straightforward.	The caption of Figure 3 should be 'corresponding tumor segmentation map (and) the uncertainty heat map, respectively' refer to the weakness	"The experiment and results in sections 3.2 and 3.3 are very clear and straight forward, and the conclusion is obvious. But the experiment in section 3.4 which validates that the proposed confidence score correlates with the quality of downstream task, is not straight forward and convincing. In Table 4, the absolute value of a correlation between the confidence score and DICE coefficient for the proposed method is up to 0.54, which indicates moderate correlation. These results can not support the conclusion ""This suggests that our method can be used most efficiently in cases where the images are generated well enough for the downstream task network to also perform well."""	Need to explain more details about how to relate segmentation mask with uncertainty maps.	Nowadays, many works on medical image processing using GANs for data augmentation or modality translation. But sometimes it doesn't bring more useful information. This paper can bring benefits to the community and give these works a reference.	The idea is new and interesting. But more convincing results are necessary.	The novelty of the idea - somehow novel. The completeness of the experiment settings - the visualisation of the relationship between uncertainty maps and the segmentation is hard to follow. The relationship betweent hem is not very strong. Maybe authors need to find a new way to demonstrate how to use this uncertainty estimation technique. The clarity and organisation of the paper - the organisation is very good.
569-Paper0115	What Makes for Automatic Reconstruction of Pulmonary Segments	This paper describes the definitions of the anatomy of pulmonary segments and the definition of dice score in segmentation metric. This paper also provides a convolutional deep network called ImPulse for the segmentation task	The authors present a method for the automatic reconstruction of lung segments using deep learning.  First the authors formulate the problem in a concise manner. Second they propose a deep-learning method for lung segment reconstruction that is not based on pixel-wise assessment. Instead, they authors probe different locations of the lung segment border and report a class for such continuous locations. The authors show the performance of the method in a database of 800 CT scans from multiple medical centres with a data split 7:1:2. The proposed implicit method achieves better performance than end-to-end networks.  Further experiments show the performance of the method when detecting bronchi and arteries.	The authors propose an implicit-function-based model for the pulmonary segment reconstruction, that makes the pulmonary anatomical lobe segmentation further. The anatomy of pulmonary segments are defined. Then, the implicit pulmonary segmentation (ImPulSe) is proposed. In the experiments, the ImPulSe achieves better performance and uses less training time, comparing to the fully-convolutional methods like UNet.	The topic is interesting and can provided useful info for the surgical treatment The ImPulSe network have the potential to generate high resolution output when the input resolution is low.	The strengths of the paper are: Extremely well written. Very good problem statement. Thorough analysis of the problem.	The paper makes the pulmonary anatomical segmentation further, from lobes to segments.  The method will benefits for clinical applications, like surgical planning for patients with lung cancer.  The method ImPulSe achieves better performance comparing to fully-convolutional methods.	The contribution is consider limited. The introduced definition is not novel. The efficiency of the ImPulSe network is very convincing. Using the dice score is not very sufficient for the reconstruction task. Labeling the pulmonary segments should be a challenging part of the task. A induction of the labeling procedure and ground truth qualify will be preferred.	"While the introduction and the title refer to pulmonary segment detection, the authors go beyond that, and also detect arteries and veins. This should be stated from the beginning. It comes as a surprise in the result section.  The main justification of the method is the ""ability to output reconstruction at arbitrary resolutions"", with the side nice features of having less parameters and faster training time. However, I should be critical with the method. The authors have a continuous function that they use to define the surface of lung segments. However, the features used as input of such continuous function are obtained by interpolating the feature space, which has the same resolution as the input image (and therefore the output image of standard methods, such as the u-net). It seems as if the authors are doing interpolation in feature space rather than interpolation in the resulting output image.  Following in that train of thought, Figure 2 is not a fair comparison between voxel-based methods and implicit functions, since the gaps between voxels makes them look artificially bad. Also, in the illustrative 2D example, a simple interpolation in the voxel space would have given a soft border. The authors up sample the reconstructions using nearest-neighbour interpolation. What would have happened if the authors used other interpolation methods (as trilinear, as used for the features)?"	1) In section 2.3, it states that the decoder predicts 19-class (including 18 segments and background). However, the Dice of pulmonary segments, bronchi and arteries are shown in the evaluation results. Does the model predict bronchi and arteries as well? It's confusing, please clarify it. 2) In section 3.3, the authors evaluate the effect of different inputs in reconstruction of pulmonary segments. It's confusing by when given inputs of bronchi and arteries, the validating the predicting of bronchi and arteries.  3) In Table 2, predicting pulmonary segments from the inputs of lobes mask (or binary mask of bronchi and vessels) doesn't make sense.  I suggest to try with I, I+L, I+B, ... 4)  During inference, ImPulSe works with the original resolution of CT images, however, the fully convolutional models work with down-sampled images (128^3) and up-sample the predictions back to the original resolution. Will the down-sampling and up-sampling operation affect the results? Why do the authors not try with sliding-window strategy, for fully convolutional models?	Without the medical data and label, this work is hard to be reproduced.	This paper can only be reproduced with the open dataset.  The method seems easy to be reproduced.	The reproducibility of the paper is OK	Introducing the definition of anatomy of pulmonary segments and dice score is not considered as novelty. There are several MICCAI papers introduced the pulmonary structure in the last two years. The experiments need to include more comparison with more recent methods. The ImPulSe network uses less parameters comparing to UNet. but it might have a bigger feature space. I am a little concerned about the efficiency. The boundary of the pulmonary segments should have a clear anatomical definition. A geometry reconstruction is needed before the segmentation output can be considered in the surgical treatment. The labeling the pulmonary segments should be a important part of the task, since there is not a lot open resource. The labeling procedure and the quality of the labels are expect to be introduced in detail.	The idea of doing interpolation on feature space instead of in the segmentation space is interesting and has value. However, the experimentation performed are not a fair comparison between the two methods due to the nearest neighbour interpolation in output space and trilinear in the feature space.	In this study, the authors annotated pulmonary segments, bronchi, arteries and veins for 800 CT scans. Making the data open access or organizing a challenge will definitely enhance the impact of this study.	Even though the topic is interesting, but the novelty of contribution is limited.	I really like the rigorous problem statement and the idea of the continuous border assessment.	The study has clearly clinical impacts, however, some details need to be clarified
570-Paper2080	White Matter Tracts are Point Clouds: Neuropsychological Score Prediction and Critical Region Localization via Geometric Deep Learning	The authors present a new approach for predicting neuropsychological assessments scores from brain MRI data using a point representation of individual white matter fiber tracts. The approach uses a point-based siamese network adapted from PointNet. Their approach further enables the identification of areas in the input point cloud that are critical for the prediction. They evaluate their approach against classic along-tract analysis as well as a simple analysis of the tract specific mean values of the used feature maps. Their results are in line with previous research and they claim better performance.	This work proposes a novel framework for predicting neuropsychological scores, which represent fiber bundles as point clouds, and then preserve point information about diffusion measurements and enable efficient processing using point-based deep neural networks.	This work presents a deep network for neuropsychological score prediction based on point cloud representation of white matter tract features. The proposed network is trained by using a Paired-Siamese cost in addition to the MSE loss. The results of prediction compare favorably to traditional measures and a 1D network defined on tractometry. Based on this network, a critical region localization algorithm is proposed to detect the informative anatomical regions relevant to language processing.	The proposed point cloud based analysis is innovative and provides additional spatial information compared to other fiber representation methods. The proposed WCRL algorithm identifies critical areas that are both consistent across subjects and with previous research regarding language performance as measured by TPVT scores. The results hint that the presented approach might yield more information and thereby enables better predictions, but this is not confirmed using statistical tests.	Looks like this is the first work to predict individual cognitive performance based on microstructure measurements of the white matter fiber tracts. - Overall, the paper is well written, and the experiments are well designed. Also, this work shows some promising results and is good for clinical application.	The idea of using deep point-cloud network for neuropsychological score prediction is new. The proposed paired-Siamese cost for training the network seems an interesting formulation. The proposed method outperforms traditional methods, and it does not require a very deep architecture, making its interpretation easier. The proposed critical region localization algorithm effectively detects the regions relevant to language processing.	No statistical testing is performed. Therefore, the performance of the model cannot be evaluated properly. It is just shown, that the MAE is slightly lower compared to the baseline. In Table one, at least the standard deviation should be reported to display stability of the predictions. A box-plot with significant differences indicated would be best. The authors compare their approach against relatively simple regression models. Why where theses baselines chosen and not more advanced approaches such as random forests or neural networks? The authors missed to discuss their method and its limitations. It is also not discussed that classical along tract analysis also enables localization, albeit only along the tract. The authors do not correct the density of their tractograms, e.g. using SIFT. Therefore, tract densities will not be representative and some regions will be over- and others under-estimated. Incorrect definition of loss function: The definition of the loss function seems to be displayed incorrectly and should probably be changed to Loss=L_pre+wxL_ps It is not explained, why the authors introduced a weight for L_ps and why it was set 0.1	The motivation and evaluation of the study are limited in most clinical applications. The explanation from the results about neuroscience seems a bit far-fetched.	"The point cloud representation does not include shape information, such as curvature, critical points etc. Hence, the network may not be able to detect alternations due to subtle shape variations. The motivation of using a point cloud based model is not sufficiently justified by its novelty. This work may be better motivated if critical region localization on fiber streamlines is a main task of interest.  The discussions and conclusions drawn regarding the critical regions are weak based on the limited quantitative results of the critical region localization. These discussions can be reduced significantly.  The intersection region figure in Fig. 3 is not clearly presented. What do the three colors (green, red, yellow) mean? Besides, the surface of white matter-gray matter interface may be a more proper choice for visualizing the intersection.  In the caption of Fig.2, the synonyms are not properly written. Lpre -> L_{pre}, Lps ->L_{ps}, (MSE -> (MSE) The definition of Loss under Eq. (1) is wrong.  Minor grammatic errors can be found. For example, 4 lines above Section 4, ""our method is non-invasively localizing..."""	By using the openly available data from the Human Connectome Project and a brief description of the statistics of the subjects used, reproducibility seems to be possible. In addition, the settings and software framework are described in Section 2.5. The authors' answers on the reproducibility checklist confirm the given reproducibility. It would be beneficial, if the code would be provided on github.	It should be easy for authors to provide source code for reproducibility analysis.	This work is fairly easy to implement given the background on white matter analysis and deep learning.	The Lossfunction and Table 1 should be fixed, or explained in more detail. All in all one should explain the choice of tuned hyperparamters more deeply and discuss the applied method and its limitations in more detail.	How to keep the smooth information of a continue streaming when representation with a point cloud? Unclear why only left AF be used in this work. It is necessary to use both AF across hemisphere, or have a enough reason. Is there any way to automated select the best weight of difference loss w, and the number of input points? In table 1, the difference in MAE across methods looks very small, but significantly improved in r. It is very interesting. Needs to include more discussion about that.	This work may be improved by presenting more quantitative results on the critical region localization or considering a weaker conclusion about it.	The novelty of representing microstructure measurements of white matter trajectories as point clouds weighed most heavily in the decision to accept the paper. The performance of the method compared to the baseline cannot be evaluated properly, as no statics are published.	New issue in the dMRI field. Promising in clinical analysis.	The point-cloud deep network and the paired-Siamese loss are new for the task of neuropsychological score prediction. The network is simple and highly interpretable. The point-wise feature representation learned by the network is used to derive the critical region localization algorithm, which is shown to be able to detect the functional regions corresponding to the neuropsychological measure.
571-Paper1180	Whole Slide Cervical Cancer Screening Using Graph Attention Network and Supervised Contrastive Learning	The authors propose a cervical cancer screening method based on whole slide images. Specifically, they use graph attention network to aggregate the features from representative patches and use supervised contrastive learning to enhance the separability of graph representations.	The authors worked on Graph Attention Network and Supervised Contrastive Learning algorithms for the whole slide cervical cancer screening.	This paper proposes to utilize the relationships between suspicious patches in whole slide images for classification. Graph attention network describes and extracts connections among suspicious patches. A loss function is designed to enlarge latent distances for positive WSIs and reduce latent distances for negative WSIs.	A novel formulation that uses graph attention network and supervised contrastive learning in whole slide cervical cancer screening.	The study objective is interesting as the authors worked on cervical cancer screening. The manuscript has been structured properly. Comparative analysis showed that the proposed approach outperforms the existing approaches.	(1) Two graphs of the top-K and bottom-K patches are built to describe relationships between suspicious patches. (2) Loss function is designed based on cross-entropy loss and supervised contrastive learning. Thus, the latent distances for positive WSIs are enlarged, while the latent distances for negative WSIs are reduced.	Lack of strong evaluation to show the superiority over other whole slide cervical cancer screening methods.	The authors mentioned the importance of whole slide images, but there is no evidence of using whole slide images in their manuscript. The authors didn't include qualitative image-based results.  Hence, it is hard to decide if the analysis is correct or wrong. Technical novelty is none or limited. The authors worked on Graph Attention Network and Contrastive Learning. These approaches may be new to the healthcare domain but have been used in other image analysis platforms. Hence, the author's technical novel contribution is limited. In Fig. 2, two different types of images have been used. The image in Slide seems H&Es, which doesn't match with tile images. It will be better if the authors introduce their data first. Then show their qualitative results in whole slide images. There is no clue on how the screening will be performed. The t-SNE plot is not sufficient as a performance measure.  The manuscript lacks sufficient qualitative and quantitative evidence. The authors should share their source codes, trained models, and a small amount of test data for review.	Some parameter values are not justified. For exmaple, what is the criteria to determine the appopriate patch size? How to determine the number of top-k patches?	Very good	Not sure as there is no sufficient source code-related information available.	The workflow is introduced clearly in this paper, including patches extraction and ranking, graphs construction, graph attention network and loss design. Overall, the reproducibility is acceptable.	"In section 2.1, the sentence ""applying them to the classification model"" should be ""applying the classification model to the selected patches"". A sensitivity analysis on some key parameters should be provided, e.g., the number of the representative patches detected by RetinaNet and the number of patches used to construct graph. Would it be better to choose the top K and bottom K patches from all patches in a WSI than from the top 200 patches detected by RetinaNet. In a positive WSI, all the top K and bottom K patches may contain lesion cells, so it seems inappropriate to force the graph representations of the top K and bottom K patches to be far from each other. The authors should also directly compare their method with the whole slide screening methods in ref No. 2 and No. 20. In section 2.1, the sentence ""applying them to the classification model"" should be ""applying the classification model to the selected patches"". A sensitivity analysis on some key parameters should be provided, e.g., the number of the representative patches detected by RetinaNet and the number of patches used to construct graph. Would it be better to choose the top K and bottom K patches from all patches in a WSI than from the top 200 patches detected by RetinaNet. In a positive WSI, all the top K and bottom K patches may contain lesion cells, so it seems inappropriate to force the graph representations of the top K and bottom K patches to be far from each other. The authors should also directly compare their method with the whole slide screening methods in ref No. 2 and No. 20."	The manuscript lacks sufficient qualitative and quantitative evidence. Technical novelty is none or limited. The authors should validate their results by pathologists.	(1) Add details of data preprocessing about WSI, big patches and cell patches  (2) Final loss consists of cross-entropy loss and supervised contrastive learning. The experiment with only cross-entropy loss should be added for comparison.	The method is technically sound and intelligible.	The authors mentioned the importance of whole slide images, but there is no evidence of using whole slide images in their manuscript. The authors didn't include qualitative image-based results.  Hence, it is hard to decide if the analysis is correct or wrong. Technical novelty is none or limited. The authors worked on Graph Attention Network and Contrastive Learning. These approaches may be new to the healthcare domain but have been used in other image analysis platforms. Hence, the author's technical novel contribution is limited. In Fig. 2, two different types of images have been used. The image in Slide seems H&Es, which doesn't match with tile images. It will be better if the authors introduce their data first. Then show their qualitative results in whole slide images. There is no clue on how the screening will be performed. The t-SNE plot is not sufficient as a performance measure.  The manuscript lacks sufficient qualitative and quantitative evidence. The authors should share the	This paper proposes a graph attention network for WSIs classification. Relationships between suspicious patches are utilized. The loss function is the combination of cross-entropy loss and a loss based on supervised contrastive learning. The method proposed reaches the best performance compared with other methods. The clarity and organization are good in this paper.
572-Paper0898	Why patient data cannot be easily forgotten?	The paper discusses the problem of machine unlearning of patient-wise data from ML models. A targeted forgetting approach is presented and evaluated on cardiac MRI data (and compared to a computer vision application).	This paper addresses the problem of forgetting patient data in a DL model when for example patient consent is withdrawn. The problem is phrased as patient-wise forgetting, i.e. one patient (all images of the patient) is selected to be forgotten. They formulate two hypothesis: the patient's data is similar to other data (common cluster hypothesis) and the patient's data is different to other data (rare case, edge case). They show that the common cluster hypothesis holds often for computer vision data, while the edge case is more common in medical image data. They propose a new approach for forgetting edge-case patient data. The hypothesis and method is evaluated on CIFAR-10 and the ACDC dataset.	In this paper the authors propose an framework that can be used to forget a patient's imaging data from AI models. The proposed approach divides patient data in two categories: edge cases and common cases. The authors claim that the proposed framework outperforms related work when edge cases need to be removed/forgotten.	This is an interesting topic and challenging problem from a machine learning perspective. The paper provides good arguments and a sensible approach for patient-wise forgetting.	This paper addresses an important aspect of AI in healthcare applications, and is of great interest to the MICCAI community. The paper is well-written, well-structured and easy to understand. The distinction between common cluster data and edge case data seems to be important and the comparison between computer vision and medical datasets is very interesting and relevant. In general, the results and conclusions in this paper are convincing and relevant.	Intuitive approach to the patient forgetting problem. Albeit the empirical validation issues that are highlighted later, the proposed idea (separation of patients in common and edge cases) is intuitive and has the potential to be a useful contribution to the field once properly validated.	The experimentation is limited to a single medical application which suffers from the problem of limited data. While this might be a particularly challenging example, as only less than a hundred patients are used for model development, the more interesting real-world applications are ML models trained on large-scale data (e.g., chest X-ray disease detection). The paper would have been stronger if such an application would have been explored.	I would have liked to see a more in-depth discussion on the implications of the findings of the paper. Is forgetting patient data a realistic method to deal with withdrawn patient consents? What is more important: respect data protection or ensuring model performances? I'm wondering about the correct definition of when a dataset is correctly forgotten. Tabel 1 suggests that data is forgotten if the accuracy is 0.0. But isn't an accuracy of 0.5 (random classification) better suited?	"The main difference of this paper compared to [9] is that the patient data is divided is two categories: edge cases and common cases. It is unclear how prevalent this division is for medical imaging data. The authors justify this with experiments in one dataset and even in this one dataset, no confidence bars are computed. With this level of empirical validation it is hard to assess the practical impact of the proposed method. In the medical imaging dataset that was used, there is a large discrepancy between the test set performance (reported as 0.19 error) and the results reported in Fig 3. that show that ""By considering a threshold of 50% on the error of the golden model, we find that > 60% of patients in ACDC can be considered to belong to the edge case hypothesis."". The big difference in between these numbers and the test set performance indicate that the model training may have not been done properly (possible overfitting issues). This raises some additional concerns about the generalizability of the empirical results. As a general remark the authors should include error bars in all their reported results, following the standards of paper [9]. For the medical imaging datasets, the authors should use an NN architecture that has close to state-of-the-art performance. It is unclear whether this is the case with the architecture that was employed."	Likely to be reproducible.	The paper evaluates the method on publicly available data.  Information on the dataset split and training process is provided. The method is sufficiently explained.	Sufficient information to reproduce the results is provided.	"Part of the problem of the presented application is the limited data, which makes almost every case an edge-case. It is unlikely that clusters emerge on high-dimensional imaging data when only a hundred samples are available. Neural networks will likely overfit to such data (they may still interpolate well in-between), but that means that the hypothesis of separating edge cases from cluster cases may not be valid. A more interesting application for this to be tested would have been image classification trained on 100k+ images (e.g., chest X-ray disease detection). I am not an expert on privacy and newer regulations such as GDPR. However, the described use case of an individual whose data would need to be removed from a trained ML model seems unlikely to be a legal requirement. The so called 'right to be forgotten' does not seem to apply here (see https://gdpr.eu/right-to-be-forgotten/). I would think that a trained ML model falls under the exemption stating ""The data represents important information that serves the public interest, scientific research, historical research, or statistical purposes and where erasure of the data would likely to impair or halt progress towards the achievement that was the goal of the processing."" With this in mind, while the paper is thought provoking and stimulating, I am unsure about its practical relevance. In particular, ML models for production are typically developed on (fully) anonymised data where data privacy regulation such as GDPR does not apply."	This is a well-written paper describing and addressing a very important aspect for AI in healthcare applications. The paper is easy to follow and the findings are relevant to the MICCAI community. I would have liked to see a more in-depth discussion on the implications of the findings of the paper. Is forgetting patient data a realistic method to deal with withdrawn patient consents? What is more important: respect data protection or ensuring model performances? What is the connection to differential privacy and can we learn something from it? If I interpret Table 1 correctly, a patient's data is forgotten if the accuracy on this data is 0.0. I would assume that the model does not have any knowledge about this data of the classification accuracy is 0.5 (random). An explanation/definition is missing here. A minor comment is that the classification error is defined too late (only in the caption of Table 1). It should be introduced earlier in the text.	"This paper presents a framework for the patient forgetting problem that is an intuitive extension of what has been proposed in the literature, however the empirical validation is insufficient to illustrate it practical usefulness. Detailed recommendations for improvement are provided in the ""paper weaknesses"" part of the review."	I may argue for accepting the paper because it is thought provoking and may stimulate interesting discussions.	The paper addresses an important aspect of AI in healthcare and is of great interest to the MICCAI community. Although I see some room for improvement, I recommend acceptance.	"Weak empirical validation. Detailed recommendations for improvement are provided in the ""paper weaknesses"" part of the review."
573-Paper0475	XMorpher: Full Transformer for Deformable Medical Image Registration via Cross Attention	This paper proposed a full transformer architecture to extend the cross-attention transformer to establish the attention mechanism between images for the multi-level semantic correspondence.	The authors introduced a parallel transformer backbone, XMorpher, for image registration task. Different to current CNN-based registration network which moving-fixed image feature fusion first or fusion last, the proposed method utilized a cross-attention block to fuse the moving-fixed feature in multi-level progressively.	The paper proposed a transformer architecture called XMorpher for DMIR. XMorpher includes dual parallel network to extract image features of the fixed and the moving image. Then, at each level the Cross Attention Transformer (CAT) blocks compute the mutual relevance between extracted features and match the corresponding regions to obtain the fine DVF. Results show some improvement compared to other methods.	This work extended the Cross Attention Transformer (CAT) for communication between a pair of features from moving and fixed images, promoting the features matching for image registration.	The idea is fine, and their experimental result shows the proposed method is efficent.	The concept of dividing XMorpher into feature extraction part and correspondence matching part using Transformer framework is valuable. The proposed CAT block advances the feature communication between the fixed and the moving images in a multi-level semantic scheme. The results show the improvement of efficiency and accuracy of the XMorpher.	The discussions on the existing deep registration and transformer-based registration models are inappropriate. There is a lack of architecture descriptions of the proposed full transformer-based registration model and the integration with existing registrations models.	Some of the comments the author express on existing methods are subjective. For example, I don't agree with their comments on the CNN-based registration methods including fusion-first and fusion-last, because the registration network is to learn the spatial deformation between the paired images, thus, the moving-fixed features extraction and matching should not split, while the authors claimed these two steps should split. Please list more experiment or reference to support your point. The proposed XMorpher seems like require a huge training dataset, thus, the generalization seems like questionable. I think the large deformation might be limited by window size if you remove the affine transformation?	Figures and their captions need to be improved. And more information can be acquired in Q8.	This paper has provided details about the models, datasets, and evaluation.	The authors claimled they will release their code publicly.	Good.	It is interesting to classify the existing registration methods as Fusion-first, Fusion-last, and cross-attention-based fusion. The authors claimed that the first two categories of methods failed to find the one-to-one correspondence between images. However, existing deep registration models, such as the diffeomorphic variant of the VM, were feasible to find the invertible registration fields. The transformer has been applied in image registration and correspondence tasks in the last two years, such as [4, 20]. The images patches from both the fixed and moving images are fed to the transformer, so the existing methods [4, 20] did not just compute the relevance in one image as claimed by the authors. It is unclear how to compute DVF \phi by the proposed XMorpher. As shown in Fig. 2, the Concat+Conv operations were required to compute DVF. Does it mean a CNN-based decoder was used to compute the registration field? In Fig. 3, all compared methods achieved reasonable deformation fields with the organ contours consistent with the fixed image. We noticed that the proposed approach achieved smooth organ boundaries. It would be helpful to discuss the scheme in the proposed approach contributing to the smooth boundary. It is unclear how to apply the XMorpher on the existing registration network of the VM or the PC-Reg. The VM utilized the U-net-based framework with the convolutional encoder for feature extraction and the decoder for the DVF. It would be helpful to discuss whether the VM-XMorpher used the CNN-based feature extraction and the field inference.	"1) I like the way the authors represent in Figure 1, but the authors should provide more evidence / reference to support what they claimed. 2) In Fig. 2, did you predict two deformation field? or just one? If you only predict one deformation field, you must fuse the moving-fixed features in up-decoder block, thus, your method should not be named X-shape, it actually is Y-shape. If you predict two deformation field, you cannot denote ""moving and fixed image"" as your inputs. Please give more details here. 3) The proposed XMorpher seems like require a huge training dataset, thus, the generalization seems questionable. Can you give more quantitative number like minimum required training dataset, network parameters number, and computing efficiency(FLOPS)? Besides, I think the large deformation might be limited by window size if you remove the affine transformation? 4) In Table1, why the model without cross block achieved the best performance on Jacobian metrics? Please discuss. Besides, your network is parallel which utilize cross block to fuse moving-fixed features. If you remove cross block, how does the model achieve registration task? Like comments 2), you fuse moving-fixed features in up-decoding block? If yes, your network is in Y-shape."	1, The figures (especially Fig.2) in paper need to be more clearly, and I list some advices in follows: The texts used in Fig. 2 is too small to read that I have to zoom in 300%. You should reorganize the layout your figure. Both in Fig.1 and Fig.2, the moving images and the fixed images seems are two different modality which is a little bit confusing. In section 2.2, you mentioned input 'b' and input 's', however, they are missing in Fig.2. Besides, the notations used in the whole text should be clearly defined and consistent, for example, typo error that in Section 2.3 'and thus S_ba has size of nxa * hxb *w xg * d', where I think 'S_ba' might be 'S_se'. Four figures in Fig.4(b) are lack of explanations. What are these pictures? Fixed image? Warp Image? or ...? And, what the arrows and the overlap map represent? 2, In your experiments, you applied the XMorpher as backbone in two CNN-based frameworks, VoxelMorph and PC-Reg. I don't really understand this because you said XMorpher is a full transformer structure, and I hope to get more detailed information about the implementations. Also, you did not introduce how you acquire the final dvf in your article which is important.  3, You claimed that your method is more efficiency, please report your inference time. 4, There are many learning-based methods fuse features in multiple level and predict the dvf in multi-scale manner. The works will be much more solid if you can make some comparisons with these methods.	The proposed full transformer architecture utilized the dual parallel feature extraction networks, which exchanged information through cross attention, discovering multi-level semantic correspondence for effective registration. The proposed XMorpher has shown performance gains over existing deep registration models in DMIR.	The idea and results are satisfactory.	The paper has a certain innovations, and the experiments also show the improvement of the proposed method compare to baseline models. Although some weakness exists, it can be accepted after correction.
574-Paper1063	Y-Net: A Spatiospectral Dual-Encoder Network for Medical Image Segmentation	-- The paper proposes a novel framework for combining spectral and spatial features for OCT layer and fluid segmentation. -- The method is evaluated on public dataset with performance gain of 1.9% over other methods.	"The paper proposes an end-to-end conditional OCT layer segmentation network, which actually is an '""U-net"" like model  with an additional Fast Fourier Block branches. Furthermore, the segmentation experimental result does reach the state-of-the-art performance. "	The authors proposed an architecture for  retinal OCT segmentation, which was consists of a spatial encoder, a spectral encoder and a spatial decoder.   The proposed method was evaluated on a public dataset, experimental results showed that proposed model outperforms existing models in fluid segmentation retinal layer segmentation.	-- The main idea of the paper to combine both the spectral and spatial features extracted from the OCT volumes for segmentation tasks. -- The authors present the dice for the proposed method in relation to the results on public dataset. -- Ablation study on different setting of the proposed framework strengthens the method. -- The paper is well written and easy to follow.	The paper seems to be orgainized well. And all training details are well adressed.	-The paper is well written and organized.	There are some minor questions that could help better understand and justify the contribution of the work -- Since there is global and local features extracted from the spectral encoder, how important/good is the segmentation without the spatial path? in the  is the depth of the network chosen in terms of applying CNN/GCN network? A U-Net is only a spatial model and Y-Net with only spectral model can help to highlight the spectral path -- Was any data augmentation used for this work? U-Net requires data augmentation to learn the data distribution and could gain some performance with this. -- The numbers for fluid segmentation is very less for RelayNet compared to the results reported in the paper. The authors of RelayNet report performance >0.75 dice for the fluid region.  -- When the spectral features range is altered, how does the performance correlate to the SNR of the OCT images?	Overall, it is lack of novelty, and its experiments are far more satisfied.	-In this paper, the authors have used only one dataset to evaluate the proposed method.  For an architecture paper, I think that additional datasets should be used to evaluate the generality of the model. -I think the design of the comparison experiment is not reasonable.  Compared to U-Net, the proposed method has an additional encoder. How can we determine whether the performance improvement is due to the extraction of spectral domain features or to the increase in the number of parameters? -The novelty and contribution of this paper are limited.	-- The authors are providing the code in the supplementary materials. -- Providing the requirement and the hyper-parameters for the framework in the paper is a plus.	The network is not too complicated to reproduce. The dataset is open access.	Good	-- Adding couple of lines on why x_g = 0 for the initial block will be helpful. -- What is the number of learnable parameters in the proposed framework? The spectral domain part is proportional to the dimension of the input image? -- Devil advocate: What could potentially downplaying the performance when skipped connection are added between the the two domains?	Comments: I): The experiments are not sufficient. Only U-net is included in the experimental comparison, which makes me doubt whether it is an ablation study or methods comparison.  II): The dice score has many advantages on training a model by materializing it as a loss. And it could be utilised as an excellent metric in segmentation. However, a network trained by dice loss should be evaluated by not only dice but also other metrics. For instance, M-IOU also is a popular segmentation metric which could be involved in the experiments. III): The work lacks novelty. FFT blocks can not be considered as a newly implemented technique for segmentation, while there is no other claimed contribution for methods.	The computation cost is not analyzed. It is better to provide the computation cost for each method in this paper. -The author mentioned that spectral encoder can make the model pay more attention to high-frequency information. How to prove this and whether the corresponding features can be visualized? -If Spectral Norm is replaced by a general convolution operation, what will be the impact on the results?	-- The authors evaluate a the proposed method with proper comparison to existed methods in the literature. The validation in the paper justifies the contribution of the proposed framework.	The	The novelty of the proposed method and rationality of experiments.
