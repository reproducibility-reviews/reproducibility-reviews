id	category	title	review1	review2	review3
001	contribution	3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images	Paper introduces a methodology to segment 3D vessels. The novelty part of this methodology is optimizing the process of getting expert annotation, i.e. taking advantage of depth information with the aim of requiring expert annotation from a single 2D slice instead of the annotation of the whole volume for model training. Specifically, expert annotation of a single randomly selected 2D slice is mapping back to the 3D space via the depth of the maximum intensity projections (MIP), which becomes the input of a 3D U-Net. Application of this methodology is demonstrated on the task of peripancreatic vessel segmentation on CT images (n=141). Performance is computed and compared with baseline (input images to the U-Net: 3D annotations), obtaining competitive performance. Authors demonstrated statistical significance between using and not using depth information.	The paper presents a novel approach to the automated segmentation of blood vessels in 3D medical images, addressing the challenges of manual annotation in 3D vessel segmentation. The authors demonstrate that it is possible to achieve accurate 3D vessel segmentation by annotating a single MIP per training image. This method substantially reduces the annotation effort required. They also show that the best annotation strategy is to label randomly chosen viewpoints, which minimizes the annotation cost. Additionally, by incorporating depth information obtained from 2D annotations, the authors almost close the gap between 3D supervision and 2D supervision. The potential impact of this work in the clinical environment is significant, as it can facilitate better visualization, assist in surgery planning, and enable the computation of biomarkers and other downstream tasks. This can ultimately lead to improved diagnosis and treatment of various diseases.	The authors present a novel approach for 3D segmentation of peripancreatic arteries using sparse 2D annotations. Their method, which leverages single random orthogonal 2D annotations for each training sample along with additional depth information acquired at no extra cost, achieves nearly the same accuracy as fully supervised models trained on 3D data while significantly reducing annotation costs.
001	strenghts	3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images	It is interesting the novelty strategy to reduce the annotation effort to get vessel segmentation by using a single 2D slice and depth information. A independent subset of data is used to test the proposed methodology It is notable the exhaustive experimentation by varying the input information to the 3D U-Net (using 2D annotations, using 3D annotations, not including depth information, including depth information, different fixed viewpoints, different data size). Authors include a good background review, taking into account two main points: Learning from weak annotations and incorporating depth information	The main strength and novelty of this paper lies in the fact that, despite the time-consuming and economically burdensome nature of manually annotating 3D vessels, the study demonstrates the possibility of achieving performance comparable to 3D annotation-based algorithms by annotating single MIP views in 2D. The research demonstrates that it is feasible to segment the full extent of vessels in 3D images with high accuracy using only a single randomly selected 2D projection per training image. This approach significantly reduces the annotation effort and provides additional depth information from 2D annotations, exhibiting performance that is almost comparable to a 3D supervision-based approach.	Interesting idea that use only one annotated 2D projection per image for segmentation task
001	weakness	3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images	Lack of clarity: authors described that the dataset contains binary 3D annotations of the peripancreatic arteries carried out by two radiologists. However, it is not clear if authors used one or the two expert annotations. In case both were used, the next question is how did authors use both annotations to compute the dice score to evaluate the obtained arterial segmentation? Not clear how authors probed random selection of 2D slices was carried out, i.e, How was the 2D slice selected? authors said randomly, but ?did they test the use of different slices and obtain the same results? Abstract does not mention the used type of medical imaging and it falls short in describing methodology (just one sentence).	"Although the author contends that by incorporating additional depth information obtained from 2D annotations at ""no extra cost,"" obtaining additional depth information appears to require a considerable amount of manual or semi-automatic effort, such as: Automatically or semi-automatically extracting objects (like vertebrae and ribs) with similar intensities as arteries, which occlude the vessels along the depth direction. Deriving vessel boundaries in the depth direction based on image intensity. Decomposing contrast materials in the depth direction based on intensity in contrast-enhanced CT scans is expected to be quite challenging, making it difficult to ensure reproducibility. Moreover, it is necessary to verify whether performance can be guaranteed when noisy depth estimation is present. Obtaining such depth information presents challenges, including the additional computational load required for forward-projection. Moreover, according to Table 1, the performance difference between 1VP+D vs random 1VP-D in terms of Dice is not as significant as the performance difference between 3D vs. 1VP+D, which limits its clinical feasibility."	The clarity of this manuscript could be further improved as it is a bit difficult to connect Figure 1 and the main content in Section 3. Figure 1 is also confusing, e.g. I see two MIP projection blocks and arrows pointing to different blocks and not sure I understand what that means. Some expressions need to be further corrected, e.g., 'MIP projections' should be just 'MIP'. I was expecting to see results comparing the proposed method with other competing methods, but only the same method with different settings were reported.
001	repro	3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images	Within supplementary material authors provide information about preprocessing steps and network architecture.	This study suggests that the reproducibility of performance can be ensured when the following two accurate annotations are secured: Depth information: Even in spectral CT, decomposing contrast material in contrast-enhanced CT is a challenging task, and the intensity of the contrast becomes blurred and faint over time. Therefore, it is expected that many voxels with similar intensity to the contrast material will be encountered in noisy contrast-enhanced CT images depending on the image, and substantial manual work will be required to remove them, which may significantly impair reproducibility. 2D annotations: While this study obtained 2D annotations by performing forward-projection from 3D annotations, securing the quality of 2D annotations in this study might not be easy if 2D annotations were performed from the beginning. For instance, as shown in Fig. 2 (g), directly performing 2D annotation would make it difficult to accurately segment small, separated contrast voxels. If the 2D annotations are noisy, the reproducibility is likely to be compromised as well.	Didn't see the code or link contained in the submission, not sure how to comment on reproducibility.
001	detailed	3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images	"Be careful about the use of expressions like ""excellent"" segmentation performance. I suggest adding references when authors describe the importance of blood vessels segmentation in medical imaging. Make the correction of the year for reference 5. I suggest to not only run 1 iteration of 5-fold, at least 5 iterations, to avoid a bias due to data partitioning. Compute Dice score between the two expert annotations could provide valuable information about the inter-reader variability and the relevance of making this task automatic. It would be interesting if authors compare the obtained results with the results obtained by Kozinski et al., the unique reported publication (conference paper) which has been reported results on the same task (peripancreatic vessel segmentation). It would be useful if authors are more specific in the main paper to reference a particular table/figure in the Supplementary Material instead of referencing the whole document as ""can be found in the supplementary material""."	Considering the complex 3D structure of arteries, it is possible that some arteries may be occluded by others in the depth direction. In estimating the depth, if a significant portion of arteries is obscured by other arteries or objects with similar intensity, please comment on the feasibility of ensuring segmentation accuracy for the obscured arteries (i.e., those without front surface information). It would be beneficial to have an analysis of performance variations depending on the sparsity or noise level of depth information. The authors have successfully demonstrated that the algorithm's performance is maintained even when randomly selecting a VP. However, it is expected that there would be performance differences depending on the VP angle. Suggesting an optimal VP angle could be beneficial for ensuring reproducibility and practicality. [Minor] In Table 1, it is difficult to interpret the table contents based solely on the caption; thus, it would be helpful to provide additional clarification in the caption description. For example, it is not clear how the '3D' experiment and '3D' model differ from each other.	It would be good to compare this method with other benchmark methods and improve the clarity of the descriptions, might be good to update a clear diagram that connect Section 3 main components with the current Figure 1.
001	justifictation	3D Arterial Segmentation via Single 2D Projections and Depth Supervision in Contrast-Enhanced CT Images	Although the paper has some weaknesses, these are mainly related to lack of clarity and some experiments suggestions to be carried out to make the work stronger. In spite of these limitations, paper introduces a novelty strategy to optimize the process of getting expert annotation to automatically segment 3D arterial structures, i.e., taking advantage of depth information with the aim of requiring expert annotation from a single 2D slice instead of the annotation of whole volume for model training.	As mentioned above, the method demonstrates originality; however, the performance improvement (+D vs -D) is not substantial, and the implementation appears to require non-negligible preprocessing or manual image processing efforts. Reproducibility and the assumption of uniform intensity in contrast may be difficult to generalize, leading to a recommendation of a weak accept.	The clarity and the results may need some further major changes before the work can be published, which I don't think can be done within the rebuttal period.
002	contribution	3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer	The authors introduced an novel semantics-based feature learning approach that effectively utilize the semantic information and captured both local and non-local dependencies using a graph-transformer. Additionally, they conducted adaptive feature aggregation of cross-domain features to achieve precise cell-wise 3D dental mesh segmentation results.	The authors propose a graph transformer-based feature learning network that decouples position and direction features and allows to perform segmentation of dental meshes. The method benefits from additional semantic predictions. The method is trained and tested on 200 dental meshes and compared to four other SOTA approaches to show superior results.	This paper proposed a 3D dental mesh segmentation method for teeth and gums with following contributions: (1) Take the semantic information related to different teeth spatial positions into account, and the extracted semantic information is fully utilized in the task, reducing the segmentation error of teeth boundaries. (2) For existing methods that only focus on local features, the results of ambiguous region segmentation are limited. This paper uses both local and global information to improve segmentation performance.  (3) Adopt adaptive weight fusion to fuse features from different domains, resulting in better segmentation performance. (4) Exceed the recent one-stage dental mesh segmentation SOTA methods in metrics of overall accuracy and mIoU.
002	strenghts	3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer	(1) They introduced a novel semantics-based feature learning approach that effectively utilized semantic information to enhance local and global features. (2) They designed a new feature fusion module to obtain global dependencies in each domain, further learning semantic information and adaptively fusing cross-domain features.	The method in particular the changes to previous methods seems to be interesting and outperforms the TSGCNet. It seems that it is well suited to create teeth segmentation with accurate borders which I believe is beneficial for the application.	This article fully utilizes the semantic information of cells and normal vectors from input images, explores the spatial differences of teeth in different positions, and combines local and global semantic information to improve the accuracy of segmentation. To learn abundant semantic information, this article predicts the pseudo labels of high-dimensional features extracted by Graph-Transformer. Each cell is classified by softmax and MLP, and then maximizes the features between cells of different categories. In this way, it can make the network easier to learn classification related features (including location, shape, etc.), and can inhibit the occurrence of overfitting to a certain extent, which makes it easier for the neural network to capture rich semantic information. At the same time, the extraction of semantic information also combines feature vectors of different scales to ensure that features of different scales can directly participate in the prediction of segmentation results. In the process of generating pseudo labels, the author also fused the information of C-domain and N-domain to make the generated pseudo labels more accurate. At the same time, in the process of feature fusion in different domains, the author used an adaptive method to fuse features from different domains based on learnable weights. This method allows the network to automatically filter out valuable features for segmentation results while suppressing the expression of irrelevant features. The method proposed in this article is like the self-supervised learning method, which allows the network to learn valuable information from the inputs themselves without labels. At the same time, combined with supervised segmentation tasks, the network can converge faster and achieve higher accuracy.
002	weakness	3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer	Many methods based on voxelization have achieved satisfactory results in the mesh segmentation domain, such as PV-RCNN, VoxelNet. However, the authors did not compare their approach with these methods in the experimental section.	The authors could have provided a more detailed evaluation. For example examing the different teeth IOU or identify failure cases.  Furthermore, the method description lacks some details in the beginning i.e. regarding input definition (C and N size) and the STN and kNN. The introduction should be clearer without having to read the TSGCNet. The approach is said to perform significantly better, but this should be backed up by statistical tests.	There are many unclear descriptions in the overall method and the combination of figures and text in this article. (1) For the pseudo label prediction section, the author did not clearly describe the process of using pseudo labels. The author introduced the process of generating pseudo labels and then maximized the differences between different categories of cells but did not provide a detailed explanation of the process of how to maximize them. (2) In the formula for calculating semantic distance, the author used predicted pseudo labels, but did not consider the possibility of inaccurate pseudo labels during the initial training stage. The author did not explain this and provided a more reasonable solution. (3) Figure 1 in the article is the overall structure of the framework. However, the semantic prediction, graph transformer, and adaptive feature fusion mentioned by the author did not show any details, and the flow direction of the data in the figure lacked arrow guidance, which is to some extent not conducive to readers' understanding.  (4) In the experimental section, the author conducted ablation experiments on the three main modules in the network but did not specify how the control group for the ablation experiment was set up. For example, if the adaptive feature fusion module is abandoned, how does feature fusion proceed? Meanwhile, which structure replaced the graph transformer, which is the main structure of the framework, in the ablation experiment?
002	repro	3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer	the reproducibility of the paper is very good	There are only mean errors/IOU etc provided, without standard deviation, error distribution nor statistical tests. There is no code provided and no data. Hyperparameters for reproduction are provided. It is unclear how easy the additions to the TSGCNet can be re-implemented. Ablation study to examine the effect of different components is provided.	Firstly, the article will make the code publicly available after publication, but there is no mention of publicly available datasets. According to the article description, readers can easily implement the overall framework, and each module has reference articles. The author provides an introduction to the TSGCNet method for extracting C-domain and N-domain from input images, as well as how to fuse the information of these two domains, how to fuse features of different scale features, and the process of generating pseudo labels. The author also mentioned the loss function, GPU configuration, training epochs and other relevant information in the implementation details. But for some hyperparameter and details in the network, the author has not explained in the article (maybe it can be directly obtained from the code), such as the l, the way to maximize different cells, encoder parameters, etc.
002	detailed	3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer	The authors should compare their approach with some voxelization based methods in the experimental section, such as PV-RCNN, VoxelNet.	The paper does not provide any discussion of failure cases or limitations of the current approach although I assume that it also has some problems (since the IOU is not perfect). Are there cases where the approach fails/ has problems? All examples given are selected because they highlight the proposed approach. Only the mean error value is given without any standard deviation or min/max Can it be evaluated tooth by tooth to see the differences? This was done for the TSGCNet. Can you comment why you did run all SOTA methods with 200 epochs instead of until convergence? This should be justified in the results since there is the possibility that the other approaches converge later than the proposed. It is mentioned in the text that opposite teeth have the same labels but on the figure they still have different colors? N+B, why 24? Some more explanations/defintions on the TSGCNet should be provided in order to improve comprehension. How do the methods compare with regard to memory and time?	The author can supplement the description of the method in details, while also reflecting the main components in the figure: (1) How did the author use the pseudo labels predicted to make a greater difference between cells of different categories. In the initial stage of training, whether the generated results of pseudo labels can be directly used to calculate semantic distance, the author should provide further descriptions and explain the relevant principles. The author can further explain the differences between the training and testing parts about the semantic prediction part. (2) Although figure 1 is the overall workflow structure, the author should also try to indicate the details as much as possible, especially for the semantic prediction, graph transformer, and adaptive feature fusion modules mentioned in the article. The author should provide a detailed graphical representation to facilitate readers' reading and reproduction. If space is limited, author can also use additional figures as a supplement to describe the main components in detail. In addition, the author should include corresponding arrows in the figure to make it clearer for the reader. (3) In the ablation experiment section, the author should clearly describe the settings of the control groups. When the semantic prediction, graph transformer, and adaptive feature fusion modules are not used, the author should provide detailed explanations on how the network achieves feature extraction and fusion. (4) The author should highlight the key point. The introduction and conclusion sections repeatedly mention the full utilization of global and local information, but the methods section does not provide a detailed description of the processing of global information, only through a global graph transformer block. (5) If possible, add more robustness experiments.
002	justifictation	3D Dental Mesh Segmentation Using Semantics-Based Feature Learning with Graph-Transformer	They desigend a novel semantics-based feature learning approach that effectively utilized semantic information to enhance local and global features  and  for the mesh segmentation. They also designed a new feature fusion module to obtain global dependencies in each domain, further learning semantic information and adaptively fusing cross-domain features.	I think it is a good paper overall but the evaluation has some flaws and no limitations are discussed although the IOU is still not perfect. On the other hand I appreciate that the method seems to manage displayed difficult cases much better. But maybe the other approaches would perform better when being trained until convergence. If the evaluation design is improved and some more descriptions added, I would be more confident to accept this paper.	(1) This article proposes a pseudo label prediction module (semantic prediction), which makes it easier for the network to focus on valuable features through the way like self-supervised learning. Although there are some missing details, such as the use of pseudo labels, the concept is novel, and can also prevent the network from overfitting, which can accelerate the convergence of the network. (2) This article effectively combines the spatial features and topological structure of teeth by embedding N-domain into C-domain, while aggregating multi-scale features to improve the accuracy of segmentation. (3) The Transformer based graph network and feature fusion modules used in this article effectively combine global and local features. Although there is a lack of innovation in them, ablation experiments have shown that the introduction of these modules can improve accuracy and surpass SOTA methods.
003	contribution	3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching between 3D and 2D Networks	This paper targets reducing the amount of annotation required for medical image segmentation. Existing approaches have you sparse annotation meaning large glass gaps to preserve slash reduce rater involvement this paper proposes a framework that robustly learns from sparse annotation. Moreover, it focuses on teacher networks for both three dimensional and two dimensional approaches. Results are shown to be comparable to fully supervised approaches.	This paper demonstrates a method on 3D segmentation with sparse annotation via cross-teaching between 3D and 2D networks. This is a practical problem setting in medical imaging field. It 1) trains 2D segmentation network with sparse 2D annotation and fuses 2D segmentation results in different views into 3D pseudo segmentation masks; 2) trains 3D segmentation network with sparse 2D annotation; 3) the prediction of 2D and 3D networks are used as pseudo label for the other network after selection.	This paper presents a novel framework for effective learning from sparsely annotated data, utilizing cross-teaching of 3D and 2D networks. To account for the unique properties of these networks, this paper devises two pseudo label selection techniques: the hard-soft confidence threshold and consistent label fusion. Through experimental evaluation on the MMWHS dataset, the proposed method outperform existing state-of-the-art techniques.
003	strenghts	3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching between 3D and 2D Networks	Prior contexts include image level bounding box scribble and point wise annotations. The key area of the field is weak label and weak supervision in deep learning training. The key limitation is the existing performance gap between supervised and semi supervised slash weakly supervised methods. The work follows up on reference number two. Reference to was published in 2020, and IEEE Journal of biomedical and health informatics for 3d image segmentation with sparse labels. Both references to and 10 train on sparsely labeled data through slides gaps registration models have been key to their prior performance. This work expands preference 13 By adapting different network dimensions The key linkage is utilizing both three dimensional and whodunit jewel networks to reduce pseudo labels, which are then used to cross train ad hoc strategies are introduced to balance two dimensional and three dimensional training and two dimensional three dimensional uncertainty. The methods appear effective, but the theoretical underpinnings of these approaches are not well explored. Traditional v net and unet backbones are used for the three dimensional and two dimensional networks respectively. An effective set of baselines including mean teacher, uncertainty aware mean teacher cross pseudo supervision and crossed teacher between CNN and transformer approaches are offered as baselines the transformer network is based on unit R. All methods are evaluated with 16 labelled slices per volume. Interestingly, the results outperform a fully supervised v net The ablation experiment  shows a reasonable dependence on the specified parameters but no overt sensitivity.	1) The problem setting is practical 2) The method is novel	(1)  3D CNNs can capture inter-slice relationships and 2D CNNs are efficiently for  inner-slice information. The 3D and 2D network can benefit from each other. (2) The  pseudo label selection strategies are novel. For the 3D network, a hard-soft threshold is used to estimate the quality of predictions and select high-quality ones as pseudo labels. Those exceeding the hard threshold are used to supervise 2D networks. For 2D networks, consistent predictions of two 2D networks are used instead of calculating uncertainty.
003	weakness	3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching between 3D and 2D Networks	In terms of constructive criticism, the primary concern is the ad hoc nature of the proposed optimization criteria. Demonstration on a single test case is effective but introduces concerns relative to generalizability of the approach, and generalizability of the ad hoc rules. A second example, even a second toy example in a different context would greatly improve stability and assurance of the reproducibility of these results.	The major concern is on the experiment section. For MMWHS dataset, the data split of 12/4/4 gives results for only 4 testing samples. It is difficult to prove any improvement with such a small testing set. It would be necessary to perform k-fold testing or experiment on more datasets.	(1) Lack the results of fully supervised 2D (U-Net) for comparison. (2) Lack of analysis why outperform Fully-supervised V-Net. (3) More ablation studies of Labeled Slices like 8 would make this paper better.
003	repro	3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching between 3D and 2D Networks	Methods are clearly presented. No concerns.	No code provided	Through Implementation Details.
003	detailed	3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching between 3D and 2D Networks	In terms of constructive criticism The primary concern is the ad hoc nature of the proposed optimization criteria. demonstration on a single test case is effective but introduces concerns relative to generalizability of the approach, and generalizability of the ad hoc rules. A second example, even a second toy example in a different context would greatly improve stability and assurance of the reproducibility of these results.	The major concern is on the experiment section. For MMWHS dataset, the data split of 12/4/4 gives results for only 4 testing samples. It is difficult to prove any improvement with such a small testing set. It would be necessary to perform k-fold testing or experiment on more datasets.	please refer the weaknesses
003	justifictation	3D Medical Image Segmentation with Sparse Annotation via Cross-Teaching between 3D and 2D Networks	Overall this paper is interesting, the results are promising. The explanation of ad hoc solutions is clear. And the organizational structure of the paper leads to a reasonable level of reproducibility The lack of a second dataset is slightly problematic. However, the extension is not absolutely essential for a publication in this venue.	The method is novel	To further explicate the proposed methodology's superiority over the fully-supervised model, it is imperative to conduct an in-depth analysis of the underlying mechanisms of the 2D network's functionality.  Additionally, conducting extensive experimentation with various labeled slices would strengthen the findings of this study.
