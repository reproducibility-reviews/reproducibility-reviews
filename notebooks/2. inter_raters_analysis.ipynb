{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inter Raters Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import random\n",
    "from pathlib import Path\n",
    "import os\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa, aggregate_raters\n",
    "import datetime\n",
    "import json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from statsmodels.stats.inter_rater import fleiss_kappa, aggregate_raters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_review_tuple(path_review_1: Path, path_review_2: Path) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Read the TSV files into Pandas DataFrames.\"\"\"\n",
    "    reviews = [pd.read_csv(path, sep = \"\\t\", index_col=False, header= None) for path in (path_review_1, path_review_2)]\n",
    "    reviews = [df.fillna('0') for df in reviews]\n",
    "    return tuple(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to the tsv file with the rating from the reviewers\n",
    "path1_tsv = Path(\"../human_rating/rating_90/rating_90_O.csv\")\n",
    "path2_tsv = Path(\"../human_rating/rating_90/rating_90_E.csv\")\n",
    "df_ratings = get_review_tuple(path1_tsv, path2_tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of categories you want to make statistics for\n",
    "CATEGORIES = [\n",
    "    \"Models and algorithms\",\n",
    "    \"Datasets\",\n",
    "    \"Code\",\n",
    "    \"Experimental results\",\n",
    "    \"Error bars or statistical significance\",\n",
    "    \"Code is or will be available\",\n",
    "    \"Statement\",\n",
    "    \"Comments\",\n",
    "]\n",
    "\n",
    "N_REVIEWS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_dataframe(list_stats: list[str]) -> pd.DataFrame:\n",
    "    \"\"\"Initialize an empty DataFrame with the specified row and column indices.\"\"\"\n",
    "    return pd.DataFrame(\n",
    "        index=pd.Index(CATEGORIES + [\"Meta-categories\", \"Repo provided\"]),\n",
    "        columns=pd.Index(list_stats),\n",
    "    )\n",
    "\n",
    "df_final = initialize_dataframe([\"kappa score\", \"ci low\", \"ci high\", \"se\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_confusion_matrix(list_1: list, list_2: list) -> pd.DataFrame:\n",
    "    \"\"\"Create a confusion matrix for comparing two lists of attributes.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    list_1: list\n",
    "        First list of attributes.\n",
    "    list_2: list\n",
    "        Second list of attributes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame :\n",
    "        The confusion matrix showing the counts of matching attributes.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    - If the lengths of list_1 and list_2 are not equal, a message is printed indicating potential mismatch.\n",
    "    - The confusion matrix includes row and column totals, providing a comprehensive summary.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    - create_confusion_matrix(['A', 'B', 'A', 'C'], ['B', 'B', 'A', 'C'])\n",
    "    \"\"\"\n",
    "    if not len(list_1) == len(list_2):\n",
    "        raise ValueError(\"Reviewer 1 and 2 may not have rated the same list of subjects.\")\n",
    "\n",
    "    list_attributes = sorted(list(set(list_1) | set(list_2)))\n",
    "    size = len(list_attributes)\n",
    "    matrix = pd.DataFrame(np.zeros((size + 1, size + 1)))\n",
    "\n",
    "    # Populate the confusion matrix\n",
    "    for k, att_1 in enumerate(list_attributes):\n",
    "        for l, att_2 in enumerate(list_attributes):\n",
    "            for i in range(len(list_1)):\n",
    "                if list_1[i] == att_1 and list_2[i] == att_2:\n",
    "                    matrix.loc[k, l] += 1\n",
    "\n",
    "    # Calculate row and column totals\n",
    "    for i in range(size):\n",
    "        for j in range(size):\n",
    "            matrix.loc[size, i] += matrix.loc[j, i]\n",
    "            matrix.loc[i, size] += matrix.loc[i, j]\n",
    "\n",
    "    # Calculate the overall total\n",
    "    for i in range(size):\n",
    "        matrix.loc[size, size] += matrix.loc[i, size]\n",
    "\n",
    "    # Normalize the matrix by dividing by the total number of ratings\n",
    "    matrix = matrix / len(list_1)\n",
    "\n",
    "    return matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_kappa_score(observed_proportion: float, expected_proportion: float) -> float:\n",
    "    \"\"\"Calculate the kappa cohen score.\"\"\"\n",
    "    return (observed_proportion - expected_proportion) / (1 - expected_proportion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expected_proportion(matrix: pd.DataFrame) -> float:\n",
    "    \"\"\"Calculate the overall proportion of agreement expected by chance from the confusion matrix..\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix: pd.DataFrame\n",
    "        Confusion matrix representing the counts of matching attributes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The overall proportion of agreement expected by chance (expected agreement).\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    - expected_proportion(create_confusion_matrix(['A', 'B', 'A', 'C'], ['B', 'B', 'A', 'C']))\n",
    "    \"\"\"\n",
    "    proportion = 0\n",
    "    k = len(matrix) - 1\n",
    "    for i in range(k):\n",
    "        proportion += matrix.loc[i, k] * matrix.loc[k, i]\n",
    "    return proportion\n",
    "\n",
    "\n",
    "def observed_proportion(matrix: pd.DataFrame) -> float:\n",
    "    \"\"\"Calculate the overall proportion of observed agreement from the confusion matrix.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    matrix: pd.DataFrame\n",
    "        Confusion matrix representing the counts of matching attributes.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    The overall proportion of observed agreement.\n",
    "\n",
    "    Example\n",
    "    -------\n",
    "    - observed_proportion(create_confusion_matrix(['A', 'B', 'A', 'C'], ['B', 'B', 'A', 'C']))\n",
    "    \"\"\"\n",
    "    proportion = 0\n",
    "    for i in range(len(matrix) - 1):\n",
    "        proportion += matrix.loc[i, i]\n",
    "    return proportion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class KappaStats:\n",
    "    \"\"\"Class describing the statistics for a Kappa score over the bootstrap samples.\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    mean : float\n",
    "        Mean of the kappa across bootstrap samples.\n",
    "    std : float\n",
    "        Standard error of kappa (standard deviation of the sampling distribution).\n",
    "    low_ci : float\n",
    "        95% CI lower bound (2.5th percentile of the sorted bootstrap distribution).\n",
    "    hight_ci : float\n",
    "        95% CI upper bound (97.5th percentile of the sorted bootstrap distribution).\n",
    "    \"\"\"\n",
    "    mean: float\n",
    "    std: float\n",
    "    low_ci: float\n",
    "    high_ci: float\n",
    "\n",
    "    @property\n",
    "    def ci(self) -> tuple[float, float]:\n",
    "        return self.low_ci, self.high_ci\n",
    "\n",
    "    @classmethod\n",
    "    def from_weighted_kappas(cls, weighted_kappas: list[float]):\n",
    "        return cls(\n",
    "            np.mean(weighted_kappas),\n",
    "            np.std(weighted_kappas),\n",
    "            np.percentile(weighted_kappas, 2.5),\n",
    "            np.percentile(weighted_kappas, 97.5),\n",
    "        )\n",
    "\n",
    "\n",
    "def bootstrap_cohen_quadratic_kappa_score(\n",
    "    y_true, y_pred, quad: bool = False, num_resamples: int = 1000,\n",
    ") -> KappaStats:\n",
    "    \"\"\"Bootstrap function for Cohen's Quadratic Kappa Score.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_true: array-like\n",
    "        True labels.\n",
    "    y_pred: array-like\n",
    "        Predicted labels.\n",
    "    quad: bool, optional (default=False)\n",
    "        If True, use quadratic weighting; otherwise, use linear weighting.\n",
    "    num_resamples: int, optional (default=1000)\n",
    "        Number of bootstrap resamples.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    KappaStats :\n",
    "        The Kappa statistics accross bootstrap samples.\n",
    "    \"\"\"\n",
    "    # Combine true and predicted labels\n",
    "    Y = np.array([y_true, y_pred]).T\n",
    "\n",
    "    # List to store weighted kappas for each bootstrap sample\n",
    "    weighted_kappas = []\n",
    "    \n",
    "    # Bootstrap resampling\n",
    "    for i in range(num_resamples):\n",
    "        # Randomly sample from the combined true and predicted labels\n",
    "        Y_resample = np.array(random.choices(Y, k=len(Y)))\n",
    "        y_true_resample = Y_resample[:, 0]\n",
    "        y_pred_resample = Y_resample[:, 1]\n",
    "        \n",
    "        # Calculate Cohen's Kappa Score based on weighting option\n",
    "        if not quad:\n",
    "            weighted_kappa = cohen_kappa_score(y_true_resample.astype(str), y_pred_resample.astype(str))\n",
    "        else:\n",
    "            weighted_kappa = cohen_kappa_score(y_true_resample.astype(str), y_pred_resample.astype(str), weights='quadratic')\n",
    "        \n",
    "        # Append the calculated kappa score to the list\n",
    "        weighted_kappas.append(weighted_kappa)\n",
    "\n",
    "    # Calculate mean, standard error, and confidence intervals of the kappa scores\n",
    "    return KappaStats.from_weighted_kappas(weighted_kappas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class KappaScore:\n",
    "    score: float\n",
    "    stats: KappaStats\n",
    "\n",
    "def write_stat(df_final: pd.DataFrame, category: str, kappa_score: KappaScore):\n",
    "    df_final.loc[category, (\"kappa score\")] = kappa_score.score\n",
    "    df_final.loc[category, (\"ci low\")] = kappa_score.stats.low_ci\n",
    "    df_final.loc[category, (\"ci high\")] = kappa_score.stats.high_ci\n",
    "    df_final.loc[category, (\"se\")] = kappa_score.stats.std\n",
    "\n",
    "def compute_kappa_score_from_lists(lists: tuple[list, list]) -> KappaScore:\n",
    "    # Create a confusion matrix to compare the ratings of the two reviewers\n",
    "    confusion_matrix = create_confusion_matrix(list_1=lists[0], list_2=lists[1])\n",
    "    \n",
    "    # Calculate Cohen's kappa between observed and expected proportions\n",
    "    kappa = compute_kappa_score(\n",
    "        observed_proportion(confusion_matrix),\n",
    "        expected_proportion(confusion_matrix),\n",
    "    )\n",
    "    # If kappa is not 1, perform bootstrap resampling to estimate standard error and confidence interval\n",
    "    kappa_stats = KappaStats(1.0, 0.0, 1.0, 1.0)\n",
    "    if kappa != 1:\n",
    "        kappa_stats = bootstrap_cohen_quadratic_kappa_score(y_true=lists[0], y_pred=lists[1])\n",
    "    return KappaScore(kappa, kappa_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 'Models and algorithms' item (over 270 reviews):\n",
      "Cohen's kappa = 0.7544080604534007\n",
      "Standard error (bootstrap) = 0.04414755323717636\n",
      "CI bootstrap = (0.6677945752385603, 0.8412028086758719)\n",
      "kappa cohen sklearn = 0.7544080604534005\n",
      "kappa fleiss statsmodels = 0.7537878787878788\n",
      "**************************************************\n",
      "For 'Datasets' item (over 270 reviews):\n",
      "Cohen's kappa = 0.9085872576177285\n",
      "Standard error (bootstrap) = 0.02745110370119088\n",
      "CI bootstrap = (0.8539060089062436, 0.9579839444859959)\n",
      "kappa cohen sklearn = 0.9085872576177285\n",
      "kappa fleiss statsmodels = 0.9085858508133396\n",
      "**************************************************\n",
      "For 'Code' item (over 270 reviews):\n",
      "Cohen's kappa = 0.9107142857142856\n",
      "Standard error (bootstrap) = 0.024752865333389745\n",
      "CI bootstrap = (0.8647837903095287, 0.9553129951994158)\n",
      "kappa cohen sklearn = 0.9107142857142857\n",
      "kappa fleiss statsmodels = 0.9107142857142858\n",
      "**************************************************\n",
      "For 'Experimental results' item (over 270 reviews):\n",
      "Cohen's kappa = 0.8637248539909151\n",
      "Standard error (bootstrap) = 0.03549494931586636\n",
      "CI bootstrap = (0.7897583916793117, 0.9267692475761748)\n",
      "kappa cohen sklearn = 0.863724853990915\n",
      "kappa fleiss statsmodels = 0.8637248539909151\n",
      "**************************************************\n",
      "For 'Error bars or statistical significance' item (over 270 reviews):\n",
      "Cohen's kappa = 1.0\n",
      "Standard error (bootstrap) = 0.0\n",
      "CI bootstrap = (1.0, 1.0)\n",
      "kappa cohen sklearn = 1.0\n",
      "kappa fleiss statsmodels = 1.0\n",
      "**************************************************\n",
      "For 'Code is or will be available' item (over 270 reviews):\n",
      "Cohen's kappa = 0.8854961832061069\n",
      "Standard error (bootstrap) = 0.033953316356673154\n",
      "CI bootstrap = (0.8117981591872816, 0.9461474841885783)\n",
      "kappa cohen sklearn = 0.8854961832061069\n",
      "kappa fleiss statsmodels = 0.8854939759036144\n",
      "**************************************************\n",
      "For 'Statement' item (over 270 reviews):\n",
      "Cohen's kappa = 0.736981934112646\n",
      "Standard error (bootstrap) = 0.03662721796642672\n",
      "CI bootstrap = (0.6604241059212579, 0.8045213999807374)\n",
      "kappa cohen sklearn = 0.7369819341126461\n",
      "kappa fleiss statsmodels = 0.7368217011331288\n",
      "**************************************************\n",
      "For 'Comments' item (over 270 reviews):\n",
      "Cohen's kappa = 0.815593659010029\n",
      "Standard error (bootstrap) = 0.026585564235817833\n",
      "CI bootstrap = (0.7629325973660602, 0.8653633923169772)\n",
      "kappa cohen sklearn = 0.8155936590100291\n",
      "kappa fleiss statsmodels = 0.8155787430235379\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "for idx_category, category in enumerate(CATEGORIES):    \n",
    "    all_reviews = ([], [])\n",
    "    \n",
    "    # Iterate over the three reviews for the current category\n",
    "    for idx_review in range(N_REVIEWS):\n",
    "        # Calculate the column index for the current category and review\n",
    "        column_id = idx_review * 9 + 3 + idx_category\n",
    "\n",
    "        for j, df_rating in enumerate(df_ratings):\n",
    "            all_reviews[j].extend(df_rating.loc[2:, column_id].values.tolist())\n",
    "\n",
    "    print(f\"For \\'{df_ratings[0].loc[1, column_id]}\\' item (over {len(all_reviews[0])} reviews):\")\n",
    "    \n",
    "    kappa_score = compute_kappa_score_from_lists(all_reviews)\n",
    "    write_stat(df_final, category, kappa_score)\n",
    "\n",
    "    # Print the results\n",
    "    print(f\"Cohen's kappa = {kappa_score.score}\")\n",
    "    print(f\"Standard error (bootstrap) = {kappa_score.stats.std}\")\n",
    "    print(f\"CI bootstrap = {kappa_score.stats.ci}\")\n",
    "\n",
    "    # ######## For sanity check\n",
    "    kappa_sklearn = cohen_kappa_score(*all_reviews)\n",
    "    print(f\"kappa cohen sklearn = {kappa_sklearn}\")\n",
    "    df_final.loc[category, (\"kappa score\", \"sklearn\")] = kappa_sklearn\n",
    "\n",
    "    # ######## For sanity check\n",
    "    # print(f\"kappa cohen bootstrap = {kappa_btp}\")\n",
    "\n",
    "    # ######## For sanity check\n",
    "    data = [all_reviews[0], all_reviews[1]]\n",
    "    data_T = np.array(data).T\n",
    "    data_fleiss_ = aggregate_raters(data_T)\n",
    "    kappa_fleiss_ = fleiss_kappa(data_fleiss_[0])\n",
    "    df_final.loc[category, (\"kappa score\", \"fleiss\")] = kappa_fleiss_\n",
    "    print(f\"kappa fleiss statsmodels = {kappa_fleiss_}\")\n",
    "\n",
    "    # ######## For sanity check\n",
    "    # print(f\"standard error (cohen) = {sd_cohen_ / sqrt(len(all_reviews_1))}\")\n",
    "    \n",
    "    # ######## For sanity check\n",
    "    # sd_cohen_ = sd_cohen(po_, pe_)\n",
    "    # se_cohen = sd_cohen_ / sqrt(len(all_reviews_1))\n",
    "    # low_parametric_cohen=-1.96 * se_cohen + kappa_\n",
    "    # high_parametric_cohen=1.96 * se_cohen + kappa_\n",
    "    # write_stat(df_final, category, \"cohen\", kappa_, -1.96 * se_cohen + kappa_, 1.96 * se_cohen + kappa_, se_cohen )\n",
    "\n",
    "    # print(f\"CI parametric from Cohen's SE = [{low_parametric_cohen}, {high_parametric_cohen}]\")\n",
    "    print(\"**************************************************\")\n",
    " \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sd_cohen(observed_proportion: float, expected_proportion: float) -> float:\n",
    "    \"\"\"Cohen standard deviation.\"\"\"\n",
    "    return sqrt(\n",
    "        (observed_proportion * (1 - observed_proportion)) /\n",
    "        ((1 - expected_proportion) * (1 - expected_proportion))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 'Meta-category' item (over 270 reviews):\n",
      "Cohen's kappa = 0.8014027898179524\n",
      "Standard error (bootstrap) = 0.035861760223139603\n",
      "CI bootstrap = (0.728436860376502, 0.8680258232176784)\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# Initialize lists to store reviews from both reviewers for meta-categories\n",
    "list_meta = ([], [])\n",
    "\n",
    "# Iterate over the three reviews for meta-categories\n",
    "for idx_review in range(N_REVIEWS):\n",
    "    # Calculate the column index for the current meta-category and review\n",
    "    column_id = idx_review + 29\n",
    "    \n",
    "    for j, df_rating in enumerate(df_ratings):\n",
    "        list_meta[j].extend(df_rating.loc[2:, column_id].values.tolist())\n",
    "\n",
    "# Count the occurrences of \"Unusable (meta)\" in both lists\n",
    "test = tuple(l.count(\"Unusable (meta)\") for l in list_meta)\n",
    "\n",
    "kappa_score = compute_kappa_score_from_lists(list_meta)\n",
    "\n",
    "# Write the calculated statistics to the DataFrame for meta-categories\n",
    "write_stat(df_final, \"Meta-categories\", kappa_score)\n",
    "\n",
    "# Print the results for meta-categories\n",
    "print(f\"For \\'Meta-category\\' item (over {len(list_meta[0])} reviews):\")\n",
    "print(f\"Cohen's kappa = {kappa_score.score}\")\n",
    "print(f\"Standard error (bootstrap) = {kappa_score.stats.std}\")\n",
    "print(f\"CI bootstrap = {kappa_score.stats.ci}\")\n",
    "print(\"**************************************************\")\n",
    "\n",
    "# ######## For sanity check\n",
    "# print(f\"We can count {test} reviews unusable for the first rater and {test2} reviews unusable for the second.\")\n",
    "\n",
    "# ######## For sanity check\n",
    "# kappa_sklearn = cohen_kappa_score(all_reviews_1, all_reviews_2)\n",
    "# print(f\"kappa cohen sklearn = {kappa_sklearn}\")\n",
    "# kappa_sklearn = cohen_kappa_score(all_reviews_1, all_reviews_2)\n",
    "# df_final.loc[list_categories[category], (\"kappa score\", \"sklearn\")]=kappa_sklearn\n",
    "\n",
    "# ######## For sanity check\n",
    "# print(f\"kappa cohen bootstrap = {kappa_btp}\")\n",
    "\n",
    "# ######## For sanity check\n",
    "# data = [all_reviews_1, all_reviews_2]\n",
    "# data_T = np.array(data).T\n",
    "# data_fleiss_ = aggregate_raters(data_T)\n",
    "# kappa_fleiss_ = fleiss_kappa(data_fleiss_[0])\n",
    "# df_final.loc[list_categories[category], (\"kappa score\", \"fleiss\")]=kappa_fleiss_\n",
    "# print(f\"kappa fleiss statsmodels = {kappa_fleiss_}\")\n",
    "\n",
    "# ######## For sanity check\n",
    "# sd_cohen_ = sd_cohen(po_, pe_)\n",
    "# print(f\"standard error (cohen) = {sd_cohen_ / sqrt(len(list_meta_1))}\")\n",
    "\n",
    "# ######## For sanity check\n",
    "# se_cohen = sd_cohen_ / sqrt(len(list_meta_1))\n",
    "# low_parametric_cohen=-1.96 * se_cohen + kappa_\n",
    "# high_parametric_cohen=1.96 * se_cohen + kappa_\n",
    "# write_stat(df_final, list_categories[category], \"cohen\", kappa_, -1.96 * se_cohen + kappa_, 1.96 * se_cohen + kappa_, se_cohen )\n",
    "\n",
    "# print(f\"CI parametric from Cohen's SE = [{low_parametric_cohen}, {high_parametric_cohen}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add repo provided review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For 'Repo provided' item (over 90 reviews):\n",
      "Cohen's kappa = 1.0\n",
      "Standard error (bootstrap) = 0.0\n",
      "CI bootstrap = (1.0, 1.0)\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "# Extract values from specific columns in dataframes and convert to lists\n",
    "list_repo = tuple(df.loc[2:, 39].values.tolist() for df in df_ratings)\n",
    "\n",
    "kappa_score = compute_kappa_score_from_lists(list_repo)\n",
    "\n",
    "# Write statistical results to a dataframe\n",
    "write_stat(df_final, \"Repo provided\", kappa_score)\n",
    "\n",
    "# Print results for 'Repo provided' item\n",
    "print(f\"For 'Repo provided' item (over {len(list_repo[0])} reviews):\")\n",
    "print(f\"Cohen's kappa = {kappa_score.score}\")\n",
    "print(f\"Standard error (bootstrap) = {kappa_score.stats.std}\")\n",
    "print(f\"CI bootstrap = {kappa_score.stats.ci}\")\n",
    "print(\"**************************************************\")\n",
    "\n",
    "\n",
    "######### For sanity check\n",
    "# print(f\"We can count {test} reviews unusable for the first rater and {test2} reviews unusable for the second.\")\n",
    "\n",
    "######### For sanity check\n",
    "# print(f\"kappa cohen sklearn = {kappa_sklearn}\")\n",
    "# kappa_sklearn = cohen_kappa_score(all_reviews_1, all_reviews_2)\n",
    "# df_final.loc[list_categories[category], (\"kappa score\", \"sklearn\")]=kappa_sklearn\n",
    "\n",
    "######### For sanity check\n",
    "# print(f\"kappa cohen bootstrap = {kappa_btp}\")\n",
    "\n",
    "######### For sanity check\n",
    "# data = [all_reviews_1, all_reviews_2]\n",
    "# data_T = np.array(data).T\n",
    "# data_fleiss_ = aggregate_raters(data_T)\n",
    "# kappa_fleiss_ = fleiss_kappa(data_fleiss_[0])\n",
    "# df_final.loc[list_categories[category], (\"kappa score\", \"fleiss\")]=kappa_fleiss_\n",
    "# print(f\"kappa fleiss statsmodels = {kappa_fleiss_}\")\n",
    "\n",
    "######### For sanity check\n",
    "# print(f\"standard error (cohen) = {sd_cohen_ / sqrt(len(list_repo_1))}\")\n",
    "\n",
    "######### For sanity check\n",
    "# sd_cohen_ = sd_cohen(po_, pe_)\n",
    "# se_cohen = sd_cohen_ / sqrt(N)\n",
    "# low_parametric_cohen=-1.96 * se_cohen + kappa_\n",
    "# high_parametric_cohen=1.96 * se_cohen + kappa_\n",
    "#write_stat(df_final, list_categories[category], \"cohen\", kappa_, -1.96 * se_cohen + kappa_, 1.96 * se_cohen + kappa_, se_cohen )\n",
    "\n",
    "#print(f\"CI parametric from Cohen's SE = [{low_parametric_cohen}, {high_parametric_cohen}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save final dataframe to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory path\n",
    "output_directory = Path(f\"../miccai2023/stats_inter_rater\")\n",
    "\n",
    "# Check if the output directory exists, and create it if it doesn't\n",
    "if not output_directory.is_dir():\n",
    "    output_directory.mkdir()\n",
    "\n",
    "# Save the final dataframe to the CSV file\n",
    "df_final.to_csv(output_directory / 'inter_rater_stats.csv', index=True, sep=\"\\t\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store metadata information\n",
    "data = {}\n",
    "\n",
    "# Calculate and store the number of papers (assuming papers are represented in all_reviews_1)\n",
    "data[\"nb_papers\"] = len(all_reviews[0]) / N_REVIEWS  # Assuming each paper has 3 reviews\n",
    "\n",
    "# Store the total number of reviews\n",
    "data[\"nb_reviews\"] = len(all_reviews[0])\n",
    "\n",
    "# Store the current date and time\n",
    "data[\"date\"] = str(datetime.date.today())\n",
    "data[\"time\"] = str(datetime.datetime.utcnow())\n",
    "\n",
    "# Store the file paths of the input data\n",
    "data[\"data_path1\"] = str(path1_tsv)\n",
    "data[\"data_path2\"] = str(path2_tsv)\n",
    "\n",
    "# Convert the dictionary to a JSON-formatted string with indentation for readability\n",
    "json_data = json.dumps(data, skipkeys=True, indent=4)\n",
    "\n",
    "# Write the JSON data to the file\n",
    "with open(output_directory / \"data.json\", \"w\") as f:\n",
    "    f.write(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------+---------------+----------+-----------+------+\n",
      "|                                        |   kappa score |   ci low |   ci high |   se |\n",
      "|----------------------------------------+---------------+----------+-----------+------|\n",
      "| Models and algorithms                  |          0.75 |     0.66 |      0.84 | 0.05 |\n",
      "| Datasets                               |          0.91 |     0.85 |      0.96 | 0.03 |\n",
      "| Code                                   |          0.91 |     0.86 |      0.96 | 0.03 |\n",
      "| Experimental results                   |          0.86 |     0.79 |      0.93 | 0.04 |\n",
      "| Error bars or statistical significance |          1.00 |     1.00 |      1.00 | 0.00 |\n",
      "| Code is or will be available           |          0.89 |     0.81 |      0.95 | 0.04 |\n",
      "| Statement                              |          0.74 |     0.67 |      0.81 | 0.04 |\n",
      "| Comments                               |          0.82 |     0.76 |      0.87 | 0.03 |\n",
      "| Meta-categories                        |          0.80 |     0.73 |      0.87 | 0.04 |\n",
      "| Repo provided                          |          1.00 |     1.00 |      1.00 | 0.00 |\n",
      "+----------------------------------------+---------------+----------+-----------+------+\n"
     ]
    }
   ],
   "source": [
    "from tabulate import tabulate\n",
    "print(tabulate(df_final, headers='keys', tablefmt='psql', floatfmt=\".2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
