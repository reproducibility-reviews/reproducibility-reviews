{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping review categories to their corresponding prompts\n",
    "list_review_text = {\n",
    "    \"contribution\": \"Please describe the contribution of the paper\",\n",
    "    \"strengths\": \"Please list the main strengths of the paper\",\n",
    "    \"weakness\": \"Please list the main weaknesses of the paper\",\n",
    "    \"clarity\": \"Please rate the clarity and organization of this paper\",\n",
    "    \"reproducibility\": \"Please comment on the reproducibility of the paper\",\n",
    "    \"detailed\": \"Please provide detailed and constructive comments for the authors\",\n",
    "    \"rate\": \"Rate the paper on a scale of 1-8, 8 being the strongest\",\n",
    "    \"justification\": \"Please justify your recommendation.\",\n",
    "    \"number of paper\": \"Number of papers in your stack\",\n",
    "    \"ranking\": \"What is the ranking of this paper in your review stack?\",\n",
    "    \"confidence\": \"Reviewer confidence\",\n",
    "    \"rate rebuttal\": \"[Post rebuttal] After reading the authorâ€™s rebuttal, state your overall opinion of the paper if it has been changed\",\n",
    "    \"justification rebuttal\": \"[Post rebuttal] Please justify your decision\",\n",
    "}\n",
    "\n",
    "# List of string categories for reviews\n",
    "list_categories_str = [\"contribution\", \"strengths\", \"weakness\", \"reproducibility\", \"detailed\", \"justification\"]\n",
    "\n",
    "# List of categories for reviews with scores\n",
    "list_categories_scores = [\"clarity\", \"rate\", \"confidence\", \"rate rebuttal\"]\n",
    "\n",
    "# Columns for the reviews data frame\n",
    "columns_reviews = [\"id\", \"category\", \"title\", \"review 1\", \"review 2\", \"review 3\"]\n",
    "\n",
    "# Columns for the statistics data frame\n",
    "columns_statistics = [\"id\", \"category\", \"title\", \"words1\", \"words2\", \"words3\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Retrieve list of paper URLs from the MICCAI conference website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accepted_paper_list(year: str = \"2023\") -> list[str]:\n",
    "    \"\"\"Get the list of all html files from the chosen year\"\"\"\n",
    "    # Base URL of the MICCAI conference website\n",
    "    miccai_website_path = \"https://conferences.miccai.org\"\n",
    "\n",
    "    # Constructing the URL to access the list of papers for the specified year\n",
    "    path_online_list = f\"{miccai_website_path}/{year}/papers/\"\n",
    "\n",
    "    # Opening the URL and reading its contents\n",
    "    reponse = urllib.request.urlopen(path_online_list)\n",
    "    content = reponse.read().decode('UTF-8')\n",
    "\n",
    "    # Parsing the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    # Finding all anchor ('a') tags in the HTML\n",
    "    links = soup.find_all('a')\n",
    "\n",
    "    # Creating a list of URLs for papers. It filters for links that end with 'html'\n",
    "    paper_list = [(miccai_website_path + link.get('href')) for link in links if link.get('href').endswith('html')]\n",
    "\n",
    "    # Returning the list of paper URLs\n",
    "    return paper_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests\n",
    "paper_urls = get_accepted_paper_list()\n",
    "assert len(paper_urls) == 730\n",
    "assert all([\"2023\" in url for url in paper_urls])\n",
    "assert all([url.endswith(\"html\") for url in paper_urls])\n",
    "\n",
    "paper_urls = get_accepted_paper_list(\"2022\")\n",
    "assert len(paper_urls) == 573\n",
    "assert all([\"2022\" in url for url in paper_urls])\n",
    "assert all([url.endswith(\"html\") for url in paper_urls])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Note: Extraction may take 5-10 minutes\n",
    "\n",
    "# Default year is 2023\n",
    "# To change to another year, modify the variable below\n",
    "year = \"2023\"\n",
    "\n",
    "# Get the list of HTML pages for the different papers\n",
    "paper_list = get_accepted_paper_list(year=year)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Paragraphs Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set output directories and files for the extraction\n",
    "output_directory = Path(f\"../miccai{year}\")\n",
    "#output_directory = Path(\"../miccaitest\")\n",
    "\n",
    "if not output_directory.is_dir():\n",
    "    output_directory.mkdir()\n",
    "\n",
    "csv_directory = output_directory / \"extract-csv\"\n",
    "if not csv_directory.is_dir():\n",
    "    csv_directory.mkdir()\n",
    "\n",
    "path_all_reviews = csv_directory / 'reviews.csv'\n",
    "path_all_stats = csv_directory / 'count_words.csv'\n",
    "path_all_scores = csv_directory / 'scores.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_paragraph(paper_list: list[str], year: str = \"2023\") -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Extract the 3 first reviews of all paper from the given list.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paper_list : list of str\n",
    "        List of paper URLs for which to extract paragraphs.\n",
    "    year : str\n",
    "        The year to consider for extraction.\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    df_reviews : pd.DataFrame\n",
    "        DataFrame containing the 3 first reviews of all papers.\n",
    "    df_stats : pd.DataFrame\n",
    "        DataFrame containing the number of words for each review of df_reviews.\n",
    "    df_scores : pd.DataFrame\n",
    "        DataFrame containing the scores given by the 3 first reviewers of all papers.\n",
    "    \"\"\"\n",
    "    iterables_str = [list_categories_str, [\"review 1\", \"review 2\", \"review 3\"]]\n",
    "    iterables_score = [list_categories_scores, [\"review 1\", \"review 2\", \"review 3\"]]\n",
    "\n",
    "    index_line_str = pd.MultiIndex.from_product(iterables_str, names=[\"category\", \"review\"])\n",
    "    index_line_score = pd.MultiIndex.from_product(iterables_score, names=[\"category\", \"review\"])\n",
    "    index_column = pd.MultiIndex.from_product([[], []], names=[\"id\", \"title\"])\n",
    "\n",
    "    df_reviews = pd.DataFrame(index=index_column, columns=index_line_str)\n",
    "    df_stats = pd.DataFrame(index=index_column, columns=index_line_str)\n",
    "    df_scores = pd.DataFrame(index=index_column, columns=index_line_score)\n",
    "\n",
    "    for i, paper in enumerate(paper_list):\n",
    "        print(f\"Processing paper {i + 1}/{len(paper_list)}\", end='\\r', flush=True)\n",
    "        list_categories = list_categories_str + list_categories_scores\n",
    "        extract_reviews(paper, df_reviews, df_stats, df_scores, year, list_categories)\n",
    "\n",
    "    return df_reviews, df_stats, df_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reviews(\n",
    "    paper_url: str,\n",
    "    df_reviews: pd.DataFrame,\n",
    "    df_stats: pd.DataFrame,\n",
    "    df_scores: pd.DataFrame,\n",
    "    year: str = \"2023\",\n",
    "    categories: list[str] = [\"reproducibility\"],\n",
    "):\n",
    "    \"\"\"\n",
    "    Extract the 3 first reviews from a given paper webpage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paper: str\n",
    "        URL of the paper webpage.\n",
    "    df_reviews: pd.DataFrame \n",
    "        DataFrame to store extracted reviews and code links.\n",
    "    df_stats: pd.DataFrame\n",
    "        DataFrame to store statistics related to reviews (e.g., word count).\n",
    "    df_scores: pd.DataFrame\n",
    "        DataFrame to store reviews containing scores.\n",
    "    year: str, optional\n",
    "        Year of the conference. Defaults to \"2023\".\n",
    "    categories: list[str], optional\n",
    "        Categories to extract reviews for. Defaults to \"reproducibility\".\n",
    "    \"\"\"\n",
    "    # Open HTML webpage and extract with BeautifulSoup\n",
    "    response = urllib.request.urlopen(paper_url)\n",
    "    content = response.read().decode('UTF-8')\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "\n",
    "    # Extract paper title and ID from the webpage\n",
    "    if year == \"2023\":\n",
    "        paper_title = soup.find(\"title\").get_text().rstrip(\"MICCAI 2023 - Accepted Papers, Reviews, Author Feedback\").rstrip(' |')\n",
    "    elif year == \"2022\":\n",
    "        paper_title = soup.find(\"title\").get_text().rstrip(\"MICCAI 2022 - Accepted Papers and Reviews\").rstrip(' |')\n",
    "    else:\n",
    "        raise ValueError(f\"Year {year} is not valid. Please select 2023 or 2022.\")\n",
    "    paper_id = Path(paper_url).name[:13]\n",
    "\n",
    "    # Extract code link from the webpage\n",
    "    code_link = soup.find(id=\"code-id\")\n",
    "    code_link = code_link.find_next(\"p\").get_text()\n",
    "    code_link = code_link.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Store code link information in the DataFrame\n",
    "    df_reviews.loc[(paper_id, unidecode(paper_title)), (\"code link\", \"code link\")] = code_link\n",
    "\n",
    "    # Loop through specified categories to extract reviews\n",
    "    for category in categories:\n",
    "        text = list_review_text[category]\n",
    "\n",
    "        # Find paragraphs containing reviews for the specified category\n",
    "        repro_reviews_paragraph = soup.find_all(lambda tag: tag.name == \"li\" and text in tag.text)\n",
    "        repro_exact_text = soup.find(lambda tag: tag.name == \"strong\" and text in tag.text).get_text()\n",
    "\n",
    "        # Loop through the first 3 reviews for the specified category\n",
    "        for i, review in enumerate(repro_reviews_paragraph[:3]):\n",
    "            tmp_review = unidecode(review.get_text().strip(repro_exact_text))\n",
    "            tmp_review = tmp_review.strip(\"\\n\")\n",
    "            tmp_review = tmp_review.replace(\"\\n          \\n\", \" \")\n",
    "            tmp_review = tmp_review.replace(\"\\t\", \" \")\n",
    "            tmp_review = tmp_review.replace(\"\\n\\n\\n\\n\", \" \")\n",
    "            tmp_review = tmp_review.replace(\"\\n\\n\\n\", \" \")\n",
    "            tmp_review = tmp_review.replace(\"\\n\\n\", \" \")\n",
    "            tmp_review = tmp_review.replace(\"\\n\", \" \")\n",
    "            tmp_review = tmp_review.replace(\"\\t\", \" \")\n",
    "\n",
    "            # Store review information in the appropriate DataFrame\n",
    "            if category in list_categories_str:\n",
    "                df_reviews.loc[(paper_id, unidecode(paper_title)), (category, f\"review {i + 1}\")] = tmp_review\n",
    "                df_stats.loc[(paper_id, unidecode(paper_title)), (category, f\"review {i + 1}\")] = len(str(tmp_review).split())\n",
    "            elif category in list_categories_scores:\n",
    "                df_scores.loc[(paper_id, unidecode(paper_title)), (category, f\"review {i + 1}\")] = tmp_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing paper 3/3\r"
     ]
    }
   ],
   "source": [
    "# Tests\n",
    "for df in extract_paragraph(get_accepted_paper_list(\"2022\")[:3], \"2022\"):\n",
    "    assert len(df) == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skip the extraction if the files already exist.\n",
    "\n",
    "To rerun the extraction, remove the directory \"miccaiYYYY\" where YYYY is the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract reviews and count words for year 2023\n",
      "Processing paper 730/730\r"
     ]
    }
   ],
   "source": [
    "if (not path_all_reviews.is_file()) or (not path_all_stats.is_file()) or (not path_all_scores.is_file()):\n",
    "    print(f\"Extract reviews and count words for year {year}\")\n",
    "    \n",
    "    # Extract reviews, stats, and scores\n",
    "    df_all_reviews, df_all_stats, df_all_scores = extract_paragraph(paper_list, year)\n",
    "    \n",
    "    # Save the results to CSV files\n",
    "    df_all_scores.to_csv(path_all_scores, index=True, sep=\"\\t\", encoding='utf-8')\n",
    "    df_all_reviews.to_csv(path_all_reviews, index=True, sep=\"\\t\", encoding='utf-8')\n",
    "    df_all_stats.to_csv(path_all_stats, index=True, sep=\"\\t\", encoding='utf-8')\n",
    "\n",
    "else:\n",
    "    print(\n",
    "        \"Files already exist.\\nSkipping extraction and importing existing \"\n",
    "        f\"CSV from {output_directory}.\\nIf you want to rerun the extraction, \"\n",
    "        \"delete the directory miccaiYYYY where YYYY is the year.\"\n",
    "    )\n",
    "    # Load existing CSV files\n",
    "    df_all_reviews = pd.read_csv(path_all_reviews, sep=\"\\t\", header=[0, 1], index_col=[0, 1], skip_blank_lines=True)\n",
    "    df_all_stats = pd.read_csv(path_all_stats, sep=\"\\t\", header=[0, 1], index_col=[0, 1], skip_blank_lines=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Count the total number of words in each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_words(df_words: pd.DataFrame, year: str = \"2023\") -> pd.DataFrame:\n",
    "    \"\"\"Count total number of words of each review by summing the number of words of each category.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_words : pd.DataFrame\n",
    "        DataFrame containing reviews and their word counts.\n",
    "    year : str \n",
    "        Year for which the function is being applied (default: \"2023\").\n",
    "    \"\"\"\n",
    "    # Iterate through unique IDs in the DataFrame\n",
    "    for id, id_df in df_words.groupby(level=0):\n",
    "        # Iterate through titles associated with the current ID\n",
    "        for _, title in id_df.index.values:\n",
    "            # Initialize word count for each of the three reviews\n",
    "            for i in range(1, 4):\n",
    "                df_words.loc[(id, title), (\"total\", f\"review {i}\")] = 0\n",
    "\n",
    "            # Iterate through categories in the list_categories_str\n",
    "            for category in list_categories_str:\n",
    "                # Sum the words from each category for each review\n",
    "                for i in range(1, 4):\n",
    "                    if math.isnan(df_words.loc[(id, title), (category, f\"review {i}\")]):\n",
    "                        # Handle NaN values by replacing them with 0\n",
    "                        df_words.loc[(id, title), (category, f\"review {i}\")] = 0\n",
    "                    df_words.loc[(id, title), (\"total\", f\"review {i}\")] += df_words.loc[(id, title), (category, f\"review {i}\")]\n",
    "\n",
    "    # Sort the resulting DataFrame\n",
    "    df_words.sort_index(axis=1, ascending=True, inplace=True)\n",
    "    \n",
    "    return df_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year 2023 and outputting result in ../miccai2023\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing year {year} and outputting result in {output_directory}\")\n",
    "\n",
    "# Count the total number of words in each review\n",
    "df_all_stats = count_total_words(df_words=df_all_stats, year=year)\n",
    "\n",
    "# Extracted results are saved in 'count_words.csv'\n",
    "df_all_stats.to_csv(csv_directory / \"count_words.csv\", index=True, sep=\"\\t\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Find reviewers that have copy/pasted the reproducibility review in another category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repro_copy_paste(df_all_reviews: pd.DataFrame, threshold: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"Find reviewers that have copy/paste the reproducibility review in another category.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_all_reviews : pd.DataFrame\n",
    "        DataFrame containing reviews for each paper.\n",
    "    threshold : int \n",
    "        Minimum word count for a review to be considered (default: 10).\n",
    "    \"\"\"\n",
    "    from copy import copy\n",
    "\n",
    "    # Create a deep copy of the input DataFrame to preserve the original data\n",
    "    df_all_reviews_wo_copy_paste = copy(df_all_reviews)\n",
    "\n",
    "    # Initialize a new DataFrame to store reviews with copy/paste\n",
    "    df_bad_reviews = pd.DataFrame(columns=columns_reviews)\n",
    "    df_bad_reviews.set_index([\"id\", \"category\"], inplace=True)\n",
    "\n",
    "    # Iterate through unique IDs in the DataFrame\n",
    "    for id, id_df in df_all_reviews.groupby(level=0):\n",
    "        # Iterate through titles associated with the current ID\n",
    "        for _, title in id_df.index.values:\n",
    "            # Iterate through categories in list_categories_str\n",
    "            for category in list_categories_str:\n",
    "                # Check if the category is not \"reproducibility\"\n",
    "                if category != \"reproducibility\":\n",
    "                    # Iterate through review indices (1 and 2)\n",
    "                    for i in range(1, 3):\n",
    "                        # Extract reproducibility and category reviews\n",
    "                        repro = id_df.loc[(id, title), (\"reproducibility\", f\"review {i}\")]\n",
    "                        cate = id_df.loc[(id, title), (category, f\"review {i}\")]\n",
    "\n",
    "                        # Check if the reproducibility review is long enough and is a substring of the category review\n",
    "                        if len(str(repro).split()) >= threshold and str(repro) in str(cate):\n",
    "                            # Store the problematic reviews in the df_bad_reviews DataFrame\n",
    "                            df_bad_reviews.loc[(id, category), \"title\"] = title\n",
    "                            df_bad_reviews.loc[(id, category), f\"review {i}\"] = id_df.loc[(id, title), (category, f\"review {i}\")]\n",
    "                            df_bad_reviews.loc[(id, category), \"reproducibility\"] = id_df.loc[(id, title), (\"reproducibility\", f\"review {i}\")]\n",
    "\n",
    "                            # Drop the row from the df_all_reviews_wo_copy_paste DataFrame (not used currently)\n",
    "                            try:\n",
    "                                df_all_reviews_wo_copy_paste.drop((id, title), inplace=True)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "    return df_bad_reviews\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Identify reproducibility reviews which contain copy/paste from other parts (need to have more than 10 consecutive words in common)\n"
     ]
    }
   ],
   "source": [
    "threshold = 10\n",
    "print(\n",
    "    \"Identify reproducibility reviews which contain copy/paste from other parts \"\n",
    "    f\"(need to have more than {threshold} consecutive words in common)\"\n",
    ")\n",
    "df_bad_reviews = get_repro_copy_paste(df_all_reviews=df_all_reviews, threshold=threshold)\n",
    "\n",
    "# Results are saved in 'reviews_copy_paste_{threshold}.csv'\n",
    "df_bad_reviews.to_csv(csv_directory / f\"reviews_copy_paste_{threshold}.csv\", index=True, sep=\"\\t\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Find reviews that mention the word 'checklist' in the reproducibility review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_checklist(df_all_reviews: pd.DataFrame, category: str = \"reproducibility\"):\n",
    "    \"\"\"Find reviewers that have mentioned the word 'checklist' in the reproducibility review.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_all_reviews : pd.DataFrame\n",
    "        DataFrame containing reviews for each paper.\n",
    "    category : str \n",
    "        The review category to analyze (default: \"reproducibility\").\n",
    "    \"\"\"\n",
    "    # Initialize a new DataFrame to store reviews mentioning 'checklist'\n",
    "    df_checklist = pd.DataFrame(columns=columns_reviews)\n",
    "    df_checklist.set_index([\"id\", \"title\"], inplace=True)\n",
    "\n",
    "    # Iterate through unique IDs in the DataFrame\n",
    "    for id, id_df in df_all_reviews.groupby(level=0):\n",
    "        # Iterate through titles associated with the current ID\n",
    "        for _, title in id_df.index.values:\n",
    "            # Iterate through review indices (1 and 2)\n",
    "            for i in range(1, 3):\n",
    "                # Extract the review text from the specified category\n",
    "                review = str(df_all_reviews.loc[(id, title), (category, f\"review {i}\")])\n",
    "\n",
    "                # Check if the review mentions 'checklist' in various formats\n",
    "                if (\"check-list\" in review) or (\"checklist\" in review) or (\"check list\" in review):\n",
    "                    # Store the problematic reviews in the df_checklist DataFrame\n",
    "                    df_checklist.loc[(id, title), f\"review {i}\"] = df_all_reviews.loc[(id, title), (category, f\"review {i}\")]\n",
    "                    df_checklist.loc[(id, title), \"category\"] = category\n",
    "\n",
    "    return df_checklist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count the number of reviews that mention the checklist\n"
     ]
    }
   ],
   "source": [
    "print(\"Count the number of reviews that mention the checklist\")\n",
    "category = \"reproducibility\"\n",
    "df_checklist = count_checklist(df_all_reviews=df_all_reviews, category=category)\n",
    "\n",
    "# Results are saved in 'reproducibility_checklist_reviews.csv'\n",
    "df_checklist.to_csv(csv_directory / f\"{category}_checklist_reviews.csv\", index=True, sep=\"\\t\", encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Create Excel files for human rating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rating_excel(df_all_reviews: pd.DataFrame, output_directory: Path, year: str = \"2023\"):\n",
    "    \"\"\"Save the excel file with the reproducibility reviews to create the rating file.\"\"\"\n",
    "    df_repro_excel = df_all_reviews.loc[:, (\"reproducibility\")]\n",
    "    df_repro_excel.to_excel(output_directory / f\"reviews_reproducibility_{year}.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create excel files for human rating\n"
     ]
    }
   ],
   "source": [
    "print(\"Create excel files for human rating\")\n",
    "create_rating_excel(df_all_reviews=df_all_reviews, output_directory=Path(\"../human_rating/\"), year=year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
