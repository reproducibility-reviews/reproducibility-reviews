{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup #Beautiful Soup is a library that makes it easy to scrape information from web pages\n",
    "import os\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "import urllib.request #Functions for opening URLs\n",
    "from pathlib import Path\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary mapping review categories to their corresponding prompts\n",
    "list_review_text = {\n",
    "    \"contribution\": \"Please describe the contribution of the paper\",\n",
    "    \"strengths\": \"Please list the main strengths of the paper\",\n",
    "    \"weakness\": \"Please list the main weaknesses of the paper\",\n",
    "    \"clarity\": \"Please rate the clarity and organization of this paper\",\n",
    "    \"reproducibility\": \"Please comment on the reproducibility of the paper\",\n",
    "    \"detailed\": \"Please provide detailed and constructive comments for the authors\",\n",
    "    \"rate\": \"Rate the paper on a scale of 1-8, 8 being the strongest\",\n",
    "    \"justification\": \"Please justify your recommendation.\",\n",
    "    \"number of paper\": \"Number of papers in your stack\",\n",
    "    \"ranking\": \"What is the ranking of this paper in your review stack?\",\n",
    "    \"confidence\": \"Reviewer confidence\",\n",
    "    \"rate rebuttal\": \"[Post rebuttal] After reading the authorâ€™s rebuttal, state your overall opinion of the paper if it has been changed\",\n",
    "    \"justification rebuttal\": \"[Post rebuttal] Please justify your decision\",\n",
    "}\n",
    "\n",
    "# List of string categories for reviews\n",
    "list_categories_str = [\"contribution\", \"strengths\", \"weakness\", \"reproducibility\", \"detailed\", \"justification\"]\n",
    "\n",
    "# List of categories for reviews with scores\n",
    "list_categories_scores = [\"clarity\", \"rate\", \"confidence\", \"rate rebuttal\"]\n",
    "\n",
    "# Columns for the reviews data frame\n",
    "columns_reviews = [\"id\", \"category\", \"title\", \"review 1\", \"review 2\", \"review 3\"]\n",
    "\n",
    "# Columns for the statistics data frame\n",
    "columns_statistics = [\"id\", \"category\", \"title\", \"words1\", \"words2\", \"words3\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_accepted_paper_list(year: str = \"2023\"):\n",
    "    \"\"\"\n",
    "    Get the list of all html files from the chosen year\n",
    "    \"\"\"\n",
    "\n",
    "    # Base URL of the MICCAI conference website\n",
    "    miccai_website_path = \"https://conferences.miccai.org\"\n",
    "\n",
    "    # Constructing the URL to access the list of papers for the specified year\n",
    "    path_online_list = miccai_website_path + f\"/{year}/papers/\"\n",
    "\n",
    "    # Opening the URL and reading its contents\n",
    "    reponse = urllib.request.urlopen(path_online_list)\n",
    "    contenu_web = reponse.read().decode('UTF-8')\n",
    "\n",
    "    # Parsing the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(contenu_web, \"html.parser\")\n",
    "\n",
    "    # Finding all anchor ('a') tags in the HTML\n",
    "    all = soup.find_all('a')\n",
    "\n",
    "    # Creating a list of URLs for papers. It filters for links that end with 'html'\n",
    "    paper_list = [(miccai_website_path + link.get('href')) for link in all if link.get('href').endswith('html')]\n",
    "\n",
    "    # Returning the list of paper URLs\n",
    "    return paper_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_reviews(paper:str, df_reviews:pd.DataFrame, df_stats:pd.DataFrame, df_scores:pd.DataFrame, year:str=\"2023\", list_categories:list=[\"reproducibility\"]):\n",
    "    \"\"\"\n",
    "    Extract the 3 first reviews from a given paper webpage.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    paper: str\n",
    "        URL of the paper webpage.\n",
    "    df_reviews: pd.DataFrame \n",
    "        DataFrame to store extracted reviews and code links.\n",
    "    df_stats: pd.DataFrame\n",
    "        DataFrame to store statistics related to reviews (e.g., word count).\n",
    "    df_scores: pd.DataFrame\n",
    "        DataFrame to store reviews containing scores.\n",
    "    year: str, optional\n",
    "        Year of the conference. Defaults to \"2023\".\n",
    "    list_categories: list[str], optional\n",
    "        Categories to extract reviews for. Defaults to \"reproducibility\".\n",
    "    \"\"\"\n",
    "\n",
    "    # Open HTML webpage and extract with BeautifulSoup\n",
    "    response = urllib.request.urlopen(paper)\n",
    "    content_web = response.read().decode('UTF-8')\n",
    "    soup = BeautifulSoup(content_web, \"html.parser\")\n",
    "\n",
    "    # Extract paper title and ID from the webpage\n",
    "    if year == \"2023\":\n",
    "        paper_title = soup.find(\"title\").get_text().rstrip(\"MICCAI 2023 - Accepted Papers, Reviews, Author Feedback\").rstrip(' |')\n",
    "    elif year == \"2022\":\n",
    "        paper_title = soup.find(\"title\").get_text().rstrip(\"MICCAI 2022 - Accepted Papers and Reviews\").rstrip(' |')\n",
    "    paper_id = Path(paper).name[:13]\n",
    "\n",
    "    # Extract code link from the webpage\n",
    "    code_link = soup.find(id=\"code-id\")\n",
    "    code_link = code_link.find_next(\"p\").get_text()\n",
    "    code_link = code_link.replace(\"\\n\", \" \")\n",
    "\n",
    "    # Store code link information in the DataFrame\n",
    "    df_reviews.loc[(paper_id, unidecode(paper_title)), (\"code link\", \"code link\")] = code_link\n",
    "\n",
    "    # Loop through specified categories to extract reviews\n",
    "    for category in list_categories:\n",
    "        text = list_review_text[category]\n",
    "\n",
    "        # Find paragraphs containing reviews for the specified category\n",
    "        repro_reviews_paragraph = soup.find_all(lambda tag: tag.name == \"li\" and text in tag.text)\n",
    "        repro_exact_text = soup.find(lambda tag: tag.name == \"strong\" and text in tag.text).get_text()\n",
    "\n",
    "        i = 0\n",
    "        # Loop through the first 3 reviews for the specified category\n",
    "        for review in repro_reviews_paragraph[:3]:\n",
    "            i += 1\n",
    "            tmp_review = unidecode(review.get_text().strip(repro_exact_text))\n",
    "            tmp_review = tmp_review.strip(\"\\n\")\n",
    "            tmp_review = tmp_review.replace(\"\\n          \\n\", \" \")\n",
    "            tmp_review = tmp_review.replace(\"\\t\", \" \")\n",
    "            tmp_review = tmp_review.replace(\"\\n\\n\\n\\n\", \" \")\n",
    "            tmp_review = tmp_review.replace(\"\\n\\n\\n\", \" \")\n",
    "            tmp_review = tmp_review.replace(\"\\n\\n\", \" \")\n",
    "            tmp_review = tmp_review.replace(\"\\n\", \" \")\n",
    "            tmp_review = tmp_review.replace(\"\\t\", \" \")\n",
    "\n",
    "            # Store review information in the appropriate DataFrame\n",
    "            if category in list_categories_str:\n",
    "                df_reviews.loc[(paper_id, unidecode(paper_title)), (category, f\"review {i}\")] = tmp_review\n",
    "                df_stats.loc[(paper_id, unidecode(paper_title)), (category, f\"review {i}\")] = len(str(tmp_review).split())\n",
    "            elif category in list_categories_scores:\n",
    "                df_scores.loc[(paper_id, unidecode(paper_title)), (category, f\"review {i}\")] = tmp_review\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def extract_paragraph(paper_list:list, year=\"2023\"):\n",
    "    \"\"\"\n",
    "    Extract the 3 first reviews of all paper from the given list.\n",
    "\n",
    "    Return 3 DataFrames:\n",
    "        - df_reviews: DataFrame containing the 3 first reviews of all papers\n",
    "        - df_stats: DataFrame containing the number of words for each review of df_reviews\n",
    "        - df_scores: DataFrame containing the scores given by the 3 first reviewers of all papers.\n",
    "    \"\"\"\n",
    "    iterables_str = [list_categories_str, [\"review 1\", \"review 2\", \"review 3\"]]\n",
    "    iterables_score = [list_categories_scores, [\"review 1\", \"review 2\", \"review 3\"]]\n",
    "\n",
    "    index_line_str = pd.MultiIndex.from_product(iterables_str, names=[\"category\", \"review\"])\n",
    "    index_line_score = pd.MultiIndex.from_product(iterables_score, names=[\"category\", \"review\"])\n",
    "    index_column = pd.MultiIndex.from_product([[], []], names=[\"id\", \"title\"])\n",
    "\n",
    "    df_reviews = pd.DataFrame(index=index_column, columns=index_line_str)\n",
    "    df_stats = pd.DataFrame(index=index_column, columns=index_line_str)\n",
    "    df_scores = pd.DataFrame(index=index_column, columns=index_line_score)\n",
    "\n",
    "    for i, paper in enumerate(paper_list):\n",
    "        print(f\"Processing paper {i + 1}/{len(paper_list)}\", end='\\r', flush=True)\n",
    "        list_categories = list_categories_str + list_categories_scores\n",
    "        extract_reviews(paper, df_reviews, df_stats, df_scores, year, list_categories)\n",
    "\n",
    "    return df_reviews, df_stats, df_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_total_words(df_words, year_=\"2023\", output_directory=\"results\"):\n",
    "    \"\"\"\n",
    "    Count total number of words of each review\n",
    "    by summing the number of words of each category\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_words: pd.DataFrame\n",
    "        DataFrame containing reviews and their word counts\n",
    "    year_: str \n",
    "        Year for which the function is being applied (default: \"2023\")\n",
    "    output_directory: str \n",
    "        Directory to save the results CSV file (default: \"results\")\n",
    "    \"\"\"\n",
    "\n",
    "    # Define the path for the output CSV file\n",
    "    path_words_count = output_directory / 'count_words.csv'\n",
    "\n",
    "    # Iterate through unique IDs in the DataFrame\n",
    "    for id, id_df in df_words.groupby(level=0):\n",
    "        # Iterate through titles associated with the current ID\n",
    "        for _, title in id_df.index.values:\n",
    "            # Initialize word count for each of the three reviews\n",
    "            for i in range(1, 4):\n",
    "                df_words.loc[(id, title), (\"total\", f\"review {i}\")] = 0\n",
    "\n",
    "            # Iterate through categories in the list_categories_str\n",
    "            for category in list_categories_str:\n",
    "                # Sum the words from each category for each review\n",
    "                for i in range(1, 4):\n",
    "                    if math.isnan(df_words.loc[(id, title), (category, f\"review {i}\")]):\n",
    "                        # Handle NaN values by replacing them with 0\n",
    "                        df_words.loc[(id, title), (category, f\"review {i}\")] = 0\n",
    "                    df_words.loc[(id, title), (\"total\", f\"review {i}\")] += df_words.loc[(id, title), (category, f\"review {i}\")]\n",
    "\n",
    "    # Sort the resulting DataFrame and save it to a CSV file\n",
    "    df_words.sort_index(axis=1, ascending=True, inplace=True)\n",
    "    df_words.to_csv(path_words_count, index=True, sep=\"\\t\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_repro_copy_paste(df_all_reviews, output_directory, threshold: int = 10):\n",
    "    \"\"\"\n",
    "    Find reviewers that have copy/paste the reproducibility review in another category.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_all_reviews: pd.DataFrame\n",
    "        DataFrame containing reviews for each paper\n",
    "    output_directory: str \n",
    "        Directory to save the results CSV file\n",
    "    threshold: int \n",
    "        Minimum word count for a review to be considered (default: 10)\n",
    "    \"\"\"\n",
    "    from copy import copy\n",
    "\n",
    "    # Create a deep copy of the input DataFrame to preserve the original data\n",
    "    df_all_reviews_wo_copy_paste = copy(df_all_reviews)\n",
    "\n",
    "    # Initialize a new DataFrame to store reviews with copy/paste\n",
    "    df_bad_reviews = pd.DataFrame(columns=columns_reviews)\n",
    "    df_bad_reviews.set_index([\"id\", \"category\"], inplace=True)\n",
    "\n",
    "    # Iterate through unique IDs in the DataFrame\n",
    "    for id, id_df in df_all_reviews.groupby(level=0):\n",
    "        # Iterate through titles associated with the current ID\n",
    "        for _, title in id_df.index.values:\n",
    "            # Iterate through categories in list_categories_str\n",
    "            for category in list_categories_str:\n",
    "                # Check if the category is not \"reproducibility\"\n",
    "                if category != \"reproducibility\":\n",
    "                    # Iterate through review indices (1 and 2)\n",
    "                    for i in range(1, 3):\n",
    "                        # Extract reproducibility and category reviews\n",
    "                        repro = id_df.loc[(id, title), (\"reproducibility\", f\"review {i}\")]\n",
    "                        cate = id_df.loc[(id, title), (category, f\"review {i}\")]\n",
    "\n",
    "                        # Check if the reproducibility review is long enough and is a substring of the category review\n",
    "                        if len(str(repro).split()) >= threshold and str(repro) in str(cate):\n",
    "                            # Store the problematic reviews in the df_bad_reviews DataFrame\n",
    "                            df_bad_reviews.loc[(id, category), \"title\"] = title\n",
    "                            df_bad_reviews.loc[(id, category), f\"review {i}\"] = id_df.loc[(id, title), (category, f\"review {i}\")]\n",
    "                            df_bad_reviews.loc[(id, category), \"reproducibility\"] = id_df.loc[(id, title), (\"reproducibility\", f\"review {i}\")]\n",
    "\n",
    "                            # Drop the row from the df_all_reviews_wo_copy_paste DataFrame (not used currently)\n",
    "                            try:\n",
    "                                df_all_reviews_wo_copy_paste.drop((id, title), inplace=True)\n",
    "                            except:\n",
    "                                pass\n",
    "\n",
    "    # Save the results to CSV files\n",
    "    df_bad_reviews.to_csv(os.path.join(output_directory, f'reviews_copy_paste_{threshold}.csv'), index=True, sep=\"\\t\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_checklist(df_all_reviews, output_directory, category=\"reproducibility\"):\n",
    "    \"\"\"\n",
    "    Find reviewers that have mentioned the word 'checklist' in the reproducibility review.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df_all_reviews: pd.DataFrame\n",
    "        DataFrame containing reviews for each paper\n",
    "    output_directory: str \n",
    "        Directory to save the results CSV file\n",
    "    category: str \n",
    "        The review category to analyze (default: \"reproducibility\")\n",
    "    \"\"\"\n",
    "    # Initialize a new DataFrame to store reviews mentioning 'checklist'\n",
    "    df_checklist = pd.DataFrame(columns=columns_reviews)\n",
    "    df_checklist.set_index([\"id\", \"title\"], inplace=True)\n",
    "\n",
    "    # Iterate through unique IDs in the DataFrame\n",
    "    for id, id_df in df_all_reviews.groupby(level=0):\n",
    "        # Iterate through titles associated with the current ID\n",
    "        for _, title in id_df.index.values:\n",
    "            # Iterate through review indices (1 and 2)\n",
    "            for i in range(1, 3):\n",
    "                # Extract the review text from the specified category\n",
    "                review = str(df_all_reviews.loc[(id, title), (category, f\"review {i}\")])\n",
    "\n",
    "                # Check if the review mentions 'checklist' in various formats\n",
    "                if (\"check-list\" in review) or (\"checklist\" in review) or (\"check list\" in review):\n",
    "                    # Store the problematic reviews in the df_checklist DataFrame\n",
    "                    df_checklist.loc[(id, title), f\"review {i}\"] = df_all_reviews.loc[(id, title), (category, f\"review {i}\")]\n",
    "                    df_checklist.loc[(id, title), \"category\"] = category\n",
    "\n",
    "    # Save the results to a CSV file\n",
    "    df_checklist.to_csv(os.path.join(output_directory, f'{category}_checklist_reviews.csv'), index=True, sep=\"\\t\", encoding='utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rating_excel(df_all_reviews, output_directory,year: str = \"2023\"):\n",
    "    \"\"\"\n",
    "    Save the excel file with the reproducibility reviews to create the rating file\n",
    "    \"\"\"\n",
    "    df_repro_excel = df_all_reviews.loc[:, (\"reproducibility\")]\n",
    "    df_repro_excel.to_excel(os.path.join(output_directory ,f'reviews_reproducibility_{year}.xlsx'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extract reviews and count words for year 2023\n",
      "Processing paper 730/730\r"
     ]
    }
   ],
   "source": [
    "# Note: Extraction may take 5-10 minutes\n",
    "\n",
    "# Default year is 2023\n",
    "# To change to another year, modify the variable below\n",
    "year_ = \"2023\"\n",
    "\n",
    "# Get the list of HTML pages for the different papers\n",
    "paper_list = get_accepted_paper_list(year=year_)\n",
    "\n",
    "# Set output directories and files for the extraction\n",
    "output_directory = Path(f\"../miccai{year_}\")\n",
    "output_directory = Path(\"../miccaitest\")\n",
    "if not output_directory.is_dir():\n",
    "    os.mkdir(output_directory)\n",
    "\n",
    "csv_directory = output_directory / \"extract-csv\"\n",
    "if not csv_directory.is_dir():\n",
    "    os.mkdir(csv_directory)\n",
    "\n",
    "path_all_reviews = csv_directory / 'reviews.csv'\n",
    "path_all_stats = csv_directory / 'count_words.csv'\n",
    "path_all_scores = csv_directory / 'scores.csv'\n",
    "\n",
    "# Skip the extraction if the files already exist\n",
    "# To rerun the extraction, remove the directory \"miccaiYYYY\" where YYYY is the year\n",
    "if (not path_all_reviews.is_file()) or (not path_all_stats.is_file()) or (not path_all_scores.is_file()):\n",
    "    print(f\"Extract reviews and count words for year {year_}\")\n",
    "    \n",
    "    # Extract reviews, stats, and scores\n",
    "    df_all_reviews, df_all_stats, df_all_scores = extract_paragraph(paper_list, year_)\n",
    "    \n",
    "    # Save the results to CSV files\n",
    "    df_all_scores.to_csv(path_all_scores, index=True, sep=\"\\t\", encoding='utf-8')\n",
    "    df_all_reviews.to_csv(path_all_reviews, index=True, sep=\"\\t\", encoding='utf-8')\n",
    "    df_all_stats.to_csv(path_all_stats, index=True, sep=\"\\t\", encoding='utf-8')\n",
    "\n",
    "else:\n",
    "    print(f\"Files already exist\")\n",
    "    print(f\"Skipping extraction and importing existing CSV from {output_directory}\")\n",
    "    print(f\"If you want to rerun the extraction, delete the directory miccaiYYYY where YYYY is the year\")\n",
    "    \n",
    "    # Load existing CSV files\n",
    "    df_all_reviews = pd.read_csv(path_all_reviews, sep=\"\\t\", header=[0, 1], index_col=[0, 1], skip_blank_lines=True)\n",
    "    df_all_stats = pd.read_csv(path_all_stats, sep=\"\\t\", header=[0, 1], index_col=[0, 1], skip_blank_lines=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year 2023 and outputting result in ../miccaitest\n",
      "Identify reproducibility reviews which contain copy/paste from other parts (need to have more than 10 consecutive words in common)\n",
      "Count the number of reviews that mention the checklist\n",
      "Create excel files for human rating\n"
     ]
    }
   ],
   "source": [
    "print(f\"Processing year {year_} and outputting result in {output_directory}\")\n",
    "# Count the total number of words in each review\n",
    "# Extracted results are saved in 'count_words.csv'\n",
    "count_total_words(df_words=df_all_stats, year_=2023, output_directory=csv_directory)\n",
    "\n",
    "print(\"Identify reproducibility reviews which contain copy/paste from other parts (need to have more than 10 consecutive words in common)\")\n",
    "# Find reviewers that have copy/pasted the reproducibility review in another category\n",
    "# Results are saved in 'reviews_copy_paste_{threshold}.csv'\n",
    "get_repro_copy_paste(df_all_reviews=df_all_reviews, output_directory=csv_directory)\n",
    "\n",
    "print(\"Count the number of reviews that mention the checklist\")\n",
    "# Find reviews that mention the word 'checklist' in the reproducibility review\n",
    "# Results are saved in 'reproducibility_checklist_reviews.csv'\n",
    "count_checklist(df_all_reviews=df_all_reviews, output_directory=csv_directory, category=\"reproducibility\")\n",
    "\n",
    "human_rating_dir = '../human_rating/'\n",
    "print(\"Create excel files for human rating\")\n",
    "# Create Excel files for human rating\n",
    "create_rating_excel(df_all_reviews=df_all_reviews, output_directory=human_rating_dir, year=year_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "reproducibility-reviews",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
