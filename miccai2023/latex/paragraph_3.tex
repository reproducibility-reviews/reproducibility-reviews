The checklist category that was most often commented upon was 'Code' ($47\%$ of reviews $(126.00/90)$, $95\% \text{CI} [41\%, 53\%]$).
The lowest frequency was found for the 'Error bars or statistical significance' category ($2\%$ of reviews $(5.00/90)$, $95\% \text{CI} [0\%, 4\%]$).
Around 2\% of reviews commented upon “Error bars and/or statistical significance”.

$60.74\%$ of reviews provided a statement (164/270, $95\% \text{CI }$ $[54\%, 66\%]$) and 
$72.22\%$ came with a comments (195/270, $95\% \text{CI }$ $[67\%, 77\%]$)\footnote{'Unusable' statements and comments are not taken into account.}. 
Of note, $39.39\%$ of reviewers which had made a positive statement regarding the reproducibility (indicating that they found that the reproducibility of the paper was overall satisfactory) provided no comments to substantiate their statement (52/132, $95\% \text{CI }$ $[31\%, 47\%]$).

Importantly, there was no agreement between reviewers with respective Fleiss'$\kappa$ values of $-0.05$ ($95\% \text{CI } [-0\%, 0\%]$) for statement 
and $0$ ($95\% \text{CI } [-0\%, 0\%]$) for meta-category.

For 87\% of papers, at least one of the reviewers said that the code was or will be available (78/90, $95\%\text{CI } [79\%, 93\%]$). 
However, for 53\% of these,  the code was actually missing in the published version (no link, broken link or empty repository) (41/78, $95\%\text{CI } [41\%, 64\%]$). 

Finally, 68\% of published papers provided an associated repository for the code (61/90, $95\%\text{CI } [58\%, 77\%]$). 
However, for 30\% of these, the link was broken or the repository was empty (18/61, $95\%\text{CI } [20\%, 41\%]$). 
