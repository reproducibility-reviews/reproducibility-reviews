The checklist category that was most often commented upon was 'Code' ($47\%$ of reviews $(126.00/90)$, $95\% \text{CI} [40\%, 52\%]$).
The lowest frequency was found for the 'Experimental results' category ($26\%$ of reviews $(69.00/90)$, $95\% \text{CI} [20\%, 30\%]$).
Around 2\% of reviews commented upon “Error bars and/or statistical significance”.

$61\%$ of reviews provided a statement (164/270, $95\% \text{CI }$ $[55\%, 67\%]$) and 
$72\%$ came with a comments (195/270, $95\% \text{CI }$ $[67\%, 77\%]$)\footnote{'Unusable' statements and comments are not taken into account.}. 
Of note, $39\%$ of reviewers which had made a positive statement regarding the reproducibility (indicating that they found that the reproducibility of the paper was overall satisfactory) provided no comments to substantiate their statement (52/132, $95\% \text{CI }$ $[31\%, 48\%]$).

Importantly, there was no agreement between reviewers with respective Fleiss'$\kappa$ values of $-0.05$ ($95\% \text{CI } [-0.14\%, 0.04\%]$) for statement 
and $0.02$ ($95\% \text{CI } [-0.08\%, 0.12\%]$) for meta-category.

For 87\% of papers, at least one of the reviewers said that the code was or will be available (78/90, $95\%\text{CI } [80\%, 93\%]$). 
However, for 53\% of these,  the code was actually missing in the published version (no link, broken link or empty repository) (41/78, $95\%\text{CI } [41\%, 64\%]$). 

Finally, 68\% of published papers provided an associated repository for the code (61/90, $95\%\text{CI } [58\%, 78\%]$). 
However, for 30\% of these, the link was broken or the repository was empty (18/61, $95\%\text{CI } [18\%, 41\%]$). 
