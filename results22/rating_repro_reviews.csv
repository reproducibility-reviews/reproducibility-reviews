Paper ID	Paper title	For scoring a given reviewer	Put 1 if the reviewer has commented on at least one of  the items falling into this category	Put 1 if the reviewer has commented on at least one of  the items falling into this category	Put 1 if the reviewer has commented on at least one of  the items falling into this category	Put 1 if the reviewer has commented on at least one of  the items falling into this category	Put 1 if the reviewer has commented on at least one of these two aspects	Put 1 if the reviewer has said that the code is or will be available	Choose the category of statement	Choose the category of comments	For scoring a given reviewer									For scoring a given reviewer									Meta-categories			Aggregate over 3 reviews						Put 1 if all reviewers agree. Put 0 if at least two disagree, or if at least one review is too vague to tell if they agree		
		Review 1	Models and algorithms	Datasets	Code	Experimental results	Error bars or statistical significance	Code is or will be available	Statement	Comments	Review 2	Models and algorithms	Datasets	Code	Experimental results	Error bars or statistical significance	Code is or will be available	Statement	Comments	Review 3	Models and algorithms	Datasets	Code	Experimental results	Error bars or statistical significance	Code is or will be available	Statement	Comments	Review 1	Review 2	Review 3	Models and algorithms	Datasets	Code	Experimental results	Error bars or statistical significance	meta-category	Agreement	Code link	Repo provided and not empty
1	3D CVT-GAN: A 3D Convolutional Vision Transformer-GAN for PET Reconstruction 	This work is reproducible.									Code is not provided.									Not sure, if the authors agree to share the code to the community, I would consider change my rate																				
2	3D Global Fourier Network for Alzheimer's Disease Diagnosis using Structural MRI 	Code will be available but the authors will not provide pre-trained models or evaluation code.									Could be reproducible if the code will be available since the data are public.									It is possible to be reproduced.																				
3	4D-OR: Semantic Scene Graphs for OR Domain Modeling 	Although the authors promise to release the codes, the reproducibility of this paper is at a high risk due to the huge pipeline of existing methods and many ad-hoc components. I would recommend the authors package all the pipeline components together into a for example Docker container.									Details about implementation, all the off-the-shelf approaches, hyperparameters, data split are provided. However, the paper lacks explanations about how clinically realistic the simulated surgeries are.									The author committed to release the code upon acceptance.																				
4	A Comprehensive Study of Modern Architectures and Regularization Approaches on CheXpert5000 	Good									The paper relies on publicly available pre-trained models and datasets. New splits defined but will be provided for the final version. The methodology is described in detail, but a few details are not very clear, in particular: The description of the fine-tuning setting is not very clear (see detailed comments to authors) Which pretrained models were used in the experiments and the link to retrieve them.									The reproducibility of the paper may be limited due to the requirement of large compute and data requirement. The authors also do not publish their code or data used.																				
5	A Deep-Discrete Learning Framework for Spherical Surface Registration 	The proposed method seems reproducible.									Ok									The method's architecture should be reproducible from the descriptions in the paper. The dataset used for training and evaluation is also public. However, to directly reproduce method and results, parameters of the method would need to be published.																				
6	A Geometry-Constrainted Deformable Attention Network for Aortic Segmentation 	It would be interesting to have access to the database, but I know that it is not evident. Moreover, the manual segmentation is questionable									Some further implementation details are provided as supplementary materials.  Do authors do not plan to publish code or trained models. CT data is also not publicly available.  Details on ground truth generation are also limited.									Although the data and code is not public, the authors list most of the details of the method in the paper.																				
7	A Hybrid Propagation Network for Interactive Volumetric Image Segmentation 	The methods are detailed and reproducible									"It would probably be difficult to reproduce the method as it includes multiple modules, implementation details of which are not completely clear, e.g. It is unclear what ""Semantic segmentation loss"" means? It is unclear which model wights are referred to be ""the pre-trained weights on video object segmentation is used for initialization"" The details of scribble generation are missing."									"Training/evaluation code, as well as pretrained models and data, are specified as ""available"" but no link has been provided."																				
8	A Learnable Variational Model for Joint Multimodal MRI Reconstruction and Synthesis 	The method presented in this paper is loaded with technical details. I think it would be challenging to reproduce this work without the help of publicized codes.									The authors didn't claim that they will release the code.									This paper can be reproduced.																				
9	A Medical Semantic-Assisted Transformer for Radiographic Report Generation 	The paper did not provide the code, part of the introduction is not clear, not easy to reproduce.									The reproducibility of the paper is good, because the author says source code and the pre-trained models will be made available to the public.									The dataset is publicly available. Authors have said that source code and pre-trained models will be made available to the public.																				
10	A Multi-task Network with Weight Decay Skip Connection Training for Anomaly Detection in Retinal Fundus Images 	Codes are promised to be released soon.									Since the authors described their method and implementation details clearly, this paper has good reproducibility. Besides, they also intend to release their source code related to this work.									This paper seems reproductive.																				
11	A New Dataset and A Baseline Model for Breast Lesion Detection in Ultrasound Videos 	OK									These seems to be no problem with reproducibility. The random shuffling and then taking the consecutive frames within the randomly ordered frames could be improved. The fact that the authors will provide data and code definitely supports the reproducibilty.									There has been no downloadable link for the dataset used in the paper.																				
12	A Novel Deep Learning System for Breast Lesion Risk Stratification in Ultrasound Images 	Most of the presented methodology is clear and easy to reproduce. However, reproducing the results may be difficult due to the following unclear parts that I mentioned above, namely: (i) derivation of the soft labels, (ii) handling multiple images by the same patient, (iii) handling inconsistencies between BI-RADS and pathology.									No code provided. 2 public datasets, 1 private.									The paper states relevant hyperparameters and present results on 2 public datasets. Though the code and inhouse dataset aren't released.																				
13	A Novel Fusion Network for Morphological Analysis of Common Iliac Artery 	Paper is clear that an expert could reproduce									No source code is given. But, the method section of the paper is introduced so that readers can hopefully repeat or refer to it.									The description of the proposed method is clear and the paper is reproducible.																				
14	A Novel Knowledge Keeper Network for 7T-Free But 7T-Guided Brain Tissue Segmentation 	based on the checklist, yes.									No source code was provided.									Good. The author has opened the code.																				
15	A Penalty Approach for Normalizing Feature Distributions to Build Confounder-Free Models 	Reproducibility is fair, except for the choice of lambda, which has not been described.									This paper should be reproducible.									The code and data can be released according to the reproducibility checklist.																				
16	A Projection-Based K-space Transformer Network for Undersampled Radial MRI Reconstruction with Limited Training Subjects 	No data or software  provided. Hard to reproduce the algorithm based on the  description.									The authors did not comment on the reproducibility.									The authors used a private dataset which makes it impossible to reproduce. They do list some of the hyper-parameters used for training the model.																				
17	A Robust Volumetric Transformer for Accurate 3D Tumor Segmentation 	Code is given and data is public,									Code is present in supplementary.									Great. The used dataset is public data, and the code is available.																				
18	A Self-Guided Framework for Radiology Report Generation 	The paper does not provide a link to the code. The description of the paper is relatively clear and generally reproducible.									Paper will be reproducible if the authors release their code.									The authors describe the model development methodology in detail and demonstrate ablation study results. Would be ideal if access can be provided to code or tools used for study.																				
19	A Sense of Direction in Biomedical Neural Networks 	The authors mention that the code and dataset will be made available.									"The paper generally presents sufficient detail and references for reimplementation. ""In practice we use a set of Gabor reference functions.."" It would be good to provide some details about the parameters used."									Although some information is missing, such as training epoch, learning rate, and footprint, the authors claimed in abstract that they will release the code and data, so the reproducibility is good.																				
20	A Spatiotemporal Model for Precise and Efficient Fully-automatic 3D Motion Correction in OCT 	Nor the code, nor the dataset are publicly available. The framework is said to be made available in the future, once further work is performed and the framework is considered closer to completion The mathematical support of the method is clear and detailed enough to try to attempt an implementation somewhat close to the one the authors might have done. While the dataset is not publicly available, it is sufficiently well described to be able to acquire a similar one, if access to a suitable device is had.									How to get the X/Y fast motion volumes may determine the reproducibility of the paper.									however the code is not provided , but provided visual inspection is satisfactory																				
21	A Transformer-Based Iterative Reconstruction Model for Sparse-View CT Reconstruction 	The manuscript has clearly described the network architecture and the reconstruction workflow. Most of the work is reproducible.									The implementation of the network looks simple, but we still hope the authors release the codes.									Well done.																				
22	AANet: Artery-Aware Network for Pulmonary Embolism Detection in CTPA Images 	The reproducibility of the paper is OK.									The reproducibility of this article is possible.The network structure parameters and loss function are described in the article.									Authors claim that the dataset and the network/weights will be made public. I believe that the reproducibility is good.																				
23	Accelerated pseudo 3D dynamic speech MR imaging at 3T using unsupervised deep variational manifold learning 	The reproducibility criteria have been met									The authors have attempted to make the code publicly available, which should ensure good reproducibility.									This paper is presented clearly and the model is easy to reimplement although the code is not publicized.																				
24	Accurate and Explainable Image-based Prediction Using a Lightweight Generative Model 	I belileve the method to be difficult to reproduce for unfamiliar audiences, not comfortable with the needed mathematics (since as far as I am aware, no code will be made available).									The authors go to pains to tell us how they implemented and evaluated competing methods, in quite some detail.  In that sense reproducibility is high.   The data set is open source.									Most implementation details are provided in the paper. Methods are easy to re-implement, although no code is provided in the supplementary details.																				
25	Accurate and Robust Lesion RECIST Diameter Prediction and Segmentation with Transformers 	What were the Adam optimizer hyperparameters (other than learning rate) and what was the batch size? While not mandatory, I strongly encourage the authors to release the code for reproducibility - this is the most productive and useful way to allow reproducibility.									Overall, the reproducibility form in my perspective well matched the manuscript. Regarding a few points, I felt that the authors might adapt their answers, as pointed out in the following: A clear declaration of what software framework and version you used. [Yes] -> No I felt, that this was not described in detail. While it was mentioned that the authors used PyTorch, no versions have been documented. For all reported experimental results, check if you include: The range of hyper-parameters considered, method to select the best hyper-parameter configuration, and specification of all hyper-parameters used to generate results. [Yes] -> No This in so far as basically no hyper-parameter optimization seems to have been conducted. Details on how baseline methods were implemented and tuned. [Yes] -> No While the authors have stated to have trained the methods on the same data, they did to the best of my understanding not comment on whether they used one of the original implementations or not, nor whether they conducted any kind of tuning for the data at hand. An analysis of situations in which the method failed. [Yes] -> No While I might have overlooked this, I did not find an error-case assesment in the manuscript.									yes																				
26	Accurate Corresponding Fiber Tract Segmentation via FiberGeoMap Learner 	Good.									good									The code and video for predicted fiber tracts openly available via GitHub.																				
27	ACT: Semi-supervised Domain-adaptive Medical Image Segmentation with Asymmetric Co-Training 	Code is provided.									The manuscript misses some technical details, for example, the implementations of SSL and UDA in the proposed ACT framework. Fortunately, the code is provided.									The author submitted the code as the support material. There are no reproducibility concerns.																				
28	Adaptation of Surgical Activity Recognition Models Across Operating Rooms 	Code is not available while the reproducibility responses are checked.									The information provided will allow the reproducibility of this paper. The release of the dataset will be a plus to allow further comparison.									Experiment parameters are given in the Supp Materials, not sure if it will be published with the paper. The model trained and assessed on private dataset make it difficult to reproduce the results.																				
29	Adapting the Mean Teacher for keypoint-based lung registration under geometric domain shifts 	Good. Authors have provided the code.									The method seems fully reproducible, and the code is provided.									* The authors shared their code, which thus is highly reproducible.																				
30	Adaptive 3D Localization of 2D Freehand Ultrasound Brain Images 	OK									The work seems reproducible									I couldn't fully understand Regression ConvNet because authors didn't provide any details. Supplementary data was helpful for better understanding the work.																				
31	AdaTriplet: Adaptive Gradient Triplet Loss with Automatic Margin Learning for Forensic Medical Image Matching 	This paper is highly reproducible.									The code is provided so it is reproducible.									All relevant hyperparameters and pipeline settings are clearly listed, giving me no reason to doubt the reproducibility of the paper results.																				
32	Addressing Class Imbalance in Semi-supervised Image Segmentation: A Study on Cardiac MRI 	Satisfactory. Datasets are publicly available, and methods are described with acceptable detail.									The paper presents enough details to reproduce the results on public datasets.									No code sharing was mentioned in the submission. The datasets are publicly avaible.																				
33	Adversarial Consistency for Single Domain Generalization in Medical Image Segmentation 	The reproducibility of this paper is relatively high because most implementation details, such as batch size, learning rate, and parameters of the optimizer, are provided. The authors also describe the details of data prepossessing and evaluation metrics.									Good. The authors listed the data source, model architecture, and promised publishing their code after acceptance.									clear for the experiment settings and network architecture.																				
34	Adversarially Robust Prototypical Few-shot Segmentation with Neural-ODEs 	No issues on reproducibility that I can think of.									Authors provide github website so it is acceptable.									The authors agree to release the code when the paper is accepted, and the datasets they used is publicly available.																				
35	Agent with Tangent-based Formulation and Anatomical Perception for Standard Plane Localization in 3D Ultrasound 	The paper mentions of the training process some relevant hyper-parameters in section 2.3 and 3.1. But the submission did not accompany any code. In the absence of a codebase, reproduction might be challenging.									The model is described in sufficient detail, and the authors outline their experimental procedure clearly. The supplementary provides a video which shows the navigation SPs in 3D US.									Sufficient details are provided in order for the paper to be reproducible																				
36	Aggregative Self-Supervised Feature Learning from Limited Medical Images 	The reproducibility of the paper is good since most of the important implementation details are provided in the manuscript.									Authors well-described experiments in this paper; thus, it is possible to reproduce their experiments.									The experimental setup is described in detail. Following the description can reproduce the results.																				
37	An Accurate Unsupervised Liver Lesion Detection Method Using Pseudo-Lesions 	Please describe the training strategy, including the optimizer, the optimizer's hyperparameters, the number of epochs used, the learning rate schedule (if any), and any other details required to reproduce the results.									The authors answered that they will make the code available which is not mentioned in the paper.									Most details of the networks and datasets are available. Please also include details about hyperparameters, and mage pre-preprocessing steps for both CycleGAN and the main anomaly detection network.																				
38	An adaptive network with extragradient for diffusion MRI-based microstructure estimation 	Results can be reproduced									The 3T in vivo dataset is freely available. It is unclear if the 7T dataset will be made available. The network will be made publicly available on Github. The experimental setup, the method description, and networks architecture structures are sufficiently described.									Authors mentioned that a demo will be provided at https://github.com/Tianshu996/AEME after this work is accepted. The availability of pre-trained models is not clear.																				
39	An Advanced Deep Learning Framework for Video-based Diagnosis of ASD 	The dataset and code will be released; all good.									OK									Details of the framework/experiments are provided in the paper. If the dataset will be released, it would be enough for reproducibility.																				
40	An End-to-End Combinatorial Optimization Method for R-band Chromosome Recognition with Grouping Guided Attention 	The method is described in great detail. However, the loss function definition is missing. Also, the hyperparameters of the Bi-RNN training are missing.									Data and code are not available.									"The results are not reproducible without the code and the dataset. It seems that the authors intend to release the code, but the dataset does not seem to become available. I do not agree with all the claims of the authors: 1) The authors mark yes to the following: ""For new data collected, a complete description of the data collection process, such as descriptions of the experimental setup, device(s) used, image acquisition parameters, subjects/objects involved, instructions to annotators, and methods for quality control."" There is absolutely not sufficient description of this in the paper. The description is just from the point of receiving the images, there is no description of the acquisition process. 2) The authors also mark yes to the following: ""Whether ethics approval was necessary for the data."". I cannot find anything about ethical approvals in the manuscript. 3) I generally do not agree with the statements under section 4), e.g. there is no statistical test for comparing the significance of the difference between the methods, although the authors state that the method significantly outperforms state-of-the-art in the conclusions, this is simply misleading... Also I could not find a description of the hardware used."																				
41	An Inclusive Task-Aware Framework for Radiology Report Generation 	Reproducible with some efforts.									positive if code are released									yes																				
42	An Optimal Control Problem for Elastic Registration and Force Estimation in Augmented Surgery 	The relavent form has been filled out by the Authors. Appears to be adequate.									The optimization problem and ajoint problem are well explained and could thus be reproduced in another simulation context.									noting special for this paper  for reproducibility																				
43	Analyzing and Improving Low Dose CT Denoising Network via HU Level Slicing 	ok									The average runtime for each result, is not presented. A declaration of what software framework and version authors used is not presented. A description of the computing infrastructure used (hardware and software) is not presented.									Since the authors used or updated the existing network such as U-Net, REDCNN and CycleGAN, it is easy to reimplement without source code release. It will be trivial to implement HU level slicing if the HU level values are given.																				
44	Analyzing Brain Structural Connectivity as Continuous Random Functions 	No indication in main paper about reproducibility.									The method is novel, the availability of the author's original code may greatly help with the reimplementation. The authors admits promised the availability of the code, but I did not see the download link in the paper or in the supplementary file.									No reproducible experiments provided in this paper. No source code is released.																				
45	Anatomy-Guided Weakly-Supervised Abnormality Localization in Chest X-rays 	Can be reproduced.									The results seem to be reasonable and reproducible.									The reproducibility of the paper is credible.																				
46	Anomaly-aware multiple instance learning for rare anemia disorder classification 	The idea in this paper is clear and the overflow of the proposed framework is also clear. So i think this paper has good reproducibility.									Authors claim they will release the code upon acceptance.									Either the dataset or the code is available. I don't think I have the confidence to reassure that this work is reproduceable.																				
47	Assessing the Performance of Automated Prediction and Ranking of Patient Age from Chest X-rays Against Clinicians 	Maybe the results can be reproductive.									Details required to train the network are missing. This includes some important hyperparameters such as the learning rate, batch size, etc.									easily reproducible																				
48	Asymmetry Disentanglement Network for Interpretable Acute Ischemic Stroke Infarct Segmentation in Non-Contrast CT Scans 	(based on the article alone) I think the article describes the method sufficiently clear, and gives some of the hyper-parameters, so that a reader can sufficiently reimplement the core method. Although the 3D ResUnet itself may not be able to be replicated (not all hyperparameters are here such as width of layers etc). Less reproducible are likely the results themselves, as they are only 1 run per experiment, which means improvements may be subject to the initialization seed, etc. Overall, average.									seems reproducibable.									Dataset available, parameters chosen given, liekly it can be impelemented whilst following the methodology.																				
49	Atlas-based Semantic Segmentation of Prostate Zones 	The reproducibility of this paper is good.									The authors provide specifics and point to the data they used.									This work utilizes publicly available datasets, Prostate x and Prostate 3T. The methods are clearly described and code publicly available, making this work reproducible.																				
50	Atlas-powered deep learning (ADL) - application to diffusion weighted MRI 	Although the architecture is simple (U-Net), the full method is rather complex. Without the code, it seems unlikely one can reproduce the experiments. Although the authors mentioned in the reproducibility checklist that the code was made available, they did not mention it in the paper.									Nothing to say									Authors state training code will be made available, and already publicly available data is used. Parameters of compared methods are not clearly stated (probably due to space limitations), authors claim were chosen according to original paper description but this does not support reproducibility of SOA results. Unclear if statistical tests are done for the ablation study.																				
51	Attention mechanisms for physiological signal deep learning: which attention should we take? 	The paper seems to be pretty reproducible. I would be concerned that if there are any necessary adjustments to apply the self attention methods in the 1D setting as opposed to the 2D setting where they were proposed, these details have been omitted. Otherwise, network diagrams are given for each of the CNNs and the dataset used seems to be open source.									It is reproducible.									The dataset and code are not disclosed in this paper, so the reproducibility of this paper needs to be further improved.																				
52	Attentional Generative Multimodal Network for Neonatal Postoperative Pain Estimation 	The data used is public and the methods are clearly explain. With these 2 elements the results should be reproducible.									Authors use many public dataset and pre-trained models such as USF-MNPAD-I. The implementation details are well described.									Implementation details have been described.																				
53	Attention-enhanced Disentangled Representation Learning for Unsupervised Domain Adaptation in Cardiac Segmentation 	* The authors shared their code, which thus is highly reproducible.									The authors included model details in the section 2 and released the main code on the GitHub.									I'm not sure if it can be reproduced as some modules seem to be unreasonable in my opinion. But the authors provides the code, which is a plus.																				
54	Attentive Symmetric Autoencoder for Brain MRI Segmentation 	Authors state code will be released									As the authors indicate in Abstract, the code and model will be released after accepted. The datasets are all open-accessible and the hyper-parameters of pretraining, fine-tuning and ASA model are also provided in paper, so this work should be reproducible.									The author did sufficient experiments on different public dataset, so it is easy to reproduce.																				
55	Autofocusing+: Noise-Resilient Motion Correction in Magnetic Resonance Imaging 	The authors included scripts but this reviewer encourages the authors to make them public accessible.									Source code is provided. Data from fastMRI database.									The study was conducted on a publicly available dataset. The method is described and could be reimplemented. The authors do not report if source code will be shared.																				
56	AutoGAN-Synthesizer: Neural Architecture Search for Cross-Modality MRI Synthesis 	The code and the data are not available to aid reproducibility.									I believe their experiments are reproducibile.									Since the authors mention they will provide the code to public, it is highly possible to reproduce this paper.																				
57	AutoLaparo: A New Dataset of Integrated Multi-tasks for Image-guided Surgical Automation in Laparoscopic Hysterectomy 	There are some improvements that need to be made on the reproducibility aspect such as: Sharing the data set or at very least a sample of it, especially since this is the data set is the main contribution of this paper. A clear explanation of some of the assumptions made with respect to the data set is needed, especially the parts of the data set on laparoscope motion prediction and anatomy segmentation.									The dataset will be released upon paper publication.									The authors stated that the dataset will be released upon the publication of the paper.																				
58	Automated Classification of General Movements in Infants Using Two-stream Spatiotemporal Fusion Network 	The code is released but the dataset is not.									The code is made available with all steps.									code is avialalbe, dataset not.																				
59	Automatic Detection of Steatosis in Ultrasound Images with Comparative Visual Labeling 	Reproducible. Code and data will be made publicly available.									Good reproducibility. Authors plan to release the codes and the dataset.									One of the datasets used in this paper is public, and the data provided about the networks structure and hyperparameters are enough to reproduce the code. Although the authors have mentioned that they will make the code public.																				
60	Automatic identification of segmentation errors for radiotherapy using geometric learning 	X									The dataset is publicly available, and the authors claim that the code will be made available as well. However, the architecture of the networks is not becoming clear with Fig.2 and 3 only. What are the values in the CNN, GNN and fully connected blocks in Fig.2+3 indicating? Also, the dimension of the features generated by the CNN and later the fully connected layer for the classification is relevant information that is missing.									Due to the provided implement details, I guess it is reproducible.																				
61	Automatic Segmentation of Hip Osteophytes in DXA Scans using U-Nets 	A public database is used, the UK BioBank. No proof of reproducibility.									Overall, I consider reproducibility of the study as high. Several remarks below. The exact model architecture is not given explicitly, eventhough it is understandable from the text to some extent. The authors have checked the code release in the checklist, however, there is no link to the source code in the article. Software versions are not specified for the critical components of the pipeline.									The results should be well reproducible. The authours employed the standard U-Net architecture for segmentation and the initial landmark detection was also carried out using bonefinder software which is also available upon request. The dataset is from UKBB which is also available upon request.																				
62	Automating Blastocyst Formation and Quality Prediction in Time-Lapse Imaging with Adaptive Key Frame Selection 	The authors state they will release the code. Also, the description of the method is clear and simple enough to replicate the proposed architecture and training methodology. The authors state they will release data in the reproducibility form. This would be great as to my knowledge there is no equivalent public, open data.									Not good, as they evaluate the method on their in-house dataset, while do not mention the release plan of both dataset and code									Dataset and codes are not publicly available.																				
63	Automation of clinical measurements on radiographs of children's hips 	Methods are well described and/or referenced and can be reproduced. Results are not reproducible since the dataset is not publicly available.									To the authors' own admission the dataset is lacking examples of cerebral palsy although the the RMP measure, for which SOTA is indicated from the results, is an important measure in cerebral palsy monitoring. As the approach applies a known method, the code is not provided (not applicable)									There is no detail about the random-forest hyperparameters as well as visual descriptors chosen. Therefore this method is barely impossible to reproduce.																				
64	BabyNet: Residual Transformer Module for Birth Weight Prediction on Fetal Ultrasound Video 	The code is publicly available, but the dataset is not.									Authors have provided their source code.									The authors have provided source code at anonymized Github repository. They state that the dataset and pre-trained models will be made available after acceptance.																				
65	Bayesian dense inverse searching algorithm for real-time stereo matching in minimally invasive surgery 	The results are reproducible.									The authors have provided their code of the implementation.									The reproducibility is standard. The authors offer to provide all useful material.																				
66	Bayesian Pseudo Labels: Expectation Maximization for Robust and Efficient Semi-Supervised Segmentation 	Authors have provided necessary information to be able to reproduce this work.									Maybe.									The method is described well enough to be reproduced. The paper uses publicly available datasets and the code will be published.																				
67	Benchmarking the Robustness of Deep Neural Networks to Common Corruptions in Digital Pathology 	Poor. No detail on implementation anywhere, and no code to be published.									This paper seems to be simple to realized but still be a bit unclear.									No code is given (although the authors state this in the questionaire). The authors relate to one of the datasets as a possible benchmark, yet it is not available and no link is provided. It will thus be hard to reproduce the results.																				
68	BERTHop: An Effective Vision-and-Language Model for Chest X-ray Disease Diagnosis 	It looks it is possible to reproduce of the proposed method as it is based on the combination of existing methods.									The dataset is public and the hyper-parameters have been listed.									The authors did not declare whether code will be made public. The individual model components are publicly available making the proposed architecture reproducible. The dataset used is publicly available.																				
69	Bi-directional Encoding for Explicit Centerline Segmentation by Fully-Convolutional Networks 	The implementation code is unavailable and  some part of implementation are unclear.									Reproducible.									According to the reproducibility checklist, the authors do not plan to provide their training or evaluation code. The method itself is fairly straightforward and well described, and a re-implementation should be possible. Most hyperparameters used for training the models are provided in the supplementary material, however, only the CLIP data set is available for reproducing results.																				
70	BiometryNet: Landmark-based Fetal Biometry Estimation from Standard Ultrasound Planes 	The part of Landmark Regression Network seems to be reproducible and it is challenging to reproduce the part of Dynamic Orientation Determination. Ideally, the source code should be released.									The work is fully reproducible since public data sets were used									It is highly recommended for the authors to release their dataset and code for better reproducibility.																				
71	BMD-GAN: Bone mineral density estimation using x-ray image decomposition into projections of bone-segmented quantitative computed tomography using hierarchical learning 	No proof of reproducibility. Experiments achieved on an in-house dataset. Code should be available.									The code is made available. The data is not made available. There is an extensive note about the parameters used in training Reproducability is likely, dependent on the code released.  There are a lot of details in creating the dataset that are probably crucial to reproducing this work.  So without the dataset it may be hard to reproduce. The source of the data is not well described, what is the fidelity (resolution, contrast, reconstruction) of the underlying images?  This lack of clarity could limit reporducability The methods are well described.									Data does not sound to be available but most codes will be available upon acceptance.																				
72	Boundary-Enhanced Self-Supervised Learning for Brain Structure Segmentation 	The proposed method is reproducible.									Reproducibility is doubtful. Please see my response to Q5.									The authors would like to make all codes publicly available, which ensures good reproducibility.																				
73	BoxPolyp: Boost Generalized Polyp Segmentation using Extra Coarse Bounding Box Annotations 	Some clarification needed.									I believe that the obtained results can be reproduced.									Authors have checked yes for the reproducibility of paper.																				
74	Brain-Aware Replacements for Supervised Contrastive Learning in Detection of Alzheimer's Disease 	The dataset is publicly available and the authors will publish their code. So the results should be reliably reproducible.									The paper reports architectural and experimental setups thoroughly. Thus the results should be with little effort reproducible									The author will release their codes in github.																				
75	Breaking with Fixed Set Pathology Recognition through Report-Guided Contrastive Training 	This looks alright to me.									Good reproducibility. Architecture is well defined and results are demonstrated in open source datasets.									I believe that the obtained results can, in principle, be reproduced. Even though key resources (code) are unavailable at this point, the key details (e.g., proof sketches, experimental setup) are sufficiently well described for an expert to confidently reproduce the main results, if given access to the missing resources.																				
76	Building Brains: Subvolume Recombination for Data Augmentation in Large Vessel Occlusion Detection 	Limited reproducibility because of the lack of clarity in the methods section									No code or data available from the paper.									No code, data or models are shared. The reasons for this are not mentioned. Most hyperparameters are shared, but the strategy to choose them is not described.																				
77	CACTUSS: Common Anatomical CT-US Space for US examinations 	Good, but of course would be better if the code could be made available and US data could be made available.									It is adequate.									The authors reference the US simulation algorithm and CUT network and provide parameters for each. As noted in their checklist code is not provided . Overall this limits reproducibility of their work. It would be of great interest to the community if they provided access to their framework to allow others to leverage well-curated CT (or MR?!) datasets for US applications.																				
78	Calibrating Label Distribution for Class-Imbalanced Barely-Supervised Knee Segmentation 	Reproducibility of the paper seems adequate.  The data used is from an open dataset so would be accessible to future investigators wishing to reproduce the result.  The methods and experiments are thoroughly described. Issues with reproducibility: Some of the methods could be more clearly described (see constructive criticism below).  The code does not appear to be open.  There is not very much detail on the software used to implement the algorithm and experiments.  It is stated that Pytorch was used, the version was omitted, further the operating system was omitted.  The hardware used as has only 1 detail, that a 3090 was used.									I think the reproducibility is OK, for the baseline has opened their code, and the implementation of the three modules for addressing the class imbalance problem is not difficult.									Data is available as a public repository. Algorithms are not available, but methods are described very well, so their implementation will be very easy.																				
79	Calibration of Medical Imaging Classification Systems with Weight Scaling 	The formulation seems clear and reproducible. However, code will not be provided. The validation is performed in three publicly available datasets.									The equations are provdided and the data available as well as code links and in this way I think it has a high chance of reproducibility.									It would be desirable to have algorithm 1 in the form of a function implemented in any language and any framework of choice for the sake of reproducibility.																				
80	Camera Adaptation for Fundus-Image-Based CVD Risk Estimation 	The description of the proposed proposed method is clear.									The paper is believed to be reproducible.									I think this article is reproducible.																				
81	Capturing Shape Information with Multi-Scale Topological Loss Terms for 3D Reconstruction 	Code does not appear to be provided, though the authors do use a pre-existing deep learning method for 3D image reconstruction with its corresponding datasets which have been made publicly available.  Some of the TDA methods (computing persistent homology, along with the optimal transport algorithm) would be difficult to reproduce from scratch.									I do not think I would be able to reproduce this paper from the information provided. Some key details are omitted that would be needed to reproduce the work. There are no statements about code availability.									The proposed method is mathematically sound, and the baseline model SHAPR has an open-source implementation. Experimental settings are clearly described. Generally, I have a positive impression of reproducibility.																				
82	Carbon Footprint of Selecting and Training Deep Learning Models for Medical Image Analysis 	Can be reproduced, but probably shouldn't in order to reduce CO2 emissions.									For all models and algorithms, check if you include A clear declaration of what software framework and version you used. [Yes] Software versions are not reported. For all code related to this work that you have made available or will release if this work is accepted, check if you include: Specification of dependencies. [Yes]  Training code. [Yes]  Evaluation code. [Yes]  (Pre-)trained model(s). [Yes]  Dataset or link to the dataset needed to run the code. [Yes]  README file including a table of results accompanied by precise command to run to produce those results. [Yes] I have not seen any references to shared code or data in the manuscript. For all reported experimental results, check if you include: The average runtime for each result, or estimated energy cost. [Yes] The training of models in this work is estimated to use 39.948 kWh of electricity contributing to 11.426 kg of CO2eq. This is equivalent to 94.898 km travelled by car. Excellent! I hope reporting this will soon be a trend.									The reproducibility of the paper is not good.																				
83	CaRTS: Causality-driven Robot Tool Segmentation from Vision and Kinematics Data 	Paper seems like it would be reproducible since the code will be released.									The authors stated that the code and dataset publicly available upon publications, thus it can be assumed that reproducing the results in the paper will be possible.									The code and data seems that will not be released																				
84	CASHformer: Cognition Aware SHape Transformer for Longitudinal Analysis 	The authors agree to release the training code.									The results are reproducible with some effort.									All parts of the model architecture blocks are well described as well as the training procedure (mostly in the supplementary materials). The dataset is exhaustively detailed as well as the splits and the proposed evaluation process. As stated in the reproducibility checklist, not all hyper-parameters tuning / setting is reported, e.g. the loss weight lambda was empirically set.																				
85	Censor-aware Semi-supervised Learning for Survival Time Prediction from Medical Images 	The authors mention that the code will be shared upon acceptance. The raw data used is public; however, the preprocessing to obtain the patches in the pathology case is not described at all, making it challenging to fully reproduce the work.									The paper is clearly written and most of the relevant information is within the paper. The authors also promised to publish their code and conducted their experiments on public data. The results should therefore be reproducible.									The authors declare the code will be publically available.																				
86	CephalFormer: Incorporating Global Structure Constraint into Visual Features for General Cephalometric Landmark Detection 	The code is promised to be public.									The reproducibility of the implementation is not easy to achieve.									No code were made public in the paper.																				
87	Cerebral Microbleeds Detection Using a 3D Feature Fused Region Proposal Network with Hard Sample Prototype Learning 	No strategy is mentioned									* the method is insufficiently described as crucial details such as the used optimizer, the number of epochs or training iterations and batchsize (if any) are missing. * The reproducibility relies on the availability of the used code and the data set as well.									Difficult to replicate. There are a large choice of hyerparameters which are not explained in detail.																				
88	CFDA: Collaborative Feature Disentanglement and Augmentation for Pulmonary Airway Tree Modeling of COVID-19 CTs 	Authors complete experiments on publicly available datasets. Authors claim to publish code and pretrained models, however no github link is provided in the text.									This work could be reproducible									Sufficient details for reproducibility.																				
89	Characterization of brain activity patterns across states of consciousness based on variational auto-encoders 	no concern									The paper meet the crieria on the checklist.									Reproducibility seems OK.																				
90	CheXRelNet: An Anatomy-Aware Model for Tracking Longitudinal Relationships between Chest X-Rays 	The paper seems to be reproducible since it is written pretty clearly, the authors intend to release the code and an open-source dataset was used for evaluation.									The authors have shown assumptions/ implementation details, etc. The evaluation is on a public dataset [26]. The authors are encouraged to make code publicly available.									the study is developed and test on a publicly available dataset. the implementation details and parameters are provided in the manuscript. The codes did not shared.																				
91	ChrSNet: Chromosome Straightening using Self-attention Guided Networks 	The main text makes no mention of code being available.									The reproducibily is adequate. Perhaps, the authors could give more details regarding the key parameters involvedin their method.									In the authors' answer to the reproducibilty checkliost, the authors claim to include all the code. But, I don't see any placeholder in the manuscript.																				
92	CIRDataset: A large-scale Dataset for Clinically-Interpretable lung nodule Radiomics and malignancy prediction 	The authors will make the masks/annotations that were used publicly available. The same is true for the code and pretrained models. Most of the parameters that are important for training are given. I, therefore, believe that the results presented will be reproducible.									Apparently, the authors have not understood how to fill out the checklist: all questions are answered with yes, yet most of the information is missing in the paper (e.g. only everage values are given for comparison and no variation, no tests for statistical significance, ...) On a positive note, datasets and code will be released, which should answer many of the open questions.									Public dataset and annotations will be shared, together with the code and model weights.																				
93	Class Impression for Data-free Incremental Learning 	Author agrees to disclose code and thus the result will be reproducible.									Good. All methods used for the proposed framework are depicted clearly and noted with appropriate references.									Highly reproducible.																				
94	Classification-aided High-quality PET Image Synthesis via Bidirectional Contrastive GAN with Shared Information Maximization 	Code is not provided.									The method description is clear.									Will code be available? It is not mentioned that code is made available.																				
95	Clinical-realistic Annotation for Histopathology Images with Probabilistic Semi-supervision: A Worst-case Study 	Codes, data and trained networks are given. Very good reproducibility of the study									This paper gives detailed explanation of its method and dataset. However, in order to reproduce this work, further details about experiment setting (such as choice of hyperparameters) and data sampling results should be provided.									More details about each training step, e.g., pre-training, fine-tuning, MC dropout configuration, need to be clarified for reproducibility. The authors checked almost all boxes in the checklist, in which it says the code would be made available. If that is the case, reproducibility is not an issue.																				
96	CLTS-GAN: Color-Lighting-Texture-Specular Reflection Augmentation for Colonoscopy 	The dataset and the code will be released.  The methods have been well described.  So the work should be reproducible.  The dataset is small sot the performance may differ with different imaging.									Source code will be available upon publication									The authors promise to publish the code and pretrained models upon acceptance. With only the explanations from the paper it would be very complicated to implement the presented approach.																				
97	Collaborative Quantization Embeddings for Intra-Subject Prostate MR Image Registration 	It is easy to reproduce since the authors provide detailed architecture design in the supp file.									The method's architecture is reproducible. However, as the method is learning-based, it is only fully reproducible with access to the processing pipeline and the model parameters. The dataset seems to be private.									The method is validated on a private dataset and thus it is hard to comment on the reproducibility given the marginal improvement.																				
98	Combining mixed-format labels for AI-based pathology detection pipeline in a large-scale knee MRI study 	The paper is very difficult to reproduce due to a) private dataset and b) highly complex multi-stage pipeline.									Overall, very high reproducibiliy score. The checklist provided by the authors is in a agreement with the provided details. Few aspects regarding the private dataset and the data management are to be clarified. See the comments in next section.									The authors satisfactorily illustrate the model development methodology, various training strategies, data used and how positional labels were obtained and the comparison with inter-reader agreement between MSK radiologists.																				
99	Combining multiple atlases to estimate data-driven mappings between functional connectomes using optimal transport 	The authors didn't provide that source code. Datasets and atlases used are publicly available.									Ok									The authors have agreed to most of the reproducibility questionnaire.																				
100	Computer-aided Tuberculosis Diagnosis with Attribute Reasoning Assistance 	CREDIBLE: I believe that the obtained results can, in principle, be reproduced. Even though key resources (e.g., proofs, code, data) are unavailable at this point, the key details (e.g., proof sketches, experimental setup) are sufficiently well described for an expert to confidently reproduce the main results, if given access to the missing resources.									The paper is simple with good reproducibility.									Most details of the proposed computational model are presented, so it should be possible to reproduce it.																				
101	Conditional Generative Data Augmentation for Clinical Audio Datasets 	No code provide. But the authors have promised to make it public.									Good for the reproducibility, as they promise to release the dataset and code									The authors state that they plan to release the code upon publication.  The methods are reasonably described so that a scientist with the code could reproduce the paper.  The dataset is small, it is not that well described, so reproducing these results may be challenging without the dataset.																				
102	Conditional VAEs for confound removal and normative modelling of neurodegenerative diseases 	method is not clear enough to be reproduced, the network architecture is not provided.									Reasonably reproducible.									anonymous git is a good idea, but it should be in main text.																				
103	Consistency-based Semi-supervised Evidential Active Learning for Diagnostic Radiograph Classification 	Code is not submitted for review.									Hyperparameters are specified in the paper, and the model has been clearly described. The dataset used is publicly available.									Since code is not released, it is not clear to which extend one can reproduce the work from the paper itself.																				
104	Consistency-preserving Visual Question Answering in Medical Imaging 	The paper will be reproducible when the code will be released.									Good.									none																				
105	Context-Aware Transformers For Spinal Cancer Detection and Radiological Grading 	The dataset for spinal cancer detection is private. No experimental codes are provided.									The authors provide some reasonable amount of details on the dataset definition and its preparation. They also provide details on the training setup. Some pseudo-code is provided in the supplementary material. The encoder is introduced as ResNet18.									The description of the method is satisfactory but not perfect, possibly because the algorithm has many parts but the space is limited.																				
106	Context-aware Voxel-wise Contrastive Learning for Label Efficient Multi-organ Segmentation 	I did not check the reproducibility.									No code is provided. But the paper provides the details for reimplementing the work.									The authors will make the code available upon acceptance of paper.																				
107	ConTrans: Improving Transformer with Convolutional Attention for Medical Image Segmentation 	It is possible to reproduce the paper. The paper presents a straightforward and well-explained model.									I have checked the 'Reproducibility Response' from authors. And I also think it is not difficult to reproduce this model.									Sufficient implementation details are provided in the manuscript.																				
108	ContraReg: Contrastive Learning of Multi-modality Unsupervised Deformable Image Registration 	Potential reproducibility of this work is moderate. The authors do a good job of explaining their method/hyperparameters, but are unwilling to release their software.									It seems to the reviewer that all the parameters are not given, to fully reproduce the paper. For instance, the number of negative and positive pairs to calculate the loss and how these pairs are obtained.  The reviewer highly recommand to provide the code of this paper, if the paper is accepted, as it could explain part of the methodolohy.									The authors do not provide any code. The data used are publicly available. The experiments and methods are clearly described in the paper. Qualitative reproduction of the results should be possible.																				
109	Contrast-free Liver Tumor Detection using Ternary Knowledge Transferred Teacher-student Deep Reinforcement Learning 	Most reproducibility checklist are fulfilled.									The reproducibility of the paper is OK.									It may not easy to reproduce the results due to some missing details and the in house dataset.																				
110	Contrastive Functional Connectivity Graph Learning for Population-based fMRI Classification 	Details of the methods are sketchy and would be difficult to reproduce.									If the codes and data are available, the results should be reproducible.									The authors haven't release code, but they provide network architecture in Supplementary material. The ADHD dataset is also publicly available.																				
111	Contrastive learning for echocardiographic view integration 	Source code will be included; all good.									Good if code is released.									Authors have provided enough detail for the paper to be reproducible and have used a public dataset																				
112	Contrastive Masked Transformers for Forecasting Renal Transplant Function 	Negative. The authors did not share the source codes and the results are limited.									I can't tell that this method can be reproducible cause they didn't applied the framework on a benchmark dataset									The paper is reproducible																				
113	Contrastive Re-localization and History Distillation in Federated CMR Segmentation 	The work is conducted on public datasets but the code is not provided. Overall the description could be improved for users to reimplement the method.									In general, the implementation details, dataset processing and training parameters (including Data Augmentation) have been explained quite well. As I said earlier, a bit more information on the model architecture would be helpful.  Also, certain practical information required for reproducibility could be useful - after the model training how was the model parameters aggregated (figure shows averaging but was it same after cross-attention in the subsequent rounds?), how do authors handle differences in the labels (see table 1) between M&Ms and Emidec datasets (any preprocessing done?) and how was the dice metric calculated?									A public dataset is used.  However, the reproducibility is limited due to inadequate description of the methods (e.g., structures of the unet used).																				
114	Contrastive Transformer-based Multiple Instance Learning for Weakly Supervised Polyp Frame Detection 	The author has provided a lot of experiment details.									Some information is missing for example how the Cls are calculated, but I assume that is explained inside their code repo.									The authors provide training details for their proposed method in the paper and they will share their code and dataset if the paper is accepted. I do not see a reproducibility issue for their proposed method.																				
115	Coronary R-CNN: Vessel-wise Method for Coronary Artery Lesion Detection and Analysis in Coronary CT Angiography 	I know that the centerline extraction and straight MPR generation tools sometimes comes with the softwares associated with the scanner. But they should be clearly specified in the paper. If the authors are using some other tools for centerline extraction and straight line MPR generation, they should specify that too. If they make the code available that would be very helpful for reproducibility too.									The authors did not release all the details about the proposed two modules, such as network architecture and feature map channels. The dataset used in the experiments are collected clinically and will not be made public.									The dataset is nice and hopefully made public.																				
116	CorticalFlow++: Boosting Cortical Surface Reconstruction Accuracy, Regularity, and Interoperability 	The general idea is expressed clearly, but I didn't see from the submission that the code will be released somewhere.									The reproducibility of the paper is good.									It seems reproducible.																				
117	CRISP - Reliable Uncertainty Estimation for Medical Image Segmentation 	I believe it is reproducible to a large extent.									The authors have provided adequate details to make the study reproducible.									Code is not available. The method is explained clearly in the paper and the experiments are performed on public datasets. So, I think it should be easy to reproduce the results in the paper.																				
118	CS2: A Controllable and Simultaneous Synthesizer of Images and Annotations with Minimal Human Intervention 	The authors promise to release code but not the in-house data. Hyper-parameters for model training are not provided.									Yes, it should be easy to reproduce the approach from the details included in the paper.									The paper seems to be reproducible, checking all the boxes.																				
119	Curvature-enhanced Implicit Function Network for High-quality Tooth Model Generation from CBCT Images 	Some details are missing (e.g. size and number of filters in conv layers), so the method cannot be exactly reproduced from it description.									I have some concerns about the reproducibility of the paper. The implementation and technical details of the method are not given in the paper. In the checklist, they claim that they will publish the data, training code, and network model. However, this information is not given in the text.									Yes																				
120	DA-Net: Dual Branch Transformer and Adaptive Strip Upsampling for Retinal Vessels Segmentation 	Is a complete submission with the training and evaluation code, the dataset, and the pre-trained weights.									The authors shared their code and weights of their network, which makes the paper reproducible.									providing code, dataset is open access																				
121	D'ARTAGNAN: Counterfactual Video Generation 	They are going to release the code. So it should be reproducible. otherwise the paper does not contain all the data to reproduce the results.									With the details in the paper it is hard to reproduce but they are planning to release the code.									I am not sure whether the MorphoMNIST dataset and Echonet-Dynamic dataset are publicly available. The given details are insufficient to reconstruct the proposed model. However, the authors promise to provide the code.																				
122	Data-Driven Deep Supervision for Skin Lesion Classification 	It is not clear, what type of images do the inhouse dataset contains; dermoscopy, close up clinical, etc... An analysis of situations in which the method failed is not given. (authors said yes to this in the reproducibility statement)									Reproducible									The method can be reproduced.																				
123	Data-driven Multi-Modal Partial Medical Image Preregistration by Template Space Patch Mapping 	"The paper is challenging to reproduce. Their algorithm uses several blocks from existing work, and the authors modified these blocks without detail. For example, when describing AirLab, the authors list that they ""modify the code as needed"" to process partial volumes. While details of this are not needed in the paper, it would be helpful to release code. Further, the training routine and data split is not well described."									The dataset and imaging information used in this study are well described. The authors describe their training procedure in sufficient detail. Source code will be made publicly available.									Method is not publicly available Dataset is not publicly available, and there is not enough technical data about the dataset to recreate a similar one Method could be implemented from the provided description																				
124	DDPNet: A novel dual-domain parallel network for low-dose CT reconstruction 	Average reproducibility. The description of the loss function and the coupled patch-discriminators is rather vague. The paper provided the details of materials and configurations, but did not tell the parameter in the loss function. Therefore, the reproducibility is not satisfactory.									This paper did not provide the detailed network architecture and key parameters such as alpha in the loss function, which make the reproducibility questionable.									The authors use public datsets but do not share the codes.																				
125	Decoding Task Sub-type States with Group Deep Bidirectional Recurrent Neural Network 	The method is clear and reproducible.									The method proposed in this paper has been validated on the HCP dataset and is highly reproducible.									It is easy to follow																				
126	Decoupling Predictions in Distributed Learning for Multi-Center Left Atrial MRI Segmentation 	I find the authors' answers do not represent the reality of their submission in several aspects. Specifically, the following are missing in the paper: Description of the study cohort. Information on sensitivity regarding parameter changes. The exact number of training and evaluation runs. Details on how baseline methods were implemented and tuned. An analysis of statistical significance of reported differences in performance between methods. The average runtime for each result, or estimated energy cost. A description of the memory footprint.									some details are not clearly described to reproduce the work.									no review																				
127	Deep filter bank regression for super-resolution of anisotropic MR brain images 	The method should be reproducible from the information provided in the paper.									Good. The paper presents their results with statistical significance, which I appreciate. The authors state that the code will not be made publicly available.									This paper has the reproducibility.																				
128	Deep Geometric Supervision Improves Spatial Generalization in Orthopedic Surgery Planning 	Some mathematical details on objective loss functions are missing. Otherwise, it should be reproducible.									Good details on experiment parameters, but would be better if code was made available to examine.									The reproducibility can be rated as sufficient																				
129	Deep is a Luxury We Don't Have 	No code is given; the description seems to be clear enough to reproduce the work.									The reproducibility of this paper is good as the author used a public implementation and evaluated their method on the public dataset.									The dataset and code are not disclosed in this paper, so the reproducibility of this paper needs to be further improved.																				
130	Deep Laparoscopic Stereo Matching with Transformers 	The architecture is clearly described, and references are given where further details can be found. The framework used for implementation is given. The metrics used to compare different methods are adequately described. However, the paper does not contain details about the running time, memory footprint and the computational platform in which the implementation was tested. Moreover, no failure cases have been included and discussed.									Okay reproducibility because datasets are public and they will open-source the code.									it looks reproducible.																				
131	Deep Learning based Modality-Independent Intracranial Aneurysm Detection 	Challenging to reproduce without access to the data									The reproducability of the vessel extraction part is rather poor.									The methodology used is reasonably clear, primarily relying on two previously published architectures (nnU-Net and PointNet). Preprocessing and parameters required are mentioned in the manuscript.																				
132	Deep Learning-based Facial Appearance Simulation Driven by Surgically Planned Craniomaxillofacial Bony Movement 	Could be reproduced.  The dataset and code are not open, neither seem to be referenced.  The methods are described however some details are lacking and this is noted in other sections of the review.									The demonstration of the method part is clear. The reproducibility is good.									The paper provides many details on their implementation, hence there is no major reason to argue about paper reproducibility. Authors report the different hyperparameters used (both in the data processing phase and network training and evaluation phase). Details on the dimension of each modules of the architecture are also provided. Although authors declared that they shared dataset and code in the reproducibility checklist, I do not see any link to such resources in the paper. I would expect that authors share their implementations upon acceptance.																				
133	Deep learning-based Head and Neck Radiotherapy Planning Dose Prediction via Beam-wise Dose Decomposition 	"The authors have essentially answered ""yes"" to every question in the reproducibility checklist. However, in reality most of the relevant points have not been included in the manuscript. While not all raised questions can be addressed in the manuscript, the checklist should be filled out correctly. I believe the most interesting information to add to the manuscript would be how the proposed method's hyperparameters were tuned (especially the loss function hyperparameters alpha and beta), how the baselines were implemented and tuned and additional discussion of the clinical significance (see comments above)."									This paper describes the experimental conditions for evaluation well.									Seems okay since author promised to provide the data and code in the reproducibility checklist. Otherwise, it is unclear to say since some key network architecture details are not provided in the paper itself.																				
134	Deep Motion Network for Freehand 3D Ultrasound Reconstruction 	Supplementary data shows that authors made a GUI for their work. I suggest to publish that GUI with your model to this research community.									without the two utilized datasets and the IMU-enriched ultrasound device hard to judge.									"Training/evaluation code, as well as pretrained models and data, are specified as ""available"" but no link has been provided."																				
135	Deep Multimodal Guidance for Medical Image Classification 	the reproducibility is good.									The authors use publicly available datasets and provide details about the techniques used, such that I feel confident I would be able to reproduce the results.									The data sets are publicly available.  The code foundation is publicly available (PyTorch, TensorFlow, etc).  The implementation description is provided in some detail.  I believe reproducibility is high.																				
136	Deep Regression with Spatial-Frequency Feature Coupling and Image Synthesis for Robot-Assisted Endomicroscopy 	X									Video dataset is constructed. Public Data and code is expected to release.									no review																				
137	Deep Reinforcement Learning for Detection of Inner Ear Abnormal Anatomy in Computed Tomography 	The reproducibility of the paper is low. Datasets and code are not available. The description of the dataset heterogeneity was not provided.									The models and algorithms are reasonable adequately described although the reader is referred to the literature for the training procedure.  The data is adequately described. No code is provided.									The authors will not release the code.																				
138	Deep Reinforcement Learning for Small Bowel Path Tracking using Different Types of Annotations 	Paper appears to be at an acceptable level with regards to reproducibility									Sufficient details have been provided in order for the work to be reproducible									The authors will make the code available upon acceptance of paper.																				
139	Deep treatment response assessment and prediction of colorectal cancer liver metastases 	Reproducible									No issue									The model was trained and tested on a specific dataset PRODIGE20, which was collected in a previously published study. All cases were all colorectal cancer patients with liver metastasis and received specific treatments. So it is difficult for readers to validate the performance of the model using other datasets.																				
140	DeepCRC: Colorectum and Colorectal Cancer Segmentation in CT Scans via Deep Colorectal Coordinate Transform 	Neither code nor the dataset will be made publically available. The description of the experiments is complete and might be sufficient to reproduce the experiments in some extent.									No training, evaluation code or trained models provided make this algorithm potentially difficult to reproduce. Especially given the fairly complex adaptations to the training loss and network to incorporate both the predicted coordinate map and attention.									The dataset is an in-house dataset. This could be a problem of code reproducibility.																				
141	Deep-learning Based T1 and T2 Quantification from Undersampled Magnetic Resonance Fingerprinting Data to Track Tracer Kinetics in Small Laboratory Animals 	A detailed description of the model and the data used is illustrated.									The checklist states that the dataset is downloadable, but I did not see an (anonymized) link in the manuscript Same for code and models The exact architecture of the UNet (number of features per layer) is not provided.  Not a problem if code is released. Hyper-parameter strategy is not mentioned, but maybe no hyper-parameter search was performed (which would be fine) Time/cost/memory not reported No statistical significance tests									The checklist mentions the code will not be available in a repository as it is still under development, which limits reproducibility.																				
142	DeepMIF: Deep learning based cell profiling for multispectral immunofluorescence images with graphical user interface 	Highly reproducible. Also the authors suggest it would be publicly available .									The reproducibility of the paper is good. The authors provided necessary details from the reproducibility checklist.									The software will be made available through github upon acceptance. This is reasonable and is not public yet, perhaps to keep the process double blind.  The data on the other hand is not public or I could not see where this is available.																				
143	DeepPyramid: Enabling Pyramid View and Deformable Pyramid Reception for Semantic Segmentation in Cataract Surgery Videos 	The authors claim the dataset will be available after acceptance of the paper.									The reproducibility of the paper is ok.									The technical contribution is limited.																				
144	DeepRecon: Joint 2D Cardiac Segmentation and 3D Volume Reconstruction via A Structure-Specific Generative Method 	Good, the author's answers to the reproducibility checklist correspond to the content of their paper.									Seems okay.									Some parts are not so easy to reproduce specially concerning the methods used, because information is not enough, and the complexity of the architectures used, it may need more information about hyperparameters. The expertise reached by their tool it is based on coupling different architectures for the segmentation and reconstruction tasks previously known.																				
145	Deformer: Towards Displacement Field Learning for Unsupervised Medical Image Registration 	Good reproducibility. Many technical details of the proposed method are reported and the authors claimed that the code will release if the work is accepted in the reproducibility checklist.									Should be good.									I think the reproducibility of the paper is good.																				
146	Degradation-invariant Enhancement of Fundus Images via Pyramid Constraint Network 	The authors intend to share the source codes. one part of data is publicly available.									The reproducibility is well sound.									The description of the method is clear. I think the proposed method could be reproduced.																				
147	Delving into Local Features for Open-Set Domain Adaptation in Fundus Image Analysis 	The reproducibility of the paper should be good since the authors will provide the code and the datasets are public. Important hyperparameters are specified in the paper as well.									The CNN model is ResNet50 and the datasets are all public, thus the reproducibility of the paper is strong with more detail of experiments' setting, for example, the input size.									There is no significant issue with reproducibility.																				
148	Denoising for Relaxing: Unsupervised Domain Adaptive Fundus Image Segmentation without Source Data 	The data is from an open challenge and the code will be open-sourced. So the reproducibility is great.									Authors claim that code will be available after review.									The authors state in the abstract that they will make the source code available upon acceptance of the paper. The used datasets are cited and publicly available. The backbone architecture is also cited.																				
149	Denoising of 3D MR images using a voxel-wise hybrid residual MLP-CNN model to improve small lesion diagnostic confidence 	The code is not available at the moment, but enough details to reproduce it are given. The dataset used by authors is publicly available. The used dataset is described and  cited.									The authors used a public dataset with URL provided.									It is hard to be reproduced as there is no detailed description about the proposed method and no code publicly available.																				
150	DentalPointNet: Landmark Localization on High-Resolution 3D Digital Dental Models 	The deep learning model is not directly given and will not be published after acceptance. The input size of the data and some details are missing for reproducibility.									No special issue here.									Based on the information provided in the methodology section, I believe the proposed method could be implemented. However, since the data is not publicly available - it is not possible to reproduce the results presented in the paper.																				
151	DeSD: Self-Supervised Learning with Deep Self-Distillation for 3D Medical Image Segmentation 	Some important details are missing, such as how to build a 3D segmentation network based on ResNet-50.									The datasets are publicly available. SSL part is easy to reproduce since it is based on DINO. The paper does not show details on how the four sub-encoders are constructed (but it may be inferred since the encoder is ResNet-50). The downstream segmentation model cannot be easily reproduced. There are some descriptions of how to construct the decoder, but it requires more details to fully reproduce.									This paper has moderate reproducibility. The code is not released, but most training details (e.g., architecture, optimizor, learning rate, batch size, and augmentations) are provided in this manuscript.																				
152	DeStripe: A Self2Self Spatio-Spectral Graph Neural Network with Unfolded Hessian for Stripe Artifact Removal in Light-sheet Microscopy 	There is no mention in the main text about code being available.									My mental test is always, whether I believe I would be able to make a student reproduce the results, and in this case, the answer is Yes.									The authors state that the code will be made available for biologists for academic use. Can you clarify how restrictive this is? What about scientists from other fields? Maybe you could comment on why you decide against open-sourcing the code.																				
153	Detecting Aortic Valve Pathology from the 3-Chamber Cine Cardiac MRI View 	The paper seems to be relatively straight-forward to reproduce. It would be great if the dataset could be made available to the public, since significant effort has to be taken to annotate the data as was done here in this study.									Mainly the open points on the description should be clarified, as right now I would not be able to implement exactly the same as many details are not really described. I realize this may be tricky given the size limit of the paper, maybe it would be possible to replace some text with a more descriptive figure.									"The authors employ a novel multi-center dataset comprised of 1017 patients. However, limited description is given regarding image acquisition parameters, instructions given to annotators (e.g., which rules were used to define a pathological jet and its extent?), methods employed for quality control (any type of consensus?), etc. Moreover, it is not understood if the dataset used for training heatmap regression (80 patients) is a subset of the main dataset (1017 patients) or an independent one. If the former, how was that dealt with when evaluating the accuracy of the classifier? Several methodological/implementation details are lacking, hampering the reproduction of the authors' method/results. See ""Weaknesses"" and associated specific comments below."																				
154	DEUE: Delta Ensemble Uncertainty Estimation for a More Robust Estimation of Ejection Fraction 	"Everything is ticked ""yes"" but I don't see own code or results being made available? Otherwise: Detailed model/parameter description use of state-of-art methods for comparison public dataset"									The reproducibility of this paper is very high. The data is publicly available and the implementation is straighforward.									relatively easy to reproduce. Not all details provided.																				
155	DGMIL: Distribution Guided Multiple Instance Learning for Whole Slide Image Classification 	The authors have clarified that the code will be available in the reproducibility checklist.									Seems good as almost all [YES] are chosen.									The dataset is publicly available. The proposed method is clear enough for coding and realization. However, I don't know if it can be confirmed for reproducibility.																				
156	Did You Get What You Paid For? Rethinking Annotation Cost of Deep Learning Based Computer Aided Detection in Chest Radiographs 	The results are convincing. However, the paper will be more impactful if the authors can release the dataset.									Would be good if the authors release the code as they claimed in the list.									The dataset is unfortunately private, but the methodology is clear and key hyper-parameters are reported. The authors will release the code.																				
157	Diffusion Deformable Model for 4D Temporal Medical Image Generation 	The authors provided code of the experiment in an anonymous github account.									Their code is ok, but it is a little difficult to follow some details.									The datasets used for evaluation are publicly available, although the exact images used for training, validation, and testing are not provided. The source code is available in a github repo. I did not check how does it run. The parameter values were provided. For the evaluation, the authors provided a clear description of metrics. Statistical significance is  stated.																				
158	Diffusion Models for Medical Anomaly Detection 	The authors have produced the anonymous codes.									The model is very clearly explained and the code is included. Evaluation is performed on public datasets. I commend the authors on releasing very clear instructions in the code repository.									Looks good.																				
159	Digestive Organ Recognition in Video Capsule Endoscopy based on Temporal Segmentation Network 	ok									This was not filled out at all, just no for everything.									Code not available. The method description is clear, as well as the training details and parameters. Data is not available (it would be very interesting if authors are planning to release, though). It is plausible that the general architecturte can be reproduced and tested on a different dataset than the one presented in this paper (there are numerous public datasets)																				
160	Discrepancy and Gradient-guided Multi-Modal Knowledge Distillation for Pathological Glioma Grading 	Data is publicly available. Code and split will be made available after review. So it's reproducible.									The dataset is open, and the authors mentioned that the source code will be released. Therefore, I'm confident the results could be easily reproduced.									Authors claim they will release the code. If so, the study could be reproduced.																				
161	Discrepancy-based Active Learning for Weakly Supervised Bleeding Segmentation in Wireless Capsule Endoscopy Images 	This paper provides source code and uses a public dataset. It should has good reproducibility.									Meet the requirement.									The authors have mentioned that code would be public.																				
162	Disentangle then Calibrate: Selective Treasure Sharing for Generalized Rare Disease Diagnosis 	The authors provide most of the details, but the network architecture needs to be clear									Generally good.  However no 'analysis of statistical significance of reported differences in performance between methods', 'average runtime for each result, or estimated energy cost', 'description of the memory footprint'  is provided. As stated in section 5, the lack of statistical comparison is a negative.									The authors give a good overview of the adopted hyper-parameters but some details are still missing (for instance number of iterations and augmentation pipeline). Also, given the ambiguity of the DTC module description, a faithful reproduction of the method might be difficult.																				
163	DisQ: Disentangling Quantitative MRI Mapping of the Heart 	I didn't find obvious problems of reproducibility. Most of hyper-parameters are mentioned, especially for the loss function. Network architecture seems missing but this work intends to release its code thus it should not be a big problem.									The manuscript mentions the code will be available in a repository and the checklist supports this statement. There are some little details not explained in the paper, but it may be out of the scope of a conference paper. Once uploaded, it will guarantee reproducibility.									No elements on paper reproducibility could be found in the paper.																				
164	Distilling Knowledge from Topological Representations for Pathological Complete Response Prediction 	Low. The used data is public but recreating this network and being able to compute the topological priors is an advanced task for anyone. If the code is made available, then this will improve. But as the code would only cover the network, there would still be some serious barriers to other users.									The reproducibility seems OK. The method is clearly explained. Ideally the code should be released upon acceptance along with hyperparameters.									Good.																				
165	Domain Adaptive Mitochondria Segmentation via Enforcing Inter-Section Consistency 	"The authors listed ""yes"" for both code and pre-trained models. In this case, it can be an easy task for both training and testing. If the reproduction was only based on the descriptions in the paper, it could be somewhat difficult."									The method appear reproducible.									"Overall it seems feasible to reproduce the results. Some information selected as ""yes"" in the reproducibility report is missing: mean, variance, statistical significance and failure case analysis. Another question is whether the authors will publish their code."																				
166	Domain Adaptive Nuclei Instance Segmentation and Classification via Category-aware Feature Alignment and Pseudo-labelling 	The authors do not provide the description of results with central tendency (e.g. mean) & variation in the paper, which they answered Yes in the checklist. It would be a plus to add mean and variance to the results.									Not sure how easy/difficult to reproduce this paper.									I think  it can be reproduced																				
167	Domain Specific Convolution and High Frequency Reconstruction based Unsupervised Domain Adaptation for Medical Image Segmentation 	Reproducible.									Authors have provided enough information for reproduciblity of the paper.									The source code of this work is provided, which is a plus. I believe this work is reproducible.																				
168	Domain-Adaptive 3D Medical Image Synthesis: An Efficient Unsupervised Approach 	Lack of explanation on the hyper-parameters selection, average run time									The code is not provided, but maybe it will be made available upon acceptance? No statistical tests nor measures of variance are provided, making it difficult to evaluate the significance of the quantitative results.									I would like to suggest the authors to make their code publicly available in an anonymous form (e.g., by creating an anonymous account on github) along with submission of the paper.																				
169	Domain-Prior-Induced Structural MRI Adaptation for Clinical Progression Prediction of Subjective Cognitive Decline 	no review									It is difficult to judge the reproducibility of the paper as the parameter settings are unclear.									Does not seem reproducible only because the 'AAA' dataset (as referenced in this work) is not publicly available.																				
170	DOMINO: Domain-aware Model Calibration in Medical Image Segmentation 	The methods are very detailed. The derivations for the approach are shown, which makes this framework reproducible. Images are not publically available, however, all parameters used, such as repetition time (TR), echo time (TE), and field of view (FOV), are all listed, which means the study is also reproducible. The authors do say they will release DOMINO in the future.									Ok									no review																				
171	Double-Uncertainty Guided Spatial and Temporal Consistency Regularization Weighting for Learning-based Abdominal Registration 	PyTorch version not reported. Data description is scarce (no scanner info,  instructions to annotators, their degree and level of experience not reported). Review board approval is claimed to have been obtained, but is not referenced (id number). Placeholder for code link not included in the manuscript, but authors claim code will be made available upon acceptance. Relevant statistical significance tests not performed.									This paper has provided details about the models, datasets, and evaluation.									Data is not currently publicly available (awaiting IRB approval), no discussion of code availability and this seems like it would be fairly difficult to implement from scratch.																				
172	DRGen: Domain Generalization in Diabetic Retinopathy Classification 	Enough details are provided for reproducibility.									Good.									The implementation details together with the source code used to run the experiments, which the authors plan to share, address reproducibility. Assuming that the source code is going to contain a readme that explains how to run the code to get the same results of the experiments, reproducibility is fulfilled.																				
173	DS3-Net: Difficulty-perceived Common-to-T1ce Semi-Supervised Multimodal MRI Synthesis Network 	"The authors should provide more detailed explanations on the differences between their work and the existing semi-supervised learning frameworks, especially those based on the teacher-student strategy. Currently, it is hard to understand the true technical contribution of this work. The proposed framework seems a straightforward application of existing semi-supervised learning work in multi-modal MRI synthesis. Page 5: ""Here we empirically set l id , l fd , l pad and l GAN as 100, 1, 1 and 1."" How to determine these parameters? What is the influence once any of them is changed? Table 1: The results for the case of 100% paired percentage look odd. The second best method, Pix2pix, provides the best image quality with highest PSNR and SSIM, however, the corresponding segmentation results are not as good as DS3-Net. Why are two sets of results inconsistent? What is the underlying reason? Following the last comment, another strange thing is that the case of 50% paired percentage outperforms 100% case in terms of TC Dice, which is inconsistent with the conclusions observed in other results, i.e., larger paired percentage better TC Dice score. Please clarify. It is helpful to provide the model size (e.g., number of parameters) of different networks. Some failure cases should be provided in the manuscript. The authors should discuss the potential reasons for the failure of their model, which is helpful for the further improvement of the proposed model. Finally, the authors can consider to provide some results regarding the computational time of different methods."									Positive for the reproducibility of the paper.  It's easy to be reproduced, because they have shared the source codes on GitHub. But, the authors should describe more details about the network.									The training schedule including the learning rate and optimizers etc are not well clarified.																				
174	DSP-Net: Deeply-Supervised Pseudo-Siamese Network for Dynamic Angiographic Image Matching 	Most of the implementation details have been addressd.									The network architecture is clear and the algorithm logic is rigorous. The experimental part uses comparative and ablation experiments, which are rich in type. And the comparison test is full. But, the experimental datasets are not clinical data and the effectiveness of clinical use is debatable.									Easy to follow Dataset is expected to release. The method is clear.																				
175	DSR: Direct Simultaneous Registration for Multiple 3D Images 	There is an algorithm diagram in the supplementary material which helps (it should be moved to the main manuscript), but most of the other implementation details such as numerical libraries, programming language etc are missing.									The authors should include more details on dataset. It would be great if they can release the dataset together with the code for reproducibility of the work conducted.									The code will be available after acceptation.																				
176	Dual-Branch Squeeze-Fusion-Excitation Module for Cross-Modality Registration of Cardiac SPECT and CT 	All looks good. It would be nice to make several cases in the dataset publicly available.									This could be assessed by looking at the source code repository which is currently anonymous.									It is easy to reproduce since the authors will provide the code upon acceptance and the description is clear in the paper.																				
177	Dual-Distribution Discrepancy for Anomaly Detection in Chest X-Rays 	The author claims to release the source code, and the paper provides enough details to reproduce the paper.									Given the uncertainty in the choice of datasets, more material is needed to demonstrate the reproducibility of the method.									Authors claim to provide codes based on the reproducibility list.																				
178	Dual-graph Learning Convolutional Networks for Interpretable Alzheimer's Disease Diagnosis 	The authors provided the code that make it is reproducible.									Code is being provided for reproducibility									manuscript should specify which ADNI Data release was used for analysis																				
179	Dual-HINet: Dual Hierarchical Integration Network of Multigraphs for Connectional Brain Template Learning 	Good.									The main idea and the major steps are provided and described, current contents seem to be OK for reproducing, but it cannot be fully confirmed unless we really dig into it. Publishing source code will be much welcomed which can greatly help in understanding the proposed method.									As mentioned in the weaknesses, a lot of technical details are missing making the reproducibility less convincing.																				
180	DuDoCAF: Dual-Domain Cross-Attention Fusion with Recurrent Transformer for Fast Multi-contrast MR Imaging 	Although some details are missing, it is still reproduceable, considering that the proposed methods is well presented, and authors promised to release code after acceptance.									The results of this paper may be reproducible.									Reproducibility is good.																				
181	Dynamic Bank Learning for Semi-supervised Federated Image Diagnosis with Class Imbalance 	This paper has shown the implementation details. Additionally, this paper has provided the codes in the supplemental material, and it will release the codes. So I think its reproducibility is very good.									Hyper-parameters are complete. Authors are going to release the code. The details of thresholds h_m/h_c may be improved.									The description of the experimental setup and the hyperparameters are clear, which is sufficient to reproduce the reported results.																				
182	EchoCoTr: Estimation of the Left Ventricular Ejection Fraction from Spatiotemporal Echocardiography 	Use of a large, well-known, and public dataset (EchoNet). Code will be available if the paper is accepted.									If authors get to publish their code, it can be reproduced. This paper it is strong in this aspect, database is publicly available also. And details on language and platform is given.									The data used by the authors is public and the code will be made available upon acceptance. The higher-level parameters used by the authors are listed in the paper. It is mentioned that the UniFormer model was adapted, but it is not clear what changes were actually performed. The paragraph detailing this (end of page 4) is not clear enough and may be not complete. Overall, the reproducibility should be excellent once the github is made public.																				
183	EchoGNN: Explainable Ejection Fraction Estimation with Graph Neural Networks 	Please address Ethics / IRB approval - you can use a placeholder for now. Otherwise I have no comments, seems sufficient.									The authors describe their methods, architecture, etc. reasonably well, but some more details would have been desirable (although some more is included in the supplementary material). They provide anonymized link to the code in github. One thing that reduces reproducibility is the use of custom architecture blocks. For example, the video encoder could be replaced with a generic frame based image encoder. Perhaps, the performance suffers a little bit? Would be good to know why they made this choice.									Authors have taken great care to make their work reproducible. In the reproducibility disclosure, authors state they will make code available. The methods are described in great detail in the paper. The EchoNet Dynamic dataset is publicly available.																				
184	Edge-oriented Point-cloud Transformer for 3D Intracranial Aneurysm Segmentation 	Although some details of the experimental protocols are missing in the paper, I would say that it is not that difficult to reproduce the experiments.									"Reproducibility is OK though parameter tuning is not provided. ""An analysis of statistical significance of reported differences in performance between methods."" is not provided though the answer to this question is YES."									The paper is well-written, with clear specification. However, the authors did not promise to release code, thus I might have some concerns in its reproducibility.																				
185	Effective Opportunistic Esophageal Cancer Screening using Noncontrast CT Imaging 	It is interesting and a good idea to improve the tumor segmentation performance by proposing the position-sensitive self-attention layer for each encoding layer in the segmentation network nnUnet. However, detailed method and trained procedure were not introduced clearly, which influence the reproducibility of this work.									Neither code nor data seems to become publicly available. The reproducibility statement is unclear to me in that under #4 the authors filled in no review on a few occasions whereas I think that for any AI method the exploration of hyper-parameters and sensitivity are applicable.									According to the Reproducibility Response, I think the reproducibility of the paper was good. Providing the common dataset and code can improve repeatability.																				
186	Efficient Bayesian Uncertainty Estimation for nnU-Net 	Easy to reproduce.									This paper is reproducible.									No issue here.																				
187	Efficient Biomedical Instance Segmentation via Knowledge Distillation 	Easy to reproduce									the data is public, the code will hopefully be made available									Some details are missing. The authors seems just check everything 'yes' in the checklist. Missing one of the loss function definition. Missing software framework and version. No new dataset proposed. Missing training time for the proposed distillation method.																				
188	Efficient population based hyperparameter scheduling for medical image segmentation 	No code or implementation has been shared. Public dataset used. Reproducibility status unknown.									the authors plan to publish their code. Data used is public									The authors would like to make all codes publicly available, which ensures good reproducibility.																				
189	Electron Microscope Image Registration using Laplacian Sharpening Transformer U-Net 	use of public challenge data unclear if code is to be made available no significance testing (not claimed in checklist but unclear why not carried out)									Please, correct me if I'm wrong: In the reproducibility statements, authors declare to provide the links to the code, however I did not find any or any statement in the paper indicating the the code will be openly available.									Publicly available dataset and simplicity of approach based on related work with available open source implementations should facilitate reproducibility.																				
190	Embedding Gradient-based Optimization in Image Registration Networks 	According to the reviewer, the paper is not clear enough to be perfectly reproducible. However the authors provided a github repository with the code.									The given answers seem to be correct. I appreciate that the authors honestly answered these questions (no one else did in my paper stack) An analysis of situations in which the method failed. [No]  Discussion of clinical significance. [No]									The procedure itself is not made available for download etc. The data used are freely available. The procedures used for comparisons have already been published and some of them are available as freely accessible code. It is unclear how the presented procedure has to be parameterized in detail. This could be discussed in more detail in the experiments section.																				
191	Embedding Human Brain Function via Transformer 	The reproducibility of the paper is good.									No code is provided									Method evaluated on a publicly available dataset. The code is not shared and minor implementation details are missing for reproducibility.																				
192	End-to-End cell recognition by point annotation 	Can be reproduced based on some related knowledge.									All the hyper-parameters are given. It is possible to reproduce the paper.									The code is not provided. All methods used for the proposed framework are depicted clearly and noted with appropriate references.																				
193	End-to-End Evidential-Efficient Net for Radiomics Analysis of Brain MRI to Predict Oncogene Expression and Overall Survival 	The reproducibility of the work is adequate. The conceptual idea behind EDL is provided, Figure 2 summarizes the network architecture, and model parameters are provided in Table 1 / section 3.1. The code will not be publicly shared.									It is hard to decide the reproducibility of the paper.									As the authors were not shared some important details of the model development and training, it would be hard to reproduce the exact same experiments.																				
194	End-to-end Learning for Image-based Detection of Molecular Alterations in Digital Pathology 	the methods are detailed, so someone should be able to reproduce their work.  The first data set seems proprietary though, not sure if they are able to share it, this could be added to methods if IRB does not allow to share this.									The explanations seems good enough to reproduce the paper but can't be sure since the code itself won't be released it seems.									Datasets and experimental setup are clearly mentioned.																				
195	End-to-end Multi-Slice-to-Volume Concurrent Registration and Multimodal Generation 	Very good; it starts from the related papers that contain source code and points out the algorithmic differences and additions.									The paper is not reproducible, as stated in the reproducibility list.									The datasets used are private. The code is not available, but is based on publicly available earlier methods. The method is generally well described and could be re-implemented to some extent, but the results are unlikely to be reproducible.																				
196	End-to-End Segmentation of Medical Images via Patch-wise Polygons Prediction 	The paper uses publicly accessible datasources and provide their code as supplementary material (not tested). The reproducibility is very good.									1.Please specify the single-class segmentation method in the paper. 2.It would be better when you show the figure for the CNN model. 3.There are several excellent papers you may cite to enchance the references:         AFP-Mask: Anchor-free Polyp Instance Segmentation in Colonoscopy         Colorectal polyp segmentation by u-net with dilation convolution									Some details are missing. The authors seems just check everything 'yes' in the checklist. Missing one of the loss function definition, i.e. L_{BCE}. Missing software framework and version. No new dataset proposed. Missing training time for the proposed network.																				
197	Enforcing connectivity of 3D linear structures using their 2D projections 	Reproducibility seems OK, public data and largely based on a publicly available code.									Apparently, the authors have not understood how to fill out the checklist: for many questions answered with yes, the information is actually missing in the paper (e.g. no tests for statistical significance, memory footprint, analysis of situations in which the method failed, etc.). Since no code will be published, these details will remain unknown.									Details about the baseline models are missing, e.g. whether trained on 2D or 3D.																				
198	Enhancing model generalization for substantia nigra segmentation using a test-time normalization-based method 	Two datasets are publicity available. Experimental part sounds technically correct, but short.									In-house data was used to train the model. Open-sourced data sets were used for testing generalization of the methods The authors give very detailed descriptions of the methods used which therefore makes the methods highly reproducible. This is mentioned in strengths as well, even though in-house images are used for training the model, the other 2 publicly available datasets that the authors used for generalization of the method can be used to reproduce the results. Also, speaking to the data, the authors were able to generalize their model to 2 publicly available datasets									Authors have done a good job in providing experimental details and the employed datasets to help reproduce their work.																				
199	Ensembled Prediction of Rheumatic Heart Disease from Ungated Doppler Echocardiography Acquired in Low-Resource Settings 	"Overall reasonably good. The model was clearly described although code has not been made available. One point of concern though - in the checklist, the authors answered ""Yes"" to ""The range of hyper-parameters considered, method to select the best hyper-parameter configuration, and specification of all hyper-parameters used to generate results."" I agree that they specified final hyperparameter values but I did not see any description of the range of values tested or how the hyperparameters were optimised (see detailed comments below)."									The paper provides a substantial methodological description.									The authors filled out the reproducibility checklists, but will not release their codes and dataset in the future. Then implementation details of all models in the paper are well described. It is easy to follow the paper and reproduce their work.																				
200	Estimating Model Performance under Domain Shifts with Class-Specific Confidence Scores 	The method should be simple to implement.									With the additional information in the supplemental material, it should be able to reproduce the results of the paper. The use of public datasets further helps with reproducibility.									Sufficient reproducibility is ensured.																				
201	Evidence fusion with contextual discounting for multi-modality medical image segmentation 	the paper is well written, and should be reproducable									The authors agree to provide the code, so the results can be reproduced. they demonstrate the performance of their method on widely-used publicly available dataset.									The description of the proposed method is not clear and the paper may be not reproducible.																				
202	Evolutionary Multi-objective Architecture Search Framework: Application to COVID-19 3D CT Classification 	The author provides code and data, but not a requirement for acceptance.									Very good. All results are based on public datasets and the authors promise to make code avaialbe.									The authors claimed to release the code once accepted.																				
203	Explainable Contrastive Multiview Graph Representation of Brain, Mind, and Behavior 	Too much details are missing for the whole model, so it will be hard to reproduce this method only from paper.									no review									The reproducibility is good for the clear elaboration in method detail.																				
204	Explaining Chest X-ray Pathologies in Natural Language 	The reproducibility statement claims code, and script to reproduce the results will be released, but I see no placeholder for this in the text.									It is OK, and the authors will release the source code.									The authors claim that their dataset and code will be made publicly available upon acceptance.																				
205	Exploring Smoothness and Class-Separation for Semi-supervised Medical Image Segmentation 	As the authors claim that they will release their codes, the reviewer has no other concerns.									The authors promise to release their codes.									The supplementary material provides detailed algorithms. No code is provided. Not sure about reproducibility.																				
206	Extended Electrophysiological Source Imaging with Spatial Graph Filters 	No code, no data available, nor algorithm given in the paper.  However, the paper gives enough details to re-implement the method. The parameter tuning is not mentioned, that provide from reproducing results.									The authors have done a very nice work in terms of reproducibility and the supplementary text helps a lot towards this direction, especially if the code will be released. One minor note, the authors answered positive in the question: The average runtime for each result, or estimated energy cost. But the average runtime is not mentioned									I think the reproducibility of the paper is good.																				
207	FairPrune: Achieving Fairness Through Pruning for Dermatological Disease Diagnosis 	Reproducibility should be good if the authors can release their code.									The datasets used were publicly available. There was no mention that the code would become public upon acceptance.									Overall good. I would like to see code made available if the paper is accepted. The authors do state that a grid search using the validation set was used to set the beta and pruning ratio hyperparameters. I would also like to see a statement of how other hyperparameters were set, e.g. batch size (for pre-training and for saliency calculations), learning rate, number of minibatches used for saliency calculation.																				
208	Fast Automatic Liver Tumor Radiofrequency Ablation Planning via Learned Physics Model 	There are some details missing to be able to reproduce it fully but most of the architecture is described. However, datasets used for validation are not open.									It is not easy for readers to follow this work since the method part is not clear and no codes are provided.									Enough details are provided for repeatability in terms of parameters used. It is not clear if the code will be shared or the data would be available.																				
209	Fast FF-to-FFPE Whole Slide Image Translation via Laplacian Pyramid and Contrastive Learning 	The method is well described using public data, and the authors engage to share the code upon acceptance. The reproducibility seems good.									The author claimed that the data, code, and models would be released if accepted. The paper should be reproducible.									Seems to follow all the requirements on reproducibility.																				
210	Fast Spherical Mapping of Cortical Surface Meshes using Deep Unsupervised Learning 	Seems reproducible.									Probably reproducible.									The paper meets the requirements of the checklist.																				
211	Fast Unsupervised Brain Anomaly Detection and Segmentation with Diffusion Models 	Not mentioned									nan									While the code is not publicly available (and the reproducibility checklist says so), I believe the authors might have used the original repositories for VQ-VAE and DDPM which are publicly available. Even though implementation details are not given, it might be possible to reproduce the method partially. Furthermore, the results are presented using public datasets.																				
212	Feature Re-calibration based Multiple Instance Learning for Whole Slide Image Classification 	The paper should be reproducible for the CAMELYON experiments, but it is not clear whether the in-house dataset will ever be released.									Implementation settings are available									The authors didn't share their source codes.																				
213	Feature robustness and sex differences in medical imaging: a case study in MRI-based Alzheimer's disease detection 	The paper provides enough details to enable reproducibility of the experiments.									The database is known http://www.adni-info.org/ , they added implementation details and the code is available on github.									The answers given by the authors seem consistent with what is present in the paper. Code will be made available on GitHub, and the methods and results are well described for an 8-page paper.																				
214	Federated Medical Image Analysis with Virtual Sample Synthesis 	The code and dataset are accessible. The VAT approach may slightly affect the training process.									The authors provide a lot of details such as training details, code, etc. I think it is easy to repoduce the results reported in the paper.									All questions in the reproducibility checklist are positive and implementation details are given in the paper.																				
215	Federated Stain Normalization for Computational Pathology 	They say that the code will be made available, but from the paper itself it would difficult to reproduce anything. The dataset used is already public.									The authors has clarified the reproducibility,for all code related to this work that they will release if this work is accepted									The authors promised availability of the code. Details related to the network architecture are provided in a supplement associated with the paper. Proposed concept is evaluated on the public  PESO dataset of prostate specimens. It contains 102 hematoxylin and eosin stained whole slide images with corresponding segmentation masks.																				
216	FedHarmony: Unlearning Scanner Bias with Distributed Data 	The reproducibility effort is decent here, but further insight into the particular hyperparameters chosen and experiments related to their settings would be welcome. These appear to be critical in the overall design in order to make the approach work properly.									Seems fine.									Good. Public datasets, implementation details are provided. Code is promised to be publicly available.																				
217	Few-shot Generation of Personalized Neural Surrogates for Cardiac Simulation via Bayesian Meta-Learning 	It would be helpful if the code and the data would be released, otherwise it might be impossible to reproduce the paper									In the Reproducibility Response, it seems the authors will release the corresponding code. However, the reviewer did not find the description of code in the main text.									Looks like the dataset is not public available and there is no plan to release the code.																				
218	Few-shot Medical Image Segmentation Regularized with Self-reference and Contrastive Learning 	"Authors checked ""Yes"" for most questions on the reproducibility of the paper"									The authors describe concisely the implementation details and dataset preprocessing. Code and pretrained models will also be made public. These are enough information to reproduce the experiments with little effort.									The authors did not mention whether the code will be released. However, considering the methods are straightforward and the datasets are public, it is possible to reproduce the results.																				
219	FFCNet: Fourier Transform-Based Frequency Learning and Complex Convolutional Network for Colon Disease Classification 	The method is explained well and is reproducible									Easy to reproduce.									Good, code will be released.																				
220	Fine-grained Correlation Loss for Regression 	I didn't check it									In the reproducibility checklist, the authors mentioned that the code has been made available or will release if this work is accepted. I think it is great if they release the code if this work is accepted. Unfortunately, there are no details regarding how the authors acquired data. I assume that the dataset was dedicated to this study since there are no citations or download links. If it is the case, it can be mentioned in the manuscript explicitly. The validations seem well-documented except for concern regarding the reported batch size, which has been explained in detail in the comments.									According to the checklist, the authors will opensource their codes in the future. In the paper, the authors also provide enough details for reproduction.																				
221	Flat-aware Cross-stage Distilled Framework for Imbalanced Medical Image Classification 	nan									Good. Experiments were performed on public datasets and the authors plan to release code after peer-review according to the checklist.									The authors promised to release their codes and all the experiments are conducted on public datasets. I believe the results could be reproduced.																				
222	Flexible Sampling for Long-tailed Skin Lesion Classification 	-									yes									Section 3 of the paper includes enough information to reproduce the method and architecture presented in the manuscript. The authors explain in detail what datasets they used, the network's hyper-parameters, the architecture, and the training recipe.																				
223	fMRI Neurofeedback Learning Patterns are Predictive of Personal and Clinical Traits 	The source code has been made public and has a good reproduction base.									The described method seems reproducible. Code has been put up on Github.									no concern																				
224	Free Lunch for Surgical Video Understanding by Distilling Self-Supervisions 	Not all parameters for the training are given, such as the mini-batch size and the number of epochs. The hardware could have been mentioned. The datasets are clearly identified. The authors did not mention that they will release the code. However, this code should share some similarities with the MoCo v2 code, which is available.									The paper is reproducible. Methods are clearly described and tested on publicly available datasets.									Datasets and code: the authors used public dataset for their methods. The authors have neither provided nor mention the availability of the models, training/evaluation code upon acceptance. Experimental results: No result on the different hyperparameters setting or on the sensitivity of hyperparameters on the results. The authors used fixed hyperparameters.																				
225	Frequency-Aware Inverse-Consistent Deep Learning for OCT-Angiogram Super-Resolution 	The network could be reproduced only based on the manuscript without major difficulties.									The network details of the paper are not described clearly enough, leading to difficulties in reproducing the method.									The paper includes information that would make it possible to reproduce the methods and experiments.																				
226	From Images to Probabilistic Anatomical Shapes: A Deep Variational Bottleneck Approach 	The dataset used for developing and validating the proposed method is not available. The code and model will be made available, and a similar public dataset will be provided for testing the code and model. Results are likely producible.									The reproducibility of the paper is good.									Reproducibility seems good.																				
227	FSE Compensated Motion Correction for MRI Using Data Driven Methods 	No issues.									Seems reproducible since the authors have committed to sharing the code upon publication.									Good.																				
228	Fusing Modalities by Multiplexed Graph Neural Networks for Outcome Prediction in Tuberculosis 	The data is publicly available through the Tuberculosis Data Exploration Portal, but some of the genomic data received additional processing to generate functional domains, and this process was only briefly described and may be difficult to reproduce. However, most of the data will be available, as will the code, so reproducibility is likely feasible.									The dataset preparation is complex. It is well explained in 3.1., but for reproducibility (and ease of entry for other research teams), I again recommend the co-publishing of the data pre-processing code, from Tuberculosis Data Exploration Portal to the level of input into the d-AEs. The dataset is publicly available. According to the reproducibility statement, the method is planned to be published open-source (license model would be interesting to mention in the paper). This is not mentioned in the paper itself, however. Importantly, otherwise the method may be difficult to reimplement purely from the paper, unless very familar with multiplexed-GNNs. Hardware requirements for reproducibility: Authors only mention the CPU. Was a GPU used for training? If not, was there a reason (e.g. multiplexed-GNN training cannot be easily accelerated by GPU)? If GPU, how much VRAM would be necessary to reproduce, or at least how much VRAM did the training GPU have?									The authors are using the Tuberculosis Data Exploration Portal dataset, and they specify that training and evaluation code will be available for download																				
229	FUSSNet: Fusing Two Sources of Uncertainty for Semi-Supervised Medical Image Segmentation 	Overall good, code has been made available. But details of how optimal hyperparameters were arrived at is lacking (see below).									The details about the experimental setup are thorough. And the authors have promised to release the code upon acceptance which is good for reproducibility.									Authors use public datasets for benchmarking and employ a widely used network architecture, etc., all aiding reproduciblity. Moreover, the code will be made available. However, it is not clear if the code will also provide information about the data split (train / validatiation and train with vs. without label for the different steps). This would not be an issue, if the authors reported results based on multiple random splits of the same data instead of single data points (e.g. reporting mean +- std.dev. of reported metrics over e.g. n=5 training runs of the full pipeline).																				
230	GaitForeMer: Self-Supervised Pre-Training of Transformers via Human Motion Forecasting for Few-Shot Gait Impairment Severity Estimation 	I think this paper has a strong reproducibility with a complete description of the training process, as well as its reasonable model design.									A lot of parameters and design details are missing. It seems that it cannot be reproduced only by reading the paper.									reproducible																				
231	GazeRadar: A Gaze and Radiomics-guided Disease Localization Framework 	Reproducible									The authors are willing to share the training and testing code. Thus, it's reproducible.									The paper seems to be reproducible since the authors intend to provide the code and open-source datasets were used for evaluation (RSNA Pneumonia Detection Challenge Dataset, SIIM-FISABIO-RSNA COVID-19 Detection Dataset, NIH Chest X-rays Dataset, and VinBigData Chest X-ray Abnormalities Detection Dataset).																				
232	Geometric Constraints for Self-supervised Monocular Depth Estimation on Laparoscopic Images with Dual-task Consistency 	The authors have confirmed in the checklist that they will share the code to the public. The dataset used in the work is from a public challenge.									The authors mention code being made available, which is great, but please remember to place a link in the paper once it's no longer anonymous.									Implementation details were provided. The evaluation dataset was cited.																				
233	Gigapixel Whole-Slide Images Classification using Locally Supervised Learning 	Seems OK.									Implementation details are available.									For all code related to this work that they will release if this work is accepted																				
234	Global Multi-modal 2D/3D Registration via Local Descriptors Learning 	The method is very well described and could easily be reproduced. The evaluation dataset is not public, though.									The steps involved in the experiment design are clearly explained but lack a bit of details. The code base seems to be unavailable.									The description is quite short and probably difficult to reproduce, see for example section 2.																				
235	GradMix for nuclei segmentation and classification in imbalanced pathology image datasets 	The description of GradMix is quite clear and shall be reproducible.									The proposed method, the dataset, and the deep learning method used in the paper are clearly described. If the codes and the datasets were to be provided, I believe the results could be reproduced.									The authors use two publicly available datasets and the authors plan to make their code available.																				
236	Graph convolutional network with probabilistic spatial regression: application to craniofacial landmark detection from 3D photogrammetry 	Algorithm described in enough detail to reproduce it with some thought (and checking the references).  No mention of a public version of the code/data in main paper.									There is a full description of the methods and data used indicating good attention to reproducibility.									The reproducibility of this paper is unclear without the released data and code.																				
237	Graph Emotion Decoding from Visually Evoked Neural Responses 	As mentioned in the weakness section, this paper only used five subject data from fMRI, it would be better to use more data to show the reproducibility.									acceptable.									It seems reproducible. Data is available.																				
238	Graph-based Compression of Incomplete 3D Photoacoustic Data 	Although related codes are not publicized, I believe this method is not difficult to reproduce with introduced technical details.									Codes will be released. Otherwise reproducibility would be difficult.									Authors do not mention the availability of source code.																				
239	Greedy Optimization of Electrode Arrangement for Epiretinal Prostheses 	Methods are clear and reproducible.									The work here is reproducible in terms of simulation and computation.									na																				
240	Hand Hygiene Quality Assessment using Image-to-Image Translation 	The manuscript is well written and contains all information required to reproduce the work. The authors will also make the pre-trained models public, although not the dataset.									The authors do not explicitly state if they make the code or the dataset available.									All the details with respect to implementing the model have been clearly discussed. I encourage the authors to consider releasing the dataset, to facilitate future research in the area.																				
241	Harnessing Deep Bladder Tumor Segmentation with Logical Clinical Knowledge 	The overall method is explained clearly, but the details of clinical knowledge integration into the network are not clear enough									The code is not available but the authors guarantee that they will provide it if the paper is accepted. In that case I do not have other way to proof the reproducibility. The method described looks clear and it looks as reproducible, but the results cannot be checked without the code and the dataset.									nan																				
242	Hierarchical Brain Networks Decomposition via Prior Knowledge Guided Deep Belief Network 	They will release the code after the paper is accepted.									It is reproducible with codes and data becoming available									The paper decribed the method clearly, but did not release the code currently.																				
243	Histogram-based unsupervised domain adaptation for medical image classification 	The paper appears to be reproducible.  The most important component will be code in pytorch and tensorflow code for the histogram-layer. The paper doesn't tell which code libraries are used, so it is hard to determine what the implementation will be without code.									Difficult to reproduce.									No abvious issue of the reproducibility as the author claimed.																				
244	How Much to Aggregate: Learning Adaptive Node-wise Scales on Graphs for Brain Networks 	The dataset is a well known public dataset, the data selection and use is clear. Crucial details on the preprocessing are missing. Please clarify the PET normalization step.									According to the reproducibility checklist and information from the paper, the results should be reproducible after the potential acceptance.									This paper does not provide code. The only experimented dataset ADNI is restrictively available.																				
245	Hybrid Graph Transformer for Tissue Microstructure Estimation with Undersampled Diffusion MRI Data 	This work showed									The data used in this work is publicly available, but the authors have not indicated whether their method will be made publicly available.									The experiments are described in sufficient detail to be reproduced.																				
246	Hybrid Spatio-Temporal Transformer Network for Predicting Ischemic Stroke Lesion Outcomes from 4D CT Perfusion Imaging 	The method is well described, and the authors promise to release the code. Therefore, implementation-wise, the proposed work should be reproducible. The main concern is related to the dataset, which is in-house. In this sense, it is not possible to reproduce the final results. The Reviewer wonders if it is possible to release at least a small set of images, together with results and manual annotations. In summary, it should be possible to implement the proposed method adequately, but the final results cannot be reproduced.									The authors state that code will be released. The in-house data is unfortunately not available; it might be difficult to reproduce the presented results.									The authors missed some important details of the method, which hinders the reproducibility of the paper. However, they promised to release the code.																				
247	Ideal Midsagittal Plane Detection using Deep Hough Plane Network for Brain Surgical Planning 	Unfortunately, implementation of the method is not given. From that perspective, reimplementation would be needed. Provided level of detail is however limited and consequently reproducibility is only moderate.									The network architecture is clear and the algorithm logic is rigorous. Only in the experimental part is not obvious, the comparison test is not full; Clinical needs studies based on real data sets; It has relatively good reproducibility.									The paper is reproducible.																				
248	Identification of vascular cognitive impairment in adult moyamoya disease via integrated graph convolutional network 	It seems that the authors have provided the necessary information to reproduce the methods, but it is still strongly suggested to publish the source code online if available, which can help the readers to fully understand its mechanism.									The reproducibility of the paper is limited.									The dataset is not sufficiently described.																				
249	Identify Consistent Imaging Genomic Biomarkers for Characterizing the Survival-associated Interactions between Tumor-infiltrating Lymphocytes and Tumors 	Yes.									Public data and adequate description of the methodology									Datasets and experimental setup are clearly mentioned.																				
250	Identifying and Combating Bias in Segmentation Networks by leveraging multiple resolutions 	Not applicable									The statement of the authors for reproducibility does not correspond with the paper. There is no mention of code availability, nor information of time/cost for training. Neither hyper parameter optimisation setting. It does not seem that this study can be reproduced due to all this lack of details. Methods are very vaguely described.									The author's results should be easily reproducible as their models are taken from other papers, and their datasets are accessible elsewhere. To improve reproducibility the authors should make the images in each of their splits available.																				
251	Identifying Phenotypic Concepts Discriminating Molecular Breast Cancer Sub-Types 	Yes, the methods are reproducible. The authors reference the ResNet-18 classification model that they train with a modified classification head to fit the task of 4-class classification. Details of the private dataset are provided although there is no reference to ethical approval.									Data from 102 patients was not made public nor is the coding.									Very good.																				
252	Implicit Neural Representations for Generative Modeling of Living Cell Shapes 	The work could be reproduced									The paper indicates that source code will be available.									Sufficient information has been provided to allow the paper to be reproduced. The data appears to already be in the public domain and the methods are sufficiently well-described to allow me to believe that I could implement this method from the information provided.																				
253	Implicit Neural Representations for Medical Imaging Segmentation 	The effectiveness of the influence of sampling hyper-parameters is not analyzed, it could affect the reproducibility of the algorithm.									Yeah, it looks reproducible									The source code is available according to the author's answer to the reproducibility checklist.																				
254	Improved Domain Generalization for Cell Detection in Histopathology Images via Test-Time Stain Augmentation 	The proposed method basically uses the existing codes and the different part was described, and thus, the reproducibility is fine.									The reproducibility response are followed in the paper.									The source code implementing the presented concept is not provided. Implementation details provided in the paper in principle suffice that experienced programmer could implement the concept. The hyperparameter of proposed concept is the number of detected cells with high enough confidence scores to be fused together. It set to 3 in the only experiment conducted in the paper. But no justification based on sensitivity analysis is provided.																				
255	Improving Trustworthiness of AI Disease Severity Rating in Medical Imaging with Ordinal Conformal Prediction Sets 	Very good. The algorithm is clearly described and code is promised to be released.									The authors stated that they will make the codes publicly available									Some parts are reproducible but overall, many details about the model employed is missing.																				
256	Incorporating intratumoral heterogeneity into weakly-supervised deep learning models via variance pooling 	Very good									All five datasets are publicly accessible. The authors stated that code will be publicly available.									The authors have provided sufficient details for reproducibility of the paper.																				
257	INSightR-Net: Interpretable Neural Network for Regression using Similarity-based Comparisons to Prototypical Examples 	The work is reproducible.									No code available.									Implementation details are given, code will be available on github once the anonymity will be removed.																				
258	InsMix: Towards Realistic Generative Data Augmentation for Nuclei Instance Segmentation 	the code will be made available, the data is public									the idea seems reproducible - the authors claim to make the code publicly available									Good. Datasets are public and code will be public.																				
259	Instrument-tissue Interaction Quintuple Detection in Surgery Videos 	Due to lacking many details about the convolutional and fully connected operations, activation functions, size of the output feature maps of different layers, number of trainable parameters, and many other important details, the paper is not reproducible. In case the dataset will not be released: Since the dataset will not be released with the acceptance of the paper, there is no possibility to reproduce the results and explore the subject further. Indeed, only the authors themselves can improve the results provided, which is regarded as a major weakness of the current paper.									The authors have checked the boxes relating release of the source code and the dataset which is specifically build and annotated for this study.									The methods have been described in detail and should be reproducible.																				
260	Interaction-Oriented Feature Decomposition for Medical Image Lesion Detection 	Positive if code and the private dataset is released.									The authors have checked all questions in the Reproducibility Checklist as Yes. However, no code or data will be released according to the paper. No statistical significance was reported either.									The paper is well-written with clear description of technical details. It should be easy to reproduce.																				
261	Interpretable differential diagnosis for Alzheimer's disease and Frontotemporal dementia 	Used data from open source MRI databases (ADNI, OASIS, AIBL, MIRIAD, and NIFD) Was not detailed enough in the preprocessing steps, for example, did not talk about algorithms used for each part of the preprocessing pipeline. There is quite a bit of information missing with respect to hyper-parameters used for some of the algorithms used in the paper. For example, AssemblyNet and for the Support vector machine used.									The authors give details about the implemented neural networks, but not about the SVM. The databases used are available. They don't have the code available.									"The data used are all from open-access databases.  The implementation of the algorithm is clearly described in the method section for an expert to reproduce the main results.  However, it might be better for the authors to further clarify how they perform ""oversampling technique"" to balance the number of training data from each category."																				
262	Interpretable Graph Neural Networks for Connectome-Based Brain Disorder Analysis 	The authors provided clear code to make it reproducible.									partly performed on public data and provide source code so reproducibility of some of the findings should be excellent									The submission meets all criteria on the reproducibility checklist.																				
263	Interpretable Modeling and Reduction of Unknown Errors in Mechanistic Operators 	Given the level of detail in the paper, it would be difficult to reproduce the author's work. The architectures and other training details aren't mentioned. In addition, I am unclear on some portions of the method itself. The data used seems to be mostly synthetic so it seems like it would be possible to reproduce the data.									There is no reproducibility problem.									Synthetic and public real data are used. Codes are also submitted for review.																				
264	Interpretable signature of consciousness in resting-state functional network brain activity 	Not Applicable									The paper provides reasonable reproducibility.									The method is reproducible, although the data is not since it was acquired using a custom surface coil (although this is perfectly fine). The atlases used are also readily available, as they are open source atlases. Each section of the 4-step framework is thoroughly explained with all models written out.																				
265	Intervention & Interaction Federated Abnormality Detection with Noisy Clients 	Looks promising									I didn't check it									The paper provides sufficient details for reproducibility. However, the authors have not clarified if the code will be released upon acceptance. Some details can be added, e.g. in instance dependent noise, the proportion of labels that were flipped based on the proposed criterion, statistical significance of the reported differences in mAP values.																				
266	Intra-class Contrastive Learning Improves Computer Aided Diagnosis of Breast Cancer in Mammography 	Ok. The dataset is in-house, so I don't know if it will be released. Otherwise, the implementation is a straightforward contrastive loss setup with a ResNet. That should be reproducible, but the heuristics of the weighting is not provided (see below in my detailed comments).									The reproducibility of the paper should not be an issue.									The experimentation is well written and i believe it can be reproducible																				
267	Invertible Sharpening Network for MRI Reconstruction Enhancement 	The authors provided details about the proposed models, datasets, and evaluation. However, the reference code was not provided and and does not seem to be released after the review process.									The dataset used in this paper is publicly available. This paper has the reproducibility.									The training details are missing. (optimizer, # of epochs etc.) The evaluation is on 2 public datasets and 1 private dataset.																				
268	Is a PET all you need? A multi-modal study for Alzheimer's disease using 3D CNNs 	The reproducibility of the paper is not very well.									The description of method and validation framework provides moderate reproducibility.									The authors mention in reproducibility statement that they will release code and trained models after acceptance																				
269	iSegFormer: Interactive Segmentation via Transformers with Application to 3D Knee MR Images 	The authors have provided the code.									Overall, the reproducibility seems to be good. The authors use public datasets for training/evaluation of their models (e.g., the OAI-ZIB dataset), and their methods are reasonably well-explained. There are, however, a couple of training-related methods that seem to be under-explained, which could impact the reproducibility of their work, should a researcher try to reproduce their exact methods: There are a few questions that are unanswered about the automated click generation procedure during training/inference: (1) How are the clicks initialized for a given sample? (2) What if there are multiple false positive or false negative regions? Is a positive or negative click generated at the center of each distinct region? As someone who is unfamiliar with training interactive segmentation models but very familiar with training non-interactive training models, how are samples/batches fed into the network? Especially since you're adding clicks to false positive/false negative regions during training, I'm assuming you feed the same training sample into the network multiple times during the same epoch, with different click configurations. If this is the case, are you consecutively feeding the same training sample (with different click configurations), or do you interleave with other training samples/slices?									Code was made available and the datasets used are publicly available.																				
270	Joint Class-Affinity Loss Correction for Robust Medical Image Segmentation with Noisy Labels 	The algorithm is simple and should be easily to reproduced.									This paper provides some details on the method implementation. But such details still may not be sufficient to reproduce the method. It would be helpful if the authors would release their source code.									The experimental details are clear. Code is not available.																				
271	Joint Graph Convolution for Analyzing Brain Structural and Functional Connectome 	Code not submitted.									The reproducibility is basically good. The method is present clearly.									1) It is better to clearly describe how the baseline methods were implemented. For example, for the multi-view GCN in comparison, how many views were uses in the experiments is not described.  2) The range of hyper-parameters considered, method to select the best hyper-parameter configuration are not described. 3) 831 individuals (ages 12-21) were recruited in NCANDA dataset. Since there are only 662 were used in this study, it is better to explain the exclusion criteria in the paper. 4) The age distribution for the 1976 pairs of scans can be added.																				
272	Joint Modeling of Image and Label Statistics for Enhancing Model Generalizability of Medical Image Segmentation 	The reproducibility file mentions code will be available, but this is not mentioned in the paper. Given the relatively complicated modelling, code availability could be important for reproducibility.									the authors use public datasets, which is great code would be nice as the model is complex									Public image databases were used. However, some training details are missing.																				
273	Joint Prediction of Meningioma Grade and Brain Invasion via Task-Aware Contrastive Learning 	Given that the authors would release the code upon acceptance, the reproducibility of the algorithm is high especially if a trained version is released. It could be higher with release of the data set used but understandably there are probably IRB, HIPAA or other issues to allow that.									Code and data do not seem to be made available for this work. However, the network architecture is described in sufficient detail for replication if the additional description of how the task-common features are aligned to the specific tasks is included.									The method is well described and code will be published. Reproducibility is good even if the dataset is not public.																				
274	Joint Region-Attention and Multi-Scale Transformer for Microsatellite Instability Detection from Whole Slide Images in Gastrointestinal Cancer 	Code will be made available. Experiment is based on public dataset.									Details on the transformer implementation, and the classifiers used, are not well detailed in the paper or suplementary material as well as other modules shown in Fig 1 (MLP, CNN, attention module).  However, this is less relevant since the authors are providing the code.									Datasets and experimental setup are clearly mentioned.																				
275	Kernel Attention Transformer (KAT) for Histopathology Whole Slide Image Classification 	There are some confusion in Pre-processing and Data preparation.  It would be better if you could explain it in more detail or give the pseudocode in supplementary.									Not clear if codes will be available.									No code availability is mentioned and the training dataset is not explicitly mentioned. The architecture is well-described but it would be difficult to reproduce the results from this paper alone.																				
276	Key-frame Guided Network for Thyroid Nodule Recognition using Ultrasound Videos 	The deep network structures used have been described in details, including the number of layers, the kernel size of each layer, etc.  The experiments is conducted on a self collected ultrasound dataset with 3000 videos.  The Reproductivity report shows that the model and codes will be released if this paper is accepted.									A clear explanation of the implementation steps is provided. A private dataset is used for the evaluation and no link is provided to access the dataset. Moreover, no information about the data collection condition (equipment, ...) is provided.									The authors should provide a complete description of the data collection process, such as descriptions of the experimental setup, device(s) used, image acquisition parameters, subjects/objects involved, instructions to annotators, and methods for quality control.																				
277	Knowledge Distillation to Ensemble Global and Interpretable Prototype-based Mammogram Classification Models 	Results presented in the paper are reproduceable.									-The authors declares they will share all the codes for the experiement when accepted. The authors also presented most of the relevant setting parameters for the expriments.									In the reproducibility checklist, the authors claim to release the training/evaluation codes with pretrained models which is a plus. Since the proposed method is a sophisticated ensemble learning framework with multiple loss functions, source code release could be a great help.																				
278	Landmark-free Statistical Shape Modeling via Neural Flow Deformations 	Yes									Many implementation details are missing (see above) such as : the number of RBF basis considered, the optimization methods, the number of PCA modes considered and how this number was chosen, the number of sampling points...									It is very difficult to reproduce the paper because of its shortness explaining the method. The supplement is not enough either for this purpose.																				
279	Layer Ensembles: A Single-Pass Uncertainty Estimation in Deep Learning for Segmentation 	Seems reproducible									the paper is reproducible to a large extent.									The data is based on publicly available data but code will be disclosed.																				
280	Learn to Ignore: Domain Adaptation for Multi-Site MRI Analysis 	code online, inhouse data can't be published due to privacy concerns, but other data used is publicly available.									The github link is provided in the paper.									Good. The authors have released the source code which is helpful to reproduce the proposed method.																				
281	Learning Incrementally to Segment Multiple Organs in a CT Image 	The clarity needs improvement to allow for reproducibility									Good									The code does not appear to be made available but can be reproduced using the explanations provided. All the datasets used for training and validation phases are publicly-available. The testing phase relies on 3 datasets including 2 which are private.																				
282	Learning iterative optimisation for deformable image registration of lung CT with recurrent convolutional networks 	The paper is reproducible.									Good reproducibility.									The paper is reproducible and source code is available online.																				
283	Learning Robust Representation for Joint Grading of Ophthalmic Diseases via Adaptive Curriculum and Feature Disentanglement 	The reproducibily is adequate. Perhaps, the authors could give more details regarding the key parameters involvedin their method.									details of the network needs to be clearer									The dataset is not well introduced, and the data split is not introduced.																				
284	Learning self-calibrated optic disc and cup segmentation from multi-rater annotations 	The experiments have been conducted on a publically available dataset. Most of the implementation details are provided.									Considering the complexity of the model design and missing detail for hyperparameters, it is not easy to reproduce this work and thus the authors are highly encouraged to release code, model architecture and hyperparameters for good reproducibility.									The authors stated that they will provide the relevant codes.																				
285	Learning shape distributions from large databases of healthy organs: applications to zero-shot and few-shot abnormal pancreas detection 	The specific details of the model are explained in the supplementary material, so I don't think the reproducibility of the paper is difficult. If the author can provide code, it will be more helpful for readers to understand the details of the paper.									it is good.									The technical details are comparatively sparse in the paper. However, given the fact that the underlying technique is well-known, the authors relying on public software (nnUnet) and the main steps are documented, I think that the paper is still reproducible.																				
286	Learning towards Synchronous Network Memorizability and Generalizability for Continual Segmentation across Multiple Sites 	Data is open source. The code will be made available at the time of publication, and authors have made a anonymous github link available. Note that no code is available now.									Reasonably reproducible.									This study is easily reproducable.																				
287	Learning Tumor-Induced Deformations to Improve Tumor-Bearing Brain MR Segmentation 	Several aspects missing, e.g., the number of training and evaluation runs, details of baseline methods, details of train/validation/test splits, No standard deviations or statistical significance tests for the results of synthesized images, clinical implications The parameters for the point cloud network and its training (architecture, batch size, learning rate, etc) are not in the main text nor the supplemental material									The authors have not provided any code. The description of the model is clear and well-written.									The paper is clear and detailed. Precise parameters of the regression network are not given. There are also missing some details in how the synthetic data is generated.																				
288	Learning Underrepresented Classes from Decentralized Partially Labeled Medical Images 	The paper provides sufficient details about the models/algorithms, datasets, and evaluation.									Would be good if the authors release the code as promised.									They could reproduce the results with some difficulty.																				
289	Learning with Context Encoding for Single-Stage Cranial Bone Labeling and Landmark Localization 	Paper appears to be meeting reproducibility criteria									It is not difficult to reproduce the method of this paper with the released code.									Reproducibility is highly likely, as the novel contextual encoding mechanism is well explained in detail. Furthermore, implementation details such as training parameters, software and hardware details are reported as per the best practices. However, access to the implementation code should increase the reproducibility further along with information of pre/post processing of data.																				
290	Learning-based and unrolled motion-compensated reconstruction for cardiac MR CINE imaging 	No  data and  software are provided. The authors appear to provide sufficient details about the  training of the network and reproducing it may be feasible with some effort.									good.									If possible, authors should make the code available. Replication of this work is not straightforward because of the complex implementation framework and use of in-house data.																				
291	Learning-based US-MR Liver Image Registration with Spatial Priors 	Not so good.									No data (existing data used) or code available but other aspects of reproducibility included.									I believe it could be reproduced from the paper.																				
292	Lesion Guided Explainable Few Weak-shot Medical Report Generation 	The paper lacks parameter details, so it is difficult to reproduce only by the paper.  The author states that the code will be published in the future.									Positive if the code is release and data partition is provided									Authors provided a checklist for reproducibility. The description of the methods, implementation and validation studies are also satisfactory.																				
293	Lesion-Aware Contrastive Representation Learning for Histopathology Whole Slide Images Analysis 	Reasonable level of implementation details has been provided in the paper.									The code will be released for reproducibility.									Authors claim they will release the code. If so, the study could be reproduced.																				
294	Lesion-aware Dynamic Kernel for Polyp Segmentation 	The code will be made available. The paper is clear enough to reproduce the method. The authors talk about a new dataset, and in the form indicate it will be released, however, details on this are very scant. Insufficient information on the data, labelling procedure, etc. etc. No link.									"The authors say that ""The code and collected dataset will be made available."". Also, the reviewer think it is not difficult to reproduce LDNet according to the details provided by authors."									Reproducibility could be improved (see detailed comments below)																				
295	Less is More: Adaptive Curriculum Learning for Thyroid Nodule Diagnosis 	The author should supplement more information about the contributed dataset, such as the source of data, device(s) used, image acquisition parameters, instructions to annotators, and methods for quality control.									Dataset will be available Code will be available									I think this paper can be reproduced according to the proposed training algorithm. It will be better to provide the new dataset used in this paper.																				
296	Leveraging Labeling Representations in Uncertainty-based Semi-supervised Segmentation 	The paper is easy to follow. Technical details are clearly described for reproducing.									yes									It can be implemented easily.																				
297	LIDP: A Lung Image Dataset with Pathological Information for Lung Cancer Screening 	This paper will only be reproducible if the database is made publicly available. The authors do not mention that on the paper. I strongly recommend doing so.									The paper is proposing to share a new dataset making this portion extremely reproducible. In terms of the Machine learning approaches, they appear to be extremely difficult or impossible to reproduce.									Even knowing that the main purpose of this paper is to present a new dataset, it should have included a description of the computer infrastructure used (hardware and software), more information about the memory requirements and other specifications about the training and testing process.																				
298	LifeLonger: A Benchmark for Continual Disease Classification 	The code of the paper will be made available publicly.  From the checklist authors claim that the software framework and runtime/memory footprint statistics are included, however both are not included in the paper.									The authors plan to publish their code									The authors fulfill all necessary reproducibility criteria.																				
299	LiftReg: Limited Angle 2D/3D Deformable Registration 	Code will be released upon acceptance, and DIRLAB is public data. In order to reproduce this network, the most difficult step will likely be the spatial warping transformer, so hopefully this will be included in the code release.									The training datasets and the associated codes are disclosed in this work. The implemented code specific to this method however is not disclosed. The amount of implementation details provided in the main text and the supplementary materials is sufficient.									nan																				
300	Light-weight spatio-temporal graphs for segmentation and ejection fraction prediction in cardiac ultrasound 	The code will be released and the used dataset is public.									Reproducibility seems fine. Components are modular. Code is shared online. One issue is that there are multiple ways their approach could be used so, for someone else to compare to this work, it could be a bit tedious.									Details on the implementation are in the manuscript and supplementary material. Authors plan to release the code repository. They use public datasets. Note to the authors: You can use https://anonymous.4open.science/ for providing an anonimous link to your repository during review.																				
301	Local Attention Graph-based Transformer for Multi-target Genetic Alteration Prediction 	The authors claimed that the implementation would be published. The paper should be reproducible.									The reproducibility of the paper is good. The authors provided necessary details from the reproducibility checklist.									Datasets and experimental setup are clearly mentioned.																				
302	Local Graph Fusion of Multi-View MR Images for Knee Osteoarthritis Diagnosis 	I am not sure whether the propsed method can be re-implemented because some points are not well illustrated.									I think the reproducibility of this work is ok.									It will be great to have the code availability included within the paper for reproducibility.																				
303	Localizing the Recurrent Laryngeal Nerve via Ultrasound with a Bayesian Shape Framework 	I believe that the obtained results can be reproduced.									The dataset will not be public: this is a concern. However, authors mention that the network and trained weights could be found later.									All the experiments are conducted with private dataset. The performance of baseline method is too low, I don't know if the authors well trained the baseline models, however, the authors didn't give any explanation, such as the training losses curves of baseline models.																				
304	Local-Region and Cross-Dataset Contrastive Learning for Retinal Vessel Segmentation 	The details of the method are introduced in the paper. It should be easy to reproduce the paper.									It seems to me that the paper can be easily reproduced.									The reproducibility is moderate as parts of training hyperparameters are introduced and network structure is shown in supplementary material.																				
305	Longitudinal Infant Functional Connectivity Prediction via Conditional Intensive Triplet Network 	All the steps of the method are detailed throughout the paper easing the task of external implementations from other teams. The data comes from the BCP so easy access for reproducibility is assured. Of course, freely available and easily readable code is always appreciated by the community. As in any Deep Learning model, this last step is crucial for testing and usage from external people, so the reviewer highly encourages to publish them.									The reproducibility is good. The method is present clearly. The data is public available.									Some hyper parameters were missing e.g. the width of hidden layer in E and G, training parameters. Thus it might be hard to reproduce this paper.																				
306	Long-tailed Multi-label Retinal Diseases Recognition via Relational Learning and Knowledge Distillation 	The authors provide sufficient implementation details, but no absolute guarantee for reproducibility.									Although the dataset are free public available, the authors should consider to release the code and some models in order to reproduce the results!									-The authors declares they will share all the codes for the experiement when accepted. The authors also presented most of the relevant setting parameters for the expriments.																				
307	Low-Dose CT Reconstruction via Dual-Domain Learning and Controllable Modulation 	The method is not clearly clarified, and it is supposed to provide the source code for reproduction. E.g., in Fig.1, the second block in FB takes four inputs, while the description in Section 2.2 shows that there are three inputs, which is not consistent. For the relatively complex reconstruction network, the authors do not provide enough information on its settings, such as kernel size. Even the setting values of many important hyperparameters are not given, like gamma_1 and gamma_2 in loss functions. One of the two datasets they used seems to be private too. Currently, the paper doesn't have a strong reproducibility.									They authors do not share resources to reproduce									The authors followed the rules of ethical and biological requirements for medical data processing.																				
308	Low-Resource Adversarial Domain Adaptation for Cross-Modality Nucleus Detection 	The method is defined clearly and all implementation details are given.									As far as I saw (please, correct me if I'm worng), the authors do not provide any link to the dataset or the code, which makes it incredibly difficult to reproduce due to the complexity of the architecture.									All the methods developed in this paper are clear and valid. Also the implementation details are properly described and values of all the parameters are reported.																				
309	LSSANet: A Long Short Slice-Aware Network for Pulmonary Nodule Detection 	The paper is very valuable, but requires more information from the authors to illustrate reproducibility. Such as code, models, etc.									The reproducibility of this paper is good.									no review																				
310	MAL: Multi-modal attention learning for tumor diagnosis based on bipartite graph and multiple branches 	The datasets used in this work are private clinical data which are not public. Authors mentioned their code will be publicly available on Github.									Implementation details have been described.									The code is claimed to be available at Github.																				
311	MaNi: Maximizing Mutual Information for Nuclei Cross-Domain Unsupervised Segmentation 	I am a little concerned about the reproducibility of this paper. I would suggest the authors cite the values from previous papers and report their own performances using the same code based on their own runs. Furthermore, I would be happy to see that the author can make their code and dataset open-source.									All datasets used in the experiments are public. Source code is not available during review.									Not sure how easy/difficult it is to reproduce the paper.																				
312	Mapping in Cycles: Dual-Domain PET-CT Synthesis Framework with Cycle-Consistent Constraints 	The description of the training details in this paper is insufficient. Moreover, the authors did not give any positive response regarding the code release in the reproducibility checklist. So I think this paper does not have good reproducibility.									The dataset is private and the codes are not released.									The method description is clear with sufficient details.																				
313	Mask Rearranging Data Augmentation for 3D Mitochondria Segmentation 	The authors do not mention the range of hyper-parameters considered nor the method to select the best hyper-parameter configuration. They only specify some of the hyper-parameters used to generate results. The exact number of training and evaluation runs (iterations or epochs) is not provided. A description of the computing infrastructure used (hardware and software) is provided together with an analysis of situations in which the method failed (too limited data). There is no description of the memory footprint nor an average runtime for each result, or estimated energy cost. There is no analysis of statistical significance of reported differences in performance between methods. The results are not described with central tendency (e.g. mean) & variation (e.g. error bars). The specific evaluation metrics and/or statistics used to report results are correctly referenced. There are no details of train / validation / test splits nor details on how baseline methods were implemented and tuned.									"The authors listed ""yes"" for both code and pre-trained models. In this case, it can be an easy task for both training and testing. If the reproduction was only based on the descriptions in the paper, it could be somewhat difficult."									"No repo link is provided in the manuscript but the authors stated that related code will be released in the reproducibility response; considering that the key parameters about mask rearrangement is not clearly stated (please see comments section), it can be helpful to provide code or provide more complete description. In the reproducibility response the authors stated ""yes"" for the following list of information but did not include corresponding information in the manuscript: mean, variation, statistical significance of experimental results; runtime, memory footprint; failure cases. Could the authors at least report mean and variation of the results?"																				
314	MaxStyle: Adversarial Style Composition for Robust Medical Image Segmentation 	Before the publication of the Authors' codes, it is difficult to replicate directly. However, the description of the method is clear and the foundation on which this manuscript is based on, namely, the MixStyle method, is easy to reproduce.									This paper is highly reproducible with detailed information in paper and the code will be released.									The authors will provide the code. Used datasets are public.																				
315	MCP-Net: Inter-frame Motion Correction with Patlak Regularization for Whole-body Dynamic PET 	The authors have given lots of information for reproducing this work, with some details missing.									The paper presents a link to the code, which I'm assuming will be public upon acceptance.									Good																				
316	Measurement-conditioned Denoising Diffusion Probabilistic Model for Under-sampled Medical Image Reconstruction 	It is expected the results can be reproduced with the given information.									very good.									The dataset is publicly available fastMRI single-coil knee image. The detail configuration is described. However, it is not clear how to split the training/validation/testing dataset. Is this the same with the challenge?																				
317	Mesh-based 3D Motion Tracking in Cardiac MRI using Deep Learning 	Good reproducibility.									The dataset and imaging information used in this study are well described and the model training parameters are detailed in sufficient detail.									Reproducible. Data available. Method detailes described well.																				
318	Meta-hallucinator: Towards few-shot cross-modality cardiac image segmentation 	The training hyperparameters are clearly described, but some methodology details are missing.									Authors agreed to release the code.									The dataset is public available. The authors promise to release the codes.																				
319	MIRST-DM: Multi-Instance RST with Drop-Max Layer for Robust Classification of Breast Cancer 	The paper seems reproducible.									The authors should release their codes, or this paper is not easy to be reproduced.									The hyperparameters for the model training are clearly given. The datasets used are publicly available. There is no mention that the code will become publicly available upon acceptance.																				
320	Mixed Reality and Deep Learning for External Ventricular Drainage Placement: a Fast and Automatic Workflow for Emergency Treatments 	All the required methods are described in sufficient detail to reproduce the work, but no code or data is provided.									The reproducibility of the paper is good.									Sufficient details have been provided for replication, but due to data limitation, the difficulty can be high.																				
321	mmFormer: Multimodal Medical Transformer for Incomplete Multimodal Learning of Brain Tumor Segmentation 	The method proposed is reproducible.									Certain implementation details are missing: Number of initial or filters per level for the CNN encoders patch size for the intra-modal transformers is not provided / were the inner-most features of the CNN encoder just flattened? what features from the encoder are forwarded as skip features? If it is the features from CNN encoders at specific levels, how are they combined across modalities? from Fig.1, it appears as if the intra and inter-modal transformers are used only at the bridge/innermost level. If so, what is used as skip features at every level in the decoder?									reproducible if hyperparameters are all provided, however not																				
322	Modality-adaptive Feature Interaction for Brain Tumor Segmentation with Missing Modalities 	"The authors listed ""yes"" for both code and pre-trained models. In this case, it can be an easy task for both training and testing. If the reproduction was only based on the descriptions in the paper, it could be somewhat difficult."									acceptable									The paper meets the standard requirement in terms of reproducibility.																				
323	ModDrop++: A Dynamic Filter Network with Intra-subject Co-training for Multiple Sclerosis Lesion Segmentation with Missing Modalities 	The authors provided sufficient information to reproduce the propsoed method.									The code and data are publicy available.									Public datasets have been used, but no code repository have been shared. I encourage authors to publicly share the implementation as I feel this work could have noticeable impact in the field.																				
324	Modelling Cycles in Brain Networks with the Hodge Laplacian 	The description seems reasonably clear. But I believe the code needs to be publicly available to ensure full reproducibility.									There are two questions about the validity of the statistical analysis (see item 8).									Good.																				
325	Momentum Contrastive Voxel-wise Representation Learning for Semi-supervised Volumetric Medical Image Segmentation 	While the majority of hyperparameters and pipeline settings are listed in the experimental section, the limited clarity of the paper make reproduction harder then it has to be.									The method and architecture is well described. I trust that the code will be publicly available, as the authors wrote in the abstract.									The reproducibility of the paper is good.																				
326	Morphology-Aware Interactive Keypoint Estimation 	The authors include code with detailed readme as part of their supplementary materials. Demo video is provided as well to demonstrate the user interaction process.									As supplementary information, the authors have provided access to the code and an illustrative video showcasing the performance of the algorithms on their test dataset. Within the paper itself however, no reference to the implementation details including the utilized libraries, programing language and the network hyperparameters are provided.									The paper is partially reproducible. The authors aim to provide the code (training and evaluation) as well as the pretrained models, which means that it should be possible to reproduce the results on the smaller, publicly available data set. The in-house data set will not be made publicly available.																				
327	Moving from 2D to 3D: volumetric medical image classification for rectal cancer staging 	Good. Sufficient details are provided to reproduce the work. Dataset will not be public but it's understandable.									nan									1, The localization of rectum region needs more details since it contains both tumor tissue and other benign tissues. this paper doses not specify either the whole rectum region or ROI of tumor is utilized in the experiment. 2, In Fig 1, the input of this CNN Network and H-W pool need to be specified.  From 4th layer to 5th layer, the procedure is ambiguous. The structure of every layer should be plotted in Fig 1 or specified in Section 2.2.  3, Which feature is omitted in the GLCM features since PyRadiomics has 24 GLCM features while this paper chooses 23 features? 4, Some important training parameters are also absent, such as epochs and batch size.																				
328	MRI Reconstruction by Completing Under-sampled K-space Data with Learnable Fourier Interpolation 	Experiments on a different larger dataset is recommended.									The list seems completed.									Some elements of the paper should be more detailed to ensure the reproducibility of the study. How do you determine the k-nearest neighbors of the missing Fourier coefficients in the k-space? Describe precisely the architecture of the deep learning method used to estimate the interpolation weights. What are the inputs and outputs of this deep learning algorithm? How much data are used for training? What is the computational time (at training and testing) of the network? What is the performance of this interpolation process? The considered dataset is small (notably the testing dataset). Due to this fact, a k-fold cross-validation will be more suitable for the fairness of the evaluation method (instead of use a simple split of the cohort in training and testing subsets).  The dataset is only composed of 2D MRI slices. To ensure that the method is suitable for clinical practice the method should be evaluated on 3D MRI volumes. The method was applied and evaluated in only one clinical dataset. Several datasets (in other anatomical sites) should be considered to ensure that the method could be used in clinical practice. Describe precisely the architecture of the two denoising convolutional neuronal networks (how they work?). Please describe the data. What is the MRI scanner used for the data acquisition? How many Tesla? What are the MRI parameters (TE, TR, flip angle, acquisition time, etc)? Please perform statistical tests (such as the Wilcoxon test) to compare the significance of the differences between the proposed method and the state-of-the-art methods.																				
329	mulEEG: A Multi-View Representation Learning on EEG Signals 	It should be reproducible based on the information provided.									It seems reproducible. Data is available.									As the experimental descriptions are enough, I believe this paper is reproducible.																				
330	Multidimensional Hypergraph on Delineated Retinal Features for Pathological Myopia Task. 	* The hyper-parameters for the proposed approach has been described, and the details of the training/testing set has been provided.									The checked points in the reproducibilty checklist match perfectly the information provided in the paper.									The reproducibility checklist was accurately filled out.  One thing that could improve reproducibility of the paper is to make the full code available after publication.																				
331	Multi-head Attention-based Masked Sequence Model for Mapping Functional Brain Networks 	"It seemed that reproducing their results is hard because as the authors filed out, they didn't provide the code, the software framework and version, and a detailed description of their model parameter and overall code. Also, even though they said ""yes"" to the question of ""The details of train/validation/test splits."", it seemed they didn't mention the detail of the split of the data. Also, I cannot find the information on the statistical significance of the reported Pearson's correlation coefficient."									I think this work is easy to reproduce.									The reproducibility of this paper is limited. This work does not release the source code but applying a public data set.																				
332	Multi-institutional Investigation of Model Generalizability for Virtual Contrast-enhanced MRI Synthesis 	Limited reproducibility.									Please add the batch size used for training the networks. Please provide the computational time (at training and testing) of the networks. Additional statistical tests are needed to evaluate the significance of the results. Please use statistical tests to compare the performance of the model trained with multi-centric MRIs to those trained with MRIs from a single center. Did data augmentations were performed to improve the method generalization? Please describe the data. What are the MRI scanners used for the data acquisition? How many Teslas? What are the MRI parameters (TE, TR, flip angle, acquisition time, etc)? Did this study consider the whole MRI volumes or only 2D slice MRIs? It will be great to consider the whole MRI volumes during the method evaluation (that is more needed for clinical practice). JIM model was trained with MRIs from institutions 1, 2, and 3 and evaluated on MRIs from institution 4. That seems not enough to conclude that the model trained with multi-centric MRIs had better generalizability and accuracy than those trained with images from a single institution. I think it will be interesting to train and evaluate the JIM model with MRIs from distinct combinations of institutions (e.g., train JIM with MRI from institutions 2, 3, and 4 and evaluate it on MRI from institution 1) and compare the obtained results (evaluation metrics + statistical tests).									high reproducibility.																				
333	Multimodal Brain Tumor Segmentation Using Contrastive Learning based Feature Comparison with Monomodal Normal Brain Images 	"Authors checked ""Yes"" for most questions on the reproducibility of the paper"									The proposed method contains many components, which makes it harder to reproduce from the implementation side. Still, in the Reproducibility Form, the authors promise to make the code available, which may help. Additionally, no details are provided about training IntroVAE. while this is not the focus of the work, it is a key component, so, details would be needed. The authors also use an in-house dataset for part of the results, therefore, it is not possible to reproduce these results. In summary, this Reviewer considers that there are some risks, and reproducibility potential is just moderate.									All the necessary checklist for the resproducibilty are provided.																				
334	Multimodal Contrastive Learning for Prospective Personalized Estimation of CT Organ Dose 	The code is not available (although not required). The datasets are not available (although not required). Experiment parameters and configurations were detailed. Please clarify how the training and test were split.									The lack of technical description regarding TCM map generation (the DCT fitting part of the paper that I missed) is a minus. Maybe it's very common CT knowledge that I just do not possess.									The data does not seem public and no mention of sharing the code.																				
335	Multi-Modal Hypergraph Diffusion Network with Dual Prior for Alzheimer Classification 	Maybe the model and results can be implemented.									If the code is provided, the algorithm will be easier to reproduce.									The results in this paper are easily reproducible ?																				
336	Multi-Modal Masked Autoencoders for Medical Vision-and-Language Pre-Training 	It is easy to implement the idea based on the existing method description.									The reproducibility looks good.									reproducible																				
337	Multi-modal Retinal Image Registration Using a Keypoint-Based Vessel Structure Aligning Network 	The paper provides sufficient implementation details.									System is very well defined and detailed, although not publicly available. Dataset is very well defined an detailed, although not available at this time (although they indicate it will be available upon acceptance) Training and execution dataset partitions and epochs utilized is well defined.									The method is reproducible.																				
338	Multi-Modal Unsupervised Pre-Training for Surgical Operating Room Workflow Analysis 	The authors present the method in such a manner that it should be reproducible. All the details provided on the checklist were truthful.									According to the materials, the paper is reproducable.									Model parameters are provided in the supplementary document. It is however important to ensure it will be published along with the paper. The used dataset are not publicly available.																				
339	Multimodal-GuideNet: Gaze-Probe Bidirectional Guidance in Obstetric Ultrasound Scanning 	The dataset is not mentioned to be publicly available, but the methodology for data collection and for evaluation are very clearly documented.									No particular issues									The experiments design and data generation are clearly described in the paper, which matches what the authors claimed in the reproducibility checklist.																				
340	Multiple Instance Learning with Mixed Supervision in Gleason Grading 	The combination of limited pixel-level labels with super pixels might cause challenges to reproduce. Suggest the code can be provided.									likely to be reproducible									Training hyper-parameters are complete The settings of backbone and position encoding are provided. The details of masking are missing. The details of limited pixel-level labels are missing.																				
341	Multi-scale Super-resolution Magnetic Resonance Spectroscopic Imaging with Adjustable Sharpness 	Experimental data process depends on the professional tools (LCModel v6.3-1 and FSL v5.0) so that maybe missing the required details for reproducibility of the paper.									I am not so sure for the re-producibility of the paper. The code and the data are not available to aid reproducibility. However, the convergence of the proposed model is not discussed.									The paper gives a clear and detailed description to the experimental details.																				
342	Multiscale Unsupervised Retinal Edema Area Segmentation in OCT Images 	no concerns									The contributions presented in the manuscript should be possible to be reproduced by modifying the base DCCS architecture. The authors have also claimed to release the code after review.									The work might not be reproducible because the AI challenge dataset is no longer available to the public.																				
343	Multi-site Normative Modeling of Diffusion Tensor Imaging Metrics Using Hierarchical Bayesian Regression 	Sufficient details have been provided regarding the datasets. For code, it's stated that PCNtoolkit (https://github.com/amarquand/PCNtoolkit) was used which is a publicly available package, but it is unclear whether the specific codes for the data analysis in this work were part of the package or will be made publicly available to enable reproducing the results.									It should be easy for authors to provide source code for reproducibility analysis.									The reproducibility report seems to be honestly filled out.																				
344	Multi-Task Lung Nodule Detection in Chest Radiographs with a Dual Head Network 	Reproducibility of the paper is high.									Although the author does not give the source code, the network model is not complex and has certain reproducibility, but because all the experimental parameters are not fully explained, the reproduction results may be biased. Of course, it's best to hope that the author publishes the source code.									The source code will not be made available. The paper provides enough details to reproduce the paper.																				
345	Multi-task video enhancement for dental interventions 	This work is not reproducible because neither source code or dataset will be made available.									No explicit signs about whether the dataset will be public available and the code will be released or not.									No code/dataset link has been provided																				
346	Multi-TransSP: Multimodal Transformer for Survival Prediction of Nasopharyngeal Carcinoma Patients 	The paper uses in-house dataset which introduces concern on reproducibility.									The dataset demographics and non-imaging features needs clarification, otherwise it is reproducible.									The experimental details are sufficiently provided in the text, and the authors indicate that they will make the corresponding code available. However, they will not make the in-house dataset used in this study publicly available.																				
347	Multi-view Local Co-occurrence and Global Consistency Learning Improve Mammogram Classification Generalisation 	Okay.									The authors declined to provide the code and the value for hyperparameters is not mentioned in the paper so the result is not reproducible.									The detailed structure of each component is required for reproduction. Two private datasets are used in the experiments.																				
348	MUSCLE: Multi-task Self-supervised Continual Learning to Pre-train Deep Models for X-ray Images of Multiple Body Parts 	There are lots of detailed information that is neccesary to be realized for this work, because they employed three learning schemes for one well-testified task.									The reproducibility of this paper is good, the algorithm is relatively easy to implement.									The paper has descriptions on some of the hyperparameters (learning rate schedule, weight decay, DNN architectures) and training methodology that could be useful in reproducing the results.																				
349	NAF: Neural Attenuation Fields for Sparse-View CBCT Reconstruction 	The method should be very well reproducible with the provided information and the code is promised to be released.									The training of the hash encoder is not clear. Is the hash function trained jointly with the fully-connected deep neural network? The setting of the sampling quantity is not clear.									The authors followed the ethical rules.																				
350	NerveFormer: A Cross-Sample Aggregation Network for Corneal Nerve Segmentation 	The explanation gives an ide of implementation. Still, it would be difficult, if not impossible, to fully reproduce this work - there are many implementations choices not covered.									Based on the description in the paper, it is difficult to reproduce the exact result due to missing details. No code is publicly available.									The detailed network architecture is not included in the paper. But the authors commit to releasing the code in the reproducibility checklist.																				
351	NestedFormer: Nested Modality-Aware Transformer for Brain Tumor Segmentation 	The work is reproducible									The method proposes quite a few unique blocks in Nestedformer, making it hard to implement. The author could consider to provide the model source code to help reproduce the results.									If one reads this paper carefully, it is possible to implement NestedFormer. It is possible to perform experiments with BratS2020, but not with MeniSeg.																				
352	Neural Annotation Refinement: Development of a New 3D Dataset for Adrenal Gland Analysis 	The authors don't have the central tendency & variation as they claimed.									all codes are available.									Code is attached in the supplementrary materials although I personally did not run it. The authors promise to release the dataset too.																				
353	Neural Rendering for Stereo 3D Reconstruction of Deformable Tissues in Robotic Surgery 	It is not clear whether the data will be made public.									It is easy to reproduce the work of this paper with released data and code.									The code will be available which is great. Thanks to the authors.																				
354	Neuro-RDM: An Explainable Neural Network Landscape of Reaction-Diffusion Model for Cognitive Task Recognition 	Reproducibility statement is acceptable.									Very high. As aforementioned, with a little more description in the generation of the model forward, I think one should not have problems to replicate the experiments and reproduce the results									Upon reading the paper and the checklist, I found several inconsistencies. For example, I found no description of the parameter settings, architecture, range of parameter values, sensitivity to parameters, memory footprint or compute software in the paper.																				
355	Noise transfer for unsupervised domain adaptation of retinal OCT images 	The SVDNA transfer method (Algorithm 1) is fairly straightforward.									Code will be available upon acceptance Public datasets/benchmarks are used									The proposed method are described in details and the full code implementation will be made available, which would make it easy to reproduce the methods.																				
356	Noise2SR: Learning to Denoise from Super-Resolved Single Noisy Fluorescence Image 	The author provides the code and all the information necessary to reproduce the results.									Yes. The paper provides enough details and the algorithm itself is easy to implement.									The authors have provided a link to the dataset and have agreed to provide train-test dataset splits.																				
357	Non-iterative Coarse-to-fine Registration based on Single-pass Deep Cumulative Learning 	It is very nice that the code will be available and the dataset is public.									Authors will submit codes on Github.									The paper meets the standard requirement in terms of reproducibility.																				
358	Nonlinear Conditional Time-varying Granger Causality of Task fMRI via Deep Stacking Networks and Adaptive Convolutional Kernels 	The synthetic dataset is probably reproducible, the other dataset will be probably distribute after acceptance									Implementing the network proposed in this work and performing the reproducible study is feasible (although complex) given the Fig. 2 and section 2.1. It will be better if the author can provide the source code for generating the synthetic data as well for further reproducible experiments.									Code are not included, but there are sufficient details for reproducing the results.																				
359	Nonlinear Regression of Remaining Surgical Duration via Bayesian LSTM-based Deep Negative Correlation Learning 	This work was implemented on the public dataset cataract-101 and a reference implementation model is provided. Network settings and hyperparameters are detailed in the text to ensure good reproducibility.									The method description is mostly very clear and detailed. The authors also promise to publish their code. Some details could be clarified: What kind of statistical test was used to obtain the p-values? Was 6-fold cross-validation performed like in CataNet or were all 81 videos used for training? In the former case, what validation metric was used to select the best model after training (e.g. MAE-ALL or loss)? In the latter case, how often were experiments repeated? How were the MAE scores computed from multiple models (either through cross-val or repeated experiments)? Are predictions from multiple models averaged to compute one score or are multiple scores computed and then averaged? It appears that the authors report the mean and standard deviation over surgeries but this is not specified in the paper. I am not sure what exactly the ablation 'w/o DNCL' means. Is simply the loss term from eq. 3 removed or is it a non-ensemble variant with only one LSTM prediction head?									The experiments are conducted on public dataset. The code will be released.																				
360	NVUM: Non-Volatile Unbiased Memory for Robust Medical Image Classification 	The method is relatively simple and the implemental details are clear. So i think this paper has good reproducibility.									The implementation is enough for the reproducibility of the paper.									As the author indicates in Abstract, the code will be made available. In addition, the training hyper-parameters and preprocessing method are provided in paper. The datasets and model backbone are all open accessed. Thus, the work should be reproducible.																				
361	On Surgical Planning of Percutaneous Nephrolithotomy with Patient-Specific CTRs 	The paper describes the generation of point clouds using Slicer3D. More detail would help with respect to filters and marching operations used to segment the CT data. Answers to reproducibility checklist make sense and the authors do a good job explaining the algorithm and tools they used.									The optimization problem and the libraries used to solve them are clearly described. The actual values used for the optimization problem are missing (every variable in (4) should be defined as a real number) as are any hyperparameters such as stopping criteria. The small patient sample limits the reproducibility of the results.									he dataset is very small, but for primary results it's ok																				
362	On the Dataset Quality Control for Image Registration Evaluation 	The paper uses publicly available data sets. The authors do not provide the code required to construct the variograms, however their description of the algorithm used is sufficient for this.  I am not confident of the reproducibility of the remainder of the classification process. As described it sounds like it may be quite operator dependent. I think the authors need to better formalise this part of the algorithm.									Data will be reproducible.									The method is clearly detailed and could be easily reproduced. Providing the code would be a plus, so that future dataset providers could use it while checking their annotations.																				
363	On the Uncertain Single-View Depths in Colonoscopies 	The paper lacks reproducibility but the novelties are well discusses and can be implemented by the reader. The paper could contain an appendix with details on their architecture and training methods. Metrics not well explained. It is a shame that a lot of the points on the given reproducibility list are not provided (the authors are honest and open about it though. The paper could be a lot better on this front by including these details. I have the strong feeling that the page limit has had a strong negative impact on the complete description of this interesting work.									Methods applied the paper are well referenced.									Reproducibility needs improvement. Model training details like hyperparameters used are not present in the current manuscript.																				
364	One-Shot Segmentation of Novel White Matter Tracts via Extensive Data Augmentation 	The paper has some descriptions of the methodology with provided equations, which should help the readers to understand the main idea of the work and reproduce its algorithm. Though it is still suggested for the authors to provide the source code online, which can be used to fully validate the proposed method and understand its mechanism.									The authors use open available data of the HPC. The authors use TractSeg as backbone, for which code is available. No code is provided in for the framework their paper is based on, nor in their paper. However, mathematical definitions are provided and based on the checklist the reproducibility seems to be given.									good																				
365	Online Easy Example Mining for Weakly-supervised Gland Segmentation from Histology Images 	Reproducibility is fine.									Reproduction is difficult because the method description is not clear enough.									Descriptions of the method are sound. Authors will provide code upon acceptance.																				
366	Online Reflective Learning for Robust Medical Image Segmentation 	I have the concern on the statistical significance analysis. I suggest the authors to address it.									Overall the method is well explained, but several components are not clear.  Please see below.									Reproducible if the code is released.																				
367	OnlyCaps-Net, a capsule only based neural network for 2D and 3D semantic segmentation 	Work can be reproducible.									This paper seems reproducible since the formulas are given (even in not always very clear to me).									The authot won't release any code or dataset																				
368	Only-Train-Once MR Fingerprinting for Magnetization Transfer Contrast Quantification 	Negative for the reproducibility of the paper, because they did not share their codes.									The reproducibility metrics have been met									The proposed method is well described though the code is not made available																				
369	Opinions Vary? Diagnosis First! 	Reproducibility is quite well covered in terms of method explanation, data and implementation details.									developed codes are not shared. data is publicly available									The code may reproducible based on the paper descriptions.																				
370	Opportunistic Incidence Prediction of Multiple Chronic Diseases from Abdominal CT Imaging Using Multi-Task Learning 	This paper will only be reproducible if the database is made publicly available. Otherwise, it is straightforward to implement.									The code of this work was not provided and the detail of proposed network have not been explained clearly. The reproducibility is slightly worse.									I think the details wrt the methodology is there but I think the actual data utilised after slice selections is an important aspect to share as often the methods are clear but when one tries to replicate the data ithe outcomes may create vastly different results.																				
371	Optimal MRI Undersampling Patterns for Pathology Localization 	Please provide the code.									The authors have presented their developed tool as supplementary material, making their approach and analysis reproducible.									The optimization implementation details are needed for reproducibility. The hyper parameters are given in the supplementary files.																				
372	Optimal Transport based Ordinal Pattern Tree Kernel for Brain Disease Diagnosis 	The paper is well explained and algorithms are detailed in a way that favours reproducibility									The reproducibility is okay. However, a more detailed description of OT will help further improve it.									No code is provided.																				
373	ORF-Net: Deep Omni-supervised Rib Fracture Detection from Chest CT Scans 	Positive if code and the private dataset is released.									Enough information to reproduce.									With more details about the proposed method, the paper should be fairly easy to reproduce.																				
374	Orientation-guided Graph Convolutional Network for Bone Surface Segmentation 	Although the authors indicated that the data and code would be available publicly, no information is provided in the paper. In other words, the paper does not indicate when and how the code or data will be made available.									The authors provide details of the method used, however it is not clear if the data itself is sharable.									The paper has a reasonable level of repeatability, providing details about the hyperparameters and systems used. The authors also indicate that code will be made available. The dataset appears to be acquired internally but the authors indicate in the reproducibility checklist that the dataset could be accessed by others.																				
375	Orientation-Shared Convolution Representation for CT Metal Artifact Learning 	The authors will release the code and the trained model after acceptance, which is very helpful for the community.									Good									The paper will not be reproducible without code. The description lacks to many details. However, the authors promise to release their code which would render this point moot. If the code is published the method should be well reproducible.																				
376	Out-of-Distribution Detection for Long-tailed and Fine-grained Skin Lesion Images 	The used framework is not mentioned.  The imaging modality for the inhouse dataset is not mentioned.  Sensitivity regarding hyperparameters is missing									"Regarding #1 in weaknesses, please ellaborate more on the prototype learning part. What is the standard prototype loss? How do you know/compute the class specific prototypes {p_i,p_j}? Why does the authors think it will help detecting the fine-grained samples in the dataset? Regarding #2 in weaknesses, please provide more details on the confidence score computing. Is that the predicted softmax probabilities? It would be interesting and more convincing to readers to see statistcal significance of the reported results. I guess the reported results on the in-house dataset is on the test set, but on ISIC2019 the authors report results on the validation set (cross-validation). That needs to be mentioned in the text, e.g. in section 3.1. ""It can further be noted that our proposed approach of M-T mixup (MX5) strategy combined with prototype learning performs the best for OOD detection of all different categories while maintaining or slightly improving the overall ID performance compared to the baseline on both the datasets."" I disagree, the proposed strategy gives the best for OOD detection, but it does not improve the overall ID performance compared to other methods, as seen from table 2. For example, ARPL is consistently better than the proposed method on the in-house dataset. This is okay as the method is primarily an OOD method, but the sentence above needs to be adjusted. Table 2: please either make all best scores in bold or unbold all numbers. For example, in the pre and rec columns for the in-house dataset, there are no bold values, unlike the rest of the table. What are the values of the different lambdas used in your experiments, e.g. in eq. 2, eq. 3, lambda_mse, and lambdas in fig. 2? You might mention only the values used for the best performing method (MX5) in the main text and move the rest to an appendix. You might also want to use a better notation for clarity, rather than refering to all of them as lambda. Minor:  8.1: ""... verified dermoscopic images categorized amongst 65 different ..."": might be better to say categorized into  8.2: ""... This simple technique has shown to increase ..."": has been shown  8.3: ""... thus limiting it's advantages ..."": its  8.4: ""... which are the standard parameters for measuring ..."": these are metrics not parameters"									The authors give some details on the used private dataset and also describe how they used the ISIC dataset. The authors provide reasonable amount of information on the training and implementation, as well as on the metrics being used.																				
377	Overlooked Trustworthiness of Saliency Maps 	ok.									Experiments were done with a public dataset (CheXpert). Code will be made available. Reproducibility is guaranteed.									The authors provide enough information for reproducing the reported results.																				
378	Parameter-free latent space transformer for zero-shot bidirectional cross-modality liver segmentation 	The reproducibility of the paper is adequate with open datasets. Code / models do not appear to be shared.									The idea is clear. it is easy to be reproduced.									This paper uses the LST method proposed by itself and a common backbone to form the model architecture. The dataset adopts the public dataset. For the LST method proposed by the author, the specific calculation formula and brief derivation process are given in the paper, which is less difficult to reproduce. On the whole, it has high reproducibility.																				
379	Patcher: Patch Transformers with Mixture of Experts for Precise Medical Image Segmentation 	The authors have claimed that they will release code, trained models, and results.									All the code and models will be released.									The authors will release the code upon paper acceptance. The implementation details in the paper could be enough to reproduce the network architecture. The stroke lesion dataset is private, making future result-proofing and comparison impossible in that dataset.																				
380	Patch-wise Deep Metric Learning for Unsupervised Low-Dose CT Denoising 	Good.									The approach is quite difficult to reproduce. On the one hand, it's not clear to know about the specific setting of the forward processing of the generative adversarial network, such as the convolution blocks. On the other hand, the description of the datasets and the experiment implementation is insufficient. Especially for the AAPM challenge, even the quantity and the division of the dataset are not given.									Reproducibility is sufficient. All implementation and dataset details are provided.																				
381	PD-DWI: Predicting response to neoadjuvant chemotherapy in invasive breast cancer with Physiologically-Decomposed Diffusion-Weighted MRI machine-learning model 	The dataset (part of a challenge) and experiments are very clearly described. Code will be made publicly available.									No issues.									The data was from BMMR2 challenge (https://wiki.cancerimagingarchive.net/pages/viewpage.action?pageId=89096426), and the authors would provide the code and trained models upon acceptance. I think the reproducibility of the paper was good.																				
382	Personalized Diagnostic Tool for Thyroid Cancer Classification using Multi-view Ultrasound 	The implementation steps of the three main contributions are very well explained. An in-house dataset is used for the evaluation. There is no information regarding the data acquisition process.									The description of the data collection process is not completed, such as descriptions of the experimental setup, device(s) used, image acquisition parameters, subjects/objects involved, instructions to annotators, and methods for quality control. Were the 4529 sets of multiview US images collected from 4529 patients or not?									The results in this paper are easily reproducible.																				
383	Personalized dMRI Harmonization on Cortical Surface 	I don't see any limitations on the reproducibility.									Study can be reproduced									Not reproducible - although publicly available datasets are used, implementation code has not been made available																				
384	PET denoising and uncertainty estimation based on NVAE model using quantile regression loss 	"The authors ticked ""Yes"" for most items of the reproductibility checklist, which is sometimes in contradiction with what is provided in the paper (e.g. range of hyper-parameters considered). It seems that the code will be made available."									The authors did not provide their statement on code availability. This would be highly desirable.									ok																				
385	PHTrans: Parallelly Aggregating Global and Local Representations for Medical Image Segmentation 	I think the paper is reproducible.									I think some numerical results of baselines are not consistent with other one. For example, in the same dataset of BCV dataset, nnUNet (most important medical image segmentation model) from UNETR(https://arxiv.org/pdf/2103.10504.pdf, table 1) is 88.8, while in this paper it is only 87.75 and PHTrans is only 88.55. I have added my comments below if the train&val dataset are not split identically.									implementation details are provided, code is not submitted																				
386	Physically Inspired Constraint for Unsupervised Regularized Ultrasound Elastography 	Reproducibility is adequate from the clear mathematical derivation.									OK									The code will be publicly available after the paper acceptance. Hence, the reproducibility seems fine.																				
387	Physiological Model based Deep Learning Framework for Cardiac TMP Recovery 	Data can be reproduced from publicly available sources, although the specific data for training is not specified.  Little detail is provided in the generation of the data. Source  code for the method is not provided. Not enough information is provided with regards to  validation.									The paper lacks in sharing details about implementation and network architecture and it's not open source. It uses a software to generate simulated data but generation parameters are not shared and the generated data is not shared. This will likely make it impossible to reproduce the results.									Authors will make the code available.																				
388	Physiology-based simulation of the retinal vasculature enables annotation-free segmentation of OCT angiographs 	"Despite github links have been provided for third party implementation of the networks and experimental settings, these details are not described in the paper. This is important, as repositories evolve, while reported results are fixed to a given version. Should the authors cite the paper (i.e. [16]), and ensure that the repository version used is preserving the settings reported, and otherwise tell the difference. Moreover, the cited paper/repository from Ma et al. IEEE TMI:40(3) 2020 [16], reports a fixed number of epochs with exponential learning rate decay (which requires a fixed number of epochs). Instead, the authors report that they ""introduce a validation split for model selection"", which indicates the use of an stopping criterion that is not specified. This is a relevant detail for reproducibility that is not sufficiently described."									The notes on reproducibility seems satisfactory									It is still necessary to further explain the implementation details, such as the selection of three-dimensional deformation-related parameters, node positioning when simulating a blood vessel tree, etc., to ensure the repeatable implementation of the method.																				
389	Point Beyond Class: A Benchmark for Weakly Semi-Supervised Abnormality Localization in Chest X-Rays 	The author did not mention the code in the paper, but according to the checklist, it seems they will release the code after acceptation. Please make clear. I suggest that at least release the splitting of the training set and the test set, which may help in pursuing the goal of setting a benchmark for the field.									Although the dataset is generally available, I do not see the code for replicting it. The result may be reproducible.									Although the code is not made public, the data used is publicly available. The author's new method is also clearly explained and the parameters for each experiment / augmentation are given. So while these exact results are not reproducible based the information present in the paper, I would expect that similar results could be reproduced when implementing from scratch.																				
390	Poisson2Sparse: Self-Supervised Poisson Denoising From a Single Image 	Convincing									The reproducibility is good. Datasets and code are available. Only possibility for major improvement is the inclusion of the pre-trained model.									Good reproducibility																				
391	Pose-based Tremor Classification for Parkinson's Disease Diagnosis from Video 	Its reproducibility is very high. The method and parameter explanations are excellent, and the data-related parts and learning environment are also well explained. I look forward to the code release of this paper.									It is very difficult to reproduce the paper and achieved results from description of the manuscript. The authors use a subset and crop the videos but any of this information is reported.									According to the authors, the code will be made public and the dataset is publicly available.																				
392	Position-prior Clustering-based Self-attention Module for Knee Cartilage Segmentation 	The reproducibility of the PCAM is not difficult.									The code is not provided and the reproducibility is questionable.									Please include a link to the code within the paper.																				
393	Predicting molecular traits from tissue morphology through self-interactive multi-instance learning 	"On page 6, it states that ""more details are available in the (temporarily anonymous) source code"", but its URL is missing."									Good.									Publicly available datasets. Code release promise. Clear description of they utilized hyper-parameters.																				
394	Predicting Spatio-Temporal Human Brain Response Using fMRI 	The reproducibility of the paper can be improved after the author completing the missing technical details.									Reasonable									I think this work is reproducible.																				
395	Privacy Preserving Image Registration 	Good enough, a bit more details about the software implementation framework & programming languages could be mentioned.									The reproducibility should be good.									The authors have indicated the source code will not be available (not aplicable). I am not very sure regarding the reproducibility of the paper.																				
396	ProCo: Prototype-aware Contrastive Learning for Long-tailed Medical Image Classification 	The overall learning scheme is clear but it is hard to reproduce, and the review highly suggest the authors release their codes.									I think the paper could be reproduced but I have no access to source code.									The proposed method is evaluated using two publicly available datasets, and most of the hyperparameters are included. Authors claimed in the Reproducibility Response that they will release code once the paper is accepted, but no reference to that is included in the text.																				
397	Prognostic Imaging Biomarker Discovery in Survival Analysis for Idiopathic Pulmonary Fibrosis 	very good									The authors claim that the code will be released with publication. I thus believe the work to be reproducible.									Reproducibility seems fair. Datasets used in the study seem to be not publicly available.																				
398	Progression models for imaging data with Longitudinal Variational Auto Encoders 	The authors used public datasets in the experiment, and they claimed that the code will be made publicly available upon acceptance of the paper.									The datasets used for evaluation are publicly available, although the exact images used for training and testing are not provided. The authors mentioned that the codes will be available upon acceptance. Otherwise, I would not feel able to reproduce the paper from scratch with the information provided in the manuscript. The parameter values were provided. For the evaluation, the authors provided a clear description of metrics. Full memory footprint was not provided, although the size of the 3D images (80x96x80) makes me guess that the method is expensive.									The reader probably has difficulty to reproduce this work, releasing the source code could be a big help on the reproducibility of this work.																				
399	Progressive Deep Segmentation of Coronary Artery via Hierarchical Topology Learning 	Reproducibility is OK. The authors will release codes and models.									The reproducibility of this paper is good, the framework described in this paper is very detailed and the algorithm is also very clear.									Although the code is not public, the authors list most of the details of the method in the paper. It should be straightforward to reproduce the results.																				
400	Progressive Subsampling for Oversampled Data - Application to Quantitative MRI 	"Run times. Run times are provided to some degree in a text file to the code, but I would expect a different format for the reporting: There run times are reported as time for a single cross-validation split, but with ""NAS"" (or hyperparameter optimization) employed, instead the TOTAL GPU hours/days should be reported, because that is what is required to achieve those results. I am fine with estimates, but this is insufficient. ""MRI signal prediction MSE"" is missing a definition or a reference to a definition that is very clear on what is compared."									Excellent. The authors even went to the lengths of making their code available to reviewers in a way respecting anonymity. Data is also available and the description of the method is detailed.									The reproducibility criteria has been met																				
401	Prostate Cancer Histology Synthesis using StyleGAN Latent Space Annotation 	The authors provide some details of the training process; StyleGAN2 code is public available; however the training dataset seems un-available to the public.									Without the code and validation on public data (PANDA dataset) it'd be hard to validate the results. Also, some technical details  are missing, for example, how GAN was trained?									Would be nice if the trained model could be made available if possible.																				
402	PRO-TIP: Phantom for RObust automatic ultrasound calibration by TIP detection 	Satisfactory									The phantom CAD model and machine learning model are freely available online. They mentioned a reference implementation that is also are available on github.									This paper demonstrates a strong commitment to reproducibility by making the 3D printing plans and code freely available; however, key implementation details, such as hyperparameters and details of the architectures and training performed are missing from the paper itself.																				
403	Prototype Learning of Inter-network Connectivity for ASD Diagnosis and Personalized Analysis 	The code is released, no reproducibility concerns. All good.									The authors have provided an anonymized link to the code base. In my opinion, this is a point in favor of the reproducibility. However, the range of hyper-parameters considered, method to select the best hyper-parameter configuration in the paper is still ambiguous									Reproducibility is fairly good - based on the checklist/submission it appears code will be shared with acceptance. However, there is some missing experimental analysis (e.g. parameter sensitivity, statistical significance, when does method fail) that could be added to improve the paper.																				
404	Pseudo Bias-Balanced Learning for Debiased Chest X-ray Classification 	not easily reproducible									All data and codes will be open-source									The authors provide all the source code in the checklist and supplemental materials. And they use public dataset which is beneficial for reproducing the work.																				
405	Radiological Reports Improve Pre-Training for Localized Imaging Tasks on Chest X-Rays 	The authors employ well-known datasets and architectures. It seems that the paper is reproducible though the authors do not provide code.									All the methods and datasets are public in this paper. I think it can be reproduced since the existing methods have open-source codes.									The hyperparameters of the experiments are described in detail, which are sufficient to reproduce the results.																				
406	RandStainNA: Learning Stain-Agnostic Features from Histology Slides by Bridging Stain Augmentation and Normalization 	The datasets are available, but the code has not been released. The method is simple so it should be easy to reproduce. There is no information about hyperparameters such as learning rate used in the experiments or how many epochs were used, limiting the reproducibility.									it seems that method is reproducible									Good.																				
407	Real-Time 3D Reconstruction of Human Vocal Folds via High-Speed Laser-Endoscopy 	The method is reproducible with some effrot.									"Appears to be satisfactory. However, given the statement in the manuscript that ""we publish a dataset containing laser-endoscopy videos of 10 healthy subjects that can be used to drive further research in this area"", the reference (or web link) to this publication will to be provided in case the manuscript is accepted."									Adequate details of the algorithms are provided with references where further details can be found. In addition, the source-code and the dataset will be made publicly available upon acceptance. No information on the sensitivity of the parameters used in the reconstruction method is reported. Several important implementation details are missing as well.																				
408	Recurrent Implicit Neural Graph for Deformable Tracking in Endoscopic Videos 	Good. The code is even already available in a github repo.									The major components are explained, but reproducing this work without access to the code would be extremely difficult. The system is so complex that it is difficult to describe in such a short paper. Even so, the authors do make a strong effort to describe as much of the system as possible, and many parameters and setup details are given.									Thanks to its detailed explanations on the method, it seems reproducible.																				
409	Reducing Positional Variance in Cross-sectional Abdominal CT Slices with Deep Conditional Generative Models 	The author says the code will be published upon the acceptance of the paper. Since this is a pioneer work on the new application. Publishing code is important for other researchers to follow.									The paper seems reproducible.									Work can be reproducible.																				
410	RefineNet: An Automated Framework to Generate Task and Subject-Specific Brain Parcellations for Resting-State fMRI Analysis 	The reproducibility of the paper seems not very high, the RefineNet  architecture is not very detailed and clear.									no concern									Good.																				
411	Region Proposal Rectification Towards Robust Instance Segmentation of Biological Images 	This paper is reproducible based on the detailed descriptions of the proposed method.									The key idea of this work is simple. Though no code is released, it wouldn't be difficult to reproduce the results on the public datasets.									Sufficient detail provided on the module and the experiments to evaluate the approach.																				
412	Region-guided CycleGANs for Stain Transfer in Whole Slide Images 	The authors claim the reproducibility of the results reported in the paper.									Reproducibility response is followed in the paper.									The core idea of the proposed method is straightforward and can be easily implemented. The implementation details of the CycleGAN are sufficiently discussed. The missing part is the datasets. Although this paper used a public dataset, to train the model it still requires the private IHC images.																				
413	Regression Metric Loss: Learning a Semantic Representation Space for Medical Images 	The paper looks reproducible. The hyper-parameters are provided in experiments. Analysis and ablation study is provided in the paper. The DNN architectures used in the paper are mentioned and cited.									The source code will be released on GitHub.									There is no detailed hyperparameter settings and the version of GPUs.																				
414	Reinforcement Learning Driven Intra-modal and Inter-modal Representation Learning for 3D Medical Image Classification 	The method is generic mathamatically but the reproducibility is not depicted in the paper. With different dataset and task other challenges may show up which makes it hard to believe that the method is easily reproducible.									The authors will open source the code. It's reproducible.									The dataset is public. The authors mention in reproducibility statement that they will release code and trained models after acceptance																				
415	Reinforcement learning for active modality selection during diagnosis 	Code and experimental settings provided - very good									The authors have provided implementation details with supplementary code. The evaluation is performed on a public dataset and a private dataset. The authors are encouraged to make their data and code available.									No major concern regarding reproducibility.																				
416	Reliability of quantification estimates in MR Spectroscopy: CNNs vs. traditional model fitting 	Code and data is not included, but the paper is reasonably detailed and relies on simulations, so it should be possible to approximately replicate the study.									Seems fine.									I am not sure how difficult it would be to reproduce the synthetic dataset used for training; however, the training of the networks (given the data) seems to have been described sufficiently in detail for reproducibility (although the learning rate is not specified, I assume this means ADAM with the default learning rate).																				
417	Reliability-aware Contrastive Self-ensembling for Semi-supervised Medical Image Classification 	Author provided code for reproducing the results									The reproducibility of the method is good, since the author provide us with almost all implementation details and the code is also released on the Github.									The experimental setup such as data pre-processing and learning hyper-parameters are provided in detail. These descriptions are sufficient to reproduce the reported results.																				
418	ReMix: A General and Efficient Framework for Multiple Instance Learning based Whole Slide Image Classification 	"Authors checked ""Yes"" for most questions on the reproducibility of the paper"									The authors claimed the code will be made available.									Hyper-parameters are complete. Authors are going to release the code.																				
419	RemixFormer: A Transformer Model for Precision Skin Tumor Differential Diagnosis via Multi-modal Imaging and Non-imaging Data 	"However, there are some places that are not clearly expressed. In Section 2.2, ""When DWP is turned on (based on p > Tp, p  [0,1])"", what does p mean here? How is the p obtained? What are the global features and local features? In fig. 2, what does patch token mean? It's never shown in the main text. Section 2.3 is very confusing.  For example, the authors mentioned gc or gd are generated by GAP layer, gm is generated by LN layer. But in the formulas below, it's zx and gx' that are generated by LN and GAP. I don't know if the gx' here represents the same gc, gd, gm mentioned before, or it means something else.  Also, I suggest to add the notations lc, ld, and lm(if there is one) to the figure 2."									The model looks computationally expensive, hence technically it should be reproducable. This claim is supported by experiments on two dataset for one application. Being said that, it is also the model seems computationally expensive.									It is very difficult to reproduce the paper as the abbreviations in the flow chart, and detailed parameter settings of the architecutre are now disclosed in full detail. It is suggest to release the code, at least the model architecture part.																				
420	Removal of Confounders via Invariant Risk Minimization for Medical Diagnosis 	In the manuscript, the Authors conduct experiments on a publicly available dataset and they claim to publish their code, at a later stage. The procedures explained in the manuscript looks sufficiently detailed to attempt the reproduction of the results presented in the article. Unfortunately, the reproducibility statement given by the Authors look vastly incomplete.									Good. The author will open source the codes for research purposes.									The authors have agreed to make code publicly available after the review period. The data used in the experiments is from publicly available datasets.																				
421	RepsNet: Combining Vision with Language for Automated Medical Reports 	Reproducible with some efforts.									It appears that authors meet the requirements for the reproducibility.									Is the reported CLEF (Abacha 2019) results of Tab.1 evaluated on the test set of  Med-rad 2019? Are the author's results evaluated on the validation set of  Med-rad 2019?																				
422	Residual Wavelon Convolutional Networks for Characterization of Disease Response on MRI 	The data sets seem to be private, but the main algorithmic concepts of paper are described in sufficient detail that a reader has a reasonable chance of implementing the framework and testing it on their own data.									Easy to reproduce.									Need of source code url link.																				
423	Rethinking Breast Lesion Segmentation in Ultrasound: A New Video Dataset and A Baseline Network 	The authos will release the dataset and the code.									Author will provide the code and the results seem reproducible.									Most technical details are well provided. I believe that the model can easily be implemented. However, more training details should be given like the optimizer used, and the learning rate scheduler.																				
424	Rethinking Surgical Captioning: End-to-End Window-Based MLP Transformer Using Patches 	The authors included their source code in supplementary, and with clear specification in their manuscript, I do not doubt its reproducibility.									"Reproducibility Response is set to ""Yes"" for every question."									The paper is able to reproduce and the dataset for training and testing are public.																				
425	Rethinking Surgical Instrument Segmentation: A Background Image Can Be All You Need 	likely reproducible									The authors have provided the code and the approach can be reproduced.									The paper is conceptually highly reproducible, regardless if code/data is provided or not. The authors should be commended for this as it means it can be applied more broadly by the community and is not dependent on their particular implementation.																				
426	Retrieval of surgical phase transitions using reinforcement learning 	The reproducibility of the paper looks fine.									The paper provides enough information for this.									"For the most part, the method and training procedure is described in detail. The authors also indicate that they intend to publish their code. Some details could be clarified: How many epochs and episodes were used for the ResNet backbone and the DQN respectively. The authors say they used a validation set for model selection. Which metric was used here? There are some open questions regarding the design of the method which are elaborated in more detail in section 8 (main weakness 2) How exactly are the metrics computed? E.g. how are NaN values in precision and recall handled if a phase is not predicted or does not occur? Were the metrics with relaxed boundaries used like in Trans-SVNet and other previous work (e.g. TMRNet, MTRCNet-CL, SV-RCNet)? Other metrics are fine but it should be made clear how they were computed. Were experiments repeated? This is not clear since the standard deviation is computed over videos. If they were repeated, how were scores computed? Are predictions first averaged to compute one score or are scores computed for each prediction and then averaged? For the RMI approach, the authors state that the indices of all possible transitions are averaged to initialize the agents. How are ""possible transitions"" defined? The authors state they used window sizes of 21 and 41. Is this L? Or is it the complete receptive field (i.e. window size of 21 = 2*L+1 with two search windows of L=10 plus the center frame)? Section 2.1, however, states that the state of each agent consists of 2L features, not 2L+1."																				
427	Revealing Continuous Brain Dynamical Organization with Multimodal Graph Transformer 	The reproducibility of the study can be improved by providing more technical details.									The reproducibility is acceptable.									I think the paper is not reproducible based on the current submission because the method section is vague and too many details are missing.																				
428	Rib Suppression in Digital Chest Tomosynthesis 	Good									Without the network and training specifics, the work is not straightforward to reproduce.									The reproducibility is feasible so that a skillful graduate student can replicate it.																				
429	Robust Segmentation of Brain MRI in the Wild with Hierarchical CNNs and no Retraining 	It looks like the authors are planning to release the code and trained models once accepted. This will help reproduce the results and help improve the model toward next frontiers.									The code is not available yet but the authors guarantee that they will provide it if the paper is accepted. In that case I do not have other way to proof the reproducibility of the method without the database and code.									The reproducibility of this method is a challenge since there are several ambiguous aspects such as some details of S2 and the parameter used to generate the low-resolution image.																				
430	RPLHR-CT Dataset and Transformer Baseline for Volumetric Super-Resolution from CT Scans 	Data and code are available at https://github.com/*.									Looks OK. The code is submitted but I did not launch it.									Good reproducibility																				
431	RT-DNAS: Real-time Constrained Differentiable Neural Architecture Search for 3D Cardiac Cine MRI Segmentation 	Satisfactory, if the code and extended data will be made public.									The paper provides some details but there are still things which are not described sufficiently, probably not possible in the allowed page limit, so reproducing the paper is probably going to be challenging.  If possible, it would be nice to publish the code.									Ok. Authors mention in the reproducibility checklist that the code will be available.																				
432	RTN: Reinforced Transformer Network for Coronary CT Angiography Vessel-level Image Quality Assessment 	Challenging: no obvious access to the data and the implementation									The paper describes the method clearly, and listed their training details such as: input size, batch-size, training epochs, loss function and evaluation metrics, so the readers should be able to reproduce the paper.									positive																				
433	S3R: Self-supervised Spectral Regression for Hyperspectral Histopathology Image Classification 	The obtained results can, in principle, be reproduced if given access to the missing resources (code, data).									The authors have provided sufficient implementation details for reproducibility of the paper.									Regarding the reproducibility of this work, the description in the method section is sufficient to understand. However, some of the detailed implementations are missing for reproduction.																				
434	S5CL: Unifying Fully-Supervised, Self-Supervised, and Semi-Supervised Learning Through Hierarchical Contrastive Learning 	Authors provide enough information on method details and experimental settings.									Good. All methods used for the proposed framework are depicted clearly and noted with appropriate references.									Code release promise after acceptance. Public datasets utilized																				
435	Sample hardness based gradient loss for long-tailed cervical cell detection 	The dataset didn't go public, but the implementation details of the main detector are described, so it would be reproduced should the dataset be accessible. The details of other detectors (FCOS, ATSS, YOLOF) are not provided.									The experimental dataset is not public available neither with the code. It would be preferable if the author can publish their dataset and the code upon the acceptance of the paper.									1.The codebase of this paper is based on mmdetection. So, the code is well-structured and the results can be easily reproduced; 2.The collection of long-tailed dataset used by this paper is time-consuming. As far as I know, there is not a public dataset about long-tailed cell detection. However, the experiment results are totally dependent on the private dataset. If this dataset will not be open-sourced, the reproducibility of this paper will be greatly reduced.																				
436	SAPJNet: Sequence-Adaptive Prototype-Joint Network for Small Sample Multi-Sequence MRI Diagnosis 	"this paper proposed to use Transformer and additive-angular-margin loss for small sample multi-sequence MR image classification. However, this paper has several major flaws and fails to illustrate their point: The first component, Transformer model, is claimed to filter intra-sequence features and aggregate inter-sequence features, based on attention mechanism. But I didn't see any details about how to achieve these 2 goals in section 2.1. The overall writing quality is poor and difficult to follow. Similarly, neither the section 2.2, prototype optimization strategy, illustrates how to approximate the intra-class prototype and alienate the inter-class prototype. For example, the query sample q^a and support sample s^b corresponding to which modalities, the loss_2 should be explicitly expressed like loss_1, and why you choose the additive-angular-margin loss instead of the ordinary cross-entropy loss for classification. Fig 2 is very ambiguous and lacks sufficient explanation. E.g., what are the vectors besides the local significance block, how to aggregate into global correlation (through concatenation, pooling, or others), where does the support prototype come from, where are the 2 stages in Prototype optimization strategy, and how to approximate intra-class prototypes and alienate inter-class prototypes. The experiment section lacks the explanation of 3 modalities of data (SAX, LAX, and LGE). Some minor issues: a. P2, 1st paragraph, the hyperparameter p's explanation is ambiguous. b. Section 2.2 mentions robust classification. So what kind of attack methods you are dealing with? c. Section 2.2 2nd paragraph says ""two outputs of the SAT ..."" what does the two outputs referring to? d. Still in Section 2.2 2nd paragraph, ""The former is reserved for ..."". After this, where is the latter one? What's the purposes of two stages here? e. Table 3 didn't explain what's VOT."									No mentioning about it, except listing of sources for the comparison methods									The authors have provided necessary parameter deatils. They intend to make the code public.																				
437	SATr: Slice Attention with Transformer for Universal Lesion Detection 	Good reproducibility if code can be provided.									The authors claim that code will be published. Also, sufficient implementation is provided in the paper.									Can be reproduced																				
438	Scale-Equivariant Unrolled Neural Networks for Data-Efficient Accelerated MRI Reconstruction 	The authors provided details about the proposed models, datasets, and evaluation. As the authors stated, the reference code seems be released after the review process.									The dataset used in this paper is publicly available. This paper has the reproducibility.									Data is publicly available and codes will be provided																				
439	Screening of Dementia on OCTA Images via Multi-projection Consistency and Complementarity 	The method seems reproducible. However, the main experiments are conducted on a private dataset, which undermines the reproducibility.									The reproducibility of the paper is credible.									The authors will provide code after the paper is accepted.																				
440	Scribble2D5: Weakly-Supervised Volumetric Image Segmentation via Scribble Annotations 	Datasets are publicly available and sufficient information is provided in the paper to reproduce the results with modest hardware.  The abstract states that code is available online.									The paper can be fairly straight-forward to reproduce. Some key modules of the proposed method are already open-sourced, such as SLIC, HED, etc.									Seems okay. The code is/will be available online.																				
441	Scribble-Supervised Medical Image Segmentation via Dual-Branch Network and Dynamically Mixed Pseudo Labels Supervision 	The reproducibility of the paper is very high. Methods, implementations and settings are quite clear. Codebase is released. Dataset used is public.									The authors released the source code of this paper, and it is not hard to reproduce the experiment.									no review																				
442	SD-LayerNet: Semi-supervised retinal layer segmentation in OCT using disentangled representation with anatomical priors 	The reproducibility is good. The authors provide the link of source code.									They could reproduce the results with some difficulty.									good																				
443	SeATrans: Learning Segmentation-Assisted diagnosis model via Transformer 	No code is provided now. Some training strategies have been described in the Experiemnetal part but not support for the reproducibility of the paper. Since this work is mainly based on the network design.  The code release would benefit other researchers.									The three datasets are available. The code is not available.									The authors have used public data and they will make their code public, according to their answers in the checklist. So I believe their work is reproducible.																				
444	Segmentation of Whole-brain Tractography: A Deep Learning Algorithm Based on 3D Raw Curve Points 	Codes and training process are revealed in the supplementary materials. The reproducibility sounds plausible.									good. It is not clear whether the author will distribute the manual labels.									The author provides code in the supplementary file.																				
445	Self-Ensembling Vision Transformer (SEViT) for Robust Medical Image Classification 	can be reproduced									The paper can be reproduced.									As listed reproducibility checklist, the work should be in good reproducibility.																				
446	Self-learning and One-shot Learning based Single-slice Annotation for 3D Medical Image Segmentation 	Not assessed.									Satisfactory - even though these are checked off on the reproducibility response, there are actually no details of how baseline methods were implemented and used, no variations reported (error bars or standard deviation), and no statistical significance analyses.									seems to be ok																				
447	SelfMix: A Self-adaptive Data Augmentation Method for Lesion Segmentation 	"Good.  More details of the ""random selected non-tumor regions"" should be given."									details look good to me.									The description is clear. Reproducible.																				
448	Self-Rating Curriculum Learning for Localization and Segmentation of Tuberculosis on Chest Radiograph 	The dataset download link is not provided. The training code and model are not provided.									It should be reproducible.									Although the authors curated a large size TB-CXR dataset, my understanding is that they are not sharing the data with the manuscript. The authors also did not share codes. The experimental parameters are provided in the text. The idea is applicable to most medical image analysis problems.																				
449	Self-supervised 3D anatomy segmentation using self-distilled masked image transformer (SMIT) 	It is easy to implement the idea based on the method description.									Authors provide enough information on method details and experimental settings.									It would be easier to reproduce this work, if the code could be released.																				
450	Self-supervised 3D Patient Modeling with Multi-modal Attentive Fusion 	The RGBD keypoint detection module does not seem challenging to implement, but its parameters are not fully explained. The 3D mesh estimation is challenging to implement.									Datasets and code: The authors used a combination of public and the private dataset. The authors have neither provided nor mention the availability of the private dataset, models, training/evaluation code upon acceptance. Experimental results: No result on the different hyperparameters setting or on the sensitivity of hyperparameters on the results. The authors used fixed hyperparameters.									Reproducibility of the method is overall given. Although the description of methods lacks many details in the main paper, they are provided in the supplementary material. The results in the paper could likely not be reproduced, since the authors use proprietary training and testing data. It would be very valuable for the field if this data would be made publicly available.																				
451	Self-Supervised Depth Estimation in Laparoscopic Image using 3D Geometric Consistency 	Good									The authors claim they will release data + code so this should be reproducible.									Implementation details were provided. The data collection process was described.																				
452	Self-Supervised Learning of Morphological Representation for 3D EM Segments with Cluster-Instance Correlations 	New dataset will be made available!									The supplementary material provides enough details for reproduction.									The method part is clear. But it lacks the discussion for the choice of hyper-parameters.																				
453	Self-Supervised Pre-Training for Nuclei Segmentation 	Level of detail is high, so it should be possible to reproduce the work. It seems authors will not provide the code.									The complete architecture is shown in the manuscript, and some implementation details reveal hidden and unclear. The author could consider open sourcing their project, which might be helpful to improve its reproducibility.									The paper provides some implementation details about the proposed method. It is unclear if the paper will provide the official code. So potentially there could be some issues in reproducing the method.																				
454	Semi-supervised histological image segmentation via hierarchical consistency enforcement 	Methods: Highly reproducible as all the technical details are well-explained.  Datasets: Both datasets are open-access which									The presented ideas are clear and can be reproduced.									Authors claimed that the code will be released, and results shown in the paper seem reproducible.																				
455	Semi-supervised Learning for Nerve Segmentation in Corneal Confocal Microscope Photography 	The method is made by many step. The overall method is clear but the reproducibility of each single step is not, probably due to the limited number of available pages.									Good									No code sharing was mentioned in the submission. The datasets are publicly avaible.																				
456	Semi-supervised learning with data harmonisation for biomarker discovery from resting state fMRI 	no review									Reproducibility is good									The authors have checked off all relevant items on the reproducibility checklist, including sharing of code, and the reporting in the paper matches the checklist, so should be highly reproducible.																				
457	Semi-Supervised Medical Image Classification with Temporal Knowledge-Aware Regularization 	The implementation details are enough for the reproducibility of the paper.									the author provides sufficient implementation details which helps reproduce the results.									The reproducibility is fine.																				
458	Semi-Supervised Medical Image Segmentation Using Cross-Model Pseudo-Supervision with Shape Awareness and Local Context Constraints 	The authors will provide the code after acceptance. Public dataset is used.									The method is clearly described, so the paper should be reproducible in terms of implementation.									Precise reproducibility is not a particular strength of neural networks. The networks are described in the implementation section and can be reimplemented.																				
459	Semi-Supervised PR Virtual Staining for Breast Histopathological Images 	As said before, the dataset not seems to be free and public									the model and additional classifiers are vaguely described, without source code the study is not reproducible									Reproducible																				
460	Semi-Supervised Spatial Temporal Attention Network for Video Polyp Segmentation 	It's unclear from the reproducibility questions whether the authors plan to release their masks they produced. The authors do not include any complexity information about their approach, where it's known that transformers tend to be parameter heavy and slow. I do not see any discussion of hyperparameter sensitivity in the supplemental materials like the authors claim.									A detailed explanation of the proposed approach is presented and public available dataset is used for the evaluation step.									The paper shows source code is available and it is acceptable.																				
461	Sensor Geometry Generalization to Untrained Conditions in Quantitative Ultrasound Imaging 	"In the reproducibility checklist, authors claim that they share a sample test dataset and their code. The current version of the manuscript has no links to open source repositories, but I expect authors would make data and code available upon acceptance. I see it very difficult to replicate the method and reproduce the results by only following the description provided in the paper, due to the complexity of the proposed architecture. Moreover, authors declare that ethical approval is ""not applicable"" to their work. Actually, since their evaluation include real patient data, ethical approval should be present."									The authors agree to make public the, Training code, Evaluation code,  (Pre-)trained model(s), Dataset or link to the dataset needed to run the code.									good																				
462	SETMIL: Spatial Encoding Transformer-based Multiple Instance Learning for Pathological Image Analysis 	the paper provided sufficient details, the reproducibility is good.									The source code has been  provided.									Sharing code is one of the best practices for reproducibility, and I would say the work is reproducible.																				
463	SGT: Scene Graph-Guided Transformer for Surgical Report Generation 	The reproducibility is good, as the code is already released.									According to the reproducibility checklist, the source code and pretrained models will be made publicly available, which is essential to guarantee the reproducibility of the results. Additionally, the method was developed using a public benchmark dataset for surgical report generation, which promotes research in the area.									Reproducibility looks good.																				
464	Shape-Aware Weakly/Semi-Supervised Optic Disc and Cup Segmentation with Regional/Marginal Consistency 	The code is already shared and the data downloadable. The documentation seems enough to be able to re-run their experiments.									Meet the requirement.									I think the method can be replicated, and the author has provided the code.																				
465	Shape-based features of white matter fiber-tracts associated with outcome in Major Depression Disorder 	This report is consistent with the paper.									Should be reproducible based on what has been reported. Though, code is missing on how the methods were executed.									They used publicly available tools. However, the dataset is not public, hopefully it will be.  Used parameters are reported but they specific code is not available or not clear whether will be. I was not able to find the the reference foot note 1 to what is referred (the github link).																				
466	ShapePU: A New PU Learning Framework Regularized by Global Consistency for Scribble Supervised Cardiac Segmentation 	The work was evaluated on a publicly available dataset. Following the checklist, I assume that the code will be available after acceptance.									The paper can be reproduced and data can be collected to compare against new algorithms									The implementation details are mentioned and the authors have used a standard Unet model for this framework. A few more details such as how long it took and how many iteration for EM (was the EM algorithm performed for each epoch?) would be helpful.																				
467	Show, Attend and Detect: Towards Fine-grained Assessment of Abdominal Aortic Calcification on Vertebral Fracture Assessment Scans 	nan									The evaluation uses a public dataset and the model is clearly described.									The authors will publish the trained model and their code upon acceptance. However, some details should be disclosed in the paper, such as the number of training epochs. Also, a citation was provided where the same dataset was used, which does not appear to be published.																				
468	Siamese Encoder-based Spatial-Temporal Mixer for Growth Trend Prediction of Lung Nodules on CT Scans 	reproducible									This paper does not specify the software framework and version used, the description of the model is not clear enough, the experimental code and other experimental details are not provided, and the reproducibility of the paper is poor.									Good reproducibility: Authors provided clear description about the pipeline of their methods, clear description about the datasets they used for experiments, as well as the parameters and environments used for training models.																				
469	Simultaneous Bone and Shadow Segmentation Network using Task Correspondence Consistency 	No comments.									Network architecture and training information are clear and helpful for reproducibility.									OK. NO particular comment.																				
470	Skin Lesion Recognition with Class-Hierarchy Regularized Hyperbolic Embeddings 	They scarcely describe the database. They mention the number of images but they do not mention how many images of each class or if it is balanced. They cited a previous work of the authors, perhaps in this reference the database is better described. They do not provide implementation details. They state ithat it  is included in an Appendix, but in the available appendix only a figure with the structure of the skin taxonomy is contained.									Excellent.									Upon release of the dataset employed in the study, the paper contains the necessary information to reproduce the paper's results. However, I encourage the authors to share more details about the classification network and clarify whether multiple layers were used and if there is any type of batch normalization or non-linearity.																				
471	SLAM-TKA: Real-time Intra-operative Measurement of Tibial Resection Plane in Conventional Total Knee Arthroplasty 	Author(s) response to reproducibility indicate that data and code will be shared, which makes this paper full reproducible.									The implementation details have been well addressed.									The reproducibility depends on whether the code is to be open-sourced. Otherwise, making a SLAM pipeline working takes a lot of efforts in fine-tuning. It is good that the authors promise to share the code after acceptance.																				
472	SMESwin Unet: Merging CNN and Transformer for Medical Image Segmentation 	likely to be reproducible									Possible to reproduce.									As the codes will be public after acceptance and dataset is already public, the reimplementation and repeat of the results should be possible and easy.																				
473	Sparse Interpretation of Graph Convolutional Networks for Multi-Modal Diagnosis of Alzheimer's Disease 	The paper includes analysis of results and performance and the dataset used are cited. However, the paper lacks implementation details for reproducibility.									Publically available data is being used for evaluation.									The ideas and experiments described in this paper are quite clear, should be repeatable.																				
474	Spatial-hierarchical Graph Neural Network with Dynamic Structure Learning for Histological Image Classification 	The code has been provided to regenerate the results.  Part of the experiments has been done on BRACS dataset which is publicly available.									The checked points in the reproducibilty checklist match perfectly the information provided in the paper.									Beyond the presence or absence of the software, further details in terms of reproducibility could be given, for example the stop criterion for the choice of the best model.																				
475	Spatiotemporal Attention for Early Prediction of Hepatocellular Carcinoma based on Longitudinal Ultrasound Images 	The experimental conditions in terms of reproducibility are well described. The authors provide source codes for training and evaluation. The dataset is not public.									Private dataset and code.									Paper is clear enough that an expert could confidently reproduce																				
476	Spatio-temporal motion correction and iterative reconstruction of in-utero fetal fMRI 	The authors meet the reproducibility criteria. The reviewer was unable to find any code or a commitment to share later.									The proposed method seems to be  reproduceable. The method is clearly presented and there is an algorithm summarizing the workflow.									The algorithm is clearly explained, though the data and code are not made available.																				
477	Stabilize, Decompose, and Denoise: Self-Supervised Fluoroscopy Denoising 	The paper is reproducible.									Implementation details and experimental settings were well-explained. It could be reproducible.									Yes																				
478	Stay focused - Enhancing model interpretability through guided feature training 	Authors will publish all relevant code, datasets, and models.									Authors will provide the code and dataset upon acceptance. However, these were not available at revision time so that they could not be assessed.									Details in terms of calculations are clear, code will be provided. The data is publicly available but the details re images might not be as clear/shared with and in this way one could face a bit of problems replicating the work.																				
479	Stepwise Feature Fusion: Local Guides Global 	I didn't spot issues about reproducibility.									I think this paper has provided enough details for models and experiments settings.									The authors indicate that the code and model will be released after being accepted. The datasets used in Experiments are all open accessed. The paper also provides the data splits, training parameters and model structure, although some of the model parameters are missed (convolution kernel size in LE module), the results of the paper should be able to reproduce.																				
480	Stereo Depth Estimation via Self-Supervised Contrastive Representation Learning 	Authors promise to release their code as well as the datasets used in this paper. Thus I believe the results should be able to reproduce following the descriptions and the released repo.									The authors provide implementaion details in the paper and also agree to release codes and pretrained models in the reproducibility checklist.									Details to train the encoder is missing.																				
481	Stroke lesion segmentation from low-quality and few-shot MRIs via similarity-weighted self-ensembling framework 	The authors indicate that they will provide access to the implementation. This will help to reproduce the results, but at the cost of a careful reading of the code, since the reader has to fill the gaps in the description found in the article.									Code and data are publicly available.									Code will be made available. Datasets are available.																				
482	Structure-consistent Restoration Network for Cataract Fundus Image Enhancement 	Upon release of the code, the reproducibility can be verified.									Two private datasets are adopted in the training and evaluation stage. Besides, several value settings of parameters are lost in the article. Adding the instructions of these parts in the paper would contribute to the improvement of reproducibilily.									It is likely to be reproducible.																				
483	Super-Focus: Domain Adaptation for Embryo Imaging via Self-Supervised Focal Plane Regression 	All the methods developed in this paper are clear and valid. Also majority of the implementation details are properly described and values of all the parameters are reported.									Since all datasets are private, reproducing the reported results is not possible. Pretrained models are also not released. One can only get ideas from reading the text on similar datasets/problems.									Seems reproducible, given that the authors indeed provide the code in case of acceptance.																				
484	SUPER-IVIM-DC: Intra-voxel incoherent motion based Fetal lung maturity assessment from limited DWI data using supervised learning coupled with data-consistency 	The authors claim that their code will be made publicly available upon publication. The simulated data the authors are using is probably easy to reproduce. The authors do not mention that they are planning to publish the in vivo data used in their experiments, which probably makes the exact reproduction of their results impossible. AT least the experiments using healthy volunteers should probably be reproducible using own acquisitions though.									The authors provide sufficient details for reproducing their methodology, including a detailed description of the implementation of their code. While they pledged to make code available, they did not rely on open data, therefore a complete reproduction is not possible.									Their code and trained models will be made publicly available upon publication.																				
485	Supervised Contrastive Learning to Classify Paranasal Anomalies in the Maxillary Sinus 	Overall, most hyperparamters are given and the methods themselves are not novel. The dataset is not provided so its hard so reproduce the exact results, but extending them to different datasets should be easy.									The authors will not release their code.									The paper seems fairly reproducible - no code nor data is available but readers could try the suggested methods on their dataset of choice.																				
486	Supervised Deep Learning for Head Motion Correction in PET 	"Language problems need to be checked, many long sentences are not clearly expressed and are confusing. The experiments section needs more clarifications: The subject group contains patients with normal cognition, cocaine dependence, and cognitive diseases. I would like to know if any strategy was used in splitting the training and test sets? For example, keeping a diversity of patient types in each subset. In the ablation study (Table 1), the test MSE value for ""more data, FWT, Deep Encoder and Normal sampling"", which is located in the second row, has a much higher value than the others. Can the authors explain this? The results In fig. 2(a) are for subject 1. Also, there is another subject 1 in Table S1. I assume they are different subjects since one is for single-subject experiments and one is for multi-subject experiments. But referring to them the same name still causes confusion. In section 3.2, the authors claimed that subject 2 has a mean MSE of 0.02. But Table S1 shows the mean MSE of subject 2 is 1.114. They are contradictory. I assume they are also different objects as I mentioned above. The confusions need to be addressed. ""The results for Subject 2 (mean MSE 0.02) show that the network is capable of accurately predicting motion from training subjects even though the motion relative to the reference frame was never used for training."" Does this mean that in the experiment in figure 2, the moving images of subject 2 have been resampled and different from the images that are used during training? The detail of the experiment settings needs to be clarified."									The authors have given enough information to reproduce the work.									So there is no code that is shared, at least I cannot see a link to the GitHub repo. The MOLAR reconstriction is accessible in principle, but is not easy to get to run, not even if one also has an HRRT scanner. Hence, the reconstruction of images is probably hard to redo. The data can probably not be shared, since it is clinical data.																				
487	Suppressing Poisoning Attacks on Federated Learning for Medical Imaging 	It seems feasible to reproduce the results with the provided information.									The implementation detail of the proposed method is well presented, as well as the experimental settings.									no review																				
489	Surgical Scene Segmentation Using Semantic Image Synthesis with a Virtual Surgery Environment 	It looks reproducible.									The dataset will be released to public on their website.									The authors promise to publish the real and synthetic datasets, as well as the baseline segmentation models for reproducibility. However, a fully functioning published version of the synthetic data generation pipeline would be valuable for the community to transfer the method to other domains and clinical applications																				
490	Surgical Skill Assessment via Video Semantic Aggregation 	Measures of variance are missing.									The reproducibility of the paper looks fine.									The authors state that they will release the source code upon publication. They experiment on publicly available datasets: one of them is the commonly used JIGSAWS with predefined experimental setups and the other one is HeiChole, for which the authors include some details to their experimental setup.																				
491	Surgical-VQA: Visual Question Answering in Surgical Scenes using Transformer 	The authors provide codes and documents.									According to the reproducibility checklist, the source code and pretrained models will be made publicly available, which is essential to guarantee the reproducibility of the results. Additionally, the method was developed using a public benchmark dataset for surgical VQA, which promotes research in the area. Despite not including any intention to publicly release the data in the main text, the reproducibility checklist mention that they will be released upon acceptance. Furthermore, there should be included more statistics about the new data. How were these annotations generated? This information will be valuable for new research on this task.									According to the materials, the paper is reproducible.																				
492	Survival Prediction of Brain Cancer with Incomplete Radiology, Pathology, Genomic, and Demographic Data 	The authors have provided necessary information for reproducibility.									The authors specify that code and data will be publicly available									The authors provide some information about the dataset used, and the experimental settings. The exact details of the split generation are missing. Overall the work looks to be reproducible.																				
493	SVoRT: Iterative Transformer for Slice-to-Volume Registration in Fetal Brain MRI 	Authors state on the reproducibility statement that code will be made available, nothing is mentioned to this sense in the paper. Still the different generated/simulated motion levels on the FeTA dataset should be made available also as to ensure reproducibility.									The reference implementation of SVoRT will be available on github.									Reproducible for the publicly available FETA dataset. The clinical dataset is not available as the authors answered not applicable to << A link to a downloadable version of the dataset (if public) >>, the code is/will be however put in Github.																				
494	Swin Deformable Attention U-Net Transformer (SDAUT) for Explainable Fast MRI 	Some model specifications are missing in the paper. But the authors have agreed to release codes and models in the checklist.									Hard to assess.									The reproducibility checklist shows the authors will upload their code and implementation details online. The experimental settings are listed in the manuscript, but the hyper-parameters for the proposed method are missing.																				
495	Swin-VoxelMorph: A Symmetric Unsupervised Learning Model for Deformable Medical Image Registration Using Swin Transformer 	very good reproducibility, with code and dataset available.									The paper seems reproducible. Although, the authors do not mention in the text whether they plan to make the code publicly available.									The paper is easy to follow, though some of the details are not clearly described.																				
496	Tagged-MRI Sequence to Audio Synthesis via Self Residual Attention Guided Heterogeneous Translator 	Authors have provided fair information to reproduce the method.									It is negative for the reproducibility of the paper, because they did not share the source codes.									The authors provide sufficient information to reimplement the proposed architecture.																				
497	Task-oriented Self-supervised Learning for Anomaly Detection in Electroencephalography 	Codes not open, should have some issues in reproducibility.									GIven the submission, it is possible to reproduce the results even without the code.									Good.																				
498	Task-relevant Feature Replenishment for Cross-centre Polyp Segmentation 	The hyperparameters are clearly listed. Although is not clear how the model is evaluated. The paper states that the maximum training epoch is set to 150. Is the model evaluated at the end of the 150 epoch? Or maybe some strategy is used to select the best epoch in which the model is evaluated?									Sufficient implementation details are provided in the paper.									Although the author does not provide source code, they mention about the hyperparamters. But i do not think the method is reproducible at current form.																				
499	TBraTS: Trusted Brain Tumor Segmentation 	This paper seems to be reproducible on any segmentation architecture.									The paper is reproducible.									No special issue here.																				
500	Test Time Transform Prediction for Open Set Histopathological Image Recognition 	The source code will be made public with the publication. The authors have used public dataset for their experiements.									The reproducibility of this work seem to be reasonable, but should be further validated on more datasets.									I feel the authors present enough information on the datasets and algorithm to allow for reproducibility of the paper. If they indeed release a working software, the impact of the paper will increase.																				
501	Test-time Adaptation with Calibration of Medical Image Classification Nets for Label Distribution Shift 	Code is not available yet. But the TTADC is expected to be readily reproducible.									Authors claim that code will be available after review.									everything is ok																				
502	Test-Time Adaptation with Shape Moments for Image Segmentation 	"-Details about the data preprocessing and the data splits are provided in the paper. -Details on training are provided in ""Training and implementation details"" section."									The datasets are publicly available, the code as well, with the correction of a few definitions in the description of the method, the work should be reproducible. However, as the method is running an optimization process at test time it would be good to include the details of the used hardware and the measured runtime in order to know what the expected inference time for a sample of the target domain is.									The authors provide source codes for this work and the main datasets used are from challenges. Therefore, the reproducibility of this paper can be achieved to a certain extent. But, it is better to provide more information, like the description of system dependencies, instructions to train and test models, and data preprocessing.																				
503	Test-time image-to-image translation ensembling improves out-of-distribution generalization in histopathology 	Very clear description, can be reproduced									No links or references for the source code and dataset, however the authors have stated in the reproducibility form that they will release it upon acceptance of the paper.									Probably fair. But it would be better if the authors could release their codes.																				
504	TGANet: Text-guided attention for improved polyp segmentation 	The key details are sufficient to reproduce the main results.									The paper introduces the network architecture clearly. The experiments are conducted on four public datasets. Code is not released.									The authors plan to release the codes.																				
505	The (de)biasing effect of GAN-based augmentation methods on skin lesion images 	Data is available online									The paper is conceptually reproducible, using well-known methods and well-defined techniques.									Implementation details are given. Manual annotations will be share publicly.																				
506	The Dice loss in the context of missing or empty labels: introducing Ph and  	The authors stated that they will release the code and the related material for the paper.									The paper mentions that the code will be released upon acceptance. This would be helpful in reproducing the results. In addition, this paper also offers some training details for reproducing the results. But these details may not be completely sufficient to obtain the experimental results.									Mention to release the code upon acceptance, public datasets																				
507	The Intrinsic Manifolds of Radiological Images and their Role in Deep Learning 	The results are reproducible, but code release would be nice.									It should be reproducible.									no review																				
508	The Semi-constrained Network-Based Statistic (scNBS): integrating local and global information for brain network inference 	As said above, the method is described in detail but it may be prone to circularity issues in the implementation. In the reproducibility statement, the Authors claim to disclose the code of all experiments, which should enable meta-reviewers and - if accepted - future readers to verify important technical aspects. I invite the Authors to clearly mention in the manuscript the complete availability of the source code.									The order of the paper was a little unfamiliar. It would be better for Sec. 2 scNBS to be included in Sec. 3 method, and for the subsection 'Synthetic Data for Benchmarking' to be included in Sec. 4 Results. The words in the figures and the figures were too small to read. In Fig. 3, R is not defined. In the second line on page 4, the parentheses are missed. There is no explanation of how S_{g}^{+} and S_{g}^{-} were used. t_{g}^{+} was selected when the correlation between V_{g}^{+}(c) and y was maximized, however, t_{g}^{-} was selected when the correlation between V_{g}^{-}(c) and y was minimized. Why was the negative effect in t_{g}^{-} selected when the correlation between V_{g}^{-}(c) and y was minimized? The cut-off values, t_{g}^{+} and t_{g}^{-} were obtained by the correlation with y, and the final inference was also estimated by the relationship between V_{g}^{+}(t_{g}^{+}) and y, and between V_{g}^{-}(t_{g}^{-}) and y. It would be better if the results of the consistency analysis of the other methods was also shown in Fig. 6.									Good. I would recommend the authors to release their code upon acceptance of their manuscript.																				
509	Thoracic Lymph Node Segmentation in CT imaging via Lymph Node Station Stratification and Size Encoding 	It is better that code is available to confirm the whole procedures proposed in the paper although the paper is well written and reproducibility can be recognized.									Adequate.									The code of this work were not provided. The reproducibility is slightly worse.																				
510	TINC: Temporally Informed Non-Contrastive Learning for Disease Progression Modeling in Retinal OCT Volumes 	Apart of few technical issues that I detailed below, and after reading the original VICReg paper, I think that the method is clear and the results can be reproduced.									Not reproducible									The authors state that they will release training code, pre-trained model and evaluation code. I believe the reproducibility of this paper is good.																				
511	TMSS: An End-to-End Transformer-based Multimodal Network for Segmentation and Survival Prediction 	Reproducible									"""An analysis of situations in which the method failed"" is missing"									"The reproducibility checklist well matches the contribution. I would like to argue, however, that significance analyses (""Not applicable"") are well applicable, and thus, that the depiction of measures of confidence would have been adequate."																				
512	Toward Clinically Assisted Colorectal Polyp Recognition via Structured Cross-modal Representation Consistency 	The technical implementation process is well described and the method is tested on public datasets.									Good.									It is very easy to follow the network design, with the information provided by the authors, it should be able to duplicate the network without major issues.																				
513	Towards Confident Detection of Prostate Cancer using High Resolution Micro-ultrasound 	Satisfactory									Reproducibility would be a challenge that requires significant effort.									The authors have provided code and details on parameter selection that may allow others to reproduce performance, however, only if similar data and ground truth information is found elsewhere.																				
514	Towards Holistic Surgical Scene Understanding 	"The Reproducibility statement is filled out correctly, to my impression. Very high reproducibility overall, as the authors state: ""we will make publicly available the PSI-AVA dataset, annotations, and evaluation code, as well as pre-trained models and source code for TAPIR"", which is fully transparent. The only missing information was the chosen license for dataset+annotations, TAPIR code, and pre-trained weights."									The paper is reproducible if authors release the dataset.									The structure of the transformer is not shown in Figure 2. This is not conducive to the reproduction of the model.																				
515	Towards performant and reliable undersampled MR reconstruction via diffusion model sampling 	The authors provide code in the supplementary material.									This paper is unclear about some implementation details about model training and settings.									The author were able to make the code and data public. That would benefit the community in recognizing this problem. Also, the evaluation metric is clearly described so that the result would be convincing.																				
516	Towards Unsupervised Ultrasound Video Clinical Quality Assessment with Multi-Modality Data 	Appears to be complete									Although the authors state that the code is provided, this is not the case. The used dataset is not public and thus is not provided. However, the authors should provide a bit more information on image acquisition/origin.									As the method includes too many details and the model has several parts, the results would only be reproducible if the code and data were available.																				
517	Tracking by weakly-supervised learning and graph optimization for whole-embryo C. elegans lineages 	It is a pity that the mskcc-confocal and nih-ls datasets and the results achieved over them were not made privately available for the peer-review purposes. Furthermore, This reviewer did not find any relevant information about the computational aspects of the proposed method (i.e., hardware requirements, execution time, memory footprint, etc.) in the paper.									The methodology is valid and clearly presented. The most convincing results were achieved on the challenge data, with the submission being evaluated by the organizers.									Seems to be reproducible to me. However, as mentioned above it would be good for the final publication to include links to data and repositories to be used for benchmarking and reproduction the community.																				
518	TractoFormer: A Novel Fiber-level Whole Brain Tractography Analysis Framework Using Spectral Embedding and Vision Transformers 	Seems fine.									Authors mentioned that the code will be made available upon request. The availability of the pre-trained model is unclear.									The code and data will be released according to the reproducibility checklist.																				
519	TransEM: Residual Swin-Transformer based regularized PET image reconstruction 	The authors have given enough details to reproduce this work.									nan									The authors made an effort to make their paper reproductible, which lots of setup values. I did not find in the paper or in supplementary material a way to access the code, whereas it seems to be done according to the reproducibility checklist filled in by the authors. Maybe this is not possible at this stage. The authors detailed the hyperparameters values of the neural network, of the method, and the initialization of the image, which is very important. However, were  several runs make ? Or was the initialization of the neural network weights fixed ? About the PET simulation, some simulation setup is missing, like the system matrix modelling, the number of line of responses.																				
520	Transformer based feature fusion for left ventricle segmentation in 4D flow MRI 	What can I say, the code and models are available on Github - as good as it gets! :-)									The experiments are performed on a private dataset while the code is avaible on git.									The description of the proposed method is clear and the paper is reproducible.																				
521	Transformer based multiple instance learning for weakly supervised histopathology image segmentation 	Probably. The authors mention that all code and models will be made available upon acceptance.									The paper gives detailed explanation of its method and experimented on a public dataset and prepares to release the code, thus the reproducibility is good.									This paper is reproductive.																				
522	Transformer Based Multi-task Deep Learning with Intravoxel Incoherent Motion Model Fitting for Microvascular Invasion Prediction of Hepatocellular Carcinoma 	No comments. Everything is clear									source code seems to be available.									No elements on paper reproducibility could be found in the paper.																				
523	Transformer Based Multi-View Network for Mammographic Image Classification 	The method proposed in the article has good reproducibility, and the referenced modules are described in more detail in the article.									Reproducibility looks good.									No code is given, but the description should be clear enough to reproduce.																				
524	Transformer Lesion Tracker 	In the abstract the author mentioned the code would be published. The dataset is public available as well.									An analysis of situations in which the method failed. [Not Applicable]  -> It is applicable but not done. code will be available on Git									no review																				
525	Transforming the Interactive Segmentation for Medical Imaging 	Some important details are missing.									The reproducibility of the paper is possible if authors submit the code to the project . Databases are available for tests . Perhaps a deeper explanation of the software modules would be necessary in order to reproduce it correctly. But it can be done if the code is well documented.									handling UI is always a bit tricky so open sourcing would be welcome.																				
526	TransFusion: Multi-view Divergent Fusion for Medical Image Segmentation with Transformers 	The authors have chosen YES in response to all the questions but I am not sure if they address the questions. For example, the scenarios where the approach may fail are not clearly reported. The average runtime for each result, or estimated energy cost is missing.									The paper looks reproducable from the response. The proposed algorithm is validated on the multi-disease and -centric data, hence the paper is reproducable.									Reproducibility of the paper is good.																				
527	TranSQ: Transformer-based Semantic Query for Medical Report Generation 	I think the paper can be reproduced sufficiently. Hopefully, the authors will provide their code and make it public as it will be of use to the medical imaging community.									Reproducibility claims software released and documented, but there is no reference to this in the paper (expecting an anonymized footnote.)    The reproducibility statement refers to the supplementary material, but it does not seem to have been submitted.									I believe that the obtained results can, in principle, be reproduced. Even though key resources (code) are unavailable at this point, the key details (e.g., proof sketches, experimental setup) are sufficiently well described for an expert to confidently reproduce the main results, if given access to the missing resources.																				
528	Trichomonas Vaginalis Segmentation in Microscope Images 	Experimental settings are presented in the paper. However, all experiments are conducted on a private dataset, which is not available during review.									The authors do not mention the range of hyper-parameters considered nor the method to select the best hyper-parameter configuration. They only specify some of the hyper-parameters used to generate results. The exact number of training and evaluation runs (iterations or epochs) is not provided. A description of the hardware infrastructure used is provided but nothing is mentioned about the deep learning framework used nor the code availability. There is no analysis of situations in which the method failed. There is no description of the memory footprint nor an average runtime for each result, or estimated energy cost. There is no analysis of statistical significance of reported differences in performance between methods. The results are not described with central tendency (e.g. mean) & variation (e.g. error bars). The specific evaluation metrics and/or statistics used to report results are correctly referenced. There are no details of train / validation / test splits nor details on how baseline methods were implemented and tuned.									The paper contains enough details for the reproduction of results, although the segmentation method should be described in more detail.																				
529	UASSR:Unsupervised Arbitrary Scale Super-resolution Reconstruction of Single Anisotropic 3D images via Disentangled Representation Learning? 	Part of the data used is publicly available. The methods are thoroughly explained in the paper, except for the fusion part. Several items for which the authors responded yes are not included: A clear declaration of what software framework and version you used. A link to a downloadable version of the dataset (if public). Whether ethics approval was necessary for the data. Information on sensitivity regarding parameter changes. Details on how baseline methods were implemented and tuned. The details of train / validation / test splits. A description of results with central tendency (e.g. mean) and variation (e.g. error bars). -An analysis of statistical significance of reported differences in performance between methods. The average runtime for each result, or estimated energy cost. A description of the memory footprint. An analysis of situations in which the method failed. A description of the computing infrastructure used (hardware and software). Discussion of clinical significance.									Authors promise to release the code as well as the dataset if the paper is accepted. Thus I think reproducibility will not be a problem for this paper.									The proposed method is complicated. Without released sourcecode, the reproduciblity of this work could be an issue.																				
530	ULTRA: Uncertainty-aware Label Distribution Learning for Breast Tumor Cellularity Assessment 	Dataset is public Implementation / code is not provided									Authors claim they will release the code. If so, the study could be reproduced.									The study is based on a public dataset and code will be provided. More detail could be given in the paper (e.g. exact augmentation strategy, exact architecture of MLP and FC layers).																				
531	Uncertainty Aware Sampling Framework of Weak-Label Learning for Histology Image Classification 	The paper seems to be reproducible.									The histopathology images are obtained from TCGA database. The authors say the code will be made public.									The paper uses a publicly available dataset and the methods are described sufficiently well.																				
532	Uncertainty-aware Cascade Network for Ultrasound Image Segmentation with Ambiguous Boundary 	since some necessary details are missing, the Reviewer thinks the reproducibility is modest.									The reproducibility is acceptable.									Easy to reproduce.																				
533	Uncertainty-Guided Lung Nodule Segmentation with Feature-Aware Attention 	Some details are in my opinion missing. Reproducing the paper would require quite some effort.									no review									The author claims to release the source code and the paper provides enough details to reproduce the paper.																				
534	Undersampled MRI Reconstruction with Side Information-Guided Normalisation 	I encourage the authors provide open access to their code.									This paper uses public dataset and reports the details of implementations. It would relatively easy to reproduce the results.									The proposed method is clearly described though the code is not made available																				
535	UNeXt: MLP-based Rapid Medical Image Segmentation Network 	While the code is not included or linked in the paper, it is easy enough to find. Unfortunately, it was necessary to look at the code to understand the paper since he method was insufficiently described in the paper. Still, the code looks like it should allow full reproducibility, which is great.									The reviewer agrees with the checklist and is happy with the reproducibility of the paper overall.									The authors promised to release the code and pre-trained models after review process. The dataset split (80:20 split thrice) is also necessary for reproducibility, however, N-fold cross validation is still preferred.																				
536	Uni4Eye: Unified 2D and 3D Self-supervised Pre-training via Masked Image Modeling Transformer for Ophthalmic Image Classification 	The authors answered positively to all questions, which does not exactly match the paper content, but the authors mention that they will make the code available which is good!									The code will be made available. Still the model and training hyperparameters are thoroughly described. I made a little comment for the authors on the ViT architecture details and the setting/tuning of the loss weights. The created / collected dataset is described in the appendix and will be made publicly available (although missing some description on the contribution to the data collection).									The code is provided. Not sure whether the dataset is open source.																				
537	Unified Embeddings of Structural and Functional Connectome via a Function-Constrained Structural Graph Variational Auto-Encoder 	-Reproducibility: Code is not provided.									The reproducibility of the paper is limited.									The reproducibility of this paper is limited. There is no source code released in this work.																				
538	Unsupervised Contrastive Learning of Image Representations from Ultrasound Videos with Hard Negative Mining 	Easy to reproduce.									Code and pretrained models are provided, thus the work is of high reproducibility.									The authors release their code and dataset.																				
539	Unsupervised Cross-Disease Domain Adaptation by Lesion Scale Matching 	"The source codes are not provided.  One of the dataset is private dataset. There is no detailed ""Algorithm"" presented in a table, hence reproducibility may not be that straightforward."									It appears that authors meet the requirements.									code and dataset will be avaialbe according to the submission.																				
540	Unsupervised Cross-Domain Feature Extraction for Single Blood Cell Image Classification 	One of the three datasets used in the work is private and not shared by the authors.  No code or link for this or an intent for sharing of this is provided.									The authors gave a satisfactory quantity of details for reproducibility.									No code is provided, but the details of the training and network architecture are provided.																				
541	Unsupervised Deep Non-Rigid Alignment by Low-Rank Loss and Multi-Input Attention 	Not so good.									Reproducibility is insufficient. The author's paper states that the codes will be provided, but there is no explanation of the selection and sensitivity of hyperparameter of losses, and the collection of the datasets.									They did not submit the code, but it will be made public according to the authors. Details including hyperparameters, network architecture, computation time, etc. are given in the manuscript. It could be reproduced as the paper.																				
542	Unsupervised Deformable Image Registration with Absent Correspondences in Pre-operative and Post-Recurrence Brain Tumor MRI Scans 	Some important parameters, such as the threshold for the inverse consistency (in Sec2.2), might be data-dependent and have to be tuned.									The method is well described and the source code will be made publicly available after acceptance of the work. The data are not directly publicly available, but can be requested from the organizers of the competition. Therefore, the reproducibility is rated as good.									This paper has provided details about the models, datasets, and evaluation.																				
543	Unsupervised Domain Adaptation with Contrastive Learning for OCT Segmentation 	No code is provided.  Some parameters have been provided in the paper.									The method is reproducible									The loss function and hyperparameters are described. The architecture is described. However, there is no link to code and no declaration of the authors to make the code publicly available. It is also unclear whether the dataset is publicly available or not.																				
544	Unsupervised Domain Adaptive Fundus Image Segmentation with Category-level Regularization 	The authors indicated sharing their code, which thus is highly reproducible.									Without a detailed algorithm, it is difficult to reproceduce the results in the paper.									Sufficient details for reproducibility.																				
545	Unsupervised Lesion-Aware Transfer Learning for Diabetic Retinopathy Grading in Ultra-Wide-Field Fundus Photography 	The paper includes the implementation deatils for reproducibility.									The note on reproducibility seems fine but the authors used a private dataset where it will be hard to access.									The UWF dataset appears to be private. Exact details for the proposed adversarial lesion generation module and lesion external attention module (including hyperparameters) do not appear to be provided, only the general description.																				
546	Unsupervised Nuclei Segmentation using Spatial Organization Priors 	The discription of training details is clear. The reproducibility will be good if the author add the details for HE database construction.									Some details of how the metrics are computed are not present. For example: How is the accuracy balanced? 2.How is the object level segmentation achieved using Unet? The proposed method uses some labeled masks to train the discriminator. How many such masks are used?									Loss functions and hyperparameters are present in the paper so the idea could be roughly reconstructed. But taking in consideration the amount of parameters and size of the model without actual code it is impossible to fully reproduce the results. There is no information about public release of the code so the reproducibility of the paper is limited.																				
547	Unsupervised Representation Learning of Cingulate Cortical Folding Patterns 	The code was provided. Dataset used is publicly open.									Excellent, the author provided their code, which is a plus.									I believe that the obtained results can, in principle, be reproduced.																				
548	Usable Region Estimate for Assessing Practical Usability of Medical Image Segmentation Models 	Datasets and models are all publicly available, so it should be easy to replicate.									Sufficient implementation details are provided.									No reproducible																				
549	USG-Net: Deep Learning-based Ultrasound Scanning-Guide for an Orthopedic Sonographer 	Dataset acquisition method, system specifications and dataset are all anonymous, hence, it is not possible to comment on reproducibility of the work.									The work is solid and easy to follow and reproduce. training and testing code will be released. Clear implementation details of hyperparamters in appendix. Clear dataset construction pipeline.									Although based on the reproducibility checklist, the authors demonstrate a commitment to publishing their code publicly, it is unclear whether the dataset will also me made available. The hyperparameters used appear to be missing in the manuscript.																				
550	Using Guided Self-Attention with Local Information for Polyp Segmentation 	no problem									Some model details are missing. The authors do state in the reproducibility checklist that they will release the code.									For all models and algorithms, check if you include A clear declaration of what software framework and version you used. [Yes] No, the version is not mentioned. For all datasets used, check if you include: For all code related to this work that you have made available or will release if this work is accepted, check if you include: Specification of dependencies. [Yes]  Training code. [Yes]  Evaluation code. [Yes]  (Pre-)trained model(s). [Yes]  Dataset or link to the dataset needed to run the code. [Yes]  README file including a table of results accompanied by precise command to run to produce those results. [Yes] None of this is mentioned in the manuscript!! For all reported experimental results, check if you include: The range of hyper-parameters considered, method to select the best hyper-parameter configuration, and specification of all hyper-parameters used to generate results. [Yes] No; no; some. An analysis of situations in which the method failed. [Yes] Not found in the manuscript. A description of the memory footprint. [Yes] Not found in the manuscript. The average runtime for each result, or estimated energy cost. [Yes] Not found in the manuscript. An analysis of statistical significance of reported differences in performance between methods. [Yes] No statistical analyses have been performed! A description of results with central tendency (e.g. mean) & variation (e.g. error bars). [Yes] No variation of results is described! The details of train / validation / test splits. [Yes] Validation splits are not mentioned. The exact number of training and evaluation runs. [Yes] Not found in the manuscript. Information on sensitivity regarding parameter changes. [Yes] Not found in the manuscript.																				
551	USPoint: Self-Supervised Interest Point Detection and Description for Ultrasound-Probe Motion Estimation during Fine-Adjustment Standard Fetal Plane Finding 	The dataset will be not publicly available but the source code is promised to be available.									I didn't find reference to code/data etc. I suspect the code may be easily available later but not sure about the data since it seems more custom generated. The components of the method should be generally easy to replicate and test.									The details about training, hyper-parameters are not provided																				
552	Vector Quantisation for Robust Segmentation 	Reproducible.									If the paper aims to claim that vector quantisation could be effective in the segmentation task universally, the author should validate this on a more general platform, such as nnUNet. Additionally, the author should try at least one more architecture except for U-Net to approve the generalization. Additionally, the author doesn't give enough provement to demonstrate the effectiveness of VQ in domain shift. Please see the below comments.									The reproducibility statements look reasonable. Authors mentioned in both the reproducibility statement and the paper that the code will be released.																				
553	Video-based Surgical Skills Assessment using Long term Tool Tracking 	I think that it would be better if the authors share more details on how they evaluated their proposed method to make it easier to reproduce their results.									Any additional comment on this.									The manuscript mentions the use of data augmentation strategies but did not provide details on different augmentation methods that may have been used. -The basic hyperparameters are not presented. The experiments performed should accompany the details of the training but there are no details found in the manuscript. The manuscript mentions Bayesian hyper-parameter search was used for learning-based models but did not include initial or any parameter details on which the search was carried out. This is missing from the experimental setup section.																				
554	Vision-Language Contrastive Learning Approach to Robust Automatic Placenta Analysis Using Photographic Images 	The reproducibility is challenging without the authors' dataset and training hyperparameters. The authors are suggested to open their datasets and algorithm that will positively impact placenta image analysis.									likely reproducible									The authors mention that the dataset comes from a large urban academic hospital but not mention it's availability, possibly the dataset is private and therefore it will be difficult to reproduce the results using the same data. The authors do not mention availability of their source code making it difficult to even do the same evaluation in other datasets. In the supplementary material there is extra information of the training hyperparameters, this might help in the reimplementation of their method.																				
555	Visual deep learning-based explanation for neuritic plaques segmentation in Alzheimer's Disease using weakly annotated whole slide histopathological images 	The training/testing code and the dataset for the study will be provided. This will help in reproducing the results and use the method as a baseline.									Code is provided as supplementary material.									good assuming the code and the data will be shared.																				
556	Visual explanations for the detection of diabetic retinopathy from retinal fundus images 	Authors do not provide source code. But with the detailed description presented in the paper, results should be reproduced.									Reproducibility is guaranteed.									The paper is mostly clear about the techniques/data it used as well as open-source implementations and public databases it uses, meaning that it could conceptually be produced relatively easily.																				
557	vMFNet: Compositionality Meets Domain-generalised Segmentation 	The authors agree that the code will be made publicly available.									Except for the potential error in the L_vMF loss, the method and experiments should be reproducible. Sharing the source code would help.									The authors have agreed to make their code publicly available. Also public datasets have been used for experimentation. Sufficient implementation details have been provided in the paper.																				
558	Vol2Flow: Segment 3D Volumes using a Sequence of Registration Flows 	I believe that the obtained results can be reproduced.									It is difficult to re-implement the exact method and reproduce the results due to missing implementation details.									seems reasonable to reproduce.																				
559	Warm Start Active Learning with Proxy Labels & Selection via Semi-Supervised Fine-Tuning 	It seems reproducible.									The authors state that the code will be shared. The data is also public.									The authors have satisfied the reproducibility checklist.																				
560	WavTrans: Synergizing Wavelet and Cross-Attention Transformer for Multi-Contrast MRI Super-resolution 	This paper is highly likely to reproduce as the authors have provided the code in the supplementary material.									While the software details are provided, the paper uses in-house data and non-open source code which will likely make it hard/impossible to reproduce the results.									This is paper is well organized, I think it is with good reproducibility.																				
561	Weakly Supervised MR-TRUS Image Synthesis for Brachytherapy of Prostate Cancer 	This paper is clear enough that an expert could confidently reproduce.									I believe it could be reproduced from the paper.									The paper has included enough details to facilitate reproducibility of this work, except for the weights in the loss function. In addition, it would be a large contribution to our community if the authors consider opening the training data.																				
562	Weakly Supervised Online Action Detection for Infant General Movements 	The paper presents a detailed description of the proposed method, and the implementation details of the training strategy are expressed in detail. However, many parameters in the design are missing, so re-implementation is difficult.									Regarding the reproducibility of the research, the authors planned to release the code and models, but there is no plan to disclose the dataset.									"The reproducibility of the paper is not very high. Because the dataset used in this paper was not publicly available. Based on the ""No Free Lunch Theorem for Machine Learning"", the proposed framework may not be able to perform well on different datasets as it performed on the in-house dataset."																				
563	Weakly Supervised Segmentation by Tensor Graph Learning for Whole Slide Images 	Of course, the paper itself is not reproducible, as too many details are not described (Likely to space limit). But the general ideas are well-reported and easy to get. The authors suggest that they will release the source code for the experiments and used public data. This should guarantee a high degree of reproducibility for the paper.									The authors state that the code will be released. The data is also public. However as the method is not easy to understand, it may be hard to reproduce the results.									Yes																				
564	Weakly Supervised Volumetric Image Segmentation with Deformed Templates 	The paper is reproducible as authors include the code (which I haven't personally tested).									Codes are promised to be released.									Fairly clearly described core approach which wouldn't be hard to reproduce, though the exact set up is not described (see notes above).  Experiments are not explained clearly though. It is claimed that code will be made available.																				
565	Weakly-supervised Biomechanically-constrained CT/MRI Registration of the Spine 	the work appears reproducible (clear description of the methods and also a robust evaluation strategy), although the primary data set is unlikely to be accessible.									It is difficult to reproduce without open source codes.									nan																				
566	Weakly-supervised High-fidelity Ultrasound Video Synthesis with Feature Decoupling 	According to the authors the code and data will be available, but it is not challenging to replicate this work.									The author checked all reproducibility questions. I think it is easy to implement the code given the description provided in this paper.									Data and code will be available, according to the response.																				
567	Weighted Concordance Index Loss-based Multimodal Survival Modeling for Radiation Encephalopathy Assessment in Nasopharyngeal Carcinoma Radiotherapy 	The author used the proposed method on the private data set, which is not conducive to the reproduction of the method.									The dataset is private, which I assume will not be made public. The authors answered the codes for this study will be released.									The authors state they will release the trained model if accepted. But the dataset will be private, so reproducing the same results would be impossible. And training a model implemented independently based on the description would also be impossible. Something similar could be done, but not exact.																				
568	What can we learn about a generated image corrupting its latent representation? 	highly reproducible									Authors will not make the code publicly available. But the method is easy to be implemented.									Implementing the method should not be a problem because the idea is straightforward.																				
569	What Makes for Automatic Reconstruction of Pulmonary Segments 	Without the medical data and label, this work is hard to be reproduced.									This paper can only be reproduced with the open dataset.  The method seems easy to be reproduced.									The reproducibility of the paper is OK																				
570	White Matter Tracts are Point Clouds: Neuropsychological Score Prediction and Critical Region Localization via Geometric Deep Learning 	By using the openly available data from the Human Connectome Project and a brief description of the statistics of the subjects used, reproducibility seems to be possible. In addition, the settings and software framework are described in Section 2.5. The authors' answers on the reproducibility checklist confirm the given reproducibility. It would be beneficial, if the code would be provided on github.									It should be easy for authors to provide source code for reproducibility analysis.									This work is fairly easy to implement given the background on white matter analysis and deep learning.																				
571	Whole Slide Cervical Cancer Screening Using Graph Attention Network and Supervised Contrastive Learning 	Very good									Not sure as there is no sufficient source code-related information available.									The workflow is introduced clearly in this paper, including patches extraction and ranking, graphs construction, graph attention network and loss design. Overall, the reproducibility is acceptable.																				
572	Why patient data cannot be easily forgotten? 	Likely to be reproducible.									The paper evaluates the method on publicly available data.  Information on the dataset split and training process is provided. The method is sufficiently explained.									Sufficient information to reproduce the results is provided.																				
573	XMorpher: Full Transformer for Deformable Medical Image Registration via Cross Attention 	This paper has provided details about the models, datasets, and evaluation.									The authors claimled they will release their code publicly.									Good.																				
574	Y-Net: A Spatiospectral Dual-Encoder Network for Medical Image Segmentation 	-- The authors are providing the code in the supplementary materials. -- Providing the requirement and the hyper-parameters for the framework in the paper is a plus.									The network is not too complicated to reproduce. The dataset is open access.									Good																				
